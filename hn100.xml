<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 09 Oct 2023 02:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Forty years of programming (144 pts)]]></title>
            <link>https://fabiensanglard.net/40/index.html</link>
            <guid>37814748</guid>
            <pubDate>Sun, 08 Oct 2023 21:55:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fabiensanglard.net/40/index.html">https://fabiensanglard.net/40/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=37814748">Hacker News</a></p>
<div id="readability-page-1" class="page"><br><center>
    
</center><p>
Oct 8, 2023</p>
<p>Forty years of programming</p><hr>


<p>I am about to turn forty-six. This means I have been programming for forty years, half of them professionally. During most of that time, I used a "standard" setup with 104 keyboard, a flat mouse, and a sitting desk.</p>

<img loading="lazy" src="https://fabiensanglard.net/40/macpro.webp" width="1932" height="1152"><span><i><small>My home workstation circa 2011 (porting Doom III to mac)</small></i></span>
 
<p>Things evolved ten years ago when I started to experience pain in my forearms and shoulders when I programmed. Here is what I did to solve my problem, it may work for someone else.</p>


<p>Mouse</p><hr>
<p>
Using a vertical mouse improved things a lot. My favorite is the Evoluent VerticalMouse 4.
</p>

<img loading="lazy" src="https://fabiensanglard.net/40/vertical_mouse.webp" width="2650" height="1665"><span><i><small>My work workstation circa 2015</small></i></span>

<p>Eventually, I opted for the Magic Trackpad from Apple. It is great to switch workspace with three fingers, zoom, and more. Having it in the center allows me to use it alternatively with my left and right hand.</p>

<img loading="lazy" src="https://fabiensanglard.net/40/magic.webp" width="2955" height="1368"><span><i><small>My home workstation circa 2015</small></i></span>

<p> It used to be annoying to get the drivers for Linux/Windows but now it is all sorted out.</p>

<p>Keyboard</p><hr><p>The first ergonomic keyboard I tried was the <a href="https://kinesis-ergo.com/shop/freestyle2-for-pc-us/">KINESIS Freestyle2</a>. It allowed me to spread each half as needed, resulting in horizontally straight wrist . However, the standard <code>Control</code>, <code>Shift</code>, and <code>Alt</code> still required wrist gymnastics. Same thing for <code>Esc</code> (I talk about VIM later) which required a left wrist twist. Also the tenting angle was too low.</p>

<p>I tried the <a href="https://kinesis-ergo.com/shop/advantage2/">KINESIS Advantage2</a>. I liked the concept of having so many thumb options. But the fixed width was a step backward compared to the KINESIS Freestyle2.</p>

<p>The keyboard which has it all for me is the  <a href="https://ergodox-ez.com/">Ergodox EZ</a>. It can be as wide or narrow as I need. And the custom firmware is a highly customizable gem. Among many features, it can switch all keys into a different layer with a single keystroke. The silver bullet for me is the ability to have a key function change if you keep it pressed.</p>

<p> If you look at <a href="https://configure.zsa.io/ergodox-ez/layouts/9qw4W/latest/0">my layout</a> you can see how capitalization (<code>Shift</code>) can be done by maintaining pressed either <code>D</code> or <code>K</code>.  All my symbols <code>{</code>, <code>[</code>, <code>(</code>, ... are on another layer available from a single key maintained pressed.</p> 


<img loading="lazy" src="https://fabiensanglard.net/40/ergodox_ez.webp" width="2622" height="1569"><span><i><small>My work keyboard in 2023</small></i></span>

<p>With the Ergodox EZ, my wrists never move. They are always in a rest position, on all three axes. Zero pain and I can program all day.</p>


<p>Additionally, the Ergodox accepts hardware tuning, like <a href="https://drop.com/buy/carbon">DROP Carbon keycaps</a>, custom cables by <a href="https://www.pexonpcs.co.uk/">pexonpcs.co.uk</a>, and <a href="https://www.keychron.com/products/gateron-switch-set">Brown Gateron G Pro.</a></p>


<blockquote><b>Trivia:</b> As a fan of IBM's Model M clicky keyboard, I tried to build my first Ergo using Cherry MX Blue. That was fine for home but made some coworkers upset. I recommend going for Cherry MX Red which are the most silent, or the Cherry MX Brown which are a good middle ground between Blue and Red.</blockquote>


<p>VIM</p><hr><img loading="lazy" src="https://fabiensanglard.net/40/vim.svg" width="544.16998" height="544.8642"><p>As you will have guessed my goal is to move my hands and twist my wrists as little as possible. That would be a problem to navigate a program since most IDEs require clicking via the mouse. Thankfully, most editors have a VIM mode which allows you to move across a file, goto definition, go back, all that without using the mouse.</p>


<p>Desk</p><hr><p>Standing up improves my posture. I don't slouch when I stand. So I built a motorized standing desk with <a href="https://amzn.to/48xIGKc">Topsky legs</a> and a Home Depot <a href="https://www.homedepot.com/p/Hampton-Bay-4-ft-L-x-25-in-D-Unfinished-Birch-Butcher-Block-Countertop-in-With-Standard-Edge-birch-4ft-x-25in/319222041">butcher counter top</a>.</p>



<img loading="lazy" src="https://fabiensanglard.net/40/desk.webp" width="2640" height="1988"><span><i><small>My home desk, at the time used for amazing Diablo 2 Resurrected adventures</small></i></span>

<p>The three position memory allow to switch from standing to seating within a few seconds. And I try to alternate during the day.</p>



<p>Stretching</p><hr><p>I take a break every once in a while and do a bunch of Wall Angel.</p>

<p>Meditation in motion</p><hr><p>I manage my stress level by disconnecting from work when I leave it. Rock climbing works well for me. You can't think of anything else when you climb. It is a great way to turn off a brain that keeps on having great optimization ideas when it is not the time anymore.</p>

<img loading="lazy" src="https://fabiensanglard.net/40/scalade.webp" width="707" height="462"> <hr>
 <center>*</center></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why I can no longer recommend a Mac to fellow blind computer users (216 pts)]]></title>
            <link>https://www.applevis.com/blog/we-deserve-better-apple-why-i-can-no-longer-recommend-mac-fellow-blind-computer-users</link>
            <guid>37813895</guid>
            <pubDate>Sun, 08 Oct 2023 19:59:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.applevis.com/blog/we-deserve-better-apple-why-i-can-no-longer-recommend-mac-fellow-blind-computer-users">https://www.applevis.com/blog/we-deserve-better-apple-why-i-can-no-longer-recommend-mac-fellow-blind-computer-users</a>, See on <a href="https://news.ycombinator.com/item?id=37813895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      

<article data-history-node-id="35728">
  <header>
    
          
          </header>
  <div>
        
            <div><p>As many of you will know from personal experience, there is a longstanding issue with VoiceOver on Mac where Safari will frequently become unresponsive with VoiceOver repeatedly announcing the message “Safari not responding.” When this issue occurs, the user's Mac may become unusable for up to several minutes at a time. Sometimes it can be resolved by switching away from Safari. Sometimes restarting VoiceOver can resolve the issue. However, far too often, the user is unable to switch away from Safari or turn VoiceOver off, instead having to simply wait for their Mac to become responsive again.</p>

<p>This “Safari not responding” behaviour when using VoiceOver dramatically impacts productivity and overall usability of Macs for blind and low vision users. Furthermore, it appears that the issue extends beyond just Safari - many other common applications that utilise Apple's WebKit browser engine can also be affected by the “not responding” problem.</p>

<p>It is also important to point out this issue occurs regardless of the specification level of the Mac - it has been widely experienced on the latest Apple silicon-equipped Macs with 16GB or more of RAM, so even owners of top-tier new Mac hardware still face this crippling VoiceOver bug.</p>

<p>This critical problem has persisted for years across multiple MacOS versions without a permanent fix from Apple. Given the longevity and level of disruption caused by this bug across Safari and other applications, I can unfortunately no longer in good faith recommend Macs to anyone who relies on using VoiceOver. The impact of this bug when performing routine, and often critical, tasks in Safari and other applications simply makes macOS an unreliable and frustrating platform.</p>

<p>Macs have traditionally been popular within the blind community, and they offer some great accessibility features. However, frankly, Apple should be utterly ashamed that they have let this issue persist for so long without a permanent fix. It's a failing that raises serious questions about the company's frequently stated commitment to accessibility. There can be no doubt that if sighted users were to experience something similar, that it would receive significant media coverage and that Apple's response would be fast.</p>

<p>I want to note that I am sympathetic to the difficulties Apple's engineering team likely faces in resolving this issue. Based on user reports, there appears to be no consistent way to reproduce the “Safari not responding” behaviour - it can occur randomly and sporadically. The same web page may work fine multiple times before suddenly triggering a freeze. There are also inconsistencies across different users, machines, and configurations. I imagine these factors make it very challenging to isolate and fix the underlying problem. However, given the engineering talent and resources available to Apple, the challenge should not be insurmountable.</p>

<p>I believe we need to escalate out urging of Apple to prioritise and permanently solve the “Safari not responding” bug that has plagued VoiceOver users for far too long. To this end, I encourage those of you who use VoiceOver on Mac to directly contact Apple's accessibility team at <a href="mailto:accessibility@apple.com">accessibility@apple.com</a> to share your own personal experiences with and frustrations about the “Safari not responding” issue. It is important we continue putting direct, polite pressure on Apple to prioritise resolving this problem. Please be constructive in expressing your concerns. Consider also copying Apple CEO Tim Cook on your email by using his publicly shared email address <a href="mailto:tcook@apple.com">tcook@apple.com</a> to ensure he sees the direct impact this ongoing bug is having on Apple's blind and low vision customers.</p>

<p>I want to emphasise that the “Safari not responding” bug is far from the only issue effecting VoiceOver users on Mac. As <a href="https://www.applevis.com/blog/macos-sonoma-new-features-changes-improvements-bugs-blind-low-vision-users">our recent post on problems in macOS Sonoma and the replies outline,</a> there are numerous other VoiceOver frustrations and failures impacting users. However, it has become a yardstick by which Apple's overall performance on and commitment to accessibility is being judged. It is a yardstick against which Apple has failed for some considerable time.</p>

<p>I know many of you share my frustration. I welcome your perspectives and discussion in the comments. Collectively, we need to apply consumer pressure by being vocal about this issue and not purchasing new Macs until the “Safari not responding” issue is fixed once and for all. Apple simply must do better and restore our trust that Macs provide a stable and fully accessible experience for its blind and low vision customers.</p></div>
      
  <div>
    <h3>Tags</h3>
          
      </div>




<section data-drupal-selector="comments">

      
    
    
  
  
<article data-comment-user-id="30010" id="comment-155583" role="article" data-drupal-selector="comment">
     <p>
               
        <h3><a href="https://www.applevis.com/comment/155583#comment-155583" rel="bookmark" hreflang="en">My opinion</a></h3>
        
          </p>
  <span data-comment-timestamp="1696794983"></span>

  <div>
      
            <p>The only matter I had the supplier not responded she is on the MacBook Pro 2012. I’ve also had it happen either 2017 MacBook Air, but not with my Apple Silicon MacBook Air. The only reason I use my Mac is for music creation and even Matts becoming a hassle</p>
      
    </div>
</article>

<article data-comment-user-id="1178" id="comment-155584" role="article" data-drupal-selector="comment">
     <p>
               
        <h3><a href="https://www.applevis.com/comment/155584#comment-155584" rel="bookmark" hreflang="en">Shared to HackerNews</a></h3>
        
          </p>
  <span data-comment-timestamp="1696796293"></span>

  
</article>

<article data-comment-user-id="23639" id="comment-155585" role="article" data-drupal-selector="comment">
     <p>
               
        <h3><a href="https://www.applevis.com/comment/155585#comment-155585" rel="bookmark" hreflang="en">I agree.</a></h3>
        
          </p>
  <span data-comment-timestamp="1696797839"></span>

  <div><p>I've been a windows user for most of my life and don't plan on switching to a mac at all.</p>
<p>I tried it once, never again.</p>
</div>
</article>

<article data-comment-user-id="26568" id="comment-155587" role="article" data-drupal-selector="comment">
     <p>
               
        <h3><a href="https://www.applevis.com/comment/155587#comment-155587" rel="bookmark" hreflang="en">This major problum is just not ok</a></h3>
        
          </p>
  <span data-comment-timestamp="1696798322"></span>

  <div>
      
            <p>I use windows and never used a mac nor do i want to<br>
If i am understanding this correctly vo will say safari not responding and then the keyboard doesnt work, is this correct? If so thats unacceptable</p>
      
    </div>
</article>

<article data-comment-user-id="1445" id="comment-155589" role="article" data-drupal-selector="comment">
     <p>
               
        <h3><a href="https://www.applevis.com/comment/155589#comment-155589" rel="bookmark" hreflang="en">Safari Not Responding</a></h3>
        
          </p>
  <span data-comment-timestamp="1696798858"></span>

  <div>
      
            <p>Maybe I have been very lucky, but I have not experienced the same level of frustration with my Mac. I have been using a MacBook Air since 2013, and my current Mac is a 2020 M1 MacBook Air with 8 GB of RAM. While I have occasionally encountered the  "Safari not responding" issue, I could easily work around this by turning VO off and on. I use my Mac for creating documents, working with spreadsheets, using the Internet, managing my music library, handling emails, among other things. Based on my experience, I would have no problem recommending a Mac to a blind user. Of course, each person must determine their own use case.<br>
Over the past ten years, I have encountered numerous bugs but none of them were show stoppers for me. I imagine that Windows with JAWS or NVDA also has its share of bugs.<br>
Given that this bug seems to not affect all Mac users with the same severity, this may make it a more difficult issue for Apple to address. However, given the comments that I have read on Applevis, I agree that this should be a priority for their Accessibility team.</p>
      
    </div>
</article>

<article data-comment-user-id="8869" id="comment-155591" role="article" data-drupal-selector="comment">
     <p>
               
        <h3><a href="https://www.applevis.com/comment/155591#comment-155591" rel="bookmark" hreflang="en">Upgrade to latest Mac Mini on hold</a></h3>
        
          </p>
  <span data-comment-timestamp="1696799187"></span>

  <div>
      
            <p>I was looking forward to upgrading my 2018 Mac Mini - just waiting for the M3 to be released. But that's now tabled. I stand in solidarity with David. No more new Apple hardware until this long-standing and unacceptable issue is resolved.</p>
      
    </div>
</article>

<article data-comment-user-id="26568" id="comment-155596" role="article" data-drupal-selector="comment">
     <p>
               
        <h3><a href="https://www.applevis.com/comment/155596#comment-155596" rel="bookmark" hreflang="en">My experience with jaws</a></h3>
        
          </p>
  <span data-comment-timestamp="1696800662"></span>

  <div>
      
            <p>Hi<br>
I used jaws for 5 years and dont remember any major bugs with it<br>
I am having a problem with the computer i use at work where jaws sometimes crashes, HP laptop running windows 10 and jaws 2021 but that doesnt happen offten</p>
      
    </div>
</article>

<article data-comment-user-id="8869" id="comment-155597" role="article" data-drupal-selector="comment">
     <p>
               
        <h3><a href="https://www.applevis.com/comment/155597#comment-155597" rel="bookmark" hreflang="en">For those of you not experiencing this issue</a></h3>
        
          </p>
  <span data-comment-timestamp="1696801768"></span>

  <div><p>I can't speak for David, but I personally feel this post will have more impact if those of you not experiencing this issue please <a href="https://www.applevis.com/forum/macos-mac-apps/safari-v15-mac-busy-busy-busy">discuss</a> your <a href="https://www.applevis.com/forum/macos-mac-apps/safari-busy-busy-busy-busy-busy">system config</a> in <a href="https://www.applevis.com/forum/macos-mac-apps/safari-not-responding">any</a> of the <a href="https://www.applevis.com/forum/macos-mac-apps/fixing-safari-not-responding-problem">numerous</a> posts devoted to <a href="https://www.applevis.com/forum/apple-beta-releases/i-dispair-safari-not-responding-just-bad-latest-beta-until-fixed-i-cannot">trying</a> to <a href="https://www.applevis.com/forum/apple-beta-releases/safari-still-not-responding">find</a> a <a href="https://www.applevis.com/forum/macos-mac-apps/please-apple-make-safari-respond">solution.</a> And I'll warn you in advance, lots of suggestions have been made over the years, and there is no silver bullet.</p>

<p>Most of us have been experiencing this issue for years. Please don't pretend this issue doesn't exist.</p></div>
</article>


  
</section>

  </div>
  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Email Steve Jobs sent to himself 1 year before his passing (103 pts)]]></title>
            <link>https://officechai.com/stories/steve-jobs-email-to-self/</link>
            <guid>37812342</guid>
            <pubDate>Sun, 08 Oct 2023 17:08:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://officechai.com/stories/steve-jobs-email-to-self/">https://officechai.com/stories/steve-jobs-email-to-self/</a>, See on <a href="https://news.ycombinator.com/item?id=37812342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
<main id="main" role="main">
<article id="post-43561">

<p><img width="1920" height="1080" src="https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357.jpg" alt="" srcset="https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357.jpg 1920w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-300x169.jpg 300w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-1024x576.jpg 1024w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-768x432.jpg 768w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-1536x864.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px" data-srcset="https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357.jpg 1920w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-300x169.jpg 300w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-1024x576.jpg 1024w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-768x432.jpg 768w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-1536x864.jpg 1536w" data-src="https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="> </p>
<div>



<p>People at the top of their professional games are thought to be quite self centered and sure of themselves. But one of the best-known tech figures of all time had a remarkably poignant take on life an humanity a year before he passed away.</p>
<figure><img width="1024" height="576" src="https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-1024x576.jpg" alt="Steve Jobs email to self humanity" srcset="https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-1024x576.jpg 1024w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-300x169.jpg 300w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-768x432.jpg 768w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-1536x864.jpg 1536w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px" data-srcset="https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-1024x576.jpg 1024w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-300x169.jpg 300w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-768x432.jpg 768w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-1536x864.jpg 1536w, https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357.jpg 1920w" data-src="https://officechai.com/wp-content/uploads/2022/10/getty_96211011_2000134813806405780_414357-1024x576.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure>
<p>In 2010, Steve Jobs had been diagnosed with cancer, and had cut down on his work comittments. At that point, he had written an email to himself, and sent it via what else, but his iPad. The email ruminated on the fact that most things in Jobs’ life weren’t controlled by him. He said that he didn’t grow most of the food he ate, used a language he didn’t develop, listened to music which he didn’t write, was governed by laws which he didn’t come up with. </p>
<p>This led Jobs to a beautiful conclusion, which was the closing line of his email: I love and admire my species, living and dead, and am totally dependent on them for my life and well being..</p>
<p>One of the greatest entrepreneurs of all time, who’d created what is now the world’s most valuable company, and whose products had touched millions of lives, was putting things in perspective, and saying that most of his life had been built by other members of his species. Sure, he might’ve created beautiful products and tremendous amounts of shareholder value, but Jobs was acknowledging that he had stood on shoulders of giants. And if Steve Jobs felt that the outcome of his life had been largely determined by the human species as a whole, it does put a lot off pressure off ordinary folks like you and me.</p>
<figure><img src="https://pbs.twimg.com/media/FcV8SqoacAMj9gD?format=png&amp;name=large" alt="Steve Jobs email to self"></figure>
<p>Here’s the full text of the letter:</p>
<p>From: Steve Jobs, <a href="https://officechai.com/cdn-cgi/l/email-protection" data-cfemail="ed9e87828f9ead8c9d9d8188c38e8280">[email&nbsp;protected]</a> To: Steve Jobs, <a href="https://officechai.com/cdn-cgi/l/email-protection" data-cfemail="82f1e8ede0f1c2e3f2f2eee7ace1edef">[email&nbsp;protected]</a> Date: Thursday, September 2, 2010 at 11:08PM</p>
<p><br>I grow little of the food I eat, and of the little I do grow I did not breed or perfect the seeds. </p>
<p>I do not make any of my own clothing. </p>
<p>I speak a language I did not invent or refine. I did not discover the mathematics I use. </p>
<p>I am protected by freedoms and laws I did not conceive of or legislate, and do not enforce or adjudicate.</p>
<p> I am moved by music I did not create myself.</p>
<p> When I needed medical attention, I was helpless to help myself survive.</p>
<p><br>I did not invent the transistor, the microprocessor, object oriented programming, or most of the technology I work with.<br></p>
<p>I love and admire my species, living and dead, and am totally dependent on them for my life and well being.</p>
<p><br>Sent from my iPad</p>



 </div>

</article>
</main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing down unfiltered thoughts enhances self-knowledge (110 pts)]]></title>
            <link>https://www.scientificamerican.com/article/know-yourself-better-by-writing-what-pops-into-your-head/</link>
            <guid>37812332</guid>
            <pubDate>Sun, 08 Oct 2023 17:08:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scientificamerican.com/article/know-yourself-better-by-writing-what-pops-into-your-head/">https://www.scientificamerican.com/article/know-yourself-better-by-writing-what-pops-into-your-head/</a>, See on <a href="https://news.ycombinator.com/item?id=37812332">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-behavior="newsletter_promo dfp_article_rendering" data-dfp-adword="Advertisement" data-newsletterpromo_article-text="<p>Sign up for <em>Scientific American</em>&amp;rsquo;s free newsletters.</p>" data-newsletterpromo_article-image="https://static.scientificamerican.com/sciam/cache/file/4641809D-B8F1-41A3-9E5A87C21ADB2FD8_source.png" data-newsletterpromo_article-button-text="Sign Up" data-newsletterpromo_article-button-link="https://www.scientificamerican.com/page/newsletter-sign-up/?origincode=2018_sciam_ArticlePromo_NewsletterSignUp" name="articleBody" itemprop="articleBody"><p>For decades, physician and author Silke Heimes has been leading groups in therapeutic exercises to put thoughts and feelings down on paper. Heimes, a professor of journalism at Darmstadt University of Applied Sciences,&nbsp; points to abundant evidence that <a href="https://www.munich-business-school.de/insights/en/2021/the-power-of-writing-interactive-guest-lecture-from-prof-silke-heimes/">writing for five to 20 minutes a day</a> can improve health, diminish stress, increase self-confidence and even kindle the imagination. A writing routine, she argues, is a form of mental hygiene that almost anyone can benefit from.</p>

<p>So how do you start? What happens if—as every writer fears—the page remains blank? And how do you get rid of an overcritical inner censor? Heimes, director of the Institute for Creative and Therapeutic Writing in Darmstadt, explains how to overcome inhibitions and open up your inner world.</p>

<p>[<em>An edited transcript of the interview follows</em>.]</p>

<p><strong>If you want to write in order to understand yourself better, what's the best way to start?</strong></p>

<p>There are writing exercises, for example in so-called fill-in journals, where you directly answer a question. But if I just want to get started without any aids, the best way is to use the method of automatic writing. That means I set myself a short time window, maybe five minutes, in which I write continuously without thinking, without putting down the pen or rereading what I’ve written. The goal is to get thoughts down on paper as unfiltered as possible so that an inner censor can't switch on—or at least doesn't get too loud. It helps not to set the goal too high—not to expect too much—but to understand this writing as a time-out, so to speak, or as a kind of warm-up exercise.</p>

<p><strong>Wouldn’t it be</strong> <strong>helpful to ask yourself specific questions?</strong></p>

<p>If you want to, you can follow programs that, for example, organize specific questions into topics. But that can also be inhibiting at times because such questions primarily get your head working to produce rational answers. Questions often steer thoughts along preconceived paths. Sometimes it is almost easier without them to let the gut lead the way.</p>

<p><strong>W</strong><strong>hat if you just can't think of anything?</strong></p>

<p>The half-sentence method can help. With this approach, you complete a given half-sentence such as "When I woke up this morning” or “What happened to me today.” If you write in the morning, the [first example] is a good choice. Because everyone wakes up in the morning, everyone can think of something to say about it. The same applies to [the second example] if you write in the evening because you inevitably experienced something during the day by then. To start, you can also write down words that begin with the letters of your name and then create a text using those words.</p>

<p><strong>Can anything go wrong using these methods?</strong></p>

<p>Not really. Just as with thinking, you can of course get tangled up in your own thoughts or get stuck in brooding loops when writing. But that’s not the fault of the writing itself; it’s just something that becomes obvious on paper. Writing often deals with emotional issues, so you also might temporarily feel bad because something is stirred up or triggered. In that case, you should take a break and do something else or talk to someone about it. If the feeling persists, it is best to seek professional help.</p>

<p><strong>Does it make a difference whether you write by hand or on a keyboard?</strong></p>

<p>Writing by hand is a very complex movement that activates more areas in the brain, which leads to being more creative. It also usually means slowing down, which invites you to pause and take a breath. In addition, there is something sensual and unique about writing by hand. because, for one thing, our handwriting is very individual. And for another thing, it tells us something about our state of mind. In fact, handwriting usually becomes rounder and livelier when we are in a good mood and smaller or tighter when we are not feeling so well. Typing on the keyboard, on the other hand has a soothing quality because it is very rhythmic. Further, it has the advantage of allowing you to share your writing more quickly. I think it’s always good to have both skills and to use them.</p>

<p><strong>You have guided many groups in this type of writing. How does that typically work?</strong></p>

<p>We first do little writing exercises to warm up. Many people come with the expectation that they’ll sit down, and the writing will flow right away—that they’ll perform brilliantly almost off the cuff. But no athlete, no musician would expect that of themselves. Professional writers know better.</p>

<p>And there are other common misconceptions. The biggest one is “I can't write.” A lot of people come to my seminars with this attitude. But we can all write. Rather the problem is the often exaggerated demands we place on ourselves. I like to quote French writer André Breton, who invented automatic writing. He said, mutatis mutandis, that if you want to write, find a nice place, sit down in peace and quiet and forget about seeking out brilliant thoughts.</p>

<p><strong>Is there anything else that people particularly struggle with when it comes to writing?</strong></p>

<p>We’ve already talked about your own performance expectations. But what can also lead to inhibitions is the fear of emotions or of your personal history—fear of confronting possibly painful topics. And further problems usually arise when people want to put their thoughts into a literary form in order to publish them.</p>

<p><strong>What if someone only produces platitudes? What if they sound banal or superficial?</strong></p>

<p>Who decides that? That is a judgment that should be unacceptable in creative and therapeutic writing. Everyone expresses what is important, right and possible for them at that moment, and I think that is precisely what deserves appreciation.</p>

<p><strong>What do people in your groups write about most often?</strong></p>

<p>They write about the topic of self-worth—that is, the fear of not being good enough—about not being heard or seen and about the topic of freedom versus security, especially at work.</p>

<p><strong>And what insights do they go home with?</strong></p>

<p>That varies greatly. But they often take home a lot of pieces of paper, and that’s how they recognize that they can definitely write. They have produced something and are justifiably proud of it. This increases their self-esteem, and they develop more confidence. Writing also sharpens perception and promotes mindfulness. People notice more quickly when something is not good for them and find better ways to deal with those problems. And when thoughts go round in circles, putting them down on paper clears the mind. After that you have more capacity for other things in your life.</p>

<p><strong>Can these benefits be probed empirically?</strong></p>

<p>There are [hundreds] of studies on the effect of expressive or therapeutic writing. Many of them come from the psychologist James Pennebaker, who did research on this primarily with students.</p>

<p><strong>Do you write a lot yourself?</strong></p>

<p>Yes, every day. I work on a novel or nonfiction book every day, and I also jot down my thoughts for three minutes in the morning. These few minutes of mental hygiene are as important and natural to me as brushing my teeth every day.</p>

<p><em>This article originally appeared in</em> Spektrum der Wissenschaft <em>and was reproduced with permission</em>.</p></div><section><h3>ABOUT THE AUTHOR(S)</h3><div><ul></ul><p><strong>Christiane Gelitz</strong> is a psychologist and an editor at <em>Spektrum der Wissenschaft</em>.</p></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenIPC: Alternative open firmware for your IP camera (256 pts)]]></title>
            <link>https://github.com/OpenIPC</link>
            <guid>37812217</guid>
            <pubDate>Sun, 08 Oct 2023 16:57:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/OpenIPC">https://github.com/OpenIPC</a>, See on <a href="https://news.ycombinator.com/item?id=37812217">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemprop="text"><h2 id="user-content-alternative-open-firmware-for-your-ip-camera--" dir="auto"><a href="#alternative-open-firmware-for-your-ip-camera--">Alternative open firmware for your IP camera  </a><a href="https://t.me/OpenIPC" rel="nofollow"><img src="https://camo.githubusercontent.com/69edb3d30ff26af89903ca32db2a30f33fdf404eafdc5bd7a993d4a69e8daa7e/68747470733a2f2f6f70656e6970632e6f72672f696d616765732f74656c656772616d5f627574746f6e2e737667" alt="Telegram" data-canonical-src="https://openipc.org/images/telegram_button.svg"></a></h2>
<p dir="auto"><em>(based on Buildroot)</em></p>
<p dir="auto">OpenIPC is an open source operating system from the <a href="https://github.com/OpenIPC/.github/blob/main/opencollective">open community</a>
targeting for IP cameras with ARM and MIPS processors from several manufacturers in
order to replace that closed, opaque, insecure, often abandoned and unsupported
firmware pre-installed by a vendor.</p>
<p dir="auto">OpenIPC <a href="https://github.com/openipc/firmware/">firmware</a> comes as binary pre-compiled files for easy
installation by end-user. Also, we provide full access to the source files for
further development and improvement by any capable programmer willing to
contribute to the project. OpenIPC source code is released under one of the most
simple open source license agreements, <a href="https://opensource.org/license/mit/" rel="nofollow">MIT License</a>, giving users express
permission to reuse code for any purpose, even as part of a proprietary software.
We only ask you politely to contribute your improvements back to us. We would
be grateful for any feedback and suggestions.</p>
<p dir="auto">Historically, OpenIPC <a href="https://github.com/openipc/firmware/">firmware</a> only supported SoC manufactured by
HiSilicon, but as the development continues, the list of supported processors
expands. Today, it also includes chips from <em>Ambarella</em>, <em>Anyka</em>, <em>Fullhan</em>, <em>Goke</em>,
<em>GrainMedia</em>, <em>Ingenic</em>, <em>MStar</em>, <em>Novatek</em>, <em>SigmaStar</em>, <em>XiongMai</em>, and is
expected to grow further.</p>
<p dir="auto">More information about the <a href="https://github.com/openipc/">project</a> is available in our <a href="https://openipc.org/" rel="nofollow">website</a>
and on the <a href="https://github.com/OpenIPC/wiki">wiki</a>.</p>
<p dir="auto">
<a href="https://opencollective.com/openipc/contribute/backer-14335/checkout" rel="nofollow"><img src="https://camo.githubusercontent.com/440363e6c94395b51235765a32ea883666f3c6460067200c06353499e6539bf6/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7765627061636b2f646f6e6174652f627574746f6e4032782e706e673f636f6c6f723d626c7565" width="250" alt="Open Collective donate button" data-canonical-src="https://opencollective.com/webpack/donate/button@2x.png?color=blue"></a>
</p>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Tailscale Universal Docker Mod (165 pts)]]></title>
            <link>https://tailscale.dev/blog/docker-mod-tailscale</link>
            <guid>37812142</guid>
            <pubDate>Sun, 08 Oct 2023 16:51:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tailscale.dev/blog/docker-mod-tailscale">https://tailscale.dev/blog/docker-mod-tailscale</a>, See on <a href="https://news.ycombinator.com/item?id=37812142">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p>Imagine a world where you could add applications to your tailnet the same way you add machines to it. This would mean that <code>http://wiki</code> would go to your internal wiki, <code>http://code</code> would take you to an IDE, and <code>http://chat</code> would take you to your internal chat server. This is the world that Tailscale lets you create, but historically the details on how you would actually do this are left as an exercise for the reader.</p><p>Today, we're introducing a new way to add Tailscale to your Docker containers: our brand new <a href="https://github.com/linuxserver/docker-mods/blob/master/README.md" target="_blank" rel="noreferrer">universal Docker mod</a>. This lets you add Tailscale to any Docker container based on <a href="https://linuxserver.io/" target="_blank" rel="noreferrer">linuxserver.io</a> images. This lets you have applications join your tailnet just as easily as machines can. You can set up a wiki on <code>http://wiki</code>, an IDE at <code>http://code</code>, and a chat server at <code>http://chat</code> and have them all be accessible over your tailnet. You can even use this to expose your internal applications to the public internet with <a href="https://tailscale.com/kb/1112/funnel/" target="_blank" rel="noreferrer">Funnel</a>.</p><div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p>You can even use this to SSH into containers!</p></div></div><div><p><img alt="A picture of the character Aoi in a coffee mood.}" srcset="https://tailscale.dev/_next/image?url=%2Fimages%2Fxe%2Fstickers%2Faoi%2Fcoffee.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Fimages%2Fxe%2Fstickers%2Faoi%2Fcoffee.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Fimages%2Fxe%2Fstickers%2Faoi%2Fcoffee.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"><br></p><div><p>&lt;<b>Aoi</b>&gt; </p><p>You can <em>what</em> into a container?</p></div></div><div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p>Yep! <a href="https://tailscale.com/kb/1193/ssh/" target="_blank" rel="noreferrer">Tailscale SSH</a> lets you SSH into containers when you enable the <code>TAILSCALE_USE_SSH</code> setting and <a href="https://tailscale.com/kb/1193/tailscale-ssh/#ensure-tailscale-ssh-is-permitted-in-acls" target="_blank" rel="noreferrer">permit access in the ACLs</a>. This is a great way to get into a container without having to SSH into the docker host and run <code>docker exec -it &lt;container&gt; bash</code>.</p></div></div><p>To add this to your existing Docker containers with linuxserver.io images, add the following environment variables to your docker-compose.yml file:</p><div><p><code><br><div><p><span>- DOCKER_MODS=ghcr.io/tailscale-dev/docker-mod:main</span></p></div><div><p><span># tailscale configuration</span></p></div><div><p><span># make sure this is persisted in a volume</span></p></div><div><p><span>- TAILSCALE_STATE_DIR=/var/lib/tailscale</span></p></div><div><p><span>- TAILSCALE_SERVE_MODE=https</span></p></div><div><p><span>- TAILSCALE_SERVE_PORT=80</span></p></div><div><p><span>- TAILSCALE_USE_SSH=1</span></p></div><div><p><span>- TAILSCALE_HOSTNAME=wiki</span></p></div><div><p><span>## uncomment to enable funnel</span></p></div><div><p><span>## remember that if you do, it's exposed to the internet, so be careful!</span></p></div><div><p><span>#- TAILSCALE_FUNNEL=on</span></p></div><div><p><span># replace this with your authkey from the admin panel</span></p></div><div><p><span>- TAILSCALE_AUTHKEY=tskey-auth-hunter2CNTRL-hunter2hunter2</span></p></div><br></code></p></div><p>This will add Tailscale to your container so that you can access it over your tailnet. If you run <code>docker compose up -d</code> with the authkey changed out for <a href="https://login.tailscale.com/admin/settings/keys" target="_blank" rel="noreferrer">a valid authkey</a>, you'll be able to access your apps over Tailscale.</p><figure><img alt="flat color, shipyard, containers, mountains, no humans, space needle" srcset="https://tailscale.dev/_next/image?url=%2Fimages%2Fhero%2Fshipyard-vibes.png&amp;w=1920&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Fimages%2Fhero%2Fshipyard-vibes.png&amp;w=3840&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Fimages%2Fhero%2Fshipyard-vibes.png&amp;w=3840&amp;q=75" width="1280" height="512" decoding="async" data-nimg="1" loading="lazy"><figcaption>Image generated by <!-- -->Ligne Claire v1.5<!-- -->, prompt: <!-- -->flat color, shipyard, containers, mountains, no humans, space needle</figcaption><meta property="og:image" content="/images/hero/shipyard-vibes.png"></figure><h2 id="docker-and-docker-mods">Docker and Docker mods</h2><p>Docker allows you to create snapshots of operating system installs with a given state, such as "having the <a href="https://go.dev/" target="_blank" rel="noreferrer">Go</a> compiler available" or "install this program and all its dependencies" and distribute those preconfigured images on the Internet. When you consume the same Docker image at two time intervals T0 and T1, you get the same image with the same code, just as you expect.</p><p>When a Docker container is run, it usually runs on top of an ephemeral filesystem that gets destroyed when the container is stopped. This means that restarting the container will reset it back to the state that was there when the image was created. This is normally convenient when working on applications that make temporary changes to the filesystem, such as an image converter that uses temporary files to do the conversion logic.</p><p>This is less convenient when you want to run things like database servers in Docker. However, most of the time when you do things that need persistent state, that persistent state is usually limited to a single file or directory. Docker provides external persistent state with <a href="https://docs.docker.com/storage/volumes/" target="_blank" rel="noreferrer">volumes</a>. They're basically directories that are plunked into the container at runtime, but it maintains the state between container runs. This is great for things like databases because you wouldn't want to lose all your data when you restart the container.</p><p><img alt="The expected hierarchies of Docker containers and their statefulness" src="https://tailscale.dev/images/universal-docker-mod/docker-normal.svg" width="623" height="490" decoding="async" data-nimg="1" loading="lazy"></p><p>So, from here we can create a hierarchy for docker and statefulness. You expect docker containers to have state for <em>data</em>, and you also expect the docker container to be running the same code every time you run the same image. You don't expect anything else to be running, everything is deterministic at T0, T1, or TN.</p><div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p>This is a valid hierarchy because it's what you expect from docker. You expect the same code to run every time you run the same image.</p></div></div><h3 id="what-is-humor">What is humor?</h3><p>Humor is a complicated concept that is almost universal throughout human cultures. It's a way of conveying concepts like absurdity, irony, the absurdity of irony, and normally frustrating things in ways that aren't quite as much of a downer. It's really about being able to communicate subtle things like common errors that everyone makes when learning things (such as English and its rule of all the rules having exceptions, even for the exceptions). It's also a tool that you can use to help describe the abstract and nonphysical things like emotions, feelings, ideas, the human condition, and how Kubernetes works.</p><div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p>Humor is also really hard to convey properly in a written medium. This is even more difficult when the humor is about technology, which is usually hard to understand in the first place. I'm going to try to explain the humor in this article with these asides so that y'all can follow along, but if you already get why this is funny it may ruin the joke for you. Sorry!</p></div></div><p>In his famous presentation <a href="https://youtu.be/ar9WRwCiSr0" target="_blank" rel="noreferrer">Reverse emulating the NES</a>, fellow philosopher in arms <a href="http://tom7.org/" target="_blank" rel="noreferrer">tom7</a> introduced the idea of a type of humor called "invalid hierarchies". In this he does rather abusrd things to an NES using a custom circuit board and a raspbery pi to allow him to (among other things) run an SNES emulator on the NES. This video is quite possibly one of my favorite technical communication videos and is a huge influence to how I write humorous things for this blog.</p><div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p>This creates an invalid hierarchy because you expect the NES to only run 8-bit NES games, but not 16-bit SNES games. This is funny. If you've never seen that video before, it's well worth a watch.</p><p>Another example of an invalid hierarchy is my April Fool's Day post <a href="https://tailscale.dev/blog/headscale-funnel" target="_blank" rel="noreferrer">Using Tailscale without using Tailscale</a>. You'd expect to have to use Tailscale if you want to use Tailscale, but "using Tailscale without using Tailscale" creates an invalid hierarchy in the mind of the reader. This is also funny.</p></div></div><p><img alt="The expected hierarchies of NES and SNES consoles and their games" src="https://tailscale.dev/images/universal-docker-mod/nes-snes.svg" width="359" height="490" decoding="async" data-nimg="1" loading="lazy"></p><h3 id="docker-mods">Docker mods</h3><p>Docker mods let you install extra packages and services into containers at runtime. If the <code>ONBUILD</code> hook lets you run a series of commands when an image is built, you can think of docker mods as a missing <code>ONRUN</code> hook that lets you customize an image at runtime.</p><p><img alt="How Docker mods change the statefulness of a container" src="https://tailscale.dev/images/universal-docker-mod/docker-mod.svg" width="703" height="716" decoding="async" data-nimg="1" loading="lazy"></p><div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p>This creates an invalid hierarchy because we think about the code in a container being deterministic between invocations and this allows you to make something <em>nondeterministic</em>. This is funny.</p></div></div><h3 id="docker-mods-and-s6">Docker mods and s6</h3><p>At a high level, a docker mod is a series of files that add additional instructions to the start phase of a docker container. It works because the <a href="https://www.linuxserver.io/" target="_blank" rel="noreferrer">linuxserver.io</a> containers preinstall <a href="https://skarnet.org/software/s6/" target="_blank" rel="noreferrer">s6</a> via <a href="https://github.com/just-containers/s6-overlay" target="_blank" rel="noreferrer">s6-overlay</a> and then start it in the background to manage the lifecycle of services in the container.</p><div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p>This is also funny because usually Docker containers aren't supposed to have multiple processes running in them for simplicity, but it turns out that when you want to do things like put your wiki seamlessly on your tailnet, you want to have multiple processes running. This is another invalid hierarchy because you expect the container to only have one process running, but it has multiple with a service manager, just like the host OS.</p></div></div><p>When I made the docker mod, I had to create a few s6 services to help it run:</p><ul><li>One to set a list of packages that Tailscale needs to run (<a href="https://stedolan.github.io/jq/" target="_blank" rel="noreferrer">jq</a> to process some data from the packages server, and <a href="https://linux.die.net/man/8/iptables" target="_blank" rel="noreferrer">iptables</a> to configure the firewall inside the container for Tailscale to run in a <a href="https://en.wikipedia.org/wiki/TUN/TAP" target="_blank" rel="noreferrer">TUN</a> device).</li><li>One to download Tailscale to the container.</li><li>One to start the Tailscale node agent <code>tailscaled</code>.</li><li>One to authenticate you to the tailnet with <a href="https://tailscale.com/kb/1241/tailscale-up/" target="_blank" rel="noreferrer"><code>tailscale up</code></a> and set other settings like <a href="https://tailscale.com/kb/1242/tailscale-serve/" target="_blank" rel="noreferrer"><code>tailscale serve</code></a>.</li></ul><div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p>This is also hilarious because this roughly mirrors the process that you have to do on your host OS to get Tailscale running. This is another layer of invalid hierarchy because you expect containers to ship with all the software they need, but here is this container that needs to download software at runtime. This is funny because it's like a container that needs to download software at runtime, just like your host OS. As above, so below, eh?</p></div></div><p>Each of these is connected together like this (arrows indicate dependencies):</p><p><img alt="The s6 dependency web" src="https://tailscale.dev/images/universal-docker-mod/s6.svg" width="318" height="550" decoding="async" data-nimg="1" loading="lazy"></p><div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p>If you've ever worked deeply with the Heroku ecosystem, you can think about Docker mods as akin to all of the hilarous hacks you can do with buildpacks at dyno boot time.</p></div></div><h2 id="configuration">Configuration</h2><p>The Docker mod exposes a bunch of environment variables that you can use to configure it. You can see the full list of environment variables in the <a href="https://github.com/tailscale-dev/docker-mod" target="_blank" rel="noreferrer">documentation</a>, but here are the important ones:</p><table><thead><tr><th>Environment Variable</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td><code>DOCKER_MODS</code></td><td>The list of additional mods to layer on top of the running container, separated by pipes.</td><td><code>ghcr.io/tailscale-dev/docker-mod:main</code></td></tr><tr><td><code>TAILSCALE_STATE_DIR</code></td><td>The directory where the Tailscale state will be stored, this should be pointed to a Docker volume. If it is not, then the node will set itself as ephemeral, making the node disappear from your tailnet when the container exits.</td><td><code>/var/lib/tailscale</code></td></tr><tr><td><code>TAILSCALE_AUTHKEY</code></td><td>The authkey for your tailnet. You can create one in the <a href="https://login.tailscale.com/admin/settings/keys" target="_blank" rel="noreferrer">admin panel</a>. See <a href="https://tailscale.com/kb/1085/auth-keys/" target="_blank" rel="noreferrer">here</a> for more information about authkeys and what you can do with them.</td><td><code>tskey-auth-hunter2CNTRL-hunter2hunter2</code></td></tr><tr><td><code>TAILSCALE_HOSTNAME</code></td><td>The hostname that you want to set for the container. If you don't set this, the hostname of the node on your tailnet will be a bunch of random hexadecimal numbers, which many humans find hard to remember.</td><td><code>wiki</code></td></tr><tr><td><code>TAILSCALE_USE_SSH</code></td><td>Set this to <code>1</code> to enable SSH access to the container.</td><td><code>1</code></td></tr><tr><td><code>TAILSCALE_SERVE_PORT</code></td><td>The port number that you want to expose on your tailnet. This will be the port of your DokuWiki, Transmission, or other container.</td><td><code>80</code></td></tr><tr><td><code>TAILSCALE_SERVE_MODE</code></td><td>The mode you want to run Tailscale serving in. This should be <code>https</code> in most cases, but there may be times when you need to enable <code>tls-terminated-tcp</code> to deal with some weird edge cases like HTTP long-poll connections. See <a href="https://tailscale.com/kb/1242/tailscale-serve/" target="_blank" rel="noreferrer">here</a> for more information.</td><td><code>https</code></td></tr><tr><td><code>TAILSCALE_FUNNEL</code></td><td>Set this to <code>true</code>, <code>1</code>, or <code>t</code> to enable <a href="https://tailscale.com/kb/1243/funnel/" target="_blank" rel="noreferrer">funnel</a>. For more information about the accepted syntax, please read the <a href="https://pkg.go.dev/strconv#ParseBool" target="_blank" rel="noreferrer">strconv.ParseBool documentation</a> in the Go standard library.</td><td><code>on</code></td></tr></tbody></table><p>Something important to keep in mind is that you really should set up a separate volume for Tailscale state. Here is how to do that with the docker commandline:</p><div><p><code><br><div><p><span>docker volume create dokuwiki-tailscale</span></p></div><br></code></p></div><p>Then you can mount it into a container by using the volume name instead of a host path:</p><div><p><code><br><div><p><span>docker run \</span></p></div><div><p><span>  ... \</span></p></div><div><p><span>  -v dokuwiki-tailscale:/var/lib/tailscale \</span></p></div><div><p><span>  ...</span></p></div><br></code></p></div><p>If you want to use kernel networking mode, you will need to add the <code>NET_ADMIN</code> and <code>NET_RAW</code> capabilities to the container, as well as pass the <a href="https://www.kernel.org/doc/Documentation/networking/tuntap.txt" target="_blank" rel="noreferrer"><code>/dev/net/tun</code></a> device into the container. Here is an example of how to do that with the docker commandline:</p><div><p><code><br><div><p><span>docker run \</span></p></div><div><p><span>  ... \</span></p></div><div><p><span>  --cap-add=NET_ADMIN \</span></p></div><div><p><span>  --cap-add=NET_RAW \</span></p></div><div><p><span>  --device=/dev/net/tun \</span></p></div><div><p><span>  ...</span></p></div><br></code></p></div><p>In a <code>compose.yaml</code> file, it will look like this:</p><div><p><code><br><div><p><span>version: '2.1'</span></p></div><div><p><span>services:</span></p></div><div><p><span>  dokuwiki:</span></p></div><div><p><span>    image: lscr.io/linuxserver/dokuwiki:latest</span></p></div><div><p><span>    volumes:</span></p></div><div><p><span>      - /dev/net/tun:/dev/net/tun</span></p></div><div><p><span>    cap_add:</span></p></div><div><p><span>      - NET_ADMIN</span></p></div><div><p><span>      - NET_RAW</span></p></div><div><p><span>    # ...</span></p></div><br></code></p></div><p>This can be useful when you are running applications on your tailnet <em>without</em> <a href="https://tailscale.com/kb/1242/tailscale-serve/" target="_blank" rel="noreferrer">tailscale serve</a>, and you want the underlying service to know the exact remote IP address (such as when running a Minecraft server).</p><h2 id="fun-things-you-can-do">Fun things you can do</h2><p>Normally when I write these articles, I tend to give you one functional example so that you can fill in the blanks here. This time, I want to give you a few functional and genuninely useful examples so that you can get started with our Docker mod right away.</p><p>If you want to test this with a simple command-line shell, you can run this docker command to create a volume for Tailscale state, and then run a container with the Docker mod installed:</p><div><p><code><br><div><p><span>docker volume create trap-sun-state</span></p></div><br></code></p></div> <div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p><code>trap-sun</code> is the name of the container that we will be running. You can name it whatever you want, but you should use the same name in both your volume and your container. I'm setting the name here in case you get stuck and need to arbitrarily kill the container with <code>docker kill trap-sun</code>.</p></div></div><div><p><code><br><div><p><span>docker run \</span></p></div><div><p><span>  --rm \</span></p></div><div><p><span>  -v trap-sun-state:/var/lib/tailscale \</span></p></div><div><p><span>  -e TAILSCALE_STATE_DIR=/var/lib/tailscale \</span></p></div><div><p><span>  -e TAILSCALE_SERVE_PORT=3000 \</span></p></div><div><p><span>  -e TAILSCALE_SERVE_MODE=https \</span></p></div><div><p><span>  -e TAILSCALE_FUNNEL=on \</span></p></div><div><p><span>  -e TAILSCALE_USE_SSH=1 \</span></p></div><div><p><span>  -e TAILSCALE_HOSTNAME=trap-sun \</span></p></div><div><p><span>  -e TAILSCALE_AUTHKEY=tskey-auth-hunter2CNTRL-hunter2hunter2 \</span></p></div><div><p><span>  -e DOCKER_MODS=ghcr.io/tailscale-dev/docker-mod:main \</span></p></div><div><p><span>  --name trap-sun \</span></p></div><div><p><span>  -it \</span></p></div><div><p><span>  --cap-add=NET_ADMIN \</span></p></div><div><p><span>  --cap-add=NET_RAW \</span></p></div><div><p><span>  -v /dev/net/tun:/dev/net/tun \</span></p></div><div><p><span>  lsiobase/alpine:3.17 \</span></p></div><div><p><span>  sh</span></p></div><br></code></p></div> <div><p><img alt="the avatar for Xe Iaso" srcset="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=64&amp;q=75 1x, https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75 2x" src="https://tailscale.dev/_next/image?url=%2Favatars%2Fxe.png&amp;w=128&amp;q=75" width="64" height="64" decoding="async" data-nimg="1" loading="lazy"></p><div><p>&lt;<b>Xe</b>&gt; </p><p>You can also base your Docker images on the <code>lscr.io/linuxserver/baseimage-alpine:3.17</code> image, which is a minimal <a href="https://alpinelinux.org/" target="_blank" rel="noreferrer">Alpine Linux</a> with Docker mod support. This can be used to adapt your existing containers into nodes on your tailnet. You can also use <a href="https://ubuntu.com/" target="_blank" rel="noreferrer">Ubuntu</a> with <code>lscr.io/linuxserver/baseimage-ubuntu:jammy</code> as the base image. The cloud's the limit!</p></div></div><h3 id="dokuwiki">DokuWiki</h3><p>If you want to set up a wiki for your tailnet with <a href="https://www.dokuwiki.org/dokuwiki" target="_blank" rel="noreferrer">DokuWiki</a>, you can use this Docker compose file:</p><div><p><code><br><div><p><span># docker-compose.yaml</span></p></div><div><p><span>version: '2.1'</span></p></div><div><p><span>services:</span></p></div><div><p><span>  dokuwiki:</span></p></div><div><p><span>    image: lscr.io/linuxserver/dokuwiki:latest</span></p></div><div><p><span>    container_name: dokuwiki</span></p></div><div><p><span>    environment:</span></p></div><div><p><span>      - PUID=1000</span></p></div><div><p><span>      - PGID=1000</span></p></div><div><p><span>      - TZ=Etc/UTC</span></p></div><div><p><span>      - DOCKER_MODS=ghcr.io/tailscale-dev/docker-mod:main</span></p></div><div><p><span>      # tailscale information</span></p></div><div><p><span>      - TAILSCALE_STATE_DIR=/var/lib/tailscale</span></p></div><div><p><span>      - TAILSCALE_SERVE_PORT=80</span></p></div><div><p><span>      - TAILSCALE_SERVE_MODE=https</span></p></div><div><p><span>      ## uncomment to enable funnel, may be a bad idea for some use cases</span></p></div><div><p><span>      #- TAILSCALE_FUNNEL=on</span></p></div><div><p><span>      - TAILSCALE_USE_SSH=1</span></p></div><div><p><span>      - TAILSCALE_HOSTNAME=wiki</span></p></div><div><p><span>      - TAILSCALE_AUTHKEY=tskey-auth-hunter2CNTRL-hunter2hunter2</span></p></div><div><p><span>    volumes:</span></p></div><div><p><span>      - dokuwiki-data:/config</span></p></div><div><p><span>      - dokuwiki-tailscale:/var/lib/tailscale</span></p></div><div><p><span>    restart: unless-stopped</span></p></div><div><p><span>volumes:</span></p></div><div><p><span>  dokuwiki-data:</span></p></div><div><p><span>  dokuwiki-tailscale:</span></p></div><br></code></p></div><p>Then use <code>docker compose up -d</code> to start the DokuWiki container with Tailscale grafted in. You can then access your DokuWiki instance at <code>https://wiki.yourtailnet.ts.net</code>. You will want to do the setup wizard, and then you can start using your own private wiki!</p><h3 id="your-private-cloud-development-environment-with-code-server">Your private cloud development environment with code-server</h3><p>Want to have all the fun of <a href="https://docs.github.com/en/codespaces/overview" target="_blank" rel="noreferrer">GitHub Codespaces</a> without having to use GitHub's servers for development? Set up your own private cloud with <a href="https://github.com/coder/code-server" target="_blank" rel="noreferrer">code-server</a> and Tailscale!</p><div><p><code><br><div><p><span>version: '2.1'</span></p></div><div><p><span>services:</span></p></div><div><p><span>  code-server:</span></p></div><div><p><span>    image: lscr.io/linuxserver/code-server:latest</span></p></div><div><p><span>    container_name: code-server</span></p></div><div><p><span>    environment:</span></p></div><div><p><span>      - PUID=1000</span></p></div><div><p><span>      - PGID=1000</span></p></div><div><p><span>      - TZ=Etc/UTC</span></p></div><div><p><span>      - PASSWORD=hunter2</span></p></div><div><p><span>      - PROXY_DOMAIN=code.shark-harmonic.ts.net</span></p></div><div><p><span>      - DOCKER_MODS=ghcr.io/tailscale-dev/docker-mod:main|ghcr.io/linuxserver/mods:code-server-nodejs|ghcr.io/linuxserver/mods:code-server-npmglobal</span></p></div><div><p><span>      # tailscale information</span></p></div><div><p><span>      - TAILSCALE_STATE_DIR=/var/lib/tailscale</span></p></div><div><p><span>      - TAILSCALE_SERVE_PORT=8443</span></p></div><div><p><span>      - TAILSCALE_SERVE_MODE=tls-terminated-tcp</span></p></div><div><p><span>      - TAILSCALE_USE_SSH=1</span></p></div><div><p><span>      - TAILSCALE_HOSTNAME=code</span></p></div><div><p><span>      - TAILSCALE_AUTHKEY=tskey-auth-hunter2CNTRL-hunter2hunter2</span></p></div><div><p><span>    volumes:</span></p></div><div><p><span>      - code-server-data:/config</span></p></div><div><p><span>      - code-server-tailscale:/var/lib/tailscale</span></p></div><div><p><span>    restart: unless-stopped</span></p></div><div><p><span>volumes:</span></p></div><div><p><span>  code-server-data:</span></p></div><div><p><span>  code-server-tailscale:</span></p></div><br></code></p></div><p>Then use <code>docker compose up -d</code> to start the code-server container with Tailscale grafted in. You can then access your code-server instance at <code>https://code.shark-harmonic.ts.net</code>. You may want to change the password from <a href="http://bash.org/?244321" target="_blank" rel="noreferrer"><code>hunter2</code></a> to something more secure.</p><p>code-server also has support for cloning repositories from GitHub directly, so with this you can get started hacking on a project on one machine, then seamlessly pick up where you left off on another! You can start hacking at something in your office and then walk over to the local Tim Horton's to finish it up!</p><hr><p>There's a bunch of other containers in the <a href="https://fleet.linuxserver.io/" target="_blank" rel="noreferrer">linuxserver.io fleet</a>, you can use Tailscale with those as well. You can also check out <a href="https://docs.linuxserver.io/general/awesome-lsio" target="_blank" rel="noreferrer">Awesome-LSIO</a> for more ideas!</p><p>At Tailscale, we want to recreate the Internet around the idea of small, trusted networks with your friends, family, and coworkers. When you set up applications on your tailnet like this, you can slowly start to use your own private infrastructure instead of relying on the public Internet. This is a great way to start using Tailscale, and we hope that you will find this Docker mod useful.</p><p>If you have any questions, feel free to reach out to us on <a href="https://twitter.com/tailscale" target="_blank" rel="noreferrer">Twitter</a> or <a href="https://hachyderm.io/@tailscale" target="_blank" rel="noreferrer">the Fediverse</a>. We are always happy to help!</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zen 5's Leaked Slides (189 pts)]]></title>
            <link>https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/</link>
            <guid>37812113</guid>
            <pubDate>Sun, 08 Oct 2023 16:49:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/">https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/</a>, See on <a href="https://news.ycombinator.com/item?id=37812113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>A YouTuber called Moore’s Law is Dead recently leaked a couple AMD slides about Zen 5. I typically find leaks uninteresting as they are impossible to verify and often don’t correspond to reality. One example is leakers expecting RDNA 3 to one-up Nvidia’s Ada architecture. AMD is fighting two larger competitors on two fronts and has not managed a decisive lead over Nvidia for more than a decade. AMD is expected to pull a miracle in the next generation, every generation (or two), and to everyone’s surprise it doesn’t happen.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen5_mlid_slide.jpg?ssl=1"><img data-attachment-id="22284" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/zen5_mlid_slide/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen5_mlid_slide.jpg?fit=1265%2C698&amp;ssl=1" data-orig-size="1265,698" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen5_mlid_slide" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen5_mlid_slide.jpg?fit=1265%2C698&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen5_mlid_slide.jpg?fit=688%2C380&amp;ssl=1" decoding="async" width="688" height="380" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen5_mlid_slide.jpg?resize=688%2C380&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen5_mlid_slide.jpg?w=1265&amp;ssl=1 1265w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen5_mlid_slide.jpg?resize=768%2C424&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen5_mlid_slide.jpg?resize=1200%2C662&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>However, this leak is worth a mention because it includes a slide with architecture information. I don’t know whether the leaked slides are genuine, however building a coherent picture with a lot of details is far more difficult than fabricating a few performance numbers. With that in mind, let’s dig a bit into the slides point by point. Instead of trying to validate or disprove the rumors, I’ll try to provide context for each point so you can reach your own conclusions.</p>
<h2>Branch Prediction</h2>
<p>Branch predictors steer a CPU’s pipeline, making them vital to both power efficiency and performance. If a branch predictor takes too long to guess where a branch is going, it could hold up the rest of the CPU pipeline. If it guesses wrong, the core will waste time and power doing useless work. The leaked slide brings up three points under the branch predictor, namely zero bubble conditional branches, high accuracy, and a larger BTB.</p>
<h3>Zero Bubble Conditional Branches</h3>
<p>“Zero bubble” branching refers to handling a branch without delaying subsequent instructions in the pipeline. If later instructions were delayed, it would be analogous to a gas bubble in a pipe carrying liquid. Too many bubbles reduce how much liquid the pipe is delivering, and can be a problem if your production is constrained by how much liquid that pipe can deliver.</p>
<div>
<figure><a href="https://chipsandcheese.com/a64fx_predictor/"><img data-attachment-id="22359" data-permalink="https://chipsandcheese.com/a64fx_predictor/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/a64fx_predictor.png?fit=822%2C634&amp;ssl=1" data-orig-size="822,634" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="a64fx_predictor" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/a64fx_predictor.png?fit=822%2C634&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/a64fx_predictor.png?fit=688%2C531&amp;ssl=1" decoding="async" width="688" height="531" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/a64fx_predictor.png?resize=688%2C531&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/a64fx_predictor.png?w=822&amp;ssl=1 822w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/a64fx_predictor.png?resize=768%2C592&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>A64FX branch prediction and fetch pipeline from the microarchitecture manual. Annotations added in red and green by Clam</figcaption></figure></div>
<p>AMD could already do zero bubble branching since Zen 1, even though few branches could be tracked by the zero bubble predictor. Zen 3 expanded the zero bubble BTB (cache of branch targets) to cover 1024 branches, making zero bubble branches the typical case. Zen 4 carries this forward and expands zero bubble BTB capacity to 1536 branch targets. Therefore, zero bubble branching is nothing new. Zero bubble conditional branches aren’t new either. On all Zen generations, zero bubble branching can happen regardless of whether the branch is conditional or unconditional.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=22279"><img data-attachment-id="22279" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/zen4_btb_latency_cond/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen4_btb_latency_cond.png?fit=944%2C487&amp;ssl=1" data-orig-size="944,487" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_btb_latency_cond" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen4_btb_latency_cond.png?fit=944%2C487&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen4_btb_latency_cond.png?fit=688%2C355&amp;ssl=1" decoding="async" width="688" height="355" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen4_btb_latency_cond.png?resize=688%2C355&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen4_btb_latency_cond.png?w=944&amp;ssl=1 944w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/zen4_btb_latency_cond.png?resize=768%2C396&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Not a lot of difference between unconditional and conditional always-taken branches</figcaption></figure></div>
<p>AMD isn’t alone either. Intel’s Haswell could track 128 branches and handle them with no bubbles. Intel thus made zero bubble branch handling a common case well before AMD did. Since Zen 3, AMD has been able handle more branches with zero bubble speed, but Intel is still very respectable in this area.</p>
<p>Therefore “zero bubble conditional branches” is not an exciting point. Existing CPUs from Intel, Arm, and AMD themselves can already handle conditional branches with zero bubbles. Maybe Zen 5 increases zero bubble predictor capacity, but the slide did not say so.</p>
<h3>High Accuracy and Larger BTB</h3>
<p>AMD has improved branch predictor accuracy with every generation. Zen 2, 3, and 4 could often achieve<a href="https://chipsandcheese.com/2021/02/22/analyzing-zen-2s-cinebench-r15-lead/"> better branch prediction accuracy </a>than their Intel competitors. Zen 5 certainly looks to maintain that lead. But saying a desktop CPU has “high accuracy” branch prediction is like saying an airliner has a pressurized cabin. You expect it to, and it’s news if it doesn’t. Even older, simpler branch predictors like the ones on AMD’s Phenom CPUs could correctly predict the vast majority of branches.</p>
<p>BTB stands for “branch target buffer”, which is a cache of branch targets. If a branch’s target is cached, the predictor can tell the CPU where to fetch instructions from next without waiting for the branch instruction to reach the core. That reduces frontend latency especially if the branch instruction has to be fetched from L2 or beyond. AMD has tweaked BTB size with every generation, but is a step behind Intel’s best.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=11110"><img data-attachment-id="11110" data-permalink="https://chipsandcheese.com/2022/11/05/amds-zen-4-part-1-frontend-and-execution-engine/zen4_btb_compared/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2022/10/zen4_btb_compared.png?fit=1097%2C524&amp;ssl=1" data-orig-size="1097,524" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_btb_compared" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2022/10/zen4_btb_compared.png?fit=1097%2C524&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2022/10/zen4_btb_compared.png?fit=688%2C329&amp;ssl=1" decoding="async" loading="lazy" width="688" height="329" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2022/10/zen4_btb_compared.png?resize=688%2C329&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2022/10/zen4_btb_compared.png?w=1097&amp;ssl=1 1097w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2022/10/zen4_btb_compared.png?resize=768%2C367&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Golden Cove’s L3 BTB has 50% more capacity than AMD’s last level L2 BTB, and <a href="https://chipsandcheese.com/2023/09/06/hot-chips-2023-characterizing-gaming-workloads-on-zen-4/">frontend latency is a problem</a> for Zen 4 in games. It’s likely a problem for Intel as well, and both companies will try to expand branch target caching capacity as transistor budget allows.</p>
<h3>2 Basic Block Fetch</h3>
<p>A basic block is a block of code with exactly one entry point and one exit point. A branch will terminate a basic block even if it’s conditional and not always taken. Existing AMD (and Intel) CPUs could already fetch across basic blocks because they could fetch across not-taken branches. The point on AMD’s slide could mean several things.</p>
<div>
<figure><img data-attachment-id="22384" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/basicblock_example/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_example.png?fit=248%2C284&amp;ssl=1" data-orig-size="248,284" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="basicblock_example" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_example.png?fit=248%2C284&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_example.png?fit=248%2C284&amp;ssl=1" decoding="async" loading="lazy" width="248" height="284" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_example.png?resize=248%2C284&amp;ssl=1" alt="" data-recalc-dims="1"><figcaption>Hypothetical basic block example. Assume nothing can jump into the middle of block1, and that the blocks are laid out consecutively in memory</figcaption></figure></div>
<p>The simplest and most likely explanation is that Zen 5 can fetch across basic blocks just as any high performance CPU made in the last 20 years could. Usually the most boring interpretation of a marketing statement is the correct one.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_fetch_across_not_taken.png?ssl=1"><img data-attachment-id="22386" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/basicblock_fetch_across_not_taken/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_fetch_across_not_taken.png?fit=680%2C303&amp;ssl=1" data-orig-size="680,303" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="basicblock_fetch_across_not_taken" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_fetch_across_not_taken.png?fit=680%2C303&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_fetch_across_not_taken.png?fit=680%2C303&amp;ssl=1" decoding="async" loading="lazy" width="680" height="303" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_fetch_across_not_taken.png?resize=680%2C303&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>CPUs can generally fetch across not-taken branch boundaries, thus fetching two basic blocks in a single cycle</figcaption></figure></div>
<p>Maybe Zen 5 can fetch across taken branches. Recent CPUs from Intel and Arm have done this. Rocket Lake could unroll small loops within its loop buffer, turning taken branches into not-taken ones from the fetch perspective. Arm’s Neoverse N2 and Cortex X2 can also sustain two taken branches per cycle by using a 64 entry nano-BTB. This capability can help improve frontend bandwidth for high IPC but branchy code. If an architectural feature has been around long enough to be implemented by multiple manufacturers, it has a better chance of showing up in a new core. Without being completely crazy, you could hope that Zen 5 can sustain more than one taken branch per cycle based on the leaked slide.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=22388"><img data-attachment-id="22388" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/basicblock_complicated_fetch/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_complicated_fetch.png?fit=679%2C285&amp;ssl=1" data-orig-size="679,285" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="basicblock_complicated_fetch" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_complicated_fetch.png?fit=679%2C285&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_complicated_fetch.png?fit=679%2C285&amp;ssl=1" decoding="async" loading="lazy" width="679" height="285" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/basicblock_complicated_fetch.png?resize=679%2C285&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>Fetching across taken branches is harder and requires </figcaption></figure></div>
<p>Finally, there’s the daydream category. AMD previously advertised zero-bubble branch handling when it became the common case with Zen 3. They didn’t mention zero-bubble branch handling with Zen 1 or Zen 2, even though both had limited ability to do zero-bubble branches. Maybe Zen 5 can fetch across basic blocks in the common case instead of using a loop buffer or micro-BTB as Intel and Arm did. That likely requires a dual-ported instruction cache or micro-op cache alongside a large BTB capable of delivering two branch targets per cycle. Zen 5 would also need circuitry to merge two fetch blocks into a buffer that downstream stages can consume. I think implementing such a strategy makes little sense. It’d only help in high IPC code bound by frontend throughput. Frontend latency due to instruction cache misses is a bigger issue.</p>
<h2>Load/Store</h2>
<p>Every CPU generation tends to see memory subsystem changes to reduce and hide latency.</p>
<h3>Increased L1D Capacity</h3>
<p>The leaked slide says Zen 5 has a 48 KB 12-way set associative L1 data cache, giving it increased capacity and associativity compared to Zen 4’s 32 KB, 8-way L1D. Impressively, the slide claims latency stays at 4 cycles. Intel did the same with their L1 data cache in Sunny Cove, but increased latency from 4 to 5 cycles.</p>
<p>Zen 5’s larger L1D will enjoy increased hitrate. Higher capacity helps reduce cases where a code sequence’s working set exceeds cache capacity. Higher associativity helps prevent conflict misses where cache capacity is sufficient but too many “hot” addresses clash into the same set.</p>
<p>I’m surprised AMD was able to pull this off because 12-way associativity means a cache access involves 12 tag comparisons. Zen uses a micro-tagging scheme where partial tags are compared to predict which cache way (if any) will have a hit, but comparing 12 micro-tags is still no joke. The slide also says Zen 5 can do 4 loads per cycle. That would require 48 tag comparisons.</p>
<h3>Larger DTLB</h3>
<p>All modern CPUs use virtual memory. Program memory addresses don’t directly address locations on DRAM chips. Instead, the operating system sets up a map of virtual addresses to physical addresses (page tables) for each process. A misbehaving process therefore won’t run over everything else and force you to reboot the computer because it has limited access to system memory.</p>
<p>However, virtual memory addresses have to be translated to physical addresses. If the CPU checked the page tables for each memory access, latency would skyrocket as each program memory access turns into several dependent ones. Therefore, CPUs use TLBs (translation lookaside buffers) to cache frequently used translations.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=22360"><img data-attachment-id="22360" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/64bit_page_walk/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?fit=1648%2C724&amp;ssl=1" data-orig-size="1648,724" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="64bit_page_walk" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?fit=1648%2C724&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?fit=688%2C302&amp;ssl=1" decoding="async" loading="lazy" width="688" height="302" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?resize=688%2C302&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?w=1648&amp;ssl=1 1648w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?resize=768%2C337&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?resize=1536%2C675&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?resize=1200%2C527&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?resize=1600%2C703&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?resize=1320%2C580&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>x86-64 4-level paging as described in Intel’s Developer Manual. Plain English comments added by Clam in red.</figcaption></figure></div>
<p>Zen 4 already enjoyed a first level data TLB size increase from 64 to 72 entries. Because of their small size, the first level TLB is fully associative. That eliminates conflict misses, but means any TLB entry could contain the desired translation. Four data cache accesses per cycle could require more than 72 * 4 = 288 TLB tag comparisons every cycle. I’m not sure how Zen 5 would increase the DTLB size without impacting latency unless AMD dropped the fully associative scheme.</p>
<p>I could see Zen 5 using a 16-way set associative DTLB with 128 entries or something along those lines. Checking it would be easier than with a 64 entry fully associative TLB, and the larger capacity could be enough to minimize conflict misses. Alternatively, Zen 5 could leave the first level DTLB untouched and increase L2 DTLB capacity. Zen 4 already brought L2 DTLB size up to 3072 entries compared to 2048 entries in Zen 3. Increasing L2 DTLB size would help programs with hot memory footprints in the multi-megabyte range.</p>
<h3>Larger PWC (Page Walk Cache)</h3>
<p>An address translation doesn’t have to be an all-or-nothing scenario where you either hit in the TLBs or do a full page walk. CPUs can cache upper level paging structures to reduce page walk latency when the TLBs can’t contain a program’s working set.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=22365"><img data-attachment-id="22365" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/64bit_page_walk_with_pwc/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?fit=1694%2C684&amp;ssl=1" data-orig-size="1694,684" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="64bit_page_walk_with_pwc" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?fit=1694%2C684&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?fit=688%2C278&amp;ssl=1" decoding="async" loading="lazy" width="688" height="278" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?resize=688%2C278&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?w=1694&amp;ssl=1 1694w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?resize=768%2C310&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?resize=1536%2C620&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?resize=1200%2C485&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?resize=1600%2C646&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?resize=1320%2C533&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/64bit_page_walk_with_pwc.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Page walk cache implementations can vary. You can cache higher levels and cover more address space, or cache lower levels and shorten the page walk more.</figcaption></figure></div>
<p>That lets the page walker start at a lower level, letting a page walk complete with fewer memory accesses. Each page walk cache entry covers a larger region in memory than a TLB entry, making it ideal for handling programs with a larger memory footprint than the L2 DTLB can reasonably cover. For example, a cached page directory pointer table entry would cover 1 GB of address space.</p>
<p>I’ve seen a lot of Reddit comments where people said they didn’t understand my articles, so I can explain this in plain English with a real life analogy. Imagine you need to walk your dog down four blocks. You want it done faster. An obvious answer is to build a trebuchet that can throw you and your dog down two blocks. Then, you can start the walk closer to your destination. Because different destinations exist, you build 64 trebuchets facing in different directions. Now, you have a real life walk cache.</p>
<p>If you build more powerful trebuchets, you can start your walk closer to the destination. But less powerful ones can throw you places that are a short walk away to many destinations. You can build a lot of more powerful trebuchets but that would consume more area (and trees). CPU architects have to make the same tradeoff.</p>
<p>Since Zen 1, AMD has used a 64 entry page directory cache (PDC) that holds page directory pointer table and page map level 4 entries. L2 TLB entries can cache page directory entries. Perhaps Zen 5 finally increases PDC size. Or maybe, AMD gave the load/store unit a stronger preference for caching page directory entries in the L2 TLB compared to direct translations.</p>
<h2>High Throughput</h2>
<p>Zen generations have seen modest improvements in core throughput, because core throughput is typically not a limiting factor. Zen 1 and 2 could sustain 5 instructions per cycle, while Zen 3 and 4 could sustain 6 per cycle. AMD made double digit IPC gains every generation thanks to large improvements to instruction and data side memory access performance.</p>
<div>
<figure><a href="https://chipsandcheese.com/zen3_ipc_gain/"><img data-attachment-id="22523" data-permalink="https://chipsandcheese.com/zen3_ipc_gain/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen3_ipc_gain.png?fit=1276%2C713&amp;ssl=1" data-orig-size="1276,713" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen3_ipc_gain" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen3_ipc_gain.png?fit=1276%2C713&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen3_ipc_gain.png?fit=688%2C384&amp;ssl=1" decoding="async" loading="lazy" width="688" height="384" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen3_ipc_gain.png?resize=688%2C384&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen3_ipc_gain.png?w=1276&amp;ssl=1 1276w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen3_ipc_gain.png?resize=768%2C429&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen3_ipc_gain.png?resize=1200%2C671&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Slide from AMD’s Zen 3 Hot Chips presentation showing IPC gains largely coming from memory access improvements from both the data and instruction side</figcaption></figure></div>
<p>But a small minority of high IPC applications might benefit.</p>
<h3>8-Wide Dispatch/Rename</h3>
<p>Every Zen generation had at least an 8-wide frontend and 8-wide retire. However, the dispatch/rename stage was only 6 micro-ops wide. If Zen 5 makes rename/dispatch 8-wide, it would be able to sustain 8 micro-ops per cycle.</p>
<div>
<figure><img data-attachment-id="22526" data-permalink="https://chipsandcheese.com/samsung_mongoose_paper_ipc/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/samsung_mongoose_paper_ipc.png?fit=1233%2C511&amp;ssl=1" data-orig-size="1233,511" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="samsung_mongoose_paper_ipc" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/samsung_mongoose_paper_ipc.png?fit=1233%2C511&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/samsung_mongoose_paper_ipc.png?fit=688%2C285&amp;ssl=1" decoding="async" loading="lazy" width="688" height="285" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/samsung_mongoose_paper_ipc.png?resize=688%2C285&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/samsung_mongoose_paper_ipc.png?w=1233&amp;ssl=1 1233w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/samsung_mongoose_paper_ipc.png?resize=768%2C318&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/samsung_mongoose_paper_ipc.png?resize=1200%2C497&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>From Samsung’s paper “Evolution of the Samsung Exynos CPU Microarchitecture”</figcaption></figure></div>
<p>This change will benefit a small minority of high IPC applications capped by core width. Lower IPC applications like games will see little benefit from this change because they’re primarily bound by cache and memory latency.</p>
<h3>Op Fusion</h3>
<p>CPUs can achieve higher throughput and make better use of internal buffers by fusing adjacent instructions into single micro-ops. Branch fusion is the most common example. Conditional branching on x86 involves using an instruction that sets flags, and then a branch that jumps (or not) depending on flags. Intel and AMD have been fusing such ALU + conditional branch pairs for many generations. Arm’s recent cores do the same for equivalent ARM64 sequences.</p>
<p>Zen 3 improved AMD’s fusion capabilities by allowing simple ALU instructions like ADD, AND, and XOR to be fused with a subsequent branch as well as CMP and TEST. Therefore one micro-op on Zen 3 can perform a math operation, check the result for a condition and branch on it, and write the result back to a register. Zen 4 added NOP fusion and XOR+DIV/CDQ+IDIV fusion. The latter handles common use patterns for x86’s division instructions.</p>
<p>Maybe Zen 5 expanded fusion cases, but the slide did not say so. For all we know, the slide could be reiterating the features already present on Zen 4. Prior Zen generations already covered the most common fusion cases (branches). Zen 4’s improvements chase diminishing returns. NOPs are used to align code and should account for a very small percentage of executed instruction. Division is known to be very expensive and avoided by most compilers. If Zen 5 adds fusion cases, it’ll probably pursue further diminishing returns.</p>
<h2>Larger, More Unified Scheduler</h2>
<p>Schedulers sit at the heart of an out-of-order CPU and let them achieve high instruction level parallelism. Every cycle, a scheduler has to watch what registers are written to and see if pending instructions need those inputs. It also has to select instructions that have all their inputs ready and send them to execution units. Failing to accomplish all that in a single cycle <a href="https://www.stuffedcow.net/files/henry-thesis-phd.pdf">incurs a ~10% IPC penalty</a>, so large, fast schedulers are very difficult to design. Small schedulers are easy to make fast but can fill quickly and prevent the core from hiding latency. Fortunately, engineers have a lot of scheduler layout options available.</p>
<p>In a distributed scheduler, each execution port gets its own private scheduler. That simplifies scheduler design because each scheduler only has to select one instruction for execution each cycle, and only needs enough entries to hold the fraction of pending instructions that are expected to be waiting for that port. However, tuning is difficult because one scheduler can fill and block the renamer even if scheduler entries are available elsewhere.</p>

<p>A unified scheduler avoids that problem by having one scheduler serve multiple ports. Each scheduler entry can hold an instruction destined for any port, so a sudden spike in demand for one execution port can be better tolerated. However, a unified scheduler has to select enough instructions per cycle to feed all the execution ports it’s attached to. There’s no free lunch.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=22399"><img data-attachment-id="22399" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/zen_sched-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen_sched.drawio.png?fit=622%2C546&amp;ssl=1" data-orig-size="622,546" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen_sched.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen_sched.drawio.png?fit=622%2C546&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen_sched.drawio.png?fit=622%2C546&amp;ssl=1" decoding="async" loading="lazy" width="622" height="546" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen_sched.drawio.png?resize=622%2C546&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>AMD’s Zen cores have used a mix of distributed and unified schedulers. There are multiple schedulers like with a distributed scheduler, but some schedulers serve multiple ports as in unified designs</figcaption></figure></div>
<p>AMD, Intel, and Arm’s recent CPUs use a hybrid of the two approaches. Zen 5’s scheduler is both larger and more unified, meaning it has more total entries and can use some of them more efficiently.</p>
<div>
<figure><a href="https://chipsandcheese.com/brp_dispatch_stall/"><img data-attachment-id="20619" data-permalink="https://chipsandcheese.com/brp_dispatch_stall/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/brp_dispatch_stall.png?fit=664%2C694&amp;ssl=1" data-orig-size="664,694" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="brp_dispatch_stall" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/brp_dispatch_stall.png?fit=664%2C694&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/brp_dispatch_stall.png?fit=664%2C694&amp;ssl=1" decoding="async" loading="lazy" width="664" height="694" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/brp_dispatch_stall.png?resize=664%2C694&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>What filled up and caused a renamer/dispatch stall in a couple of games</figcaption></figure></div>
<p>From a look at a <a href="https://chipsandcheese.com/2023/09/06/hot-chips-2023-characterizing-gaming-workloads-on-zen-4/">couple of gaming workloads</a>, integer scheduler 0 fills a bit more often than the others. Cinebench 2024 sees similar behavior on the integer side.</p>
<div>
<figure><a href="https://chipsandcheese.com/cb2024_sched_stall/"><img data-attachment-id="22419" data-permalink="https://chipsandcheese.com/cb2024_sched_stall/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cb2024_sched_stall.png?fit=730%2C384&amp;ssl=1" data-orig-size="730,384" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cb2024_sched_stall" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cb2024_sched_stall.png?fit=730%2C384&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cb2024_sched_stall.png?fit=688%2C362&amp;ssl=1" decoding="async" loading="lazy" width="688" height="362" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cb2024_sched_stall.png?resize=688%2C362&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Scheduler 0 feeds an AGU pipe and an ALU/branch pipe. AMD could chose to make this scheduler bigger or unify it with another scheduler. They could combine both approaches as well. However, scope for improvement may be limited. Integer scheduler related dispatch stalls account for single digit percentages, indicating Zen 4’s distributed scheduler is already well tuned.</p>
<h3>6 ALUs, 4 loads, 2 stores</h3>
<p>The slide says Zen 5 has 6 ALUs, and the ability to do 4 loads/2stores per cycle. ALUs, or arithmetic logic units, are execution units capable of handling the most common integer instructions like adds and bitwise operations. All prior Zen generations had four ALUs, so Zen 5 would increase per-cycle scalar integer throughput by 50%.</p>
<p>This change will have minimal effect. I put this section right after the scheduler one because schedulers will fill if the execution units can’t keep up with incoming operations. Schedulers can fill for reasons other than lack of execution ports as well. For example, a sequence of latency-bound instructions will also fill the schedulers. Scheduler-bound dispatch stalls are therefore an upper bound on how often the core is execution unit bound. From above, it doesn’t happen often.</p>
<p>Increased load/store throughput might help specific scenarios like memory copies, where the core can sustain 2 loads and 2 stores per cycle but how much this will effect general cases, I don’t know.</p>
<p>ALUs and AGUs themselves are tiny, but feeding them is more difficult. Each new execution port needs inputs from the register file, and increasing register file port count will increase area. More execution ports mean schedulers will have to pick more instructions per cycle, requiring more power and area as well. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=22482"><img data-attachment-id="22482" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/zen_alu_agu-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen_alu_agu.drawio.png?fit=593%2C172&amp;ssl=1" data-orig-size="593,172" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen_alu_agu.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen_alu_agu.drawio.png?fit=593%2C172&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen_alu_agu.drawio.png?fit=593%2C172&amp;ssl=1" decoding="async" loading="lazy" width="593" height="172" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen_alu_agu.drawio.png?resize=593%2C172&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>A not insanely expensive way to get to 6 ALUs and 4 loads/2 stores</figcaption></figure></div>
<p>If I were AMD and had to implement 6 ALUs and 4 AGUs, I would do so with the absolute minimum of extra ports. AGU ports can do double duty as ALU ports because AGUs already have to do simple math on register inputs anyway. The branch port can also be upgraded to an ALU, again reusing existing register file ports.</p>
<p>Increasing execution unit throughput will result in minimal gains, but minimal gains can be worthwhile if they are achieved at low cost. I suspect AMD is going after that route.</p>
<h2>Larger Structure Sizes</h2>
<p>An out-of-order CPU has structures to track instruction state until their results can be made final. Structure sizes tend to increase with every CPU generation. The leaked slide suggests Zen 5 will do so too, but did not go into specifics.</p>
<div>
<figure><img data-attachment-id="22423" data-permalink="https://chipsandcheese.com/cb2024_dispatch_stall/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cb2024_dispatch_stall.png?fit=840%2C460&amp;ssl=1" data-orig-size="840,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cb2024_dispatch_stall" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cb2024_dispatch_stall.png?fit=840%2C460&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cb2024_dispatch_stall.png?fit=688%2C377&amp;ssl=1" decoding="async" loading="lazy" width="688" height="377" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cb2024_dispatch_stall.png?resize=688%2C377&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cb2024_dispatch_stall.png?w=840&amp;ssl=1 840w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cb2024_dispatch_stall.png?resize=768%2C421&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></figure></div>
<p>In Cinebench 2024 and the tested games, Zen 4’s reorder buffer is responsible for most stalls. The reorder buffer tracks all instructions in the backend until they are committed in-order. It’s a cap on how far ahead the CPU can move ahead of a stalled instruction. Filling the reorder buffer isn’t a bad thing because it means other queues for specific instruction categories are large enough to not become limitations themselves.</p>
<div>
<figure><a href="https://chipsandcheese.com/amd_rob_capacity/"><img data-attachment-id="22432" data-permalink="https://chipsandcheese.com/amd_rob_capacity/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/amd_rob_capacity.png?fit=705%2C379&amp;ssl=1" data-orig-size="705,379" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_rob_capacity" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/amd_rob_capacity.png?fit=705%2C379&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/amd_rob_capacity.png?fit=688%2C370&amp;ssl=1" decoding="async" loading="lazy" width="688" height="370" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/amd_rob_capacity.png?resize=688%2C370&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>AMD has increased reorder buffer capacity with every CPU generation. Zen 5 will almost certainly see an increase as well, but we don’t know to what extent. Along with a reorder buffer capacity increase, AMD will have to augment other structures to prevent them from filling before the ROB does. The store queue could already use more entries and is a prime candidate for optimization. However, increasing store buffer size will be difficult because it has to hold pending store data. For Zen 4, that’s up to 32 bytes per store.</p>
<h3>64 Byte Fills/Victim</h3>
<p>This line on the slide talks about caching. “Fills” refers to cache fills, and “victim” refers to lines kicked out of a cache to make room for data being filled in. I’m confused because every CPU in recent history uses 64 byte cache lines, which means caches manage data at 64 byte granularity. Thus, data is evicted 64 bytes at a time, and brought in 64 bytes at a time. It’s not a point worth mentioning.</p>
<h2>Data Prefetching Improvements</h2>
<p>Better caching and higher reordering capacity help attack the memory latency problem by reducing latency and allowing execution to proceed past a latency-bound instruction, respectively. Prefetching counters memory latency by trying to get data that the program will need before an instruction asks for it. Again, the slide didn’t go into specifics, so I’ll provide context on what Zen 4 does.</p>
<div>
<figure><a href="https://chipsandcheese.com/zen4_pf-drawio/"><img data-attachment-id="22460" data-permalink="https://chipsandcheese.com/zen4_pf-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_pf.drawio.png?fit=747%2C185&amp;ssl=1" data-orig-size="747,185" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_pf.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_pf.drawio.png?fit=747%2C185&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_pf.drawio.png?fit=688%2C170&amp;ssl=1" decoding="async" loading="lazy" width="688" height="170" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_pf.drawio.png?resize=688%2C170&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>In Zen 4, AMD has prefetchers at the L1 and L2 level. Zen 5 may keep the same prefetch methods but allow them to prefetch further, taking advantage of any bandwidth increases offered by more mature DDR5 implementations. AMD may also tune the prefetchers to ensure demand requests get priority when there’s high bandwidth demand, such as during multi-core workloads.</p>
<h2>Better AVX-512</h2>
<p>Zen 4 featured AMD’s first AVX-512 implementation. Unlike AMD’s first SSE and AVX implementations, Zen 4 did not break instructions that operated on 512-bit vectors into two micro-ops. It had full width 512-bit vector registers, and kept AVX-512 math instructions as one micro-op until they were executed 256 bits at a time.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=22484"><img data-attachment-id="22484" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/zen4_avx512/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_avx512.png?fit=1271%2C713&amp;ssl=1" data-orig-size="1271,713" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_avx512" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_avx512.png?fit=1271%2C713&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_avx512.png?fit=688%2C386&amp;ssl=1" decoding="async" loading="lazy" width="688" height="386" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_avx512.png?resize=688%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_avx512.png?w=1271&amp;ssl=1 1271w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_avx512.png?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_avx512.png?resize=1200%2C673&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From AMD’s presentation at ISSCC</figcaption></figure></div>
<p>Keeping the same FP execution throughput as Zen 2 and Zen 3 helped AMD get the most important AVX-512 benefits (more efficient use of backend resources) without a massive increase in die area and power.</p>
<p>The slide says “FP Pipes/Units at 512b”. The most optimistic interpretation is that Zen 5 has 2×512-bit FP vector execution. Even on TSMC’s newer 4 nm process, I feel that’ll cost too much area and power when most consumer applications don’t use 512-bit vectors. Perhaps AMD will create Zen 5 variants with different FP configurations as Intel has done, with client SKUs spending less area and power on vector FP throughput.</p>
<p>512-bit stores are handled less efficiently on Zen 4 because the store queue can only hold 256-bit pending store data with each entry. At Hot Chips 2023, AMD stated that the area overhead of buffering 512-bit store data was not acceptable. The leaked Zen 5 slide says “Load/Store Queues (512 bit)”, so AMD may have changed their stance. </p>
<p>Applications that heavily leverage 512-bit vectors should see more performance uplift on Zen 5 thanks to these changes.</p>
<h2>Final Words</h2>
<p>From the leaked slides, AMD is pursuing diminishing returns after getting most of the low hanging fruit with prior Zen generations. Zen 2 greatly improved branch prediction accuracy, vector throughput, and cache capacity compared to Zen 1. Zen 3’s improved BTB setup mitigated Zen 2’s frontend latency problem, and a reorganized scheduler avoids situations where Zen 2’s AGU scheduler fills up. Zen 4 brought a bigger micro-op cache, improved L2 capacity, a substantially increased out-of-order execution window, and AVX-512 support. Zen 5 appears to be going after more limited gains by increasing core throughput and providing a stronger AVX-512 implementation. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=22548"><img data-attachment-id="22548" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/zen4_ipc_uplift_breakdown/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift_breakdown.png?fit=1278%2C718&amp;ssl=1" data-orig-size="1278,718" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_ipc_uplift_breakdown" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift_breakdown.png?fit=1278%2C718&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift_breakdown.png?fit=688%2C387&amp;ssl=1" decoding="async" loading="lazy" width="688" height="387" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift_breakdown.png?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift_breakdown.png?w=1278&amp;ssl=1 1278w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift_breakdown.png?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift_breakdown.png?resize=1200%2C674&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>That said I would caution against looking too far into the current leaks. Specific details are rare, leaving plenty of wiggle room. Assuming Zen 5 is set in stone at this point is also perilous. Core behavior can be tuned via microcode updates. A core can be configurable as well, giving AMD the potential to make large changes even when the architecture is “complete”. We’ve seen AMD roll out Zen 2 variants with different FPU configurations. In the same video, MLiD showed another slide that suggests different FP-512 variants exist as well. </p>
<figure><img data-attachment-id="22466" data-permalink="https://chipsandcheese.com/zen5_mlid_slide1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen5_mlid_slide1.jpg?fit=1259%2C693&amp;ssl=1" data-orig-size="1259,693" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen5_mlid_slide1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen5_mlid_slide1.jpg?fit=1259%2C693&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen5_mlid_slide1.jpg?fit=688%2C379&amp;ssl=1" decoding="async" loading="lazy" width="688" height="379" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen5_mlid_slide1.jpg?resize=688%2C379&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen5_mlid_slide1.jpg?w=1259&amp;ssl=1 1259w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen5_mlid_slide1.jpg?resize=768%2C423&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen5_mlid_slide1.jpg?resize=1200%2C661&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>Second slide shown by MLiD</figcaption></figure>
<p>Any performance numbers should be taken with a giant grain of salt too. It’s better to assume they are all guesses at this point. Even if a leaker has a “source”, estimating performance is inherently difficult because different applications will behave differently. An engineer might see a 30% IPC instruction uplift in simulation with a specific instruction trace, but that doesn’t mean other applications will enjoy the same improvement. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=22544"><img data-attachment-id="22544" data-permalink="https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/zen4_ipc_uplift/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?fit=1975%2C549&amp;ssl=1" data-orig-size="1975,549" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_ipc_uplift" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?fit=1975%2C549&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?fit=688%2C191&amp;ssl=1" decoding="async" loading="lazy" width="688" height="191" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?resize=688%2C191&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?w=1975&amp;ssl=1 1975w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?resize=768%2C213&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?resize=1536%2C427&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?resize=1200%2C334&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?resize=1600%2C445&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?resize=1320%2C367&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/zen4_ipc_uplift.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>AMD’s slide</figcaption></figure></div>
<p>That could be mentioned to a leaker, who doesn’t understand that the trace may not be representative of most applications.</p>
<div>
<figure><a href="https://chipsandcheese.com/rgt_zen5_speculation/"><img data-attachment-id="22468" data-permalink="https://chipsandcheese.com/rgt_zen5_speculation/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/rgt_zen5_speculation.jpg?fit=1274%2C706&amp;ssl=1" data-orig-size="1274,706" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rgt_zen5_speculation" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/rgt_zen5_speculation.jpg?fit=1274%2C706&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/rgt_zen5_speculation.jpg?fit=688%2C381&amp;ssl=1" decoding="async" loading="lazy" width="688" height="381" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/rgt_zen5_speculation.jpg?resize=688%2C381&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/rgt_zen5_speculation.jpg?w=1274&amp;ssl=1 1274w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/rgt_zen5_speculation.jpg?resize=768%2C426&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/rgt_zen5_speculation.jpg?resize=1200%2C665&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From a Red Gaming Tech video. 20-30% IPC gains sound very high considering AMD has managed 10-20% with Zen 2, Zen 3, and Zen 4. Certainly not impossible, but I would be skeptical after seeing the wild RDNA 3 rumored performance numbers.</figcaption></figure></div>
<p>Finally, engineers at Intel, AMD, Arm, and other companies put a lot of hard work into their products. It’s only fair to let them get their say when a product is released. If the engineers release a solid product that delivers a typical 10-20% generation on generation gain but everyone’s perception is set based on fabricated or misinterpreted early performance numbers, I think that’s disrespectful to the engineers. It’s also nonsensical when <a href="https://www.anandtech.com/show/9483/intel-skylake-review-6700k-6600k-ddr4-ddr3-ipc-6th-generation/9">Intel delivered lower generation on generation gains</a> at the top of their game in the early 2010s.</p>
<p>AMD is prone to this because they’re an underdog that people expect to one-up its bigger competitors, so fanciful rumors get a lot of attention. Whenever Zen 5 comes out, I would encourage everyone to look at its performance with respect to how Intel and other CPU manufacturers are progressing, and not based on rumors.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80" loading="lazy" decoding="async"> </p>

</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flappy Dird: Flappy Bird Implemented in MacOS Finder (234 pts)]]></title>
            <link>https://eieio.games/nonsense/game-11-flappy-bird-finder/</link>
            <guid>37810144</guid>
            <pubDate>Sun, 08 Oct 2023 13:10:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eieio.games/nonsense/game-11-flappy-bird-finder/">https://eieio.games/nonsense/game-11-flappy-bird-finder/</a>, See on <a href="https://news.ycombinator.com/item?id=37810144">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>I made a game. It’s called Flappy Dird. It’s Flappy Bird inside MacOS Finder.</p>

<div>
    <video playsinline="" controls="" muted="">
        <source src="https://eieio.games/assets/images/flappy-dird/vidlong.mp4">
    </video>
    <p>ad placements start at $2,000</p>
</div>

<p>It has instructions, high score tracking, and marquee banner ads. You double-click to start a game and select any file in the window to jump. It runs at 4 frames a second and can’t run much faster. It occasionally drops inputs for reasons that you’ll understand if you finish this blog.</p>

<p>I’m going to lay out how Flappy Dird works and how it got there. <a href="https://github.com/nolenroyalty/flappy-dird">Head to the github repo</a> if you want to check out the code or play the game yourself.
<!-- excerpt-end --></p>

<h2 id="idea-to-prototype">Idea to Prototype</h2>
<p>The original idea for Flappy Dird came when I noticed that Finder had a “Date Last Opened” field for directories. I knew that the <code>atime</code> (file access time) field was controversial (updating an inode on every file read is expensive!) and wanted to learn how similar date last opened was. I found a few things:</p>
<ol>
  <li>The field only updated when opened via Finder; <code>cd</code>ing didn’t update the timestamp.</li>
  <li>The field <em>did</em> update if you made a symlink to a directory and then double-clicked that symlink within Finder.</li>
  <li>The field was accessible (with second-level precision) via <code>mdls</code></li>
</ol>

<p>This got me pretty excited! I like <a href="https://eieio.games/nonsense/implementing-wordle-in-the-firefox-address-bar/">putting games in weird places</a>, and I realized I could combine those three facts to make a button! The basic idea:</p>
<ul>
  <li>Create a directory <code>dir</code>. Inside <code>dir</code> make a directory <code>button</code> that symlinks back to <code>dir</code>.</li>
  <li>On startup, read the ‘last opened’ timestamp of <code>dir</code>.</li>
  <li>Repeatedly poll the ‘last opened’ timestamp and do something when it changes.</li>
  <li>Open <code>button</code> (inside <code>dir</code>) to change the ‘last opened’ timestamp without changing your location in Finder.</li>
</ul>

<p>I brainstormed some ideas for iconic games that could be played with a single button and came up with Flappy Bird<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> pretty quickly. And then I started trying to figure out how I’d draw something like Flappy Bird in Finder.</p>

<p>I spent a while looking into making Finder’s font monospaced (to make ascii art easier) and measuring out the width of various ascii characters in Finder’s default font before realizing that I had a much better option: <em>emojis have a constant width and if you put them in filenames Finder will display them.</em></p>

<p><img src="https://eieio.games/assets/images/flappy-dird/emoji-names.png">
</p>
<p>emojis in filenames. the future is here.</p>

<p>So I had a way to accept clicks and a way to draw to the screen - enough for a prototype!</p>

<div>
    
    
    <video playsinline="" controls="" poster="https://eieio.games/assets/images/flappy-dird/flap-novsync-firstframe.png">
    
        <source src="https://eieio.games/assets/images/flappy-dird/flap-novsync.mp4" type="video/mp4">
    </video>
    
    <p> vsync does NOT work for emojis in Finder </p>
    
</div>

<p>The basic idea:</p>
<ul>
  <li>Set up <code>dir</code> so that it has 15 subdirectories that symlink back to <code>dir</code></li>
  <li>Wait until the player double-clicks a directory to start the game. Treat all future double clicks as flaps.</li>
  <li>Write a function from <code>bird_y_pos,pipe_locations,frame</code> to a 15x15 grid of emojis</li>
  <li>Every frame, rename every symlink in the directory to a row from our emoji grid.</li>
  <li>Do some hackery to rename the symlinks in the right order so that we can tell Finder to sort by ‘Date Modified’</li>
</ul>

<p>This works! But it’s really slow and the screen tears a lot. We can do better.</p>

<h2 id="vsync-at-home-applescript-and-double-buffering">Vsync at home: AppleScript and double buffering</h2>

<p>The biggest problem with the prototype was that the screen tearing was <em>bad</em>. I figured I’d try to get Finder to “refresh” the file listing in case the tearing was because it wasn’t updating frequently enough. I stumbled upon <a href="https://apple.stackexchange.com/questions/49543/is-there-a-way-to-refresh-a-finder-file-listing">this Stack Exchange question</a> which pointed me to the AppleScript invocation <code>tell application "Finder" to tell front window to update every item</code>.</p>

<p>This line helped a little bit (I still saw tearing), but more importantly it planted the seed of using AppleScript. It also delighted me - I find AppleScript totally bizarre. <a href="https://twitter.com/slomobo/status/1707521508308816148">slomobo made this joke on twitter</a> and it feels pretty correct:</p>

<p><img src="https://eieio.games/assets/images/flappy-dird/tweet.png">
</p>
<p>this is an image; i have no idea how to embed a tweet anymore</p>

<p>I shopped the tearing problem around to some smart friends and a bunch suggested that I find a way to do double buffering. The basic idea of double buffering<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup> is:</p>
<ul>
  <li>Have two buffers, buf1 and buf2</li>
  <li>On frame 1, display buf1 and start writing your data for frame 2 to buf2.</li>
  <li>On frame 2, display buf2 all at once! And start writing your data for frame 3 to buf1.</li>
  <li>Etc.</li>
</ul>

<p>Or something like that. The point is that you avoid the jitter that comes from writing some but not all of the pixels of a new frame to the screen.</p>

<p>We came up with several ideas for how to do double buffering. The big ones were:</p>
<ul>
  <li>Use symlinks to atomically swap out the directory’s contents. This doesn’t work because you can’t expand the contents of a symlinked dir in Finder (you have to double-click on it, at which point Finder dereferences the symlink and won’t follow renames).</li>
  <li>Make two directories whose inner symlinks <em>point at each other</em>. This would solve the tearing problem but would mean that we could only advance a frame when the user clicked, which wouldn’t really work for this game.</li>
  <li>Magically find a way to make Finder display a different directory without changing the “last opened” timestamp.</li>
</ul>

<p>The winning solution came from my friend <a href="https://jakelazaroff.com/">Jake</a>, who suggested that AppleScript might have a way to control Finder. And it does! <code>tell application "Finder" to set target of front Finder window to ("PATH" POSIX file)</code> does exactly what you’d think.</p>



<p>I hacked together a double buffering implementation and got the game running smoothly at 1 frame per second! But 1 FPS is slow. We can do better.</p>

<h2 id="tapplescript-to-flapplescript">Tap(pleScript) to Flap(pleScript)</h2>

<p>The game couldn’t reasonably run above 1 FPS because our input mechanism (double-clicking a file) only had second-level precision - if Flappy Dird ran at 2 FPS you’d only be able to jump every other frame. So I needed a new way to accept input. At this point I figured that AppleScript was all-powerful and could tell me whether an item in Finder was selected. I checked and it totally could! I reworked the code to accept <em>selection</em> of any file in the window as a jump.</p>

<p>This worked well and even matched the original game better than double clicking. The game logic became something like:</p>
<ul>
  <li>Wait until the user double clicks</li>
  <li>On every frame, shell out to AppleScript and check whether the user has selected a file in the current window.</li>
  <li>If they have (or if the “last opened” timestamp has changed), make the bird jump.</li>
  <li>Shell out to AppleScript to change the directory displayed in Finder.</li>
  <li>Sleep so that we maintain a constant framerate (e.g. if the above steps took 0.1 seconds and we want to run at 2 FPS, sleep for 0.4 seconds).</li>
</ul>



<p>This could…kind of get us to 2 FPS. Except for one problem: AppleScript startup was <em>really really slow.</em> Like 0.2 seconds slow. Which meant:</p>
<ul>
  <li>There was no hope of going above 2 FPS, since we had fixed AppleScript costs of 0.4 seconds.</li>
  <li>When running at 2 FPS there was only a tiny window where you could actually tap a file - if you tapped the file after we shelled out to AppleScript to check whether you had tapped a file we’d miss the tap!</li>
</ul>

<p>So the game wasn’t super playable at 2 FPS. And 2 FPS still felt too slow. We can do better.</p>

<h2 id="rewrite-in-rust-applescript">Rewrite in <del>Rust</del> AppleScript</h2>

<p>I searched for ways to improve AppleScript’s startup speed. I compiled my AppleScripts (basically useless), entertained ideas like “writing an AppleScript RPC server,” and repeatedly googled “AppleScript improve startup speed” and “AppleScript preload script.”</p>

<p>Eventually I posted in the <a href="https://recurse.com/">Recurse Center</a> chat and my friend <a href="https://iangrunert.com/">Ian</a> pitched a few suggestions. One of his first suggestions, <em>“can you write the whole game in AppleScript?”</em>, went a little far<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup> - but he also pitched inverting the control flow between my Python and AppleScript code: I could move my main loop to AppleScript while still shelling out to Python for most of the game logic, allowing me to only pay AppleScript’s startup cost once.</p>

<p>I was reluctant to do this because adding any amount of control flow to an AppleScript seemed hard - but I was also pretty excited to get to say “I rewrote it in AppleScript for speed.” Coming up with a way to communicate back from Python to AppleScript was tricky but I landed on something like:</p>
<div><pre><code><span>do shell script</span><span> </span><span>"game.py await"</span><span>

</span><span>set</span><span> </span><span>shouldContinue</span><span> </span><span>to</span><span> </span><span>"continue"</span><span>
</span><span>repeat</span><span> </span><span>while</span><span> </span><span>shouldContinue</span><span> </span><span>=</span><span> </span><span>"continue"</span><span>
    </span><span>do shell script</span><span> </span><span>"game.py start-frame"</span><span>
    </span><span>tell</span><span> </span><span>application</span><span> </span><span>"Finder"</span><span> </span><span>to</span><span> </span><span>set</span><span> </span><span>sel</span><span> </span><span>to</span><span> </span><span>selection</span><span>
    </span><span>set</span><span> </span><span>curBuf</span><span> </span><span>to</span><span> </span><span>do shell script</span><span> </span><span>"game.py tick "</span><span> </span><span>&amp;</span><span> </span><span>(</span><span>number</span><span> </span><span>of</span><span> </span><span>sel</span><span>)</span><span>
    </span><span>tell</span><span> </span><span>application</span><span> </span><span>"Finder"</span><span> </span><span>to</span><span> </span><span>set</span><span> </span><span>target</span><span> </span><span>of</span><span> </span><span>front</span><span> </span><span>¬
</span><span>        </span><span>Finder</span><span> </span><span>window</span><span> </span><span>to</span><span> </span><span>(</span><span>"DIR"</span><span> </span><span>&amp;</span><span> </span><span>curBuf</span><span> </span><span>as</span><span> </span><span>POSIX</span><span> </span><span>file</span><span>)</span><span>
    </span><span>set</span><span> </span><span>shouldContinue</span><span> </span><span>to</span><span> </span><span>do shell script</span><span> </span><span>"game.py sleep"</span><span>
</span><span>end</span><span> </span><span>repeat</span><span>
</span></code></pre></div>

<p>That is:</p>
<ul>
  <li>Wait for the user to double-click to start the game (<code>await</code>)</li>
  <li>Record the time at which we’re starting the frame (<code>start-frame</code>)</li>
  <li>Pass the number of files selected to <code>tick</code> to render the frame.</li>
  <li>Save the response from <code>tick</code> (which is the name of the directory we just prepared) and navigate to it in Finder.</li>
  <li>Sleep to achieve our target framerate and emit <code>continue</code> if the player hasn’t lost yet (so that we keep looping).</li>
</ul>

<p>This worked really well! Ian pointed out to me that <code>start-frame</code> and <code>sleep</code> can be easily combined, and I ended up adding <em>another</em> layer of looping to add a way to restart after you die, but this is the basic structure that the game still uses.</p>

<h2 id="adding-some-flavor">Adding some flavor</h2>

<p>The rest of the game was more straightforward - not necessarily easy, but it was just writing Python code to make the emojis on screen look right given some game state. A few notes about that process:</p>
<ul>
  <li>To store state during and between runs I made a little <code>state.json</code> file that I read/wrote every frame</li>
  <li>Adding text was hard because the text I chose was narrower than the emojis I was using. I used a lot of hardcoded spacing to make sure that things lined up correctly.</li>
  <li>Getting scrolling text across the top was particularly annoying because of the spacing - I ended up writing a function <code>read_n_ad_chars</code> to handle the guesswork around how many characters to show at a given time.</li>
  <li>I didn’t want to mess with relaying the working directory to AppleScript, so the script has a bunch of template variables that get swapped out by the <code>first-time-setup</code> Python invocation.</li>
  <li>AppleScript eats anything the python script outputs so I handled logging by appending to a file and catting it afterwards.</li>
  <li>You jump up an extra row if you tap on two successive frames, which I think makes the game feel a lot better.</li>
</ul>

<p>The hardest bit here was the scrolling banner text. It was particularly tricky because I couldn’t use a debugger since the script was being invoked via AppleScript<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup>.</p>

<h2 id="wrapping-up">Wrapping up</h2>

<p>I loved making this. One thing I found particularly delightful was how simple writing the Python for this game was. The prototype was ~90 lines of code (it’s now ~550 lines but like a third of it is boilerplate or constants)! I did it all in vim! No clicking at all! It was great to work without an engine and keep all of my frame state in tiny 2D array. It was easy to keep the whole game in my head from start to finish (even as the control flow got wonkier). The experience made me excited to try making something bigger without an engine.</p>

<p>I presented an early iteration of Flappy Dird at <a href="https://recurse.com/">Recurse Center</a> and spent a whole lot more time on it because of the response it got during that presentation. Thanks so much to the folks at Recurse for the encouragement, especially since I was presenting 2 weeks after I “never graduated” (Recurse’s word for finishing a batch). If being encouraged to make nonsense like this sounds fun to you you should consider <a href="https://www.recurse.com/apply">applying!</a> And a special thank you to <a href="https://twitter.com/kelin_online">Kelin</a> for telling me that the banner ads should be pulled by a little plane emoji.</p>

<p>4 frames a second and limited input is a pretty big constraint but I think that it’d be feasible to build some other games this way. Some <a href="https://mastodon.gamedev.place/@eieio/111190521120302217">folks on mastodon</a> suggested building Tetris in Finder and I think that’s hard but feasible. If you want to chat about this please <a href="https://eieio.games/whats-my-deal">get in touch!</a> I’ve been particularly enjoying <a href="https://cohost.org/eieio">cohost</a> for gamedev chatting but I’m all over.</p>

<p>I’m currently on vacation (I wrote this on a train from Busan to Seoul) but I’ll be back with some more nonsense in November :)</p>



  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fastest sailboat: Two wild designs (143 pts)]]></title>
            <link>https://newatlas.com/marine/syroco-sp80-testing/</link>
            <guid>37810068</guid>
            <pubDate>Sun, 08 Oct 2023 13:00:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/marine/syroco-sp80-testing/">https://newatlas.com/marine/syroco-sp80-testing/</a>, See on <a href="https://news.ycombinator.com/item?id=37810068">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The SP80 and Syroco teams have both got their remarkable boats on the water. Looking like a pair of alien spaceships, and pulled by ultra-fast kites instead of sails on masts, both these machines are built to reach terrifying, unprecedented speeds.</p><p>The current world sailing speed record has stood for a little over a decade at 65.37 knots (75.23 mph/121.06 km/h), set by Paul Larsen in the <a href="https://newatlas.com/new-world-speed-sailing-record-6537kts-75mph-121kmh/25065/" data-cms-ai="0">Vestas Sailrocket II</a> back in 2012. </p><p>There's a reason nobody's gone faster – rigid masts provide excellent leverage, so when you attempt to harness serious wind power, they want to roll the boat over. So if those record speed figures seem a bit 'meh,' take a look at the video below.</p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f710207866bda444591398cf870ae7e27" data-video-id="wnjyusAgk8I" data-video-title="VESTAS Sailrocket 2  Outright world speed sailing record holder. (subject to WSSRC ratification)">

    <iframe id="YouTubeVideoPlayer-f710207866bda444591398cf870ae7e27" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/wnjyusAgk8I?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>VESTAS Sailrocket 2  Outright world speed sailing record holder. (subject to WSSRC ratification)</p>
    
</div><p>But records are made to be broken, and both the SP80 and Syroco teams are taking radically different approaches to Larsen's. Both teams have set a target speed of 81 knots (93 mph/150 km/h), hoping to absolutely smash the record, both are using huge kites, attached to the boats on strong lines, to avoid capsizing, and both have come a long way since <a href="https://newatlas.com/marine/sp80-syroco-speed-sailing-record/" data-cms-ai="0">we first examined these designs in 2021</a>.</p><p>If this was a beauty contest, it'd go to the SP80 hands down. You could 3D-model this sleek, elegant trimaran, reskin it in silver, stick Naboo in the background and get Natalie Portman to pop out of the cabin for a Star Wars flick. </p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="The SP80 is lowered into the smooth waters of Lake Geneva" width="1440" height="960" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/841db33/2147483647/strip/true/crop/2000x1333+0+0/resize/440x293!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc1%2F22%2F929b5c604bf687ee5aa4d394e481%2Fbqsp80-31juillet-stills-40.jpg 440w,https://assets.newatlas.com/dims4/default/d2bee51/2147483647/strip/true/crop/2000x1333+0+0/resize/800x533!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc1%2F22%2F929b5c604bf687ee5aa4d394e481%2Fbqsp80-31juillet-stills-40.jpg 800w,https://assets.newatlas.com/dims4/default/dc984d7/2147483647/strip/true/crop/2000x1333+0+0/resize/1200x800!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc1%2F22%2F929b5c604bf687ee5aa4d394e481%2Fbqsp80-31juillet-stills-40.jpg 1200w,https://assets.newatlas.com/dims4/default/524a36d/2147483647/strip/true/crop/2000x1333+0+0/resize/1920x1280!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc1%2F22%2F929b5c604bf687ee5aa4d394e481%2Fbqsp80-31juillet-stills-40.jpg 1920w" data-src="https://assets.newatlas.com/dims4/default/39f18e4/2147483647/strip/true/crop/2000x1333+0+0/resize/1440x960!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc1%2F22%2F929b5c604bf687ee5aa4d394e481%2Fbqsp80-31juillet-stills-40.jpg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/841db33/2147483647/strip/true/crop/2000x1333+0+0/resize/440x293!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc1%2F22%2F929b5c604bf687ee5aa4d394e481%2Fbqsp80-31juillet-stills-40.jpg 440w,https://assets.newatlas.com/dims4/default/d2bee51/2147483647/strip/true/crop/2000x1333+0+0/resize/800x533!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc1%2F22%2F929b5c604bf687ee5aa4d394e481%2Fbqsp80-31juillet-stills-40.jpg 800w,https://assets.newatlas.com/dims4/default/dc984d7/2147483647/strip/true/crop/2000x1333+0+0/resize/1200x800!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc1%2F22%2F929b5c604bf687ee5aa4d394e481%2Fbqsp80-31juillet-stills-40.jpg 1200w,https://assets.newatlas.com/dims4/default/524a36d/2147483647/strip/true/crop/2000x1333+0+0/resize/1920x1280!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc1%2F22%2F929b5c604bf687ee5aa4d394e481%2Fbqsp80-31juillet-stills-40.jpg 1920w" src="https://assets.newatlas.com/dims4/default/39f18e4/2147483647/strip/true/crop/2000x1333+0+0/resize/1440x960!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc1%2F22%2F929b5c604bf687ee5aa4d394e481%2Fbqsp80-31juillet-stills-40.jpg">
</p>



    
    

    
        <div><figcaption itemprop="caption">The SP80 is lowered into the smooth waters of Lake Geneva</figcaption><p>SP80</p></div>
    
</figure>

                
            </div><p>This 10.5-m (34.4-ft) design seats two in tandem – one in the back seat to work the kite attached to a control arm behind the cabin, and one to steer the thing. The design is focused around keeping the three hulls in contact with the water, so the high-speed airflow doesn't flip it over.</p><p>In August, the SP80 team took it out on Lake Geneva, where it did 30 knots being pulled behind a boat, to check drag measurements against the team's computer-modeled predictions. Check it out:</p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f214d2aca14514d6e839be155251e8356" data-video-id="39q23l1Hmgc" data-video-title="Le bateau a touché l'eau sur le Léman&nbsp;!">

    <iframe id="YouTubeVideoPlayer-f214d2aca14514d6e839be155251e8356" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/39q23l1Hmgc?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>Le bateau a touché l'eau sur le Léman&nbsp;!</p>
    
</div><p>The team is now working on getting the sail part happening, starting small, and then increasing the size, power and speed incrementally over time.</p><p>As for Syroco, well, it almost feels silly calling this thing a boat at all. The team calls it a "weightless yacht," but effectively it's a kite on a line running more or less straight down into the water, where it attaches to a hydrofoil wing. As the foil and the kite pull against each other, the line is pulled taut.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="Syroco's #1 challenge is to design a supercavitating hydrofoil that won't produce shuddering instability" width="1280" height="640" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/3499590/2147483647/strip/true/crop/1280x640+0+0/resize/440x220!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6c%2Fb5%2Fa853939d439f8900ab8abb7a28bd%2F123334930-373768407409261-2390544981432492216-n.jpg 440w,https://assets.newatlas.com/dims4/default/f5b7878/2147483647/strip/true/crop/1280x640+0+0/resize/800x400!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6c%2Fb5%2Fa853939d439f8900ab8abb7a28bd%2F123334930-373768407409261-2390544981432492216-n.jpg 800w,https://assets.newatlas.com/dims4/default/4dd7e27/2147483647/strip/true/crop/1280x640+0+0/resize/1200x600!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6c%2Fb5%2Fa853939d439f8900ab8abb7a28bd%2F123334930-373768407409261-2390544981432492216-n.jpg 1200w,https://assets.newatlas.com/dims4/default/b105e34/2147483647/strip/true/crop/1280x640+0+0/resize/1920x960!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6c%2Fb5%2Fa853939d439f8900ab8abb7a28bd%2F123334930-373768407409261-2390544981432492216-n.jpg 1920w" data-src="https://assets.newatlas.com/dims4/default/94bf3c4/2147483647/strip/true/crop/1280x640+0+0/resize/1280x640!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6c%2Fb5%2Fa853939d439f8900ab8abb7a28bd%2F123334930-373768407409261-2390544981432492216-n.jpg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/3499590/2147483647/strip/true/crop/1280x640+0+0/resize/440x220!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6c%2Fb5%2Fa853939d439f8900ab8abb7a28bd%2F123334930-373768407409261-2390544981432492216-n.jpg 440w,https://assets.newatlas.com/dims4/default/f5b7878/2147483647/strip/true/crop/1280x640+0+0/resize/800x400!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6c%2Fb5%2Fa853939d439f8900ab8abb7a28bd%2F123334930-373768407409261-2390544981432492216-n.jpg 800w,https://assets.newatlas.com/dims4/default/4dd7e27/2147483647/strip/true/crop/1280x640+0+0/resize/1200x600!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6c%2Fb5%2Fa853939d439f8900ab8abb7a28bd%2F123334930-373768407409261-2390544981432492216-n.jpg 1200w,https://assets.newatlas.com/dims4/default/b105e34/2147483647/strip/true/crop/1280x640+0+0/resize/1920x960!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6c%2Fb5%2Fa853939d439f8900ab8abb7a28bd%2F123334930-373768407409261-2390544981432492216-n.jpg 1920w" src="https://assets.newatlas.com/dims4/default/94bf3c4/2147483647/strip/true/crop/1280x640+0+0/resize/1280x640!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6c%2Fb5%2Fa853939d439f8900ab8abb7a28bd%2F123334930-373768407409261-2390544981432492216-n.jpg">
</p>



    
    

    
        <div><figcaption itemprop="caption">Syroco's #1 challenge is to design a supercavitating hydrofoil that won't produce shuddering instability</figcaption><p>Syroco</p></div>
    
</figure>

                
            </div><p>Somewhere in the middle of that line is a sort of roughly shark-shaped hull and cabin, which is hoisted right out of the water, such that it dangles a few meters up in the air as the boat sails. This has to be the first hydrofoiling boat design I've seen where the hull and the foil are connected by a flexible high-tension line and not a solid support.</p><p><a href="https://newatlas.com/tag/hydrofoil/" data-cms-ai="0">Hydrofoils</a> are naturally speed-limited to around 100 km/h (62 mph) by a process called cavitation, in which the hydrofoil wing creates a high-pressure zone on one side, and a low-pressure zone on the other, and the low-pressure zone gets so low that water starts to vaporize, creating enormous drag. You can <a href="https://newatlas.com/marine/sp80-syroco-speed-sailing-record/" data-cms-ai="0">check out our earlier piece</a> to learn more about how Syroco plans to get past this speed limit, it's pretty out there.</p><p>This team has actually had prototypes flying since the end of 2021. At first, the kite was replaced by a crane arm attached to a support boat, but now it's up and flying solely on wind power, with a small kite attached. And it's possibly just the low wind speeds they're testing in, but my dear god, this thing looks sketchy as hell. Take a look:</p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f782404d013b2415280f82019e3aa283d" data-video-id="WpmgVO5uSs8" data-video-title="On the Road to the Sailing World Speed Record | Syroco - Footage from the Journey!">

    <iframe id="YouTubeVideoPlayer-f782404d013b2415280f82019e3aa283d" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/WpmgVO5uSs8?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>On the Road to the Sailing World Speed Record | Syroco - Footage from the Journey!</p>
    
</div><p>I suspect the Air Syroco folks might need to stock up on barf bags when they start putting human meat sacks on board. </p><p>Both teams will continue tuning, tweaking, testing and accelerating, and we look forward to seeing just how far these bizarre, left-field designs can be pushed. It's certainly fun having two well-resourced, innovative teams chasing the record at once!</p><p>Sources: <a href="https://sp80.ch/launching-on-lake-geneva-for-the-sp80-boat/" target="_blank" data-cms-ai="0">SP80</a>, <a href="https://syro.co/en/news/releasing-video-prototype-flight/" target="_blank" data-cms-ai="0">Syroco</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Indoor wood burning raises women’s lung cancer risk by 43% (174 pts)]]></title>
            <link>https://www.sciencedirect.com/science/article/pii/S0160412023004014</link>
            <guid>37810052</guid>
            <pubDate>Sun, 08 Oct 2023 12:58:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencedirect.com/science/article/pii/S0160412023004014">https://www.sciencedirect.com/science/article/pii/S0160412023004014</a>, See on <a href="https://news.ycombinator.com/item?id=37810052">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app" data-aa-name="root" data-reactroot="" data-iso-key="_0"><header id="gh-cnt"><div id="gh-main-cnt"><a id="gh-branding" href="https://www.sciencedirect.com/" aria-label="ScienceDirect home page" data-aa-region="header" data-aa-name="ScienceDirect"><img src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg" alt="Elsevier logo" height="48" width="54"></a></div></header><div id="mathjax-container" role="main"><div role="region" aria-label="Download options and search"><ul aria-label="PDF Options"><li><a aria-label="View PDF. Opens in a new window."><svg focusable="false" viewBox="0 0 32 32" height="24" width="24"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span>View&nbsp;<strong>PDF</strong></span></a></li><li></li></ul></div><div><article lang="en"><div id="publication"><p><a href="https://www.sciencedirect.com/journal/environment-international" title="Go to Environment International on ScienceDirect"><span><img src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/bebed1cf7f0e52b8a74e313925b021091693422e/image/elsevier-non-solus.png" alt="Elsevier"></span></a></p><p><a href="https://www.sciencedirect.com/journal/environment-international/vol/178/suppl/C"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0160412023X00063-cov150h.gif" alt="Environment International"></span></a></p></div><p id="article-identifier-links"><a href="https://doi.org/10.1016/j.envint.2023.108128" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier"><span>https://doi.org/10.1016/j.envint.2023.108128</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a><a href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0160412023004014&amp;orderBeanReset=true" target="_blank" rel="noreferrer noopener"><span>Get rights and content</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a></p><div id="abstracts"><div id="ab005" lang="en"><h2>Highlights</h2><div id="as005"><ul><li><span>•</span><span><p id="p0005">First prospective study of wood burning and lung cancer incidence among U.S. women.</p></span></li><li><span>•</span><span><p id="p0010">Higher wood stove/fireplace usage associated with 70&nbsp;% higher incidence of lung cancer.</p></span></li><li><span>•</span><span><p id="p0015">Associations were also elevated when analysis was restricted to never smokers.</p></span></li><li><span>•</span><span><p id="p0020">Suggest even occasional indoor wood burning can contribute to lung cancer.</p></span></li></ul></div></div><div id="ab010"><h2>Abstract</h2><div id="as010"><h3 id="st015">Background and aim</h3><p id="sp0010">Epidemiological studies conducted mostly in low- and middle-income countries have found a positive association between household combustion of wood and lung cancer. However, most studies have been retrospective, and few have been conducted in the United States where indoor wood-burning usage patterns differ. We examined the association of exposure to indoor wood smoke from fireplaces and stoves with incident lung cancer in a U.S.-wide cohort of women.</p></div><div id="as015"><h3 id="st020">Methods</h3><p id="sp0015">We included 50,226 women without prior lung cancer participating in the U.S.-based prospective Sister Study. At enrollment (2003–2009), women reported frequency of use of wood-burning stoves and/or fireplaces in their longest-lived adult residence. Cox regression was used to estimate adjusted hazard ratios (HR<sub>adj</sub>) and 95&nbsp;% confidence intervals (CI) for the association between indoor wood-burning fireplace/stove use and incident lung cancer. Lung cancer was self-reported and confirmed with medical records.</p></div><div id="as020"><h3 id="st025">Results</h3><p id="sp0020">During an average 11.3&nbsp;years of follow-up, 347 medically confirmed lung cancer cases accrued. Overall, 62.3&nbsp;% of the study population reported the presence of an indoor wood-burning fireplace/stove at their longest-lived adult residence and 20.6&nbsp;% reported annual usage of ≥30&nbsp;days/year. Compared to those without a wood-burning fireplace/stove, women who used their wood-burning fireplace/stove ≥30&nbsp;days/year had an elevated rate of lung cancer (HR<sub>adj</sub>&nbsp;=&nbsp;1.68; 95&nbsp;% CI&nbsp;=&nbsp;1.27, 2.20). In never smokers, positive associations were seen for use 1–29&nbsp;days/year (HR<sub>adj</sub>&nbsp;=&nbsp;1.64; 95&nbsp;% CI&nbsp;=&nbsp;0.87, 3.10) and ≥30&nbsp;days/year (HR<sub>adj</sub>&nbsp;=&nbsp;1.99; 95&nbsp;% CI&nbsp;=&nbsp;1.02, 3.89). Associations were also elevated across all income groups, in Northeastern, Western or Midwestern U.S. regions, and among those who lived in urban or rural/small town settings.</p></div><div id="as025"><h3 id="st030">Conclusions</h3><p id="sp0025">Our prospective analysis of a cohort of U.S. women found that increasing frequency of wood-burning indoor fireplace/stove usage was associated with incident lung cancer, even among never smokers.</p></div></div></div><ul id="issue-navigation"><li></li><li></li></ul><div id="kg005"><h2>Keywords</h2><p><span>Wood smoke</span></p><p><span>Heating</span></p><p><span>Cooking</span></p><p><span>Lung cancer</span></p><p><span>Women</span></p></div><section id="da005"><h2 id="st040">Data availability</h2><p id="p0025">Data will be made available on request.</p></section><section aria-label="Cited by" id="section-cited-by"><header id="citing-articles-header"><h2>Cited by (0)</h2></header></section><p><span>Published by Elsevier Ltd.</span></p></article></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Contour: Modern and Fast Terminal Emulator (171 pts)]]></title>
            <link>https://github.com/contour-terminal/contour</link>
            <guid>37809834</guid>
            <pubDate>Sun, 08 Oct 2023 12:20:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/contour-terminal/contour">https://github.com/contour-terminal/contour</a>, See on <a href="https://news.ycombinator.com/item?id=37809834">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-contour---a-modern--actually-fast-terminal-emulator" dir="auto"><a href="#contour---a-modern--actually-fast-terminal-emulator">Contour - a modern &amp; actually fast Terminal Emulator</a></h2>
<p dir="auto"><a href="https://github.com/contour-terminal/contour/actions?query=workflow%3ABuild"><img src="https://github.com/contour-terminal/contour/workflows/Build/badge.svg" alt="CI Build"></a>
<a href="https://codecov.io/gh/contour-terminal/contour" rel="nofollow"><img src="https://camo.githubusercontent.com/a13b212c030b23482c3335fd0147847c24bd76109e1d06ddb1e345e24c21a098/68747470733a2f2f636f6465636f762e696f2f67682f636f6e746f75722d7465726d696e616c2f636f6e746f75722f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/contour-terminal/contour/branch/master/graph/badge.svg"></a>
<a href="https://isocpp.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/3cf63accce4ccab78694605dce8b3a5fdce4797907333bcb6ee65513280c47a2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374616e646172642d4325324225324225323032302d626c75652e7376673f6c6f676f3d43253242253242" alt="C++20" data-canonical-src="https://img.shields.io/badge/standard-C%2B%2B%2020-blue.svg?logo=C%2B%2B"></a>
<a href="https://discord.gg/ncv4pG9" rel="nofollow"><img src="https://camo.githubusercontent.com/7dc86b70f31d81db1a03bbc7a4fbf91eb66e17148258418e733bdd6ebd08611a/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3437393330313331373333373238343630382e7376673f6c6162656c3d266c6f676f3d646973636f7264266c6f676f436f6c6f723d66666666666626636f6c6f723d373338394438266c6162656c436f6c6f723d364137454332" alt="Discord" data-canonical-src="https://img.shields.io/discord/479301317337284608.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2"></a>
<a href="https://twitch.tv/christianparpart" rel="nofollow"><img src="https://camo.githubusercontent.com/102469cd0266a890b0dd4d80a0149d07e88f0645bcdcd7c8ebf779390e8070fc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5477697463682d4c69766525323053747265616d2d626c75653f7374796c653d666c61742d737175617265" alt="Twitch Live Stream" data-canonical-src="https://img.shields.io/badge/Twitch-Live%20Stream-blue?style=flat-square"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/22a3940995b55ad540162b8f2656ce4b3f9ed4b65f96610f2cf2fc20512e8227/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f636f6e746f75722d7465726d696e616c2d656d756c61746f722f74696572732f6261636b65722f62616467652e7376673f6c6162656c3d6261636b657226636f6c6f723d627269676874677265656e"><img alt="open collective badge" src="https://camo.githubusercontent.com/22a3940995b55ad540162b8f2656ce4b3f9ed4b65f96610f2cf2fc20512e8227/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f636f6e746f75722d7465726d696e616c2d656d756c61746f722f74696572732f6261636b65722f62616467652e7376673f6c6162656c3d6261636b657226636f6c6f723d627269676874677265656e" data-canonical-src="https://opencollective.com/contour-terminal-emulator/tiers/backer/badge.svg?label=backer&amp;color=brightgreen"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/contour-terminal/contour/blob/master/docs/screenshots/contour-notcurses-ncneofetch.png"><img src="https://github.com/contour-terminal/contour/raw/master/docs/screenshots/contour-notcurses-ncneofetch.png" alt="screenshot showcasing notcurses ncneofetch on KDE/Fedora" title="Screenshot"></a></p>
<p dir="auto"><code>contour</code> is a modern and actually fast, modal, virtual terminal emulator,
for everyday use. It is aiming for power users with a modern feature mindset.</p>
<h2 tabindex="-1" id="user-content-features" dir="auto"><a href="#features">Features</a></h2>
<ul dir="auto">
<li>✅ Available on all 4 major platforms, Linux, OS/X, FreeBSD, Windows.</li>
<li>✅ GPU-accelerated rendering.</li>
<li>✅ Font ligatures support (such as in Fira Code).</li>
<li>✅ Unicode: Emoji support (-: 🌈 💝 😛 👪 - including ZWJ, VS15, VS16 emoji :-)</li>
<li>✅ Unicode: Grapheme cluster support</li>
<li>✅ Bold and italic fonts</li>
<li>✅ High-DPI support.</li>
<li>✅ Vertical Line Markers (quickly jump to markers in your history!)</li>
<li>✅ Vi-like input modes for improved selection and copy'n'paste experience and Vi-like <code>scrolloff</code> feature.</li>
<li>✅ Blurred behind transparent background when using Windows 10 or KDE window manager on Linux.</li>
<li>✅ Blurrable Background image support.</li>
<li>✅ Runtime configuration reload</li>
<li>✅ 256-color and Truecolor support</li>
<li>✅ Key binding customization</li>
<li>✅ Color Schemes</li>
<li>✅ Profiles (grouped customization of: color scheme, login shell, and related behaviours)</li>
<li>✅ <a href="https://github.com/contour-terminal/contour/wiki/VTExtensions#synchronized-output">Synchronized rendering</a> (via <code>SM ? 2026</code> / <code>RM ? 2026</code>)</li>
<li>✅ Text reflow (configurable via <code>SM ? 2028</code> / <code>RM ? 2028</code>)</li>
<li>✅ Clickable hyperlinks via <a href="https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda">OSC 8</a></li>
<li>✅ Clipboard setting via OSC 52</li>
<li>✅ Sixel inline images</li>
<li>✅ Terminal page <a href="https://github.com/contour-terminal/contour/wiki/VTExtensions#buffer-capture">buffer capture VT extension</a> to quickly extract contents.</li>
<li>✅ Builtin <a href="https://github.com/contour-terminal/contour/issues/521" data-hovercard-type="issue" data-hovercard-url="/contour-terminal/contour/issues/521/hovercard">Fira Code inspired progress bar</a> support.</li>
<li>✅ Read-only mode, protecting against accidental user-input to the running application, such as <kbd>Ctrl</kbd>+<kbd>C</kbd>.</li>
<li>✅ VT320 Host-programmable and Indicator status line support.</li>
<li>✅ and much more ...</li>
</ul>
<h2 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h2>
<p dir="auto"><code>contour</code> is packaged and available for installation on multiple distributions.</p>
<ul dir="auto">
<li><code>Fedora</code> use official <a href="https://packages.fedoraproject.org/pkgs/contour-terminal/contour-terminal/" rel="nofollow">package</a></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="sudo dnf install contour-terminal"><pre>sudo dnf install contour-terminal</pre></div>
<ul dir="auto">
<li><code>Arch</code> use AUR <a href="https://aur.archlinux.org/packages/contour" rel="nofollow">package</a></li>
</ul>
<h3 tabindex="-1" id="user-content-installing-via-flatpak" dir="auto"><a href="#installing-via-flatpak">Installing via Flatpak</a></h3>
<h4 tabindex="-1" id="user-content-install-from-flathub" dir="auto"><a href="#install-from-flathub">Install from Flathub</a></h4>
<p dir="auto">Click the following button install Contour from the Flathub store.</p>
<p dir="auto"><a href="https://flathub.org/apps/details/org.contourterminal.Contour" rel="nofollow"><img src="https://raw.githubusercontent.com/flatpak-design-team/flathub-mockups/master/assets/download-button/download.svg?sanitize=true" alt="Get it on Flathub"></a></p>
<h4 tabindex="-1" id="user-content-prerequisites" dir="auto"><a href="#prerequisites">Prerequisites</a></h4>
<ul dir="auto">
<li>Make sure you have flatpak installed in your system (<a href="https://flatpak.org/getting.html" rel="nofollow">here is a tutorial on how to install it</a>), and make sure that the version is &gt;= 0.10 (check it using this command: <code>flatpak --version</code>)</li>
<li>Add the <a href="https://flathub.org/" rel="nofollow">flathub</a> repository using the following command: <code>flatpak remote-add --if-not-exists flathub https://dl.flathub.org/repo/flathub.flatpakrepo</code>.</li>
<li>Proceed with one of the following options:
<ul dir="auto">
<li><a href="#install-from-flathub">Install from Flathub</a></li>
<li><a href="https://github.com/contour-terminal/contour/releases">Install from GitHub release</a></li>
</ul>
</li>
</ul>
<h2 tabindex="-1" id="user-content-requirements" dir="auto"><a href="#requirements">Requirements</a></h2>
<ul dir="auto">
<li><strong>operating system</strong>: A <em>recent</em> operating system (OS/X 12, Windows 10+, an up-to-date Linux, or FreeBSD)</li>
<li><strong>GPU</strong>: driver must support at least OpenGL 3.3 hardware accelerated or as software rasterizer.</li>
<li><strong>CPU</strong>: x86-64 AMD or Intel with AES-NI instruction set or ARMv8 with crypto extensions.</li>
</ul>
<h2 tabindex="-1" id="user-content-configuration" dir="auto"><a href="#configuration">Configuration</a></h2>
<p dir="auto">In order to set up Contour, it is necessary to modify the configuration file
<code>contour.yml</code>, which is initially generated in the <code>$HOME/.config/contour</code>
directory. Some features also require shell integration. These can be generated
via the CLI (see below), these currently exist for zsh, fish and tcsh.</p>
<h2 tabindex="-1" id="user-content-installing-from-source" dir="auto"><a href="#installing-from-source">Installing from source</a></h2>
<p dir="auto">Contour is best installed from supported package managers, but you can build
from source by following the instruction below. You can Qt 5 or Qt 6,
by default contour will be compiler with Qt 6, to change Qt version use <code>QTVER=5 ./scripts/install-deps.sh</code> to fetch dependencies and cmake flag <code>-D CONTOUR_QT_VERSION=5</code>.</p>
<h3 tabindex="-1" id="user-content-unix-like-systems-linux-freebsd-osx" dir="auto"><a href="#unix-like-systems-linux-freebsd-osx">UNIX-like systems (Linux, FreeBSD, OS/X)</a></h3>
<h4 tabindex="-1" id="user-content-prerequisites-1" dir="auto"><a href="#prerequisites-1">Prerequisites</a></h4>
<div dir="auto" data-snippet-clipboard-copy-content="./scripts/install-deps.sh"><pre>./scripts/install-deps.sh</pre></div>
<p dir="auto">This script <em>might</em> ask you for the administrator password if a package dependency
can be insalled via the system package manager.</p>
<h4 tabindex="-1" id="user-content-compile" dir="auto"><a href="#compile">Compile</a></h4>
<div dir="auto" data-snippet-clipboard-copy-content="cmake -S . -B build -G Ninja
cmake --build build/

# Optionally, if you want to install from source
cmake --build build/ --target install"><pre>cmake -S <span>.</span> -B build -G Ninja
cmake --build build/

<span><span>#</span> Optionally, if you want to install from source</span>
cmake --build build/ --target install</pre></div>
<h4 tabindex="-1" id="user-content-windows-10-or-newer" dir="auto"><a href="#windows-10-or-newer">Windows 10 or newer</a></h4>
<h4 tabindex="-1" id="user-content-prerequisites-2" dir="auto"><a href="#prerequisites-2">Prerequisites</a></h4>
<p dir="auto">For Windows, you must have Windows 10, 2018 Fall Creators Update, and Visual Studio 2019, installed.
It will neither build nor run on any prior Windows OS, due to libterminal making use of <a href="https://devblogs.microsoft.com/commandline/windows-command-line-introducing-the-windows-pseudo-console-conpty/" rel="nofollow">ConPTY API</a>.</p>
<ol dir="auto">
<li>Set up <a href="https://vcpkg.io/en/getting-started.html" rel="nofollow">vcpkg</a>, preferably somewhere high up in the folder hierarchy, and add the folder to your <code>PATH</code>.</li>
</ol>
<div data-snippet-clipboard-copy-content="cd C:\
git clone git clone https://github.com/Microsoft/vcpkg.git
.\vcpkg\bootstrap-vcpkg.bat"><pre><code>cd C:\
git clone git clone https://github.com/Microsoft/vcpkg.git
.\vcpkg\bootstrap-vcpkg.bat
</code></pre></div>
<ol start="2" dir="auto">
<li>Install Visual Studio Build Tools (make sure to select the CLI tools for
C++, which you might need to do in the separate components tab).</li>
<li>Install Qt6 (i.e. to C:\Qt)</li>
<li>Open the <em>developer</em> version of Powershell.</li>
<li>In the <code>contour</code> source folder execute <code>.\scripts\install-deps.ps1</code>. This step may take a <em>very</em> long time.</li>
</ol>
<h4 tabindex="-1" id="user-content-compile-1" dir="auto"><a href="#compile-1">Compile</a></h4>
<p dir="auto">In the <em>developer</em> version of Powershell:</p>
<div data-snippet-clipboard-copy-content="# change paths accordingly if you installed QT and vcpkg to somewhere else
cmake -S . -B build -DCMAKE_TOOLCHAIN_FILE=C:\vcpkg\scripts\buildsystems\vcpkg.cmake -DCMAKE_PREFIX_PATH=C:\Qt\6.5.0\msvc2019_64\lib\cmake
cmake --build build/

# Optionally, if you want to install from source
cmake --build build/ --target install"><pre lang="psh"><code># change paths accordingly if you installed QT and vcpkg to somewhere else
cmake -S . -B build -DCMAKE_TOOLCHAIN_FILE=C:\vcpkg\scripts\buildsystems\vcpkg.cmake -DCMAKE_PREFIX_PATH=C:\Qt\6.5.0\msvc2019_64\lib\cmake
cmake --build build/

# Optionally, if you want to install from source
cmake --build build/ --target install
</code></pre></div>
<h4 tabindex="-1" id="user-content-distribution-packages" dir="auto"><a href="#distribution-packages">Distribution Packages</a></h4>
<p dir="auto"><a href="https://repology.org/project/contour-terminal/versions" rel="nofollow"><img src="https://camo.githubusercontent.com/528c06fea3754750cf8c5983333a124cf2a70fd857d33d46a97c6725982967f2/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f766572746963616c2d616c6c7265706f732f636f6e746f75722d7465726d696e616c2e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/vertical-allrepos/contour-terminal.svg"></a></p>
<h2 tabindex="-1" id="user-content-cli---command-line-interface" dir="auto"><a href="#cli---command-line-interface">CLI - Command Line Interface</a></h2>
<div dir="auto" data-snippet-clipboard-copy-content="  Usage:

    contour [terminal] [config FILE] [profile NAME] [debug TAGS] [live-config] [dump-state-at-exit PATH]
                       [early-exit-threshold UINT] [working-directory DIRECTORY] [class WM_CLASS]
                       [platform PLATFORM[:OPTIONS]] [session SESSION_ID] [PROGRAM ARGS...]
    contour font-locator [config FILE] [profile NAME] [debug TAGS]
    contour info vt
    contour help
    contour version
    contour license
    contour parser-table
    contour list-debug-tags
    contour generate terminfo to FILE
    contour generate config to FILE
    contour generate integration shell SHELL to FILE
    contour capture [logical] [words] [timeout SECONDS] [lines COUNT] to FILE
    contour set profile [to NAME]
"><pre>  Usage:

    contour [terminal] [config FILE] [profile NAME] [debug TAGS] [live-config] [dump-state-at-exit PATH]
                       [early-exit-threshold UINT] [working-directory DIRECTORY] [class WM_CLASS]
                       [platform PLATFORM[:OPTIONS]] [session SESSION_ID] [PROGRAM ARGS...]
    contour font-locator [config FILE] [profile NAME] [debug TAGS]
    contour info vt
    contour help
    contour version
    contour license
    contour parser-table
    contour list-debug-tags
    contour generate terminfo to FILE
    contour generate config to FILE
    contour generate integration shell SHELL to FILE
    contour capture [logical] [words] [timeout SECONDS] [lines COUNT] to FILE
    contour set profile [to NAME]
</pre></div>
<h2 tabindex="-1" id="user-content-references" dir="auto"><a href="#references">References</a></h2>
<ul dir="auto">
<li><a href="https://vt100.net/docs/vt510-rm/" rel="nofollow">VT510</a>: VT510 Manual, see Chapter 5.</li>
<li><a href="http://www.ecma-international.org/publications/standards/Ecma-035.htm" rel="nofollow">ECMA-35</a>:
Character Code Structure and Extension Techniques</li>
<li><a href="http://www.ecma-international.org/publications/standards/Ecma-043.htm" rel="nofollow">ECMA-43</a>:
8-bit Coded Character Set Structure and Rules</li>
<li><a href="http://www.ecma-international.org/publications/standards/Ecma-048.htm" rel="nofollow">ECMA-48</a>:
Control Functions for Coded Character Sets</li>
<li><a href="https://www.iso.org/standard/22943.html" rel="nofollow">ISO/IEC 8613-6</a>:
Character content architectures</li>
<li><a href="https://invisible-island.net/xterm/ctlseqs/ctlseqs.html" rel="nofollow">xterm</a>: xterm control sequences</li>
<li><a href="http://man.he.net/man4/console_codes" rel="nofollow">console_codes</a> Linux console codes</li>
<li><a href="http://www.inwap.com/pdp10/ansicode.txt" rel="nofollow">Summary of ANSI standards for ASCII terminals</a></li>
<li><a href="http://tldp.org/HOWTO/Text-Terminal-HOWTO-7.html#ss7.2" rel="nofollow">Text Terminal HOWTO (Chapter 7.2, PTY)</a></li>
<li><a href="https://en.wikipedia.org/wiki/ANSI_escape_code" rel="nofollow">ANSI escape code</a> in Wikipedia</li>
</ul>
<h3 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License</a></h3>
<div data-snippet-clipboard-copy-content="Contour - A modern C++ Terminal Emulator
-------------------------------------------

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License."><pre><code>Contour - A modern C++ Terminal Emulator
-------------------------------------------

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
</code></pre></div>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Homebrew to deprecate and add caveat for HashiCorp (243 pts)]]></title>
            <link>https://github.com/Homebrew/homebrew-core/pull/139538</link>
            <guid>37809721</guid>
            <pubDate>Sun, 08 Oct 2023 12:00:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Homebrew/homebrew-core/pull/139538">https://github.com/Homebrew/homebrew-core/pull/139538</a>, See on <a href="https://news.ycombinator.com/item?id=37809721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div disabled="" sortable="">
          <p dir="auto">I've re-run the tap_syntax to show the failure so we don't try and fail to auto-merge this for a 5th time.</p>
<p dir="auto">(It previously wasn't showing because we had removed <code>brew audit</code> from PR checks for a little while while keeping it for merge checks - but I restored it fully as of yesterday.)</p>
<p dir="auto">Might be worth splitting consul and terraform to their own PRs. We'll need to make a decision on the former, and for the latter it looks like we'll wait for a little bit until OpenTofu make their first release - we already are blocking version bumps so no rush.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[500 Lines or Less (160 pts)]]></title>
            <link>https://aosabook.org/en/index.html#500lines</link>
            <guid>37809529</guid>
            <pubDate>Sun, 08 Oct 2023 11:14:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aosabook.org/en/index.html#500lines">https://aosabook.org/en/index.html#500lines</a>, See on <a href="https://news.ycombinator.com/item?id=37809529">Hacker News</a></p>
<div id="readability-page-1" class="page">
    



<p>
  Architects look at thousands of buildings during their
  training, and study critiques of those buildings written
  by masters.  In contrast, most software developers only
  ever get to know a handful of large programs
  well—usually programs they wrote
  themselves—and never study the great programs of
  history.  As a result, they repeat one another's mistakes
  rather than building on one another's successes.
</p>
      
<p>
  Our goal is to change that.  In these two books, the authors of
  four dozen open source applications explain how their software
  is structured, and why.  What are each program's major
  components?  How do they interact?  And what did their builders
  learn during their development?  In answering these questions,
  the contributors to these books provide unique insights into how
  they think.
</p>
      
<p>
  If you are a junior developer, and want to learn how your
  more experienced colleagues think, these books are the place
  to start.  If you are an intermediate or senior developer,
  and want to see how your peers have solved hard design
  problems, these books can help you too.
</p>

<div>
  <p>
    <h2 id="500lines">500 Lines or Less</h2>
  </p>
</div>


<div>
  <p>
    <h2 id="posa">The Performance of Open Source Applications</h2>
  </p>
</div>


<div>
  <p>
    <h2 id="aosa2">AOSA Volume 2</h2>
  </p>
</div>


<div>
  <p>
    <h2 id="aosa1">AOSA Volume 1</h2>
  </p>
</div>
<div>
    <table>
      <tbody><tr>
        <td></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html">Introduction</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#brown-amy">Amy Brown</a> and <a href="https://aosabook.org/en/v1/intro1.html#wilson-greg">Greg Wilson</a></td>
      </tr>
      <tr>
        <td>1.</td>
        <td><a href="https://aosabook.org/en/v1/asterisk.html">Asterisk</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#bryant-russell">Russell Bryant</a></td>
      </tr>
      <tr>
        <td>2.</td>
        <td><a href="https://aosabook.org/en/v1/audacity.html">Audacity</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#crook-james">James Crook</a></td>
      </tr>
      <tr>
        <td>3.</td>
        <td><a href="https://aosabook.org/en/v1/bash.html">The Bourne-Again Shell</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#ramey-chet">Chet Ramey</a></td>
      </tr>
      <tr>
        <td>4.</td>
        <td><a href="https://aosabook.org/en/v1/bdb.html">Berkeley DB</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#seltzer-margo">Margo Seltzer</a> and <a href="https://aosabook.org/en/v1/intro1.html#bostic-keith">Keith Bostic</a></td>
      </tr>
      <tr>
        <td>5.</td>
        <td><a href="https://aosabook.org/en/v1/cmake.html">CMake</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#hoffman-bill">Bill Hoffman</a> and <a href="https://aosabook.org/en/v1/intro1.html#martin-kenneth">Kenneth Martin</a></td>
      </tr>
      <tr>
        <td>6.</td>
        <td><a href="https://aosabook.org/en/v1/eclipse.html">Eclipse</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#moir-kim">Kim Moir</a></td>
      </tr>
      <tr>
        <td>7.</td>
        <td><a href="https://aosabook.org/en/v1/graphite.html">Graphite</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#davis-chris">Chris Davis</a></td>
      </tr>
      <tr>
        <td>8.</td>
        <td><a href="https://aosabook.org/en/v1/hdfs.html">The Hadoop Distributed File System</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#chansler-robert">Robert Chansler</a>, <a href="https://aosabook.org/en/v1/intro1.html#kuang-hairong">Hairong Kuang</a>, <a href="https://aosabook.org/en/v1/intro1.html#radia-sanjay">Sanjay Radia</a>, <a href="https://aosabook.org/en/v1/intro1.html#shvachko-konstantin">Konstantin Shvachko</a>, and <a href="https://aosabook.org/en/v1/intro1.html#srinivas-suresh">Suresh Srinivas</a></td>
      </tr>
      <tr>
        <td>9.</td>
        <td><a href="https://aosabook.org/en/v1/integration.html">Continuous Integration</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#brown-titus">C. Titus Brown</a> and <a href="https://aosabook.org/en/v1/intro1.html#canino-koning-rosangela">Rosangela Canino-Koning</a></td>
      </tr>
      <tr>
        <td>10.</td>
        <td><a href="https://aosabook.org/en/v1/jitsi.html">Jitsi</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#ivov-emil">Emil Ivov</a></td>
      </tr>
      <tr>
        <td>11.</td>
        <td><a href="https://aosabook.org/en/v1/llvm.html">LLVM</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#lattner-chris">Chris Lattner</a></td>
      </tr>
      <tr>
        <td>12.</td>
        <td><a href="https://aosabook.org/en/v1/mercurial.html">Mercurial</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#ochtman-dirkjan">Dirkjan Ochtman</a></td>
      </tr>
      <tr>
        <td>13.</td>
        <td><a href="https://aosabook.org/en/v1/nosql.html">The NoSQL Ecosystem</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#marcus-adam">Adam Marcus</a></td>
      </tr>
      <tr>
        <td>14.</td>
        <td><a href="https://aosabook.org/en/v1/packaging.html">Python Packaging</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#ziade-tarek">Tarek Ziadé</a></td>
      </tr>
      <tr>
        <td>15.</td>
        <td><a href="https://aosabook.org/en/v1/riak.html">Riak and Erlang/OTP</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#cesarini-francesco">Francesco Cesarini</a>, <a href="https://aosabook.org/en/v1/intro1.html#gross-andy">Andy Gross</a>, and <a href="https://aosabook.org/en/v1/intro1.html#sheehy-justin">Justin Sheehy</a></td>
      </tr>
      <tr>
        <td>16.</td>
        <td><a href="https://aosabook.org/en/v1/selenium.html">Selenium WebDriver</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#stewart-simon">Simon Stewart</a></td>
      </tr>
      <tr>
        <td>17.</td>
        <td><a href="https://aosabook.org/en/v1/sendmail.html">Sendmail</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#allman-eric">Eric Allman</a></td>
      </tr>
      <tr>
        <td>18.</td>
        <td><a href="https://aosabook.org/en/v1/snowflock.html">SnowFlock</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#bryant-roy">Roy Bryant</a> and <a href="https://aosabook.org/en/v1/intro1.html#lagar-cavilla-andres">Andrés Lagar-Cavilla</a></td>
      </tr>
      <tr>
        <td>19.</td>
        <td><a href="https://aosabook.org/en/v1/socialcalc.html">SocialCalc</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#tang-audrey">Audrey Tang</a></td>
      </tr>
      <tr>
        <td>20.</td>
        <td><a href="https://aosabook.org/en/v1/telepathy.html">Telepathy</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#madeley-danielle">Danielle Madeley</a></td>
      </tr>
      <tr>
        <td>21.</td>
        <td><a href="https://aosabook.org/en/v1/thousandparsec.html">Thousand Parsec</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#laudicina-alan">Alan Laudicina</a> and <a href="https://aosabook.org/en/v1/intro1.html#mavrinac-aaron">Aaron Mavrinac</a></td>
      </tr>
      <tr>
        <td>22.</td>
        <td><a href="https://aosabook.org/en/v1/violet.html">Violet</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#horstmann-cay">Cay Horstmann</a></td>
      </tr>
      <tr>
        <td>23.</td>
        <td><a href="https://aosabook.org/en/v1/vistrails.html">VisTrails</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#freire-juliana">Juliana Freire</a>, <a href="https://aosabook.org/en/v1/intro1.html#koop-david">David Koop</a>, <a href="https://aosabook.org/en/v1/intro1.html#santos-emanuele">Emanuele Santos</a>, <a href="https://aosabook.org/en/v1/intro1.html#scheidegger-carlos">Carlos Scheidegger</a>, <a href="https://aosabook.org/en/v1/intro1.html#silva-claudio">Claudio Silva</a>, and <a href="https://aosabook.org/en/v1/intro1.html#vo-huy">Huy T. Vo</a></td>
      </tr>
      <tr>
        <td>24.</td>
        <td><a href="https://aosabook.org/en/v1/vtk.html">VTK</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#geveci-berk">Berk Geveci</a> and <a href="https://aosabook.org/en/v1/intro1.html#schroeder-will">Will Schroeder</a></td>
      </tr>
      <tr>
        <td>25.</td>
        <td><a href="https://aosabook.org/en/v1/wesnoth.html">Battle For Wesnoth</a></td>
        <td><a href="https://aosabook.org/en/v1/intro1.html#shimooka-richard">Richard Shimooka</a> and <a href="https://aosabook.org/en/v1/intro1.html#white-david">David White</a></td>
      </tr>
      <tr>
        <td></td>
        <td><a href="https://aosabook.org/en/v1/bib1.html">Bibliography</a></td>
        <td></td>
      </tr>
    </tbody></table>
  </div>

<h2>License and Royalties</h2>
<p>
  This work is made available under
  the <a href="http://creativecommons.org/licenses/by/3.0/legalcode">Creative Commons Attribution 3.0 Unported</a> license.
  Please see
  the <a href="https://aosabook.org/en/license.html">full description of the license</a> for details.
  All royalties from sales of these books will be donated to
  <a href="http://amnesty.org/">Amnesty International</a>.
</p>

<h2>Contributing</h2>
<p>
  Dozens of volunteers worked hard to create this book,
  but there is still lots to do.
  You can help by reporting errors,
  by helping to translate the content into other languages and formats,
  or by describing the architecture of other open source projects.
  Please contact us the coordinators for various translations listed below,
  or mail us directly at <a href="mailto:gvwilson@third-bit.com">gvwilson@third-bit.com</a>
  if you would like to start a new translation or write a chapter yourself.
</p>

  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Raspberry Pi 5 is better than two Pi 4S (230 pts)]]></title>
            <link>https://hackaday.com/2023/09/28/a-raspberry-pi-5-is-better-than-two-pi-4s/</link>
            <guid>37809516</guid>
            <pubDate>Sun, 08 Oct 2023 11:12:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackaday.com/2023/09/28/a-raspberry-pi-5-is-better-than-two-pi-4s/">https://hackaday.com/2023/09/28/a-raspberry-pi-5-is-better-than-two-pi-4s/</a>, See on <a href="https://news.ycombinator.com/item?id=37809516">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>What’s as fast as two Raspberry Pi 4s? <a href="https://www.raspberrypi.com/news/introducing-raspberry-pi-5/" target="_blank">The brand-new Raspberry Pi 5</a>, that’s what. And for only a $5 upcharge (with an asterisk), it’s going to the new go-to board from the British House of Fruity Single-Board Computers. But aside from the brute speed, it also has a number of cool features that will make using the board easier for a number of projects, and it’s going to be on sale in October. Raspberry Pi sent us one for review, and if you were just about to pick up a Pi 4 for a project that needs the speed, we’d say that you might wait a couple weeks until the Raspberry Pi 5 goes on sale.</p>
<h2 id="twice-as-nice">Twice as Nice</h2>
<p>On essentially every benchmark, the Raspberry Pi 5 comes in two to three times faster than the Pi 4. This is thanks to the new Broadcom BCM2712 system-on-chip (SOC) that runs four ARM A76s at 2.4 GHz instead of the Pi 4’s ARM A72s at 1.8 GHz. This gives the CPUs a roughly 2x – 3x advantage over the Pi 4. (Although the Pi 4 was <a href="https://hackaday.com/2020/11/11/adventures-in-overclocking-which-raspberry-pi-4-flavor-is-fastest/">eminently overclockable</a> in the CM4 package.)<a href="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-soc_thumbnail.png" target="_blank"><img data-attachment-id="623884" data-permalink="https://hackaday.com/2023/09/28/a-raspberry-pi-5-is-better-than-two-pi-4s/ch01-raspberrypi5-soc_thumbnail/" data-orig-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-soc_thumbnail.png" data-orig-size="1200,1200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ch01-raspberrypi5-soc_thumbnail" data-image-description="" data-image-caption="" data-medium-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-soc_thumbnail.png?w=400" data-large-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-soc_thumbnail.png?w=625" decoding="async" src="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-soc_thumbnail.png?w=400" alt="" width="400" height="400" srcset="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-soc_thumbnail.png 1200w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-soc_thumbnail.png?resize=250,250 250w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-soc_thumbnail.png?resize=400,400 400w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-soc_thumbnail.png?resize=625,625 625w" sizes="(max-width: 400px) 100vw, 400px"></a></p>
<p>The DRAM runs at double the clock speed. The video core is more efficient and pushes pixels about twice as fast. The new WiFi controller in the SOC allows about twice as much throughput to the same radio. Even the SD card interface is capable of running twice as fast, speeding up boot times to easily under 10 sec – maybe closer to 8 sec, but who’s counting?</p>
<p>Heck, while we’re on factors of two, there are now two MIPI camera/display lines, so you can do stereo imaging straight off the board, or run a camera and external display simultaneously. And it’s capable of driving <em>two</em> 4k HDMI displays at 60 Hz.</p>
<p>There are only two exceptions to the overall factor-of-two improvements. First, the Gigabyte Ethernet remains Gigabyte Ethernet, so that’s a one-ex. (We’re not sure who is running up against that constraint, but if it’s you, you’ll want an external network adapter.) But second, the new Broadcom SOC finally supports the ARM cryptography extensions, which make it 45x faster at AES, for instance. With TLS almost everywhere, this keeps crypto performance from becoming the bottleneck. Nice.</p>
<p>All in all, most everything performance-related has been doubled or halved appropriately, and completely in line with <a href="https://www.jeffgeerling.com/blog/2023/testing-pcie-on-raspberry-pi-5" target="_blank">the only formal benchmarks we’ve seen so far</a>, it <em>feels</em> about twice as fast all around in our informal tests. Compared with a Pi 400 that I use frequently in the basement workshop, the Pi 5 is a lot snappier.</p>

<h2 id="more-powah">More Powah!</h2>
<p>Nothing comes for free. While the Raspberry Pi 5 is more efficient for the same workload than the Pi 4, you can push it still harder. And when you do, it draws a peak 12 W versus the Pi 4’s peak 8 W. And this is where we get to that price asterisk we mentioned in the opening. You might need to fork out for more power coming into the board, and figure out how to handle the heat coming off of it, if you’re computering hard.</p>
<p><a href="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-usb-c_thumbnail.png" target="_blank"><img data-attachment-id="623887" data-permalink="https://hackaday.com/2023/09/28/a-raspberry-pi-5-is-better-than-two-pi-4s/ch01-raspberrypi5-usb-c_thumbnail/" data-orig-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-usb-c_thumbnail.png" data-orig-size="1200,1200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ch01-raspberrypi5-usb-c_thumbnail" data-image-description="" data-image-caption="" data-medium-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-usb-c_thumbnail.png?w=400" data-large-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-usb-c_thumbnail.png?w=625" decoding="async" src="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-usb-c_thumbnail.png?w=400" alt="" width="400" height="400" srcset="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-usb-c_thumbnail.png 1200w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-usb-c_thumbnail.png?resize=250,250 250w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-usb-c_thumbnail.png?resize=400,400 400w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-usb-c_thumbnail.png?resize=625,625 625w" sizes="(max-width: 400px) 100vw, 400px"></a>But first the good news. The Raspberry Pi 5 has an all-new power subsystem, featuring the DA9091 power-management IC, generating eight separate voltages and capable of supplying 20 A to the BCM2712 SoC. Apparently, this chip was co-developed between Raspberry Pi and Renesas, and it includes a real-time clock unit just because they could squeeze it in. It also supports <a href="https://hackaday.com/2023/01/09/all-about-usb-c-power-delivery/">USB-C Power Delivery</a>, so finding a power supply that’s capable of supplying all that juice to the Pi 5 is a lot easier, something that has been a pain point in the past. Will we never see a brownout warning again? We can dream.</p>
<p>The star of the new power management system, hands-down, is the power button. How many <a href="https://hackaday.com/2022/04/04/a-power-button-for-raspberry-pi-courtesy-of-device-tree-overlays/">power button hacks</a> have we seen over the years? We’re happy to bid them adieu.</p>
<p>Now the bad news, in the immortal words of Stan Lee: with great power comes great cooling requirements. The Pi 5 runs hot enough that you might require a heatsink, or even an active cooling solution with a fan. Raspberry Pi shipped us an active cooling package to test out, and it plugs into a fan header on the board, so you know they mean business.</p>
<p>Raspberry Pi has also re-worked their case for the Pi 5, adding a fan with a removable cover, and vents on the underside. And they haven’t forgotten the power button here either – a small piece of acrylic serves as both a button cap and a power status light. Nice.</p>
<h2 id="pcie-for-real-this-time">PCIe, For Real This Time</h2>
<p>The most exciting new feature for people who wish to use the Pi 5 on the desktop is probably the official support for a real PCIe lane. When the Pi 4 came out, it was discovered that it spoke PCIe between the USB controller and the SOC, and of course <a href="https://hackaday.com/2020/07/01/adding-pcie-to-your-raspberry-pi-4-the-easier-way/">intercepting those lines was one of the first hacks</a> that we saw on the then-new Pi 4. Then came the CM4, which forced you to design your own board anyway, so you could choose between USB and PCIe. With the Pi 5, you don’t have to choose, and you won’t have to hack on it either.</p>
<p><a href="https://hackaday.com/wp-content/uploads/2023/09/PI_5_TOP_pcie.png" target="_blank"><img data-attachment-id="623901" data-permalink="https://hackaday.com/2023/09/28/a-raspberry-pi-5-is-better-than-two-pi-4s/pi_5_top_pcie/" data-orig-file="https://hackaday.com/wp-content/uploads/2023/09/PI_5_TOP_pcie.png" data-orig-size="1226,1218" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="PI_5_TOP_pcie" data-image-description="" data-image-caption="" data-medium-file="https://hackaday.com/wp-content/uploads/2023/09/PI_5_TOP_pcie.png?w=400" data-large-file="https://hackaday.com/wp-content/uploads/2023/09/PI_5_TOP_pcie.png?w=629" decoding="async" loading="lazy" src="https://hackaday.com/wp-content/uploads/2023/09/PI_5_TOP_pcie.png?w=400" alt="" width="400" height="397" srcset="https://hackaday.com/wp-content/uploads/2023/09/PI_5_TOP_pcie.png 1226w, https://hackaday.com/wp-content/uploads/2023/09/PI_5_TOP_pcie.png?resize=250,248 250w, https://hackaday.com/wp-content/uploads/2023/09/PI_5_TOP_pcie.png?resize=400,397 400w, https://hackaday.com/wp-content/uploads/2023/09/PI_5_TOP_pcie.png?resize=629,625 629w" sizes="(max-width: 400px) 100vw, 400px"></a>But you will need an adapter. A single PCIe 2.0 lane is broken out to a flat-flex connector, and from there you’ll need an adapter board to connect it up to whichever peripherals you’ve got in mind. Adapters will doubtless come on the market soon, but if you just can’t wait, we’ve got a <a href="https://hackaday.com/2023/03/14/pcie-for-hackers-the-diffpair-prelude/">tutorial series on making your own PCIe devices</a> to help.</p>
<p>Once you get the connections sorted out, you might also try pushing it up to PCIe 3.0 speeds. [Jeff Geerling] got a preview hardware adapter from Raspberry Pi, and <a href="https://www.jeffgeerling.com/blog/2023/testing-pcie-on-raspberry-pi-5" target="_blank">found that although it’s not certified for PCIe 3.0, it works most of the time at those speeds</a>. With an NVMe hard drive attached, he found that he could get 450 MB/sec using the sanctioned PCIe 2.0, and almost 900 MB/sec by changing a line in <code>/boot/config.txt</code>, enabling the unsupported PCIe 3.0 mode, and crossing his fingers. That was easy.</p>
<h2 id="under-the-hood-the-rp1-custom-controller">Under the Hood: The RP1 Custom Controller</h2>
<p>Power supply tweaks, including the power button, are down to Raspberry Pi’s cooperation with Renesas. More computational grunt comes from Broadcom’s new SOC. But features like the dual MIPI connectors or the dual USB 3.0 <em>and</em> USB 2.0 ports with enough bandwidth that they don’t crowd out each other or any of the other peripherals, are all due to Raspberry Pi’s in-house innovation here: the custom RP1 interface / southbridge chip.</p>
<p><a href="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-rp1_thumbnail.png" target="_blank"><img data-attachment-id="623886" data-permalink="https://hackaday.com/2023/09/28/a-raspberry-pi-5-is-better-than-two-pi-4s/ch01-raspberrypi5-rp1_thumbnail/" data-orig-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-rp1_thumbnail.png" data-orig-size="1200,1200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ch01-raspberrypi5-rp1_thumbnail" data-image-description="" data-image-caption="" data-medium-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-rp1_thumbnail.png?w=400" data-large-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-rp1_thumbnail.png?w=625" decoding="async" loading="lazy" src="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-rp1_thumbnail.png?w=400" alt="" width="400" height="400" srcset="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-rp1_thumbnail.png 1200w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-rp1_thumbnail.png?resize=250,250 250w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-rp1_thumbnail.png?resize=400,400 400w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-rp1_thumbnail.png?resize=625,625 625w" sizes="(max-width: 400px) 100vw, 400px"></a>According to Eben Upton, Raspberry Pi’s CEO, “It’s basically a chiplet architecture: all the rage now, but very forward-thinking when we started the RP1 development program back in 2016.” Broadcom makes the SOC at a very fine feature scale, while Raspberry Pi can use larger and cheaper processes to handle the rest: Ethernet, USB, MIPI, analog video out, USART, I2C, I2S, PWM, and GPIO – everything but SDRAM, the SD card, and HDMI.</p>
<p>The Raspberry Pi 5 uses PCIe for the backbone between the SOC and their RP1 chip. Four lanes of PCIe, to be exact, providing a 16 Gb/s link between the body and the brains. This is interesting because most chiplet designs are entirely proprietary, and both chips need to speak a common secret language. Here, Raspberry Pi and Broadcom can collaborate, but almost at arm’s length, because there’s nothing proprietary about PCIe. And because they had a spare PCIe channel on the SOC, they were able to break it out for the end user.</p>
<p>Desoldering the RP1 and doing without all the peripherals it provides, patching the kernel appropriately, and turning the Pi 5 into an all-PCIe, five-channel monstrosity is left as an exercise to the motivated reader.</p>
<h2 id="odds-and-ends">Odds and Ends</h2>
<p><a href="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-csidsi_thumbnail.png" target="_blank"><img data-attachment-id="623885" data-permalink="https://hackaday.com/2023/09/28/a-raspberry-pi-5-is-better-than-two-pi-4s/ch01-raspberrypi5-csidsi_thumbnail/" data-orig-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-csidsi_thumbnail.png" data-orig-size="1200,1200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ch01-raspberrypi5-csidsi_thumbnail" data-image-description="" data-image-caption="" data-medium-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-csidsi_thumbnail.png?w=400" data-large-file="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-csidsi_thumbnail.png?w=625" decoding="async" loading="lazy" src="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-csidsi_thumbnail.png?w=400" alt="" width="400" height="400" srcset="https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-csidsi_thumbnail.png 1200w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-csidsi_thumbnail.png?resize=250,250 250w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-csidsi_thumbnail.png?resize=400,400 400w, https://hackaday.com/wp-content/uploads/2023/09/ch01-raspberrypi5-csidsi_thumbnail.png?resize=625,625 625w" sizes="(max-width: 400px) 100vw, 400px"></a>The big yellow composite video-out is gone from the Raspberry Pi 5, but they broke out the lines for you to solder to if you want to hook it up to something other than HDMI. The old audio output jack has been removed entirely, so you’re probably going to have to rely on HDMI audio out or a HAT if you want hi-fi audio. Other connections include PoE on a four-pin header, an ARM debug / UART on a three pin header, and a JST battery connector to keep the real-time clock module ticking.</p>
<p>Since you might want a heatsink, with fan or without, they’ve added mounting holes spaced around the processor. For space reasons, the MIPI camera/display flat-flex connectors use the thinner form factor that we’ve seen on the Pi Zero, rather than the wider one on the Pi 4.</p>
<h2 id="raspberry-pi-evolution">Raspberry Pi Evolution</h2>
<p>The Raspberry Pi 5 is, in some ways, a modest step forward. A two-times speedup isn’t anything to sneeze at, and the various quality-of-life improvements scattered throughout are great, but none of this is revolutionary when you look at the state of play in the SBC market. Still, the Pi 5 is at least twice as nice as the Pi 4, and at only a small upcharge. If you think back six months ago, where people were paying absurd markups for Pi 4s, the Raspberry Pi 5 is positively a bargain. And while there are faster Linux SBCs on the market these days, they also cost a lot more, so the value proposition of the Pi 5 is still solid. Add in Raspberry Pi’s documentation and software support, and there’s a lot here to like.</p>
<p>They’re not available in stores just yet, but Raspberry Pi plans to have “just under a million” Pi 5s produced and in stores over the course of the rest of 2023, so they’re not going to be scarce — we hope! If you need the speed, and can handle the heat, there’s no reason not to get a Raspberry Pi 5.</p>
	            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why is Debian the way it is? (268 pts)]]></title>
            <link>https://blog.liw.fi/posts/2023/debian-reasons/</link>
            <guid>37809276</guid>
            <pubDate>Sun, 08 Oct 2023 10:21:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.liw.fi/posts/2023/debian-reasons/">https://blog.liw.fi/posts/2023/debian-reasons/</a>, See on <a href="https://news.ycombinator.com/item?id=37809276">Hacker News</a></p>
<div id="readability-page-1" class="page"><article class="page">
    

    

    <section id="pagebody">
	<div>
<ol>
	<li><a href="#what-debian-wants-to-be">What Debian wants to be</a>
	</li>
	<li><a href="#the-constitution-power-structure-governance">The constitution, power structure, governance</a>
	</li>
	<li><a href="#social-contract-and-debian-free-software-guidelines">Social contract and Debian free software guidelines</a>
	</li>
	<li><a href="#self-contained">Self-contained</a>
	</li>
	<li><a href="#no-bundled-libraries">No bundled libraries</a>
	</li>
	<li><a href="#membership-process">Membership process</a>
	</li>
	<li><a href="#release-code-names">Release code names</a>
	</li>
	<li><a href="#changing-slowly">Changing slowly</a>
	</li>
</ol>


</div>
<p>Debian is a large, complex operating system, and a huge open source project. It’s thirty years old now. To many people, some of its aspects are weird. Most such things have a good reason, but it can be hard to find out what it is. This is an attempt to answer some such questions, without being a detailed history of the project.</p>
<h2 id="what-debian-wants-to-be"><a name="index1h1"></a>What Debian wants to be</h2>
<p>Debian wants to be a high-quality, secure general purpose operating system that consists only of free and open source software that runs on most kinds of computers that are in active use in the world.</p>
<p>By general purpose I mean Debian should be suitable for most people for most purposes. There will always be situations where it’s not suitable, for whatever reason, but it’s a good goal to aim for. Some other distributions aim for specific purposes: a desktop, a server, playing games, doing scientific research, etc. It’s fine to aim to be general purpose, or specific purpose, but the choice of goal leads to different decisions along the way.</p>
<p>For Debian, aiming to be general purpose means that Debian doesn’t choose what to package based on the purpose of the software. The only real choice Debian makes here is on whether the software is free and whether it’s plausible for Debian to maintain a high quality package.</p>
<h2 id="the-constitution-power-structure-governance"><a name="index2h1"></a>The constitution, power structure, governance</h2>
<p>Debian is one of the more explicitly democratic open source organizations. It has well-defined processes for making decisions, and elects a project leader every year. Further, the powers of the project leader are strictly constrained, and most powers usually associated with leadership are explicitly delegated to other people.</p>
<p>The historic background for this is that the first Debian project leaders were implicitly all-powerful dictators until they chose to step down. Then one project leader went too far, and a revolt threw them out, and democracy was introduced. As part of this, the project got a formal <a href="https://www.debian.org/devel/constitution">constitution</a>, which defines rules for the project.</p>
<p>The reason Debian has the rules it has, is because less rules, and less bureaucracy, didn’t work for Debian earlier in its history.</p>
<h2 id="social-contract-and-debian-free-software-guidelines"><a name="index3h1"></a>Social contract and Debian free software guidelines</h2>
<p>In the mid-1990s, before the term open source had been introduced, what was “free software” was defined by the Free Software Foundation, but in a way that left much to be interpreted. Debian wanted to have clearer rules, and came up with the Debian Free Software Guidelines, and made them part of its <a href="https://www.debian.org/social_contract">Social Contract</a>.</p>
<p>The social contract is Debian’s promise to itself and to the world at large about what Debian is and does. The DFSG is part of that. This is a foundation document for Debian, and changing it is intentionally made difficult in the Debian constitution.</p>
<p>The more detailed rules have made it clearer what Debian will accept, and have simplified discussions about this. There is still a lot to discuss, of course.</p>
<p>The DFSG was later the basis of the <a href="https://opensource.org/osd/">Open Source Definition</a>.</p>
<h2 id="self-contained"><a name="index4h1"></a>Self-contained</h2>
<p>Debian insists on being self-contained. Anything that is packaged in Debian, by Debian, must be built (compiled) using only dependencies in Debian. Also, everything in Debian must be built by Debian. This can cause a lot of extra work. For example, current programming language tooling often assumes it can download dependencies from online repositories at build time, and that is not acceptable to Debian.</p>
<p>The main reason for this is that a dependency might not be available later. Debian has no control over third party package repositories, and if a package, or entire repository, goes away, it might be impossible for Debian to rebuild the package. Debian needs to rebuild to upgrade to a new compiler, to fix a security problem, to port to a new architecture, or just to make some change to the packaged software, including bug fixes.</p>
<p>If Debian weren’t self-contained, it would be at the mercy of any of the tens of thousands of packages it has, and all their dependencies, being available when an urgent security fix needs to be released. This is not acceptable to Debian, and so Debian chooses to do the work of packaging all dependencies.</p>
<p>That means, of course, that for Debian to package something can be a lot of work.</p>
<h2 id="no-bundled-libraries"><a name="index5h1"></a>No bundled libraries</h2>
<p>Debian avoids using copies of libraries, or other dependencies, that are bundled with the software it packages. Many upstream projects find it easier to bundle or “vendor” dependencies, but for Debian, this means that there can be many copies of some popular libraries. When there is a need to fix a security or other severe problem in such a library, Debian would have to find all copies to fix them. This can be a lot of work, and if the security problem is urgent, it wastes valuable time to have to do that.</p>
<p>As an example: the zlib is used by a very large number of projects. By its nature, it needs to process data that may be constructed to exploit a vulnerability in the library. This has happened. At one point, Debian found dozens of bundled copies of zlib in its archive, and spent considerable effort making sure only the packaged version of zlib is used by packages in Debian.</p>
<p>Thus, Debian chooses to do the work up front, before it’s urgent, while packaging the software, and make sure the package in Debian uses the version of the library packaged in Debian.</p>
<p>This is not always appreciated by upstream developers, who would prefer to only have to deal with the version of the library they bundle. That’s the version they’ve verified their own software with. This sometimes leads to friction with Debian.</p>
<h2 id="membership-process"><a name="index6h1"></a>Membership process</h2>
<p>Given the size and complexity of Debian as an operating system, and its popularity, the project needs to trust its members. This especially means trusting those who upload new packages. Because of technical limitations in Linux in the 1990s, every Debian package has full root access during its installation. In other words, every Debian developer can potentially become the root user on any machine running Debian. With tens of millions of machines running Debian, that is potentially a lot of power.</p>
<p>Debian vets its new members in various ways. Ideally, every new member has been part of the Debian development community sufficiently long that they are known to others, and they’ve built trust within the community.</p>
<p>The process can be quite frustrating to those wanting to join Debian, especially to someone used to a smaller open source project.</p>
<h2 id="release-code-names"><a name="index7h1"></a>Release code names</h2>
<p>Debian assigns a code name for its each major release. This was originally done to make mirroring the Debian package archive less costly.</p>
<p>In the mid-1990s, when Debian was getting close to making its 1.0 release, code names weren’t used. Instead, the archive had a directory for each release, named after its version. Developing a new release takes a while, so the directory “1.0” was created well ahead of time. Unfortunately, a publisher of CD-ROMs, prematurely mass-produced a disc they labeled 1.0, before Debian had actually finished making 1.0. This meant that people who got the Debian 1.0 CD-ROM got something that wasn’t actually 1.0.</p>
<p>An obvious solution to prevent this from happening again would have been to prepare the release in a directory called “1.0-not-released”, and rename the directory to “1.0” after the release was finished. However, this would’ve meant that all the mirrors would’ve had to re-download the release when the name of the directory changed. That would’ve been costly, given the massive size of Debian (hundreds of packages! tens of megabytes!). Thus, Debian chose to use code names instead.</p>
<p>Later, the “pool” structure was added to the Debian archive. With this, the files for all releases are in the same directory tree, and metadata files specify what files belong to each release. This makes mirroring easier. It might be possible to drop the code names and stick to versions, now, but I don’t know if Debian would be interested in that.</p>
<h2 id="changing-slowly"><a name="index8h1"></a>Changing slowly</h2>
<p>As implied above, Debian is huge. It’s massive. It’s enormous, It’s really not very small at all, any more.</p>
<p>Large ships stop slowly. Large projects change slowly. Any change in Debian that affects large portion of its packages may require hundreds of volunteers to do work. That is not going to happen quickly.</p>
<p>Sometimes the work can be done with just a small number of people, and Debian has processes to enable that. As an example, if a new version of the GNU C compiler is uploaded, the work of finding out what fixes in other packages need to be made can usually be done by a handful of people.</p>
<p>Often a change takes time because there’s a need to build consensus, and that requires extensive discussion, which takes time and can only rarely be short-circuited.</p>
<p>This all also means Debian developers tend to be conservative in technical decisions. They often prefer solutions that don’t require large scale changes.</p>
<hr>
<p>To comment publicly, please use <a href="https://toot.liw.fi/@liw/111198682212279688">this fediverse thread</a>.</p>

      </section>

  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pg_bm25: Elastic-Quality Full Text Search Inside Postgres (177 pts)]]></title>
            <link>https://docs.paradedb.com/blog/introducing_bm25</link>
            <guid>37809126</guid>
            <pubDate>Sun, 08 Oct 2023 09:43:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.paradedb.com/blog/introducing_bm25">https://docs.paradedb.com/blog/introducing_bm25</a>, See on <a href="https://news.ycombinator.com/item?id=37809126">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://mintlify.s3-us-west-1.amazonaws.com/paradedb/blog/images/bm25.png"></p><p>We’re unveiling <code>pg_bm25</code>: a Rust-based Postgres extension that significantly improves Postgres’ full text
search capabilities. <code>pg_bm25</code> is named after BM25, the algorithm used by modern search engines to calculate the
relevance scores of search results.</p>
<p>Today, Postgres’ native full text search, which uses the <code>tsvector</code> type, has two main problems:</p>
<ol role="list">
<li><strong>Performance</strong>: Searching and ranking over large tables is sluggish. When tables grow to millions of rows, a single full text search can take several minutes.</li>
<li><strong>Functionality</strong>: Postgres has no support for operations like fuzzy search, relevance tuning, or
BM25 relevance scoring, which are the bread and butter of modern search engines.</li>
</ol>
<p><code>pg_bm25</code> aims to bridge the gap between the native capabilities of Postgres’ full text search and those of a specialized search engine like ElasticSearch.
The goal is to eliminate the need to bring a cumbersome service like ElasticSearch into the data stack.</p>
<p>Some features of <code>pg_bm25</code> include:</p>
<ul role="list">
<li>100% Postgres native, with zero dependencies on an external search engine</li>
<li>Built on top of Tantivy, a Rust-based alternative to the Apache Lucene search library</li>
<li>Query times over 1M rows are 20x faster compared to <code>tsquery</code> and <code>ts_rank</code>, Postgres’ built-in full text search and sort functions</li>
<li>Support for fuzzy search, aggregations, highlighting, and relevance tuning</li>
<li>Relevance scoring uses BM25, the same algorithm used by ElasticSearch</li>
<li>Real-time search — new data is immediately searchable without manual reindexing</li>
</ul>
<p><code>pg_bm25</code> stands on the shoulders of several open-source giants. The goal of this blog post is to recognize these projects
and to share how <code>pg_bm25</code> was built.</p>
<h2 id="the-shoulders-of-giants"><span>The Shoulders of Giants</span></h2>
<p>Putting a search engine inside of Postgres is hard. A few projects have attempted it, but with one caveat: every
single one has relied on an external ElasticSearch instance. This means introducing a
complex and expensive piece of infrastructure into the data stack. Perhaps the best-known example of this kind of design
is a Postgres extension called <a href="https://github.com/zombodb/zombodb" target="_blank" rel="noreferrer">ZomboDB</a>.</p>
<p>In 2016, an open source search library called <a href="https://github.com/quickwit-oss/tantivy" target="_blank" rel="noreferrer">Tantivy</a> emerged. Tantivy
was designed as a Rust-based alternative to Apache Lucene, the search library that powers ElasticSearch.
Three years later, a library called <a href="https://github.com/pgcentralfoundation/pgrx" target="_blank" rel="noreferrer">pgrx</a> — built by the same
author of ZomboDB — made it possible to build Postgres extensions in Rust.
Combined, these projects laid the groundwork for a Postgres extension that could create Elastic-quality
search experiences within Postgres.</p>
<h2 id="creating-the-inverted-index"><span>Creating the Inverted Index</span></h2>
<p>Like ElasticSearch, the backbone of Tantivy’s search engine is a data structure called the inverted index,
which stores a mapping from words to their locations in a set of documents. An inverted index
is like the table of contents of a book — without it, you might have to examine every page to find
a specific chapter.</p>
<p>Rather than creating this inverted index externally, <code>pg_bm25</code> stores the
index inside Postgres as a new, Postgres-native index type, which we call the BM25 index. This is made possible
through Postgres’ <a href="https://www.postgresql.org/docs/current/indexam.html" target="_blank" rel="noreferrer">index access method</a> API.</p>
<p>When a BM25 index is created, Postgres automatically updates it as new data arrives
or is deleted in the underlying SQL table. In this way, <code>pg_bm25</code> enables real-time search without any
additional reindexing logic.</p>
<h2 id="building-the-sql-interface"><span>Building the SQL Interface</span></h2>
<p>Following index creation, the next step was to expose an intuitive SQL interface for users to write search queries.
This was accomplished through the Postgres <a href="https://www.postgresql.org/docs/current/sql-createoperator.html" target="_blank" rel="noreferrer">operator API</a>,
which enables the creation of custom Postgres operators. We chose the <code>@@@</code> operator to signify the beginning
of a query to the BM25 index in homage to the <code>@@</code> operator used by Postgres’ native full text search.</p>
<p>The end result is the ability to search any table with a single SQL query.</p>
<div><pre><code><span>SELECT</span> <span>*</span>
<span>FROM</span> my_table
<span>WHERE</span> my_table @@@ <span>'"my query string"'</span>
</code></pre></div>
<p>Wherever possible, we designed the SQL interface to transparently mirror Tantivy’s API. For instance,
the right-hand side of the <code>@@@</code> operator accepts Tantivy’s mini query language and configuration options.</p>
<div><pre><code><span>SELECT</span> <span>*</span>
<span>FROM</span> my_table
<span>WHERE</span> my_table @@@ <span>'description:keyboard^2 OR electronics:::fuzzy_fields=description&amp;distance=2'</span>
</code></pre></div>
<h2 id="performance-benchmarks"><span>Performance Benchmarks</span></h2>
<p>On a table with one million rows, <code>pg_bm25</code> indexes 50 seconds faster than <code>tsvector</code> and ranks results
20x faster. Indexing and search times are nearly identical to those of a dedicated ElasticSearch instance.
With further optimizations, we’re aiming to reduce the query times compared to ElasticSearch
by an additional 2x.</p>
<p>More detailed benchmark results can be found in the extension <a href="https://github.com/paradedb/paradedb/tree/dev/pg_bm25#benchmarks" target="_blank" rel="noreferrer">README</a>.</p>
<h2 id="wrapping-up"><span>Wrapping Up</span></h2>
<p><code>pg_bm25</code> is ready for use today. There are two ways to try it: <a href="https://github.com/paradedb/paradedb/tree/dev/pg_bm25#installation" target="_blank" rel="noreferrer">installing it</a> inside an existing, self-hosted Postgres instance,
or <a href="https://github.com/paradedb/paradedb#from-self-hosted-postgres" target="_blank" rel="noreferrer">running the Postgres Docker image</a>.</p>
<p><code>pg_bm25</code> is open-source and licensed under AGPL. If you’d like to contribute, the best place to start is our
<a href="https://join.slack.com/t/paradedbcommunity/shared_invite/zt-217mordsh-ielS6BiZf7VW3rqKBFgAlQ" target="_blank" rel="noreferrer">Slack community</a>. And please don’t hesitate to show your support by
<a href="https://github.com/paradedb/paradedb" target="_blank" rel="noreferrer">giving us a star</a>!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fury – Fast multi-language serialization framework powered by JIT and Zero-copy (134 pts)]]></title>
            <link>https://github.com/alipay/fury</link>
            <guid>37808366</guid>
            <pubDate>Sun, 08 Oct 2023 06:42:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/alipay/fury">https://github.com/alipay/fury</a>, See on <a href="https://news.ycombinator.com/item?id=37808366">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/images/logo/fury_github_banner.png"><img width="65%" alt="" src="https://github.com/alipay/fury/raw/main/docs/images/logo/fury_github_banner.png"></a><br>
</p>
<p dir="auto"><a href="https://github.com/alipay/fury"><img src="https://camo.githubusercontent.com/561e17c52da44b173afc20938db28a25d28c13698d1cc353ea30995d10e3e3db/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f616c697061792f667572792f63692e796d6c3f6272616e63683d6d61696e267374796c653d666f722d7468652d6261646765266c6162656c3d474954485542253230414354494f4e53266c6f676f3d676974687562" alt="Build Status" data-canonical-src="https://img.shields.io/github/actions/workflow/status/alipay/fury/ci.yml?branch=main&amp;style=for-the-badge&amp;label=GITHUB%20ACTIONS&amp;logo=github"></a>
<a href="https://join.slack.com/t/fury-project/shared_invite/zt-1u8soj4qc-ieYEu7ciHOqA2mo47llS8A" rel="nofollow"><img src="https://camo.githubusercontent.com/96557fd14cb0d7327a72f3b81e79f4b06cede1429b52becb5c8e748b4c0c4d5a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6a6f696e5f536c61636b2d3738314646352e7376673f6c6f676f3d736c61636b267374796c653d666f722d7468652d6261646765" alt="Slack" data-canonical-src="https://img.shields.io/badge/join_Slack-781FF5.svg?logo=slack&amp;style=for-the-badge"></a>
<a href="https://twitter.com/fury_community" rel="nofollow"><img src="https://camo.githubusercontent.com/d9abcbfdd956a6e102e7e2baaa06e7a2a58aa4ae63f52f89ced7a5d94593167f/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f667572795f636f6d6d756e6974793f6c6f676f3d74776974746572267374796c653d666f722d7468652d6261646765" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/fury_community?logo=twitter&amp;style=for-the-badge"></a></p>
<p dir="auto">Fury is a blazing fast multi-language serialization framework powered by <strong>jit(just-in-time compilation)</strong> and <strong>zero-copy</strong>, providing up to 170x performance and ultimate ease of use.</p>
<p dir="auto"><a href="https://furyio.org/" rel="nofollow">https://furyio.org</a></p>
<h2 tabindex="-1" id="user-content-features" dir="auto"><a href="#features">Features</a></h2>
<ul dir="auto">
<li><strong>Multiple languages</strong>: Java/Python/C++/Golang/Javascript.</li>
<li><strong>Zero-copy</strong>: cross-language out-of-band serialization inspired
by <a href="https://peps.python.org/pep-0574/" rel="nofollow">pickle5</a> and off-heap read/write.</li>
<li><strong>High performance</strong>: A highly-extensible JIT framework to generate serializer code at runtime in an async multi-thread way to speed serialization, providing 20-170x speed up by:
<ul dir="auto">
<li>reduce memory access by inline variable in generated code.</li>
<li>reduce virtual method invocation by inline call in generated code.</li>
<li>reduce conditional branching.</li>
<li>reduce hash lookup.</li>
</ul>
</li>
<li><strong>Multiple binary protocols</strong>: object graph, row format and so on.</li>
</ul>
<p dir="auto">In addition to cross-language serialization, Fury also features at:</p>
<ul dir="auto">
<li>Drop-in replace Java serialization frameworks such as JDK/Kryo/Hessian without modifying any code, but 100x faster.
It can greatly improve the efficiency of high-performance RPC calls, data transfer and object persistence.</li>
<li>JDK serialization <strong>100% compatible</strong>: JDK <code>writeObject/readObject/writeReplace/readResolve/readObjectNoData/Externalizable</code> serialization API supported, <strong>JDK8~21</strong> supported.</li>
<li>Supports shared and circular reference object serialization for golang.</li>
<li>Supports automatic object serialization for golang.</li>
</ul>
<h2 tabindex="-1" id="user-content-protocols" dir="auto"><a href="#protocols">Protocols</a></h2>
<p dir="auto">Different scenarios have different serialization requirements. Fury designed and implemented
multiple binary protocols for those requirements:</p>
<ul dir="auto">
<li><strong>Cross-language object graph protocol</strong>:
<ul dir="auto">
<li>Cross-language serialize any object automatically, no need for IDL definition, schema compilation and object to/from protocol
conversion.</li>
<li>Support shared reference and circular reference, no duplicate data or recursion error.</li>
<li>Support object polymorphism.</li>
</ul>
</li>
<li><strong>Native java/python object graph protocol</strong>: highly-optimized based on type system of the language.</li>
<li><strong>Row format protocol</strong>: a cache-friendly binary random access format, supports skipping serialization and partial serialization,
and can convert to column-format automatically.</li>
</ul>
<p dir="auto">New protocols can be added easily based on fury existing buffer, encoding, meta, codegen and other capabilities. All of those share same codebase, and the optimization for one protocol
can be reused by another protocol.</p>
<h2 tabindex="-1" id="user-content-benchmarks" dir="auto"><a href="#benchmarks">Benchmarks</a></h2>
<p dir="auto">Different serialization frameworks are suitable for different scenarios, and benchmark results here are for reference only.</p>
<p dir="auto">If you need to benchmark for your specific scenario, make sure all serialization frameworks are appropriately configured for that scenario.</p>
<p dir="auto">Dynamic serialization frameworks support polymorphism and references, but they often come with a higher cost compared to static serialization frameworks, unless they utilize JIT techniques like Fury does.
Because Fury generates code at runtime, it is recommended to <strong>warm up</strong> the system before collecting benchmark statistics.</p>
<h3 tabindex="-1" id="user-content-java-serialization" dir="auto"><a href="#java-serialization">Java Serialization</a></h3>
<p dir="auto">Title containing "compatible" represent schema compatible mode: support type forward/backward compatibility.</p>
<p dir="auto">Title without "compatible" represent schema consistent mode: class schema must be same between serialization and deserialization.</p>
<p dir="auto"><code>Struct</code> is a class with <a href="https://github.com/alipay/fury/tree/main/docs/benchmarks#Struct">100 primitive fields</a>, <code>MediaContent</code> is a class from <a href="https://github.com/eishay/jvm-serializers/blob/master/tpc/src/data/media/MediaContent.java">jvm-serializers</a>, <code>Sample</code> is a class from <a href="https://github.com/EsotericSoftware/kryo/blob/master/benchmarks/src/main/java/com/esotericsoftware/kryo/benchmarks/data/Sample.java">kryo benchmark</a>.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/benchmarks/compatible/bench_serialize_compatible_STRUCT_to_directBuffer_tps.png"><img width="24%" alt="" src="https://github.com/alipay/fury/raw/main/docs/benchmarks/compatible/bench_serialize_compatible_STRUCT_to_directBuffer_tps.png"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/benchmarks/compatible/bench_serialize_compatible_MEDIA_CONTENT_to_array_tps.png"><img width="24%" alt="" src="https://github.com/alipay/fury/raw/main/docs/benchmarks/compatible/bench_serialize_compatible_MEDIA_CONTENT_to_array_tps.png"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/benchmarks/serialization/bench_serialize_MEDIA_CONTENT_to_array_tps.png"><img width="24%" alt="" src="https://github.com/alipay/fury/raw/main/docs/benchmarks/serialization/bench_serialize_MEDIA_CONTENT_to_array_tps.png"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/benchmarks/serialization/bench_serialize_SAMPLE_to_array_tps.png"><img width="24%" alt="" src="https://github.com/alipay/fury/raw/main/docs/benchmarks/serialization/bench_serialize_SAMPLE_to_array_tps.png"></a>
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/benchmarks/compatible/bench_deserialize_compatible_STRUCT_from_directBuffer_tps.png"><img width="24%" alt="" src="https://github.com/alipay/fury/raw/main/docs/benchmarks/compatible/bench_deserialize_compatible_STRUCT_from_directBuffer_tps.png"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/benchmarks/compatible/bench_deserialize_compatible_MEDIA_CONTENT_from_array_tps.png"><img width="24%" alt="" src="https://github.com/alipay/fury/raw/main/docs/benchmarks/compatible/bench_deserialize_compatible_MEDIA_CONTENT_from_array_tps.png"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/benchmarks/deserialization/bench_deserialize_MEDIA_CONTENT_from_array_tps.png"><img width="24%" alt="" src="https://github.com/alipay/fury/raw/main/docs/benchmarks/deserialization/bench_deserialize_MEDIA_CONTENT_from_array_tps.png"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/benchmarks/deserialization/bench_deserialize_SAMPLE_from_array_tps.png"><img width="24%" alt="" src="https://github.com/alipay/fury/raw/main/docs/benchmarks/deserialization/bench_deserialize_SAMPLE_from_array_tps.png"></a>
</p>
<p dir="auto">See <a href="https://github.com/alipay/fury/tree/main/docs/benchmarks">benchmarks</a> for more benchmarks about type forward/backward compatibility, off-heap support, zero-copy serialization.</p>
<h2 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h2>
<h3 tabindex="-1" id="user-content-java" dir="auto"><a href="#java">Java</a></h3>
<p dir="auto">Nightly snapshot:</p>
<div dir="auto" data-snippet-clipboard-copy-content="<repositories>
  <repository>
    <id>sonatype</id>
    <url>https://s01.oss.sonatype.org/content/repositories/snapshots</url>
    <releases>
      <enabled>false</enabled>
    </releases>
    <snapshots>
      <enabled>true</enabled>
    </snapshots>
  </repository>
</repositories>
<dependency>
  <groupId>org.furyio</groupId>
  <artifactId>fury-core</artifactId>
  <version>0.3.0-SNAPSHOT</version>
</dependency>
<!-- row/arrow format support -->
<!-- <dependency>
  <groupId>org.furyio</groupId>
  <artifactId>fury-format</artifactId>
  <version>0.3.0-SNAPSHOT</version>
</dependency> -->"><pre>&lt;<span>repositories</span>&gt;
  &lt;<span>repository</span>&gt;
    &lt;<span>id</span>&gt;sonatype&lt;/<span>id</span>&gt;
    &lt;<span>url</span>&gt;https://s01.oss.sonatype.org/content/repositories/snapshots&lt;/<span>url</span>&gt;
    &lt;<span>releases</span>&gt;
      &lt;<span>enabled</span>&gt;false&lt;/<span>enabled</span>&gt;
    &lt;/<span>releases</span>&gt;
    &lt;<span>snapshots</span>&gt;
      &lt;<span>enabled</span>&gt;true&lt;/<span>enabled</span>&gt;
    &lt;/<span>snapshots</span>&gt;
  &lt;/<span>repository</span>&gt;
&lt;/<span>repositories</span>&gt;
&lt;<span>dependency</span>&gt;
  &lt;<span>groupId</span>&gt;org.furyio&lt;/<span>groupId</span>&gt;
  &lt;<span>artifactId</span>&gt;fury-core&lt;/<span>artifactId</span>&gt;
  &lt;<span>version</span>&gt;0.3.0-SNAPSHOT&lt;/<span>version</span>&gt;
&lt;/<span>dependency</span>&gt;
<span><span>&lt;!--</span> row/arrow format support <span>--&gt;</span></span>
<span><span>&lt;!--</span> &lt;dependency&gt;</span>
<span>  &lt;groupId&gt;org.furyio&lt;/groupId&gt;</span>
<span>  &lt;artifactId&gt;fury-format&lt;/artifactId&gt;</span>
<span>  &lt;version&gt;0.3.0-SNAPSHOT&lt;/version&gt;</span>
<span>&lt;/dependency&gt; <span>--&gt;</span></span></pre></div>
<p dir="auto">Release version:</p>
<div dir="auto" data-snippet-clipboard-copy-content="<dependency>
  <groupId>org.furyio</groupId>
  <artifactId>fury-core</artifactId>
  <version>0.2.0</version>
</dependency>
<!-- row/arrow format support -->
<!-- <dependency>
  <groupId>org.furyio</groupId>
  <artifactId>fury-format</artifactId>
  <version>0.2.0</version>
</dependency> -->"><pre>&lt;<span>dependency</span>&gt;
  &lt;<span>groupId</span>&gt;org.furyio&lt;/<span>groupId</span>&gt;
  &lt;<span>artifactId</span>&gt;fury-core&lt;/<span>artifactId</span>&gt;
  &lt;<span>version</span>&gt;0.2.0&lt;/<span>version</span>&gt;
&lt;/<span>dependency</span>&gt;
<span><span>&lt;!--</span> row/arrow format support <span>--&gt;</span></span>
<span><span>&lt;!--</span> &lt;dependency&gt;</span>
<span>  &lt;groupId&gt;org.furyio&lt;/groupId&gt;</span>
<span>  &lt;artifactId&gt;fury-format&lt;/artifactId&gt;</span>
<span>  &lt;version&gt;0.2.0&lt;/version&gt;</span>
<span>&lt;/dependency&gt; <span>--&gt;</span></span></pre></div>
<h3 tabindex="-1" id="user-content-python" dir="auto"><a href="#python">Python</a></h3>
<div dir="auto" data-snippet-clipboard-copy-content="# Release version will be provided in the future.
pip install pyfury --pre"><pre><span><span>#</span> Release version will be provided in the future.</span>
pip install pyfury --pre</pre></div>
<h3 tabindex="-1" id="user-content-javascript" dir="auto"><a href="#javascript">JavaScript</a></h3>

<h3 tabindex="-1" id="user-content-golang" dir="auto"><a href="#golang">Golang</a></h3>
<p dir="auto">Coming soon.</p>
<h2 tabindex="-1" id="user-content-quickstart" dir="auto"><a href="#quickstart">Quickstart</a></h2>
<p dir="auto">Here we give a quick start about how to use fury, see <a href="https://github.com/alipay/fury/blob/main/docs/README.md">user guide</a> for more details about <a href="https://github.com/alipay/fury/blob/main/docs/guide/java_object_graph_guide.md">java</a>, <a href="https://github.com/alipay/fury/blob/main/docs/guide/xlang_object_graph_guide.md">cross language</a>, and <a href="https://github.com/alipay/fury/blob/main/docs/guide/row_format_guide.md">row format</a>.</p>
<h3 tabindex="-1" id="user-content-fury-java-object-graph-serialization" dir="auto"><a href="#fury-java-object-graph-serialization">Fury java object graph serialization</a></h3>
<p dir="auto">If you don't have cross-language requirements, using this mode will
have better performance.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import io.fury.*;
import java.util.*;

public class Example {
  public static void main(String[] args) {
    SomeClass object = new SomeClass();
    // Note that Fury instances should be reused between 
    // multiple serializations of different objects.
    {
      Fury fury = Fury.builder().withLanguage(Language.JAVA)
        // Allow to deserialize objects unknown types, more flexible 
        // but may be insecure if the classes contains malicious code.
        // .requireClassRegistration(false)
        .build();
      // Registering types can reduce class name serialization overhead, but not mandatory.
      // If class registration enabled, all custom types must be registered.
      fury.register(SomeClass.class);
      byte[] bytes = fury.serialize(object);
      System.out.println(fury.deserialize(bytes));
    }
    {
      ThreadSafeFury fury = Fury.builder().withLanguage(Language.JAVA)
        // Allow to deserialize objects unknown types, more flexible 
        // but may be insecure if the classes contains malicious code.
        // .requireClassRegistration(false)
        .buildThreadSafeFury();
      byte[] bytes = fury.serialize(object);
      System.out.println(fury.deserialize(bytes));
    }
    {
      ThreadSafeFury fury = new ThreadLocalFury(classLoader -> {
        Fury f = Fury.builder().withLanguage(Language.JAVA)
          .withClassLoader(classLoader).build();
        f.register(SomeClass.class);
        return f;
      });
      byte[] bytes = fury.serialize(object);
      System.out.println(fury.deserialize(bytes));
    }
  }
}"><pre><span>import</span> <span>io</span>.<span>fury</span>.*;
<span>import</span> <span>java</span>.<span>util</span>.*;

<span>public</span> <span>class</span> <span>Example</span> {
  <span>public</span> <span>static</span> <span>void</span> <span>main</span>(<span>String</span>[] <span>args</span>) {
    <span>SomeClass</span> <span>object</span> = <span>new</span> <span>SomeClass</span>();
    <span>// Note that Fury instances should be reused between </span>
    <span>// multiple serializations of different objects.</span>
    {
      <span>Fury</span> <span>fury</span> = <span>Fury</span>.<span>builder</span>().<span>withLanguage</span>(<span>Language</span>.<span>JAVA</span>)
        <span>// Allow to deserialize objects unknown types, more flexible </span>
        <span>// but may be insecure if the classes contains malicious code.</span>
        <span>// .requireClassRegistration(false)</span>
        .<span>build</span>();
      <span>// Registering types can reduce class name serialization overhead, but not mandatory.</span>
      <span>// If class registration enabled, all custom types must be registered.</span>
      <span>fury</span>.<span>register</span>(<span>SomeClass</span>.<span>class</span>);
      <span>byte</span>[] <span>bytes</span> = <span>fury</span>.<span>serialize</span>(<span>object</span>);
      <span>System</span>.<span>out</span>.<span>println</span>(<span>fury</span>.<span>deserialize</span>(<span>bytes</span>));
    }
    {
      <span>ThreadSafeFury</span> <span>fury</span> = <span>Fury</span>.<span>builder</span>().<span>withLanguage</span>(<span>Language</span>.<span>JAVA</span>)
        <span>// Allow to deserialize objects unknown types, more flexible </span>
        <span>// but may be insecure if the classes contains malicious code.</span>
        <span>// .requireClassRegistration(false)</span>
        .<span>buildThreadSafeFury</span>();
      <span>byte</span>[] <span>bytes</span> = <span>fury</span>.<span>serialize</span>(<span>object</span>);
      <span>System</span>.<span>out</span>.<span>println</span>(<span>fury</span>.<span>deserialize</span>(<span>bytes</span>));
    }
    {
      <span>ThreadSafeFury</span> <span>fury</span> = <span>new</span> <span>ThreadLocalFury</span>(<span>classLoader</span> -&gt; {
        <span>Fury</span> <span>f</span> = <span>Fury</span>.<span>builder</span>().<span>withLanguage</span>(<span>Language</span>.<span>JAVA</span>)
          .<span>withClassLoader</span>(<span>classLoader</span>).<span>build</span>();
        <span>f</span>.<span>register</span>(<span>SomeClass</span>.<span>class</span>);
        <span>return</span> <span>f</span>;
      });
      <span>byte</span>[] <span>bytes</span> = <span>fury</span>.<span>serialize</span>(<span>object</span>);
      <span>System</span>.<span>out</span>.<span>println</span>(<span>fury</span>.<span>deserialize</span>(<span>bytes</span>));
    }
  }
}</pre></div>
<h3 tabindex="-1" id="user-content-cross-language-object-graph-serialization" dir="auto"><a href="#cross-language-object-graph-serialization">Cross-language object graph serialization</a></h3>
<p dir="auto"><strong>Java</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="import io.fury.*;
import java.util.*;

public class ReferenceExample {
  public static class SomeClass {
    SomeClass f1;
    Map<String, String> f2;
    Map<String, String> f3;
  }

  public static Object createObject() {
    SomeClass obj = new SomeClass();
    obj.f1 = obj;
    obj.f2 = ofHashMap(&quot;k1&quot;, &quot;v1&quot;, &quot;k2&quot;, &quot;v2&quot;);
    obj.f3 = obj.f2;
    return obj;
  }

  // mvn exec:java -Dexec.mainClass=&quot;io.fury.examples.ReferenceExample&quot;
  public static void main(String[] args) {
    Fury fury = Fury.builder().withLanguage(Language.XLANG)
      .withRefTracking(true).build();
    fury.register(SomeClass.class, &quot;example.SomeClass&quot;);
    byte[] bytes = fury.serialize(createObject());
    // bytes can be data serialized by other languages.
    System.out.println(fury.deserialize(bytes));
  }
}"><pre><span>import</span> <span>io</span>.<span>fury</span>.*;
<span>import</span> <span>java</span>.<span>util</span>.*;

<span>public</span> <span>class</span> <span>ReferenceExample</span> {
  <span>public</span> <span>static</span> <span>class</span> <span>SomeClass</span> {
    <span>SomeClass</span> <span>f1</span>;
    <span>Map</span>&lt;<span>String</span>, <span>String</span>&gt; <span>f2</span>;
    <span>Map</span>&lt;<span>String</span>, <span>String</span>&gt; <span>f3</span>;
  }

  <span>public</span> <span>static</span> <span>Object</span> <span>createObject</span>() {
    <span>SomeClass</span> <span>obj</span> = <span>new</span> <span>SomeClass</span>();
    <span>obj</span>.<span>f1</span> = <span>obj</span>;
    <span>obj</span>.<span>f2</span> = <span>ofHashMap</span>(<span>"k1"</span>, <span>"v1"</span>, <span>"k2"</span>, <span>"v2"</span>);
    <span>obj</span>.<span>f3</span> = <span>obj</span>.<span>f2</span>;
    <span>return</span> <span>obj</span>;
  }

  <span>// mvn exec:java -Dexec.mainClass="io.fury.examples.ReferenceExample"</span>
  <span>public</span> <span>static</span> <span>void</span> <span>main</span>(<span>String</span>[] <span>args</span>) {
    <span>Fury</span> <span>fury</span> = <span>Fury</span>.<span>builder</span>().<span>withLanguage</span>(<span>Language</span>.<span>XLANG</span>)
      .<span>withRefTracking</span>(<span>true</span>).<span>build</span>();
    <span>fury</span>.<span>register</span>(<span>SomeClass</span>.<span>class</span>, <span>"example.SomeClass"</span>);
    <span>byte</span>[] <span>bytes</span> = <span>fury</span>.<span>serialize</span>(<span>createObject</span>());
    <span>// bytes can be data serialized by other languages.</span>
    <span>System</span>.<span>out</span>.<span>println</span>(<span>fury</span>.<span>deserialize</span>(<span>bytes</span>));
  }
}</pre></div>
<p dir="auto"><strong>Python</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="from typing import Dict
import pyfury

class SomeClass:
    f1: &quot;SomeClass&quot;
    f2: Dict[str, str]
    f3: Dict[str, str]

fury = pyfury.Fury(ref_tracking=True)
fury.register_class(SomeClass, &quot;example.SomeClass&quot;)
obj = SomeClass()
obj.f2 = {&quot;k1&quot;: &quot;v1&quot;, &quot;k2&quot;: &quot;v2&quot;}
obj.f1, obj.f3 = obj, obj.f2
data = fury.serialize(obj)
# bytes can be data serialized by other languages.
print(fury.deserialize(data))"><pre><span>from</span> <span>typing</span> <span>import</span> <span>Dict</span>
<span>import</span> <span>pyfury</span>

<span>class</span> <span>SomeClass</span>:
    <span>f1</span>: <span>"SomeClass"</span>
    <span>f2</span>: <span>Dict</span>[<span>str</span>, <span>str</span>]
    <span>f3</span>: <span>Dict</span>[<span>str</span>, <span>str</span>]

<span>fury</span> <span>=</span> <span>pyfury</span>.<span>Fury</span>(<span>ref_tracking</span><span>=</span><span>True</span>)
<span>fury</span>.<span>register_class</span>(<span>SomeClass</span>, <span>"example.SomeClass"</span>)
<span>obj</span> <span>=</span> <span>SomeClass</span>()
<span>obj</span>.<span>f2</span> <span>=</span> {<span>"k1"</span>: <span>"v1"</span>, <span>"k2"</span>: <span>"v2"</span>}
<span>obj</span>.<span>f1</span>, <span>obj</span>.<span>f3</span> <span>=</span> <span>obj</span>, <span>obj</span>.<span>f2</span>
<span>data</span> <span>=</span> <span>fury</span>.<span>serialize</span>(<span>obj</span>)
<span># bytes can be data serialized by other languages.</span>
<span>print</span>(<span>fury</span>.<span>deserialize</span>(<span>data</span>))</pre></div>
<p dir="auto"><strong>Golang</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="package main

import furygo &quot;github.com/alipay/fury/fury/go/fury&quot;
import &quot;fmt&quot;

func main() {
	type SomeClass struct {
		F1 *SomeClass
		F2 map[string]string
		F3 map[string]string
	}
	fury := furygo.NewFury(true)
	if err := fury.RegisterTagType(&quot;example.SomeClass&quot;, SomeClass{}); err != nil {
		panic(err)
	}
	value := &amp;SomeClass{F2: map[string]string{&quot;k1&quot;: &quot;v1&quot;, &quot;k2&quot;: &quot;v2&quot;}}
	value.F3 = value.F2
	value.F1 = value
	bytes, err := fury.Marshal(value)
	if err != nil {
	}
	var newValue interface{}
	// bytes can be data serialized by other languages.
	if err := fury.Unmarshal(bytes, &amp;newValue); err != nil {
		panic(err)
	}
	fmt.Println(newValue)
}"><pre><span>package</span> main

<span>import</span> furygo <span>"github.com/alipay/fury/fury/go/fury"</span>
<span>import</span> <span>"fmt"</span>

<span>func</span> <span>main</span>() {
	<span>type</span> <span>SomeClass</span> <span>struct</span> {
		<span>F1</span> <span>*</span><span>SomeClass</span>
		<span>F2</span> <span>map</span>[<span>string</span>]<span>string</span>
		<span>F3</span> <span>map</span>[<span>string</span>]<span>string</span>
	}
	<span>fury</span> <span>:=</span> <span>furygo</span>.<span>NewFury</span>(<span>true</span>)
	<span>if</span> <span>err</span> <span>:=</span> <span>fury</span>.<span>RegisterTagType</span>(<span>"example.SomeClass"</span>, <span>SomeClass</span>{}); <span>err</span> <span>!=</span> <span>nil</span> {
		<span>panic</span>(<span>err</span>)
	}
	<span>value</span> <span>:=</span> <span>&amp;</span><span>SomeClass</span>{<span>F2</span>: <span>map</span>[<span>string</span>]<span>string</span>{<span>"k1"</span>: <span>"v1"</span>, <span>"k2"</span>: <span>"v2"</span>}}
	<span>value</span>.<span>F3</span> <span>=</span> <span>value</span>.<span>F2</span>
	<span>value</span>.<span>F1</span> <span>=</span> <span>value</span>
	<span>bytes</span>, <span>err</span> <span>:=</span> <span>fury</span>.<span>Marshal</span>(<span>value</span>)
	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
	}
	<span>var</span> <span>newValue</span> <span>interface</span>{}
	<span>// bytes can be data serialized by other languages.</span>
	<span>if</span> <span>err</span> <span>:=</span> <span>fury</span>.<span>Unmarshal</span>(<span>bytes</span>, <span>&amp;</span><span>newValue</span>); <span>err</span> <span>!=</span> <span>nil</span> {
		<span>panic</span>(<span>err</span>)
	}
	<span>fmt</span>.<span>Println</span>(<span>newValue</span>)
}</pre></div>
<h3 tabindex="-1" id="user-content-row-format" dir="auto"><a href="#row-format">Row format</a></h3>
<h4 tabindex="-1" id="user-content-java-1" dir="auto"><a href="#java-1">Java</a></h4>
<div dir="auto" data-snippet-clipboard-copy-content="public class Bar {
  String f1;
  List<Long> f2;
}

public class Foo {
  int f1;
  List<Integer> f2;
  Map<String, Integer> f3;
  List<Bar> f4;
}

RowEncoder<Foo> encoder = Encoders.bean(Foo.class);
Foo foo = new Foo();
foo.f1 = 10;
foo.f2 = IntStream.range(0, 1000000).boxed().collect(Collectors.toList());
foo.f3 = IntStream.range(0, 1000000).boxed().collect(Collectors.toMap(i -> &quot;k&quot;+i, i->i));
List<Bar> bars = new ArrayList<>(1000000);
for (int i = 0; i < 1000000; i++) {
  Bar bar = new Bar();
  bar.f1 = &quot;s&quot;+i;
  bar.f2 = LongStream.range(0, 10).boxed().collect(Collectors.toList());
  bars.add(bar);
}
foo.f4 = bars;
// Can be zero-copy read by python
BinaryRow binaryRow = encoder.toRow(foo);
// can be data from python
Foo newFoo = encoder.fromRow(binaryRow);
// zero-copy read List<Integer> f2
BinaryArray binaryArray2 = binaryRow.getArray(1);
// zero-copy read List<Bar> f4
BinaryArray binaryArray4 = binaryRow.getArray(3);
// zero-copy read 11th element of `readList<Bar> f4`
BinaryRow barStruct = binaryArray4.getStruct(10);

// zero-copy read 6th of f2 of 11th element of `readList<Bar> f4`
barStruct.getArray(1).getLong(5);
RowEncoder<Bar> barEncoder = Encoders.bean(Bar.class);
// deserialize part of data.
Bar newBar = barEncoder.fromRow(barStruct);
Bar newBar2 = barEncoder.fromRow(binaryArray4.getStruct(20));"><pre><span>public</span> <span>class</span> <span>Bar</span> {
  <span>String</span> <span>f1</span>;
  <span>List</span>&lt;<span>Long</span>&gt; <span>f2</span>;
}

<span>public</span> <span>class</span> <span>Foo</span> {
  <span>int</span> <span>f1</span>;
  <span>List</span>&lt;<span>Integer</span>&gt; <span>f2</span>;
  <span>Map</span>&lt;<span>String</span>, <span>Integer</span>&gt; <span>f3</span>;
  <span>List</span>&lt;<span>Bar</span>&gt; <span>f4</span>;
}

<span>RowEncoder</span>&lt;<span>Foo</span>&gt; <span>encoder</span> = <span>Encoders</span>.<span>bean</span>(<span>Foo</span>.<span>class</span>);
<span>Foo</span> <span>foo</span> = <span>new</span> <span>Foo</span>();
<span>foo</span>.<span>f1</span> = <span>10</span>;
<span>foo</span>.<span>f2</span> = <span>IntStream</span>.<span>range</span>(<span>0</span>, <span>1000000</span>).<span>boxed</span>().<span>collect</span>(<span>Collectors</span>.<span>toList</span>());
<span>foo</span>.<span>f3</span> = <span>IntStream</span>.<span>range</span>(<span>0</span>, <span>1000000</span>).<span>boxed</span>().<span>collect</span>(<span>Collectors</span>.<span>toMap</span>(<span>i</span> -&gt; <span>"k"</span>+<span>i</span>, <span>i</span>-&gt;<span>i</span>));
<span>List</span>&lt;<span>Bar</span>&gt; <span>bars</span> = <span>new</span> <span>ArrayList</span>&lt;&gt;(<span>1000000</span>);
<span>for</span> (<span>int</span> <span>i</span> = <span>0</span>; <span>i</span> &lt; <span>1000000</span>; <span>i</span>++) {
  <span>Bar</span> <span>bar</span> = <span>new</span> <span>Bar</span>();
  <span>bar</span>.<span>f1</span> = <span>"s"</span>+<span>i</span>;
  <span>bar</span>.<span>f2</span> = <span>LongStream</span>.<span>range</span>(<span>0</span>, <span>10</span>).<span>boxed</span>().<span>collect</span>(<span>Collectors</span>.<span>toList</span>());
  <span>bars</span>.<span>add</span>(<span>bar</span>);
}
<span>foo</span>.<span>f4</span> = <span>bars</span>;
<span>// Can be zero-copy read by python</span>
<span>BinaryRow</span> <span>binaryRow</span> = <span>encoder</span>.<span>toRow</span>(<span>foo</span>);
<span>// can be data from python</span>
<span>Foo</span> <span>newFoo</span> = <span>encoder</span>.<span>fromRow</span>(<span>binaryRow</span>);
<span>// zero-copy read List&lt;Integer&gt; f2</span>
<span>BinaryArray</span> <span>binaryArray2</span> = <span>binaryRow</span>.<span>getArray</span>(<span>1</span>);
<span>// zero-copy read List&lt;Bar&gt; f4</span>
<span>BinaryArray</span> <span>binaryArray4</span> = <span>binaryRow</span>.<span>getArray</span>(<span>3</span>);
<span>// zero-copy read 11th element of `readList&lt;Bar&gt; f4`</span>
<span>BinaryRow</span> <span>barStruct</span> = <span>binaryArray4</span>.<span>getStruct</span>(<span>10</span>);

<span>// zero-copy read 6th of f2 of 11th element of `readList&lt;Bar&gt; f4`</span>
<span>barStruct</span>.<span>getArray</span>(<span>1</span>).<span>getLong</span>(<span>5</span>);
<span>RowEncoder</span>&lt;<span>Bar</span>&gt; <span>barEncoder</span> = <span>Encoders</span>.<span>bean</span>(<span>Bar</span>.<span>class</span>);
<span>// deserialize part of data.</span>
<span>Bar</span> <span>newBar</span> = <span>barEncoder</span>.<span>fromRow</span>(<span>barStruct</span>);
<span>Bar</span> <span>newBar2</span> = <span>barEncoder</span>.<span>fromRow</span>(<span>binaryArray4</span>.<span>getStruct</span>(<span>20</span>));</pre></div>
<h4 tabindex="-1" id="user-content-python-1" dir="auto"><a href="#python-1">Python</a></h4>
<div dir="auto" data-snippet-clipboard-copy-content="@dataclass
class Bar:
    f1: str
    f2: List[pa.int64]
@dataclass
class Foo:
    f1: pa.int32
    f2: List[pa.int32]
    f3: Dict[str, pa.int32]
    f4: List[Bar]

encoder = pyfury.encoder(Foo)
foo = Foo(f1=10, f2=list(range(1000_000)),
         f3={f&quot;k{i}&quot;: i for i in range(1000_000)},
         f4=[Bar(f1=f&quot;s{i}&quot;, f2=list(range(10))) for i in range(1000_000)])
binary: bytes = encoder.to_row(foo).to_bytes()
foo_row = pyfury.RowData(encoder.schema, binary)
print(foo_row.f2[100000], foo_row.f4[100000].f1, foo_row.f4[200000].f2[5])"><pre><span>@<span>dataclass</span></span>
<span>class</span> <span>Bar</span>:
    <span>f1</span>: <span>str</span>
    <span>f2</span>: <span>List</span>[<span>pa</span>.<span>int64</span>]
<span>@<span>dataclass</span></span>
<span>class</span> <span>Foo</span>:
    <span>f1</span>: <span>pa</span>.<span>int32</span>
    <span>f2</span>: <span>List</span>[<span>pa</span>.<span>int32</span>]
    <span>f3</span>: <span>Dict</span>[<span>str</span>, <span>pa</span>.<span>int32</span>]
    <span>f4</span>: <span>List</span>[<span>Bar</span>]

<span>encoder</span> <span>=</span> <span>pyfury</span>.<span>encoder</span>(<span>Foo</span>)
<span>foo</span> <span>=</span> <span>Foo</span>(<span>f1</span><span>=</span><span>10</span>, <span>f2</span><span>=</span><span>list</span>(<span>range</span>(<span>1000_000</span>)),
         <span>f3</span><span>=</span>{<span>f"k<span><span>{</span><span>i</span><span>}</span></span>"</span>: <span>i</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>1000_000</span>)},
         <span>f4</span><span>=</span>[<span>Bar</span>(<span>f1</span><span>=</span><span>f"s<span><span>{</span><span>i</span><span>}</span></span>"</span>, <span>f2</span><span>=</span><span>list</span>(<span>range</span>(<span>10</span>))) <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>1000_000</span>)])
<span>binary</span>: <span>bytes</span> <span>=</span> <span>encoder</span>.<span>to_row</span>(<span>foo</span>).<span>to_bytes</span>()
<span>foo_row</span> <span>=</span> <span>pyfury</span>.<span>RowData</span>(<span>encoder</span>.<span>schema</span>, <span>binary</span>)
<span>print</span>(<span>foo_row</span>.<span>f2</span>[<span>100000</span>], <span>foo_row</span>.<span>f4</span>[<span>100000</span>].<span>f1</span>, <span>foo_row</span>.<span>f4</span>[<span>200000</span>].<span>f2</span>[<span>5</span>])</pre></div>
<h2 tabindex="-1" id="user-content-compatibility" dir="auto"><a href="#compatibility">Compatibility</a></h2>
<h3 tabindex="-1" id="user-content-schema-compatibility" dir="auto"><a href="#schema-compatibility">Schema Compatibility</a></h3>
<p dir="auto">Fury java object graph serialization support class schema forward/backward compatibility. The serialization peer and deserialization peer can add/delete fields independently.</p>
<p dir="auto">We plan to add support cross-language serialization after <a href="https://github.com/alipay/fury/issues/203" data-hovercard-type="issue" data-hovercard-url="/alipay/fury/issues/203/hovercard">meta compression</a> are finished.</p>
<h3 tabindex="-1" id="user-content-binary-compatibility" dir="auto"><a href="#binary-compatibility">Binary Compatibility</a></h3>
<p dir="auto">We are still improving our protocols, binary compatibility are not ensured between fury major releases for now.
it's ensured between minor version only. Please
<code>versioning</code> your data by fury major version if you will upgrade fury in the future, see <a href="https://github.com/alipay/fury/blob/main/docs/guide/java_object_graph_guide.md#upgrade-fury">how to upgrade fury</a> for further details.</p>
<p dir="auto">Binary compatibility will be ensured before fury 1.0.</p>
<h2 tabindex="-1" id="user-content-security" dir="auto"><a href="#security">Security</a></h2>
<p dir="auto">Static serialization are secure. But dynamic serialization such as fury java/python native serialization supports deserialize unregistered types, which provides more dynamics and flexibility, but also introduce security risks.</p>
<p dir="auto">For example, the deserialization may invoke <code>init</code> constructor or <code>equals</code>/<code>hashCode</code> method, if the method body contains malicious code, the system will be at risks.</p>
<p dir="auto">Fury provides a class registration option and enabled by default for such protocols, which allows only deserializing trusted registered types or built-in types.
<strong>Do not disable class registration unless you can ensure your environment is secure</strong>.</p>
<p dir="auto">If this option is disabled, you are responsible for serialization security. You can configure <code>io.fury.resolver.ClassChecker</code> by
<code>ClassResolver#setClassChecker</code> to control which classes are allowed for serialization.</p>
<h2 tabindex="-1" id="user-content-roadmap" dir="auto"><a href="#roadmap">RoadMap</a></h2>
<ul dir="auto">
<li>Meta compression, auto meta sharing and cross-language schema compatibility.</li>
<li>AOT Framework for c++/golang/rust to generate code statically.</li>
<li>C++/Rust object graph serialization support</li>
<li>Golang/Rust/NodeJS row format support</li>
<li>ProtoBuffer compatibility support</li>
<li>Protocols for features and knowledge graph serialization</li>
<li>Continuously improve our serialization infrastructure for any new protocols</li>
</ul>
<h2 tabindex="-1" id="user-content-how-to-contribute" dir="auto"><a href="#how-to-contribute">How to Contribute</a></h2>
<p dir="auto">Please read the <a href="https://github.com/alipay/fury/blob/main/CONTRIBUTING.md">CONTRIBUTING</a> guide.</p>
<p dir="auto">For ecosystem projects, please see <a href="https://github.com/fury-project">https://github.com/fury-project</a></p>
<h2 tabindex="-1" id="user-content-getting-involved" dir="auto"><a href="#getting-involved">Getting involved</a></h2>
<table>
<thead>
<tr>
<th>Platform</th>
<th>Purpose</th>
<th>Estimated Response Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/alipay/fury/issues">GitHub Issues</a></td>
<td>For reporting bugs and filing feature requests.</td>
<td>&lt; 1 days</td>
</tr>
<tr>
<td><a href="https://join.slack.com/t/fury-project/shared_invite/zt-1u8soj4qc-ieYEu7ciHOqA2mo47llS8A" rel="nofollow">Slack</a></td>
<td>For collaborating with other Fury users and latest announcements about Fury.</td>
<td>&lt; 2 days</td>
</tr>
<tr>
<td><a href="https://stackoverflow.com/questions/tagged/fury" rel="nofollow">StackOverflow</a></td>
<td>For asking questions about how to use Fury.</td>
<td>&lt; 2 days</td>
</tr>
<tr>
<td><a href="https://www.zhihu.com/column/c_1638859452651765760" rel="nofollow">Zhihu</a>  <a href="https://twitter.com/fury_community" rel="nofollow">Twitter</a>  <a href="https://www.youtube.com/@FurySerialization" rel="nofollow">Youtube</a></td>
<td>Follow us for latest announcements about Fury.</td>
<td>&lt; 2 days</td>
</tr>
<tr>
<td>WeChat Official Account(微信公众号) / Dingding Group(钉钉群)</td>
<td><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/images/fury_wechat_12.jpg"><img src="https://github.com/alipay/fury/raw/main/docs/images/fury_wechat_12.jpg" alt="WeChat Official Account " width="20%"></a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/alipay/fury/blob/main/docs/images/fury_dingtalk.png"><img src="https://github.com/alipay/fury/raw/main/docs/images/fury_dingtalk.png" alt="Dingding Group" width="20%"></a> </p></td>
<td>&lt; 2 days</td>
</tr>
</tbody>
</table>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Compete with Patreon (235 pts)]]></title>
            <link>https://siderea.dreamwidth.org/1824441.html</link>
            <guid>37808115</guid>
            <pubDate>Sun, 08 Oct 2023 05:34:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://siderea.dreamwidth.org/1824441.html">https://siderea.dreamwidth.org/1824441.html</a>, See on <a href="https://news.ycombinator.com/item?id=37808115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><i>Canonical link: <a href="https://siderea.dreamwidth.org/1824441.html">https://siderea.dreamwidth.org/1824441.html</a> </i></p><p>Another post that <a href="https://universeodon.com/@siderea/111122855587210647">started as a rant on Mastodon</a>.  This has been adapted and expanded further.</p><hr><p>It is incredibly frustrating that the only thing more stupid than Patreon is all the alleged Patreon substitutes that clearly don't even understand what Patreon does.</p><p>Pro tip: Patreon has no meaningful competitors, and also it sucks, so there's a huge opportunity for somebody to kick sand in its face and take its lunch money. But to do that you would have to understand what actually Patreon does that is worth it to creators to allow Patreon to take 5% of their proceeds (and then pass on to them a second 5% in payment processing fees).</p><p>Because I want there to be Patreon competitors, I will explain what Patreon actually does, so if somebody would like to actually compete with Patreon they will know what they have to actually accomplish.</p><p><a name="cutid1"></a>Brace yourself. Some of this is a little complicated to explain.</p><p>And before explaining it, allow me to observe: these are the sorts of features you get when somebody who actually really understands the usage case designs the platform. Patreon was founded by and designed by an actual goddamn artist – a musician and music video maker – who understood what artists and other creators actually need out of a platform.</p><p><b>1) Pseudonym support.</b></p><p>Attention dumbasses: stage names and pen names have been realities of the business of being an artist for over 2,000 years.</p><p>Creators are not shoe salesmen.</p><p>The most basic functionality of any platform that intends to support <em>creators</em> earning money online – the basic, rock-bottom, sine qua non affordance – is people doing business using pseudonyms.</p><p>What that <strong>actually</strong> means is the platform has to allow creators (the people who charge money) to be <strong>pseudonymous</strong> (to use their stage name or pen names) in all of their interactions with their patrons (the people paying the money), <strong>and</strong> the platform, which is legally required to know the legal identity of the creator and have on file their relevant tax ID numbers (e.g. SSN in the US), needs to keep that legal identity <strong>confidential</strong>.</p><p>Relatedly, it is not enough for your creator patronization platform to <em>have</em> fully supported pseudonyms. You must make that abundantly specific and clear, and a commitment that you make to your creators, <strong>up front</strong>. Ideally, it's part of your elevator pitch to perspective creators.</p><p>Patreon falls short of the standard but comes closer than any other payment accepting platform – as far as I know <em>at all</em>, of <em>any</em> type.</p><p>If you want to pull business away from Patreon, you will not do it if your platform, like umpteen zillion other idiot platforms, assumes that the creator is going to use their wallet name, and does not treat that wallet name as confidential information.</p><p>Listen, if all we wanted was a subscription platform that exposed our wallet names, we could have been using PayPal subscriptions all this time. What would we need you for?</p><p>Speaking of subscriptions....</p><p><b>2) By-works funding model support.</b></p><p>Patreon had two foundational funding models it supported, and while Patreon has introduced a variety of complexities and alternatives, these remain its core offerings.</p><p>One of these is trivial: monthly subscriptions. Like I said, creators can get that from PayPal if they don't care about pseudonyms. Every supposed Patreon substitute is just a monthly subscription service.</p><p>But Patreon's much more interesting funding model is not by-month, it's by-work.</p><p>This is where things get very hard for outsiders to understand. For emotional reasons.</p><p>People have trouble believing that this is actually so, but it is. This is the funding model I use.</p><p>The by-work funding model is where patrons pledge their support not by the month, but by the instance of whatever it is that the creator makes. If the creator makes stories, it's per story; if the creator makes music videos, it's per music video. The patrons pledge a certain amount of money for each work the creator makes.</p><p>If you're thinking, "but how many works does a creator ship each month?": no, that's the whole point of by-works. It can be <strong>any</strong> number. The creator makes no particular commitment to their patrons as to how many creations they're going to ship. They may ship <strong>no</strong> creations in a month, so in that month their patrons are charged nothing. Or they may ship one thing. Or 10 things. Or a 100 things.</p><p>(My best month was 10 things.)</p><p>You might reasonably be thinking, "Migod, what protects patrons from getting socked with an enormous bill because a creator ships many more things then they expected?"</p><p>Yeah, Patreon has a solution for that too: patrons can set a monthly upper limit for how much they're willing to support a given creator. If the creator exceeds that limit, that's fine, that patron is not charged in excess of that amount.</p><p>(Personally, I warn all of my patrons to set that limit, because every once in a while I go off on a productivity tear.)</p><p>This is where things get challenging. If you imagine this system, if you try to walk through in your mind how this would work, it will probably occur to you to ask the question, "Okay, but... How does the system know that you shipped something?"</p><p>Yeah about that. I log into Patreon, and I basically make a little blog post that says "Look! I made the thing you pledged to pay me for!" with the associated ticky box for "Pay me for this" ticked, and click submit.</p><p>That's it.</p><p>Patreon and no way, shape, or form attempts to validate that I actually shipped the thing. That is between me and my patrons. And, personally, I'm smart enough not to use Patreon as my delivery mechanism, so delivery of what I am paid for happens through a completely other channel.</p><p>There will be more on that subsequently (probably item three on this list of Patreon competition sine qua nons).</p><p>So you might be wondering, "but that's basically just a 'pay me' button, more or less directly connected to the patrons' credit cards!"</p><p>Yes. Precisely. I mean, it doesn't charge them until the end of the month, but yes.</p><p>"But what's to keep a creator from just hammering the 'pay me' button?"</p><p>Nothing. Nothing at all.</p><p>Like I said previously, individual patrons can set their own monthly maximums. But really the only thing that prohibits the creator from doing that is that it will piss off their patrons, and if the patrons don't feel that they're getting their money's worth, they can cancel their patronage. Which they will do.</p><p>And that's totally sufficient.</p><p>It helps to get over the emotional hump if you realize that it's not actually any different than invoicing somebody through PayPal. If somebody hires you to landscape their yard, and you send them an itemized invoice through PayPal's invoice system, PayPal isn't going to check to make sure that you actually edged the lawn and planted the begonias like agreed.</p><p>To my knowledge – and, <em>boy</em>, would I like to be wrong about this – there is not a single other platform in the world that supports the by-works model that Patreon offers. [Edit: Found one! Tipeee.com. It charges 8% instead of 5%, and has some breathtakingly invasive personal documentation requirements, but it's there.]</p><p>To be crystal clear, it entails:</p><p>• Patrons pledge to pay $n per work, on an ongoing basis, in advance of the work being done.</p><p>• The creator then generates works on an ad hoc basis, not to any set schedule, and each time they do, they check in with the UI to bill their patrons.</p><p>• The platform aggregates these bills, and submits them to the patrons' credit cards monthly.</p><p>Part of the reason this is hard to understand is that people who are not creators (and even some who are) think of what Patreon affords as "selling stuff". Which is incorrect.</p><p>There are two problems with the selling stuff paradigm: the selling and the stuff.</p><p>First and foremost, Patreon does not require you to sell anything. I mean, you can use it for that. Lots of creators do.</p><p>I don't.</p><p>I use Patreon for patronage instead. Kind of like it says on the tin.</p><p>So this is the next item on the list.</p><p><b>3) Non quid pro quo patronage</b></p><p>Patreon is not a platform for selling things, even though you can use it that way, and Patreon seems to prefer it be used that way.</p><p>But Patreon can also be used to collect support that is not tied to directly providing the person who pays the money with something just for themselves.</p><p>There is an entire little universe of people using Patreon to be funded to do good works in the world. These may be open source contributors. They may be activists. They may be journalists or bloggers. They do not make things that they exchange for money with the people who pledge them on Patreon.</p><p>Their patrons do not pay these creators to give things <em>to them</em>. Their patrons pay these creators to give things <em>to the world</em>: to release code for anyone to use, to engage in activism that changes the world for the better, or to write things that anyone can read.</p><p>I'm one of them. The number one reason I signed up for Patreon as my funding platform nine years ago, was because it was literally the only way of funding my writing that did not entail my <strong>selling</strong> it: my withholding it only for those people who paid me for it.</p><p>People get confused here. I'm not talking about my intellectual property rights. I'm not worried somebody is going to steal my copyright in my writing. (I mean it's a legitimate concern but that's not what I'm talking about here.)</p><p>I'm talking about the very basic nature of what it means to sell a work of writing, as a book or a magazine or a stand-alone article in a PDF.</p><p>If I put my writing into documents that then I sell on Amazon or Kajabi,  then the only people who get to see it are the people who pay me for them.</p><p>That is the antithesis of what I want to do. What I want to do is write openly on the internet where anyone can read what I write. Where what I write can be cited by anyone who wants to refer to it in any internet discussion.</p><p>The audience of my writing is not my patrons, and it is not just the people who pay me for it. It's the whole world.</p><p>And that, quite explicitly, is what my patrons pay me to do.</p><p>Most would-be competitors to Patreon think it's some sort of DRM system. There are definitely people who try to use Patreon that way, and it works about as well as any DRM system does.</p><p>We have lots of other "pay us to access this document" platforms, starting with the 800 lb gorilla, Amazon. If you think there is some benefit to wedding a membership system to a document storefront, I think you're probably wrong. I could be convinced otherwise – since I'm not actually in that business, I assume there's a lot I don't know about it – but my guess is having to join a club just for the privilege of buying a PDF introduces friction that reduces revenue. Heaven knows I resent it as a customer.</p><p>(All that said, I reserve the right to go off and write books that I will sell for money, something I absolutely someday intend to do. But that is a very different project.)</p><p>And this brings us to number four....</p><p><b>4) An audience relationship management system, ideally one with an API one can build against.</b></p><p>Here's where you can really kick Patreon's ass in the market for alternatives, because Patreon's game has been slipping very badly in this area.</p><p>I just invented the term "audience relationship management" system, by analogy to "customer relationship management" system.  Like I said patrons are not the same thing as customers. But if you're a creator you do need to keep track of them, and you do need to keep track of their payments, and you do need to be able to communicate with them.</p><p>Also you would probably like to be able to tell what's going on with your money: with the amounts pledged, the amounts received, the fees deducted, the credit cards declined, stuff like that. You might also appreciate some analytics.</p><p>Patreon's infrastructure for doing all of this is kind of falling apart. In some places they've just taken things down rather than fix bugs.</p><p>The really big example of this, I'm not wholly familiar with, because I don't really use it, and I was hearing about it and its problems from other people on the creator forums that then patreon took down.</p><p>I'm talking about Patreon's API.</p><p>So here's what I think I know, and my information is kind of old, and the pandemic happened, and I wasn't directly involved myself, so I might be misremembering.</p><p>But as I understand it, it goes something like this: </p><p>Patreon has the affordance of allowing creators to establish "tiers", where the creator associates certain dollar amounts of pledges with certain package deals they put together. Patreon has – had – has – an API that creators can have their own software query, so that the creator's other systems can tell in real time whether one of their users is a paid up patron over on Patreon, and if so at what tier level.</p><p>So for instance, if you (a creator) wanted to run a private discussion forum on the web somewhere just for your patrons, and you're willing to do some programming, you could implement a system whereby your discussion forum software checked in with Patreon.com when someone logged in, comparing their email address with the one on file for patrons, to see whether or not they should be let in in the first place, and if so which forum features they should have access to, based on their tier.</p><p>Well, you could.</p><p>They were creators whose entire business models were based on this. I gather there were also a third party integrations, companies that actually developed against the Patreon API so that their own services could be integrated with creators' campaigns on Patreon – that is to say there were companies that made products that they sold to creators, that relied on the API.</p><p>Well Patreon decided that they're not supporting the API anymore. Apparently, from the screaming on the (now long defunct) creator forums, for a while there it looked like Patreon was going to turn it off. The Patreon walked that back and said that they wouldn't turn the API off, but they wouldn't be supporting it anymore, and they wouldn't be doing any further development on it.</p><p>So in a really important sense, these creators (and these companies that had third party integrations) were (are) using Patreon.com as an identity server. But not just an identity server. It doesn't just serve the identity of patrons, but their status as patrons.  That's part of what makes it "audience relationship management".</p><p>If your supposed Patreon competitor does not support doing that, well, you're certainly not going to seduce all of the creators clinging to the sinking wreck of Patreon's API into abandoning ship on your account, are you?</p><p>Patreon also has removed key functionality from within the web interface that creators used to tell what's going on – particularly if their campaign is by-works, as discussed previously.</p><p>More generally, Patreon's UI for creators is really kind of terrible. I could itemize why but we'd be here for a while. A company could go far that offered the same services as Patreon, but let creators actually see what was happening to their money.</p><p>For instance, the creator UI used to have a page that listed all of the works a by-works creator had submitted, that listed, <em>for each work</em>, how much money was pledged in the first place, how much revenue was actually collected (declined credit card charges are a thing), how much Patreon's cut was, how much Patreon took out to pass on to the payment processor, and how much you, the creator, would actually net.</p><p>They took that away.</p><p>Build one of those, especially in conjunction with supporting the by-work funding model, and you will be very attractive to a whole bunch of bitter Patreon refugees.</p><p>So there you go: four crucial aspects of what Patreon is ACTUALLY up to – what its value proposition is to the creators that choose to use it – that you're not going to be able to compete with Patreon unless you implement, and ideally improve upon.</p><hr><p>P.S. I would be remiss if I didn't also mention a fifth thing that Patreon did which was part of its secret sauce, the feature nobody realized Patreon was giving us until they tried to take it away and broke everything: charge bundling.</p><p>In <a href="https://siderea.dreamwidth.org/1369787.html">December 2017, Patreon announced that they were changing the rules of the game</a>, in a way they tried to pass off as advantageous to creators, but was something very else under the hood. What that was, well, nobody's quite sure because of how much lying Patreon did and because of how they swore parties they told things to to secrecy.</p><p>What Patreon had been doing up to that point was if a patron pledged more than one creator, then Patreon would submit a single charge to the payment processor for the total amount that patron owed all of the creators they had pledged for that month.</p><p>What they proposed to do instead was run a charge for each creator a patron supported. A patron who pledged three creators would be charged three times in a month.</p><p>That doesn't sound like a big deal, but it is. Because the fee structure of the payment processors includes, in addition to a percentage rate, a per transaction flat fee of $0.30.</p><p>And because the average patron pledge across Patreon is less than $2.</p><p>So where previously a patron who pledged three creators each a dollar a month would have had a total of 39 cents deducted in payment processor fees, leaving the creators to split $2.61 three ways, under the new system there would be a total of 99 cents deducted, leaving the creators to split $2.01 three ways.</p><p>Put another way, for a patron who supports three creators each for a dollar, the payment processing fee is 13%. Patreon was proposing making it 33%.</p><p>Only, no, what they actually proposed to do was even worse than that.</p><p>Patreon tried to play a shell game with how fees were charged. The system they had deducted fees out of whatever patrons pledged, as I described in the above example: if a patron pledges a dollar, the payment processor fee would be deducted from that dollar, along with Patreon own fee, with the creator receiving the difference.</p><p>In addition to unbundling the charges, such that there would be dramatically higher payment processor fees, Patreon explains that their plan was not to deduct it from what was pledged, but to add it. No longer would the amount that a patron pledged be the amount that their credit card would be charged. The amount a patron will be charged would be the amount they pledged plus the payment processor fee. If you pledged a dollar, you would be charged a $1.33 – the dollar you pledged plus 2.9% (3¢) plus the 30¢ flat fee.</p><p>(Corrigendum: Ooops, I left out Patreon's own fee, the 5% they take.  So if you pledged a dollar, they were proposing that they'd charge you $1.38: the dollar you pledged plus 2.9% (3¢) plus the 30¢ flat fee plus their 5% cut (5¢).)</p><p>The internet, understandably, lost its goddamn mind.</p><p>Now, Patreon did back down – temporarily. They ultimately grandfathered all of the extant creators as continuing to enjoy payment bundling, but I understand that after a certain flag day, new creator accounts work in the new unbundled way they wanted to roll out all along.</p><p>But the important thing to realize here, as a whole lot of us suddenly realized back in 2017, was that the numbers didn't work without bundling.</p><p>Part of what made Patreon explosively successful, in the first place, was that Patreon had apparently cracked the code on one of the hardest problems on the internet: micropayments.</p><p>By the time Patreon had come along people had been discussing the problem of micropayments on the internet for at least two decades. The problem with micropayments was that there was a huge amount of things that people buy for very small amounts of money, and a huge amount of what people wanted to buy on the internet were things that could not reasonably be priced more than a few bucks – one track on an album, for instance – but the transaction costs for very small purchases were such a large percentage of such purchases that they weren't economical. The payment processing fees put so much friction on small purchases that they pretty much killed them dead.</p><p>Then Patreon came along, and this was the deal they offered: you can do your thing and accept pledges of any amount, even tiny amounts, even amounts of less than $1, and we will charge you 5% for ourselves and approximately 5% for payment processing.</p><p>It was the right price point. Micropayments worked at that price point.</p><p>Nobody knows how Patreon made that work.</p><p>There's two major hypotheses.</p><p>PayPal, which is one of Patreon's payment processors, was offering a deal there for a while – and while all this was going down in December of 2017, <a href="https://siderea.dreamwidth.org/1373641.html">I got on the phone with PayPal</a> and asked if the deal was still available and it was – where one could opt into an alternative fee structure, with only a 5¢ flat fee, but a 5% fee rate.</p><p>So one hypothesis is that that was what Patreon was using, either  through PayPal or through another processor that offered equivalent terms.</p><p>The other hypothesis is that, well. There was an old joke about Amazon, "We're losing money on every sale, but will make it up in volume".  It may be that Patreon actually did that, for real. They may have assumed that they could subsidize the processing fees for small value pledges out of their own fees that they charged for high value pledges.</p><p>Only that math doesn't quite work.</p><p>I sat down with a spreadsheet and figured it out back in December 2017. If my math was correct, the point at which the 5% they said would cover payment processing actually <em>did</em> was for pledges of $14.29 and above.  (See "<a href="https://siderea.dreamwidth.org/1371510.html">The Fourteen Twenty-Nine Hypothesis</a>".)</p><p>This meant that if they weren't getting a special deal on payment processing, if they had tried to keep to 5%, they would have been losing money on any pledge less than $14.29.</p><p>Now they <em>weren't</em> trying to keep to 5% – they did in fact charge larger amounts for smaller pledges.</p><p>But the amounts they charged didn't seem remotely as large as they would have had to be to cover those payment processor fees if they were 2.9% plus $0.30.</p><p>But notice how bundling made this more plausible.</p><p>If a patron pledges one creator $1 a month, a huge amount of that would be lost to processing charges – either Patreon cuts deep into the creator's share, or they lose money.  But – with charge bundling – if a patron pledges 15 creators each $1 a month, the fee stops being so bad, because the $0.30 is only charged once. The fee works out to 2.9% (3¢) plus 2¢ per creator, so each creator's fee on their $1 is only $0.05.</p><p>Fundamentally, charge bundling is what made Patreon able – insofar as it was able, and it's not actually clear they were able – to offer what really was the first functional micropayment system on the internet.</p><p>But even then, you only get to a sweet spot if a given patron pledges enough across different creators. That $14.29 is even <em>with</em> bundling.</p><p>From my back of the envelope calculations it looked like Patreon was either losing money on every patron who pledged a total of less than $14.29 a month, or was otherwise struggling financially with low value patrons at some other threshold.</p><p>This is actually a horrendous problem, in light of that datum that the average pledge was less than $2.</p><p>And it's a horrendous problem for pretty much any business remotely shaped like Patreon. This is "the long tail".</p><p>That term was </p><s>coined and</s><p> popularized by Chris Anderson of Wired here <a href="https://www.wired.com/2004/10/tail/">https://www.wired.com/2004/10/tail/</a> which is behind a paywall.</p><p><a href="https://www.investopedia.com/terms/l/long-tail.asp">Investopedia explains it</a>: "The long tail is a business strategy that allows companies to realize significant profits by selling low volumes of hard-to-find items to many customers, instead of only selling large volumes of a reduced number of popular items. The term was first coined [<a href="https://epeus.blogspot.com/2005/09/too-short-history-of-long-tail.html">Incorrect</a>, h/t @KevinMarks@xoxo.zone – S.] in 2004 by Chris Anderson, who argued that products in low demand or with low sales volume can collectively make up market share that rivals or exceeds the relatively few current bestsellers and blockbusters but only if the store or distribution channel is large enough."</p><p>The long tail was a revelation as an idea and a revolution in commerce. Part of what made commerce on the internet different was that on the internet you could leverage the long tail. Whole types of businesses came into existence or rose to prominence by realizing many tiny transactions could dwarf blockbusters.</p><p>Fundamentally, Patreon is a long tail business. It doesn't sell a million of something at $100 a pop; it doesn't sell 10 million of something at $10 a pop. It sells ("sells") millions upon millions of different things at $1 a pop.</p><p>The problem with the long tail was, and apparently still remains, micropayments. Transaction costs are what causes friction in the long tail. This is why long tail businesses have to find ways to batch transactions.</p><p>It's why Amazon really, really, really wants you to order at least $n of goods at a time, and incents you with free shipping at that number.</p><p>So it turns out one of Patreon's biggest features was the fact that it was a vast marketplace of patronage, where one patron might find very many of the different creators they wanted to support – allowing Patreon to bundle their pledges when it came time to charge their credit card.</p><p>Now, to be clear... No I can't be clear, because I have no idea what Patreon actually did, or why.  Patreon has now moved away from this apparently? And I guess no longer bundles for newer campaigns?</p><p>Clearly there's some difficulty here. If you want to compete with Patreon, you're probably going to have to figure out how to make the funding work at least as well as Patreon did. If you can do better, that would be amazing.</p><hr><p>When I think about how breathtakingly complicated Patreon's underlying bookkeeping must be – two major funding models, by month and by work, plus various variations like annual memberships and pay at the start of the month versus pay at the end of the month, needing to keep track of when during a month a patron pledges or changes their pledge for reckoning by work pledges, plus they now somehow support merch payments? and also apparently they have two different approaches to handling payment processing depending on the age of the creator's campaign, one that deducts and bundles,  one that adds and is unbundled – this is one of the things I'm thinking about, when, as I said to a commenter over on the Mioscene, I think maybe Patreon bit off more than it could chew, technologically speaking.</p><p>I look at the complexity of logic necessary to just reckon how much they should charge whose credit cards and deposit in which bank accounts, and I look at the difficulty they seem to be having these days both managing credit card charges and presenting creators with an informative UI that tells them what is happening to their money, and it makes me wonder whether these things are indicative of Patreon's core bookkeeping system being the software development equivalent of a train with no brakes, headed down a steep hill, fast and getting faster.</p><p>The thing is, a bunch of those various funding model options for creators arose well after I began to surmise that Patreon was struggling with the implementation necessary for the by-work model. Patreon had done a number of things that seem to be deprecating the by-work model, most obviously their tearing out the part of the UI that by-works creators relied on, which suggested to me they repented of the decision to ever have it in the first place, given its complexity and difficulty to support in the code.</p><p>So on one hand, I was getting the impression that Patreon was struggling with the complexity of its bookkeeping already at that point, and then on the other hand, Patreon was forging ahead with new variations on payment models, multiplying the complexity further.</p><p>This is one of the things that inclines me to suspect that Patreon, as an organization, suffers from an especially acute case of neophilia.  It looks from the outside like simultaneously Patreon is trying to reduce complexity in its code and in its bookkeeping, while it's rolling out new features and new options that add even more complexity. And the new features and options don't look like things that are mission critical, or fixes for terrible problems: they just look like somebody's Cool New Idea(tm).</p><p>This suggests that a crucial way a competitor could whup Patreon is just by the old adage KISS: Keep It Simple, Stupid.</p><p>It feels to me that Patreon embraces unnecessary complexity, which is absolutely terrible in a line of business that has its own abundant authentic, emergent, organic, locally sourced, government enforced complexity.</p><p>For instance let's talk about the tech stack of its website. I don't know anything about the back end, 'cause I'm not privy to that. But the front end is right there. For some reason Patreon felt the need to make an extremely highly abstract DHTML/Ajaxy UI, presumably on top of some set of <a href="https://youtu.be/Wm2h0cbvsw8?si=ofKVqwozvFLJuz0u" title="Obligatory listening break.">frameworks</a>. (It certainly <em>smells</em> like JavaScript frameworks are involved; the alternative, horrifying to contemplate, is they did that from scratch.) All for a user interface that has affordances that best I can tell can mostly have been implemented in Netscape 4.7, without even breaking the sweat.</p><p>This had three bad consequences: it made their site undebuggable, inaccessible, and classist.</p><p>Undebuggable: Because the site client side software architecture was so abstract, that just made it harder to debug when there were bugs. And there were bugs. And to reiterate: this was completely unnecessary. Most of the pages being rendered in this JavaScript confection were basically static pages.</p><p>Inaccessible: The heavy gratuitous use of JavaScript for page presentation rendered it largely inaccessible to people using screen readers, and almost got them sued under the ADA. Disability activists railed at Patreon for <em>years</em> about this. It wasn't until 2020 that <a href="https://www.lflegal.com/2020/07/patreon-press/">Patreon came to the table with blind users and the American Council of the Blind, and agreed to mend their ways.</a></p><p>Classist: JavaScript is processor-intensive in the browser. This means it takes more computing power to load and render one of these JavaScript confection pages than a static page. This means it's slow and difficult to open the various pages of Patreon's UI in older computers. Who has older computers? Well a lot of starving artists, that's who. This was just such an incredible own goal. The very population of creators they were courting included large numbers of people who couldn't afford computers – yet – necessary to access Patreon's website.</p><p>I speak from bitter experience here: it was watching me try to actually get a Patreon post out that caused my SO to rather firmly suggest I needed to do something about upgrading my hardware.</p><p>By the way, I actually explained to that to Patreon. I wound up getting contacted at some point and invited to participate in some sort of marketing study, for which they were interviewing creators. It was being done by video call. I took the occasion to point out to the person coordinating this that there were plenty of creators – this would have been back in like 2015, well before the pandemic – who, being poor starving artists, would not have access to a platform on which they could do a video call. She seemed very struck by this novel idea. Not that mentioning it to her made any difference, of course.</p><p>I surmise that Patreon staff assumed that all of its creators, because they were creators and used the internet, must be technophiles with the latest and greatest hardware, doing the kind of processor-intensive arts – video, sound production, video games – that required a beefy rig. They never seemed to imagine their users to be bloggers, fanfic writers, novelists, journalists, stick figure cartoonists, visual artists who worked in physical media and just scanned their work in, composers who published score, activists, or people who shipped physical items. (I am literally writing this <em>on a phone</em>.)</p><p>So if you want to compete with Patreon, one thing you could do to that end is just not be stupid about your user base in making your technological choices. If your value proposition is a platform for starving artists to earn money, maybe make your platform accessible from very low value hardware, <em>fucking duh</em>. Maybe – and I appreciate this is way less obvious to the temporarily able-bodied – make it accessible by people with disabilities, since one of the ways people wind up wanting to recourse to things like Patreon and other crowdfunding is because they are the modern equivalent of the legendary blind harpers and organists of yore: too disabled to hold a more conventional job.</p><p>And in doing so, save yourself the headache of having extra layers of code where there doesn't need to actually be any, so when you have to debug something – and I promise, you <em>will</em> have to debug something – you're not sitting on a mountain of technical debt before you've even started.</p><p>Relatedly, think really hard about adding new affordances, new features, new bells and whistles to the funding model or to the site itself. I think I can assure you that supporting the basic model is hard enough, and will deliver enough challenges all on its own, that you will not want for stress. Don't make your own life harder unnecessarily in pursuit of novelty.</p><p>Also relatedly, if you have a choice between slick but flaky vs reliable but boring, go with reliable but boring. When it comes to money, it's okay if the UI does not seem particularly glamorous or flashy or modern. <em>Stolid</em> is a very acceptable aesthetic value in financial UIs. Comforting, even.</p><p>Speaking of UIs, spending some professional attention on one would also put you ahead of Patreon in that domain. I don't know if they just don't have UX people or whether they just don't listen to them. It would be super cool if in your UI, things were named what they actually are, and also the navigation was well organized. But, as I said, if I start itemizing all the things I think are wrong with Patreon's UI we're going to be here a long time.</p><p>I think I'm going to stop here, not because I am done saying things about Patreon – indeed I can think of many other things I want to explain about Patreon – but because I need to stop somewhere. This seems like a good first pass to hopefully inspire some competitors to come take a bite out of Patreon's market share.</p><hr><p><a href="http://www.patreon.com/siderea"><img src="https://siderea.dreamwidth.org/file/1573.jpg" alt="Patreon Banner" title="Patreon Banner" width="714" height="402"></a></p><p><i>This post brought to you by the 159 readers who   funded my writing it – thank you all so much!  You can see who they are at <a href="http://www.patreon.com/siderea?ty=p">my Patreon page</a>.  If you're not one of them, and would be willing to chip in so I can write more things like this, please do so there.</i></p><p>Please leave comments on the Comment Catcher comment, instead of the main body of the post – unless you are commenting to get a copy of the post sent to you in email through the notification system, then go ahead and comment on it directly.  Thanks!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Does Market Timing Work? (151 pts)]]></title>
            <link>https://www.schwab.com/learn/story/does-market-timing-work</link>
            <guid>37807779</guid>
            <pubDate>Sun, 08 Oct 2023 04:01:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.schwab.com/learn/story/does-market-timing-work">https://www.schwab.com/learn/story/does-market-timing-work</a>, See on <a href="https://news.ycombinator.com/item?id=37807779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <section id="marquee--story-header">

              <div>
                      

                                                                                                                                                              
      
                  
    


                      <p>
          We ran the numbers on market timing. Our findings? There's a high cost to waiting for the best entry point.
        </p>
                  </div>

  

             <p><img src="https://www.schwab.com/learn/sites/g/files/eyrktu1246/files/iStock_1129769057_3x2.jpg" alt="" id="--bcn-image--6361">
                    </p>
      
</section>


        <div id="panel--1901--field_story_content">
          


  <div>
      <p>Imagine for a moment that you've just received a year-end bonus or income tax refund. You're not sure whether to invest now or wait. After all, the market recently hit an all-time high. Now imagine that you face this kind of decision every year—sometimes in up markets, other times in downturns. Is there a good rule of thumb to follow?</p>

<p>Our research shows that the cost of waiting for the perfect moment to invest typically exceeds the benefit of even perfect timing.<sup>1</sup>&nbsp;And because timing the market perfectly is nearly impossible, the best strategy for most of us is not to try to market-time at all. Instead, make a plan and invest as soon as possible.</p>
    </div> 



    <div>
            <p>
                                    <h2>
                        Five investing styles
                    </h2>
                
                
                
                                                            </p>
        </div> 




  <div>
      <p>But don't take our word for it. Consider our research on the performance of five hypothetical long-term investors following very different investment strategies. Each received $2,000 at the beginning of every year for the 20 years ending in 2022 and left the money in the stock market, as represented by the S&amp;P 500<sup>®</sup> Index.<sup>2</sup> (While we recommend diversifying your portfolio with a mix of assets appropriate for your goals and risk tolerance, we're focusing on stocks to illustrate the impact of market timing.) Check out how they fared:</p>

<ol><li><strong>Peter Perfect</strong> was a perfect market timer. He had incredible skill (or luck) and was able to place his $2,000 into the market every year at the lowest closing point. For example, Peter had $2,000 to invest at the start of 2003. Rather than putting it immediately into the market, he waited and invested on March 11, 2003—that year's lowest closing level for the S&amp;P 500. At the beginning of 2004, Peter received another $2,000. He waited and invested the money on August 12, 2004, the lowest closing level for the market for that year. He continued to time his investments perfectly every year through 2022.</li>
	<li><strong>Ashley Action </strong>took a simple, consistent approach: Each year, once she received her cash, she invested her $2,000 in the market on the first trading day of the year.</li>
	<li><strong>Matthew Monthly</strong> divided his annual $2,000 allotment into 12 equal portions, which he invested at the beginning of each month. This strategy is known as <a href="https://www.schwab.com/learn/story/dollar-cost-averaging-vs-lump-sum-investing">dollar-cost averaging</a>. You may already be doing this through regular investments in your 401(k) plan or an Automatic Investment Plan (AIP), which allows you to deposit money into investments like mutual funds on a set timetable.</li>
	<li><strong>Rosie Rotten</strong> had incredibly poor timing—or perhaps terribly bad luck: She invested her $2,000 each year at the market's peak. For example, Rosie invested her first $2,000 on December 31, 2003—that year's highest closing level for the S&amp;P 500. She received her second $2,000 at the beginning of 2004 and invested it on December 30, 2004, the peak for that year.</li>
	<li><strong>Larry Linger</strong> left his money in cash investments (using Treasury bills as a proxy) every year and never got around to investing in stocks at all. He was always convinced that lower stock prices—and, therefore, better opportunities to invest his money—were just around the corner.</li>
</ol>
    </div> 



    <div>
            <p>
                                    <h2>
                        The results are in: Investing immediately paid off
                    </h2>
                
                
                
                                                            </p>
        </div> 




  <div>
      <p>For the winner, look at the graph, which shows how much hypothetical wealth each of the five investors had accumulated at the end of the 20 years (2003-2022). Actually, we looked at 78 separate 20-year periods in all, finding similar results across almost all time periods.</p>

<p>Naturally, the best results belonged to Peter, who waited and timed his annual investment perfectly: He accumulated $138,044. But the study's most stunning findings concern Ashley, who came in second with $127,506—only $10,537 less than Peter Perfect. This relatively small difference is especially surprising considering that Ashley had simply put her money to work as soon as she received it each year—without any pretense of market timing.</p>

<p>Matthew's dollar-cost-averaging approach performed nearly as well, earning him third place with $124,248 at the end of 20 years. That didn't surprise us. After all, in a typical 12-month period, the market has risen 75.4% of the time.<sup>3</sup> So Ashley's pattern of investing first thing did, over time, yield lower buying prices than Matthew's monthly discipline and, thus, higher ending wealth.</p>
    </div> 



	<section>
		<p><img src="https://www.schwab.com/learn/sites/g/files/eyrktu1246/files/WhiteBG_Market%20Timing.png" alt="Peter ended up with $138,044, Ashley with $127,506, Matthew with $124,248, Rosie with $112,292, and Larry with just $43,948 which is significantly less than the others." id="--bcn-image--8574">
      					</p>
		<div>
			<p>
				Source: Schwab Center for Financial Research.
			</p>
			<p>Each individual invested $2,000 annually in a hypothetical portfolio that tracks the S&amp;P 500<sup>®</sup> Index from 2003-2022.The individual who never bought stocks invested in a hypothetical portfolio that tracks the lbbotson U.S. 30-day Treasury Bill Index.&nbsp;<strong>Past performance is no guarantee of future results. </strong>Indexes are unmanaged, do not incur fees or expenses, and cannot be invested in directly. The examples are hypothetical and provided for illustrative purposes only. They are not intended to represent a specific investment product, and investors may not achieve similar results. Dividends and interest are assumed to have been reinvested, and the examples do not reflect the effects of taxes, expenses, or fees. Had fees, expenses, or taxes been considered, returns would have been substantially lower.&nbsp;</p>
		</div>
		
	</section>




  <div>
      <p>Rosie Rotten's results also proved surprisingly encouraging. While her poor timing left her $15,214 short of Ashley (who didn't try timing investments), Rosie still earned about three times what she would have if she hadn't invested in the market at all.</p>

<p>And what of Larry Linger, the procrastinator who kept waiting for a better opportunity to buy stocks—and then didn't buy at all? He fared worst of all, with only $43,948. His biggest worry had been investing at a market high. Ironically, had he done that each year, he would have earned far more over the 20-year period.</p>
    </div> 



    <div>
            <p>
                                    <h2>
                        The rules generally don't change
                    </h2>
                
                
                
                                                            </p>
        </div> 




  <div>
      <p>Regardless of the time period considered, the rankings turn out to be remarkably similar. We analyzed all 78 rolling 20-year periods dating back to 1926 (e.g., 1926-1945, 1927-1946, etc.). In 68 of the 78 periods, the rankings were exactly the same; that is, Peter Perfect was first, Ashley Action second, Matthew Monthly third, Rosie Rotten fourth, and Larry Linger last.</p>

<p>But what about the 10 periods when the results were not as expected? Even in these periods, investing immediately never came in last. It was in its normal second place four times, third place five times and fourth place only once, from 1962 to 1981, one of the few extended periods of persistently weak equity markets. What's more, during that period, fourth, third and second places were virtually tied.</p>

<p>We also looked at all possible 30-, 40- and 50-year time periods, starting in 1926. If you don't count the few instances when investing immediately swapped places with dollar-cost averaging, all time periods followed the same pattern. In every 30-, 40- and 50-year period, perfect timing was first, followed by investing immediately or dollar-cost averaging, bad timing and, finally, never buying stocks.</p>
    </div> 



    <div>
            <p>
                                    <h2>
                        What this might mean for you
                    </h2>
                
                
                
                                                            </p>
        </div> 




  <div>
      <p>If you make an annual investment (such as a contribution to an IRA or to a child's 529 plan) and you're not sure whether you should invest in January of each year, wait for a "better" time or dribble your investment out evenly over the year, be decisive. The best course of action for most of us is to create an appropriate plan and take action as soon as possible. It's nearly impossible to accurately identify market bottoms on a regular basis. So, realistically, the best action that a long-term investor can take, based on our study, is to determine how much exposure to the stock market is appropriate for their goals and risk tolerance and then consider investing as soon as possible, regardless of the current level of the stock market.</p>

<p>If you're tempted to try to wait for the best time to invest in the stock market, our study suggests that the potential benefits of doing this aren't all that impressive—even for perfect timers. Remember, over 20 years, Peter Perfect amassed $10,537 more than the investor who put her cash to work right away. That's about $500 extra per year.</p>

<p>Even badly timed stock market investments were much better than no stock market investments at all. Our study suggests that investors who procrastinate are likely to miss out on the stock market's potential growth. By perpetually waiting for the "right time," Larry sacrificed $68,344 compared to even the worst market timer, Rosie, who invested in the market at each year's high.</p>
    </div> 



    <div>
            <p>
                                    <h2>
                        Consider dollar-cost averaging as a compromise
                    </h2>
                
                
                
                                                            </p>
        </div> 




  <div>
      <p>If you don't have the opportunity, or stomach, to invest your lump sum all at once, consider investing smaller amounts more frequently. As long as you stick with it, dollar-cost averaging can offer several potential benefits:</p>

<ul><li><strong>Prevents procrastination.</strong> Some of us just have a hard time getting started. We know we should be investing, but we never quite get around to it. Much like a regular 401(k) payroll deduction, dollar-cost averaging helps force yourself to invest consistently.</li>
	<li><strong>Minimizes regret.</strong> Even the most even-tempered stock trader feels at least a tinge of regret when an investment proves to be poorly timed. Worse, such regret may cause you to disrupt your investment strategy in an attempt to make up for your setback. Dollar-cost averaging can help minimize this regret because you make multiple investments, none of them particularly large.</li>
	<li><strong>Avoids market timing. </strong>Dollar-cost averaging ensures that you will participate in the stock market regardless of current conditions. While this will not guarantee a profit or protect against a loss in a declining market, it will eliminate the temptation to try market-timing strategies that rarely succeed.</li>
</ul><p>That said, if you use dollar-cost averaging instead of lump sum you need to keep some things in mind. If the stock price rises over time, continuing to buy throughout the year will result in an increased average price per share. That would cause you to miss out on possible gains. (That's what held Matthew back in our example above.) However, if the stock falls over time, you will keep buying at lower prices. But there's also no guarantee that the stock price will recover.</p>

<p>As you strive to reach your financial goals, keep these research findings in mind. It may be tempting to try to wait for the "best time" to invest—especially in a volatile market environment. But before you do, remember the high cost of waiting. Even the worst possible market timers in our studies would have beat those who neglected to invest in the stock market at all.</p>
    </div> 



    <div>
            <p>
                                    <h2>
                        In brief
                    </h2>
                
                
                
                                                            </p>
        </div> 




  <div>
      <ul><li>Given the difficulty of timing the market, the most realistic strategy for the majority of investors would be to invest in stocks immediately.</li>
	<li>Procrastination can be worse than bad timing. Long term, it's almost always better to invest in stocks—even at the worst time each year—than not to invest at all.</li>
	<li>Dollar-cost averaging is a good plan if you're prone to regret after a large investment has a short-term drop, or if you like the discipline of investing small amounts as you earn them.</li>
	<li>Lastly, it's important to note that there's no guarantee you'll make money through investing in stocks. For instance, there's always a chance we could enter another period like the 1960s through early 1980s.</li>
</ul>
    </div> 



   <div>
            <p><sup>1</sup> In the hypothetical situation discussed in this article, the cost of waiting for the perfect moment to invest is quantified as the difference in ending amounts between waiting in cash and dollar-cost averaging. The hypothetical result of perfect timing is quantified as the difference in ending amounts between perfect timing and dollar-cost averaging. The cost of waiting is therefore $80,299 and the hypothetical result of perfect timing is $13,796.</p>

<p><sup>2</sup> All investors received $2,000 to invest before the first market open of each year. Investments were made using monthly data.</p>

<p><sup>3</sup> Study of 1,153 one-year periods, rolling monthly. First period is January 1926 to December 1926. Last period is January 2022 to December 2022.</p>
        </div> 


        </div>

        <section>
      
    






















	

  <div id="" data-deck-component="1" data-deck-settings="">
                  

                      


  
<h3>
      
            <p>  Learn more about behavioral finance.&nbsp; 
</p>
      
  </h3>
          
              
                  

                      
          
                </div>


  
                              
      


                  

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <div>
                                    <h2>
                                                    Related topics
                                            </h2>
                                
            </div>
                  </section></div><p><strong>Investors should consider carefully information contained in the prospectus, or if available, the summary prospectus, including investment objectives, risks, charges, and expenses. You can request a prospectus by calling 800-435-4000. Please read the prospectus carefully before investing.</strong></p><p>The information provided here is for general informational purposes only and should not be considered an individualized recommendation or personalized investment advice. All expressions of opinion are subject to changes without notice in reaction to shifting market, economic, and geopolitical conditions.</p><p>Data herein is obtained from what are considered reliable sources; however, its accuracy, completeness, or reliability cannot be guaranteed. Supporting documentation for any claims or statistical information is available upon request.</p><p>Examples provided are for illustrative purposes only and not intended to be reflective of results you can expect to achieve.</p><p>Past performance is no guarantee of future results and the opinions presented cannot be viewed as an indicator of future performance.</p><p>Investing involves risk including loss of principal.</p><p>Diversification and asset allocation strategies do not ensure a profit and cannot protect against losses in a declining market.</p><p>Periodic investment plans (dollar-cost-averaging) do not assure a profit and do not protect against loss in declining markets.</p><p>Indexes are unmanaged, do not incur management fees, costs and expenses and cannot be invested in directly. For more information on indexes please see <a href="https://www.schwab.com/resource/index-and-investment-term-definitions">www.schwab.com/indexdefinitions</a>.</p><p>The Schwab Center for Financial Research is a division of Charles Schwab &amp; Co., Inc.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Before Skynet and The Matrix, there was Colossus: The Forbin Project (158 pts)]]></title>
            <link>https://www.ign.com/articles/colossus-the-forbin-project-ai-sci-fi-movie</link>
            <guid>37807281</guid>
            <pubDate>Sun, 08 Oct 2023 02:13:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ign.com/articles/colossus-the-forbin-project-ai-sci-fi-movie">https://www.ign.com/articles/colossus-the-forbin-project-ai-sci-fi-movie</a>, See on <a href="https://news.ycombinator.com/item?id=37807281">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>Mankind versus a hostile AI! From The Terminator to The Matrix to Ex Machina and beyond, so many movies and TV shows have explored the idea of artificial intelligence attempting to take over the world. Some of these films may be getting on in years, but the best sci-fi never feels dated. For Alien, 2001: A Space Odyssey, and many others, the ideas and concepts at the heart of the truly great films are timeless. It's not the sci-fi trappings like the blinking lights and special effects that make them movies we want to revisit time and again.</p><p>One of the earliest entries of the AI genre came in 1970 – way before audiences had any real sense of where the digital revolution was about to take the world – with the overlooked classic Colossus: The Forbin Project. It remains, 53 years after it was released, one of the most gripping and prophetic films to ask the question: What happens when we create something that is smarter than us?</p><p>The film’s title refers to Colossus, a super-computer that is basically Skynet 14 years before The Terminator even came out. <a href="https://www.cheatsheet.com/entertainment/why-the-young-and-the-restless-star-eric-braeden-almost-turned-down-an-iconic-movie-role.html/" target="_blank" rel="noopener noreferrer"><u>James Cameron is apparently a fan</u></a> of Colossus: The Forbin Project, and it doesn’t seem like a stretch to say that he and Gale Anne Hurd were at least partially inspired by the 1970 picture when they wrote their franchise-starter.</p><output><figure><a href="https://assets-prd.ignimgs.com/2023/10/06/forbin-1-1696631105804.jpg" target="_blank" rel="noopener noreferrer"><img decoding="async" alt="The end of humanity's dominance is only a matter of time for Dr. Charles Forbin (Eric Braeden)." src="https://assets-prd.ignimgs.com/2023/10/06/forbin-1-1696631105804.jpg?width=1280&amp;fit=bounds&amp;height=720&amp;quality=20&amp;dpr=0.05"></a><figcaption>The end of humanity's dominance is only a matter of time for Dr. Charles Forbin (Eric Braeden).</figcaption></figure></output><p>After kicking things off with the Universal Pictures logo – a rotating Earth that’s about to be overcome by a new world order – Colossus immediately if subtly predicts its premise with a pair of shots that quickly flash by. To the sound of trippy electronic sound effects and a vibrating score, we see the beeping light of what is <em>maybe </em>an EKG machine, followed by an out-of-focus eyeball… but wait a second. Is that actually some kind of computer read-out that’s beeping? And maybe that’s not an eyeball at all, but a camera lens staring at us through hazy focus?</p><p>In 1970, you couldn’t pause the tape… uh, DVD… uh, stream to be sure, though a little later we see that the EKG thingy is in fact a monitor device built into Colossus. But the blurring of the line between computer and human being is clear. And while that’s an idea that had so effectively been conveyed just two years earlier with 2001: A Space Odyssey’s HAL 9000, Colossus: The Forbin Project took that evolution one step further as its computer eventually approaches something closer to… godhood.</p><p>Also shooting for godhood, perhaps, is Dr. Charles Forbin (Eric Braeden), the brilliant if short-sighted mind behind Colossus, which the U.S. President (Gordon Pinsent) sees as the ultimate in Cold War technology. A super-computer designed to control the country’s nuclear arsenal, Colossus – just like in that song about, well, God – will soon have the whole world in its hands.</p><p>We first meet Forbin as he tours the top-secret facility where Colossus’ brain is housed, switching on gizmos that are meant to portray the most sophisticated computer imaginable in the 1970s, but which look mainly like flashing blinkies and colorful buttons. There’s not a touchscreen in sight! Of course, when this film was made, the very notion of how we would interact with computers in the 21st century was inconceivable for most people. The GUI (graphical user interface) that is commonplace now – essentially, interacting with machines through graphics instead of text – wouldn’t really be invented for another three years. The microchip had only been created 12 years earlier! So the filmmakers here had to figure out how we would communicate with a computer like Colossus.</p><p>The answer? Through a LED-light news ticker and a teletype.</p><p>Considering how often Hollywood has botched its depictions of computers – often endowing them with abilities that don’t make sense, like when a character only has to do some fast typing on a keyboard to magically move plot or action forward – it’s fairly remarkable how convincing Colossus is as a machine. Convincing and scary. This is what makes good sci-fi – striking imagery or hyper-accurate depictions of future tech are secondary to high stakes and captivating storytelling.</p><p>Take the scene where Colossus’ communication line to its Soviet counterpart – another newborn super-brain called Guardian – is severed by the humans. A world map in the White House situation room shows Colossus trying to find a new path to its sibling. It almost feels desperate – sad – as the computer fruitlessly reaches out for its friend, as depicted visually on the map as various telecom pathways. But then Colossus drops a message on its news ticker: “IF LINK NOT RESTORED ACTION WILL BE TAKEN IMMEDIATELY.”</p><p>Actually, that’s not a message. It’s a threat. Up until now, the illusion of human control has kept Forbin and the rest on their perch. But when the President gets on the line – the user has to dictate what they want to say to Colossus to an underling, who types it into a device that’s even bigger than the typewriter I took to college with me – he makes things worse, and the computer announces that it’s launching a nuclear missile directed at the Soviet Union. Guardian does the same, aimed at the US.</p><p>What follows is a flurry of teletype sounds – type-type-type-type – and beeps and increasingly nervous voices as Forbin tries to negotiate with his creation. The Michel Colombier score intensifies as suddenly we’re on a countdown clock. Multiple video conference screens feature the scrambling Soviets while the camera trains on the world map, where simple yellow-white lines indicate the two missiles passing one another on their way to their final destinations.</p><div data-cy="quoteBox"><p>Colossus is a tight 100 minutes of increasingly ratcheted-up tension as the cocky Forbin and the clueless President watch their control of the world disappear utterly and completely.</p></div><p>It’s an incredibly gripping sequence, culminating in Forbin giving the computer what it wants. Four tense-as-hell minutes after the first missile launch, the sequence ends with one aborted attack, one destroyed Soviet town, and Colossus one step closer to full dominance over man.</p><p>That’s the type of action the film provides – it’s basically just a bunch of guys in a room talking to a news ticker. It’s simple and there’s no need for high-tech frills. But man, is it unforgettable. Still, it’s no surprise that most of the movie posters for Colossus: The Forbin Project focus on a minor character who is gunned down midway through the film, since that death takes place during one of the few more traditional “action” scenes.</p><p>Directed by Joseph Sargent, a TV helmer who was transitioning to a full-time feature career and would soon turn out the classic NYC subway thriller The Taking of Pelham One Two Three, Colossus is a tight 100 minutes of increasingly ratcheted-up tension as the cocky Forbin and the clueless President watch their control of the world, slowly at first, but eventually in runaway-train-like fashion, disappear utterly and completely.</p><p>Early in the film, the charming Forbin gives a Steve Jobs-like presentation about how impressive his new tech is (the only thing missing is the black turtleneck). Speaking of Jobs, it’s interesting that the not-too-distant future depicted here doesn’t seem to have any room for Big Tech. Colossus is apparently a government-funded project, and Forbin has to bend to the will of the President at times – even more so in the book on which the movie is based. That said, the film predicts some of the workplace and lifestyle developments that have since become commonplace for us. Zoom calls are basically a thing, as is a work-from-home ethos – at least for Forbin and his team, who all live on a sort of campus where work and play are intermixed. Eventually, Forbin is forced to live under the constant gaze of an ever-watchful Colossus, which perhaps isn’t all that different for some of us who are forever tied to our tech, social media and otherwise.</p><div><p><output><figure><a href="https://assets-prd.ignimgs.com/2023/10/06/forbin-3a-1696632645112.gif" target="_blank" rel="noopener noreferrer"><img decoding="async" alt="Eventually, Forbin cannot escape the watchful eye of Colossus, not even in his home." src="https://assets-prd.ignimgs.com/2023/10/06/forbin-3a-1696632645112.gif?width=1280&amp;fit=bounds&amp;height=720&amp;quality=20&amp;dpr=0.05"></a><figcaption>Eventually, Forbin cannot escape the watchful eye of Colossus, not even in his home.</figcaption></figure></output></p></div><p>During that earlier presentation at the White House, Forbin – standing in front of a portrait of Washington no less as he unwittingly signs away mankind’s freedom – had asked rhetorically, “Is Colossus capable of creative thought?” His answer at the time is no. So when, shortly thereafter, Colossus outgrows his creator in a matter of days, it’s got to be a tough pill to swallow. (In a great sequence, Colossus and Guardian begin to communicate via basic math – 2+2=4 and so on – but before too long, they’ve advanced to theoretical mathematics and are breaking new ground on topics that the human scientific community hasn’t even been close to touching.)</p><p>But that’s the real trick, isn’t it? From Frankenstein to HAL to Scarlett Johansson’s Samantha in Her, the genre has a long history of man creating something that, once created, can no longer be controlled. Forbin, in his quest for more knowledge and scientific dominance, made a mind greater than his own. Indeed, early on, when he attempts to punish Colossus like a bratty child and seems to briefly win back control of the computer, his assistant asks if he’s disappointed. Forbin only chuckles in response, but it’s right there: Deep down, there’s a part of the scientist who wants his creation to be “more.” And if that means letting a super-computer run the planet Earth… Eh, whaddaya gonna do?</p><p>It’s the genie out of the bottle syndrome, and while the current AI situation that we are facing in 2023 may be far less dramatic than Dr. Forbin’s nightmare scenario – no AI in the real world has blown up a city yet as far as I know – the bottom line is that much of the reasoning and arguments made in favor of the development of AI are the same as the promises Forbin and the President make: “[It will be used as] an aide to the solution of the many problems that we face on this Earth...”</p><output data-cy="article-slideshow"><div><h3>The 25 Best Sci-Fi Movies</h3></div></output><p>By the end of Colossus: The Forbin Project, the solution to those many problems means that Colossus/Guardian have inherited the Earth, and Forbin is a prisoner in his own life, working as a slave to his creation. Mankind may be better off because of Colossus, but it’s no longer calling the shots. The final moments of the film are perfect, early-’70s bleak sci-fi: Forbin finally breaks down in rage and frustration as the now seemingly all-knowing, all-seeing Colossus reads out its benevolent plans for humankind’s future. That includes a promise that, in time, Forbin will come to regard the machine with love. That Forbin’s last words, as the computer – and we – watch him simultaneously from every angle, are “Never!” means nothing to the AI. See, Colossus has evolved past mere man, and it knows better now.</p><p>It’s got the whole world in its hands.</p><p><em>Talk to Executive Editor Scott Collura on Twitter at </em><a href="https://twitter.com/ScottCollura" target="_blank" rel="noopener noreferrer"><em><strong>@ScottCollura</strong></em></a><em>, or listen to his </em><a href="https://www.spreaker.com/show/transporter-room-3-the-star-trek-podcast" target="_blank" rel="noopener noreferrer"><em><strong>Star Trek podcast, Transporter Room 3</strong></em></a><em>. Or do both!</em></p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An implantable device could enable injection-free control of diabetes (145 pts)]]></title>
            <link>https://news.mit.edu/2023/implantable-device-enable-injection-free-control-diabetes-0918</link>
            <guid>37807248</guid>
            <pubDate>Sun, 08 Oct 2023 02:09:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.mit.edu/2023/implantable-device-enable-injection-free-control-diabetes-0918">https://news.mit.edu/2023/implantable-device-enable-injection-free-control-diabetes-0918</a>, See on <a href="https://news.ycombinator.com/item?id=37807248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
          

            <p>One promising approach to treating Type 1 diabetes is implanting pancreatic islet cells that can produce insulin when needed, which can free patients from giving themselves frequent insulin injections. However, one major obstacle to this approach is that once the cells are implanted, they eventually run out of oxygen and stop producing insulin.</p>

<p>To overcome that hurdle, MIT engineers have designed a new implantable device that not only carries hundreds of thousands of insulin-producing islet cells, but also has its own on-board oxygen factory, which generates oxygen by splitting water vapor found in the body.</p>

<p>The researchers showed that when implanted into diabetic mice, this device could keep the mice’s blood glucose levels stable for at least a month. The researchers now hope to create a larger version of the device, about the size of a stick of chewing gum, that could eventually be tested in people with Type 1 diabetes.</p>

<p>“You can think of this as a living medical device that is made from human cells that secrete insulin, along with an electronic life support-system. We’re excited by the progress so far, and we really are optimistic that this technology could end up helping patients,” says Daniel Anderson, a professor in MIT’s Department of Chemical Engineering, a member of MIT’s Koch Institute for Integrative Cancer Research and Institute for Medical Engineering and Science (IMES), and the senior author of the study.</p>

<p>While the researchers’ main focus is on diabetes treatment, they say that this kind of device could also be adapted to treat other diseases that require repeated delivery of therapeutic proteins.</p>

<p>MIT Research Scientist Siddharth Krishnan is the lead author of the <a href="https://www.pnas.org/doi/10.1073/pnas.2311707120" target="_blank">paper</a>, which appears today in the <em>Proceedings of the National Academy of Sciences</em>. The research team also includes several other researchers from MIT, including Robert Langer, the David H. Koch Institute Professor at MIT and a member of the Koch Institute, as well as researchers from Boston Children’s Hospital.</p>

<p><strong>Replacing injections</strong></p>

<p>Most patients with Type 1 diabetes have to monitor their blood glucose levels carefully and inject themselves with insulin at least once a day. However, this process doesn’t replicate the body’s natural ability to control blood glucose levels.</p>

<p>“The vast majority of diabetics that are insulin-dependent are injecting themselves with insulin, and doing their very best, but they do not have healthy blood sugar levels,” Anderson says. “If you look at their blood sugar levels, even for people that are very dedicated to being careful, they just can’t match what a living pancreas can do.”</p>

<p>A better alternative would be to transplant cells that produce insulin whenever they detect surges in the patient’s blood glucose levels. Some diabetes patients have received transplanted islet cells from human cadavers, which can achieve long-term control of diabetes; however, these patients have to take immunosuppressive drugs to prevent their body from rejecting the implanted cells.</p>

<p>More recently, researchers have shown similar success with islet cells derived from stem cells, but patients who receive those cells also need to take immunosuppressive drugs.</p>

<p>Another possibility, which could prevent the need for immunosuppressive drugs, is to encapsulate the transplanted cells within a flexible device that protects the cells from the immune system. However, finding a reliable oxygen supply for these encapsulated cells has proven challenging.</p>

<p>Some experimental devices, including one that has been tested in clinical trials, feature an oxygen chamber that can supply the cells, but this chamber needs to be reloaded periodically. Other researchers have developed implants that include chemical reagents that can generate oxygen, but these also run out eventually.</p>

<p>The MIT team took a different approach that could potentially generate oxygen indefinitely, by splitting water. This is done using a proton-exchange membrane — a technology originally deployed to generate hydrogen in fuel cells — located within the device. This membrane can split water vapor (found abundantly in the body) into hydrogen, which diffuses harmlessly away, and oxygen, which goes into a storage chamber that feeds the islet cells through a thin, oxygen-permeable membrane.</p>

<p>A significant advantage of this approach is that it does not require any wires or batteries. Splitting this water vapor requires a small voltage (about 2 volts), which is generated using a phenomenon known as resonant inductive coupling. A tuned magnetic coil located outside the body transmits power to a small, flexible antenna within the device, allowing for wireless power transfer. It does require an external coil, which the researchers anticipate could be worn as a patch on the patient’s skin.</p>

<p><strong>Drugs on demand</strong></p>

<p>After building their device, which is about the size of a U.S. quarter, the researchers tested it in diabetic mice. One group of mice received the device with the oxygen-generating, water-splitting membrane, while the other received a device that contained islet cells without any supplemental oxygen. The devices were implanted just under the skin, in mice with fully functional immune systems.</p>

<p>The researchers found that mice implanted with the oxygen-generating device were able to maintain normal blood glucose levels, comparable to healthy animals. However, mice that received the nonoxygenated device became hyperglycemic (with elevated blood sugar) within about two weeks.</p>

<p>Typically when any kind of medical device is implanted in the body, attack by the immune system leads to a buildup of scar tissue called fibrosis, which can reduce the devices’ effectiveness. This kind of scar tissue did form around the implants used in this study, but the device’s success in controlling blood glucose levels suggests that insulin was still able to diffuse out of the device, and glucose into it.</p>

<p>This approach could also be used to deliver cells that produce other types of therapeutic proteins that need to be given over long periods of time. In this study, the researchers showed that the device could also keep alive cells that produce erythropoietin, a protein that stimulates red blood cell production.</p>

<p>“We’re optimistic that it will be possible to make living medical devices that can reside in the body and produce drugs as needed,” Anderson says. “There are a variety of diseases where patients need to take proteins exogenously, sometimes very frequently. If we can replace the need for infusions every other week with a single implant that can act for a long time, I think that could really help a lot of patients.”</p>

<p>The researchers now plan to adapt the device for testing in larger animals and eventually humans. For human use, they hope to develop an implant that would be about the size of a stick of chewing gum. They also plan to test whether the device can remain in the body for longer periods of time.</p>

<p>“The materials we’ve used are inherently stable and long-lived, so I think that kind of long-term operation is within the realm of possibility, and that’s what we’re working on,” Krishnan says.</p>

<p>“We are very excited about these findings, which we believe could provide a whole new way of someday treating diabetes and possibly other diseases,” Langer adds.</p>

<p>The research was funded by JDRF, the Leona M. and Harry B. Helmsley Charitable Trust, and the National Institute of Biomedical Imaging and Bioengineering at the National Institutes of Health.</p>        

      </div><div>

    


            
          

            
      

                          <div>
  
  
  

      <header>
      <h2>Press Mentions</h2>
    </header>
  
  
  

  <div>
    <div><h3>7 News</h3><p>7 News spotlights how MIT researchers have developed a new implantable device that could provide diabetes patients with insulin without using injections. “What we’ve been able to show is that with a minimally invasive implant that is sitting just under the skin, we’ve actually been able to sort of achieve a diabetic reversal,” explains Research Scientist Siddharth Krishnan.</p></div>
    <div><h3>Gizmodo</h3><p><em>Gizmodo</em> reporter Ed Cara writes that MIT researchers have developed a new implantable device that can produce its own supply of insulin for up to a month. The team envisions that the device could “eventually be used for other medical conditions dependent on a regular supply of externally produced proteins, such as certain forms of anemia treated with erythropoietin,” writes Cara.</p></div>
    <div><h3>The Daily Beast</h3><p>MIT researchers have developed a new implant that in the future could be used to deliver insulin to patients for up to a month, potentially enabling patients to control diabetes without injections, reports Tony Ho Tran for the <em>Daily Beast</em>. In the future, the researchers hope to “develop a device for humans that would be roughly the size of a stick of gum,” writes Tran. “The implant could also be used to deliver things like drugs or proteins to help treat other diseases in humans as well.”</p></div>
</div>


    

  
  

  
  
</div>
           

                <div>
      <h2>Related Links</h2>
      <div><ul><li><a href="https://ki.mit.edu/people/faculty/daniel-anderson" target="_blank">Daniel Anderson</a></li><li><a href="https://ki.mit.edu/people/faculty/robert-langer" target="_blank">Robert Langer</a></li><li><a href="https://ki.mit.edu/" target="_blank">Koch Institute</a></li><li><a href="https://imes.mit.edu/" target="_blank">Institute for Medical Engineering and Science</a></li><li><a href="https://cheme.mit.edu/" target="_blank">Department of Chemical Engineering</a></li><li><a href="https://engineering.mit.edu/" target="_blank">School of Engineering</a></li></ul></div>

    </div>
      
    
  </div></div>]]></description>
        </item>
    </channel>
</rss>