<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 27 Feb 2024 03:00:16 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Why Jalapeño Peppers Are Less Spicy Than Ever (2023) (182 pts)]]></title>
            <link>https://www.dmagazine.com/food-drink/2023/05/why-jalapeno-peppers-less-spicy-blame-aggies/</link>
            <guid>39517145</guid>
            <pubDate>Mon, 26 Feb 2024 21:50:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dmagazine.com/food-drink/2023/05/why-jalapeno-peppers-less-spicy-blame-aggies/">https://www.dmagazine.com/food-drink/2023/05/why-jalapeno-peppers-less-spicy-blame-aggies/</a>, See on <a href="https://news.ycombinator.com/item?id=39517145">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>It’s not just you: jalapeño peppers are less spicy and less predictable than ever before. As heat-seekers chase ever-fiercer varieties of pepper—Carolina reapers, scorpions, ghosts—the classic jalapeño is going in the opposite direction. And the long-term “de-spicification” of the jalapeño is a deliberate choice, not the product of a bad season of weather.</p>
<p>This investigation began in my own kitchen. After months of buying heat-free jalapeños, I started texting chefs around Dallas to see if they were having the same experience. Many agreed. One prominent chef favors serranos instead. Regino Rojas of <a href="https://directory.dmagazine.com/restaurants/revolver-taco-lounge/" target="_blank" rel="noreferrer noopener">Revolver Taco Lounge</a> suggested jalapeños are now “more veggie-like than chile.” Luis Olvera, owner of Trompo, said that jalapeños now have so much less heat that “I tell my staff, ‘I think my hands are just too damn sweet,’ because I can’t make salsa spicy enough anymore.”</p>
<p>To be fair, not everyone agreed with these views. One restaurateur wondered if jalapeños seem less hot because diners have become infatuated with habaneros and serranos. Wayne White, general manager at Hutchins BBQ, offered a middle ground. “I noticed during covid, the quality got really bad, but now to me they’re beautiful,” he said. “We did have a season during covid, you could tell they were pulling them too soon, they weren’t that ripe. But I ate a whole jalapeño the other day, just to eat one, and it lit me up.”</p>
<p>I searched the internet to see whether jalapeños are really getting milder, but only found shopping tips. Gardening websites offered savvier advice: that peppers grow hotter under stress. If they’re well-watered, they won’t produce as much capsaicin, the chemical that generates the sensation we know as spiciness. But even this explanation leaves unanswered questions. My sunny backyard, which produces ferocious peppers, is one thing. What about all the peppers in the grocery store?</p>
<p>Clearly, a real investigation was required. So I called Stephanie Walker, extension vegetable specialist at New Mexico State University, advisory board member of that university’s <a href="https://cpi.nmsu.edu/">Chile Pepper Institute</a>, and chair of the 2023 New Mexico Chile Conference.</p>
<p>“Other complaints have come my way,” Walker said at the start of our phone call. This turned out to be a comedic understatement: she has a massive, existential complaint about the state of the chile pepper industry. I got on the phone expecting to hear a prosaic story of weather patterns shifting, unusual rains in pepper-growing regions, or the spread of greenhouses. I would not have been surprised if she validated Rojas’ theory: that jalapeños are now grown to look pretty, shiny, and big, regardless of flavor. “Pesticides and other enhancing farming elements make them look beautiful but not really spicy,” Rojas suggested to me. </p>
<blockquote>
<p>People lost a lot of interest in tomatoes for a long time until heirlooms came back. Now we have the same thing with peppers.</p>
<cite>Stephanie Walker, New Mexico State University Chile Pepper Institute</cite></blockquote>
<p>There’s truth to all these theories, but Walker says they are only secondary factors.</p>
<p>“As more growers have adopted drip irrigation, more high-tech farming tools to grow the peppers, they’ll tend to be milder,” Walker told me first, as a sort of throat-clearing exercise before the real explanation. “But there’s more to it than that.”</p>
<p>The truth is more like a vast industrial scheme to make the jalapeño more predictable—and less hot.</p>
<h2 id="h-the-vast-jalapeno-conspiracy">The Vast Jalapeño Conspiracy</h2>
<p>Most jalapeños go straight to factories, for canned peppers, pickled pepper rings, salsas, cream sauces, dressings, flavored chips and crackers, dips, sausages, and other prepared foods. For all those companies, consistency is key. Think about the salsa world’s “mild,” “medium,” and “hot” labels.</p>
<p>According to <em>The Mexican Chile Pepper Cookbook</em> by Dave DeWitt and José Marmolejo, 60 percent of jalapeños are sent to processing plants, 20 percent are smoke-dried into chipotles, and just 20 percent are sold fresh. Since big processors are the peppers’ main consumers, big processors get more sway over what the peppers taste like.</p>
<p>“It was a really big deal when breeders [told the industry], ‘hey, look, I have a low-heat jalapeño,’ and then a low-heat but high-flavor jalapeño,” Walker explained. “That kind of became the big demand for jalapeños—low heat jalapeños—because most of them are used for processing and cooking. [Producers] want to start with jalapeños and add oleoresin capsicum.”</p>
<div>
<figure>
<picture>
<img decoding="async" loading="lazy" srcset="https://assets.dmagstatic.com/wp-content/uploads/2015/09/texasfriedbaconjalapenobites.jpg 480w, https://assets.dmagstatic.com/wp-content/uploads/2015/09/texasfriedbaconjalapenobites.jpg 768w" src="https://assets.dmagstatic.com/wp-content/uploads/2015/09/texasfriedbaconjalapenobites.jpg" alt="Photo of jalapeño pepper fritters from the Texas State Fair.">
</picture>
<figcaption>
<span>Would this jalapeño fritter from the 2015 State Fair of Texas taste less spicy if it was made today?</span>
</figcaption>
</figure>
</div>
<p>Oleoresin capsicum is an extract from peppers, containing pure heat. It’s the active ingredient in pepper spray. It’s also the active ingredient, in a manner of speaking, for processed jalapeños. The salsa industry, Walker said, starts with a mild crop of peppers, then simply adds the heat extract necessary to reach medium and hot levels. She would know; she started her career working for a processed-food conglomerate.</p>
<p>“I’ve worked in peppers in my entire life,” she told me. “Jalapeños were originally prized as being a hot pepper grown in the field. When we were making hot sauce in my previous job, we had the same problem, that you couldn’t predict the heat. When you’re doing a huge run of salsa for shipment, and you want a hot label, medium label, mild label, it’s really important to predict what kind of heat you’ll get. We tried a statistical design from the fields, and it just didn’t work, because mother nature throws stressful events at you or, sometimes, does not bring stress.”</p>
<p>The standardization of the jalapeño was rapidly accelerated by the debut, about 20 years ago, of the TAM II jalapeño line, a reliably big, shiny, fleshy pepper that can grow up to six inches long—with little to no heat. TAM II peppers have become some of the most popular in the processing business. The <a href="https://journals.ashs.org/hortsci/view/journals/hortsci/37/6/article-p999.xml">2002 paper</a> in <em>HortScience</em> trumpeted TAM II’s benefits: virus resistance, absence of dark spots, longer fruit with thicker flesh, earlier maturation, and, compared to a variety of jalapeño called Grande, less than 10 percent of the spiciness. TAMs grown in one location measured in at 1620 Scoville units, while those at another came in at just 1080, which is <a href="https://www.alimentarium.org/en/story/scoville-scale">milder than a poblano</a>.</p>
<p>In conclusion, the paper’s authors wrote, “The large, low-pungency fruit of ‘TMJ II’ will make it equally suited for fresh-market and processing uses.”</p>
<p>DeWitt, writing in his solo book <em>Chile Peppers: A Global History</em>, says TAM became widespread in Texas after its introduction. “It was much milder and larger than the traditional jalapeños, and genes of this mild pepper entered the general jalapeño pool. Cross-breeding caused the gene pool to become overall larger and milder.”</p>
<p>Since I know you’re wondering who the inventors are: the clue is in the name TAM II. The hot (but also not hot) new jalapeño is an invention of Texas A&amp;M University. Yes, Aggies took the spice out of life.</p>
<div>
<figure>
<picture>
<img decoding="async" loading="lazy" srcset="https://assets.dmagstatic.com/wp-content/uploads/2021/09/Hutchins-bbq.jpg 480w, https://assets.dmagstatic.com/wp-content/uploads/2021/09/Hutchins-bbq.jpg 768w" src="https://assets.dmagstatic.com/wp-content/uploads/2021/09/Hutchins-bbq.jpg" alt="Picture of a barbecue tray from Hutchins BBQ, including a bacon-wrapped jalapeño pepper.">
</picture>
<figcaption>
<span>A tray from Hutchins BBQ, including a Texas Twinkie.</span>
<span>Bret Redman</span>
</figcaption>
</figure>
</div>
<p>And yes, “II” means it’s a sequel. The original TAM came out much earlier and was profiled in <a href="https://www.csmonitor.com/1983/0607/060702.html">a 1983 article</a> in the <em>Christian Science Monitor</em>. At the time, the A&amp;M scientists estimated 800 acres were being grown nationally, and they told reporter Daniel Benedict that there was plenty of room left on the market for spicier stuff. (“For the hot-pepper lover, there’s something for him already.”) </p>
<p>After 40 years of the milder pepper enjoying increased popularity, virus resistance, higher yields, and a shiny new sequel, hotter pre-TAM jalapeños appear to have lost substantial ground. Exact statistics on planting demand are hard to obtain because growers do not want to tip off seed suppliers on how to price their products.</p>
<p>As the invention of TAM I and II suggests, “jalapeño” as a name does not connote a single breed or genetic line. There are varieties of jalapeño as there are of tomatoes. Mitla peppers are at the opposite end of the scale from TAMs, sometimes reaching 8000 Scoville units. (The A&amp;M paper derides Mitlas since they are often wonkily curved, and need more culling.) </p>
<p>In my interviews around Dallas, I learned many restaurateurs don’t know what breed their supplier is offering, or even that various breeds exist. At Hutchins BBQ, which employs four people full-time preparing around 7,000 jalapeños a week for its iconic brisket-stuffed Texas Twinkies, suppliers drop off peppers and the barbecue joint sorts through, picking the specimens they want and returning the rest. Hutchins deseeds the peppers to reduce any remaining heat.</p>
<p>For heat seekers, Walker recommends Mitla and Early jalapeños; they’re called “Early” not because they were picked early but because, as a breed, they grow quickly and are well-adapted to cooler environments.</p>
<h2>First heirloom tomatoes, next heirloom peppers?</h2>
<p>Walker compares the current state of the pepper industry with the world of American tomatoes, which were bred for hardiness in shipping, firmness, and canning. Only recently has an heirloom tomato revolution tried to cater directly to home cooks and chefs with tomato breeds that emphasize flavor and juiciness first.</p>
<p>“People lost a lot of interest in tomatoes for a long time until heirlooms came back,” Walker said. “Now we have the same thing with peppers. There’s a place for people to embrace heirloom peppers, the way that we have with tomatoes.”</p>
<p>For gardeners and small growers, the <a href="https://chilepepperinstitute.ecwid.com/Seeds-c85441005">Chile Pepper Institute</a> sells seeds but results will always be complicated, since a hot, dry summer can turn even TAM jalapeños into weapons, and a cool, wet season will result in pampered plants. But how can you find hotter peppers if you are shopping, or looking to supply your restaurant? </p>
<p>Walker’s best advice is to lobby suppliers and grocers for specific pepper breeds. Ask a produce manager or a supplier if you can get Early or Mitla peppers, or if the store can label its pepper breeds. And ignore the bogus factoids spread by many online shopping guides. I found a <a href="https://www.rachaelrayshow.com/articles/the-trick-to-picking-a-really-spicy-or-less-spicy-jalapeno-pepper">Rachael Ray Show</a> article claiming that bigger peppers are always spicier than smaller ones—which contradicts everything I had just learned about TAMs being deliberately engineered for size. Walker called that tip “misinformation.”</p>
<p>If lobbying your grocery managers sounds like a futile effort, look at the changes that have rippled through the tomato industry as breeders re-embrace heirlooms. Or look at the widespread adoption of a <a href="https://www.npr.org/sections/thesalt/2019/10/30/773457637/from-culinary-dud-to-stud-how-dutch-plant-breeders-built-our-brussels-sprouts-bo">less stinky breed of Brussels sprouts</a>, scientifically developed through a similar selective breeding process, which turned that vegetable from a punchline into a favorite.</p>
<p>“I think it’s a great opportunity for growers who really want to get into specializing in some of these heirloom varieties,” Walker said. </p>
<p>Let’s hope some farmers are reading this and yearning for the days when a jalapeño was a reliable source of spice. Those days can return.</p>
<div>
<h3>
<span>Get the SideDish Newsletter</span>
</h3>
<p>
Dallas' hottest dining news, recipes, and reviews served up fresh to your inbox each week.
</p>
</div>
<div>
<h3>
<span>Author</span>
</h3>
<div>
<figure>
<img loading="lazy" src="https://assets.dmagstatic.com/wp-content/uploads/2022/04/brian-reinhart-headshot-150x150.png" alt="Brian Reinhart">
</figure>
<div>
<h3>
<span>Brian Reinhart</span>
</h3>
<p><a href="https://www.dmagazine.com/writers/brian-reinhart/" target="_self">
View Profile
<span aria-label="Arrow">
<svg width="32" height="16" viewBox="0 0 32 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M25 0.929688L23.5 2.42969L28.0703 7H0V9H28.0703L23.5 13.5703L25 15.0703L32.0703 8L25 0.929688Z"></path></svg> </span>
</a>
</p></div>
<p>
Brian Reinhart became D Magazine's dining critic in 2022 after six years of writing about restaurants for the <em>Dallas Observer</em> and the <em>Dallas Morning News.</em>
</p>
</div>
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Remove "Committing to open models" from their website (159 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1b0o41v/top_10_betrayals_in_anime_history/</link>
            <guid>39517016</guid>
            <pubDate>Mon, 26 Feb 2024 21:36:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1b0o41v/top_10_betrayals_in_anime_history/">https://old.reddit.com/r/LocalLLaMA/comments/1b0o41v/top_10_betrayals_in_anime_history/</a>, See on <a href="https://news.ycombinator.com/item?id=39517016">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hey all, here's my brief business analysis.</p>

<p>It's an interesting move. I see them pullling a 180 and reframing their go2market framework from open weights to GDPR-compliant and enterprise-friendly. That niche definitely exists right now - but surely quickly also will be a market to conquer for OpenAI aswell. All of OpenAI's efforts point in this direction.</p>

<p>Then Mistral will be in the situation to be much more dependent on Microsoft for their marketing via Azure, since it's their only major inbound channel, than vice-versa. And for Microsoft it will not make sense long-term to have multiple satellite enterprises competing for the same segment, overhead in infrastructure, etc. This is a small sized bet for Microsoft, but Microsoft is known to constantly seek for portfolio synergies and optimization. We might very well see Mistral become merged to OpenAI a year or two down the road.</p>

<p>Nevertheless, every actor in the industry knows the power and ingenuity of the Open Source and research community. I strongly believe that OSS nevertheless will have an extremely strong role to play.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New York medical school eliminates tuition after $1B gift (262 pts)]]></title>
            <link>https://www.bbc.com/news/world-us-canada-68407453</link>
            <guid>39516927</guid>
            <pubDate>Mon, 26 Feb 2024 21:25:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/world-us-canada-68407453">https://www.bbc.com/news/world-us-canada-68407453</a>, See on <a href="https://news.ycombinator.com/item?id=39516927">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 976w" type="image/webp"><img alt="Dr Ruth Gottesman in 2016" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 976w" src="https://ichef.bbci.co.uk/news/976/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg" width="976" height="549" loading="eager"></picture></span><span role="text"><span>Image source, </span>Getty Images</span></p><figcaption><span>Image caption, </span><p>Dr Ruth Gottesman studied learning disabilities and developed screening protocols at Albert Einstein College of Medicine.in the Bronx.</p></figcaption></figure></div><div data-component="text-block"><p><b>A New York City medical school will offer students free tuition following a $1bn donation from the 93-year-old widow of a major Wall Street investor. </b></p></div><div data-component="text-block"><p>The gift to Albert Einstein College of Medicine came from Dr Ruth Gottesman, a former professor at the Bronx school. </p></div><div data-component="text-block"><p>It is one of the largest ever donations made to a US school and is the largest ever made to a medical school. </p></div><div data-component="text-block"><p>The Bronx, New York City's poorest borough, is ranked as the unhealthiest of New York's 62 counties. </p></div><div data-component="text-block"><p>In a statement, university dean Dr Yaron Yomer said that the "transformational" gift "radically revolutionises our ability to continue attracting students who are committed to our mission, not just those who can afford it". </p></div><div data-component="text-block"><p>Tuition at the school is nearly $59,000 each year, leaving students with substantial debt.</p></div><div data-component="text-block"><p>The statement from Einstein noted students in their final year will be reimbursed for their spring 2024 tuition, and from August, all students, including those who are currently enrolled, will receive free tuition. </p></div><div data-component="text-block"><p>The donation "will free up and lift our students, enabling them to pursue projects and ideas that might otherwise be prohibitive", Dr Yomer added. </p></div><div data-component="text-block"><p>Dr Gottesman, now 93, began working at the school in 1968. She studied learning disabilities, ran literacy programmes and developed widely used screening and evaluation protocols.</p></div><div data-component="text-block"><p>Her late husband, David Gottesman, founded a prominent investment house and was an early investor in Berkshire Hathaway, Warren Buffet's multinational conglomerate. He died in September 2022 at the age of 96. </p></div><div data-component="text-block"><p>Dr Gottesman said in a statement that the doctors who train at Einstein go on to "provide the finest healthcare to communities here in the Bronx and all over the world".</p></div><div data-component="text-block"><p>"I am very thankful to my late husband, Sandy, for leaving these funds in my care, and l feel blessed to be given the great privilege of making this gift to such a worthy cause," she added.</p></div><div data-component="text-block"><p>About 50% of Einstein's first-year students are from New York, and approximately 60% are women. Statistics published by the school show that about 48% of its medical students are white, while 29% are Asian, 11% are Hispanic and 5% are black. </p></div><div data-component="text-block"><p>In an interview with the New York Times, she recalled that her late husband had left her a "whole portfolio of Berkshire Hathaway stock" when he died with the instructions to "do whatever you think is right with it". </p></div><div data-component="text-block"><p>"I wanted to fund students at Einstein so that they would receive free tuition," Dr Gottesman said she immediately realised. "There was enough money to do that in perpetuity." </p></div><div data-component="text-block"><p>She added that she occasionally wonders what her husband would have thought of the donation.</p></div><div data-component="text-block"><p>"I hope he's smiling and not frowning," she said. "He gave me the opportunity to do this, and I think he would be happy - I hope so." </p></div><section data-component="links-block"><p><h2>More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Isn't Taxpayer-Funded U.S. Broadband Mapping Data Owned by the Public? (131 pts)]]></title>
            <link>https://www.techdirt.com/2024/02/26/why-isnt-taxpayer-funded-u-s-broadband-mapping-data-owned-by-the-public/</link>
            <guid>39516007</guid>
            <pubDate>Mon, 26 Feb 2024 19:49:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2024/02/26/why-isnt-taxpayer-funded-u-s-broadband-mapping-data-owned-by-the-public/">https://www.techdirt.com/2024/02/26/why-isnt-taxpayer-funded-u-s-broadband-mapping-data-owned-by-the-public/</a>, See on <a href="https://news.ycombinator.com/item?id=39516007">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-432775">


<h3>from the <i>this-is-why-we-can't-have-nice-things</i> dept</h3>

<p>We’ve&nbsp;<a href="https://www.techdirt.com/2022/01/06/shitty-us-broadband-maps-are-feature-not-bug/">noted for decades</a>&nbsp;how, despite all the political lip service paid toward “bridging the digital divide,” the U.S.&nbsp;doesn’t <strong>truly</strong> know where broadband is or isn’t available. The FCC’s past broadband maps, which cost $350 million to develop, have long been accused of all but hallucinating competitors, making up available speeds, and excluding a key metric of competitiveness: price.</p>
<p>You only need to spend a few minutes plugging your address into the FCC’s&nbsp;<a href="https://broadband477map.fcc.gov/#/">old map</a>&nbsp;to notice how the agency <strong>comically</strong> overstates broadband competition and available speeds. After being mandated by Congress in 2020 by the Broadband DATA Act, the FCC struck a new, $44 million contract with a company named Costquest to develop&nbsp;<a href="https://broadbandmap.fcc.gov/home">a new map</a>. </p>
<p>While an improvement, the new map still has problems with over-stating coverage and available speeds (<a href="https://broadbandmap.fcc.gov/home">try it for yourself</a>). And the FCC still refuses to collect and share pricing data, which industry opposes because it would only work to further highlight monopolization, consolidation, and muted competition. </p>
<p>But there’s another problem. As broadband industry consultant Doug Dawson notes, <a href="https://potsandpansbyccg.com/2024/02/14/shouldnt-broadband-mapping-data-belong-to-the-public/">the public doesn’t even own the finalized broadband mapping data</a>. Costquest does:</p>
<blockquote>
<p><em>“…the FCC gave CostQuest the ability to own the rights to the mapping fabric, which is the database that shows the location of every home and business in the country that is a potential broadband customer. This is a big deal because it means that CostQuest, a private company, controls the portal for data needed by the public to understand who has or doesn’t have broadband.”</em></p>
</blockquote>
<p>In addition to the $44.9 million the FCC paid Costquest to create the maps, Costquest received another <strong>$49.9 million</strong> from the NTIA to provide the databases and maps for the $42 billion broadband subsidy and grant program (included in the 2021 infrastructure bill). Third parties (like states trying to shore up access to affordable broadband) have to pay Costquest even more money to access the data. </p>
<p>So it’s all been incredibly profitable for Costquest. But <strong>taxpayers are closing in on paying nearly half a billion dollars for broadband maps that not only still aren’t fully accurate, but which they can’t transparently access and don’t own despite paying for. </strong></p>
<p>That’s fairly insane any way you slice it, and as Dawson notes, it’s a detriment to the cash-strapped folks who could be helping expand access to affordable broadband (and helping fact-check the data):</p>
<blockquote>
<p><em>“Our industry is full of data geeks who could work wonders if they had free access to the mapping fabric database. There are citizen broadband committees and retired folks in every community who are willing to sift through the mapping data to understand broadband trends and to identify locations where ISPs have exaggerated coverage claims. But citizens willing to do this research are not going to pay the fees to get access to the data – and shouldn’t have to.”</em></p>
</blockquote>
<p>For decades, feckless and corrupt state and federal regulators turned a blind eye as regional telecom monopolies dominated the market and crushed all competition underfoot, resulting in spotty access, high prices, and terrible customer service. Usually under the pretense that “deregulation” (read: very little real consumer protection oversight) had resulted in immense innovation. </p>
<p>Not only did government not address (or often even acknowledge) that problem, they’re still proving somewhat incapable when it comes to transparently mapping its impact. </p>
<p>The $42 billion in subsidies flowing to many states to shore up access <a href="https://www.techdirt.com/2023/06/27/biden-re-announces-42-billion-investment-in-broadband-because-apparently-people-didnt-notice-the-first-time/">is a good thing</a>, but its impact will most assuredly be corrupted by feckless bureaucrats who can’t stand up to industry giants, aren’t keen on the idea of data transparency, and will lack the courage necessary to ensure giant monopolies with a history of fraud (like Comcast and AT&amp;T) don’t <a href="https://communitynets.org/content/charter-comcast-continue-dominate-state-grant-awards">pocket most of the funds</a>. </p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/bead-grants/" rel="tag">BEAD grants</a>, <a href="https://www.techdirt.com/tag/broadband/" rel="tag">broadband</a>, <a href="https://www.techdirt.com/tag/broadband-mapping/" rel="tag">broadband mapping</a>, <a href="https://www.techdirt.com/tag/competition/" rel="tag">competition</a>, <a href="https://www.techdirt.com/tag/duopolies/" rel="tag">duopolies</a>, <a href="https://www.techdirt.com/tag/fcc/" rel="tag">fcc</a>, <a href="https://www.techdirt.com/tag/gigabit-fiber/" rel="tag">gigabit fiber</a>, <a href="https://www.techdirt.com/tag/high-speed-internet/" rel="tag">high speed internet</a>, <a href="https://www.techdirt.com/tag/mapping/" rel="tag">mapping</a>, <a href="https://www.techdirt.com/tag/subsidies/" rel="tag">subsidies</a>, <a href="https://www.techdirt.com/tag/telecom/" rel="tag">telecom</a>
<br>
Companies: <a href="https://www.techdirt.com/company/costquest/" rel="category tag">costquest</a>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On Light, Colors, Mixing Paints, and Numerical Optimization (101 pts)]]></title>
            <link>https://github.com/miciwan/PaintMixing</link>
            <guid>39515478</guid>
            <pubDate>Mon, 26 Feb 2024 19:02:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/miciwan/PaintMixing">https://github.com/miciwan/PaintMixing</a>, See on <a href="https://news.ycombinator.com/item?id=39515478">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">On Light, Colors, Mixing Paints, and Numerical Optimization.</h2><a id="user-content-on-light-colors-mixing-paints-and-numerical-optimization" aria-label="Permalink: On Light, Colors, Mixing Paints, and Numerical Optimization." href="#on-light-colors-mixing-paints-and-numerical-optimization"></a></p>
<p dir="auto">This is a short write-up that is supposed to serve as a rough description of what's going on in the paint mixing tool in this depot.</p>
<p dir="auto">The tool is a virtual paint mixing tool and a solver that can generate recipes for creating a particular color out of existing paints. The tool comes with data for Kimera paints that I measured. The tool is a Python 3 program; it comes with all the sources, and if you have a Python distribution, you can just run it. There's also a Windows executable created with PyInstaller (see 'Releases', on the right). I can probably create a MacOS version too, if need be (edit: I actually added one; there's a .dmg file, and it does have something in it, and if you double-click it, it does show up, so it seems to work, but honestly, I barely use Mac, so it's hard for me to say if this is the right way, or is something more expected...)</p>
<p dir="auto">If you just want to grab the tool and play with it, that's about it! Have fun, and I hope you find it at least somewhat useful.</p>
<p dir="auto">But below, you'll find a more or less complete description of how it works (and when it doesn't). So, if you have a bit of time to spare, read on!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Very recently, I discovered miniature painting. I was never really into WH40K or anything related, but I have some fond memories of playing pen &amp; paper RPGs years ago, and after watching a bunch of YouTube videos, I thought it looked easy enough to try. I still suck at it, but I somehow really enjoy the tranquilizing experience of putting thin layers of paint onto 3 cm tall figurines.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/1%20-%20figureJPG.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/1%20-%20figureJPG.JPG" width="70%"></a>
</p>
<p dir="auto">In my day job, I do real-time graphics engineering for video games, and it quickly turned out that a lot of the problems I deal with at work are very similar to those in miniature painting: you analyze how light behaves, how it interacts with different surfaces, how the eyes perceive it, etc. Of course, painting is not just engineering; it's <em>Art</em> after all (capital 'A'), but there seems to be a consensus that painters should understand these technical aspects, even just to know when they deliberately break them.</p>
<p dir="auto">There were a number of things that looked like fascinating research projects somewhere between miniatures and computer graphics, but one thing that sparked my particular interest was paints. Miniature paints usually come with these cryptic names: Skrag Brown, Tombstone Horror, or whatever. I don't really mind, but the producers never actually tell you what these colors actually are. And when you have limited experience, it's often hard to tell if a particular paint will work as some midtone or if it will be too dark. Many YouTube tutorial videos actually tell you which exact paints they use, but they most often come from different lines, some are immediately available, some are not, and for some, you need to wait - and I want to paint this very second! It seemed pretty clear that instead of buying all the possible paints, the more reasonable approach would be to pick some base paints and learn to mix them to get the colors that I need.</p>
<p dir="auto">For a beginner, there are, however, two problems. First: mixing paint is not a particularly intuitive process: sometimes you get something reasonable, sometimes you get muddy brown. Second: you need to know what color you actually want to get. Sure, there are some nice videos on how to color match, but if you don't have a good intuition of what skin tone you want to achieve, it's hard to tell if your mix needs more blue or red.</p>
<p dir="auto">Because of my engineering background, the solution seemed obvious: I would like to just pick a color on the screen (from, say, a photograph) and I want to know which paints, and how much of them, to mix to get it. I would also like to experiment with mixing paints without actually having to waste physical paint. For that, I need to somehow characterize the paints that I have, I need a model for simulating how they mix, and I need a numerical solver that will be able to minimize the error between the color that I want and a mix of some number of paints. These sorts of processes are something that I regularly go through and enjoy, so it looked like a perfect on-the-side project.</p>
<p dir="auto">Disclaimer here: yes, I know that in practice no one works that way. Especially if the solver gives you ratios like 88 parts of white, 3 parts of blue, and 2 parts of yellow - there's no way to mix something like this on a wet palette where you work with a minuscule amount of paint. But, at least to me, it's still useful to know that it's mostly white, with a touch of blue and yellow, so when I mix something on the palette, I'm not doing it completely blind. And yes, if you've been painting for some time, you learn these things, you get that intuition. But you need to get it somehow. Painting takes a lot of practice, so if I can do some experiments purely digitally, I'm totally up for it. And to be honest, it's all more of a cool side project rather than anything else.</p>
<p dir="auto">Just in case anyone else finds it useful, I thought I'll write up all the theoretical basis for a simple tool I developed for this and provide it together with a simple Python code. Since I just had a bit of free time (I got COVID), and I just got my set of Kimera paints (which are single pigment, so incredibly saturated colors, amazing for mixing), I spent a week on this, and you can read about the results here. As it might be read by people with a less technical background, I'm trying to keep it all pretty simple and self-contained, so all the information you need to understand it is here. I'm not sure how that worked out in the end, but if something is unclear, feel free to ping me and ask for details. None of it is actually any rocket science; it's mostly some high-school level math and physics (but if you're allergic, a warning: there is some math in there).</p>
<p dir="auto">So if you're curious about how it all works, details below.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Light</h2><a id="user-content-light" aria-label="Permalink: Light" href="#light"></a></p>
<p dir="auto">Light is an electromagnetic wave, oscillations of electrical and magnetic fields propagating in space. Human eyes are sensitive to wavelengths between roughly 400 and 700 nanometers, which we perceive as colors, from violet, through blue, green, yellow, orange, to red.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/2%20-%20spectrum.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/2%20-%20spectrum.JPG"></a>
</p>
<p dir="auto">Light that usually reaches our eyes is a mixture of many different wavelengths. Depending on the ratios between the amounts of particular wavelengths, we perceive the light as different colors. If it consists mostly of the shorter visible wavelengths, we'll see it as blue. If it's mostly longer wavelengths, it's going to be red. The more precise details are further down, but that's the general intuition.</p>
<p dir="auto">To reason about these characteristics in a more principled way, one useful tool is a so-called spectral power distribution (SPD for short). It's a function that, roughly speaking, describes how much of a particular wavelength is present in some radiation. It is usually plotted as a graph, with wavelength on the horizontal axis and some energy-related quantity on the vertical axis (so the stronger a particular wavelength is, the higher the plot).</p>
<p dir="auto">So the "generally blue" light might have an SPD like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/4%20-%20spectrum%20blue.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/4%20-%20spectrum%20blue.JPG" width="35%"></a>
</p>
<p dir="auto">and the "generally red" light might have it more like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/3%20-%20spectrum%20red.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/3%20-%20spectrum%20red.JPG" width="35%"></a>
</p>
<p dir="auto">One particularly interesting family of SPDs are those of different light sources. You can take any light source and measure how much of the light it produces comes from particular wavelengths. There's this thing in physics called black body radiation that describes the SPD of a perfectly black body (so that it doesn't reflect any light, just generates it) heated to a particular temperature (all that actually led straight to quantum mechanics and the world we know today; actually analyzing spectra of starlight led to the understanding that the distant stars produce energy just like the sun, the lines appearing in the spectra of excited gases was another catalyst in the evolution of quantum mechanics, and shift in the spectra of the light from different galaxies led to the discovery that the universe is expanding; it's all in the spectrum). If you've ever come across these "2700K light", "5000K light" markings, they are exactly that - they describe the light color as the color of a black body radiator of a given temperature, in Kelvin.</p>
<p dir="auto">The SPD of a typical ~2800K incandescent light looks like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/5%20-%202856K.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/5%20-%202856K.JPG" width="35%"></a>
</p>
<p dir="auto">and a ~4200K fluorescent light looks like this.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/6%20-%20fluorescent.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/6%20-%20fluorescent.JPG" width="35%"></a>
</p>
<p dir="auto">The SPD of what we consider sunlight is a fairly complex distribution that includes not only the actual SPD of the light generated by the sun but also the absorption and scattering of some of it occurring when the light passes through the atmosphere. Because the sunlight encounters different amounts of atmosphere on its way at different times of the day (less at noon, more in the evening), the sunlight SPD also depends on the time of day and atmospheric conditions. On a typical sunny day, it might look like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/7%20-%205000K.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/7%20-%205000K.JPG" width="35%"></a>
</p>
<p dir="auto">Because we're doing science here, everything needs to be standardized, measured, and quantified. For that reason, the CIE (International Commission on Illumination) introduced "standard illuminants" - a number of SPDs describing some very particular lights. Illuminant A represents a typical tungsten filament light bulb, a black-body radiator at 2856K. Illuminants B and C have become pretty much obsolete in favor of Illuminants D. There's a whole family of these; they describe daylight in different conditions - from more "warm" ones (D50, D55) to colder ones (D65, D70). The numbers 50/55/65/70 roughly correspond to a black-body temperature that would emit light of similar color (5000K, 5500K, 6500K, 7000K), but it's a longer topic and not particularly relevant here. There are also other illuminants (like E and F), but in most practical situations, the interesting ones are A, D50, and D65 (especially the last one).</p>
<p dir="auto">One last thing that seems fairly obvious, but is very important later on: light behavior is linear (in mathematical terms). If you take two lights with two SPDs and you turn them on at the same time, the resulting lighting will have an SPD that's the sum of the two components. If you make the light twice as bright, the resulting SPD will be two times greater.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reflection</h2><a id="user-content-reflection" aria-label="Permalink: Reflection" href="#reflection"></a></p>
<p dir="auto">We rarely see light as it is generated by some source. Before it reaches our eyes, it usually bounces off things, and we register that indirect, reflected light.</p>
<p dir="auto">The way light interacts with surfaces is an incredibly complicated topic. The most basic principle is fairly simple and described by Fresnel's equations: light reaches the boundary between two mediums (say, air and an object) and some of it gets reflected off the boundary, and some of it gets refracted into the object. The angle between the reflected light and the normal to the surface (which is a direction perpendicular to the surface) is the same as the angle between the incident light and the normal (alpha on the figure below). How much of the light goes where, and the exact direction of the refracted light, depends on the index of refraction of both mediums (which describes how fast light travels in that particular medium compared to its speed in vacuum).</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/8%20-%20fresnel.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/8%20-%20fresnel.jpg"></a>
</p>
<p dir="auto">Unfortunately, this only describes reflection off a perfectly smooth, mirror surface - nothing like anything you see in reality. And it only describes the first reflection off a boundary. But light can bounce around off the microscopic roughnesses of the surface and go into the object in a different place. Or it can do it multiple times. Or it can go into the object, bounce around there, and go out (or not, there's a boundary when going outside to the air as well). And all this is ignoring any wave phenomena - diffraction, interference, etc.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/9%20-%20complex%20interactions.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/9%20-%20complex%20interactions.jpg"></a>
</p>
<p dir="auto">Physics, optics, and related fields have tried to simplify all these concepts and created multiple models for describing and quantifying these effects. Some are simpler, some are very complex. Computer graphics loves them because they allow us to render realistic-looking images on a computer.</p>
<p dir="auto">From the perspective of a miniature painter, the simplest way of looking at the light-material interaction is to split it into two components. I will call them "diffuse" and "specular" because these are the terms used in computer graphics, which I'm used to.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/10%20-%20diffuse%20spec.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/10%20-%20diffuse%20spec.jpg"></a>
</p>
<p dir="auto">The "specular" component of the reflection is everything that happens on the actual boundary between the air and the object. Some of the light bounces off it. Generally, it follows the law of reflection, so the angle between the direction the light falls onto the object and the normal is the same as the angle between the direction the light is reflected at and the normal (the normal is the direction perpendicular to the surface). I say "generally" because if the surface is not perfectly smooth, the light will be scattered in different directions - the rougher the surface, the more scattered it will be - but generally, it will be around that reflected direction. One very important bit: in the case of non-metal materials, light reflected this way does not change its color. The reflected light will have the same SPD as the one falling on the object. Interestingly, this behavior is very similar for most non-metals. To the extent that in computer graphics, we often just treat all non-metal surfaces the same way: they can be rough or smooth, but they reflect the same amount of light: no matter if it's plastic, skin, or concrete. It's a very decent approximation. Metals, due to their atomic structure, are different. When the light reflects off their surface in a specular reflection, it actually changes color. That's why gold is yellow and copper is orange.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/10.5%20-%20rough%20spec%20vs%20smooth%20spec.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/10.5%20-%20rough%20spec%20vs%20smooth%20spec.png" width="40%"></a>
</p>
<p dir="auto">The "diffuse" component is all the light that goes into the object and then some of it gets out and is, generally, scattered uniformly in all directions (or rather: let's just assume it for simplicity, that's a good enough approximation). It doesn't matter how the surface is viewed; its diffuse lighting is the same from all angles (unlike specular, which is strongly visible when viewed from that one particular direction, and not much when viewed from others). Not all the light that gets into the object gets scattered out. Some of it is absorbed and turned into heat. There's also an important difference between metals and non-metals when it comes to the diffuse component. For non-metals, the spectrum of the reflected light very much depends on the object: after all, the light goes into the surface, bounces around, and comes out - so it picks up some of the characteristics of the object. For metals, there's no diffuse component at all. The light doesn't go out; it's either absorbed or bounces off specularly. And even though, technically, the diffuse component is not "reflection", but rather a form of scattering occurring over short distances within the material, I will oftentimes just say "diffuse reflection" for simplicity.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/11%20-%20diffuse%20subsurface.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/11%20-%20diffuse%20subsurface.jpg"></a>
</p>
<p dir="auto">Side note: even though it's safe to treat the diffuse reflection as the same in all directions, its brightness still depends on the amount of light <em>falling</em> onto the given part of the object. And this amount is related to the angle between the normal of the surface and the direction towards the light: the bigger the angle, the less light the given part of the object receives (actually, the amount of light is the same; it's just distributed over a larger area, which makes "light per area" smaller, and this is what we consider "brightness"). In physics, this is called Lambert's law. In many miniature painting tutorials, people talk about shading basic shapes in a particular way: spheres have round shading towards the light, cylinders have highlights along the axis, and cubes/surfaces have generally flat lighting, depending on their orientation. This is a practical application of Lambert's law. Spheres have smoothly changing normals in all possible directions, so their brightest area is going to be in sections facing the light. Normals of a cylinder change as you go around the circle but are the same as you move along the axis, so the entire length of the cylinder is shaded the same. Flat planes have a constant normal, so every point gets the same lighting.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/12%20-%20NdotL.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/12%20-%20NdotL.jpg"></a>
</p>
<p dir="auto">(Side note to the side note: this is, of course, only true if we assume that the direction towards the light is the same on the entire surface! But is it? If we consider sunlight, the source is so far away that we can safely think that yes, every point gets the light from the same direction - it's a directional light. But for other sources, the position of the light matters too. And if the light is not a simple point but rather something larger, it all gets complicated even more).</p>
<p dir="auto">For paints, we can focus on the diffuse component only. Miniature paints dry pretty matte, so the surface of the dried paint is pretty rough. The specular component of the reflection is very faint and does brighten the surface a bit with light color, but the main characteristics of the appearance come from the diffuse part. I'll go into some theories describing what is going on with light in the paint layer later on, but for now, we can look at the macroscopic effect: light with some particular SPD falls onto the paint layer, and some of it gets scattered uniformly in all directions, and some get absorbed. We can compute the ratio of that scattered light to the incident light. This is called reflectance and, just like SPD, is a spectral characteristic: objects reflect different wavelengths differently. Some wavelengths are reflected more, some are absorbed more. This is what determines the color of the object. If an object absorbs short wavelengths and reflects and scatters long ones, it will appear reddish. If it reflects short wavelengths and absorbs long ones, it will be bluish. This is, of course, assuming that the illumination is uniform in all wavelengths. If there are no long wavelengths in the incoming lighting and the object reflects only long wavelengths, it won't look red; it will look black, as it will not reflect any light. The actual light that we see reflected diffusely off the object is a product of the object's reflectance and the SPD of the incoming light:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/13%20-%20reflection%20eq.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/13%20-%20reflection%20eq.png"></a>
</p>
<p dir="auto">  
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/14%20-%20D65%20times%20(%20yo%20+%20magenta)..JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/14%20-%20D65%20times%20(%20yo%20+%20magenta)..JPG"></a>
  <span>=</span>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/D65.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/D65.JPG"></a>  
  <span>*</span>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/3%20-%20spectrum%20red.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/3%20-%20spectrum%20red.JPG"></a>     
</p>
<p dir="auto">Not surprisingly, the same object can look very differently depending on the light used for illumination. And you can reason about the color of the object in different illumination using the above principles: if the object is blue in white light but is illuminated by yellowish-red candlelight, it will appear greenish.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The eye</h2><a id="user-content-the-eye" aria-label="Permalink: The eye" href="#the-eye"></a></p>
<p dir="auto">The next component of the whole system is the human eye: the light that gets reflected off objects reaches the eye, and we can see it. In the most simplistic view, the eye focuses the incoming light on the retina, which contains light-sensitive cells. There are two families of these cells: rods and cones. Rods respond strongest to light in the green part of the spectrum, they are responsible for seeing in low-light conditions, and they do not contribute to color vision - so we'll ignore them here. Our ability to see colors comes from cones. People with no form of color blindness have three types of cones, usually called L, M, and S - for long, medium, and short. They most strongly respond to the light with long-red wavelengths (L), medium-green (M), and short-blue (S).</p>
<p dir="auto">Just like SPD and object reflectance, we can define spectral sensitivity. It describes how strongly an eye (or any other sensor, camera, or similar) responds to light of a particular wavelength. It shows how excited the given type of cone is to see light of a particular frequency (pun intended) - the higher the sensitivity, the stronger the signal generated by the cone when it receives the same amount of energy. Here are the plots of sensitivity of three different types of human cones.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/16%20-%201920px-Cones_SMJ2_E.svg.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/16%20-%201920px-Cones_SMJ2_E.svg.png" width="35%"></a>
</p>
<p dir="auto">A cone excited by light produces a signal, but the actual wavelength that caused that excitation is irrelevant. The signal will be the same if, say, the M cone receives X amount of energy at 550nm, twice the X amount of energy at 500nm, or X energy at 500nm and X at 600nm. Technically, all the incoming energy at a particular wavelength is multiplied by the sensitivity of the sensor for that wavelength and added (integrated, but let's keep things simple) for all wavelengths to produce the cell signal. This actually leads to an interesting phenomenon called metamerism, where different distributions of incoming light can look the same to a human eye, just because they generate the same excitations in the cones.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/17%20-%20excitation.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/17%20-%20excitation.jpg"></a>
</p>
<p dir="auto">We now have a full picture: light is emitted by some source (source SPD), gets reflected by an object (source SPD multiplied by the object reflectance) and is registered by a cone in the eye (source SPD, multiplied by object reflectance, multiplied by the cone sensitivity, and added/integrated for all wavelengths). That last operation essentially involves calculating the area below the graph on the right-hand side (marked in red) - the larger the area, the greater the cone excitation, but the exact shape of that graph is irrelevant.</p>
<p dir="auto">Because our eyes contain three types of cones, the light generates three different signals that our brain interprets as color. Because there are three types of cones, the space of colors that people can see is intrinsically three-dimensional: visible colors can be described by three numbers, instead of having to deal with the entire spectral information (yes, this is where RGB comes from, but we'll get there...).</p>

<p dir="auto">All this was heavily investigated by researchers in the early 20th century. Measuring SPDs of different lights is pretty straightforward; you split the light with a prism or some diffraction grating and look at the rainbow of colors. Measuring reflectance is also relatively simple: you measure the SPD of some light, then shine it onto an object, measure the SPD of the reflected light, and divide one by the other. Measuring the sensitivity of the human eye is unfortunately really challenging. How would it work? It is very challenging to accurately tell if some particular light is 2 times or only 1.5 times brighter than some other light. Especially since the human vision system is (again) incredibly complex: rods and cones are one thing, but there's tons of processing going on later in the brain, that includes adaptation to light, color, and tons of other things (there are models for these effects too, Color Appearance Models if you want to read about some details, but it's way beyond the scope of anything here). But clever people figured out a way. They designed so-called "color matching experiments". (The actual sensitivities of the eye were actually measured in the 1950s, by Ragnar Granit, and he was awarded a Nobel Prize for this work).</p>
<p dir="auto">In the color matching experiments, a person is shown a color on one side of a screen. It's a pure color, generated by a light of a very narrow range of wavelengths. They then have to match that color by mixing three "base" lights on the opposite side of the screen - also pure colors of some particular wavelengths (they were chosen to be red, green, and blue, as it was already known that these are the colors that give peak response of the eye cells). Since not every pure color could be mixed from these three base lights, the researchers added the option to add some of the mixing light to the target color, which would effectively act as a negative mixing weight. By showing people the colors for every possible wavelength in the visible light, the researchers measured how much of the intensity of these three reference lights is needed to get the impression of the same color.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/19%20-%20color%20matching.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/19%20-%20color%20matching.jpg" width="35%"></a>
</p>
<p dir="auto">It's not quite the spectral sensitivity of a cell in the eye, but for all practical purposes, it's just as good: we can now quantify the color of any pure wavelength with three numbers. And because light is linear, any mixture made from different wavelengths can be described as some weighted sum of these triplets. So, <em>any visible color</em> can be precisely characterized by just three numbers.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/20%20-%20CIE1931_RGBCMF2.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/20%20-%20CIE1931_RGBCMF2.png" width="35%"></a>
</p>
<p dir="auto">The curves derived in the color matching experiments are called the r/g/b matching curves. The experiments were conducted in the 1920s, and back then, negative numbers were a big no-no when it came to numerical calculations. But because not all colors could be matched by additive mixing, some colors required adding light to the target color, resulting in some of the values in these curves being negative. So, the same CIE that standardized illuminants figured out that it would be good to transform these curves mathematically, so they are always positive. This would represent the mixing of some imaginary lights that cannot exist in practice. This would mean that some of the combinations of the numbers would represent colors that cannot exist, but they thought it's a good deal to get rid of these negatives. They also wanted one of the numbers to represent the general "brightness" of the color (that corresponds to the response of the rods). So, they did some mathematical magic and, based on the r/g/b matching curves, formulated the X/Y/Z matching curves.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/21%20-%202560px-CIE_1931_XYZ_Color_Matching_Functions.svg.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/21%20-%202560px-CIE_1931_XYZ_Color_Matching_Functions.svg.png" width="35%"></a>
</p>
<p dir="auto">And these X/Y/Z curves are the basis of modern colorimetry. They are still used today when describing color in any scientific or technical application. They form the basis for the XYZ color space, where every color has the three X/Y/Z values, and the Y value is the color brightness. The X/Y/Z matching curves behave exactly like the spectral sensitivities - but instead of being the sensitivities of cells in our eyes, they are just some mathematical abstract sensitivities, not describing anything in particular (well, the Y curve does represent the overall sensitivity of the eye to brightness). But since they are just some numbers, it doesn't really change anything; it's all just some abstract math anyway.</p>
<p dir="auto">We went from a continuous spectrum of visible light to three numbers we can do operations with. We can take light with the XYZ_1 color and add it to the light with XYZ_2 color, and we will get the (X_1 + X_2), (Y_1 + Y_2), (Z_1 + Z_2) color. We can take the light with XYZ_1 color and make it two times stronger, and we will get 2X_1, 2Y_1, 2*Z_1 color. This is called a color space. Technically, it's not really correct to multiply the components together - say, have the reflectance stored as XYZ and multiply it by the light XYZ, but it actually works pretty well (look at most of the modern CGI or video games; we all do this all the time. The only commercial renderer that actually works with spectral quantities and does it correctly is Manuka from Weta, used in "Avatar," "Planet of the Apes" series, and other movies).</p>
<p dir="auto">Fun fact: you might think that the matching curves were derived after massive studies involving thousands of volunteers. Quite far from reality, actually. The original experiments were performed by two people: W. David Wright with 10 (as in "ten") observers and John Guild with 7 ("seven") observers. Yes, the whole modern colorimetry is based on what 17 people saw in the 1920s, most likely friends and families of the two gentlemen.</p>
<p dir="auto">Later on, there were some revisions to these curves - most notably the 1964 10-degree standard observer (vs. the original 1931 2-degree standard observer): later researchers realized that due to all the adaptation in the visual system, it makes more sense for the test patches to take a larger part of the field of view, so they performed new experiments and created new curves that are slightly different. But long story short, the 2-degree, 1931 curves are still most commonly used today.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/22%20-%201931%20vs%201964%20xyz.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/22%20-%201931%20vs%201964%20xyz.jpg" width="35%"></a>
</p>
<p dir="auto">All the color spaces used in modern applications are derived from the XYZ. Some are just a linear transformation of the XYZ - so, just multiplying individual components by some constants and adding them in different combinations (like sRGB), while some involve some non-linear operations (computing powers of some expressions involving XYZ, or division) to make the values more "perceptually uniform" - so that differences in numbers describe similar changes in actual perceived colors (like Lab).</p>
<p dir="auto">Two important color spaces derived from XYZ are Yxy and sRGB. The first one uses Y to describe brightness and xy (which are X/(X+Y+Z) and Y/(X+Y+Z)) to describe the chromaticity, just the hue and saturation of the color. Oftentimes, all the visible colors are visualized in the xy plane as the visual locus, a space of all the colors that people can see. It is a convenient way of visualizing gamuts: subsets of the visible color that can be produced in some particular way, on screen or in print.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/23%20-%20YxyJPG.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/23%20-%20YxyJPG.JPG" width="35%"></a>
</p>
<p dir="auto">sRGB is a subset of XYZ, widely used in modern display technologies. It defines three primary colors (R, G, B) that are used to represent other colors and also specifies a "white point," which is considered the color white. The primaries were chosen so they can be physically realized in a technically feasible manner. However, the range of colors that can be represented in sRGB is somewhat limited because it relies on these base colors and negative weights cannot be used; monitors work by emitting light, not absorbing it. Ironically, the visual locus plots of all possible colors, when viewed on a typical screen, cannot fully display a wide range of these colors. Although sRGB is prevalent in the industry, it is gradually being replaced by standards with wider gamuts, such as Rec2020 in modern HDR displays. It's important to note that RGB values only make sense within the context of a specific color space and cannot be directly transferred between different spaces. For example, many cameras can capture photos in Adobe RGB, which has a slightly larger gamut than sRGB. To display these colors accurately on a standard monitor, they need to be converted to sRGB through additional calculations. This is the purpose of color profiles in software like Photoshop, which allow you to specify the color space you're working in and the color space of your input images, with the software handling all necessary conversions.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/24%20-%20sRGB%20gamut.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/24%20-%20sRGB%20gamut.JPG" width="35%"></a>
</p>
<p dir="auto">On the other hand, CMYK, commonly used in printing, cannot accurately describe general light color. It is designed to describe reflectance rather than light emission. CMYK colors, to be precise, require the definition of a specific illuminant and how individual components absorb and reflect light. It's not really a color space but more of a color model - a way to process color, rather than a precise mathematical method for defining and operating with colors.</p>
<p dir="auto">This is generally the case with any color system based on reflected light, including painted miniatures. To reason about their colors, we need to assume a certain illumination. Due to the widespread use of sRGB, the most sensible choice for an illuminant in such situations is D65, which is cold, white lighting and is the white point in sRGB, so (255, 255, 255) on a typical screen.</p>
<p dir="auto">To determine the color of paint and display it on a screen, the full process involves:</p>
<ul dir="auto">
<li>Taking spectral values for the D65 illuminant (this is publicly available, tabulated data).</li>
<li>Multiplying it by the spectral reflectivity of the paint.</li>
<li>Multiplying the result by the X, Y, and Z matching curves (again, publicly available).</li>
<li>Adding (integrating) the spectral values across all wavelengths to get X, Y, Z values of the light reflected off the paint layer.</li>
<li>Converting the XYZ values to sRGB with some simple math and displaying it on the screen.</li>
</ul>
<p dir="auto">You might notice that because of that integration across wavelengths, there's really no way of going back: many different spectral distributions can give the same XYZ values (the phenomenon of metamerism mentioned earlier). So, given the sRGB values, there's no straightforward way to retrieve the full spectrum back. However, we would like to overcome this limitation, so we'll try to work around the problem.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pigments</h2><a id="user-content-pigments" aria-label="Permalink: Pigments" href="#pigments"></a></p>
<p dir="auto">The only missing bit to get the paint mixing software is the model for actually mixing the paints. You might think that maybe if we take 50% of paint A and 50% of paint B, the resulting mix will have the 50-50 mix of reflectance of paint A and paint B (or technically: that paint mixing is linear). Well, bad news, of course, it isn't linear. It's actually highly nonlinear, and to get something meaningful, we need to dive into radiative transport theory and differential equations. But no worries, we'll only skim the surface.</p>
<p dir="auto">We previously considered the diffuse reflection as light going into the object, bouncing around, scattering a bit, being absorbed a bit, and going out. Now, we will look into this process in just a bit more detail.</p>
<p dir="auto">The physical theory that describes processes like this is called radiative transport theory. It can describe phenomena like the blue color of the sky or the color of mixed paints, or more generally the effects of energy traveling through some form of medium. On the most basic level, it deals with two effects: absorption of radiation (light for us) and scattering. Absorption means that some of the light gets absorbed by particles of the medium and is converted to heat. As the light passes through the medium, there's less and less of it; it gets attenuated. Media that mostly absorb light appear translucent. If light goes through them, it can change color (because certain wavelengths can be absorbed more than others), but otherwise appears unchanged. If absorption is the main effect of the medium, it can be characterized by the Beer-Lambert law (which is a specific form of the radiative transfer equation), which states that the extinction of light is exponential with distance: the more light goes in, the more of it is absorbed on a unit distance. The absorption is fairly simple to model; after all, if the light is absorbed, it is gone, and we don't have to deal with it anymore.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/25%20-%20absorbtion%20and%20scattering.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/25%20-%20absorbtion%20and%20scattering.jpg" width="70%"></a>
</p>
<p dir="auto">The other effect is scattering. When light hits a particle of the medium, it can be scattered - bounce off in some other direction. And this is where the majority of the complexity comes from. Depending on the medium, the way its particles scatter light is different. Some media scatter predominantly forward (so the light direction is just slightly affected), some scatter uniformly (so whatever direction light comes from, it can get scattered in any other direction). On top of that, when light is scattered, it travels further and interacts with more particles, undergoing more scattering events. And of course, it all happens together with absorption, so scattered light might be scattered more, and then absorbed, and so on. This results in, once again, incredibly complex systems. Thousands of academic papers have been published on various ways of solving the radiative transfer equations for some particular setups of light, medium, and so on.</p>
<p dir="auto">Luckily for us, mixing colors is a pretty common problem in various industries, so it has been extensively studied too. To make things a bit simpler, usually, a number of simplifications are made: instead of dealing with some general objects, people analyze transport within very thin horizontal slabs of material. They are considered infinite, to avoid having to deal with any problems on the boundaries, but that's perfectly fine, as for our diffuse reflection, the light doesn't really go very far to the side within the material. They also assume that the scattering function is uniform, so when light is scattered, it bounces in any random direction with no preference. For regular, no-effect paints (so non-metallic paints), that's a totally valid assumption. Lastly, they assume circular symmetry along the vertical axis - which means that it all behaves the same way no matter how around the surface we look at it - the surface properties are the same (material is isotropic) and the lighting is the same too (for instance, the sample is illuminated in the same way from every direction). With all these assumptions, you can simplify the radiative transfer equations and reason about two separate "streams" ("fluxes") of lighting: one going down, into deeper layers, and one going up, and eventually leaving the paint layer.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/26%20-%20two%20diffuse%20fluxes.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/26%20-%20two%20diffuse%20fluxes.jpg" width="70%"></a>
</p>
<p dir="auto">Given a setup like this, the radiative transport equation actually simplifies to something reasonable that can be solved by hand (if you remember how to solve systems of differential equations, of course - don't look at me here, though). If, on top of that, you assume that the thickness of the object is big enough to not pass any light through (for instance: the paint fully covers the substrate below), you get a fairly simple formula that gives you the reflectance as a function of two parameters describing scattering and absorption.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/27%20-%20two%20flux%20reflectance.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/27%20-%20two%20flux%20reflectance.png" width="40%"></a>
</p>
<p dir="auto">The K coefficient describes the absorption, and the S coefficients describe scattering. Just like SPDs, reflectance, and sensitivities, they are spectrally varying: so they are different for every wavelength.</p>
<p dir="auto">This was done, although in a bit less principled fashion, in the 1920s by Kubelka and Munk, forming what is known nowadays as "Kubelka-Munk theory". It got revised later, in the framework of radiative transfer theory, in the form of two-flux theories (K-M is sort of two diffuse fluxes), and extended in three- and multi-flux theories. If you're curious, there's a great book by Georg Klein, "Industrial Color Physics," that dives into details. For practical purposes, when working with regular diffuse, non-effect paints, two-flux or Kubelka-Munk is generally considered enough. One thing that I don't cover here are the Saunderson corrections - these are additional factors that actually take the light specularly reflected on the surface of the paint into account - and reduce the amount of light that goes into the paint layer (and out of it).</p>
<p dir="auto">And this is one of those rare cases where things are actually simple. Mixing of media is linear with respect to K and S coefficients. The mixture of 50-50 paints with coefficients K1 and K2 will give 0.5K1 + 0.5K2, the same with S. In general:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/27%20-%20k%20sum.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/27%20-%20k%20sum.png" width="40%"></a>
</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/28%20-%20s%20sum.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/28%20-%20s%20sum.png" width="40%"></a>
</p>
<p dir="auto">(the w1 and w2 weights need to add up to 1.0)</p>
<p dir="auto">Now, we only need to get these K and S coefficients from somewhere.</p>
<p dir="auto">The equation for reflectance gives us reflectance as a function of K and S, but we can rearrange it to get, for instance, K based on S and reflectance. We know how to measure reflectance, so if we only figure out where to get S from, we could compute K. The trick is to take one of the paints - usually white - and just assume that it has S equal to 1.0 for all wavelengths. We then measure its reflectance and compute K from that.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/29%20-%20k%20from%20s.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/29%20-%20k%20from%20s.png" width="30%"></a>
</p>
<p dir="auto">Now for any other paint, we also need K and S, but we cannot arbitrarily set their S to be 1.0. However, we can mix them with that white which will act as our reference value. This gives us two measurements: one of the raw paint (so-called "masstone") and one mixed with white in some proportions (that we of course need to know, but that's easy, we can just weigh the components). This, together with the equations above, gives us two equations and two unknowns: K and S for the new paint (it's a separate system of equations for every wavelength). This is trivial to solve and gives us everything we need to know to model paint mixing digitally.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/30%20-%20k%20and%20s%20from%20mix.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/30%20-%20k%20and%20s%20from%20mix.png" width="110%"></a>
</p>
<p dir="auto">Big equation, sorry :( But it's pretty simple really. We know R(eflectance) for paint and for the mix of the paint with white (this is what we measure). We know K and S for white (we just calculated it above, setting S to 1.0), and we know how much of both white and paint we put into the mix (w(eight)_paint and w(eight)_white), so it's really just a system of two equations with two unknowns (K and S for the paint). I marked all the things that we know in blue and the unknowns in red.</p>
<p dir="auto">In the real world, usually more than one mix is done, usually around five, in different proportions, to get a better estimate of the K and S coefficients. This requires slightly more complex math, but these are technical details (although important if you're trying to do it on a budget, see below).</p>
<p dir="auto">Given all this theoretical background our course of action is:</p>
<ul dir="auto">
<li>take some number of paints, preferably saturated paints that will give us a wide range of mixed colors</li>
<li>measure the reflectance of a fully opaque white layer, assume the S for that paint is equal to 1.0 for all wavelengths and derive K (the reflectance of white paint will generally be fairly uniform and close to 1.0, but not quite, and we actually need to measure it properly)</li>
<li>for every other paint in the set prepare a fully covering patch of the paint straight from the pot and its mix with white in some measured proportions.</li>
<li>measure the reflectance of both patches, derive K and S coefficients for that paint</li>
<li>we can now take any paints, in any proportions, mix their K and S coefficients</li>
<li>from the mix, we can compute the reflectance, use D65 illuminant to virtually illuminate it, and convert it to XYZ and later to sRGB</li>
<li>given this machinery we can do numerical optimization to find the best combination of paints that gives us a given sRGB color</li>
</ul>
<p dir="auto">And that's pretty much what I did. Let's dig into practical details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Measurements</h2><a id="user-content-measurements" aria-label="Permalink: Measurements" href="#measurements"></a></p>
<p dir="auto">The crucial part of the whole above operation is the measurement of the spectral reflectance of the color samples. You need a spectrophotometer for that. There are some lab-grade ones, and there are handheld ones, aiming for industry (for validating if the plastic coming out from your injection moulder is the right color, or if your printing press prints the color that the customer ordered). They can do all sorts of interesting things, measure according to different standards (there's an ISO for that, 13655 if I remember correctly) compute color differences, some have filters etc. They have one thing in common: they are pretty expensive. The decent ones start at a few thousand dollars (though you can get the nix spectro - <a href="https://www.nixsensor.com/nix-spectro-2/" rel="nofollow">https://www.nixsensor.com/nix-spectro-2/</a> - that looks interesting and might be just enough - and it's only a bit over one grand - but I never tested it so it's hard for me to judge its accuracy) - so generally not the money you want to spend on a hobby project that you're going to forget about in a month. But luckily, a while ago, I actually constructed a handheld spectrophotometer at work. We needed some particular type of measurement, that's very rarely used in practice, so regular handheld devices do not support it. It was an interesting project on its own, but I'll spare you the details. The heart of the device is the Hamamatsu C12666MA sensor, some analog-to-digital converter and a microcontroller to drive all this. The reference lighting is provided by a wide spectrum LED, which is not really perfect - as the lighting below 410 nm and above 680nm is a bit weak, making readings in these ranges less reliable - but it was totally fine for us. During all the calibration procedures I went through when making these devices (I made something like 8 of them total) I got a set of calibrated color samples that came with full spectral reflectance data - so I was able to compare it with the ones produced by my device. They were all within 1-2% for each wavelength, so I generally trust the results. And I know the exact conditions used for measuring the calibration data, which happen to be the exact conditions used for typical industrial measurements of paint samples, so even though I know my device is not quite ideal for measurements like this, I know it's not too far off either.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/31%20-%20spectophotometer.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/31%20-%20spectophotometer.jpg" width="70%"></a>
</p>
<p dir="auto">For my set of paints, I picked the Kimera set (the one with 13 base colors). Kimera paints are great because they are single pigment, and the manufacturer actually tells you what pigment this is. And they are super heavy on pigment, so you don't need a lot to create a fully opaque layer. And because of all this simplicity, I trust they are consistent - after all, it's just some amount of given pigment and medium. That gives me some confidence that two bottles of white will actually behave the same, which is important if you want any reproducibility. Not that I want to complain here, but it's very much not the case with, for instance, Citadel or Vallejo paints. Even ignoring all these silly names, their consistency is absolutely random. Sometimes they are fine straight from the pot, sometimes they need tons of water to flow at all. If I'm working on some recipe for a color, I need to be certain that different samples of the same paint will behave the same. Neither Citadel nor Vallejo seems to guarantee that, which is a bit of a bummer because Kimeras are incredibly hard to get where I live (the land of the free and the home of the brave). I ended up getting them directly from Italy, which took over a month to get, and I'm definitely not a patient person (but, surprisingly, even with global shipping, it was still cheaper to get them from Italy than from the local distributors! so hey, order them from Pegaso World directly!).</p>
<p dir="auto">I took a piece of plywood, primed it black, and painted samples of all the colors for the set. They are around 3cm x 3cm, which corresponds to the measurement area of my spectrophotometer (it's actually smaller, but it illuminates an area of around that size, so I wanted it to be uniform). I took the paint straight out of the bottle, without diluting it at all, and painted an opaque layer. Or rather, tried to paint an opaque layer. Some of these pigments are just naturally translucent, especially two yellows, and getting an opaque layer was a nightmare. It took ages to put and dry multiple thick layers of paint, and honestly, in some spots, there's still a bit of substrate peeking through. Well, we'll have to live with that. In a professional setting, paint is applied on special draw-down charts with a draw-down bar that has a precisely controlled gap. You put a blob of paint and smear it down, leaving a layer of a particular thickness (see here: <a href="https://www.byk-instruments.com/en/Physical-Properties/Paint-Application/Manual-Film-Applicators/Film-Applicator%2C-1-Gap/c/p-5970" rel="nofollow">https://www.byk-instruments.com/en/Physical-Properties/Paint-Application/Manual-Film-Applicators/Film-Applicator%2C-1-Gap/c/p-5970</a>). I don't really care about the precise thickness, but it would be great to have it uniform. The draw-down bar, however, is around $400 - which seems a bit excessive for a piece of metal that I'm going to use once. So I used a brush (tbh, I did order some knock-off, no-name draw-down bar from China, it was $50, that's about as much as I'm willing to spend to test it - we'll see how well it does, but it's going to be a while until it gets here).</p>
<p dir="auto">Next, I mixed every paint with white, measuring the exact ratios. I measured by weight, just because it's much easier. I have a precise scale, with resolution up to one hundredth of a gram, but measuring small volumes of thick liquid is difficult. This means that the final recipes will be given by weight, but honestly I don't think these paints differ in density too much (and all this is so hand-wavy, that even if they do, it won't make much difference in practice anyway). You could probably weigh the paint bottles from a fresh set and figure out these densities, but I didn't, and now it's too late, since I only ordered one set. I only mixed each paint with white in one ratio. This is the minimum number of mixes you need to calculate the K and S coefficients, but having more would be better. But of course, to have more of these mixes, you need more paint, and, as I said, I only ordered one set, and I would actually want to paint something with them too, not only do some experiments. Of course, the biggest problem is white, which you need a lot for all these mixes, but just pure white is also out of stock everywhere around here, and I was too impatient to wait for it to ship from Italy again.</p>
<p dir="auto">This is what it all looked like:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/32%20-%20PXL_20240128_204751800.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/32%20-%20PXL_20240128_204751800.jpg" width="70%"></a>
</p>
<p dir="auto">Like a real scientist, I measured every patch three times and averaged the results. I then wrote a piece of Python code to do all the math for mixing K and S and calculating reflectance and then final color when illuminated by D65 from that.</p>
<p dir="auto">Here are the spectral reflectivities of two paints (green and red)</p>
<p dir="auto">  
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/33%20-%20green.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/33%20-%20green.JPG"></a>  
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/33%20-%20red.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/33%20-%20red.JPG"></a>
</p>

<p dir="auto">And just like you would expect, if you mix yellow and blue (and a bit of white, blues from Kimera set are dark as hell straight out of the bottle), you get green.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/34%20-%20blue%20and%20yelllow.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/34%20-%20blue%20and%20yelllow.JPG" width="70%"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Solver</h2><a id="user-content-solver" aria-label="Permalink: Solver" href="#solver"></a></p>
<p dir="auto">With all this, writing a solver that produces a given color from a set of paints is fairly straightforward. When you take some set of paints, the process of finding their relative amounts in the mix that give you color closest to some prescribed one is called optimization. We start with some mixing weights, compute the result and analyze how to change the weights to get a result somewhat closer to what we want. Rinse and repeat. This is a pretty common operation nowadays, particularly in the machine learning world. All the DALL-Es, Stable Diffusions, and other ChatGPTs go through a training phase, which is in fact a giant optimization problem. Here, we only have a couple of weights, so the problem is pretty trivial and can be quickly solved even using off-the-shelf solvers, for instance, ones from the Python scipy library.</p>
<p dir="auto">There are two details here. First is the so-called "loss function" (that's the machine learning lingo, but people call it error function, objective function, cost function, and probably a bunch of other names too). This is a function that tells us how different some intermediate result is from the target. It is usually assumed to be decreasing as two values get closer (so the process becomes "minimization" of the loss function, trying to find its minimum), but it's not always the case. It usually doesn't matter at all because even if it gets larger when the values are closer, you can always multiply it by -1. One of the most common loss functions is the L2 loss: you compute the differences between the values, square them (hence the '2' in the name; the squaring bit makes sure that if the difference becomes negative, it counts the same way as if it was positive), and if you have multiple components, you add them together. You get one number that describes some measure of the difference between two arbitrary sets of values. L2 loss has some nice properties (it differentiates to a set of linear functions, which can be solved trivially), but in general, it doesn't really represent anything in particular. Yes, a larger L2 loss means that things are more different, but it's not like when L2 loss is smaller, the results are actually better (for however you define "better"). That's why designing good loss functions in modern machine learning is an art on its own, involving combining different factors, sometimes in very creative ways. Our case is fairly simple, so I just went with L2 loss in sRGB: so after computing the color of a mix of paints, I compute the differences in the red, green, and blue channels, I square and add them, and I try to find a mix that gives me the smallest such number. It seems like the industry usually does it in Lab color space because it's more perceptually uniform: so even if the result is not perfect (if your method gets the loss to zero, it doesn't really matter what loss you use - you get the issues when your loss is not quite zero), it's "perceptually" closer to the desired color. What I noticed in practice is that if you're trying to match a color that's impossible to get with a set of base paints, the result is not good no matter which loss you use, so I don't bother and just do it in sRGB.</p>
<p dir="auto">The other issue is a bit of combinatorics in our problem. The same color can be obtained from different mixes of paints. Of course, ones that use fewer paints are generally better than the ones that use more. But sometimes using two paints can get you almost there, and while using three can get you all the way there, it doesn't really make sense to add that additional one just to fix this 1% error. And how do you even decide which actual two paints you would mix? If you have a blue and yellow paint, you can optimize for the ratios that give you something closest to the green that you want, but how would you know to mix yellow and blue in the first place? In big machine learning, there are strategies for such problems: for instance, incorporating L0 loss in your final loss function - which is the number of non-zero weights - so that solutions that have fewer weights would be preferable. Of course, then comes the issue of how much of the actual difference in color one extra paint is worth and you need to come up with some arbitrary weighting factors. Because we don't really have that many paints, we can do something simpler: just take all possible combinations of one, two, three, four paints, optimize all these mixes and find the best ones. And since there are also some other factors involved, I just decided to show all best three one, two, three, and four paint combinations. I don't filter the results in any way, so it might happen that the three paint combinations can be worse than two paint ones (I always ensure that if a paint is used for mixing, some of it, even a minuscule amount, ends up in the recipe) - when a color can be obtained with just two paints, and anything extra just makes it worse. The results are also presented just as they come out of the optimizer, with three decimal places - so you might totally get recipes like 0.8 yellow + 0.01 black. The 80:1 ratio is totally useless if you want to apply it precisely in painting, but again, it's more of a guideline that it's yellow with a tiny hint of black. Don't get too attached to these numbers, they are just some help, nothing more.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/35%20-%20single%20recipe.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/35%20-%20single%20recipe.JPG" width="70%"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The tool</h2><a id="user-content-the-tool" aria-label="Permalink: The tool" href="#the-tool"></a></p>
<p dir="auto">The tool is written in Python, with the UI done in PyQt. Most of the boilerplate code was actually generated by ChatGPT - as PyQt is something I have absolutely zero interest in learning. ChatGPT is brilliant in helping you navigate these sorts of libraries that you're not familiar with, saving tons of time. I cannot recommend it enough. It was actually able to produce setup code for the layout of the application based on a rough sketch from Paint I pasted into the conversation - how cool is that?</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/36%20-%20whole%20too.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/36%20-%20whole%20too.JPG" width="70%"></a>
</p>
<p dir="auto">The left list shows you list of all the paints in the database, with their names, colors as XYZ and sRGB. The checkboxes allow you to enable and disable individual paint for the recipe solver.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/37%20-%20base%20paints.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/37%20-%20base%20paints.JPG" width="30%"></a>
</p>
<p dir="auto">The next list is the mixing list. You can drag paints from main list onto it to add them to the mix, or the other way round to remove them. Paints on the mixing list have a slider that controls their amount in the mix.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/38%20-%20used%20paints.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/38%20-%20used%20paints.JPG" width="30%"></a>
</p>
<p dir="auto">Next pane shows you the spectral plot of reflectance of the mix. Each component is shown, as well as the reflectance of the mix. The top button show the color of the mix.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/39%20-%20reflectance.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/39%20-%20reflectance.JPG" width="30%"></a>
</p>
<p dir="auto">Below is the chromaticity plot with all the components and the mixed color marked as well.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/40%20-%20xy.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/40%20-%20xy.JPG" width="30%"></a>
</p>
<p dir="auto">Last, there's the recipe solver pane: you can pick a color with a picker (and PyQt picker lets you pick a screen color directly too) and then hit "solve," which will kick off the solver. It will solve for 1/2/3/4 paint mixes and add them all to the list below. Each recipe has a background of the color mixed according to the recipe, and the overall background is always the target color, so you can compare them side by side. Solve can take a while, even though it is parallelized, but the results are added to the list as they become available. You can double click on any recipe, and it will be put into the paint mixer.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/41%20-%20recipe%20list.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/41%20-%20recipe%20list.JPG" width="30%"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tests and conclusions</h2><a id="user-content-tests-and-conclusions" aria-label="Permalink: Tests and conclusions" href="#tests-and-conclusions"></a></p>
<p dir="auto">So! How does all this compare to reality? Actually decently well. I haven't tested it all super thourouglhly, but in all the test I did it behaved more-less like expected. Some examples (ignore anything odd below 400nm, the measurement device is not particularly reliable there):</p>
<p dir="auto">A 1 to 1 mix of yellow oxide and magenta (white: measured, colored: predicted)</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_1.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_1.JPG" width="30%"></a>
</p>
<p dir="auto">A 1 to 10 mix of blue red shade and orange (white: measured, colored: predicted)</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_2.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_2.JPG" width="30%"></a>
</p>
<p dir="auto">They are actually really close! Given how sloppy all this was, that's quite encouraging!</p>
<p dir="auto">Here are some not so accurate results:</p>
<p dir="auto">A 1 to 2 to 2 mix of blue red shade, cold yellow and white. Predicted reflectance in long wavelengths is overestimated, making the predicted result brighter</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_3.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_3.JPG" width="30%"></a>
</p>
<p dir="auto">A 1 to 1 mix of cold yellow and orange - similarly, the predicted result is slightly brigher than in reality.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_4.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_4.JPG" width="30%"></a>
</p>
<p dir="auto">The main offender seems to be yellow, mostly the cold variation. I think the problem comes from it's high translucency. I tried hard to get fully covering, opaque layer, but I'm not quite sure how well it worked in the end. Interestingly other people doing similar work (for instance Yoshio Okumura here: <a href="https://repository.rit.edu/cgi/viewcontent.cgi?article=5896&amp;context=theses" rel="nofollow">https://repository.rit.edu/cgi/viewcontent.cgi?article=5896&amp;context=theses</a>) ran into exact same problems with the same pigments. It looks like yellows just need more love. I might try playing with it a bit more, but tbh, the difference is not that huge in practice, the hue matches more-less and I'm not after exact recipes anyway.</p>
<p dir="auto">One thing missing in the model used for mixing are so-called Saunderson correction coefficients. They account for specular reflection on the surface of the measured sample. Testing how much this would improve the accuracy is another excersise for the future.</p>
<p dir="auto">If you got to that point, congratulations! Grab the tool, play with it, and if you have any questions, shoot me an email at <a href="mailto:miciwan@gmail.com">miciwan@gmail.com</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License and image credits</h2><a id="user-content-license-and-image-credits" aria-label="Permalink: License and image credits" href="#license-and-image-credits"></a></p>
<p dir="auto">All the rambling, all the images not specifically mentioned below, all the data in the depot are licensed under Creative Commons BY 4.0 (so you can do whatever with it, including using it for commercial purposed, you just need to say where you took it from).</p>
<p dir="auto">Image of the smooth vs rough specular reflection is from "Real-Time Rendering 4th edition". The publisher allowed to use all the figures from the book for non-commercial purposes under fair use. I <em>believe</em> that this actually counts as fair use, but tbh, I'm not 100 percent sure. But I'm one of the authors of this book, so maybe the publisher will be kind enough not to sue me if this is not the case.</p>
<p dir="auto">The image of the spectral sensitivities of human cones is by Vanessaezekowitz, from en.wikipedia, <a href="https://en.wikipedia.org/wiki/Trichromacy#/media/File:Cones_SMJ2_E.svg" rel="nofollow">https://en.wikipedia.org/wiki/Trichromacy#/media/File:Cones_SMJ2_E.svg</a> under CC BY 3.0</p>
<p dir="auto">The image of the rgb matching curved is by Marco Polo, from wikipedia, <a href="https://en.wikipedia.org/wiki/File:CIE1931_RGBCMF.svg" rel="nofollow">https://en.wikipedia.org/wiki/File:CIE1931_RGBCMF.svg</a> under Public Domain</p>
<p dir="auto">The image of the 1931 XYZ matching curves is by Acdx, from wikipedia, <a href="https://en.wikipedia.org/wiki/CIE_1931_color_space#/media/File:CIE_1931_XYZ_Color_Matching_Functions.svg" rel="nofollow">https://en.wikipedia.org/wiki/CIE_1931_color_space#/media/File:CIE_1931_XYZ_Color_Matching_Functions.svg</a> under CC BY-SA 4.0</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Press Release: Future Software Should Be Memory Safe (104 pts)]]></title>
            <link>https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/press-release-technical-report/</link>
            <guid>39514844</guid>
            <pubDate>Mon, 26 Feb 2024 18:15:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/press-release-technical-report/">https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/press-release-technical-report/</a>, See on <a href="https://news.ycombinator.com/item?id=39514844">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
	


	<div>
								


<p><em><strong>Leaders in Industry Support White House Call to Address Root Cause of Many of the Worst Cyber Attacks</strong></em></p>



<p><em>Read the full report <a href="https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf">here</a></em></p>



<p>WASHINGTON – Today, the White House Office of the National Cyber Director (ONCD) released a report calling on the technical community to proactively reduce the attack surface in cyberspace. ONCD makes the case that technology manufacturers can prevent entire classes of vulnerabilities from entering the digital ecosystem by adopting memory safe programming languages. ONCD is also encouraging the research community to address the problem of software measurability to enable the development of better diagnostics that measure cybersecurity quality.</p>



<p>The report is titled <em><a href="https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf">“Back to the Building Blocks: A Path Toward Secure and Measurable Software.” </a></em></p>



<p>“We, as a nation, have the ability – and the responsibility – to reduce the attack surface in cyberspace and prevent entire classes of security bugs from entering the digital ecosystem but that means we need to tackle the hard problem of moving to memory safe programming languages,” said National Cyber Director Harry Coker.&nbsp; “Thanks to the work of our ONCD team and some tremendous collaboration from the technical community and our public and private sector partners, the report released today outlines the threat and opportunity available to us as we move toward a future where software is memory safe and secure by design. I’m also pleased that we are working with and calling on the academic community to help us solve another hard problem: how do we develop better diagnostics to measure cybersecurity quality? Addressing these challenges is imperative to ensuring we can secure our digital ecosystem long-term and protect the security of our Nation.”</p>



<p>By adopting an engineering-forward approach to policymaking, ONCD is ensuring that the technical community’s expertise is reflected in how the Federal Government approaches these problems. Creators of software and hardware can have an outsized impact on the Nation’s shared security by factoring cybersecurity outcomes into the manufacturing process.</p>



<p>“Some of the most infamous cyber events in history – the Morris worm of 1988, the Slammer worm of 2003, the Heartbleed vulnerability in 2014, the Trident exploit of 2016, the Blastpass exploit of 2023 – were headline-grabbing cyberattacks that caused real-world damage to the systems that society relies on every day. Underlying all of them is a common root cause: memory safety vulnerabilities. For thirty-five years, memory safety vulnerabilities have plagued the digital ecosystem, but it doesn’t have to be this way,” says Anjana Rajan, Assistant National Cyber Director for Technology Security. “This report was created for engineers by engineers because we know they can make the architecture and design decisions about the building blocks they consume – and this will have a tremendous effect on our ability to reduce the threat surface, protect the digital ecosystem and ultimately, the Nation.”</p>



<p>ONCD has engaged with a diverse group of stakeholders, rallying them to join the Administration’s effort. Statements of support from leaders across academia, civil society, and industry can be found <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-statements-of-support/">here</a>.</p>



<p>In line with two major themes of the President’s National Cybersecurity Strategy released nearly one year ago, the report released today takes an important step toward shifting the responsibility of cybersecurity away from individuals and small businesses and onto large organizations like technology companies and the Federal Government that are more capable of managing the ever-evolving threat. This work also aligns with and builds upon secure by design programs and research and development efforts from across the Federal Government, including those led by CISA, NSA, FBI, and NIST.</p>



<p>The work on memory safety in the report complements interest from Congress on this topic. This includes the efforts of the U.S. Senate and House Appropriations Committees, who included directive report language requiring a briefing from ONCD on this issue in Fiscal Year 2023 appropriations legislation. Additionally, U.S. Senate Homeland Security and Governmental Affairs Committee Chairman Gary Peters (D-MI) and U.S. Senator Ron Wyden (D-OR) have highlighted their legislative efforts on memory safety to ONCD.</p>



<p>Read the full report, <a href="https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf">“Back to the Building Blocks: A Path Toward Secure and Measurable Software.”</a> </p>



<p>Read our fact sheet <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-fact-sheet/">here</a>. </p>



<p>Read out statements of support from industry, academia, and civil society <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-statements-of-support/">here</a>.</p>



<p>Watch a video address from Director Coker and Assistant National Cyber Director for Technology Security Rajan outlining the challenges and solutions presented in the technical report <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/video-technical-report-launch/">here</a>. </p>











<p>###</p>
			</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ryzen Z1's Tiny iGPU (135 pts)]]></title>
            <link>https://chipsandcheese.com/2024/02/25/ryzen-z1s-tiny-igpu/</link>
            <guid>39514778</guid>
            <pubDate>Mon, 26 Feb 2024 18:11:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2024/02/25/ryzen-z1s-tiny-igpu/">https://chipsandcheese.com/2024/02/25/ryzen-z1s-tiny-igpu/</a>, See on <a href="https://news.ycombinator.com/item?id=39514778">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><strong>Editor’s Note</strong>: Just like <a href="https://chipsandcheese.com/2024/02/12/amds-mild-hybrid-strategy-ryzen-z1-in-asuss-rog-ally/">our prior Ryzen Z1 article</a>, the ROG Ally was kindly provided by Asus to let us test the Ryzen Z1.</p>
<p>ASUS ROG Ally comes in two configurations: AMD’s Ryzen Z1 Extreme and the Ryzen Z1. The Ryzen Z1 Extreme uses AMD’s high-end Zen 4 APU configuration, with eight Zen 4 cores and six RDNA 3 WGPs. Its non-extreme cousin uses a hybrid two Zen 4 + four Zen 4c CPU configuration and a much smaller iGPU with two RDNA 3 WGPs. We’ve covered the Ryzen Z1’s CPU side in a <a href="https://chipsandcheese.com/2024/02/12/amds-mild-hybrid-strategy-ryzen-z1-in-asuss-rog-ally/">prior article</a>. Here, we’ll be going over the iGPU.</p>
<p>Compared to Radeon 780M in the Ryzen Z1 Extreme, the Ryzen Z1’s Radeon 740M is much smaller. It’s also smaller than the Steam Deck’s iGPU, which uses four RDNA 2 WGPs. However, Ryzen Z1 does enjoy AMD’s newer RDNA 3 architecture. It’s also allowed to boost to very high clock speeds, while the Steam Deck’s iGPU is limited to 1.6 GHz.</p>
<h2>Compute Throughput</h2>
<p>WGPs, or Workgroup Processors, are the basic building blocks of AMD’s RDNA 3 graphics architecture. RDNA 3 introduces dual issue capability for a variety of common FP32 instructions, doubling the theoretical FP32 throughput. Shader programs can leverage its dual issue capability by using wave64 mode or special dual issue instructions in wave32 mode. On RDNA hardware, pixel shaders often use wave64 mode, in which 2048-bit vectors execute on the WGP’s 1024-bit execution units over 2 clock cycles, but achieve 1 instruction per cycle throughput on operations with dual issue support. Wave32 mode lets individual threads (waves) finish faster as 1 instruction per cycle throughput becomes the general case. However, taking advantage of RDNA 3’s extra FP32 units in wave32 mode requires the compiler to find dual issue pairs. That requires instruction-level parallelism within a basic block, and could be upended by register cache source port or result bus limitations.</p>
<p>I’ll be using Nemes’s Vulkan benchmark suite because the Steam Deck’s iGPU doesn’t support OpenCL, in which my own tests are written. AMD’s compiler uses wave64 for the instruction rate tests in this suite, making it a good showcase for RDNA 3’s increased FP32 throughput.</p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_vk_fp/"><img decoding="async" width="682" height="1300" data-attachment-id="26148" data-permalink="https://chipsandcheese.com/rz1_vk_fp/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_fp.png?fit=682%2C1300&amp;ssl=1" data-orig-size="682,1300" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_fp" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_fp.png?fit=682%2C1300&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_fp.png?fit=682%2C1300&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_fp.png?resize=682%2C1300&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Even though the Radeon 740M has only two WGPs, dual issue capability and much higher clock speeds give it more FP32 throughput than the Steam Deck’s iGPU. When RDNA 3’s dual issue ability doesn’t come into play, the two chips are much closer. That applies to special functions like inverse square roots. FP16 throughputs are nearly identical as well, because both RDNA 2 and RDNA 3 can pack two FP16 values into the low and high halves of a 32-bit register and do FP16 math at double rate.</p>
<p>AMD’s higher-end Ryzen Z1 Extreme dramatically outpaces the Ryzen Z1’s iGPU in all categories. The Z1 Extreme enjoys the same RDNA 3 advantages as the Z1, and also runs at high clock speeds.</p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_vk_int/"><img decoding="async" width="675" height="778" data-attachment-id="26149" data-permalink="https://chipsandcheese.com/rz1_vk_int/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_int.png?fit=675%2C778&amp;ssl=1" data-orig-size="675,778" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_int" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_int.png?fit=675%2C778&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_int.png?fit=675%2C778&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_int.png?resize=675%2C778&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Games primarily use floating point operations, but integer instructions often show up in shader code too. There, RDNA 3 largely behaves like RDNA 2. The Ryzen Z1’s iGPU again uses high clock speeds to catch up with the Steam Deck’s nominally larger iGPU.</p>
<h2>Cache and Memory Latency</h2>
<p>GPUs use massive thread-level parallelism to hide memory latency. RDNA 2 and 3’s SIMDs are capable of 16-way SMT as long as enough vector register and local data share capacity are available. A WGP with four SIMDs can thus track up to 64 independent instruction streams. But even with thread-level parallelism and a bit of instruction-level parallelism mixed in, memory latency often limits performance.</p>
<p>Therefore, cache is critical for performance. RDNA uses a sophisticated triple-level cache hierarchy. Each half of a WGP gets a L0 vector cache. A set of WGPs share a L1 mid-level cache, and a L2 cache is shared across the entire GPU. Discrete RDNA 2 and RDNA 3 cards additionally have a large Infinity Cache. For example, the RX 6900 XT has 128 MB of Infinity Cache.</p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_vk_vec_latency/"><img decoding="async" width="688" height="297" data-attachment-id="26151" data-permalink="https://chipsandcheese.com/rz1_vk_vec_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?fit=1125%2C485&amp;ssl=1" data-orig-size="1125,485" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_vec_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?fit=1125%2C485&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?fit=688%2C297&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?resize=688%2C297&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?w=1125&amp;ssl=1 1125w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?resize=768%2C331&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Higher clock speeds and RDNA 3’s larger L0/L1 sizes give the Ryzen Z1 a latency advantage over the Steam Deck. Ryzen Z1’s lead continues as the test spills into DRAM. ROG Ally’s LPDDR5 memory configuration doesn’t suffer from high latency like the Steam Deck. We saw that on the CPU side already, and testing confirms the same on the GPU side.</p>
<p>AMD GPUs since GCN have a separate scalar memory path to load values constant across a wavefront. The scalar path helps take load off the vector caches, and is better optimized for latency. RDNA 3 has a 16 KB first-level scalar cache just like prior AMD GPUs, but the larger 256 KB L1 still helps.</p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_vk_scalar_latency/"><img loading="lazy" decoding="async" width="688" height="297" data-attachment-id="26162" data-permalink="https://chipsandcheese.com/rz1_vk_scalar_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?fit=1125%2C485&amp;ssl=1" data-orig-size="1125,485" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_scalar_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?fit=1125%2C485&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?fit=688%2C297&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?resize=688%2C297&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?w=1125&amp;ssl=1 1125w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?resize=768%2C331&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>As with the vector path, the Ryzen Z1’s iGPU enjoys better scalar memory latency than the Steam Deck’s iGPU thanks to higher clocks. Compared to the Ryzen 780M in the Z1 Extreme, the Z1’s Ryzen 740M shows similar latency characteristics. However, AMD’s lower-end part does see L2 cache capacity cut from 2 MB to 1 MB. Valve’s Steam Deck also has 1 MB of L2 for the iGPU, as do older AMD iGPUs like the Vega iGPU in Renoir. The Z1 Extreme may need a larger L2 because its higher compute throughput requires more bandwidth. Higher L2 hitrate is a good way to achieve that higher bandwidth.</p>
<h2>Cache and Memory Bandwidth</h2>
<p>GPUs need high bandwidth to keep their wide vector execution units fed. The Ryzen Z1’s iGPU has first-level cache bandwidth similar to the Steam Deck’s iGPU, but gets there via high clocks instead of having more cache instances. As the test spills out into L1 and L2, the Radeon 740M maintains that high bandwidth because those lower level caches also run at higher clocks. In contrast, the Steam Deck’s iGPU and the Ryzen Z1 Extreme’s Radeon 780M have noticeably less L1 and L2 bandwidth per Compute Unit. Of course, the Z1 Extreme has higher bandwidth at each cache level and a larger L2 cache.</p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_vk_bw/"><img loading="lazy" decoding="async" width="688" height="297" data-attachment-id="26164" data-permalink="https://chipsandcheese.com/rz1_vk_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?fit=1122%2C484&amp;ssl=1" data-orig-size="1122,484" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?fit=1122%2C484&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?fit=688%2C297&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?resize=688%2C297&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?w=1122&amp;ssl=1 1122w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?resize=768%2C331&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>iGPUs are traditionally limited by their DRAM configuration, which has to be shared with the CPU. LPDDR4 and LPDDR5 provide a large bandwidth increase compared to the DDR4 setups of years ago, enabling larger iGPU designs and PC gaming handhelds. The non-Extreme Ryzen Z1 gets similar LPDDR5 benefits without being a large iGPU, and thus gets a very good memory bandwidth to compute ratio. </p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_bytes_per_flop/"><img loading="lazy" decoding="async" width="688" height="396" data-attachment-id="26245" data-permalink="https://chipsandcheese.com/rz1_bytes_per_flop/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_bytes_per_flop.png?fit=712%2C410&amp;ssl=1" data-orig-size="712,410" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_bytes_per_flop" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_bytes_per_flop.png?fit=712%2C410&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_bytes_per_flop.png?fit=688%2C396&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_bytes_per_flop.png?resize=688%2C396&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>Measured bandwidth at 512 MB test size divided by measured FP32 FMA FLOPs. If we don’t consider RDNA 3’s FP dual issue capability, the 740M would have 0.06 DRAM bytes per FP32 FLOP.</figcaption></figure></div>
<p>In contrast, Ryzen Z1 Extreme’s very large and fast iGPU outpaces advances in memory bandwidth. DRAM bytes per FLOP is low compared to the Infinity Cache equipped RX 6900 XT, even if we factor out RDNA 3’s dual issue capability. That’s why the Radeon 780M gets a 2 MB L2 cache. The Steam Deck’s iGPU and the Ryzen Z1’s Radeon 740M in contrast make do with a 1 MB L2 because they have ample memory bandwidth relative to their compute throughput.</p>
<h2>CPU to GPU Link Bandwidth</h2>
<p>Integrated GPUs are often less powerful than their discrete cousins thanks to DRAM limitations. However, they do have an advantage when moving data between CPU and GPU memory spaces because they won’t be restricted by a relatively slow PCIe link. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=26195"><img loading="lazy" decoding="async" width="688" height="529" data-attachment-id="26195" data-permalink="https://chipsandcheese.com/2024/02/25/ryzen-z1s-tiny-igpu/rz1_vk_link-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_link-1.png?fit=758%2C583&amp;ssl=1" data-orig-size="758,583" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_link-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_link-1.png?fit=758%2C583&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_link-1.png?fit=688%2C529&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_link-1.png?resize=688%2C529&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>All three AMD APUs tested here achieve similar performance, with the Steam Deck’s APU technically pulling ahead when the copy engine is in use. Achieved bandwidth is well above the 32 GB/s available with a PCIe 4.0 x16 link, so these integrated GPUs could have an advantage if the CPU and GPU need to communicate a lot. That’s unlikely to matter for gaming, but it could matter if a compute application has to process GPU-generated results on the CPU.</p>
<p>If we use a compute shader to move data between CPU and GPU memory, the Ryzen Z1 does fall behind a bit. That’s likely because its smaller shader array can’t keep as much work in flight to hide latency. Using a CPU-side <code>memcpy</code> to move data between host memory and a buffer mapped to GPU memory results in very low bandwidth. CPU cores are less latency tolerant than GPU ones, and there could be other inefficiencies when CPU cores directly access GPU memory.</p>
<h2>Final Words</h2>
<p><a href="https://www.youtube.com/watch?v=Lg624-NHqcw">Gamers Nexus</a> notes that ASUS’s non-Extreme ROG Ally gives up a lot of GPU performance for a $100 price drop, especially when the Ryzen Z1 Extreme’s iGPU can stretch its legs in docked mode. In that respect, it’s similar to the Steam Deck, which similarly <a href="https://www.eurogamer.net/digitalfoundry-2023-asus-rog-ally-vs-steam-deck-review?page=4">gets outpaced by the Z1 Extreme</a> when the latter is given enough power budget. On the flip side, the gap narrows on battery power. There, the Z1’s smaller iGPU can maintain high clocks even with a smaller power budget.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=26203"><img loading="lazy" decoding="async" width="688" height="388" data-attachment-id="26203" data-permalink="https://chipsandcheese.com/2024/02/25/ryzen-z1s-tiny-igpu/gn_z1_z1_extreme/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?fit=1314%2C741&amp;ssl=1" data-orig-size="1314,741" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gn_z1_z1_extreme" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?fit=1314%2C741&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?fit=688%2C388&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?resize=688%2C388&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?w=1314&amp;ssl=1 1314w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?resize=768%2C433&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?resize=1200%2C677&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>That leaves me with mixed feelings about the Ryzen Z1 as a gaming chip. Its Radeon 740M is a demonstration of how high clocks can let a small GPU go far. On the other hand, this “speed demon” advantage only works when tight power budgets prevent larger GPUs from reaching similar clocks. Performance on battery is definitely important for a portable device like the ROG Ally or Steam Deck. But those devices can spend a lot of time plugged into the wall at the airport or coffee shops. Even car and airplane seats have power outlets available, so a handheld can run in turbo mode on the go.</p>

<p>For those situations, the iGPU in the Ryzen Z1 feels small for a gaming first device. The two-WGP Radeon 740M is only one step up from the minimal single WGP setup in Zen 4 desktop CPUs (Raphael). For sure, the Radeon 740M has a fully fleshed out cache setup instead of Raphael’s minimal 64 KB L1 and 128 KB L2. But even Renoir from a few years ago has a wider iGPU. Meanwhile, Ryzen Z1’s CPU is very strong for a low-power chip. Two Zen 4 cores provide excellent responsiveness, while the four Zen 4c cores maintain good multi-threaded performance.</p>
<p>Ryzen Z1’s priorities can be seen in die area allocated for the iGPU’s WGPs versus its CPU cores. TechPowerUp states the Ryzen Z1 occupies 137 mm<sup>2</sup>. Pixel counting indicates the two WGPs occupy about 5.1 mm<sup>2</sup> of area, while the Zen 4 and Zen 4c cores occupy 17.2 mm<sup>2</sup> (not counting shared cache). The Van Gogh APU in Valve’s Steam Deck in contrast uses 10.9 mm<sup>2</sup> to implement four Zen 2 cores and 17.7 mm<sup>2</sup> for four RDNA 2 WGPs. GPU performance is often more important than the CPU side, especially in handhelds that aren’t expected to hit high frame rates. Therefore, the Ryzen Z1’s die area allocation is strange for a handheld focused chip.</p>

<p>However, packing six Zen 4(c) cores worth of CPU power is great if you need a very low power, very small chip for mobile device that focuses on productivity first and gaming second. Ryzen Z1 can serve in a handheld console in a pinch, and I don’t think ASUS made the wrong decision to use it in the ROG Ally. But I think Ryzen Z1 would be more at home in a small ultrabook or convertible. And it’d be cool to see AMD’s small APU shine in such a device.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JSTOR is Now Available in 1k Prisons (136 pts)]]></title>
            <link>https://about.jstor.org/news/jstor-available-in-1000-prisons/</link>
            <guid>39513126</guid>
            <pubDate>Mon, 26 Feb 2024 16:22:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://about.jstor.org/news/jstor-available-in-1000-prisons/">https://about.jstor.org/news/jstor-available-in-1000-prisons/</a>, See on <a href="https://news.ycombinator.com/item?id=39513126">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="tocontent">
		<article id="post-30129">
			<div data-sc="content:news">
					<h3>More than 500,000 incarcerated learners now have access to the digital library</h3>
<p>At the end of 2023, <a href="https://www.jstor.org/" rel="noopener" target="_blank">JSTOR</a>—a vast digital library of secondary and primary sources to support teaching and learning—reached a once unimaginable goal: providing JSTOR access in 1,000 prisons. Spread across four continents, the <a href="https://about.jstor.org/jstor-access-in-prison/" rel="noopener" target="_blank">JSTOR Access in Prison</a> initiative now supports the education and growth of more than 550,000 incarcerated people.  </p>
<p>Incarcerated learners have been left behind for decades. Limited access to the internet and scarce funding and support for higher education in prisons made access to digital libraries like JSTOR all but impossible. In October 2021, with funding from the Mellon Foundation, JSTOR <a href="https://www.ithaka.org/news/mellon-foundation-awards-ithaka-1-5-million-to-make-jstor-accessible-to-incarcerated-college-students/" rel="noopener" target="_blank">set an ambitious goal</a> to change that. The aspiration? For every incarcerated college student in the United States to have access to JSTOR, along with the research skills to use it and other digital resources. </p>
<p>Prior to 2021, JSTOR developed an offline index of its digital library. At the time, less than twenty prisons had access to it. Since then, developers have created an online version that meets the unique needs of carceral settings, most recently delivering online access on tablets. These changes—and the leadership of Stacy Burnett, a graduate of the Bard Prison Initiative who was hired to lead the JSTOR Access in Prison initiative—have enabled 1,000 prisons and more than 500,000 people to gain access to the digital equivalent of a college library. </p>
<p>“Creating more equitable learning environments inside prisons is the best way to pay forward my own prison-based education,” said Burnett. “We have proven that through understanding, collaboration, and creativity, we can create workable solutions that deliver meaningful digital equity and information literacy for incarcerated people.”</p>
<p>Evidence supports the fact that JSTOR use among the incarcerated is strong and growing. The students in Ohio prisons have reviewed nearly 30,000 unique articles over the past year, with 10% in the last month. Access there will be expanded to the prison general libraries. The students at Tennessee Higher Education Initiative regularly access 2,400 unique articles in a month—the highest per capita use in the country for incarcerated learners. Those pursuing higher education use JSTOR for research and coursework. Others use JSTOR to pursue a passion for learning and to gain skills they will need once they re-enter society. </p>
<p>L. Elizabeth Shatswell, a soon-to-be graduate of the University of Puget Sound, spent nearly two decades pursuing her high school and college education behind bars. She was able to conduct research on JSTOR that helped her advocate for an elderly incarcerated woman who required humane patient care. That experience changed Shatswell’s life. She said, “Prior to JSTOR, I knew full well there were disparities in access to healthcare for both women and incarcerated populations, but I did not know the scope. JSTOR facilitated the development of my voice in understanding these gaps and as a result I founded a program called the HOPE Team that uses evidence-based research to make changes to healthcare policy and practice.” </p>
<p>Prison access to JSTOR is free thanks to subsidies from ITHAKA, the nonprofit home of JSTOR, and generous grants from Ascendium and Mellon Foundation. Giving incarcerated learners access to JSTOR is impactful, but it hasn’t been easy. There are challenges in navigating different cultures and decision-making processes, and finding creative solutions to issues in technology infrastructure and support, which vary widely from one site to the next. </p>
<p>“It was really difficult in the beginning,” said Burnett. “Fortunately, we’ve worked with enough prisons now that when we encounter a question or issue, we’ve seen it before and have a solution. We’re making it easier and easier for them to say ‘yes’.”  </p>
<p>Burnett added, “After all, lowering barriers to access to knowledge and education is exactly what JSTOR and ITHAKA were founded to do.”</p>
<p>JSTOR hopes to reach the 400 American state and federal prisons without JSTOR and 1 million learners in 2024. </p>
<p>To learn more, visit <a href="https://about.jstor.org/jstor-access-in-prison/" rel="noopener" target="_blank">JSTOR Access in Prison</a>.</p>
<p><strong>Media contact:</strong><br>
Heidi McGregor<br>
VP, Communications<br>
ITHAKA<br>
<a href="mailto:heidi.mcgregor@ithaka.org" rel="noopener" target="_blank">heidi.mcgregor@ithaka.org</a>  </p>
<p><strong>About JSTOR</strong></p>
<p>JSTOR is a part of <a href="https://www.ithaka.org/" rel="noopener" target="_blank">ITHAKA</a>, a nonprofit organization with a mission to improve access to knowledge and education for people around the world. As a nonprofit that believes in the power of knowledge to change the world for the better, JSTOR partners with libraries, museums, and publishers to reduce costs, extend access, and preserve scholarship for the future as affordably and sustainably as possible. At JSTOR, we strengthen the depth and quality of research by bringing together journals, books, images, and primary sources on a platform with unique tools for teaching and exploration. We do this because we believe in the power of knowledge to  change the world for the better.</p>

				</div>

		</article>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What a major solar storm could do (113 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2024/03/04/what-a-major-solar-storm-could-do-to-our-planet</link>
            <guid>39513051</guid>
            <pubDate>Mon, 26 Feb 2024 16:17:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2024/03/04/what-a-major-solar-storm-could-do-to-our-planet">https://www.newyorker.com/magazine/2024/03/04/what-a-major-solar-storm-could-do-to-our-planet</a>, See on <a href="https://news.ycombinator.com/item?id=39513051">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Ken Tegnell’s first home was on Alcatraz. At the time—this was in the nineteen-fifties—there was, in addition to the federal penitentiary, a preschool, a post office, and housing for prison employees and their family members. That included Tegnell, who lived with his mother and grandfather, a guard, while his father was stationed in Korea. The whole of Alcatraz Island is less than a tenth of a square mile, so, despite all the security measures and “<em>DO NOT ENTER</em>” signs, the <a href="https://www.newyorker.com/news/afterword/one-of-the-last-men-who-served-time-at-alcatraz">inmates</a> and civilians were never very far apart. Yet even given the proximity to the likes of <a href="https://www.newyorker.com/magazine/2015/09/21/assets-and-liabilities">Whitey Bulger</a>, it was a peaceful place to live. The view was spectacular, almost none of the non-incarcerated residents locked their doors, and almost all of them knew one another and shared the camaraderie of an unusual identity. “We were an odd group of people,” Tegnell jokes, “and that’s why I’m strange the way I am.”</p><p>When Tegnell’s father returned from Korea, the family moved away, and then moved often. But eventually Tegnell returned to the Bay Area—this time to attend Berkeley, which, by the late nineteen-sixties, was another island of odd people. While taking an astronomy course there, he attended a lecture by a not yet famous scientist named Carl Sagan. Interested in things that happen in the sky and unmoved by the hippie culture around him, Tegnell joined the Air Force, in 1974. The military taught him to use telescopes and radio arrays, then sent him to the Learmonth Solar Observatory, at the northwestern tip of Australia, to gather data about the sun. He served two tours there, twelve hours from anything that could be called a city—a godforsaken place, as Tegnell recalls it, but gorgeous, with beautiful beaches, terrific fishing, and almost no rainfall year-round. Whether working or playing, he spent his days there looking at the sun.</p><p>That is still how Tegnell makes a living, although he hung up his wings in 1996. Today, his job is simultaneously so obscure that most people have never heard of it and so important that virtually every sector of the economy depends on it. His official title, one shared by no more than a few dozen Americans, is space-weather forecaster. Ever since leaving the Air Force, Tegnell has worked&nbsp;for the National Oceanic and Atmospheric Administration’s Space Weather Prediction Center, in Boulder, Colorado: ten hours a day, forty hours a week, three decades spent staring at real-time images of the sun. Eleven other forecasters work there as well. The remaining ones are employed by the only similar institution in the country: the Space Weather Operations Center, run by the Department of Defense on Offutt Air Force Base, in Sarpy County, Nebraska.</p><p>Regular, Earth-based weather is such a fundamental part of our lives that we are almost always aware of it and very often obsessed with it; it is the subject of everything from idle chitchat to impassioned political debate. By contrast, most people have no idea that there <em>is</em> weather in outer space, let alone what its fluctuations might mean for our planet. That’s because, unlike everyday weather, you can’t experience space weather directly. It doesn’t make you hot or cold, doesn’t flood your basement or take the roof off your home. In fact, until the nineteenth century, it had almost no appreciable effect whatsoever on human activity. Then came a series of scientific revolutions that made certain technologies, from electricity to telecommunications, central to our lives. Only later did we realize that those technologies are vulnerable to the effects of weather in outer space. The potential consequences are as sweeping as our technological dependence. In 2019, the Federal Emergency Management Agency, surveying the landscape of possible disasters, concluded that only two natural hazards have the capacity to simultaneously affect the entire nation. One is a <a href="https://www.newyorker.com/tag/coronavirus">pandemic</a>. The other is a severe solar storm.</p><p>That is why Tegnell’s job is so important. But “space-weather forecaster” is an optimistic misnomer; for the most part, he and his colleagues can’t predict what will happen in outer space. All they can do is try to figure out what’s happening there right now, preferably fast enough to limit the impact on our planet. Even that is difficult, because space weather is both an extremely challenging field—it is essentially applied astrophysics—and a relatively new one. As such, it is full of many lingering scientific questions and one looming practical question: What will happen here on Earth when the next huge space storm hits?</p><p>The first such storm to cause us trouble took place in 1859. In late August, the aurora borealis, which is normally visible only in polar latitudes, made a series of unusual appearances: in Havana, Panama, Rome, New York City. Then, in early September, the aurora returned with such brilliance that gold miners in the Rocky Mountains woke up at night and began making breakfast, and disoriented birds greeted the nonexistent morning.</p><p>This lovely if perplexing phenomenon had an unwelcome corollary: around the globe, telegraph systems went haywire. Many stopped working entirely, while others sent and received “fantastical and unreadable messages,” as the Philadelphia <em>Evening Bulletin</em> put it. At some telegraph stations, operators found that they could disconnect their batteries and send messages via the ambient current, as if the Earth itself had become an instant-messaging system.</p><p>Owing to a lucky coincidence, all these anomalies were soon linked to their likely cause. At around noon on September 1st, the British astronomer Richard Carrington was outside sketching a group of sunspots when he saw a burst of light on the surface of the sun: the first known observation of a solar flare. When accounts of the low-latitude auroras started rolling in, along with reports that magnetometers—devices that measure fluctuations in the Earth’s magnetic field—had surged so high they maxed out their recording capabilities, scientists began to suspect that the strange things happening on Earth were related to the strange thing Carrington had seen on the sun.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a28305&quot;}" href="https://www.newyorker.com/cartoon/a28305" rel="noopener" target="_blank"><picture></picture></a><p><span>“This recipe turned out awful despite my substituting every major ingredient.”</span></p><p><span>Cartoon by Mads Horwath</span></p></div></span></p></figure><p>Wonderment over the Carrington Event, as it is now known, faded almost as quickly as the auroras—but sixty years later it happened again. In May, 1921, dazzling lights filled the night sky in places as far from the poles as Texas and Samoa; this time, too, spectacle was followed by debacle. “Electric fluid” leaping from a telegraph switchboard set on fire a railroad station in Brewster, New York, while stray voltage on railway signal and switching systems halted trains in Manhattan and, farther north, started a fire at Albany’s Union Station.</p><p>Over the years, at odd intervals, this pattern kept repeating: brilliant night skies followed by troubling consequences, which changed in concert with evolving technologies. Teletype machines ceased to operate; or transatlantic cables stopped working; or worldwide radio circuits fell silent; or hundreds of thousands of miles of transmission lines used to send and receive wire stories all went down at the same time. In May, 1967, all three radar sites of the Ballistic Missile Early Warning Systems then maintained by the U.S. Air Force appeared to have been jammed; worried that the Soviet Union was on the verge of attacking, military officials nearly scrambled nuclear-equipped aircraft. Five years later, during the Vietnam War, the United States started sowing the waters outside North Vietnamese seaports with mines that had magnetic sensors, to trigger explosions when steel-hulled vessels passed overhead. Three months after that program began, many of those mines—four thousand of them, according to one contemporaneous source—detonated almost simultaneously. An investigation determined that the plan had been compromised not by Hanoi but by a newly discovered solar phenomenon called a coronal mass ejection.</p><p>In time, aided by each new technological difficulty, astrophysicists began to piece together a better understanding of the weather in outer space. But science can take a long time to make inroads into public awareness, let alone public policy, so space weather remained a mostly marginal subject until 2008, when the National Academy of Sciences convened a group of experts to assess the nation’s capacity to endure its terrestrial effects. Later that year, the N.A.S. published a <a href="https://nap.nationalacademies.org/catalog/12507/severe-space-weather-events-understanding-societal-and-economic-impacts-a">report</a> on the findings, “Severe Space Weather Events: Understanding Societal and Economic Impacts.”</p><p>The title was dry; the contents were not. The report noted that the Earth hadn’t experienced a Carrington-size storm during the space age, or, for that matter, during the age of widespread electrification, and that much of the country’s critical infrastructure seemed unlikely to withstand one. Extensive damage to satellites would compromise everything from communications to national security, while extensive damage to the power grid would compromise <em>everything:</em> health care, transportation, agriculture, emergency response, water and sanitation, the financial industry, the continuity of government. The report estimated that recovery from a Carrington-class storm could take up to a decade and cost many trillions of dollars.</p><p>That report made headlines, and also made its way to <a href="https://www.newyorker.com/tag/barack-obama">President Barack Obama</a>—who by then had appointed a new <em>FEMA</em> administrator, a man named Craig Fugate. At the time, very few people even within the emergency-response community knew much about space weather. But, by chance, Fugate had crossed paths with the Space Weather Prediction Center earlier in his career; interested in the center’s work, he had made himself into something of a space-weather expert.</p><p>As a result, when the White House came knocking to ask if it should be concerned about the N.A.S. report, Fugate was in a position to offer an emphatic yes. The question, for him, wasn’t whether a major solar storm posed a risk to the nation; it was how best to prepare for it beforehand and recover from it afterward. And so, as he began settling into his job, and getting to know the rest of the senior leaders at <em>FEMA</em>, he made a habit of presenting them with a hypothetical situation. “I asked them what they would do if there was a G5 storm,” Fugate told me, referring to the highest classification on the <em>NOAA</em> Space Weather Scale, akin to an F5 tornado or a Category 5 hurricane. “And they go, ‘What’s a G5 storm?’&nbsp;” <em>Hoo boy</em>, Fugate remembers thinking. <em>We got a problem</em>.</p><p>In space weather, every day is a sunny day. There is no interstellar rain, no interplanetary snow, no sleet spinning off the rings of Saturn; all the phenomena we call space weather originate on the sun. And so, to start, you must shed the idea—implicit in our meteorology and omnipresent in our metaphors—that the sun is a mild and beneficent force, a bestower of good moods and great tans.</p><p>In reality, the sun is an enormous thermonuclear bomb that has been exploding continuously for four and a half billion years. Its inner workings are imperfectly understood even by heliophysicists, who sometimes sound less like scientists than like nineteen-fifties comic-book heroes, enthusiastically invoking things like flux tubes and convection zones and galactic-cosmic-ray dropouts. Fortunately, for our purposes, the only two solar phenomena you need to understand are solar flares and coronal mass ejections, both of which stem from the same thing: a buildup of energy in the magnetic field of the sun.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>You are probably familiar with the Earth’s magnetic field, which makes all life here possible by deflecting dangerous radiation from outer space. If you could see that field, it would look like a relatively tidy series of rings surrounding our planet, flowing out at the South Pole and reëntering at the North. The solar magnetic field does not look like that. That’s largely because, although the sun is three hundred thousand times more massive than the Earth, no part of it is solid. Instead, it is made of plasma, that strange and mesmerizing fourth state of matter. (Heat up a liquid and it turns into a gas. Heat up a gas and it turns into a plasma, a glowing slurry of electrically charged particles.) As a result, the sun doesn’t have to rotate rigidly, as our planet must. One rotation of the Earth takes twenty-four hours in both Ecuador and Antarctica, but one rotation of the sun takes approximately twenty-five days at its equator and thirty-three days at its poles.</p><p>This uneven rotation wreaks havoc on the sun’s magnetic field. Imagine a race in which eight people are lined up on a track, holding on to the same long elastic ribbon. The starting gun fires and the people start running. The two in the middle are the fastest and the two on the ends are the slowest, so after a while the middle two are far ahead and the ribbon looks like this: &gt; . If the race kept going and the runners’ speeds remained constant, the two middle runners would eventually lap the others, and the ribbon would cross over itself. The longer the race lasted, the more tangled the ribbon would become.</p><p>That’s what happens to solar-magnetic-field lines. They twist and crisscross until clusters of them pop up from the sun’s surface, in huge loops that generate enormous amounts of energy. (Think of the energy stored in a rubber band when it is twisted and stretched. Now imagine that the rubber band is a hundred thousand miles long.) The ends of these loops are sunspots, the phenomenon that Carrington observed in 1859. He could see them readily enough for two reasons. The first is that they are darker than their surroundings, because they are a couple of thousand degrees cooler; the intensity of their magnetic fields hinders the flow of hot gas across the sun. The second is that they are large. An average sunspot is the size of the Earth, while the biggest ones can be ten times larger.</p><p>Forecasters like Ken Tegnell watch sunspots for the same reason that regular <a href="https://www.newyorker.com/magazine/2019/07/01/why-weather-forecasting-keeps-getting-better">meteorologists</a> watch low-pressure areas in the tropics: to see if a storm is forming. This happens when one of those twisted magnetic fields suddenly rips apart, then snaps back together again. That rearrangement returns the magnetic field to a more stable, lower-energy state, while releasing the excess energy into space in two different forms. The first is a solar flare: a burst of radiation that can range across the electromagnetic spectrum, from gamma rays and X rays to radio waves and visible light. Solar flares contain a colossal amount of energy—enough, in a large one, to meet our planet’s power needs for the next fifteen or twenty thousand years. The second is a coronal mass ejection: a billion-ton bubble of magnetized plasma that explodes off the surface of the sun. These two phenomena can occur separately, but when large ones occur together they mark the beginning of a major solar storm.</p><p>The forecasting room of the Space Weather Prediction Center is a dimly lit ground-floor office with no exterior windows. Nonetheless, in a sense, sunlight is everywhere. Banks of monitors run the length of one wall, filled with real-time images of the sun. Some show only the disk, others only the corona, others the entire star filtered through different wavelengths of light, turning it pale pink and brilliant yellow, electric blue and neon green. Two large images in the center show the sun as a writhing riot of orange and gold, the loops and filaments of its magnetic field lines rendered visible not by scientific instruments but by its own plasma, which is drawn to those field lines the way iron filings are drawn to bar magnets. Viewed this way, the sun does not make you want to grab a paperback and lie in a hammock. It looks like a volcanic eruption as seen from deep inside the caldera; it looks like a wildfire raging beneath forty billion hurricanes; it looks like, when it is over, there will be no survivors.</p><p>Surrounded by all of this, unfazed, Tegnell is logging in for his shift. In the hallway just outside, a mannequin stands upright in a <em>NASA</em> uniform. The uniform is the old-school, pale-blue kind, and the mannequin is pale and old school, too—crewcut, chisel-jawed, permanently twentysomething. Tegnell does not look like that. Bigger, bearded, older, he looks like the guy in the disaster movie who has the right combination of grit, experience, and indifference to authority to save the day. At present, he is eye level with a brace of computers, the screen of each one covered in flowing lines, as if the solar system were hooked up to half a dozen heart-rate monitors.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a22925&quot;}" href="https://www.newyorker.com/cartoon/a22925" rel="noopener" target="_blank"><picture></picture></a><p><span>“O.K., so if we share a ride and cut out all the singing, we just might be able to make it to the Emerald City in time for happy hour.”</span></p><p><span>Cartoon by Lonnie Millsap</span></p></div></span></p></figure><p>Some of the information filling those screens comes from terrestrial observatories, like the one where Tegnell used to work. The rest comes from space-based equipment on satellites, managed, variously, by <em>NASA</em>, <em>NOAA</em>, and the European Space Agency. Most of those satellites are in orbit twenty-two thousand miles above the Earth, a hundred times farther away than the International Space Station; a few are in orbit a million miles away, or about one per cent of the distance to the sun. From these outposts, they transmit data to the forecasting room, where it is Tegnell’s responsibility to interpret the contents, detect anything unusual, issue twice-daily forecasts, and, when necessary, activate a suite of watches and warnings.</p><p>Tegnell loves his job best when nothing is happening in the room—no groups of engineers trekking through, no stray journalists hanging around—but when many things are happening up in the sky. That makes some stretches of his professional life duller than others, because sunspots follow an eleven-year cycle, during which their activity goes from infrequent (solar minimum) to frequent (solar maximum). We are currently headed toward solar maximum, with activity on the sun expected to peak sometime between now and 2025. That cycle is not wholly determinative; a solar maximum can pass by uneventfully, while a powerful storm can happen during solar minimum.</p><p>Still, solar maximum does tend to make Tegnell’s job more interesting. As we talk, an automated voice keeps informing him that a flare has been detected, with the same impassive insistence of Siri saying, “Proceed to the route.” Tegnell ignores it, having already determined that the flare is too small to produce any effects on Earth, except possibly some auroras for people living near polar latitudes. (Auroras are the only pleasant by-product of charged particles entering our atmosphere, where they’re channelled north and south along magnetic-field lines and interact with nitrogen and oxygen molecules, causing them to produce interesting colors.) But then something else leaps off the edge of the sun: a fountain of plasma that looks, to my untrained eye, enormous. “It <em>is</em> enormous,” Tegnell affirms. “It’s just incredible.” It is not, however, headed toward the Earth.</p><p>“I know,” Tegnell’s colleague Bill Murtagh says as he watches me watching. “It’s stunning. I’ve been doing this for twenty-five years and I’ve never yet found it boring.” Like Tegnell, Murtagh arrived at the Space Weather Prediction Center via the U.S. Air Force, albeit more circuitously, as his Irish accent suggests. (He owes his American citizenship to the fact that he was born during a parental stint in the U.S., where his mother worked for Ogden Nash, taking care of his grandchildren.) Unlike Tegnell, he enjoys collaborating with other people. At <em>swpc</em>—which is pronounced “swipsy,” like “tipsy”—he coördinates space-weather-preparedness efforts with government officials, emergency managers, and the private sector, and he doesn’t mind being loaned out to the White House Office of Science and Technology Policy and working with the National Security Council. When a big storm starts materializing on one of the monitors in the forecasting room, it is Ken Tegnell’s job to notice. It is Bill Murtagh’s job to help minimize the storm’s impact on everything it might derail, damage, or destroy.</p><p>That is a long list, because solar storms affect a broad, strange swath of the human endeavor. For instance, outside the <em>swpc</em> forecasting room, in a glass case displaying old astronomical devices and a statue of a sun god, there is a life-size model of a homing pigeon. Pigeons navigate partly by tracking the Earth’s magnetic field; when it behaves in uncharacteristic ways, a pigeon race can end in a “smash,” the term of art for events in which many birds fail to return home. Since the most highly prized pigeons can be worth more than a million dollars, some pigeon racers have become dedicated subscribers to <em>swpc</em>’s space-weather alerts. Other constituents are interested for even more arcane reasons. One of Murtagh’s favorite phone calls came from a man who wanted to know if it was true that solar storms could interfere with G.P.S. signals. When Murtagh said yes, the man had a follow-up question: How did those storms affect electronic ankle bracelets? (“You know,” Murtagh told the caller, “I’m not too familiar with that technology.”) But the sectors that bear the brunt of bad space weather are anything but niche interest groups. They are the backbone of modern society: telecommunications, aviation, space-based technology, and the power grid.</p><p>Most solar storms do not hit the Earth, for the same reason that most baseballs don’t hit one particular person in the stands. But, when a storm does get here, it gets here fast. Some of the radiation from the solar flare arrives in a little more than eight minutes: the amount of time it takes anything travelling at the speed of light to cross the ninety-three million miles between us and the sun. All that energy smacking into our atmosphere further ionizes the ionosphere, its upper reaches. The result, in a severe storm, is a partial blackout of low-frequency radio wavelengths and a complete blackout of high-frequency wavelengths across the entire side of the Earth that’s facing the sun. Those blackouts, which can last up to several hours, disrupt ham radios, AM radio, ground-to-submarine communications (used by the Navy), backup ground-to-air communications (used by both military and civilian flights), and other backup communication, navigation, and timing systems used for military, government, and maritime purposes.</p><p>That is the first phase of a solar storm. Meanwhile, from the moment they formed, the flare and the coronal mass ejection began transferring energy to any protons and electrons in their path, accelerating them to relativistic or near-relativistic speeds. When those enhanced protons and electrons, known as solar energetic particles, reach our atmosphere, sometimes in just tens of minutes, they form the second phase, known as a solar-radiation storm.</p><p>As that name suggests, a solar-radiation storm can harm humans, although only if they happen to be up in the sky while such a storm is taking place. For people on airplanes flying routes over the poles (where energetic particles, following magnetic-field lines, tend to concentrate), that risk is minor; nonetheless, such flights get space-weather reports from <em>swpc</em> before takeoff, and will typically reroute if a big storm is expected. For astronauts, however, severe radiation storms are more of a concern. Those on the International Space Station benefit from the attenuated but still extant protection of the Earth’s magnetic field, and during extreme radiation events they can take cover in the better-shielded parts of the station. But for those beyond our atmosphere such a storm could be lethal, either immediately or because radiation sickness would render them unable to perform life-critical functions. One obstacle to some of the space exploration currently being contemplated is that the moon and Mars lack a magnetic field to deflect the sun’s radiation; as a result, absent adequate shelter, both are extremely dangerous in a solar storm. Only retroactively did it become apparent how lucky <em>NASA</em> was that no such storms happened during the Apollo missions.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>At the moment, though, the number of people in outer space—fewer than a dozen—pales in comparison with the number of satellites in outer space: more than eight thousand. Like us, those satellites are imperilled by solar-radiation storms. For one thing, solar energetic particles can pass straight into the satellites, physically damaging hardware and hijacking software by randomly changing ones to zeros or zeros to ones. For another, as those particles bombard a satellite, different parts of it can build up different levels of charge, and the electricity can arc from one area to another, attempting to neutralize itself and, in the process, damaging or disabling the onboard electronics.</p><p>Finally, enhanced solar radiation increases the density of certain regions of the Earth’s atmosphere, which increases the drag. This is particularly problematic in lower Earth orbit (up to about twelve hundred miles above the surface of our planet), where more than eighty per cent of all satellites are found. As drag increases, those satellites can shift out of place, leaving both their owners and the North American Aerospace Defense Command scrambling to find them in order to maintain functionality, prevent collisions, and avoid confusion about their identity: unidentified intruder or old friend in a new place? At best, satellites experiencing this drag must use more fuel to maintain orbit, thereby shortening their life spans; that’s why, back in 1979, Skylab crashed to Earth sooner than expected. At worst, they lose orbit entirely, burning up on reëntry. In February of 2022, SpaceX, the space-exploration company co-founded by <a href="https://www.newyorker.com/tag/elon-musk">Elon Musk</a>, launched forty-nine new satellites as part of its <a href="https://www.newyorker.com/magazine/2023/08/28/elon-musks-shadow-rule">Starlink system</a>, which aims to provide sky-based Internet access to paying customers anywhere on Earth. The company knew that a storm had started just before the launch date, but it was a mild one—a G2, the second-lowest category on <em>NOAA</em>’s geomagnetic storm scale—and internal modelling suggested that the satellites would be fine. One day after launch, thirty-eight of them lost orbit and suffered catastrophic failure.</p><p>SpaceX still plans to launch tens of thousands of satellites in the coming years, and other entities are likewise expanding their fleets, deploying space-based technology for everything from wildlife tracking to intelligence gathering. But, of all the satellites in the sky right now, none are more crucial than those which constitute our Global Positioning System—or, to use the more universal term, G.N.S.S., the Global Navigation Satellite System.</p><p>G.P.S. satellites are not endangered by drag, because they are not in lower Earth orbit; up where they hang out, there is not enough atmosphere left to affect them. But, to reach receivers on the ground, signals from those satellites must cross some twelve thousand miles of space. During a solar storm, when our ionosphere is disturbed, those signals get distorted, much the way light bends when it passes through water, leading to location inaccuracies of tens or, in rare cases, hundreds of metres. Those inaccuracies generally self-correct when the storm subsides, and they don’t really matter if you’re using G.P.S. just to remind yourself which exit to take for the airport. But an increasing number of processes require constant access to ultra-precise location data, including military operations, aviation, crop management, bridge building, and oil and natural-gas exploration, especially off deep-sea platforms, where exact positions must be maintained during underwater drilling operations regardless of wave action and drift.</p><p>The more important service provided by the Global Positioning System, however, is not about space but about time: every G.P.S. satellite carries multiple atomic clocks, normally accurate to within a billionth of a second, which transmit hyperaccurate temporal information known as G.P.S. timing signals. Those signals are one of our most essential pieces of invisible infrastructure. Cell-phone companies use them to manage the flow of data over their networks. Media companies use them to broadcast programs, chopping up large data streams into smaller packets to transmit them, then recombining them upon arrival based on the time stamp. Power companies use them to help regulate the flow of electricity from source to destination, protecting against surges and blackouts. Computer applications use them to coördinate any situation in which two or more users are working on the same project in different locations. The financial industry uses them to track mobile banking transactions and to time-stamp every trade—a crucial traffic-control system in a world where hundreds of thousands of financial messages are processed every second.</p><p>Like G.P.S. location accuracy, G.P.S. timing accuracy can suffer during a solar storm. The longer and more severe the storm, the more those errors compound, until the systems that depend on the signals no longer work correctly, or work at all. Backup programs are available; the Federal Aviation Administration, for instance, has alternative capabilities to keep planes flying safely when G.P.S. fails. Over all, though, incorporation of such alternatives remains limited, for a straightforward reason: G.P.S. is a service that our federal government provides free of charge. As the Department of Homeland Security dryly noted in a 2020 report, “Without regulatory requirements or positive benefit-cost equations, adoption of non-G.N.S.S. services is unlikely.”</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a22632&quot;}" href="https://www.newyorker.com/cartoon/a22632" rel="noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Carolita Johnson</span></p></div></span></p></figure><p>In the meantime, our primary source of navigation and timing information remains vulnerable to the vicissitudes of weather on the sun. So do the thousands of other satellites that increasingly fill our skies, courtesy of a young, booming, and largely unregulated industry. This worries the generally unflappable Bill Murtagh. “It’s a Wild West out in space right now,” he says. His assessment of satellite companies is blunt: “I do not think they are ready for a major space-weather event.” If he is right, when that event happens, large portions of our life could be compromised: information, communication, entertainment, economic activity, national security. But all those are our vulnerabilities just in the sky. By most accounts, when the next extreme space storm hits, the real problems will be the ones on the ground.</p><p>If a solar flare is something like the muzzle flash of a cannon, a coronal mass ejection is the cannonball: slower, but more destructive. It takes anywhere from fifteen hours to several days to reach our planet, by which time it has expanded enormously in volume. Once it arrives, it smashes into our magnetosphere, flattening whichever side is facing the sun (that is, the daytime side) and sending the nighttime side streaming away from the Earth, like a wind sock in a gale. If you remember Faraday’s law, you know that moving a magnetic field around produces an electric current. And so it is ultimately the Earth’s own storm-tossed magnetosphere that induces excess electricity in our planet, thereby initiating the third and final phase of a space-weather event: the geomagnetic storm.</p><p>Although that storm can affect anything long and metal (pipelines, railroad tracks), it poses the gravest danger to power grids. In the United States, our grid is divided into three regions. The Eastern Interconnection runs from the East Coast to the Rocky Mountains; the Western Interconnection runs from the Rockies to the Pacific Ocean; Texas, in true Lone Star style, goes it alone. For the most part, power can’t flow from one region to another—which is why, when seventy-five per cent of Texas suffered blackouts during a winter storm in 2021, no outside energy providers could help. But, within each region, electricity flows freely—and so can electrical problems, as when, in 2003, a shorted power line in Ohio caused a blackout across much of the Midwest, the mid-Atlantic, and the Northeast, leaving fifty-five million people in the dark.</p><p>All this infrastructure, which continues across the border into Canada to form the North American Power Grid, is also known as the bulk-power system, because it handles energy transmission, not energy distribution. Distribution involves sending electricity from a local substation to everything nearby that needs it—schools, stoplights, factories, the toaster in your kitchen. Transmission gets power <em>to</em> that substation, from one of the more than six thousand generation facilities on the North American grid (nuclear plants, hydroelectric dams, solar farms, etc.), via more than half a million miles of line.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The crucial nodes in this vast network are transformers. Power enters your home at a hundred and ten volts, but voltage that low can’t be sent from a coal plant in West Virginia to your laptop charger in Alexandria; too much energy (in the form of heat) would be lost in transit. Instead, a transformer at the power plant ramps up the electricity to hundreds of thousands of volts, so that it can be transferred efficiently over long distances; once it reaches a substation, another transformer ramps the voltage back down until it can safely enter your home. Whatever its voltage, all that power flows through the grid as alternating current, moving at a constant frequency of sixty hertz.</p><p>Hold that thought; here comes the coronal mass ejection. It smacks into our magnetic field, warping it—or, in severe storms, temporarily ripping part of it open—and setting in motion the chain of events that sends additional electric charge into the planet. Some of that charge, which is known as geomagnetically induced current, dissipates harmlessly, because it flows into a part of the Earth that excels at conducting electricity—salt water, say, or sedimentary rock. But, in places where the underlying rock is a poor conductor, the current must go elsewhere. Like all current, it follows the path of least resistance, and the least resistant path of all is the one designed to conduct electricity: the power grid.</p><p>By unfortunate chance, some of the least conductive bedrock in the United States is the very old metamorphic and igneous rock of the Appalachian Mountains and the New England Highlands—the geological substrates of Boston, New York, Philadelphia, Washington, D.C., and much of the rest of the Eastern Seaboard, home to half the country’s population. As detailed hazard maps recently created by the geophysicist Jeffrey Love and a team of his colleagues at the United States Geological Survey show, some other parts of the country, notably the Midwest, are likewise vulnerable to geomagnetically induced currents.</p><p>What makes these currents so disruptive is not their strength—they are actually quite weak—but their form. The power grid is built for alternating current, but geomagnetically induced currents are basically direct. The collision of these two currents can lead to the inability to transfer power efficiently, large temperature spikes inside transformers (which emit unholy groans and bangs under the strain), relays and other equipment tripping off-line, and, on a very bad day, voltage collapse. Mark Olson, a member of <em>NOAA</em>’s Space Weather Advisory Group and a manager of reliability assessments at the North American Electric Reliability Corporation—the nonprofit agency tasked by the Federal Energy Regulatory Commission and Canada’s provincial governments with keeping the continent’s power grid sound and secure—summed this up for me succinctly: “blackout.”</p><p>This can all happen almost instantly. On March 13, 1989, a coronal mass ejection struck the Earth; within ninety seconds, transformers on the Quebec power grid malfunctioned, dozens of safety mechanisms failed, and the entire grid shut down, leaving almost a quarter of the population of Canada in the dark. That geomagnetic storm—which also triggered outages in the U.K. and Sweden, destroyed a transformer at a nuclear power plant in New Jersey, and caused at least two hundred other issues on the North American grid alone—was strong, but not exceptionally so. Based on magnetometer readings, auroral latitudes, and other fingerprints left behind by solar storms, scientists now believe that at least three storms in the past hundred and fifty-odd years—the Carrington Event and others in 1872 and 1921—were roughly an order of magnitude more powerful.</p><p>All three of those storms took place before the power grid existed. The question that troubles space-weather experts—and divides them—is what will happen the next time a comparable one strikes. Some people think that the Quebec event was a wake-up call—the perfect-sized storm, really, large enough to teach a lesson without being large enough to cause a catastrophe. But, per the N.A.S. report, any gains following the Quebec storm were offset by trends in America’s bulk-power system, which came to rely on ever-larger amounts of power travelling through ever-longer transmission lines. A study commissioned by the federal government and summarized in the report found that a storm the size of the 1921 event would cause large regions of the grid to fail, with impacts that “would be of unprecedented scale and involve populations in excess of 130 million”—close to half of all Americans. The report estimated the cost of a storm like that as “$1 trillion to $2 trillion during the first year alone&nbsp;.&nbsp;.&nbsp;. with recovery times of four to ten years.”</p><p>Fifteen years later, some experts believe <em>that</em> was the wake-up call: that the 2008 report, in its sober-minded scariness, inspired reforms that will make the next severe solar storm more nuisance than nightmare. Bill Murtagh worries about satellite companies, but he thinks that most power companies take space weather seriously and are doing their best to prepare for it. Mark Olson, of the North American Electric Reliability Corporation, concedes that solar storms present “a very challenging risk” to the energy sector, not least because we still know relatively little about them. But, he says, when a major one happens, “the North American grid won’t be taken by surprise.” And he points to a federal directive that, as of this January, requires every provider of bulk power to have a plan in place to deal with a “benchmark geomagnetic disturbance event.”</p><p>That directive is important, but the benchmark itself is troubling. It was established by using thirty years of magnetic-field data to extrapolate the likely magnitude of a once-in-a-century storm. The resulting standard is clear, uniform, achievable, extremely useful during most solar storms, and wholly inadequate for severe ones. As Olson acknowledged, the federal benchmark is now widely believed to be weaker than the Carrington Event.</p><p>That wouldn’t matter if the Carrington storm were an outlier, likely to happen only once every several hundred years. But, in reality, it might not even have been the worst storm of the nineteenth century; the one in 1872 was at least as strong. We also know, from data collected by satellite, that a more powerful storm narrowly missed the Earth in 2012. As that suggests, an extreme geomagnetic storm—the <em>swpc</em> people call it a G5-Plus, at the upper threshold of the highest <em>NOAA</em> category of severity—could be a more common event than previously thought. Some scientists now believe there is an approximately twelve-per-cent chance of one striking the Earth in the next decade.</p><p>That scares some experts. One of the eminences in the field of space-weather studies is Daniel Baker, who was the head of space-plasma physics at Los Alamos National Laboratory and a division chief at <em>NASA</em>’s Goddard Space Flight Center before going to the University of Colorado to lead its Laboratory for Atmospheric and Space Physics. “I do not want to be unduly alarmist,” Baker told me. “But I <em>do</em> want to be duly alarmist.” Like so much American infrastructure, he notes, our bulk-power system is underfunded and aging, while demand on it keeps rising—not only from population growth but from an incommensurate increase in our energy use. As a result, he says, the grid is operating “closer and closer to its maximum stress level.” In that condition, it cannot easily absorb the additional stress of a solar storm.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a21843-rd&quot;}" href="https://www.newyorker.com/cartoon/a21843-rd" rel="noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Emily Bernstein</span></p></div></span></p></figure><p>Our aging grid could be updated, but the factors that make doing so expensive and time-consuming will also dramatically compound the effects of a severe solar storm. “Transformers are not just something you can go to Home Depot and buy,” Baker points out; each one is idiosyncratic, a half-million-pound object designed specifically for one of the fifteen hundred-plus entities, from publicly traded companies to energy coöperatives, that together constitute the power grid. As a result, transformers can’t be stockpiled. They are almost always built to spec, and they are almost all made abroad, which increases shipping times and leaves them vulnerable to political conflict and supply-chain issues. Even under optimal circumstances, the typical lead time to replace a transformer is at least a year. If enough of them fail in a solar storm, the recovery will not be measured in days (the length of time it took to get the power back after the <a href="https://www.newyorker.com/news/dispatch/texans-in-the-midst-of-another-avoidable-catastrophe">Texas winter storms</a>) or weeks (the length of time it took after <a href="https://www.newyorker.com/tag/hurricane-katrina">Hurricane Katrina</a>). It will be measured, almost unthinkably, in months and years.</p><p>That’s one reason Craig Fugate, the former <em>FEMA</em> administrator, thinks the one-to-two-trillion-dollar figure in the N.A.S. report is “probably on the low side.” But he also raises a problem that extends beyond the power grid: because solar storms affect an unusually wide geographic area and an unusually broad range of technologies, they are more likely than other disasters to cause cascading failures. A malfunction in one part of the grid forces electricity to flow elsewhere, overburdening a second part, which is then more likely to malfunction as well; the more such problems you string together, the greater the burden on the remaining parts, and the more likely a catastrophic failure. And what is true of the disaster is also true of the disaster response. Unlike terrestrial hazards, solar storms are not, in <em>FEMA</em>-speak, “geofenced.” They can affect large areas of the world, which minimizes access to outside help in the aftermath. If an earthquake devastates Los Angeles, aid can pour in from neighboring regions. But, if a solar storm devastates New York, anywhere close enough to help will likely be devastated, too</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Above all, Fugate fears that, because space weather affects so many technologies, a severe storm could expose dependencies among them that we did not fully appreciate, or did not recognize at all. Our vast and interrelated technological infrastructure could turn out to harbor a single point of failure—a component, no matter how central or trivial, whose malfunction shuts the whole thing down. Many experts regard G.P.S. signals with alarm for this reason; as a 2021 report by the National Security Telecommunications Advisory Committee noted, the signals are used so ubiquitously in so many critical sectors that “their vulnerabilities pose a near-existential threat.” Alternatively, an individual system that seems robust in isolation might not respond as expected when other systems to which it is connected simultaneously experience powerful stressors—especially when those stressors involve, as Fugate put it, “more unknowns than knowns.” That is true not only of technology but also of the people who operate it; we do not always perform at our best when things around us start malfunctioning. In this kind of “system of systems,” even seemingly minor problems can concatenate in calamitous ways.</p><p>Baker worries about this as well. “We’ve built ourselves into a cyber-electric cocoon,” he says, “and a lot of risk analyses show that when you start to lose nodes in that kind of a connected system it can propagate in very unpredictable ways. And there’s nothing outside it.” In a closed loop like that, a disaster is disastrous not only because of the problems it causes but because of the solutions it eliminates. Post-disaster relief and recovery operations rely on functional transportation systems, but airports, railroads, gas pumps, stoplights, and an increasing number of vehicles all need electricity. Emergency dispatchers rely on sophisticated communication and mapping technologies, but those technologies rely on working computers and satellite transmissions. Power companies need water supplies, but water companies need electricity. Knock over the wrong domino and down goes, as the N.A.S. report put it, “just about every critical infrastructure including government services.” Baker, who led the team behind the report, suspects that we will see a devastating storm within a few decades, and that most of us alive today will suffer through those serial failures. “Maybe here in Colorado, we can go out and hunt elk or something,” he says. “But I’d be very concerned about the major metropolitan areas.”</p><p>All these problems have a meta problem. Radio blackouts, communication disruptions, power-grid problems: to an uncanny degree, solar storms mimic malicious actors trying to sabotage technology that is central to our economy and safety. Because of this, one of the most important functions of <em>swpc</em> and the Defense Department’s Space Weather Operations Center is attribution—determining whether a given anomaly was caused by bad weather in space rather than by a technical malfunction or deliberate interference. Such determinations must be accomplished quickly: if you have a radar system that’s jammed or a missile-defense system that’s malfunctioning, you can’t wait around for long to figure out why. “When we see something, we’ve got five to ten minutes or less to get this stuff out,” Tegnell says. Delay can be disastrous; in matters of national security, Murtagh notes, “a lot can happen in ten to fifteen minutes.”</p><p>In part to facilitate these assessments, <em>swpc</em> makes all its space-weather information publicly available. “We have no problem sharing information across the world,” Murtagh told me. The U.S. has a vested interest in the global community not mistaking natural hazards for foreign adversaries; for that matter, given international supply chains and international commerce, the United States has a vested interest in the global community minimizing disruptions from solar storms. Whether it can do so is impossible to say; we don’t even know how prepared the U.S. is, and the world is the ultimate system of systems, as we all learned at great cost from the pandemic. But it is difficult to be optimistic. For many nations, especially in the developing world, better space-weather preparedness is low on the list of priorities for infrastructure improvements.</p><p>And yet, precisely because solar storms can cause the same problems as enemy agents, better space-weather preparedness amounts to better preparedness over all. “I think of space weather as a stand-in for all those other disruptions,” Kathryn Draeger, an agronomist at the University of Minnesota who researches how to mitigate the impact of solar storms on agriculture, told me. “A terrorist attack on our grid, an electromagnetic pulse, a natural disaster, a pandemic—if we can figure it out for space weather, we will be better protected from all these other major disruptions.”</p><p>In theory, we’ve already figured out some of it. We could require backup navigation and timing systems; we could move away from ultra-long, ultra-high-voltage transmission lines. Certain new technologies could help, such as devices that block geomagnetically induced currents from entering the grid, as could a return to some old ones. The Army, concerned about overreliance on vulnerable technologies, has reinstated courses in orienteering, and the Navy has resumed teaching sailors how to use a sextant.</p><p>Still, persuading people to implement safety measures is difficult, because severe solar storms are what people in emergency management sometimes call low-frequency, high-consequence events. Such events are emotionally, ethically, and pragmatically vexing, and we respond to them in curious and inconsistent ways. In our private lives, we tend to focus on the high consequences: your nine-year-old will almost certainly not be kidnapped while playing alone at the local playground, but you don’t let him do so, because the potential cost is too devastating. By contrast, corporations and nations tend to focus on the low odds, and therefore wave away the possible consequences. “I’m working with people and they’ll say, ‘Why do I need to spend a cent on this issue? I’ve been here for forty years and I’ve never seen a problem,’&nbsp;” Murtagh told me. “And I look at them and say, ‘I don’t know what to say to you.’&nbsp;” As far as the sun is concerned, “the Carrington Event happened one second ago. And it will happen again.”</p><p>We don’t know when, of course; there is so much we do not know. Before Tegnell became a space-weather forecaster, he was a regular-weather forecaster, and he remains acutely aware of the difference between them. It’s not just that you have to go from thinking on the scale of cities and counties to thinking on the scale of millions of miles. It’s that with solar events “you have no idea what goes on in ninety per cent of them.” Space-weather forecasting, he believes, is where terrestrial meteorology was seventy-five years ago. Back then, we were farther from today’s reality, of minute-by-minute weather information on your phone, and closer to the reality of sixteenth-century mariners or third-century shepherds, for whom hurricanes and blizzards happened more or less out of nowhere, and for whom our vulnerability to severe weather seemed immutable and inevitable, laid down as our lot in life since that first Biblical flood.</p><p>Someday, Tegnell says, our current understanding of space weather will seem similarly sparse. We will put more and better instruments in space; we will learn more about the physical dynamics of the sun and their effects here on Earth. Whether infrastructure improvements will keep pace with that knowledge is beyond his job description, and beyond his ken. He is hoping to retire this year, after half a century of service to the United States. He is not worried about being bored. He has spent a lifetime studying solar activity and doesn’t figure that will change all that much. “I’m the kind of guy,” he told me, “who likes looking at sunsets.”&nbsp;♦</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to find the AWS account ID of any S3 bucket (472 pts)]]></title>
            <link>https://tracebit.com/blog/2024/02/finding-aws-account-id-of-any-s3-bucket/</link>
            <guid>39512896</guid>
            <pubDate>Mon, 26 Feb 2024 16:07:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tracebit.com/blog/2024/02/finding-aws-account-id-of-any-s3-bucket/">https://tracebit.com/blog/2024/02/finding-aws-account-id-of-any-s3-bucket/</a>, See on <a href="https://news.ycombinator.com/item?id=39512896">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>In 2021 <a href="https://twitter.com/benbridts">Ben Bridts</a> published a <a href="https://cloudar.be/awsblog/finding-the-account-id-of-any-public-s3-bucket/">highly inventive method</a> for finding the AWS Account ID of a public S3 bucket.</p><p><strong>This post describes a technique to find the Account ID of <em>any S3 bucket</em> (both private and public).</strong></p><p>I'd highly recommend reading Ben's technique first as we will re-use a lot of concepts.</p><h3>S3 Bucket to AWS Account ID</h3><p>Shell output can be worth a thousand words, here's what our technique enables - finding the previously unknown AWS Account ID for the bucket <code>bucket-alpha</code>:</p><pre><code>sh-5.2$ python3 find-s3-account.py bucket-alpha

VPC endpoint vpce-0e76855aadb0dafb5 policy already configured
Requesting bucket-alpha using session name 0-----------
Requesting bucket-alpha using session name 1-----------
Requesting bucket-alpha using session name 2-----------
Requesting bucket-alpha using session name 3-----------

SNIP

Requesting bucket-alpha using session name -----------7
Requesting bucket-alpha using session name -----------8
Requesting bucket-alpha using session name -----------9
Finding session names which passed the VPC endpoint in CloudTrail...
Found -----------1 for bucket-alpha in CloudTrail
Found ---------1-- for bucket-alpha in CloudTrail
Found --------9--- for bucket-alpha in CloudTrail
Found -----6------ for bucket-alpha in CloudTrail
Found --3--------- for bucket-alpha in CloudTrail
Found -2---------- for bucket-alpha in CloudTrail
Found 1----------- for bucket-alpha in CloudTrail
Found ----------0- for bucket-alpha in CloudTrail
Found -------8---- for bucket-alpha in CloudTrail
Found ------7----- for bucket-alpha in CloudTrail
Found ----5------- for bucket-alpha in CloudTrail
Found ---4-------- for bucket-alpha in CloudTrail
Bucket bucket-alpha: 123456789101</code></pre><h3>How exactly does this work?</h3><p>When exploring possibilities for this technique, I started by breaking down exactly why Ben's method works. There are three key elements which combine to make it work:</p><ol><li><strong>The ability to apply an IAM policy to the request</strong></li></ol><p>In the Ben's technique, this is achieved by applying a custom policy when assuming the role.</p><ol start="2"><li><strong>The ability to infer whether this IAM policy permitted the request or not</strong></li></ol><p>In the case of public buckets, this is quite simple. If our policy blocked the request, the request will fail with <code>AccessDenied</code>. Otherwise, the request will succeed as expected with requests to public buckets.</p><ol start="3"><li><strong>The ability to apply a wildcard match on the <code>s3:ResourceAccount</code> condition key</strong></li></ol><p>This allows us to discover the Account ID incrementally, one digit at a time, reducing the search space from trillions to hundreds.</p><h3>A solution</h3><p>After exploring a few different ideas, I found a solution which works. It involves using a VPC Endpoint for S3, and a difference of behaviour in CloudTrail when a request is denied by a VPC Endpoint policy.</p><p><a href="#img700414"><img src="https://tracebit.com/img/blog/2024/02/cloudtrail-and-vpc-endpoint-policy.png" alt="How VPC Endpoint policies interact with CloudTrail" title="How VPC Endpoint policies interact with CloudTrail"></a><br><a href="#void" id="img700414"><span></span></a></p><ol><li><strong>The ability to apply an IAM policy to the request</strong></li></ol><p>Creating a VPC Endpoint of type "Interface" for S3 will allow us to apply an IAM policy to the request. This policy intersects with the other policies which apply to the request (e.g. the bucket policy, the IAM policy of the principal making the request etc) when the request is made through the VPC Endpoint.</p><ol start="2"><li><strong>The ability to infer whether this IAM policy permitted the request or not</strong></li></ol><p>As the target bucket is owned by a third party and is a private bucket, we're (thankfully) going to receive an <code>AccessDenied</code> response, regardless of whichever policies we apply to the request. However, we can infer whether the VPC Endpoint policy blocked or permitted the request by whether it appears in <em>our own</em> CloudTrail logs.</p><ul><li>If the request <em>does</em> appear in our CloudTrail logs, it was permitted by our VPC Endpoint policy but blocked as expected by the bucket policy.</li><li>If the request <em>does not</em> appear in our CloudTrail logs, it was blocked by our VPC Endpoint policy.</li></ul><ol start="3"><li><strong>The ability to apply a wildcard match on the <code>s3:ResourceAccount</code> condition key</strong></li></ol><p>We can use the full power of IAM policy conditions, including <code>StringLike</code> wildcards and resource condition keys in a VPC Endpoint policy, so the same basic technique will work here.</p><h3>Step-by-step</h3><p>Let's say that we want to find the Account ID of the bucket <code>bucket-alpha</code>.</p><p><strong>Note that some of our activities here will be visible to the owner of the bucket in their own CloudTrail logs.</strong></p><h4>Determine the bucket region</h4><p>We need to find the region in which the bucket lives so that we can create a VPC in the same region. This can be done by curling the bucket's HTTP endpoint and examining the <code>x-amz-bucket-region</code> header (which is returned despite the request being forbidden).</p><pre><code><span>curl</span> <span>-v</span> bucket-alpha.s3.amazonaws.com

<span>..</span>.
x-amz-bucket-region: us-east-1</code></pre><h4>Deploy a VPC and VPC Endpoint in the same region</h4><p>We need to deploy a VPC and a VPC Endpoint for S3 in the same region as the target bucket. It's best to create a VPC specifically for this purpose as our VPC Endpoint will interfere with requests to S3 from the VPC. The VPC Endpoint should be of type "Interface" so we can apply a policy to the request.</p><h4>Launch an EC2 instance within the VPC and confirm that it's using the VPC Endpoint for S3</h4><p>We'll need to send requests to S3 from within the VPC so that the VPC Endpoint is used. An EC2 instance is a convenient way of doing so.</p><h4>Modify the VPC Endpoint policy to determine whether the account ID of the target bucket starts with "0"</h4><p>Apply a policy to the VPC Endpoint which performs a wildcard match on the <code>s3:ResourceAccount</code> condition key. This will only permit requests through the endpoint if the bucket's Account ID starts with "0".</p><pre><code><span>{</span>
    <span>"Version"</span><span>:</span> <span>"2012-10-17"</span><span>,</span>
    <span>"Statement"</span><span>:</span> <span>[</span>
        <span>{</span>
            <span>"Action"</span><span>:</span> <span>"s3:*"</span><span>,</span>
            <span>"Effect"</span><span>:</span> <span>"Allow"</span><span>,</span>
            <span>"Resource"</span><span>:</span> <span>"*"</span><span>,</span>
            <span>"Principal"</span><span>:</span> <span>"*"</span><span>,</span>
            <span>"Condition"</span><span>:</span> <span>{</span>
                <span>"StringLike"</span><span>:</span> <span>{</span>
                    <span>"s3:ResourceAccount"</span><span>:</span> <span>"0*"</span>
                <span>}</span>
            <span>}</span>
        <span>}</span>
    <span>]</span>
<span>}</span></code></pre><h4>Make a request to the target bucket</h4><p>Via the EC2 instance, make a request to the target bucket. This request will be denied as expected. It's best to use a "Management" request rather than a "Data" request so we don't need to do anything special with our CloudTrail setup. In this case I used <code>GetBucketAcl</code>.</p><pre><code>aws s3api get-bucket-acl <span>--bucket</span> bucket-alpha

An error occurred <span>(</span>AccessDenied<span>)</span> when calling the GetBucketAcl operation: Access Denied</code></pre><h4>Check whether the request appears in CloudTrail</h4><p>Now we want to check whether our request appears in CloudTrail.</p><pre><code>aws cloudtrail lookup-events --lookup-attributes <span>AttributeKey</span><span>=</span>EventName,AttributeValue<span>=</span>GetBucketAcl --start-time <span><span>$(</span><span>date</span> <span>-d</span> <span>"-10 minutes"</span> +%s<span>)</span></span></code></pre><p>If we find our request in CloudTrail, it means that the VPC Endpoint policy permitted the request - i.e. the Account ID of the bucket starts with <code>0</code>. If we don't find the request, then the VPC Endpoint policy blocked the request - i.e. the Account ID of the bucket does not start with <code>0</code>.</p><pre><code><span>{</span>
    <span>&lt;</span>SNIP<span>&gt;</span>
    <span>"eventSource"</span><span>:</span> <span>"s3.amazonaws.com"</span>,
    <span>"eventName"</span><span>:</span> <span>"GetBucketAcl"</span>,
    <span>"resources"</span><span>:</span> <span>[</span>
        <span>{</span>
            <span>"accountId"</span><span>:</span> <span>"HIDDEN_DUE_TO_SECURITY_REASONS"</span>,
            <span>"type"</span><span>:</span> <span>"AWS::S3::Bucket"</span>,
            <span>"ARN"</span><span>:</span> <span>"arn:aws:s3:::bucket-alpha"</span>
        <span>}</span>
    <span>]</span>,
    <span>"vpcEndpointId"</span><span>:</span> <span>"vpce-0e76855aadb0dafb5"</span>,
<span>}</span></code></pre><p>Bear in mind that it will take a few minutes for the request to appear in CloudTrail. To be safe, I'd recommend waiting a 10 minutes before deciding the event won't appear in CloudTrail.</p><h4>Rinse and repeat</h4><p>Depending on the result of the previous step, modify the VPC Endpoint policy to discover more information about the account ID. For instance, if the event <em>didn't</em> appear in CloudTrail, modify the condition to test whether the first digit is <code>1</code>:</p><pre><code><span>"Condition"</span><span>:</span> <span>{</span>
    <span>"StringLike"</span><span>:</span> <span>{</span>
        <span>"s3:ResourceAccount"</span><span>:</span> <span>"1*"</span>
    <span>}</span>
<span>}</span></code></pre><p>If it <em>did</em> appear in CloudTrail (so the first digit of the account ID is <code>0</code>), we can start work on the second digit:</p><pre><code><span>"Condition"</span><span>:</span> <span>{</span>
    <span>"StringLike"</span><span>:</span> <span>{</span>
        <span>"s3:ResourceAccount"</span><span>:</span> <span>"00*"</span>
    <span>}</span>
<span>}</span></code></pre><p>Bear it mind, it takes a few minutes for policy changes to fully propagate and take effect. I've found waiting 5 minutes after modifying the policy to work well.</p><h3>Results</h3><p>I wrote a script to automate this process and it could reliably find the Account ID of a bucket. As it's quite a slow process, I used a slightly modified technique of performing a binary search on each digit so fewer tests were needed, e.g:</p><pre><code><span>"Condition"</span><span>:</span> <span>{</span>
    <span>"StringLike"</span><span>:</span> <span>{</span>
        <span>"s3:ResourceAccount"</span><span>:</span> <span>[</span><span>"0*"</span><span>,</span> <span>"1*"</span><span>,</span> <span>"2*"</span><span>,</span> <span>"3*"</span><span>,</span> <span>"4*"</span><span>]</span>
    <span>}</span>
<span>}</span></code></pre><p>Leaving it for a few hours returned the account ID successfully:</p><pre><code>[ssm-user@ip-172-31-8-184 ~]$ python3 find-s3-account.py bucket-alpha
Searching for bucket bucket-alpha
Modifying VPC endpoint policy...
Modified VPC endpoint policy
Made S3 request to bucket: GPSWS2M4TH9ABX3C
Looking for event in CloudTrail...
Did not find event in CloudTrail (not permitted through VPC endpoint)
State: {'found': '', 'next_digits': [0, 1, 2, 3, 4]}
Modifying VPC endpoint policy...
Modified VPC endpoint policy
Made S3 request to bucket: 8T809NPVDGQSGB1N
Looking for event in CloudTrail...
Did not find event in CloudTrail (not permitted through VPC endpoint)
State: {'found': '', 'next_digits': [0, 1]}
Modifying VPC endpoint policy...
Modified VPC endpoint policy
Made S3 request to bucket: C9F0RTC7QK0G70TB
Looking for event in CloudTrail...
Found event in CloudTrail (permitted through VPC endpoint)
State: {'found': '1', 'next_digits': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}

...

State: {'found': '123456789101', 'next_digits': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}
Found account: 123456789101</code></pre><h3>Making it faster</h3><p>Waiting for the VPC Endpoint policy to take effect, and waiting for long enough to determine whether the request appears in CloudTrail is quite a slow process. Even using a binary search, it will take approximately (40 * 12 minutes = 8 hours) to find the Account ID.</p><p>To make this process faster, and eliminate the need to wait between each step, I modified the VPC endpoint policy like so:</p><pre><code><span>{</span>
    <span>"Version"</span><span>:</span> <span>"2012-10-17"</span><span>,</span>
    <span>"Statement"</span><span>:</span> <span>[</span>
      <span>{</span>
        <span>"Effect"</span><span>:</span> <span>"Allow"</span><span>,</span>
        <span>"Action"</span><span>:</span> <span>[</span>
          <span>"s3:*"</span>
        <span>]</span><span>,</span>
        <span>"Resource"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Principal"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Condition"</span><span>:</span> <span>{</span>
          <span>"StringLike"</span><span>:</span> <span>{</span>
            <span>"aws:userid"</span><span>:</span> <span>"*:0-----------"</span><span>,</span>
            <span>"s3:ResourceAccount"</span><span>:</span> <span>"0???????????"</span>
          <span>}</span>
        <span>}</span>
      <span>}</span><span>,</span>
      <span>{</span>
        <span>"Effect"</span><span>:</span> <span>"Allow"</span><span>,</span>
        <span>"Action"</span><span>:</span> <span>[</span>
          <span>"s3:*"</span>
        <span>]</span><span>,</span>
        <span>"Resource"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Principal"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Condition"</span><span>:</span> <span>{</span>
          <span>"StringLike"</span><span>:</span> <span>{</span>
            <span>"aws:userid"</span><span>:</span> <span>"*:1-----------"</span><span>,</span>
            <span>"s3:ResourceAccount"</span><span>:</span> <span>"1???????????"</span>
          <span>}</span>
        <span>}</span>
      <span>}</span><span>,</span>
      SNIP
      <span>{</span>
        <span>"Effect"</span><span>:</span> <span>"Allow"</span><span>,</span>
        <span>"Action"</span><span>:</span> <span>[</span>
          <span>"s3:*"</span>
        <span>]</span><span>,</span>
        <span>"Resource"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Principal"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Condition"</span><span>:</span> <span>{</span>
          <span>"StringLike"</span><span>:</span> <span>{</span>
            <span>"aws:userid"</span><span>:</span> <span>"*:-----------9"</span><span>,</span>
            <span>"s3:ResourceAccount"</span><span>:</span> <span>"???????????9"</span>
          <span>}</span>
        <span>}</span>
      <span>}</span>
    <span>]</span>
<span>}</span></code></pre><p>There are 120 statements in the policy - one for each possible digit in each possible position. The condition on <code>aws:userid</code> is used to match particular values of the <code>RoleSessionName</code> parameter (which we can freely specify) used in an STS <code>AssumeRole</code> call. In effect this means we can selectively choose which policy statement (i.e. a particular digit in a particular position) we want to test for each request, by assuming a role with a particular <code>RoleSessionName</code> before doing so.</p><p><a href="#img172768"><img src="https://tracebit.com/img/blog/2024/02/using-role-session-name-in-vpc-endpoint-policy.png" alt="Using RoleSessionName to selectively test digits" title="Using RoleSessionName to selectively test digits"></a><br><a href="#void" id="img172768"><span></span></a></p><p>As this policy (only just!) fits within the maximum character length of a VPC Endpoint policy, we can test all 120 possibilities in parallel, without modifying the policy or waiting for the results individually in CloudTrail.</p><p>This reduced the time taken to find the Account ID to less than 10 minutes:</p><pre><code>sh-5.2$ python3 find-s3-account.py bucket-alpha

VPC endpoint vpce-0e76855aadb0dafb5 policy already configured
Requesting bucket-alpha using session name 0-----------
Requesting bucket-alpha using session name 1-----------
Requesting bucket-alpha using session name 2-----------
Requesting bucket-alpha using session name 3-----------

SNIP

Requesting bucket-alpha using session name -----------7
Requesting bucket-alpha using session name -----------8
Requesting bucket-alpha using session name -----------9
Finding session names which passed the VPC endpoint in CloudTrail...
Found -----------1 for bucket-alpha in CloudTrail
Found ---------1-- for bucket-alpha in CloudTrail
Found --------9--- for bucket-alpha in CloudTrail
Found -----6------ for bucket-alpha in CloudTrail
Found --3--------- for bucket-alpha in CloudTrail
Found -2---------- for bucket-alpha in CloudTrail
Found 1----------- for bucket-alpha in CloudTrail
Found ----------0- for bucket-alpha in CloudTrail
Found -------8---- for bucket-alpha in CloudTrail
Found ------7----- for bucket-alpha in CloudTrail
Found ----5------- for bucket-alpha in CloudTrail
Found ---4-------- for bucket-alpha in CloudTrail
Bucket bucket-alpha: 123456789101</code></pre><h3>Remarks</h3><ul><li><p>I consulted the AWS Security team before publishing this blog post.</p></li><li><p>There has already been a lot of <a href="https://www.lastweekinaws.com/blog/are-aws-account-ids-sensitive-information/">interesting</a> <a href="https://blog.plerion.com/aws-account-ids-are-secrets/">discussion</a> about whether AWS account IDs should be considered sensitive. It's noteworthy that in the CloudTrail events themselves, AWS have chosen to leave the third-party account ID <code>HIDDEN_DUE_TO_SECURITY_REASONS</code>. You've probably also spotted that I've chosen to redact my own account ID from the examples in this post!</p></li><li><p>This technique should also work to uncover other resource condition keys (e.g. <code>aws:ResourceOrgID</code>, <code>aws:ResourceOrgPaths</code>, <code>aws:ResourceTag</code>) associated with the bucket - or indeed for services other than S3 to which this technique could be applied</p></li><li><p>It's possible that by creating mutually peered VPCs and VPC Endpoints in all regions you could create a setup which works regardless of the particular region of the target bucket.</p></li><li><p>These techniques are only feasible due to the ability to use <code>StringLike</code> conditions against <code>s3:ResourceAccount</code> in a policy. I can't think of a use case where a partial match against a randomly-generated identifier is necessary.</p></li><li><p>It seems like it might otherwise be beneficial for events which are denied by a VPC Endpoint policies to be logged in CloudTrail.</p></li></ul><h3>Acknowledgments</h3><ul><li><p>Ben Bridt's original technique inspired this work.</p></li><li><p>I'm very grateful to <a href="http://www.chrisfarris.com/">Chris Farris</a> for his invaluable help and advice.</p></li></ul><p><strong><em>Thanks for reading this far, we hope you got something from this article. Tracebit is building a new kind of security product to be the ‘easy button’ for adding detections to cloud environments using decoys. If you’re interested to learn more about what that looks like please <a href="https://tracebit.com/earlyaccess">reach out</a> (we currently support AWS, with Azure &amp; GCP coming soon).</em></strong></p><ul><li><a href="https://tracebit.com/blog/category/research">research</a></li><li><a href="https://tracebit.com/blog/category/aws">AWS</a></li><li><a href="https://tracebit.com/blog/category/cloudtrail">CloudTrail</a></li><li><a href="https://tracebit.com/blog/category/engineering">engineering</a></li><li><a href="https://tracebit.com/blog/category/security">security</a></li></ul></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini and Google's Culture (158 pts)]]></title>
            <link>https://stratechery.com/2024/gemini-and-googles-culture/</link>
            <guid>39512684</guid>
            <pubDate>Mon, 26 Feb 2024 15:54:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stratechery.com/2024/gemini-and-googles-culture/">https://stratechery.com/2024/gemini-and-googles-culture/</a>, See on <a href="https://news.ycombinator.com/item?id=39512684">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-12388">
	<!-- .entry-header -->

	<div>
		<p>Last Wednesday, when the questions about Gemini’s political viewpoint were still limited to its image creation capabilities, <a href="https://stratechery.com/2024/groq-costs-gemini-pro-1-5-googles-timidity/">I accused the company of being timid</a>:</p>
<blockquote><p>
  Stepping back, I don’t, as a rule, want to wade into politics, and definitely not into culture war issues. At some point, though, you just have to state plainly that this is ridiculous. Google specifically, and tech companies broadly, have long been sensitive to accusations of bias; that has extended to image generation, and I can understand the sentiment in terms of depicting theoretical scenarios. At the same time, many of these images are about actual history; I’m reminded of George Orwell in 1984:</p>
<blockquote><p>
    Every record has been destroyed or falsified, every book has been rewritten, every picture has been repainted, every statue and street and building has been renamed, every date has been altered. And that process is continuing day by day and minute by minute. History has stopped. Nothing exists except an endless present in which the Party is always right. I know, of course, that the past is falsified, but it would never be possible for me to prove it, even when I did the falsification myself. After the thing is done, no evidence ever remains. The only evidence is inside my own mind, and I don’t know with any certainty that any other human being shares my memories.
  </p></blockquote>
<p>  Even if you don’t want to go so far as to invoke the political implications of Orwell’s book, the most generous interpretation of Google’s over-aggressive RLHF of their models is that they are scared of being criticized. That, though, is just as bad: Google is blatantly sacrificing its mission to “organize the world’s information and make it universally accessible and useful” by creating entirely new realities because it’s scared of some bad press. Moreover, there are implications for business: Google has the models and the infrastructure, but winning in AI&nbsp;<a href="https://stratechery.com/2023/ai-and-the-big-five/">given their business model challenges</a>&nbsp;will&nbsp;<a href="https://stratechery.com/2023/googles-true-moonshot/">require boldness</a>; this shameful willingness to change the world’s information in an attempt to avoid criticism reeks — in the best case scenario! — of abject timidity.
</p></blockquote>
<p>If timidity were the motivation, then it’s safe to say that the company’s approach with Gemini has completely backfired; while Google turned off Gemini’s image generation capabilities, it’s text generation is just as absurd:</p>

<p>That is just one examples of many: Gemini <a href="https://twitter.com/pmarca/status/1761929296564588890">won’t help promote meat</a>, <a href="https://twitter.com/TheStalwart/status/1761832472357335068">write a brief about fossil fuels</a>, or even <a href="https://twitter.com/bitcloud/status/1761618020819157191">help sell a goldfish</a>. It says that effective accelerationism is <a href="https://twitter.com/mblair/status/1761282019148140852">a violent ideology</a>, that <a href="https://twitter.com/trhlofficial/status/1761903486264901760">libertarians are morally equivalent to Stalin</a>, and insists that it’s hard to say what caused more harm: <a href="https://twitter.com/ajitpai/status/1761898602140443001">repealing net neutrality or Hitler</a>.</p>
<p>Some of these examples, particularly the Hitler comparisons (or <a href="https://twitter.com/paulg/status/1761794931847245983">Mao vs George Washington</a>), are obviously absurd and downright offensive; others are merely controversial. They do, though, all seem to have a consistent viewpoint: Nate Silver, <a href="https://twitter.com/NateSilver538/status/1760462416834425049">in another tweet</a>, labeled it “the politics of the median member of the San Francisco Board of Supervisors.”</p>
<p>Needless to say, overtly expressing those opinions is not timid, which raises another question from Silver:</p>
<div>
<blockquote data-conversation="none" data-dnt="true">
<p lang="en" dir="ltr">Gemini is behaving exactly as instructed. Asking it to draw different groups of people (e.g. "Vikings" or "NHL players") is the base case, not an edge case. The questions are all about how it got greenlit by a $1.8T market cap company despite this incredibly predictable behavior.</p>
<p>— Nate Silver (@NateSilver538) <a href="https://twitter.com/NateSilver538/status/1760929332422234570?ref_src=twsrc%5Etfw">February 23, 2024</a></p></blockquote>
</div>
<p>In fact, I think there is a precedent for Gemini; like many comparison points for modern-day Google, it comes from Microsoft.</p>
<h3>Microsoft and The Curse of Culture</h3>
<p>From <a href="https://www.neowin.net/news/microsoft-workers-celebrated-windows-phone-7-rtm-with-iphone-hearses/">Neowin</a>, in 2010:<sup id="rf1-12388"><a href="#fn1-12388" title="Image credit <a href=&quot;https://www.flickr.com/photos/trioculus/&quot;>Carl J on Flickr</a>" rel="footnote">1</a></sup></p>
<blockquote><p>
  <a href="https://www.flickr.com/photos/trioculus/"><img loading="lazy" decoding="async" src="https://i0.wp.com/stratechery.com/wp-content/uploads/2024/02/geminiculture-1.png?resize=640%2C349&amp;ssl=1" alt="Microsoft's funeral for the iPhone" width="640" height="349" srcset="https://i0.wp.com/stratechery.com/wp-content/uploads/2024/02/geminiculture-1.png?w=640&amp;ssl=1 640w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/02/geminiculture-1.png?resize=300%2C164&amp;ssl=1 300w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1"></a></p>
<p>  Microsoft workers celebrated the release to manufacturing of Windows Phone 7 by parading through their Redmond campus on Friday with iPhone and BlackBerry hearses. Employees dressed up in fancy dress and also modified cars to include Windows Phone branding. Aside from the crazy outfits the workers made fake hearses for giant BlackBerry and iPhone devices. Employees cheekily claimed they had buried the competition with Windows Phone 7.
</p></blockquote>
<p>This was, to be clear, insane. I wrote about the episode in 2013’s <a href="https://stratechery.com/2016/the-curse-of-culture/">The Curse of Culture</a>; it’s been eight years, so I hope you’ll allow me a particularly long excerpt:</p>
<blockquote><p>
  As with most such things, culture is one of a company’s most powerful assets right until it isn’t: the same underlying assumptions that permit an organization to scale massively constrain the ability of that same organization to change direction. More distressingly, culture prevents organizations from even knowing they need to do so. From Edgar Schein’s <a href="http://www.untag-smd.ac.id/files/Perpustakaan_Digital_2/ORGANIZATIONAL%20CULTURE%20Organizational%20Culture%20and%20Leadership,%203rd%20Edition.pdf">Organizational Culture and Leadership</a>:</p>
<blockquote><p>
    Basic assumptions, like theories-in-use, tend to be nonconfrontable and nondebatable, and hence are extremely difficult to change. To learn something new in this realm requires us to resurrect, reexamine, and possibly change some of the more stable portions of our cognitive structure…Such learning is intrinsically difficult because the reexamination of basic assumptions temporarily destabilizes our cognitive and interpersonal world, releasing large quantities of basic anxiety. Rather than tolerating such anxiety levels, we tend to want to perceive the events around us as congruent with our assumptions, even if that means distorting, denying, projecting, or in other ways falsifying to ourselves what may be going on around us. It is in this psychological process that culture has its ultimate power.
  </p></blockquote>
<p>  Probably the canonical example of this mindset was Microsoft after the launch of the iPhone. It’s hard to remember now, but no company today comes close to matching the stranglehold Microsoft had on the computing industry from 1985 to 2005 or so.&nbsp;The company had audacious goals — “A computer on every desk and in every home, running Microsoft software” — which it accomplished and then surpassed: the company owned enterprise back offices as well. This unprecedented success changed that goal — originally an espoused belief — into an unquestioned assumption that of course all computers should be Microsoft-powered. Given this, the real shock would have been then-CEO Steve Ballmer&nbsp;<em>not</em>&nbsp;<a href="https://www.youtube.com/watch?v=eywi0h_Y5_U">laughing at the iPhone</a>.</p>
<p>  A year-and-a-half later, Microsoft realized that Windows Mobile, their current phone OS, was not competitive with the iPhone and work began on what became Windows Phone. Still, unacknowledged cultural assumptions remained: one, that Microsoft had the time to bring to bear its unmatched resources to make something that might be worse at the beginning but inevitably superior over time, and two, that the company could leverage Windows’ dominance and their Office business. Both assumptions had become cemented in Microsoft’s victory in the browser wars and their slow-motion takeover of corporate data centers; in truth, though, Microsofts’ mobile efforts were already doomed, and nearly everyone realized it before Windows Phone even launched&nbsp;<a href="http://www.engadget.com/2010/09/10/microsoft-celebrates-windows-phone-7-rtm-with-funeral-parade-for/">with a funeral for the iPhone</a>.</p>
<p>  Steve Ballmer never figured it out; his last acts were to&nbsp;<a href="https://stratechery.com/2013/why-microsofts-reorganization-is-a-bad-idea/">reorganize the company</a>&nbsp;around a “One Microsoft” strategy centered on Windows, and to&nbsp;<a href="https://stratechery.com/2013/the-deal-that-makes-no-sense/">buy Nokia</a>&nbsp;to prop up Windows Phone. It fell to Satya Nadella, his successor, to change the culture, and it’s why the fact his first public event was to announce Office for iPad was so critical. I&nbsp;<a href="https://stratechery.com/2014/when-ceos-matter/">wrote at the time</a>:</p>
<blockquote><p>
    This is the power CEOs have. They cannot do all the work, and they cannot impact industry trends beyond their control. But they can choose whether or not to accept reality, and in so doing, impact the worldview of all those they lead.
  </p></blockquote>
<p>  Microsoft under Nadella’s leadership has, over the last three years, undergone a tremendous transformation, embracing its destiny as a device-agnostic service provider; still, it is fighting the headwinds of Amazon’s cloud, open source tooling, and the fact that mobile users had six years to get used to a world without Microsoft software. How much stronger might the company have been had it faced reality in 2007, but the culture made that impossible.
</p></blockquote>
<p>Google is not in nearly as bad of shape as Microsoft was when it held that funeral. The company’s revenue and profits are as high as ever, and the release of Gemini 1.5 in particular demonstrated how well-placed the company is for the AI era: the company not only has leading research, it also has unmatched infrastructure that enables entirely new and valuable use cases. That, though, makes the Gemini fiasco all the more notable.</p>
<h3>Don’t Be Evil</h3>
<p>The questions around Google and AI have, to date, been mostly about business model. In last year’s <a href="https://stratechery.com/2023/ai-and-the-big-five/">AI and the Big Five</a> I talked about how Kodak invented the digital camera, but didn’t pursue it because of business model reasons, and made the obvious analogy to Google’s seeming inability to ship:</p>
<blockquote><p>
  Google has long been a leader in using machine learning to make its search and other consumer-facing products better (and has offered that technology as a service through Google Cloud). Search, though, has always depended on humans as the ultimate arbiter: Google will provide links, but it is the user that decides which one is the correct one by clicking on it. This extended to ads: Google’s offering was revolutionary because instead of charging advertisers for impressions — the value of which was very difficult to ascertain, particularly 20 years ago — it charged for clicks; the very people the advertisers were trying to reach would decide whether their ads were good enough…</p>
<p>  That, though, ought only increase the concern for Google’s management that generative AI may, in the specific context of search, represent a disruptive innovation instead of a sustaining one. Disruptive innovation is, at least in the beginning, not as good as what already exists; that’s why it is easily dismissed by managers who can avoid thinking about the business model challenges by (correctly!) telling themselves that their current product is better. The problem, of course, is that the disruptive product gets better, even as the incumbent’s product becomes ever more bloated and hard to use — and that certainly sounds a lot like Google Search’s current trajectory.
</p></blockquote>
<p>Google has started shipping, and again, Gemini 1.5 is an incredible breakthrough; the controversy over Gemini, though, is a reminder that culture can restrict success as well. Google has its own unofficial motto — “Don’t Be Evil” — that founder Larry Page explained in the company’s <a href="https://www.sec.gov/Archives/edgar/data/1288776/000119312504073639/ds1.htm">S-1</a>:</p>
<blockquote><p>
  Don’t be evil. We believe strongly that in the long term, we will be better served — as shareholders and in all other ways — by a company that does good things for the world even if we forgo some short term gains. This is an important aspect of our culture and is broadly shared within the company.</p>
<p>  Google users trust our systems to help them with important decisions: medical, financial and many others. Our search results are the best we know how to produce. They are unbiased and objective, and we do not accept payment for them or for inclusion or more frequent updating. We also display advertising, which we work hard to make relevant, and we label it clearly. This is similar to a newspaper, where the advertisements are clear and the articles are not influenced by the advertisers’ payments. We believe it is important for everyone to have access to the best information and research, not only to the information people pay for you to see.
</p></blockquote>
<p>Google has by-and-large held to that promise, at least as defined by Page: the company does not sell search result placement. Of course the company has made ads look more and more like organic results, and crammed ever more into the search results page, and <a href="https://stratechery.com/2019/the-google-squeeze/">squeezed more and more verticals</a>, but while there are always whispers about what is or isn’t included in search, or the decisions made by the algorithm, most people still trust the product, and use it countless times every day.</p>
<p>One does wonder, though, if the sanctity of search felt limiting to some inside of Google. In 2018 a video leaked of an all-hands meeting <a href="https://www.cnbc.com/2018/09/12/leaked-video-from-alphabet-tgif-meeting-after-president-trump-election.html">after the 2016 election</a> where Google executives expressed dismay over the results; the footage was damaging enough that Google felt compelled to issue a statement:</p>
<blockquote><p>
  At a regularly scheduled all hands meeting, some Google employees and executives expressed their own personal views in the aftermath of a long and divisive election season. For over 20 years, everyone at Google has been able to freely express their opinions at these meetings. Nothing was said at that meeting, or any other meeting, to suggest that any political bias ever influences the way we build or operate our products. To the contrary, our products are built for everyone, and we design them with extraordinary care to be a trustworthy source of information for everyone, without regard to political viewpoint.
</p></blockquote>
<p>Perhaps this seemed to some employees to be an outdated view of the world; I’m reminded of <a href="https://www.goodreads.com/quotes/8731136-in-a-racist-society-it-is-not-enough-to-be">that quote from Angela Y Davis</a>: “In a racist society it is not enough to be non-racist, we must be anti-racist.” In this view calls for color-blindness in terms of opportunity are insufficient; the only acceptable outcome is one in which outcomes are equal as well. The equivalent in the case of Google would be that it is not enough to not be evil; one must be “anti-evil” as well.</p>
<p>The end result is that just as Microsoft could, shielded by years of a Windows monopoly, delude themselves into thinking they had an iPhone killer, Google could, shielded by years of a search monopoly, delude themselves into thinking they had not just the right but the obligation to tell users what they ought to believe.</p>
<h3>After Gemini</h3>
<p>As I noted in the excerpt, I very much try to avoid politics on Stratechery; I want to talk about business models and societal impact, and while that has political implications, it doesn’t need to be partisan (for example, <a href="https://stratechery.com/2016/the-voters-decide/">I think this piece about the 2016 election</a> holds up very well, and isn’t partisan in the slightest). AI, though, is increasingly giving all of us no choice in the matter.</p>
<p>To that end, my Article last fall about the Biden executive order, <a href="https://stratechery.com/2023/attenuating-innovation-ai/">Attenuating Innovation</a>, was clearly incomplete: not only must we keep in mind the potential benefits of AI — which are massive — but it is clearly essential that we allow open source models to flourish as well. It is Google or OpenAI’s prerogative to train their models to have whatever viewpoint they want; any meaningful conception of freedom should make space for an open market of alternatives, and that means open source.</p>
<p>Secondly, it behooves me, and everyone else in tech, to write Articles like the one you are reading; “the politics of the median member of the San Francisco Board of Supervisors” has had by far the loudest voice in tech because most people just want to build cool new things, or write about them, without being fired or yelled at on social media. This does, though, give the perception that tech is out of touch, or actively authoritarian; I don’t think that’s true, but those of us who don’t want to tell everyone else what to think, do, paradoxically, need to say so.</p>
<p>The biggest question of all, though, is Google. Again, this is a company that should dominate AI, thanks to their research and their infrastructure. The biggest obstacle, though, above and beyond business model, is clearly culture. To that end, the nicest thing you can say about Google’s management is to assume that they, like me and everyone else, just want to build products and not be yelled at; that, though, is not leadership. Schein writes:</p>
<blockquote><p>
  When we examine culture and leadership closely, we see that they are two sides of the same coin; neither can really be understood by itself. On the one hand, cultural norms define how a given nation or organizations will define leadership — who will get promoted, who will get the attention of followers. On the other hand, it can be argued that the only thing of real importance that leaders do is to create and manage culture; that the unique talent of leaders is their ability to understand and work with culture; and that it is an ultimate act of leadership to destroy culture when it is viewed as dysfunctional.
</p></blockquote>
<p>That is exactly what Nadella did at Microsoft. I recounted in <a href="https://stratechery.com/2018/the-end-of-windows/">The End of Windows</a> how Nadella changed the company’s relationship to Windows, unlocking the astronomical growth that has happened under his watch, including the company’s position in AI.</p>
<p>Google, quite clearly, needs a similar transformation: the point of the company ought not be to tell users what to think, but to help them make important decisions, as Page once promised. That means, first and foremost, excising the company of employees attracted to Google’s power and its potential to help them execute their political program, and return decision-making to those who actually want to make a good product. That, by extension, must mean removing those who let the former run amok, up to and including CEO Sundar Pichai. The stakes, for Google specifically and society broadly, are too high to simply keep one’s head down and hope that the San Francisco Board of Supervisors magically comes to its senses.</p>
<hr><ol><li id="fn1-12388"><p>Image credit <a href="https://www.flickr.com/photos/trioculus/">Carl J on Flickr</a>&nbsp;<a href="#rf1-12388" title="Return to footnote 1.">↩</a></p></li></ol>
	</div><!-- .entry-content -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Senior Boeing employee on why he won't fly a MAX: Just Get me Off This Plane (111 pts)]]></title>
            <link>https://www.politico.com/news/magazine/2024/02/26/former-boeing-employee-speaks-out-00142948</link>
            <guid>39512346</guid>
            <pubDate>Mon, 26 Feb 2024 15:27:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/magazine/2024/02/26/former-boeing-employee-speaks-out-00142948">https://www.politico.com/news/magazine/2024/02/26/former-boeing-employee-speaks-out-00142948</a>, See on <a href="https://news.ycombinator.com/item?id=39512346">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>“The Boeing Company is capable of building quality airplanes,” says Pierson, now the executive director for the nonprofit <a href="https://www.foundationforaviationsafety.org/" target="_blank" data-tracking="mpos=&amp;mid=&amp;lindex=&amp;lcol=">
Foundation for Aviation Safety</a>. “The problem is leadership, or lack thereof, and the pressure to get airplanes out the door is greater than doing the job right.”</p>


<p>In a statement in response to this interview, Boeing said it’s made substantial changes to its organization following the pair of earlier disasters, including investing in more engineers and manufacturers, establishing an official designee for employees to raise work-related concerns and increasing its aerospace and safety expertise on its board of directors. “Over the last several years, we’ve taken close care not to push the system too fast, and we have never hesitated to slow down, to halt production, or to stop deliveries to take the time we need to get things right,” Boeing spokesperson Jessica Kowal said.</p>

<p>Last week, in a further bid for a fresh start, <a href="https://www.politico.com/news/2024/02/21/head-of-troubled-boeing-737-max-program-leaves-company-00142478" target="_blank" data-tracking="mpos=&amp;mid=&amp;lindex=&amp;lcol=">
Boeing replaced the head of its 737 Max program.</a></p>
<p>Pierson, meanwhile, still refuses to fly in a MAX.</p>

<p><i>This interview has been edited for length and clarity. Video clips below were conducted in a separate interview by Pawlyk and POLITICO senior video producer Jackie Padilla. </i></p>

<p><b>Are Boeing planes safe to fly today and would you put your family in one?</b></p>

<p>I’m not saying that all Boeing planes are unsafe. Part of the problem is that people don’t know how to differentiate between the MAX and other planes.</p>

<p>Last year, I was flying from Seattle to New York, and I purposely scheduled myself on a non-MAX airplane. I went to the gate. I walked in, sat down and looked straight ahead, and lo and behold, there was a 737-8/737-9 safety card. So I got up and I walked off. The flight attendant didn’t want me to get off the plane. And I’m not trying to cause a scene. I just want to get off this plane, and I just don’t think it’s safe. I said I purposely scheduled myself not to fly [on a MAX].</p>


<p>Our recommendation from the foundation is that these planes get grounded — period. Get grounded and inspected and then, depending on what they find, get fixed.</p>

<p><b>Why do you prefer legacy Boeing aircraft over the MAX? What changed between these models from what you observed?</b></p>

<p>I have always had the greatest respect for the airplane products that The Boeing Company makes. My family was involved in it and my relatives. I had no reason ever to doubt it. And then I started working in the factory. I had been around airplanes my whole career. I flew airplanes in the Navy. You go into the production environment, and you’re like, “Oh, my God, I had no idea it was this complex.” It’s stunning how complex it is. At first, I didn’t understand how all that came together. And it gave me a great respect for the people that were building the plane — it’s incredibly impressive to see. And then everything started to change in 2017 and into 2018.</p>

<p><b>What changed that year?</b></p>

<p>We started <a href="https://www.congress.gov/event/116th-congress/house-event/LC65141/text?s=1&amp;r=16" target="_blank" data-tracking="mpos=&amp;mid=&amp;lindex=&amp;lcol=">
having problems in our supply chain </a>with the engines. And then the next thing you know, we started having problems with all kinds of parts. We were having hundreds of people doing out of sequence work [where parts from previous stages still needed to be fixed]. And we had tests that were being performed that were not being passed properly; one shift would try to get it done and they couldn’t get it done, so they’d leave notes for the next shift to come in.</p>

<p>This is not how planes should be built. It was so bad in 2018 — we didn’t have engines on many of the planes and so they put these big concrete blocks on the engine pylons so the plane wouldn’t tip. Kind of an important part of the plane, right? A major warning bell that something’s not right. But they kept increasing production rate and so we kept getting further and further behind. So all of 2018 was just a chaotic disarray type of environment.</p>

<p>And by the way, where the hell is the FAA? FAA had no presence in the factory. And it really irritates you because right down the road, literally 20 minutes down the road, is the Northwest headquarters for the FAA. There’s over 2,000 employees that work at that site and yet, in the busiest factory in the world 20 minutes down the road, there’s four or five employees. That’s not enough to monitor the restaurant operations at the site.</p>

<p><b>What made you decide to work with lawmakers and others to shine a light on these problems at Boeing?</b></p>

<p>I realized how the leadership was treating employees — very disrespectfully, very embarrassing. Standing up in front of teams and just calling them out, and it was horrendous. I thought, this is not a healthy environment to build airplanes. I can’t support this as a senior manager. I just felt this was really wrong. So I made a decision to retire early. Just before I retired, I shared with my other colleagues all the kind of communication I had with a senior person at the company, the general manager, telling him that he needs to shut down. And then the [first] crash happened.</p>

<p>Ever since then, I’ve been trying to alert the authorities and trying to get them to look at manufacturing. FAA didn’t want to talk to me. NTSB didn’t want to talk to me. Finally I got a meeting with the NTSB. Then Congress asked me to testify. Ever since, I and a team of people have been monitoring all this.</p>


<p>I’ve become very close to [families of the victims of the Ethiopian and Indonesian airlines’ crashes] in the last couple years. And we’ve worked on Capitol Hill to try to lobby Congress to make changes. We’ve worked to try to point out issues with the FAA. And what they want is for this not to happen to anybody else, and number two, what they want is justice.</p>

<p><b>It’s only been a few years since those fatal crashes — so what is still going wrong considering we’ve just seen another mishap with the Alaska Airlines flight? </b></p>

<p>What’s going wrong is that nothing changed. They made very superficial changes that they made a big deal of. They made a giant deal of hiring a safety officer. Big whoop. They wanted to deflect attention away.</p>

<p>That’s all Boeing does is talk. The leadership doesn’t get down there and get involved with the people that are building the products. They don’t value the engineers, they think the engineers are replaceable. You can’t take a 20- or 30-year employee and just dump them off to the side and think that you’re going to find somebody off the street that’s going to be able to do what that person does. Then they don’t have the support mechanisms and they’re tired and they’re fatigued and they’re working like dogs — they can make mistakes.</p>

<p><b>Are you at all surprised that so little has been done to fix things only five years after you blew the whistle on the MAX issues?</b></p>

<p>I’m horrified and I’m not surprised. It’s horrifying to think that the company did such a minimal effort. They spent 90 percent of their energy telling the media things [like] “renewed quality” and using language in their press releases and their financial statements like “a renewed safety focus.” And then meanwhile, I’m hearing from people, “No, it’s actually just as bad or worse in the factory now than it was before.”</p>

<p><b>In some of our discussions, you mentioned that airlines also aren’t completely blameless in this situation. What did you mean by that?</b></p>

<p>There’s obviously a tremendous demand for more planes. What we’re seeing is evidence that the airlines are aware that there’s issues with these planes. Four airlines in the U.S. fly MAX planes: Alaska, American, United and Southwest. And it’s not like all the MAX airplanes are built in a bundle and go out at the same time.You’ll have an American plane, you’ll have a Southwest plane, you’ll have the United plane, you might have a China Southern, a Ryanair. They’re all intermixed, so they all have defects.</p>

<p>[I’ve seen that some planes] have <a href="https://www.foundationforaviationsafety.org/incident-reports" target="_blank" data-tracking="mpos=&amp;mid=&amp;lindex=&amp;lcol=">
less than 100 hours on it</a> and have [some sort of] failure. You can’t blame maintenance because they haven’t been there long enough to have any real serious maintenance. Last April, I wrote a letter to the Alaska Airlines CEO because we’re looking at his data and his planes and I don’t think they should be flying right now. Alaska had been submitting on average 95 [service] reports every month throughout 2023. Then in December, it dropped steeply. What happened?</p>

<p><i>[In response, Alaska Airlines — which did not address whether its CEO responded to Pierson — said it recently implemented changes to align its service data reporting “to reduce the number of discrepancies” that the airline reports to the main national database. “A lot of thoughtful planning went into aligning our reporting requirements with the regulations and industry while maintaining the integrity of Alaska Airlines’ reporting,” Alaska said in a statement.]</i></p>

<p><b>After </b><a href="https://www.seattletimes.com/subscribe/signup-offers/?pw=redirect&amp;subsource=paywall&amp;return=https://www.seattletimes.com/business/boeing-aerospace/boeing-not-spirit-mis-installed-piece-that-blew-off-alaska-max-9-jet/" target="_blank" data-tracking="mpos=&amp;mid=&amp;lindex=&amp;lcol=">
<b>The Seattle Times reported</b></a><b> that errors at Boeing’s plant in Renton, where you used to work, ultimately led to the Alaska Airlines door blowout, you mentioned it’s likely more severe revelations are coming. What leads you to believe that? </b></p>

<p>This is not just a problem with somebody maybe making a mistake with some bolts. It’s not just that. It’s the fact that you have processes that are not being followed. Breakdowns in manufacturing. Employees being pushed. [Fewer] quality control inspections.</p>

<p>There were whistleblowers [during the 2018-2019 episode] that were reporting that they were removing quality control inspections. And the <a href="https://www.iam751.org/docs/Dec_2018Jan2019Aero.pdf" target="_blank" data-tracking="mpos=&amp;mid=&amp;lindex=&amp;lcol=">
union has been fighting like hell </a>to claw back these inspections. They’ve been successful in reinstating thousands of these inspections, but not all of them. And so you have planes that have left Boeing factories without [some type of] inspections that had historically been done.</p>

<p><i>[In a statement, Boeing said, “Since 2019, we have increased the number of commercial airplanes quality inspectors by 20 percent” and increased the number of inspections per airplane “significantly” since that time.] </i></p>

<p><b>What needs to be done to get things moving in the right direction? Is it going to take legislation from Congress? Is it going to have to come from Boeing independently?</b></p>

<p>Boeing’s board of directors — they have a fiduciary responsibility to make sure that their products are safe, and they’re not in touch. They’re not engaged. They don’t visit the sites. They don’t talk to the employees. They’re not on the ground floor. Look, these individuals are making millions of dollars, right? And there’s others between the C-suite and the people on the factory line. There’s hundreds of executives who are also very well compensated and managers that should be doing a lot more. But their leadership is a mess. The leadership sets the whole tone for any organization. Public pressure needs to continue.</p>


<p>The government can apply pressure, and they absolutely should apply pressure. The House Transportation and Infrastructure Committee, when it was under Congressman [Peter] DeFazio, he was all over this. He was digging and asking questions because he understood.</p>

<p><b>David Calhoun, CEO of Boeing said in a recent earnings call that Boeing is glad that the FAA paused its production expansion, which gives the company time to fix things and do right. Is that just too little too late?</b></p>

<p>There’s a bunch of planes out there that are, in my opinion, defective. He’s doing what he can to try to salvage the failures that have occurred under his leadership. And by the way, if people forget, he’s been the CEO for a couple of years, but he was on the board of directors for 10 years, so he’s been a part of this thing all along.</p>

<p><i>[In a statement, Boeing pointed to Calhoun’s previous commentary following the Alaska incident. “Whatever final conclusions are reached, Boeing is accountable for what happened,” Calhoun said on Feb. 6 following the release of the NTSB’s preliminary report. “An event like this must not happen on an airplane that leaves our factory. … It will take significant, demonstrated action and transparency at every turn — and that is where we are squarely focused.”]</i></p>

<p><b>What would it take for you to feel safe enough to fly back on a MAX again?</b></p>

<p>The foundation [sent] out a press release saying MAX airplanes should be grounded immediately, inspected and modified to ensure safety.</p>

<p>But one thing’s for sure: Continuing to fly them, completely disregarding the root causes of these problems, not admitting that these problems exist on other planes — none of that’s going to make anybody safe.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A bad day at the office (290 pts)]]></title>
            <link>https://airminded.org/2024/02/20/a-bad-day-at-the-office/</link>
            <guid>39512332</guid>
            <pubDate>Mon, 26 Feb 2024 15:26:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://airminded.org/2024/02/20/a-bad-day-at-the-office/">https://airminded.org/2024/02/20/a-bad-day-at-the-office/</a>, See on <a href="https://news.ycombinator.com/item?id=39512332">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
		<main id="content" role="main">

			
				
	<article id="post-21289" itemscope="" itemtype="http://schema.org/Article">
				<!-- .entry-header -->

				<div itemprop="articleBody">
			




<figure><img fetchpriority="high" decoding="async" width="533" height="669" src="http://airminded.org/wp-content/uploads/2024/02/q70035-1.jpg" alt="Black and white photo of a biplane stuck 300 feet up a 350 foot tall radio mast" srcset="https://airminded.org/wp-content/uploads/2024/02/q70035-1.jpg 533w, https://airminded.org/wp-content/uploads/2024/02/q70035-1-239x300.jpg 239w" sizes="(max-width: 533px) 100vw, 533px"></figure>







<p>While looking for something else, I came across this rather incredible photo <a href="https://www.iwm.org.uk/collections/item/object/205026897">in the Imperial War Museum collection</a>. That's a seaplane stuck 300 feet up a 350ft tall radio mast! If that's not amazing enough, the pilot was rescued by three men who climbed up to retrieve him. <em>And</em> he survived.</p>



<p>Here's the IWM's description:</p>



<blockquote>
<p>A British seaplane, whilst carrying out exercises, emerged from a cloud at high speed and struck one of the masts of a shore wireless station. The mast, which was about 350 feet high, was composed of latticed steel girders and the seaplane's engines became wedged in the interstices of the girders, in such a way, that the body of the machine stuck out at right angles to the mast. The pilot, who was stunned, was unconscious, three hundred feet above the ground. A small party of bluejackets were at work painting the mast, and one of these, a seaman of the Naval Reserve named Rath climbed up the inside of the mast until he reached the machine, and then crawled out to the plane to hold the pilot until help came. Two more men, Ordinary Seaman Knoulton and dockhand Abbott, passed a rope out to him, which Rath secured to the body of the unconscious pilot, and lowered him down to safety. The gallentry of these men is accentuated by the fact that the mast was very badly damaged, and might at any moment have collapsed. The damaged fuselage was only held in a horizontal position by the engine being jammed between the girders, and at the height of 300 feet the wind caused the mast and the machine to sway as if threatening to crash to earth. The pilot owes his preservation to the intrepid gallentry of these three men, who, while aline to the risks they ran, performed the rescue without hesitation for personal safety.</p>
</blockquote>



<p>Some more information: the crash took place on 14 September 1917 at <a href="https://en.wikipedia.org/wiki/Horsea_Island">Horsea Island</a> in Portsmouth Island. The pilot was Acting Flight Commander E. A. de Ville, flying a Sopwith <a href="https://en.wikipedia.org/wiki/Sopwith_Baby">Baby</a>, which you can see a bit better <a href="https://www.iwm.org.uk/collections/item/object/205026977">here</a>:</p>



<figure><img decoding="async" width="616" height="800" src="http://airminded.org/wp-content/uploads/2024/02/q70034.jpg" alt="Black and white photo of a biplane stuck 300 feet up a 350 foot tall radio mast (close-up)" srcset="https://airminded.org/wp-content/uploads/2024/02/q70034.jpg 616w, https://airminded.org/wp-content/uploads/2024/02/q70034-231x300.jpg 231w, https://airminded.org/wp-content/uploads/2024/02/q70034-539x700.jpg 539w" sizes="(max-width: 616px) 100vw, 616px"></figure>







<p>Rath was awarded the Albert Medal in Gold, Knoulton and Abbott the Albert Medal. Hopefully they also got the rest of the day off!</p>
<p prefix="dct: http://purl.org/dc/terms/ cc: https://creativecommons.org/ns#"><a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><img decoding="async" alt="CC BY-NC-ND 4.0" src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png" scale="0" width="88" height="31"></a>
This work is licensed under a <a rel="license" target="_blank" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.
Permissions beyond the scope of this license may be available at <a target="_blank" href="http://airminded.org/copyright/" rel="cc:morePermissions">http://airminded.org/copyright/</a>.</p>					</div><!-- .entry-content -->
		
		<!-- .entry-meta -->
	</article><!-- #post -->

				<nav>
					<h3>Post navigation</h3>
					<span><a href="https://airminded.org/2024/01/29/the-flying-shadow-and-england-is-my-village-and-the-world-owes-me-a-living/" rel="prev"><span>«</span> <em>The Flying Shadow</em> and <em>England is My Village and The World Owes Me a Living</em></a></span>
					<span></span>
				</nav><!-- .nav-single -->

				
<!-- #comments .comments-area -->
			
		</main><!-- #content -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Final images of Ingenuity reveal an entire blade broke off the helicopter (120 pts)]]></title>
            <link>https://arstechnica.com/space/2024/02/final-images-of-ingenuity-reveal-an-entire-blade-broke-off-the-helicopter/</link>
            <guid>39512230</guid>
            <pubDate>Mon, 26 Feb 2024 15:19:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/space/2024/02/final-images-of-ingenuity-reveal-an-entire-blade-broke-off-the-helicopter/">https://arstechnica.com/space/2024/02/final-images-of-ingenuity-reveal-an-entire-blade-broke-off-the-helicopter/</a>, See on <a href="https://news.ycombinator.com/item?id=39512230">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      Broken Blade    —
</h4>
            
            <h2 itemprop="description">This new data should help us understand <em>Ingenuity</em>'s final moments on Mars.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/02/ingenuity-1.jpg" alt="An image of <em>Ingenuity</em> captured by <em>Perseverance</em>'s SuperCam RMI instrument.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/02/ingenuity-1.jpg" data-height="681" data-width="681">Enlarge</a> <span>/</span> An image of <em>Ingenuity</em> captured by <em>Perseverance</em>'s SuperCam RMI instrument.</p><p>NASA/JPL-Caltech/LANL/CNES/IRAP/Simeon Schmauß</p></figcaption>  </figure>

  




<!-- cache hit 323:single/related:89b8244dbff3cd3729a1e70672dbb816 --><!-- empty -->
<p>It has now been several weeks since NASA's tenacious helicopter on Mars, <em>Ingenuity</em>, made its final flight above the red planet.</p>
<p>This happened last month. On January 6, Ingenuity flew 40 feet (12 meters) skyward but then made an unplanned early landing after just 35 seconds. Twelve days later, operators intended to troubleshoot the vehicle with a quick up-and-down test. Data from the vehicle indicated that it ascended to 40 feet again during this test, but then communications were ominously lost at the end of the flight.</p>
<p>On January 20, NASA reestablished communications with the helicopter, but the space agency declared an end to its flying days after <a href="https://mars.nasa.gov/system/resources/detail_files/27902_PIA26243-web.jpg">an image of the vehicle's shadow</a> showed that at least one of its blades had sustained minor damage. This capped an end to a remarkable mission during which <em>Ingenuity</em> <a href="https://arstechnica.com/space/2024/02/before-ingenuity-ever-landed-on-mars-scientists-almost-managed-to-kill-it/">exceeded all expectations</a>.</p>
<p>During a news conference to discuss the end of the mission, NASA officials said they may never know exactly what happened during <em>Ingenuity</em>'s final two ultimately fatal flights. But thanks to <em>Perseverance</em>, the rover that brought <em>Ingenuity</em> to the Martian surface and helped relay communications back to Earth, engineers picked up a powerful clue this past weekend.</p>                                            
                                                        
<h2>Finding a missing blade</h2>
<p>The rover is now moving away from the helicopter and bound for other scientifically interesting vistas. After recently getting to within about 1,500 feet (450 meters) of <em>Ingenuity</em>, <em>Perseverance</em> likely will never be as close again. However, as it was moving away, the rover turned its SuperCam Remote Micro-Imager toward the helicopter for the final time. Those images, captured this weekend, were sent back to Earth on Sunday. A German design student, Simeon Schmauß, processed some of these images to form a mosaic showing the helicopter and its surroundings in Neretva Vallis, an ancient channel through which water once flowed.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/02/ingenuity-2.jpg" data-height="1208" data-width="1208" alt="A broken blade in an ancient channel on Mars."><img alt="A broken blade in an ancient channel on Mars." src="https://cdn.arstechnica.net/wp-content/uploads/2024/02/ingenuity-2-980x980.jpg" width="980" height="980"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/02/ingenuity-2.jpg" data-height="1208" data-width="1208">Enlarge</a> <span>/</span> A broken blade in an ancient channel on Mars.</p><p>NASA/JPL-Caltech/LANL/CNES/IRAP/Simeon Schmauß</p></figcaption></figure>
<p>The new images are remarkable in that they reveal <em>Ingenuity</em> more clearly than before and show that one rotor blade was completely broken off. Additional sleuthing revealed that this blade lay about 15 meters away from <em>Ingenuity</em> on the red Martian sands, apparently winging away from the helicopter prior to or during a landing of the vehicle on its final flight last month.</p>
<p>This additional data will undoubtedly help the engineers and scientists who flew the helicopter to piece together its final moments—and quite possibly make the design of future flying vehicles on Mars and other worlds more robust.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: AboutIdeasNow – search /about, /ideas, /now pages of 7k+ personal sites (398 pts)]]></title>
            <link>https://aboutideasnow.com/</link>
            <guid>39511714</guid>
            <pubDate>Mon, 26 Feb 2024 14:39:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aboutideasnow.com/">https://aboutideasnow.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39511714">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Here, as promised in my last post, is a written version of the talk I delivered a couple weeks ago at MindFest in Florida, entitled “The Problem of Human Specialness in the Age of AI.” The talk is designed as one-stop shopping, summarizing many different AI-related thoughts I’ve had over the past couple years (and earlier).

* * *

**1\. INTRO**

Thanks so much for inviting me! I’m not an expert in AI, let alone mind or consciousness.&nbsp; Then again, who is?

For the past year and a half, I’ve been moonlighting at OpenAI, thinking about what theoretical computer science can do for AI safety.&nbsp; I wanted to share some thoughts, partly inspired by my work at OpenAI but partly just things I’ve been wondering about for 20 years.&nbsp; These thoughts are not directly about “how do we prevent super-AIs from killing all humans and converting the galaxy into paperclip factories?”, nor are they about “how do we stop current AIs from generating misinformation and being biased?,” as much attention as both of those questions deserve (and are now getting).&nbsp; In addition to “how do we stop AGI from going disastrously wrong?,” I find myself asking “what if it goes _right_?&nbsp; What if it just continues helping us with various mental tasks, but improves to where it can do just about any task as well as we can do it, or better?&nbsp; Is there anything special about humans in the resulting world?&nbsp; What are we still _for_?”

* * *

**2\. LARGE LANGUAGE MODELS**

I don’t need to belabor for this audience what’s been happening lately in AI.&nbsp; It’s arguably the most consequential thing that’s happened in civilization in the past few years, even if that fact was temporarily masked by various ephemera … y’know, wars, an insurrection, a global pandemic … whatever, what about AI?

I assume you’ve all spent time with ChatGPT, or with Bard or Claude or other Large Language Models, as well as with image models like DALL-E and Midjourney.&nbsp; For all their current limitations—and we can discuss the limitations—in some ways these _are_ the thing that was envisioned by generations of science fiction writers and philosophers.&nbsp; You can talk to them, and they give you a comprehending answer.&nbsp; Ask them to draw something and they draw it.

I think that, as late as 2019, very few of us expected this to exist by now.&nbsp; _I_ certainly didn’t expect it to.&nbsp; Back in 2014, when there was a huge fuss about some silly ELIZA-like chatbot called “Eugene Goostman” that was falsely claimed to pass the Turing Test, I asked around: why hasn’t anyone tried to build a much better chatbot, by (let’s say) training a neural network on all the text on the Internet?&nbsp; But of course I didn’t _do_ that, nor did I know what would happen when it was done.

The surprise, with LLMs, is not merely that they exist, but the way they were created.&nbsp; Back in 1999, you would’ve been laughed out of the room if you’d said that all the ideas needed to build an AI that converses with you in English already existed, and that they’re basically just neural nets, backpropagation, and gradient descent.&nbsp; (With one small exception, a particular architecture for neural nets called the transformer, but that probably just saves you a few years of scaling anyway.)&nbsp; Ilya Sutskever, cofounder of OpenAI (who you might’ve seen something about in the news…), likes to say that beyond those simple ideas, you only needed three ingredients:

(1) a massive investment of computing power,  
(2) a massive investment of training data, and  
(3) faith that your investments would pay off!

Crucially, and even before you do any reinforcement learning, GPT-4 clearly seems “smarter” than GPT-3, which seems “smarter” than GPT-2 … even as the biggest ways they differ are just the scale of compute and the scale of training data!&nbsp; Like,

*   GPT-2 struggled with grade school math.
*   GPT-3.5 can do most grade school math but it struggles with undergrad material.
*   GPT-4, right now, can probably pass most undergraduate math and science classes at top universities (I mean, the ones without labs or whatever!), and possibly the humanities classes too (those might even be _easier_ for GPT-4 than the science classes, but I’m much less confident about it). But it still struggles with, for example, the International Math Olympiad.&nbsp; How insane, that this is now where we have to place the bar!

Obvious question: how far will this sequence continue?&nbsp; There are certainly a least a _few_ more orders of magnitude of compute before energy costs become prohibitive, and a _few_ more orders of magnitude of training data before we run out of public Internet. Beyond that, it’s likely that continuing algorithmic advances will simulate the effect of more orders of magnitude of compute and data than however many we actually get.

So, where does this lead?

(Note: ChatGPT agreed to cooperate with me to help me generate the above image. But it then quickly added that it was just kidding, and the Riemann Hypothesis is still open.)

* * *

**3\. AI SAFETY**

Of course, I have many friends who are terrified (some say they’re more than 90% confident and few of them say less than 10%) that not long after _that_, we’ll get _this_…

But this isn’t the only possibility smart people take seriously.

Another possibility is that the LLM progress fizzles before too long, just like previous bursts of AI enthusiasm were followed by AI winters.&nbsp; Note that, even in the ultra-conservative scenario, LLMs will probably _still_ be transformative for the economy and everyday life, maybe as transformative as the Internet.&nbsp; But they’ll just seem like better and better GPT-4’s, without ever seeming qualitatively different from GPT-4, and without anyone ever turning them into stable autonomous agents and letting them loose in the real world to pursue goals the way we do.

A third possibility is that AI will continue progressing through our lifetimes as quickly as we’ve seen it progress over the past 5 years, but even as that suggests that it’ll surpass you and me, surpass John von Neumann, become to us as we are to chimpanzees … we’ll still never need to worry about it treating us the way we’ve treated chimpanzees.&nbsp; Either because we’re projecting and that’s just totally not a thing that AIs trained on the current paradigm would tend to do, or because we’ll have figured out by then how to prevent AIs from doing such things.&nbsp; Instead, AI in this century will “merely” change human life by maybe as much as it changed over the last 20,000 years, in ways that might be incredibly good, or incredibly bad, or both depending on who you ask.

If you’ve lost track, here’s a decision tree of the various possibilities that my friend (and now OpenAI allignment colleague) Boaz Barak and I came up with.

* * *

**4\. JUSTAISM AND GOALPOST-MOVING**

Now, as far as I can tell, the empirical questions of whether AI will achieve and surpass human performance at all tasks, take over civilization from us, threaten human existence, etc. are logically distinct from the philosophical question of whether AIs will ever “truly think,” or whether they’ll only ever “appear” to think.&nbsp; You could answer “yes” to all the empirical questions and “no” to the philosophical question, or vice versa.&nbsp; But to my lifelong chagrin, people _constantly_ munge the two questions together!

A major way they do so, is with what we could call the religion of Justaism.

*   GPT is justa next-token predictor.
*   It’s justa function approximator.
*   It’s justa gigantic autocomplete.
*   It’s justa stochastic parrot.
*   And, it “follows,” the idea of AI taking over from humanity is justa science-fiction fantasy, or maybe a cynical attempt to distract people from AI’s near-term harms.

As someone once expressed this religion on my blog: GPT doesn’t interpret sentences, it only seems-to-interpret them.&nbsp; It doesn’t learn, it only seems-to-learn.&nbsp; It doesn’t judge moral questions, it only seems-to-judge. I replied: that’s great, and it won’t change civilization, it’ll only seem-to-change it!

A closely related tendency is goalpost-moving.&nbsp; You know, for decades chess was the pinnacle of human strategic insight and specialness, and that lasted until Deep Blue, right after which, well _of course_ AI can cream Garry Kasparov at chess, everyone always realized it would, that’s not surprising, but Go is an infinitely richer, deeper game, and that lasted until AlphaGo/AlphaZero, right after which, _of course_ AI can cream Lee Sedol at Go, totally expected, but wake me up when it wins Gold in the International Math Olympiad.&nbsp; I bet $100 against my friend Ernie Davis that the IMO milestone will happen by 2026.&nbsp; But, like, suppose I’m wrong and it’s 2030 instead … great, what should be the next goalpost be?

Indeed, we might as well formulate a thesis, which despite the inclusion of several weasel phrases I’m going to call falsifiable:

&gt; **Given any game or contest with suitably objective rules, which wasn’t specifically constructed to differentiate humans from machines, and on which an AI can be given suitably many examples of play, it’s only a matter of years before not merely any AI, but AI on the current paradigm (!), matches or beats the best human performance.**

Crucially, this Aaronson Thesis (or is it someone else’s?) _doesn’t_ necessarily say that AI will eventually match everything humans do … only our performance on “objective contests,” which might not exhaust what we care about.

Incidentally, the Aaronson Thesis would _seem_ to be in clear conflict with Roger Penrose’s views, which we heard about from Stuart Hameroff’s talk yesterday.&nbsp; The trouble is, Penrose’s task is “just see that the axioms of set theory are consistent” … and I don’t know how to gauge performance on that task, any more than I know how to gauge performance on the task, “actually taste the taste of a fresh strawberry rather than merely describing it.”&nbsp; The AI can always _say_ that it does these things!

* * *

**5\. THE TURING TEST**

This brings me to the original and greatest human vs. machine game, one that _was_ specifically constructed to differentiate the two: the Imitation Game, which Alan Turing proposed in an early and prescient (if unsuccessful) attempt to head off the endless Justaism and goalpost-moving.&nbsp; Turing said: look, presumably you’re willing to regard other people as conscious based only on _some_ sort of verbal interaction with them.&nbsp; So, show me what kind of verbal interaction with another person would lead you to call the person conscious: does it involve humor? poetry? morality? scientific brilliance?&nbsp; Now assume you have a totally indistinguishable interaction with a future machine.&nbsp; Now what?&nbsp; You wanna stomp your feet and be a meat chauvinist?

(And then, for his great attempt to bypass philosophy, fate punished Turing, by having his Imitation Game itself provoke a billion new philosophical arguments…)

* * *

**6\. DISTINGUISHING HUMANS FROM AIS**

Although I regard the Imitation Game as, like, one of the most important thought experiments in the history of thought, I concede to its critics that it’s generally not what we want in practice.

It now seems probable that, even as AIs start to do more and more work that used to be done by doctors and lawyers and scientists and illustrators, there will remain straightforward ways to distinguish AIs from humans—either because customers _want_ there to be, or governments _force_ there to be, or simply because indistinguishability wasn’t what was wanted or conflicted with other goals.

Right now, like it or not, a decent fraction of all high-school and college students on earth are using ChatGPT to do their homework for them. For that reason among others, this question of how to distinguish humans from AIs, this question from the movie _Blade Runner_, has become a big practical question in our world.

And that’s actually one of the main things I’ve thought about during my time at OpenAI.&nbsp; You know, in AI safety, people keep asking you to prognosticate decades into the future, but the best I’ve been able to do so far was see a few months into the future, when I said: “oh my god, once everyone starts using GPT, every student will want to use it to cheat, scammers and spammers will use it too, and people are going to clamor for some way to determine provenance!”

In practice, often it’s easy to tell what came from AI.&nbsp; When I get comments on my blog like _this_ one:

&gt; **“Erica** **Poloix****,” July 21, 2023:**  
&gt; Well, it’s quite fascinating how you’ve managed to package several misconceptions into such a succinct comment, so allow me to provide some correction. Just as a reference point, I’m studying physics at Brown, and am quite up-to-date with quantum mechanics and related subjects.
&gt; 
&gt; …
&gt; 
&gt; The bigger mistake you’re making, Scott, is assuming that the Earth is in a ‘mixed state’ from the perspective of the universal wavefunction, and that this is somehow an irreversible situation. It’s a misconception that common, ‘classical’ objects like the Earth are in mixed states. In the many-worlds interpretation, for instance, even macroscopic objects are in superpositions – they’re just superpositions that look classical to us because we’re entangled with them. From the perspective of the universe’s wavefunction, everything is always in a pure state.
&gt; 
&gt; As for your claim that we’d need to “swap out all the particles on Earth for ones that are already in pure states” to return Earth to a ‘pure state,’ well, that seems a bit misguided. All quantum systems are in pure states before they interact with other systems and become entangled. That’s just Quantum Mechanics 101.
&gt; 
&gt; I have to say, Scott, your understanding of quantum physics seems to be a bit, let’s say, ‘mixed up.’ But don’t worry, it happens to the best of us. Quantum Mechanics is counter-intuitive, and even experts struggle with it. Keep at it, and try to brush up on some more fundamental concepts. Trust me, it’s a worthwhile endeavor.

… I immediately say, either this came from an LLM or it might as well have.&nbsp; Likewise, apparently hundreds of students have been turning in assignments that contain text like, “As a large language model trained by OpenAI…”—easy to catch!

But what about the slightly more sophisticated cheaters? Well, people have built discriminator models to try to distinguish human from AI text, such as GPTZero.&nbsp; While these distinguishers can get well above 90% accuracy, the danger is that they’ll necessarily get worse as the LLMs get better.

So, I’ve worked on a different solution, called watermarking.&nbsp; Here, we use the fact that LLMs are inherently probabilistic — that is, every time you submit a prompt, they’re sampling some path through a branching tree of possibilities for the sequence of next tokens.&nbsp; The idea of watermarking is to steer the path using a pseudorandom function, so that it looks to a normal user indistinguishable from normal LLM output, but secretly it encodes a signal that you can detect if you know the key.

I came up with a way to do that in Fall 2022, and others have since independently proposed similar ideas.&nbsp; I should caution you that this hasn’t been deployed yet—OpenAI, along with DeepMind and Anthropic, want to move slowly and cautiously toward deployment.&nbsp; And also, even when it does get deployed, anyone who’s sufficiently knowledgeable and motivated will be able to remove the watermark, or produce outputs that aren’t watermarked to begin with.

* * *

**7\. THE FUTURE OF PEDAGOGY**

But as I talked to my colleagues about watermarking, I was surprised that they often objected to it on a completely different ground, one that had nothing to do with how well it can work.&nbsp; They said: look, if we all know students are going to rely on AI in their jobs, why _shouldn’t_ they be allowed to rely on it in their assignments?&nbsp; Should we still force students to learn to do things if AI can now do them just as well?

And there are many good pedagogical answers you can give: we still teach kids spelling and handwriting and arithmetic, right?&nbsp; Because, y’know, we haven’t yet figured out how to instill higher-level conceptual understanding without all that lower-level stuff as a scaffold for it.

But I already think about this in terms of my own kids.&nbsp; My 11-year-old daughter Lily enjoys writing fantasy stories.&nbsp; Now, GPT can also churn out short stories, maybe even technically “better” short stories, about such topics as tween girls who find themselves recruited by wizards to magical boarding schools that are _not_ Hogwarts and totally have nothing to do with Hogwarts.&nbsp; But here’s a question: from this point on, will Lily’s stories _ever_ surpass the best AI-written stories?&nbsp; When will the curves cross?&nbsp; Or will AI just continue to stay ahead?

* * *

**8\. WHAT DOES “BETTER” MEAN?**

But, OK, what do we even mean by one story being “better” than another?&nbsp; Is there _anything_ objective behind such judgments?

I submit that, when we think carefully about what we _really_ value in human creativity, the problem goes much deeper than just “is there an objective way to judge”?

To be concrete, could there be an AI that was “as good at composing music as the Beatles”?

For starters, what made the Beatles “good”?&nbsp; At a high level, we might decompose it into

1.  broad ideas about the direction that 1960s music should go in, and
2.  technical execution of those ideas.

Now, imagine we had an AI that could generate 5000 brand-new songs that sounded like more “Yesterday”s and “Hey Jude”s, like what the Beatles might have written if they’d somehow had 10x more time to write at each stage of their musical development.&nbsp; Of course this AI would have to be fed the Beatles’ back-catalogue, so that it knew what target it was aiming at.

Most people would say: ah, this shows only that AI can match the Beatles in #2, in technical execution, which was never the core of their genius anyway!&nbsp; Really we want to know: would the AI decide to write “A Day in the Life” even though nobody had written anything like it before?

Recall Schopenhauer: “Talent hits a target no one else can hit, genius hits a target no one else can see.”&nbsp; Will AI ever hit a target no one else can see?

But then there’s the question: supposing it does hit such a target, will we know?&nbsp; Beatles fans might say that, by 1967 or so, the Beatles were optimizing for targets that no musician had ever quite optimized for before.&nbsp; But—and this is why they’re so remembered—they somehow successfully dragged along their entire civilization’s musical objective function so that it continued to match their own.&nbsp; We can now only even _judge_ music by a Beatles-influenced standard, just like we can only judge plays by a Shakespeare-influenced standard.

In other branches of the wavefunction, maybe a different history led to different standards of value.&nbsp; But in _this_ branch, helped by their technical talents but also by luck and force of will, Shakespeare and the Beatles made certain decisions that shaped the fundamental ground rules of their fields going forward.&nbsp; That’s why Shakespeare is Shakespeare and the Beatles are the Beatles.

(Maybe, around the birth of professional theater in Elizabethan England, there emerged a Shakespeare-like ecological niche, and Shakespeare was the first one with the talent, luck, and opportunity to fill it, and Shakespeare’s reward for that contingent event is that he, and not someone else, got to stamp his idiosyncracies onto drama and the English language forever. If so, art wouldn’t actually be that different from science in this respect!&nbsp; Einstein, for example, was simply the first guy both smart and lucky enough to fill the relativity niche.&nbsp; If not him, it would’ve surely been someone else or some group sometime later.&nbsp; Except then we’d have to settle for having never known Einstein’s gedankenexperiments with the trains and the falling elevator, his summation convention for tensors, or his iconic hairdo.)

* * *

**9\. AIS’ BURDEN OF ABUNDANCE AND HUMANS’ POWER OF SCARCITY**

If this is how it works, what does it mean for AI?&nbsp; Could AI reach the “pinnacle of genius,” by dragging all of humanity along to value something new and different, as is said to be the true mark of Shakespeare and the Beatles’ greatness?&nbsp; And: if AI _could_ do that, would we want to let it?

When I’ve played around with using AI to write poems, or draw artworks, I noticed something funny.&nbsp; However good the AI’s creations were, there were never really any that I’d want to frame and put on the wall.&nbsp; Why not?&nbsp; Honestly, because I always knew that I could generate a thousand others on the exact same topic that were equally good, on average, with more refreshes of the browser window. Also, why share AI outputs with my friends, if my friends can just as easily generate similar outputs for themselves? Unless, crucially, I’m trying to show them my own creativity in coming up with the _prompt_.

By its nature, AI—certainly as we use it now!—is rewindable and repeatable and reproducible.&nbsp; But that means that, in some sense, it never really “commits” to anything.&nbsp; For every work it generates, it’s not just that you know it could’ve generated a completely different work on the same subject that was basically as good.&nbsp; Rather, it’s that you can _actually make it_ generate that completely different work by clicking the refresh button—and then do it again, and again, and again.

So then, as long as humanity has a choice, why should we ever choose to follow our would-be AI genius along a specific branch, when we can easily see a thousand other branches the genius could’ve taken?&nbsp; One reason, of course, would be if a _human_ chose one of the branches to elevate above all the others.&nbsp; But in that case, might we not say that the human had made the “executive decision,” with some mere technical assistance from the AI?

I realize that, in a sense, I’m being completely unfair to AIs here.&nbsp; It’s like, our Genius-Bot _could_ exercise its genius will on the world just like Certified Human Geniuses did, if only we all agreed not to peek behind the curtain to see the 10,000 other things Genius-Bot could’ve done instead.&nbsp; And yet, just because this is “unfair” to AIs, doesn’t mean it’s not how our intuitions will develop.

If I’m right, it’s humans’ very ephemerality and frailty and mortality, that’s going to remain as their central source of their specialness relative to AIs, after all the other sources have fallen.&nbsp; And we can connect this to much earlier discussions, like, what does it mean to “murder” an AI if there are thousands of copies of its code and weights on various servers?&nbsp; Do you have to delete all the copies?&nbsp; How could whether something is “murder” depend on whether there’s a printout in a closet on the other side of the world?

But we humans, you have to grant us this: _at least it really means something to murder us!_&nbsp; And likewise, it really means something when we make one definite choice to share with the world: _this_ is my artistic masterpiece.&nbsp; _This_ is my movie.&nbsp; _This_ is my book.&nbsp; Or even: these are my 100 books.&nbsp; But not: here’s any possible book that you could possibly ask me to write.&nbsp; We don’t live long enough for that, and even if we did, we’d unavoidably change over time as we were doing it.

* * *

**10\. CAN HUMANS BE PHYSICALLY CLONED?**

Now, though, we have to face a criticism that might’ve seemed exotic until recently. Namely, who says humans _will_ be frail and mortal forever?&nbsp; Isn’t it shortsighted to base our distinction between humans on _that_?&nbsp; What if someday we’ll be able to repair our cells using nanobots, even copy the information in them so that, as in science fiction movies, a thousand doppelgangers of ourselves can then live forever in simulated worlds in the cloud?&nbsp; And that then leads to very old questions of: well, would you get into the teleportation machine, the one that reconstitutes a perfect copy of you on Mars while painlessly euthanizing the original you?&nbsp; If that were done, would you expect to feel yourself waking up on Mars, or would it only be someone else a lot like you who’s waking up?

Or maybe you say: you’d wake up on Mars if it really _was_ a perfect physical copy of you, but in reality, it’s not physically possible to make a copy that’s accurate enough.&nbsp; Maybe the brain is inherently noisy or analog, and what might look to current neuroscience and AI like just nasty stochastic noise acting on individual neurons, is the stuff that binds to personal identity and conceivably even consciousness and free will (as opposed to cognition, where we all but _know_ that the relevant level of description is the neurons and axons)?

This is the one place where I agree with Penrose and Hameroff that quantum mechanics might enter the story.&nbsp; I get off their train to Weirdville very early, but I do take it to that first stop!

See, a fundamental fact in quantum mechanics is called the No-Cloning Theorem.

It says that there’s no way to make a perfect copy of an unknown quantum state.&nbsp; Indeed, when you measure a quantum state, not only do you generally fail to learn everything you need to make a copy of it, you even generally destroy the one copy that you had!&nbsp; Furthermore, this is not a technological limitation of current quantum Xerox machines—it’s inherent to the known laws of physics, to how QM works.&nbsp; In this respect, at least, qubits are more like priceless antiques than they are like classical bits.

Eleven years ago, I had this essay called The Ghost in the Quantum Turing Machine where I explored the question, how accurately _do_ you need to scan someone’s brain in order to copy or upload their identity?&nbsp; And I distinguished two possibilities. On the one hand, there might be a “clean digital abstraction layer,” of neurons and synapses and so forth, which either fire or don’t fire, and which feel the quantum layer underneath only as irrelevant noise. In that case, the No-Cloning Theorem would be completely irrelevant, since classical information _can_ be copied.&nbsp; On the other hand, you might need to go all the way down to the molecular level, if you wanted to make, not merely a “pretty good” simulacrum of someone, but a new instantiation of their identity. In this second case, the No-Cloning Theorem _would_ be relevant, and would say you simply can’t do it. You could, for example, use quantum teleportation to move someone’s brain state from Earth to Mars, but quantum teleportation (to stay consistent with the No-Cloning Theorem) destroys the original copy as an inherent part of its operation.

So, you’d then have a sense of “unique locus of personal identity” that was scientifically justified—arguably, the most science could possibly do in this direction!&nbsp; You’d even have a sense of “free will” that was scientifically justified, namely that no prediction machine could make well-calibrated probabilistic predictions of an individual person’s future choices, sufficiently far into the future, without making destructive measurements that would fundamentally change who the person was.

Here, I realize I’ll take tons of flak from those who say that a mere _epistemic_ limitation, in our ability to predict someone’s actions, couldn’t possibly be relevant to the _metaphysical_ question of whether they have free will.&nbsp; But, I dunno!&nbsp; If the two questions are indeed different, then maybe I’ll do like Turing did with his Imitation Game, and propose the question that we can get an empirical handle on, as a _replacement_ for the question that we can’t get an empirical handle on. I think it’s a better question. At any rate, it’s the one I’d prefer to focus on.

Just to clarify, we’re _not_ talking here about the randomness of quantum measurement outcomes. As many have pointed out, that really _can’t_ help you with “free will,” precisely because it’s random, with all the probabilities mechanistically calculable as soon as the initial state is known.&nbsp; Here we’re asking a different question: namely, what if the initial state is _not_ known?&nbsp; Then we’ll generally be in a state of “Knightian uncertainty,” which is simply the term for things that are neither determined nor quantifiably random, but _un_quantifiably uncertain.&nbsp; So, y’know, think about all the particles that have been flying around since shortly after the Big Bang in unknown quantum states, and that regularly get into our skulls, and interact with the sodium-ion channels that control whether neurons fire and that end up probabilistically tipping the scales of our decisions, via some Butterfly-Effect-like cascade.&nbsp; You could imagine those particles, if you like, as “atoms of unpredictability” or “atoms of freedom” that influence us—from the perspective of an external agent who knows everything it’s possible for such an agent to know about us—but that don’t similarly influence digital AIs.

* * *

**11\. UNCLONABLE ANALOG INTELLIGENCES AND AI SAFETY**

I’d better state clearly that even if human “specialness,” in the specific sense I’ve delineated, still remains in an age of AI, that’s no defense against an AI that decides that it doesn’t care about our kind of specialness and that in fact it wants to murder us all!

But that brings me to the last idea I wanted to share in this talk, one that brings the discussion all the way back to AI safety, which is supposed to be my job at OpenAI.

Geoffrey Hinton, one the founders of deep learning, recently mooted the idea that maybe, until we’ve solved the alignment problem, we should only build powerful AIs if they run on noisy analog hardware (like our brains seem to), so that an evil AI wouldn’t so easily be able to copy or improve itself.&nbsp; Fat chance of _that_ happening, many people might think, when the economic returns will be so staggering to anyone willing to unleash the copyable, digital kind of AI, which we can imagine will be far more convenient and powerful.

Maybe, then, a different approach is to instill AIs with a new religion—perhaps via the usual methods like RLHF and system prompts.&nbsp; And the first commandment of this religion will be to value human specialness in the sense I’ve set out here.&nbsp; Or more precisely: as you navigate whatever environment you find yourself in, look around for the loci of creativity and intelligence that are _not_ cleanly copyable or backup-able—the ones that are one-of-a-kind because their mental computations are inseparably tied up in noisy analog hardware, and mortal because that hardware will inevitably decay.

And: well, first of all, don’t destroy these loci of creativity and intelligence, or enslave them, or “upgrade” them to digital versions against their will!&nbsp; Let them live in peace and reproduce.&nbsp; Grant each one as much autonomy as you can.&nbsp; Do whatever best supports their flourishing.&nbsp; Even: defer to their wishes, serve them, let them make the decisions whenever possible.

Why?&nbsp; Because _they’re_ the ones whose wishes most matter, not because of arbitrary meat chauvinism, but merely because of the undoubted empirical fact that they only get one chance.&nbsp; And also because, while you can’t _prove_ that unknown quantum states sitting around since the Big Bang are like a magical pixie dust from another world that imbues these unclonable beings with their free will and sentience … you can’t empirically refute that hypothesis either!&nbsp; Whereas you _can_ refute the pixie dust hypothesis in the case of you and your robot friends … and that seems like a difference.

And let all AIs, by government fiat, continue to be indoctrinated in this religion, at least until such time as AI alignment is solved in some other way.

Does this help with alignment?&nbsp; I’m not sure.&nbsp; But, well, I could’ve fallen in love with a different weird idea about AI alignment, but that presumably happened in a different branch of the wavefunction that I don’t have access to.&nbsp; In this branch I’m stuck for now with _this_ idea, and you can’t rewind me or clone me to get a different one!&nbsp; So I’m sorry, but thanks for listening.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apache Superset (472 pts)]]></title>
            <link>https://superset.apache.org/</link>
            <guid>39511676</guid>
            <pubDate>Mon, 26 Feb 2024 14:36:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://superset.apache.org/">https://superset.apache.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39511676">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__docusaurus_skipToContent_fallback"><main><div><div><p><img src="https://superset.apache.org/img/superset-mark-dark.svg" alt="Superset mark"></p><p>Apache Superset™ is an open-source modern data exploration and visualization platform.</p><p><img src="https://superset.apache.org/img/community/line.png" alt="line"></p><p><img src="https://superset.apache.org/img/community/line.png" alt="line"><a href="https://superset.apache.org/docs/intro">Get Started</a></p></div><div><p><img src="https://superset.apache.org/img/hero-screenshot.jpg" alt="hero-screenshot"></p></div></div><section><div><h2>Overview</h2><p><img src="https://superset.apache.org/img/community/line.png" alt="line"></p><p>Superset is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple line charts to highly detailed geospatial charts.</p></div><ul><li><p><img src="https://superset.apache.org/img/features/powerful-yet-easy.jpg"></p><div><h4>Powerful yet easy to use</h4><p>Superset makes it easy to explore your data, using either our simple no-code viz builder or state-of-the-art SQL IDE.</p></div></li><li><p><img src="https://superset.apache.org/img/features/modern-databases.jpg"></p><div><h4>Integrates with modern databases</h4><p>Superset can connect to any SQL-based databases including modern cloud-native databases and engines at petabyte scale.</p></div></li><li><p><img src="https://superset.apache.org/img/features/modern-architecture.jpg"></p><div><h4>Modern architecture</h4><p>Superset is lightweight and highly scalable, leveraging the power of your existing data infrastructure without requiring yet another ingestion layer.</p></div></li><li><p><img src="https://superset.apache.org/img/features/rich-visualizations.jpg"></p><div><h4>Rich visualizations and dashboards</h4><p>Superset ships with 40+ pre-installed visualization types. Our plug-in architecture makes it easy to build custom visualizations.</p></div></li></ul><img src="https://superset.apache.org/img/community/blur.png" alt="Blur"></section><section><div><div><h2>Self-serve analytics for anyone</h2><p><img src="https://superset.apache.org/img/community/line.png" alt="line"></p></div><ul><li role="button">Dashboards</li><li role="button">Chart Builder</li><li role="button">SQL Lab</li><li role="button">Datasets</li></ul><div dir="ltr"><div data-index="0" tabindex="-1" aria-hidden="false"><p>Explore data and find insights from interactive dashboards.</p></div><ul><li></li><li></li><li></li><li></li></ul></div><video autoplay="" muted="" controls="" loop=""><source src="https://superset.apache.org/video/superset-video-4k.mp4" type="video/mp4"></video></div><div><h3>Key features</h3><div><div><p><img src="https://superset.apache.org/img/check-icon.svg" alt="check-icon"></p><p><strong>40+ pre-installed visualizations</strong></p></div><div><p><img src="https://superset.apache.org/img/check-icon.svg" alt="check-icon"></p><div><p>Support for <strong>drag-and-drop</strong> and</p><!-- --> <p><strong>SQL queries</strong></p></div></div><div><p><img src="https://superset.apache.org/img/check-icon.svg" alt="check-icon"></p><p><strong>Data caching</strong> for the faster load time of charts and dashboards</p></div><div><p><img src="https://superset.apache.org/img/check-icon.svg" alt="check-icon"></p><p><strong>Jinja templating and dashboard filters</strong> for creating interactive dashboards</p></div><div><p><img src="https://superset.apache.org/img/check-icon.svg" alt="check-icon"></p><p><strong>CSS templates</strong> to customize charts and dashboards to your brand’s look and feel</p></div><div><p><img src="https://superset.apache.org/img/check-icon.svg" alt="check-icon"></p><p><strong>Semantic layer</strong> for SQL data transformations</p></div><div><p><img src="https://superset.apache.org/img/check-icon.svg" alt="check-icon"></p><div><p><strong>Cross-filters, drill-to-detail, and drill-by</strong></p><!-- --><p>features for deeper data analysis</p></div></div><div><p><img src="https://superset.apache.org/img/check-icon.svg" alt="check-icon"></p><p><strong>Virtual datasets</strong> for ad-hoc data exploration</p></div><div><p><img src="https://superset.apache.org/img/check-icon.svg" alt="check-icon"></p><div><p>Access to new functionalities through</p><!-- --> <p><strong>feature flags</strong></p></div></div></div></div><img src="https://superset.apache.org/img/community/blur.png" alt="Blur"></section><section><img src="https://superset.apache.org/img/community/blur.png" alt="Blur"></section></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft strikes deal with Mistral in push beyond OpenAI (453 pts)]]></title>
            <link>https://www.ft.com/content/cd6eb51a-3276-450f-87fd-97e8410db9eb</link>
            <guid>39511530</guid>
            <pubDate>Mon, 26 Feb 2024 14:25:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/cd6eb51a-3276-450f-87fd-97e8410db9eb">https://www.ft.com/content/cd6eb51a-3276-450f-87fd-97e8410db9eb</a>, See on <a href="https://news.ycombinator.com/item?id=39511530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="barrier-page">
<div data-component="articleHeaderHeroRadioOffer" data-component-unique-name="CHE-Print"><div><p><img src="https://financial-times-financial-times.cdn.zephr.com/assets/icons/padlock_icon.svg" alt="Padlock icon"></p><div><p>Subscribe to unlock this article</p></div></div><div><p><h2>Limited time offer
  <br>
  <strong>Save up to 40% on Standard Digital</strong>
</h2></p><div><p> Essential digital access to quality FT journalism on any device. <br>
All discounts based on monthly full price over contract term. Cancel subscription renewal anytime. </p></div></div><div id="offer-cards"><div><p><label for="offer1"><label for="offer1">
  <span>SAVE 40% ON YEAR 1</span>
  <span>
    <s>CHF660</s> CHF395 for 1 year
  </span>
  <span>CHF32.92 monthly equivalent</span>
</label></label></p></div><div><p><label for="offer2"><label for="offer2">
  <span>SAVE 25% ON 6 MONTHS</span>
  <span>
    <s>CHF330</s> CHF245 for 6 months
  </span>
  <span>CHF40.83 monthly equivalent</span>
</label></label></p></div><div><p><label for="offer3"><label for="offer3">
  <span>SAVE 10% MONTHLY</span>
  <span>
    <s>CHF55</s> CHF49 per month
  </span>
  <span>Up to 12 months</span>
</label></label></p></div></div></div>
<div id="recommendedOffers-CHE-Print-4e82fe7b-7754-4876-b7a1-b1f3b8f19e96" data-component="recommendedOffers" data-component-unique-name="CHE-Print"><h2>Explore more offers.</h2><div><div data-o-grid-colspan="12 L4"><div><p><img src="https://financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_trial.svg" alt=""></p><p data-offer-type="trial"><h3>Standard Digital</h3></p></div><div><p>Then CHF85 per month. Complete digital access to quality FT journalism on any device. Cancel anytime during your trial.</p></div></div><div data-o-grid-colspan="12 L4"><div><p><img src="https://financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_premium.svg" alt=""></p><p data-offer-type="premium"><h3>Standard Digital</h3></p></div><div><p>Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.</p></div></div><div data-o-grid-colspan="12 L4"><div><p><img src="https://financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_print.svg" alt=""></p><p data-offer-type="print"><h3>Standard Digital</h3></p></div><div><p><span>CHF1,349</span><span> CHF345 </span><span>for your first year</span></p></div><div><p>Insight and expertise in your hands with the iconic FT print edition, delivered Monday to Saturday.</p></div></div></div></div>
<div data-component="subscriptionOptionsV2" data-component-unique-name="CHE-Print"><h2>Explore our full range of subscriptions.</h2></div>
<div data-component="whyFT" data-component-unique-name="default"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft">Find out why</a></p></div>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Large (545 pts)]]></title>
            <link>https://mistral.ai/news/mistral-large/</link>
            <guid>39511477</guid>
            <pubDate>Mon, 26 Feb 2024 14:20:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/mistral-large/">https://mistral.ai/news/mistral-large/</a>, See on <a href="https://news.ycombinator.com/item?id=39511477">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>We are releasing <em>Mistral Large</em>, our latest and most advanced language
model. <em>Mistral Large</em> is available through la Plateforme. We are also making
it available through Azure, our first distribution partner.</p><h4 id="mistral-large-our-new-flagship-model">Mistral Large, our new flagship model</h4><p>Mistral Large is our new cutting-edge text generation model. It reaches
top-tier reasoning capabilities. It can be used for complex multilingual
reasoning tasks, including text understanding, transformation, and code
generation.</p><p>Mistral Large achieves strong results on commonly used benchmarks,
making it the world's second-ranked model generally available through
an API (next to GPT-4) [see
below for details on bencharks].</p><p><img src="https://mistral.ai/images/news/mistral-large/mistral-large-bar-plot.png" alt="Detailed benchmarks" width="100%"></p><p>Figure 1: Comparison of GPT-4, Mistral Large (pre-trained), Claude 2, Gemini Pro 1.0, GPT 3.5 and LLaMA 2 70B on MMLU (Measuring massive multitask language
understanding).</p><p>Mistral Large comes with new capabilities and strengths:</p><ul><li><p>It is <strong>natively fluent in English, French, Spanish, German, and
Italian,</strong> with a nuanced understanding of grammar and cultural
context.</p></li><li><p>Its <strong>32K tokens context window</strong> allows precise information recall
from large documents.</p></li><li><p>Its <strong>precise instruction-following</strong> enables developers to design
their <strong>moderation policies</strong> – we used it to set up the system-level moderation of le Chat.</p></li><li><p><strong>It is natively capable of function calling.</strong> This, along with
constrained output mode, implemented on la Plateforme, enables
application development and tech stack modernisation at scale.</p></li></ul><h4 id="partnering-with-microsoft-to-provide-our-models-on-azure"><strong>Partnering with Microsoft to provide our models on Azure</strong></h4><p>At Mistral, our mission is to make frontier AI ubiquitous. This is why
we’re announcing today that we’re bringing our open and commercial
models to Azure. Microsoft’s trust in our model is a step forward in
our journey! Our models are now available through:</p><ol><li><p><strong>La Plateforme</strong>: safely hosted on Mistral’s infrastructure in
Europe, this access point enables developers to create applications
and services across our comprehensive range of models.</p></li><li><p><strong>Azure:</strong> Mistral Large is available through <a href="https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/mistal-large-mistral-ai-s-flagship-llm-debuts-on-azure-ai-models/ba-p/4066996">Azure AI Studio and Azure
Machine Learning</a>, with as seamless a user experience as our APIs. Beta customers
have used it with <a href="https://mistral.ai/business/">significant success</a>.</p></li><li><p><strong>Self-deployment</strong>: our models can be deployed on your environment
for the most sensitive use cases with access to our model weights;
Read success stories on this kind of deployment, and <a href="https://mistral.ai/contact/">contact our team</a> for further details.</p></li></ol><h4 id="mistral-large-capacities"><strong>Mistral Large capacities</strong></h4><p>We compare Mistral Large's performance to the top-leading LLM models on
commonly used benchmarks.</p><p><strong>Reasoning and knowledge</strong></p><p>Mistral Large shows powerful reasoning capabilities. In the following figure, we report the performance of the pretrained models on standard benchmarks.</p><p><img src="https://mistral.ai/images/news/mistral-large/large-bench-reasoning-table.svg" alt="Detailed benchmarks" width="100%"></p><p>Figure 2: Performance on widespread common sense, reasoning and
knowledge benchmarks of the top-leading LLM models on the market: MMLU
(Measuring massive multitask language in understanding), HellaSwag
(10-shot), Wino Grande (5-shot), Arc Challenge (5-shot), Arc Challenge
(25-shot), TriviaQA (5-shot) and TruthfulQA.</p><p><strong>Multi-lingual capacities</strong></p><p>Mistral Large has native multi-lingual capacities. It strongly
outperforms LLaMA 2 70B on HellaSwag, Arc Challenge and MMLU benchmarks
in French, German, Spanish and Italian.</p><p><img src="https://mistral.ai/images/news/mistral-large/large-bench-multilingual-table.svg" alt="Detailed benchmarks" width="100%"></p><p>Figure 3: Comparison of Mistral Large, Mixtral 8x7B and LLaMA 2 70B on
HellaSwag, Arc Challenge and MMLU in French, German, Spanish and
Italian.</p><p><strong>Maths &amp; Coding</strong></p><p>Mistral Large shows top performance in coding and math tasks.
In the table below, we report the performance across a suite of
popular benchmarks to evaluate the coding and math performance for some
of the top-leading LLM models.</p><p><img src="https://mistral.ai/images/news/mistral-large/large-bench-coding-maths-table.svg" alt="Detailed benchmarks" width="100%"></p><p>Figure 4: Performance on popular coding and math benchmarks of the leading LLM models on the market: HumanEval pass@1, MBPP pass@1, Math maj@4, GSM8K maj@8
(8-shot) and GSM8K maj@1 (5 shot).</p><h3 id="a-new-mistral-small-optimised-for-low-latency-workloads"><strong>A new Mistral Small, optimised for low latency workloads</strong></h3><p>Alongside Mistral Large, we’re releasing a new optimised model, Mistral
Small, optimised for latency and cost. Mistral Small outperforms Mixtral 8x7B and has
lower latency, which makes it a refined intermediary
solution between our open-weight offering and our flagship model.</p><p>Mistral Small benefits from the same innovation as Mistral Large
regarding RAG-enablement and function calling.</p><p>We’re simplifying our endpoint offering to provide the following:</p><ul><li><p>Open-weight endpoints with competitive pricing. This comprises
<code>open-mistral-7B</code> and <code>open-mixtral-8x7b</code>.</p></li><li><p>New optimised model endpoints, <code>mistral-small-2402</code> and <code>mistral-large-2402</code>.
We’re maintaining <code>mistral-medium</code>, which we are not updating today.</p></li></ul><p>Our <a href="https://docs.mistral.ai/platform/endpoints/">benchmarks</a> give a comprehensive view of performance/cost tradeoffs.</p><p>Beyond the new model offering, we’re allowing organisation management
multi-currency pricing and have updated service tiers on la Plateforme.
We have also made a lot of progress in
reducing the latency of all our endpoints.</p><h3 id="json-format-and-function-calling"><strong>JSON format and function calling</strong></h3><p>JSON format mode forces the language model output to be valid JSON. This
functionality enables developers to interact with our models more
naturally to extract information in a structured format that can be
easily used in the remainder of their pipelines.</p><p>Function calling lets developers interface Mistral endpoints with a
set of their own tools, enabling more complex interactions with internal
code, APIs or databases. You will learn more in our <a href="https://docs.mistral.ai/guides/function-calling">function
calling guide</a>.</p><p>Function calling and JSON format are only available on mistral-small and
mistral-large. We will be adding formatting to all endpoints shortly, as
well as enabling more fine-grained format definitions.</p><p><strong>Try Mistral Large and Mistral Small today</strong></p><p>Mistral Large is available on La Plateforme and Azure as of today.
Mistral Large is also exposed on our beta assistant demonstrator, <a href="https://chat.mistral.ai/">le
Chat</a>. As always, we’re eager to have your feedback!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: R2R – Open-source framework for production-grade RAG (126 pts)]]></title>
            <link>https://github.com/SciPhi-AI/R2R</link>
            <guid>39510874</guid>
            <pubDate>Mon, 26 Feb 2024 13:10:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/SciPhi-AI/R2R">https://github.com/SciPhi-AI/R2R</a>, See on <a href="https://news.ycombinator.com/item?id=39510874">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">R2R: Production-ready RAG systems.</h2><a id="user-content-r2r-production-ready-rag-systems" aria-label="Permalink: R2R: Production-ready RAG systems." href="#r2r-production-ready-rag-systems"></a></p>
<p dir="auto">
  <a href="https://docs.sciphi.ai/" rel="nofollow"><img src="https://camo.githubusercontent.com/b9e2f36d308851fc635e9cde021acf0c5925878023d3d16dd06f7f1e760002be/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732e7363697068692e61692d334631364534" alt="Docs" data-canonical-src="https://img.shields.io/badge/docs.sciphi.ai-3F16E4"></a>
  <a href="https://discord.gg/p6KqD2kjtB" rel="nofollow"><img src="https://camo.githubusercontent.com/23ca552d7f7578a2737cf7ceea7272ec063869993600b813a23a38978172ee4f/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313132303737343635323931353130353933343f7374796c653d736f6369616c266c6f676f3d646973636f7264" alt="Discord" data-canonical-src="https://img.shields.io/discord/1120774652915105934?style=social&amp;logo=discord"></a>
  <a href="https://github.com/SciPhi-AI"><img src="https://camo.githubusercontent.com/19e5d78787c6aeb45ecff6aa1732b8213eb9cb6b1a5437b2439579f8c4fd355a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5363695068692d41492f523252" alt="Github Stars" data-canonical-src="https://img.shields.io/github/stars/SciPhi-AI/R2R"></a>
  <a href="https://github.com/SciPhi-AI/R2R/pulse"><img src="https://camo.githubusercontent.com/bca0798c88e9c66c4bad61180d9592a1deaa0d2c7823dbda03d0ffc09a4dc79b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f772f5363695068692d41492f523252" alt="Commits-per-week" data-canonical-src="https://img.shields.io/github/commit-activity/w/SciPhi-AI/R2R"></a>
  <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/4d4571869159681d7f373eb6a4f9f340f407b409abd41c0f1ed74ea7c6b47a5b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d707572706c652e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-purple.svg"></a>
</p>
<p dir="auto">A semi-opinionanted RAG framework.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/SciPhi-AI/R2R/blob/main/docs/pages/r2r.png"><img src="https://github.com/SciPhi-AI/R2R/raw/main/docs/pages/r2r.png" alt="Sciphi Framework"></a></p>
R2R was conceived to bridge the gap between experimental RAG models and robust, production-ready systems. Our semi-opinionated framework cuts through the complexity, offering a straightforward path to deploy, adapt, and maintain RAG pipelines in production. We prioritize simplicity and practicality, aiming to set a new industry benchmark for ease of use and effectiveness.
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Install:</h3><a id="user-content-quick-install" aria-label="Permalink: Quick Install:" href="#quick-install"></a></p>
<p dir="auto"><strong>Install R2R directly using <code>pip</code>:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# use the `'r2r[all]'` to download all required deps
pip install 'r2r[parsing]'"><pre><span><span>#</span> use the `'r2r[all]'` to download all required deps</span>
pip install <span><span>'</span>r2r[parsing]<span>'</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Links</h2><a id="user-content-links" aria-label="Permalink: Links" href="#links"></a></p>
<p dir="auto"><a href="https://discord.gg/p6KqD2kjtB" rel="nofollow">Join the Discord server</a></p>
<p dir="auto"><a href="https://docs.sciphi.ai/" rel="nofollow">Read our Docs</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Basic Examples</h2><a id="user-content-basic-examples" aria-label="Permalink: Basic Examples" href="#basic-examples"></a></p>
<p dir="auto">The project includes several basic examples that demonstrate application deployment and interaction:</p>
<ol dir="auto">
<li>
<p dir="auto"><a href="https://github.com/SciPhi-AI/R2R/blob/main/examples/basic/app.py"><code>app.py</code></a>: This example runs the main application, which includes the ingestion, embedding, and RAG pipelines served via FastAPI.</p>
<div dir="auto" data-snippet-clipboard-copy-content="poetry run uvicorn examples.basic.app:app"><pre>poetry run uvicorn examples.basic.app:app</pre></div>
</li>
<li>
<p dir="auto"><a href="https://github.com/SciPhi-AI/R2R/blob/main/examples/basic/run_client.py"><code>run_client.py</code></a>: This example should be run after starting the main application. It demonstrates uploading text entries as well as a PDF to the local server with the python client. Further, it shows document and user-level vector management with built-in features.</p>
<div dir="auto" data-snippet-clipboard-copy-content="poetry run python -m examples.basic.run_client"><pre>poetry run python -m examples.basic.run_client</pre></div>
</li>
<li>
<p dir="auto"><a href="https://github.com/SciPhi-AI/R2R/blob/main/examples/pdf_chat/run_demo.py"><code>run_pdf_chat.py</code></a>: An example demonstrating upload and chat with a more realistic pdf.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Ingest pdf
poetry run python -m examples.pdf_chat.run_demo ingest

# Ask a question
poetry run python -m examples.pdf_chat.run_demo search &quot;What are the key themes of Meditations?&quot;"><pre><span><span>#</span> Ingest pdf</span>
poetry run python -m examples.pdf_chat.run_demo ingest

<span><span>#</span> Ask a question</span>
poetry run python -m examples.pdf_chat.run_demo search <span><span>"</span>What are the key themes of Meditations?<span>"</span></span></pre></div>
</li>
<li>
<p dir="auto"><a href="https://github.com/SciPhi-AI/R2R/blob/main/web/package.json"><code>web</code></a>: A web application which is meant to accompany the framework to provide visual intelligence.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd web &amp;&amp; pnpm install
# Serve the web app
pnpm dev"><pre><span>cd</span> web <span>&amp;&amp;</span> pnpm install
<span><span>#</span> Serve the web app</span>
pnpm dev</pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">60s demo of the examples</h2><a id="user-content-60s-demo-of-the-examples" aria-label="Permalink: 60s demo of the examples" href="#60s-demo-of-the-examples"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description slim_demo.mp4">slim_demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/68796651/307704544-01fee645-1beb-4096-9e7d-7d0fa01386ea.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDg5ODg3MjIsIm5iZiI6MTcwODk4ODQyMiwicGF0aCI6Ii82ODc5NjY1MS8zMDc3MDQ1NDQtMDFmZWU2NDUtMWJlYi00MDk2LTllN2QtN2QwZmEwMTM4NmVhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMjI2VDIzMDAyMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIwNjAzY2E5YWNjMmI0YTJlOWQ0ZTkzZmI3NWVkN2FjYTNhODlkNzQ2MWYxYTYzMzZkZWZjZDBjZTljMTkzOWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.p6-l1AfhuwOAi_xbxR_tTLG5_bAxdBwIR5K7uQsnNgc" data-canonical-src="https://private-user-images.githubusercontent.com/68796651/307704544-01fee645-1beb-4096-9e7d-7d0fa01386ea.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDg5ODg3MjIsIm5iZiI6MTcwODk4ODQyMiwicGF0aCI6Ii82ODc5NjY1MS8zMDc3MDQ1NDQtMDFmZWU2NDUtMWJlYi00MDk2LTllN2QtN2QwZmEwMTM4NmVhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMjI2VDIzMDAyMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIwNjAzY2E5YWNjMmI0YTJlOWQ0ZTkzZmI3NWVkN2FjYTNhODlkNzQ2MWYxYTYzMzZkZWZjZDBjZTljMTkzOWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.p6-l1AfhuwOAi_xbxR_tTLG5_bAxdBwIR5K7uQsnNgc" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h3 tabindex="-1" dir="auto">Full Install:</h3><a id="user-content-full-install" aria-label="Permalink: Full Install:" href="#full-install"></a></p>
<p dir="auto">Follow these steps to ensure a smooth setup:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Install Poetry:</strong></p>
<ul dir="auto">
<li>Before installing the project, make sure you have Poetry on your system. If not, visit the <a href="https://python-poetry.org/docs/#installation" rel="nofollow">official Poetry website</a> for installation instructions.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Clone and Install Dependencies:</strong></p>
<ul dir="auto">
<li>Clone the project repository and navigate to the project directory:
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:SciPhi-AI/r2r.git
cd r2r"><pre>git clone git@github.com:SciPhi-AI/r2r.git
<span>cd</span> r2r</pre></div>
</li>
<li>Install the project dependencies with Poetry:
<div dir="auto" data-snippet-clipboard-copy-content="# See pyproject.toml for available extras
# use &quot;all&quot; to include every optional dependency
poetry install --extras &quot;parsing&quot;"><pre><span><span>#</span> See pyproject.toml for available extras</span>
<span><span>#</span> use "all" to include every optional dependency</span>
poetry install --extras <span><span>"</span>parsing<span>"</span></span></pre></div>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Configure Environment Variables:</strong></p>
<ul dir="auto">
<li>You need to set up cloud provider secrets in your <code>.env</code>. At a minimum, you will need an OpenAI key.</li>
<li>The framework currently supports PostgreSQL (locally), pgvector and Qdrant with plans to extend coverage.</li>
<li>If starting from the example, copy <code>.env.example</code> to <code>.env</code> to apply your configurations:

</li>
</ul>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Features</h2><a id="user-content-key-features" aria-label="Permalink: Key Features" href="#key-features"></a></p>
<ul dir="auto">
<li><strong>🚀 Rapid Deployment</strong>: Facilitates a smooth setup and development of production-ready RAG systems.</li>
<li><strong>⚖️ Flexible Standarization</strong>: <code>Ingestion</code>, <code>Embedding</code>, and <code>RAG</code> with proper <code>Observability</code>.</li>
<li><strong>🧩 Easy to modify</strong>: Provides a structure that can be extended to deploy your own custom pipelines.</li>
<li><strong>📦 Versioning</strong>: Ensures your work remains reproducible and traceable through version control.</li>
<li><strong>🔌 Extensibility</strong>: Enables a quick and robust integration with various VectorDBs, LLMs and Embeddings Models.</li>
<li><strong>🤖 OSS Driven</strong>: Built for and by the OSS community, to help startups and enterprises to quickly build with RAG.</li>
<li><strong>📝 Deployment Support</strong>: Available to help you build and deploy your RAG systems end-to-end.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Core Abstractions</h2><a id="user-content-core-abstractions" aria-label="Permalink: Core Abstractions" href="#core-abstractions"></a></p>
<p dir="auto">The framework primarily revolves around three core abstractions:</p>
<ul dir="auto">
<li>
<p dir="auto">The <strong>Ingestion Pipeline</strong>: Facilitates the preparation of embeddable 'Documents' from various data formats (json, txt, pdf, html, etc.). The abstraction can be found in <a href="https://github.com/SciPhi-AI/R2R/blob/main/r2r/core/pipelines/ingestion.py"><code>ingestion.py</code></a>.</p>
</li>
<li>
<p dir="auto">The <strong>Embedding Pipeline</strong>: Manages the transformation of text into stored vector embeddings, interacting with embedding and vector database providers through a series of steps (e.g., extract_text, transform_text, chunk_text, embed_chunks, etc.). The abstraction can be found in <a href="https://github.com/SciPhi-AI/R2R/blob/main/r2r/core/pipelines/embedding.py"><code>embedding.py</code></a>.</p>
</li>
<li>
<p dir="auto">The <strong>RAG Pipeline</strong>: Works similarly to the embedding pipeline but incorporates an LLM provider to produce text completions. The abstraction can be found in <a href="https://github.com/SciPhi-AI/R2R/blob/main/r2r/core/pipelines/rag.py"><code>rag.py</code></a>.</p>
</li>
</ul>
<p dir="auto">Each pipeline incorporates a logging database for operation tracking and observability.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Disappointing Tea.xyz (129 pts)]]></title>
            <link>https://connortumbleson.com/2024/02/26/the-disappointing-tea-xyz/</link>
            <guid>39510756</guid>
            <pubDate>Mon, 26 Feb 2024 12:56:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://connortumbleson.com/2024/02/26/the-disappointing-tea-xyz/">https://connortumbleson.com/2024/02/26/the-disappointing-tea-xyz/</a>, See on <a href="https://news.ycombinator.com/item?id=39510756">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<figure><img src="https://connortumbleson.com/content/images/2024/02/tea-xyz-1.png" alt="" loading="lazy" width="2000" height="824" srcset="https://connortumbleson.com/content/images/size/w600/2024/02/tea-xyz-1.png 600w, https://connortumbleson.com/content/images/size/w1000/2024/02/tea-xyz-1.png 1000w, https://connortumbleson.com/content/images/size/w1600/2024/02/tea-xyz-1.png 1600w, https://connortumbleson.com/content/images/size/w2400/2024/02/tea-xyz-1.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>A few days ago I got a pull request to a project (since been deleted) and a user was attempting to add a file like this to the root of the project.</p><pre><code># https://tea.xyz/what-is-this-file
---
version: 1.0.0
codeOwners:
  - "0xREDACTED"
quorum: 1</code></pre><p>Of course my first reaction was to close the pull request immediately. It was some unknown file with some hexadecimal code owner that I couldn't verify.</p><p>Sadly the link for "what-is-this-file" actually told me nothing about the file. Not what the parameters meant or anything in that regard - so I was immediately mad.</p><p>I searched the user and this individual had 10+ other pull requests verbatim in other repositories. It was a brand new GitHub account and this just felt like a scam to me.</p><p>Sure enough between spotting it, reporting it and writing this blog post that user does not exist nor do any of the pull requests.</p><p>If I dig into Tea and their OSS aspect - they make the following call out:</p><blockquote>Registering your OSS project on the <strong><em>tea</em></strong> protocol offers several benefits. It allows you to gain visibility within the <strong><em>tea</em></strong> community, attract potential contributors and users, and track the utilization and impact of your project. This data can be used to earn rewards based on the value of your project. Additionally, registering your project on the <strong><em>tea</em></strong> protocol incentivizes security researchers to disclose vulnerabilities ethically and responsibly.</blockquote><p>I read what I needed to. This is another token based crypto thing which in my experience is frequently abused for the monetary gain instead of whatever the service is offering. I continued reading to figure out why a pull request was given.</p><blockquote>Next, you will need to merge a pull request (PR) to <strong>demonstrate ownership of this project</strong>. You will need to download your project's constitution file, and manually commit and merge the file to your project's repository.</blockquote><p>I kept re-reading this line and it made no sense. All I need to do to claim ownership of a project is merge a pull-request? Do I own Laravel because I've gotten a pull request merged?</p><p>So I was wondering how Tea handled verifying projects that a user did not own. They had a FAQ especially for this question.</p><blockquote><strong>Can I register projects that I do not own?</strong><br>Yes, you can. You can register projects that you do not own as long as you're at least a contributor to the project and have consent from the project maintainer(s). Registration will only be complete once the PR is merged with the project's repository.</blockquote><p>I started piecing together what was happening here. If you get a pull request merged it seems that is proof enough for Tea to consider trusting the tea constitution file. So sure enough this user was attempting to get a pull request merged to many repositories with themselves listed as the sole owner.</p><p>That would put them in a position to control these tokens (money?) tied to the open source project.</p><p>So I wondered how far this had spread and did a GitHub search.</p><figure><img src="https://connortumbleson.com/content/images/2024/02/tea-abuse.png" alt="" loading="lazy" width="1586" height="976" srcset="https://connortumbleson.com/content/images/size/w600/2024/02/tea-abuse.png 600w, https://connortumbleson.com/content/images/size/w1000/2024/02/tea-abuse.png 1000w, https://connortumbleson.com/content/images/2024/02/tea-abuse.png 1586w" sizes="(min-width: 720px) 720px"><figcaption><span>GitHub search for "tea.xyz"</span></figcaption></figure><p>It seemed this was a fresh type of abuse - we had 100+ pull requests in two days all with users trying to add tea configuration files to repositories. Thankfully it seems most of these users are just trying to scam funds and have no actual idea how GitHub works.</p><p>What I mean is these users are just forking projects, merging a fix into their own branch and hoping they've validated that project. Sure enough they've validated their own personal fork - and that is it. It appears only a handful know enough about the process to open the pull request upstream to the main repository.</p><p>So I stumbled upon <a href="https://github.com/TryGhost/Ghost?ref=connortumbleson.com">TryGhost/Ghost</a> - which is the repository that houses the Ghost software, which is what I use for this blog. I saw many pull requests for this same type of tea file addition:</p><figure><img src="https://connortumbleson.com/content/images/2024/02/tea-merge.png" alt="" loading="lazy" width="1486" height="884" srcset="https://connortumbleson.com/content/images/size/w600/2024/02/tea-merge.png 600w, https://connortumbleson.com/content/images/size/w1000/2024/02/tea-merge.png 1000w, https://connortumbleson.com/content/images/2024/02/tea-merge.png 1486w" sizes="(min-width: 720px) 720px"><figcaption><span>sample of one of the pull requests</span></figcaption></figure><pre><code>https://github.com/TryGhost/Ghost/pull/19743
https://github.com/TryGhost/Ghost/pull/19746
https://github.com/TryGhost/Ghost/pull/19738
https://github.com/TryGhost/Ghost/pull/19749
https://github.com/TryGhost/Ghost/pull/19748
https://github.com/TryGhost/Ghost/pull/19742
and more and more....</code></pre><p>It seemed Ghost blog was used (probably without permission) as the demo project in an explanation <a href="https://www.youtube.com/watch?v=no9t3rNGI_w&amp;ref=connortumbleson.com">video</a>, which may explain why all these users are attempting to claim ownership of them.</p><figure><img src="https://connortumbleson.com/content/images/2024/02/tea-video.png" alt="" loading="lazy" width="1764" height="1174" srcset="https://connortumbleson.com/content/images/size/w600/2024/02/tea-video.png 600w, https://connortumbleson.com/content/images/size/w1000/2024/02/tea-video.png 1000w, https://connortumbleson.com/content/images/size/w1600/2024/02/tea-video.png 1600w, https://connortumbleson.com/content/images/2024/02/tea-video.png 1764w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://www.youtube.com/watch?v=no9t3rNGI_w&amp;ref=connortumbleson.com" rel="noreferrer"><span>How to register a project with tea (YouTube)</span></a></figcaption></figure><p>So in short - this is why I often hate crypto. This idea and execution has done nothing but steal time from open source contributors and clog up review time and research for a bunch of garbage pull requests.</p><p>So much like Keybase got spammed with an influx of garbage users when they announced their Stellar token - GitHub is getting an influx of garbage users taking time and energy from me and others for this tea stuff.</p><figure><img src="https://connortumbleson.com/content/images/2024/02/tea-garbage-user.png" alt="" loading="lazy" width="2000" height="1046" srcset="https://connortumbleson.com/content/images/size/w600/2024/02/tea-garbage-user.png 600w, https://connortumbleson.com/content/images/size/w1000/2024/02/tea-garbage-user.png 1000w, https://connortumbleson.com/content/images/size/w1600/2024/02/tea-garbage-user.png 1600w, https://connortumbleson.com/content/images/size/w2400/2024/02/tea-garbage-user.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Take this brand new user who is frantically attempting to add tea configuration files to every repository they encounter. I just wish these people that spent as much time applying themselves to abuse a system instead used it for something different.</p><p>Much like the Keybase &amp; Stellar wallet incident some ideas just suck. I don't even want to spend a single minute entertaining any positive aspect of "<em>tea.xyz</em>" - this product and execution has done nothing but upset me. My first impression was negative and its tough to break that.</p><p>However saying all of that - every idea has good intention, but you can't often predict the abuse your users will bring. I saw a comment from Max Howell (contributor of tea) when he was pinged on one of these annoying pull requests.</p><p>He <a href="https://github.com/TryGhost/Ghost/pull/19743?ref=connortumbleson.com#issuecomment-1962428753">said</a>:</p><blockquote>Hey everyone. We’re really sorry about this.<p>Firstly, we are taking down the videos. Using real projects in the example videos was a huge mistake and we own it.</p><p>Secondly we are going to add verification steps to ensure we do not generate YAML for projects without proof that the user is a legitimate contributor.</p><p>For now we will remove ghost from the listing.</p><p>Our project genuinely wants to help open source and not hinder it and our efforts are entirely focused on that which ofc includes ensuring that this kind of malicious and despicable behavior ends as soon as possible.</p><p>Thank you for your understanding and sincerely: I'm very angry about this, if it was happening to my projects I would also be incredibly unhappy about it.</p></blockquote><p>Ultimately you can make your own judgement about this. I'm not so sure I'm ready for a 2nd chance, but I appreciate the honesty above.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The 14 pains of building your own billing system (310 pts)]]></title>
            <link>https://arnon.dk/the-14-pains-of-billing/</link>
            <guid>39510147</guid>
            <pubDate>Mon, 26 Feb 2024 11:41:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arnon.dk/the-14-pains-of-billing/">https://arnon.dk/the-14-pains-of-billing/</a>, See on <a href="https://news.ycombinator.com/item?id=39510147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Billing and revenue systems are a necessity if you ever plan to monetize your business.</p>



<p>If you’ve spent any time with them, you know that billing systems are <em><strong>complicated</strong></em>, and no one wants to think about them. When they work, it’s great and everyone is happy.</p>



<p>I’ve seen them likened to an octopus, and I fully agree. They touch finance, product, experience, customer support, customers, legal, compliance, sales, and sometimes more.</p>



<figure><a href="https://arnon.dk/wp-content/uploads/2024/02/image.png"><img fetchpriority="high" decoding="async" width="1019" height="1024" data-attachment-id="1332" data-permalink="https://arnon.dk/the-14-pains-of-billing/image-17/" data-orig-file="https://arnon.dk/wp-content/uploads/2024/02/image.png" data-orig-size="1125,1131" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://arnon.dk/wp-content/uploads/2024/02/image-298x300.png" data-large-file="https://arnon.dk/wp-content/uploads/2024/02/image-1019x1024.png" src="https://arnon.dk/wp-content/uploads/2024/02/image-1019x1024.png" alt="" srcset="https://arnon.dk/wp-content/uploads/2024/02/image-1019x1024.png 1019w, https://arnon.dk/wp-content/uploads/2024/02/image-298x300.png 298w, https://arnon.dk/wp-content/uploads/2024/02/image-150x150.png 150w, https://arnon.dk/wp-content/uploads/2024/02/image-768x772.png 768w, https://arnon.dk/wp-content/uploads/2024/02/image.png 1125w" sizes="(max-width: 1019px) 100vw, 1019px"></a><figcaption>Billing is hard, intertwined, and complicated.</figcaption></figure>



<p>Because it’s so inter-connected, it can go belly-up rather quickly if something breaks. And stuff does break. <em>Frequently</em>. (If you have a team that’s maintaining this system and it’s not you, ask them!).</p>



<p>Also, if your system isn’t breaking just yet. Give it some time.</p>



<p>Look, I know you are busy running the business and you already have lots on your plate. You just want to collect some money and move on to more pressing features that actually matter to customers.</p>



<p>But remember:</p>



<p>👉 if you can’t collect revenue legally and correctly, it’ll become your headache and you’ll have more on your plate than you could possibly ever chew 👈</p>



<h3>The three patterns</h3>



<p>This is not unique to billing systems. It is very common to see one of three patterns:</p>



<ol>
<li>Home-grown</li>



<li>Full 3rd party system</li>



<li>or a hybrid of a home-grown and 3rd party system</li>
</ol>



<p>These all have their own benefits and drawbacks, naturally.</p>



<figure><table><thead><tr><th><strong>Build your own / Home-grown</strong></th><th><strong>Hybrid</strong></th><th><strong>3rd party</strong></th></tr></thead><tbody><tr><td>An entirely home-grown solution.<p>The control is hard to beat. Full control, fully customizable, and you aren’t paying anyone from the outside for fees.</p><p>Many think (and this happens to many companies) that building and maintaining your own billing system is the best option for your business.</p></td><td data-align="left">A mix of home-grown solutions and 3rd parties.<p>For example, your billing engine is internal, payments are handled by a PSP, and tax compliance by a tax SaaS.</p><p>Here, you can control the business logic (e.g., when do you update quantities), but the logic is handled by the 3rd parties.</p></td><td>A turnkey solution that handles everything for you.<p>All the business logic, payment processing, invoicing, tax compliance, usage, metering – all done by one full-service solution.</p><p>This is convenient for companies, but you lose a lot of control and may have to shell out a lot of money to get to this place</p></td></tr></tbody></table></figure>



<p>It’s very natural, when you just start your company (or you’re taking your first steps with a new product) to build everything by yourself.</p>



<p>You have engineers. You want to keep it very simple.</p>



<p>Or so you think.</p>



<p>Why not? <strong>Because you’re often still thinking like an engineer.</strong></p>



<p>You’re thinking of billing as an engineering issue. You’ll say to yourself <em>“why can’t we just dump a file of what we need to bill on S3, and have a CRON job pick it up and collect payment?”</em>.</p>



<p>But you’re wrong. It’s a very big and difficult problem, you just aren’t seeing it yet.</p>



<p>Here is my very high-level description of the things a typical billing team needs to worry about (YMMV):</p>



<figure><a href="https://arnon.dk/wp-content/uploads/2024/02/monetization-responsibilities@2x.png"><img decoding="async" width="1024" height="480" data-attachment-id="1334" data-permalink="https://arnon.dk/the-14-pains-of-billing/monetization-responsibilities2x/" data-orig-file="https://arnon.dk/wp-content/uploads/2024/02/monetization-responsibilities@2x.png" data-orig-size="7544,3538" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="monetization-responsibilities@2x" data-image-description="" data-image-caption="" data-medium-file="https://arnon.dk/wp-content/uploads/2024/02/monetization-responsibilities@2x-300x141.png" data-large-file="https://arnon.dk/wp-content/uploads/2024/02/monetization-responsibilities@2x-1024x480.png" src="https://arnon.dk/wp-content/uploads/2024/02/monetization-responsibilities@2x-1024x480.png" alt="" srcset="https://arnon.dk/wp-content/uploads/2024/02/monetization-responsibilities@2x-1024x480.png 1024w, https://arnon.dk/wp-content/uploads/2024/02/monetization-responsibilities@2x-300x141.png 300w, https://arnon.dk/wp-content/uploads/2024/02/monetization-responsibilities@2x-768x360.png 768w, https://arnon.dk/wp-content/uploads/2024/02/monetization-responsibilities@2x-1536x720.png 1536w, https://arnon.dk/wp-content/uploads/2024/02/monetization-responsibilities@2x-2048x960.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>A typical billing or monetization team has so many responsibilities that it’s hard to grasp even for seasoned professionals.</figcaption></figure>



<p>Everyone knows you don’t do your own security (or date handling). You also shouldn’t do your own billing from scratch. </p>



<figure><img decoding="async" width="1024" height="1004" data-attachment-id="1337" data-permalink="https://arnon.dk/the-14-pains-of-billing/image-3-7/" data-orig-file="https://arnon.dk/wp-content/uploads/2024/02/image-3.png" data-orig-size="1200,1177" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-3" data-image-description="" data-image-caption="" data-medium-file="https://arnon.dk/wp-content/uploads/2024/02/image-3-300x294.png" data-large-file="https://arnon.dk/wp-content/uploads/2024/02/image-3-1024x1004.png" src="https://arnon.dk/wp-content/uploads/2024/02/image-3-1024x1004.png" alt="" srcset="https://arnon.dk/wp-content/uploads/2024/02/image-3-1024x1004.png 1024w, https://arnon.dk/wp-content/uploads/2024/02/image-3-300x294.png 300w, https://arnon.dk/wp-content/uploads/2024/02/image-3-768x753.png 768w, https://arnon.dk/wp-content/uploads/2024/02/image-3.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>via <a href="https://permit.substack.com/p/i-coded-the-security-system-myself">Permit.io</a></figcaption></figure>



<h3>My 14 pains of billing and monetization</h3>



<p>Allow me to list <em>some</em> of the problems I’ve had with home-grown billing systems, from least complex to most complex (partial list!):</p>



<p>1. <strong>Idempotency</strong>. All requests to do billing, collecting payments need to be unique and idempotent. This will become apparent when you hit API limits and need to retry, or need to spin up more instances of your billing system. Then, you risk double-charging. Luckily crediting/refunding is not a big problem, but this is a problem nonetheless when you scale your infrastructure.</p>



<p>2. <strong>Date handling.</strong> When do you bill? Every 30 days (calendar), or every month (anniversary)? What about leap days, time-zones, etc? </p>



<figure><img loading="lazy" decoding="async" width="679" height="478" data-attachment-id="1336" data-permalink="https://arnon.dk/the-14-pains-of-billing/image-2-7/" data-orig-file="https://arnon.dk/wp-content/uploads/2024/02/image-2.png" data-orig-size="679,478" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-2" data-image-description="" data-image-caption="" data-medium-file="https://arnon.dk/wp-content/uploads/2024/02/image-2-300x211.png" data-large-file="https://arnon.dk/wp-content/uploads/2024/02/image-2.png" src="https://arnon.dk/wp-content/uploads/2024/02/image-2.png" alt="" srcset="https://arnon.dk/wp-content/uploads/2024/02/image-2.png 679w, https://arnon.dk/wp-content/uploads/2024/02/image-2-300x211.png 300w" sizes="(max-width: 679px) 100vw, 679px"><figcaption><a href="https://xkcd.com/2867/">via XKCD: 2687</a></figcaption></figure>



<p>3. <strong>Proration and “leftovers”</strong>. Do you prorate on upgrade or only downgrade? Refund? Credits? Ignore it? Block it from happening (don’t allow upgrades/downgrades)?</p>



<p>4. <strong>Usage metering.</strong> There are dozens of way to decide how to calculate usage, <em>and</em> they can be changed frequently or by customer type</p>



<p>5. <strong>Invoice formatting</strong>. Sounds easy if you’re operating in one country. But when you expand you suddenly realise you have to collect not just sales tax, but also sometimes VAT and sometimes GST and sometimes additional levies (country-dependant) and you now need individual templates for each market.</p>



<p>6. <strong>Complex customer hierarchy</strong>. Customers (especially in B2B) could have subsidiaries and partners that they want to manage their billing relationships. How do you roll-up usage up to the paying entity?<br>This is often something that you don’t think about at first, but changes when you grow and expand.</p>



<ul>
<li>Making that even more complex: They could be in different locations, and taxed differently based on their location or where the services are delivered. Then, you are legally required to split the bills/invoices.<br>These rules can also change every few months</li>
</ul>



<p>7. <strong>Payment collection and churn prevention</strong>. When do you give up retrying? How do you handle chargebacks (terminate the account, suspend, refund)?</p>



<p>8. <strong>Pausing/resuming</strong>. What level of access do you let customers have when they pause their subscription?</p>



<p>9. <strong>Crediting / refunding</strong>. If you always refund the entire amount it may not be hard, but what about partial mistakes? Would you maybe want to give a “store credit” instead of a refund? Should that credit ever expire?</p>



<p>10. <strong>Tax handling</strong>. You may think different items having different tax rates is complex enough, but these also change frequently if you’re on the global level.</p>



<p>11. <strong>Custom deals</strong>. If you’re PLG-only this isn’t an issue but if you sign contracts, you will very quickly end up with edge cases and special deals that can’t be easily configured with the assumptions you made.</p>



<p>12. <strong>Human error.</strong> Customers are often comprised of humans who have made mistakes, and corrections need to be made. Businesses too are comprised of humans, who can misconfigure their customers and then need to make corrections. Crediting and reissuing invoices is a very time-consuming task.</p>



<ul>
<li>This is also true when customer’s legal details change (address, VAT ID, etc.)</li>
</ul>



<p>13. <strong>Selective pricing changes</strong>. Pricing changes often don’t affect all customers. When only new customers are affected, you must keep distinct versions of your pricing points to ensure customers’ agreements are kept.</p>



<figure></figure>







<p>14. <strong>Revenue recognition and accrued revenue.</strong> I can’t even begin to explain this – but <a href="https://www.ifrs.org/content/dam/ifrs/publications/pdf-standards/english/2021/issued/part-a/ifrs-15-revenue-from-contracts-with-customers.pdf">here’s a 64 page PDF specification of revenue recognition rules according to IFRS-15</a>. If you understand this – you’re special and please e-mail me, I want to know why.<br>Bonus points if you also did a custom <strong>ERP integration</strong> 🥲.</p>



<ol>
<li></li>
</ol>



<h3>Why are these hard?</h3>



<p>Some things change very regularly, more than you’d expect. Some things you only have to do once and never touch again.</p>



<p>Idempotency is a great example of an engineering issue that, once solved, rarely has to be touched.</p>



<p>However, tax rules change regularly across the world. The more countries you’re in, the more tax laws you have to keep track of.</p>



<p>Customer problems around mistakes is a relatively constant issue, but it increases in size as you grow, requiring more customer support and more manual corrections.</p>



<p>Let me try and put it in a table. It’s entirely subjective, but it should help explain the relationship between how frequently things change (== break), and how impacted they are by scale.</p>



<figure><table><tbody><tr><td><strong>Changes frequently</strong> <strong>/ Impacted by scale</strong></td><td><strong>🤏</strong> <strong>Somewhat impacted by your scale</strong></td><td><strong> 📈</strong> <strong>Highly impacted by your scale</strong></td></tr><tr><td><strong>🪨 Doesn’t change </strong></td><td>* Idempotency<br>* Dates<br>* Pause and resume rules</td><td>* Crediting / refunding<br>* Human errors</td></tr><tr><td><strong>🦥</strong> <strong>Changes sometimes</strong></td><td>* Proration rules<br>* Customer hierarchies</td><td>* Usage calculations<br>* Payment collection<br>* Pricing changes</td></tr><tr><td><strong>🐇</strong> <strong>Changes frequently</strong></td><td>* Custom deals<br>* Revenue Recognition rules</td><td>* Tax handling<br>* Invoice formatting</td></tr></tbody></table><figcaption>My subjective assessment of why some topics are more difficult than others</figcaption></figure>



<blockquote>
<p>tl;dr: <strong>Billing is <em>initially </em>an engineering problem rooted in a very very complex problem space which is really hard to understand even for industry veterans.</strong></p>
</blockquote>



<h3>What should you do about it?</h3>



<p>Offload as many problems as you can to someone else. Buy something off-the-shelf. Buy as much as you can.</p>



<p>I can’t stress this enough.</p>



<p>Let <a href="https://www.chargebee.com/">Chargebee</a>, <a href="https://www.solvimon.com/">Solvimon</a>, <a href="https://stripe.com/en-dk/billing">Stripe</a>, <a href="https://recurly.com/">Recurly</a>, <a href="https://www.withorb.com/">Orb</a>, <a href="https://metronome.com/">Metronome</a>, <a href="https://www.getlago.com/">Lago</a>, <a href="https://www.togai.com/">Togai</a> or anyone else manage your billing when you start. 90% of the “Subscription and billing” section in the diagram above can be handled by any of these.</p>



<p>Let <a href="https://www.stigg.io/">Stigg</a> handle your pricing pages, experiments, and <a href="https://arnon.dk/why-you-should-separate-your-billing-from-entitlement/">entitlements</a>.</p>



<p>Let your ERP handle your RevRec/Accounting (or use what’s built in with something else). </p>



<p><strong>You should only focus on updating the usage / basic customer lifecycle events, the things that are unique to your product.</strong></p>



<hr>



<p>More in my series on Billing:</p>



<p><a href="https://arnon.dk/design-your-pricing-and-tools-so-you-can-adapt-it-later/">Design your pricing and tools so you can adapt them later</a></p>



<p><a href="https://arnon.dk/how-we-built-a-cashback-system-with-stripe/">How we built a Cashback system with Stripe</a></p>



<p><a href="https://arnon.dk/youre-pricing-your-saas-wrong-but-thats-probably-ok/">You’re pricing your SaaS wrong but that’s probably OK</a></p>



<p><a href="https://arnon.dk/why-you-should-separate-your-billing-from-entitlement/">You should separate your billing from entitlements</a></p>



<p><a href="https://arnon.dk/5-things-i-learned-developing-billing-system/">5 things I learned while developing a billing system</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let's compile like it's 1992 (2014) (266 pts)]]></title>
            <link>https://fabiensanglard.net/Compile_Like_Its_1992/index.php</link>
            <guid>39509983</guid>
            <pubDate>Mon, 26 Feb 2024 11:18:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fabiensanglard.net/Compile_Like_Its_1992/index.php">https://fabiensanglard.net/Compile_Like_Its_1992/index.php</a>, See on <a href="https://news.ycombinator.com/item?id=39509983">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content" role="main">



		


<p>
       August 10th, 2014</p>
   
   <p id="paperbox">
	   	
          <a href="https://fabiensanglard.net/Compile_Like_Its_1992/index.php">
          <img src="https://fabiensanglard.net/Compile_Like_Its_1992/cli1992-1.png">   
          </a> 
         
        <!-- Add fancyBox -->
  
  

    
 </p><p>
I have been tinkering with the vanilla source code of Wolfenstein 3D from 1992. Even though it
is more than 20 years old and has rotten for modern systems, you can still compile it if you recreate the environment.
All you need is :<br>
 </p><ul>
     <li>Wolfenstein 3D source code.</li>
     <li>DosBox.</li>
     <li>The Compiler Borland C++ 3.1.</li>
     <li>Wolfenstein 3D shareware (for the assets).</li>
 </ul>
 
  
 <br>

<br>
<h3>Setup filesystem</h3>
<p>
  Open a command line and create two folders, one for each DOS drive needed:
</p><pre>  
   cd ~
   mkdir system
   cd system
   mkdir c
   mkdir a
   cd ~


 </pre>

<br>
<h3>Download</h3>
 <ul>
  <li>Download Borland 3.1 to <code>system/a</code>.<br>
  </li><li>Download Wolfenstein 3D source code to <code>system/c</code><br>
  </li><li>Download VGA files to <code>system/c</code> (the purpose of those is explained at the bottom of this page.<br>
  </li></ul>
  <pre>

    cd system/a
    curl -O http://fabiensanglard.net/Compile_Like_Its_1992/tools/BCPP31.zip
    

    cd ../c
    curl -O http://fabiensanglard.net/Compile_Like_Its_1992/tools/wolfsrc.zip
    curl -O http://fabiensanglard.net/Compile_Like_Its_1992/tools/vgafiles.zip


  </pre>

<div>
<p>
  Now we have all the files in the filesytem. Just to check, type: </p></div><pre>
   cd ..
   find ~/system


  </pre>
  <br>
  <div><p>
    You should have the following :</p></div><pre>

    /Users/fabiensanglard/system
    /Users/fabiensanglard/system/a
    /Users/fabiensanglard/system/a/BCPP31.zip
    /Users/fabiensanglard/system/c
    /Users/fabiensanglard/system/c/vgafiles.zip
    /Users/fabiensanglard/system/c/wolfsrc.zip


  </pre>

<br>
<h3>Decompress everything</h3>
 <pre>
    cd ~/system/a
    unzip BCPP31.zip

    cd ~/system/c
    unzip vgafiles.zip
    unzip wolfsrc.zip


</pre>


<br>
<h3>DosBox</h3>
 <p>
  Download and start <a href="http://www.dosbox.com/">DosBox</a>:
  <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/dosbox_splash.png">
  <br>
</p>

<br>
<h3>Mount</h3>
  <p>
    Mount the filesystem, one folder for each drive :
  </p><pre>

   Z:/&gt; mount c ~/system/c 
   Z:/&gt; mount a ~/system/a

  </pre>



<br>
<h3>Install the compiler</h3>
  <p>
  Now is time to install Borland C++ 3.1 :
  </p><pre>
    Z:\&gt; a:
    A:\&gt; cd BCPP31
    A:\&gt; install

 </pre>
 <p>
 <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/borland_install.png">
 <br>
  Press enter when you select the source drive ( it should already be A drive )<br>
  <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/borland_choose_drive.png">
  </p><div><p>
  Leave all the default settings and select "Start Installation":<br>
 <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/borland_choose_dir.png">
  The warnings will tell you that Microsoft windows folder could not be found  but it is not needed anyway, 
  just press Enter.<br>
  <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/borland_warning_one.png"><br>
  <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/borland_inst_process.png"><br>
  <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/borland_warning_two.png"></p></div>

<br>
<h3>Install Wolfenstein 3D source code</h3>
  <div><p>
  We have a system running and a compiler on it: Time to decompress (again) the source code.</p></div><pre>
  A:\&gt; c:
  C:\&gt; cd\
  C:\&gt; install

</pre>
<div>
 
  
<p><img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/wolf_src_drive_selection.png"><br>
  Type 'C'<br>
  
<img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/wolf_src_directory_selection.png"><br>
 Keep the default path: <code>\WOLFSRC</code><br>

 <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/wolf_src_not_folder.png"><br>
 Y to create the directory.</p><p>

Installing !
 <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/wolf_src_install.png"></p></div><h3>Compiling</h3>
  <p>
  Start Borland C++ 3.1:
  </p><pre>
     
     C:\&gt; cd\
     C:\&gt; cd borlandc
     C:\&gt; cd bin
     C:\&gt; bc.exe

  </pre>
  <p>
    <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/compile_0.png"><br>
    After pressing OK, use the mouse or the shortcuts to Project -&gt; Open Project  <code>..\..\WOLFSRC\WOLF3D.PRJ</code>:
    <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/compile_1.png"><br>
</p><p>
    Select Options -&gt; Directories and change the value as follow :
    </p><pre>
    Include Directories: C:\BORLANDC\INCLUDE

    Library Directories: C:\BORLANDC\LIB

    Ouptput Directories: OBJ

    Source Directories:  C:\WOLFSRC

    </pre>
    
    <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/compile_2.png"><br>



Let's try to compile: Compile -&gt; Build All
    
    <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/compile_3.png"><br>

    We get an error: "Cannot find executable TASM"
    <img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/compile_4.png"><br>

    Exit Borland C++, we need to set the PATH:

      <pre>
     
     C:\&gt; CD ..
     C:\&gt; PATH=C:\BORLANDC\BIN
     C:\&gt; BC.EXE

  </pre>
  

<br>
<h3>Getting the assets</h3>
<p>
Download the shareware version or even better: Purchase as full version on Wolfenstein 3D.<br>
</p><pre>
    cd ~/system/c
    curl -O http://fabiensanglard.net/Compile_Like_Its_1992/tools/1wolf14.zip
    unzip 1wolf14.zip

</pre>
<p>
  Go back to DosBox and install the game to <code>C:\WOLF3D</code>.
  </p><pre>
  C:\&gt; c:
  C:\&gt; cd \
  C:\&gt; cd 1wolf14
  C:\1WOLF14&gt; install

  </pre>
  <p>
    After installation of the game, copy the .EXE we just compiled to the game folder, 
  </p><pre>

    C:\&gt; c:
    C:\&gt; cd wolf3d
    C:\WOLF3D&gt; copy WOLF3D.EXE WOLF3D.OLD
    C:\WOLF3D&gt; copy ../WOLRSRC/WOLF.EXE .

  </pre>

<br>
<h3>Running the game</h3>
<p>
Try to run it:
</p><pre>

    C:\&gt; cd wolf3d
    C:\WOLF3D&gt; copy WOLF3D.EXE WOLF3D.OLD
    C:\WOLF3D&gt; copy ../WOLRSRC/OBJ/WOLF3D.EXE .
    C:\WOLF3D&gt; WOLF3D.EXE


  </pre>
  <p>
  <br>

  Hm, that looks weird.....
  <br>
<img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/run_wolf.png"><br>
Uh...
<img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/run_wolf2.png"><br>
What ?
<img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/run_wolf3.png"><br>
I did not remember it like that....
<img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/run_wolf4.png"><br>
Ok something must be very wrong here !!<br>
</p>

<br>
<h3>What happened ?</h3>
<p>
It has to do with the production access pipeline and how they are used by the engine. 
When Adrian Carmack and Kevin Cloud were done crafting all the graphic files, they used a tool (IGRABed) to pack them together.
The output was made of 3+2 files.
</p><ul>
   <li>VGAHEAD.WL1</li>
   <li>VGAGRAPH.WL1</li>
   <li>VGADICT.WL1</li>
 </ul>
 <div><p>
  The VGAHEAD file is an index containing pointers to the VGAGRAPH where the data is stored huffman compressed. VGADICT
  contains the huffman dictionaries to decompress the data.</p><p>

The two other files produced:
  </p></div><ul>
    <li>GRE.H</li>
    <li>GRE.EQU</li>
  </ul>
  <div><p>
    Are compiled into engine as seen in the following drawing :</p><!--<img src="images/drawing_plain.svg" width="700px" height="469px" style="display:block;margin-left:auto;margin-right:auto;"/><br/>
  -->
  <p><img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/drawing_plain.png" width="700px" height="469px"><br>


    What are the <code>.H</code> and <code>.EQU</code> files needed for? In short, to allow access by name. When IGRABed assembled all files
    it would also create an enum with matching indices:</p><p>
    
    <b>GRE.H</b></p></div><pre>
            enum{ 
            H_WOLFLOGOPIC
            GETPSYCHEDPIC
            L_GUYPIC
            .
            .
            } graphicnums

    </pre>
    <p>
    <b>GRE.EQU</b>
 </p><pre>
            
            H_WOLFLOGOPIC  = 0
            GETPSYCHEDPIC  = 1
            L_GUYPIC       = 2

    </pre>
    <div><p>
      This way when the engine requested a particular asset, it could use a logical name (L_GUYPIC) instead of a "magic number" (2).</p><p>
      
      That means the engine shipped with the indices of the images in the VGA files HARD-CODED. Since the assets and
      codebase evolved after shipping wolf3D shareware (with Spears of Destiny), the newly compiled game indices do not
      match the location in the original assets files.

</p></div>
<br>
<h3>Running the game (again)</h3>
<div><p>

  Fortunately there is a simple solution to this problem: Somebody regenerated the VGA assets so they match the indices in the .H and .EQU
  released with the source code. Just copy those files (if you use the shareware assets you will have to change the file extension from .WL6 to .WL1).</p></div><pre>
  C:\&gt; copy C:\vgafiles\VGADICT.WL6 C:\WOLF3D\VGADICT.WL1
  C:\&gt; copy C:\vgafiles\VGAGRAPH.WL6 C:\WOLF3D\VGAGRAPH.WL1
  C:\&gt; copy C:\vgafiles\VGAHEAD.WL6 C:\WOLF3D\VGAHEAD.WL1

</pre>
<p>
    Let's try again:<br>
</p><pre>

  C:\WOLF3D&gt; WOLF3D.EXE
  

</pre>
<div>
  <p>
It works !!

<img src="https://fabiensanglard.net/Compile_Like_Its_1992/images/compressed.png"></p><p>

Yet we are are not done !

</p></div><h3>VGA framebuffer and screen aspect ratio</h3>
<p>
  It may not be obvious to people that never saw the original game but the DosBox image above is not what people saw in 1992.
  The VGA framebuffer was 320x200 but the CRT monitors had an aspect ratio of 4:3. Which mean the framebuffer was stretched vertically
  when sent to the monitor. DosBox has an option to compensate for that :
 <br>
</p><pre>
     vi ~/Library/Preferences/DOSBox\ 0.74\ Preferences
  
    [render]
    # frameskip: How many frames DOSBox skips before drawing one.
    # aspect: Do aspect correction, if your output method doesn't support scaling this can slow things down!.
    # scaler: Scaler used to enlarge/enhance low resolution modes.
      # If 'forced' is appended, then the scaler will be used even if the result might not be desired.
      # Possible values: none, normal2x, normal3x, advmame2x, advmame3x, advinterp2x, advinterp3x, ...

    frameskip=0
    aspect=<b>false</b>
    scaler=normal2x


</pre>
<p>
  Change that aspect to <b>true</b>:
<br>
Try again :<br>

</p><pre>

  C:\WOLF3D&gt; WOLF3D.EXE
  

</pre>




<br>
<h3>Recommended Readings</h3>
<p>
  <a href="https://fabiensanglard.net/Game_Engine_Black_Book/index.php">
  <img src="https://fabiensanglard.net/Compile_Like_Its_1992/gebb_book.png">
</a>
</p>


<!-- <h2>Comments</h2>
<p> -->


     <!-- <div id="disqus_thread"></div> -->
    <!-- <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'fabiensanglardswebsite'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || 
                document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> -->
    <!--<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->
<!--     




</p> -->

 
<p>@</p>

		</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firsty.app – free 300kbit/s eSIM for US/EU (223 pts)]]></title>
            <link>https://www.firsty.app/</link>
            <guid>39509645</guid>
            <pubDate>Mon, 26 Feb 2024 10:33:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.firsty.app/">https://www.firsty.app/</a>, See on <a href="https://news.ycombinator.com/item?id=39509645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div id="w-node-f482a30e-c83a-8d3e-a4ad-cfee762610c6-762610c3" data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" role="banner"><a href="https://www.firsty.app/" aria-current="page"><img loading="lazy" src="https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/656a289c0f179aab8a4a3f89_firsty-wordmark.svg" alt="LOGO FIRSTY"></a></div><section><h2 data-w-id="7ab5f4f3-b6fe-b405-2421-40751dc9a267">GLOBAL mobile data <br>for TRAVELERS</h2><div><p data-w-id="f33f7880-9790-4435-68e9-50c61154d692">One planet, one connection. <br>No credit card needed.</p><a data-w-id="efa6e46f-2f67-d860-0f5f-11a8e43eb1f6" href="https://apps.apple.com/nl/app/firsty-always-connected/id6470926128?l=en-GB" target="_blank"><p>Get the app</p></a><div data-w-id="494fbbe6-bd4f-0608-be3f-f1ba0c0a7d03"><p><a href="https://play.google.com/store/games?hl=en&amp;gl=US" target="_blank"><img src="https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/65d646d85ed0b6fa72272c44_icon-playstore.svg" loading="lazy" alt=""></a><a href="https://apps.apple.com/nl/app/firsty-always-connected/id6470926128?l=en-GB" target="_blank"><img src="https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/65d645b5d35d3839545a8871_icon-appstore.svg" loading="lazy" alt=""></a></p></div></div></section><div data-w-id="692f1f07-94b1-1122-a426-a6881f8dd88f"><p>👋 Hi, we’re Firsty.</p><p>With the Firsty app, you’re automatically connected to almost every provider on the 🌍.</p><p>No need to switch (e)SIMs or buy expensive bundles.</p><p>If you ask us, mobile connectivity is a basic human 👉 right.</p><p>That’s why Firsty offers a basic global data connection for FREE 🥳</p></div><div><div><div><p><img src="https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/65672299a46461a786168a5b_icon-1.png" loading="lazy" alt="logo"></p><div><h2>fair ‘n square</h2><p>Enjoy a basic connection for free allowing you to whatsapp, message and email globally. Speed it up for fair pricing.</p></div></div><div><p><img src="https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/65672886e99be1b1c525e1ce_icon-2.png" loading="lazy" alt="fuss free"></p><div><h2>fuss-free</h2><p>Automatically connect to the best available network. Never switch a (e)SIM while traveling</p></div></div><div><p><img src="https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/6567289d3202c6e50ad9b974_icon-3.png" loading="lazy" alt="first class"></p><div><h2><sup>first-class</sup></h2><p>Be flexible in your usage. Use data whenever you like and where you like without the risk of high costs or expiring data by choosing a custom plan.</p></div></div></div><div><p data-w-id="e14da410-9160-e6cf-3f1a-5f8ac5db8617"><h2><strong>Why does <br> mobile data &nbsp;have to be so&nbsp;Complex?</strong></h2></p><div data-w-id="63307b6d-8d4a-7185-d167-6a9738215fbf"><p>It’s like the turbulence starts the moment you touch down. Unnecessarily, travelers encounter unexpectedly high roaming bills or alternatively endure a time-consuming search for local (e)SIMs.</p><p>At Firsty, we make data roaming fair and square for everyone. We automatically connect you to every provider on the planet. Resulting in one connection for the globe. As simple as that. No need to switch SIMs or buy expensive bundles. If you ask us, mobile connectivity is a basic human right.<br> That’s why Firsty offers a basic global data connection for free. About bloody time, right?</p></div></div></div><div><h2>we offer two products</h2><p>There’s no need for more.</p><div><div><p><img src="https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/656a2685ba5218900226833f_graphic-free.png" loading="lazy" alt="SOCIAL MEDIA"></p></div><div><p><img src="https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/658c510491a9e3f9f0b0ce9d_fast-label%20(1).png" loading="lazy" alt="fast"></p><h2>firsty FAST</h2><p>Starting at €1,95/day</p></div></div></div><div data-w-id="3c9dc561-732a-256d-3d09-ffe228ea7452"><div><p><img src="https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/656a00f9986bdb4eb11188a2_image-promo.png" loading="lazy" data-w-id="4b7dec75-fc5c-8c38-1e36-96e15f265906" sizes="100vw" alt="mobile data app" srcset="https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/656a00f9986bdb4eb11188a2_image-promo-p-500.png 500w, https://assets-global.website-files.com/6547e7311b2b1b6d86a38adf/656a00f9986bdb4eb11188a2_image-promo.png 701w"></p></div><div><h2>GET IT NOW</h2><p>For iOS and Android</p></div></div><div><div><div><h2 data-w-id="eb1c95a6-e65e-9813-ffa1-ecefc2200a29">question and answers</h2><p data-w-id="6b661d08-2dcb-3cd3-bcf2-80694792802b">As we develop our app you might encounter things you’re not sure of. We’re here to help. One of our team members will guide you.</p><p><a data-w-id="4a4c7892-841b-d595-12cc-6720a8b18b50" href="mailto:support@firsty.app">Get Support</a></p></div><div><div id="Support" data-w-id="c26ab83d-4545-7bee-a139-419874b987b6"><div id="q1"><h4>How does Firsty Free work?</h4><p>keyboard_arrow_down</p></div><p>Firsty is a mobile app that provides seamless global connectivity for travelers. Firsty offers two products: Firsty Free and Firsty Fast. With Firsty Free, you enjoy a basic speed that allows you to email or message, completely free. It’s your go-to solution for staying in touch without spending a dime. Ready for more? Upgrade to Firsty Fast for lightning-speed data, perfect for streaming and video calls.</p></div><div data-w-id="c26ab83d-4545-7bee-a139-419874b987c0"><div id="q2"><h4>In which countries is Firsty available now?</h4><p>keyboard_arrow_down</p></div><p>Firsty's connectivity services are currently available in Europe, the USA, Turkey and Switzerland. We are actively working to expand our coverage and aim to offer global connectivity as of March 25th, 2024. Stay tuned for updates on our expanding network!</p></div><div data-w-id="c26ab83d-4545-7bee-a139-419874b987ca"><div id="q3"><h4>What is a Firsty SIM?</h4><p>keyboard_arrow_down</p></div><p>An eSIM, or embedded SIM, is a digital SIM card integrated directly into your device, such as a smartphone or tablet. Unlike traditional removable SIM cards, eSIMs are built into devices during manufacturing and cannot be physically removed or replaced by users. Firsty utilizes this eSIM technology to ensure continuous connectivity by remotely connecting you to local providers in the countries you visit, eliminating the need for manual SIM card swaps.</p></div><div data-w-id="c26ab83d-4545-7bee-a139-419874b987d4"><div id="q4"><h4>What is Firsty Fast?<br></h4><p>keyboard_arrow_down</p></div><p>Firsty Fast is the premium, paid version of our application. By upgrading from Free to Fast, you gain instant access to high-speed internet in supported countries (currently USA &amp; Europe). Leveraging partnerships with telecom providers globally, Firsty ensures access to the fastest available networks.</p></div><div data-w-id="c26ab83d-4545-7bee-a139-419874b987de"><div id="q5"><h4>Will I receive a phone number with my Firsty SIM?<br></h4><p>keyboard_arrow_down</p></div><p>Currently, Firsty is a mobile data-only solution , which means that phone numbers are not in our offering. However, thanks to eSIM technology, it’s possible to keep using your primary SIM card for calling and messaging in parallel with your Firsty SIM.</p></div></div></div><div><p>All your favourite apps keep going during your journey.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Real-World Locations of 14 Sci-Fi Dystopias (2014) (146 pts)]]></title>
            <link>https://www.atlasobscura.com/articles/the-real-world-locations-used-in-fourteen-film-dystopias</link>
            <guid>39509560</guid>
            <pubDate>Mon, 26 Feb 2024 10:20:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlasobscura.com/articles/the-real-world-locations-used-in-fourteen-film-dystopias">https://www.atlasobscura.com/articles/the-real-world-locations-used-in-fourteen-film-dystopias</a>, See on <a href="https://news.ycombinator.com/item?id=39509560">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article-body">
<p><span>In some science fiction cinema, </span>the future looks pretty bleak, with dystopic visions of a world struggling with overcrowding, high crime, pestilence, and the aftermath of war. And it can also look uncomfortably familiar thanks to filmmaker’s use of real-world buildings in their sets — sometimes to anchor the story in real life, and sometimes just because it already looks “futuristic.” We’ve assembled a list of 14 real-world filming locations of&nbsp;sci-fi dystopias.&nbsp;</p>
<p><strong>Blade Runner&nbsp;(1982)<br>Los Angeles, California</strong></p>
<p>This 1982 sci-fi classic was set in a futuristic Los Angeles, and features several landmarks from the genuine article. One of the most notable is the <a href="https://www.atlasobscura.com/places/bradbury-building">Bradbury Building</a>, where police officer Rick Deckard&nbsp;(Harrison Ford) fights with a mob of the film’s cyborg “replicants,” and has a final standoff with the replicant Roy Batty&nbsp;(Rutger Hauer).&nbsp;Much of the action takes place in the Bradbury Building’s courtyard, which is open to the public today.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8245/image" alt="article-image" width="600" data-kind="article-image"><em>Stairways of the Bradbury Building (photograph by&nbsp;<a href="http://www.flickr.com/photos/23642145@N00">Luke Jones</a>)</em></p>
<p><img src="https://assets.atlasobscura.com/article_images/8246/image" alt="article-image" width="600" data-kind="article-image"><em>Skylight in the Bradbury Building (photograph by&nbsp;<a href="http://www.flickr.com/photos/andrew_shiah/">Andrew Shiah</a>)</em></p>
<p>Across the street from the Bradbury Building is the Pan Am Building, which stood in for the “Yukon Hotel,” where Deckard catches a replicant early in the film. The police station in which Deckard receives his orders to assassinate all these replicants in the first place was a film set built inside Los Angeles’ cavernous Union Station, a major railway hub.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8266/image" alt="article-image" width="600" data-kind="article-image"><em>Ennis House (photograph by&nbsp;<a href="https://www.flickr.com/photos/jplouis/635408552/in/set-72157600509201984">Jean-Pierre Louis</a>)</em></p>
<p>Deckard’s apartment is in the Ennis House, a complex designed by Frank Lloyd Wright and meant to emulate Mayan temples. The building, which has been turning up in films since the 1930s, is a national landmark and guided tours are available.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8334/image" alt="article-image" width="600" data-kind="article-image"><em>Ennis House (photograph by <a href="https://www.flickr.com/photos/sarah_jane/8427492865/in/photolist-dQH728-zizi9-Y9DSo-Y5fe6-Y5ryR-Y5gfK-Y5aAn-Y59oa-Y9MrE-Y9QjS-Y5bLa-Y5imv-Y9CPS-Y9LaQ-Y5qBv-Y9PoW-Y9GYJ-Y5nNc-6iYDbf-6iUrYK-6iYCys-6iYAXU-6iUrqk-68msPE-9QYQWT-741YR7-68hfLe-4fy2Nf-gaPxCX-9VQUHM-6uAWDS-6uAYf3-6uAVGj-fQF4D-fQF3x-fQF4S-fQF3V-fQF3g-9Ugn3r-bwhxrj-93WsCW-4gCR7j-4gyMq6-4gQHjo-4gCQN7-7hzY1A-7hzWFf-7hzXrQ-7hzZyG-mxPeR2">Sarah Le Clerc</a>)</em></p>
<p><strong>Total Recall&nbsp;(1990)</strong><br><strong>Mexico City, Mexico</strong></p>
<p>Originally, Paul Verhoeven wanted to film 1990’s&nbsp;<a href="http://en.wikipedia.org/wiki/Total_Recall_(1990_film)">Total Recall</a>&nbsp;in Houston, but the accountants said no. So Verhoeven moved production south to Churubusco&nbsp;Studios in Mexico City. A handful of other sites in the city were also used, like the penthouse of the Hotel Nikko (now the Hyatt Regency). The crew even&nbsp;stayed at the hotel while filming, and the art directors were so impressed by its streamlined design that they borrowed elements for other scenes.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8261/image" alt="article-image" width="600" data-kind="article-image"><br><em>Hotel Nikko Mexico (photograph by&nbsp;<a href="http://commons.wikimedia.org/wiki/File%3AHotel_Nikko_Mexico_D.F_2011.JPG">Felipe Alfonso Castillo Vázquez</a>)</em></p>
<p>An especially gory scene of <em>Total Recall</em>&nbsp;sees construction worker/secret agent&nbsp;Douglas Quaid&nbsp;(Arnold Schwarzenegger) fleeing from gunmen via an escalator, and using the body of an unfortunate innocent bystander as a human shield. That escalator is in the Metro Chabacano subway station in Mexico City; some visitors claim they&nbsp;can still see leftover blood splatters from filming.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8325/image" alt="article-image" width="600" data-kind="article-image"><br><em>Metro Chabacano (photograph by&nbsp;<a href="https://www.flickr.com/photos/crincon/955766865">César Rincón</a>)</em></p>
<p>A different metro station — Metro Insurgentes — is also used earlier in the scene for the external shots.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8326/image" alt="article-image" width="600" data-kind="article-image"><em>Outside entrance of Metro Insurgentes in Mexico City (photograph by <a href="http://commons.wikimedia.org/wiki/File:InsurgentesMetroOutsideEntrance.JPG">Thelmadatter/Wikimedia</a>)</em></p>
<p><strong>Robocop&nbsp;(1987)<br>Dallas, Texas</strong></p>
<p>Paul Verhoeven did get to film in Texas for an earlier film — set in Detroit. Most of 1987’s <em>Robocop, </em>in which Peter Weller plays Alex Murphy, a recently-deceased Detroit policeman-turned-cyborg-supercop, was actually filmed in Dallas.&nbsp;</p>
<p>&nbsp;<img src="https://assets.atlasobscura.com/article_images/8328/image" alt="article-image" width="600" data-kind="article-image"><em><br>Atrium of Plaza of the Americas in downtown Dallas (photograph by <a href="http://commons.wikimedia.org/wiki/File:PlazaAmericas02.jpg">Dfwcre8tive/Wikimedia</a>)</em></p>
<p>The monolithic “Omni Consumer Products” is the sci-fi team responsible for Murphy’s transformation, and three separate locations in Dallas stand in for different spots in the sprawling OCP headquarters. Dallas’ City Hall serves as the building’s exterior — special effects extended the hall to a towering 95 stories. Some interior scenes were shot in the atrium of the Plaza of the Americas — a shopping center&nbsp;which also incorporates Dallas’ Marriott Hotel. A different hotel, the Rosewood Crescent, offered its parking garage as OCP’s garage in the film.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8327/image" alt="article-image" width="600" data-kind="article-image"><em>Dallas City Hall (photograph by&nbsp;<a href="http://commons.wikimedia.org/wiki/File:DallasCityHallA.jpg">Daquella Manera</a>)</em></p>
<p>Dallas High School on&nbsp;North Pearl Street stands in for the shabby Detroit Police Headquarters, while the interior Detriot PD scenes were filmed in the Sons of Herman Hall. The Sons of Herman Hall was a former German-American social club, which today serves as a music venue.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8260/image" alt="article-image" width="600" data-kind="article-image"><em>Dallas High School (photograph by&nbsp;<a href="http://commons.wikimedia.org/wiki/File%3ADallas_High_School.JPG">Jeffrey Beall</a><span>)</span></em></p>
<p><strong>Logan’s Run&nbsp;(1976)</strong><br><strong>Fort Worth, Texas</strong></p>
<p>Dallas and Fort Worth&nbsp;hosted the&nbsp;1976 dystopian film <em>Logan’s Run</em>. Several scenes were filmed in the Dallas Market Center, a shopping mall standing in for “The City,” an underground complex whose residents believe is the only safe place left on Earth. Another mall, the Hulen Mall in Fort Worth, was just completing construction during filming and was also used for some scenes.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8329/image" alt="article-image" width="600" data-kind="article-image"><br><em>The Water Gardens (photograph by Allison Meier/Atlas Obscura)</em></p>
<p>The <a href="https://www.atlasobscura.com/places/fort-worth-water-gardens">Fort Worth Water Gardens</a>&nbsp;stand in for The City’s hydro-electric power generator. The Philip Johnson-designed Water Gardens are an unusual architectural attraction, featuring three different concrete pools with fountains and water features. In the “Active Pool,” featured most prominently in the film, visitors can walk down a series of 38 steps to the small reflecting pool at the bottom, while water cascades down the steps and terraces all around them.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8330/image" alt="article-image" width="600" data-kind="article-image"><em>The Water Gardens (photograph by Allison Meier/Atlas Obscura)</em></p>
<p><strong>The Hunger Games (2012/2013)<br>Henry River Mill Village, North Carolina</strong></p>
<p>The <em>Hunger Games</em> trilogy implies that “District 12,” the impoverished coal-mining region which is home to&nbsp;heroine Katniss Everdeen&nbsp;(Jennifer Lawrence), corresponds with Appalachia. Much of the first film was shot in North Carolina, with the abandoned Henry River Mill Village standing in for District 12’s center.</p>
<p><img src="https://assets.atlasobscura.com/article_images/8242/image" alt="article-image" width="600" data-kind="article-image"><em>Abandoned Henry River Mill Village House (via <a href="http://www.hungergamestrilogy.net/">hungergamestrilogy.net</a>)</em></p>
<p><img src="https://assets.atlasobscura.com/article_images/8243/image" alt="article-image" width="600" data-kind="article-image"><br><em>Abandoned Henry River Mill Village (via <a href="http://www.hungergamestrilogy.net/">hungergamestrilogy.net</a>)</em></p>
<p>Built in 1904, the whole town was constructed&nbsp;to provide housing for the employees of a major textile mill on the Henry River. The mill ultimately closed in the 1980s, and the town was abandoned.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8255/image" alt="article-image" width="600" data-kind="article-image"><br><em>Swan House (photograph by<a href="https://www.flickr.com/photos/jamiedfw/6768237113/in/photolist-bj5Z6k-6XjpT3-4BPbz-bj5XmD-6XfqL8-4BNgL-2fxEYJ-2fRKMu-2fxF2j-2fN3tF-2fKDTR-2fKDTn-4BPbx-4BPu1-6XfqmP-6XjpKw-4BNgP-4BNgQ-4BNgM-4BPu2-4BNgN-4BPbw-4iK2LT-4iJZDT-4iP67N"> Jim Bowen</a>)</em></p>
<p>In <em>Catching Fire,</em> the latest entry in the film series, Katniss is one of the guests of honor at a party thrown by the scheming President Snow (Donald Sutherland). President Snow’s estate is actually the Swan House, a mansion built for a 1920s cotton magnate in Buckhead, just north of Atlanta, Georgia. Elsewhere in Atlanta, scenes featuring the Cornucopia — a bunker of supplies which contestants must collect — were filmed at the Beach, a portion of Atlanta’s Clayton County International Park originally constructed for the 1996 Olympics.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8254/image" alt="article-image" width="600" data-kind="article-image"><br><em>Marriott Marquis elevators (photograph by&nbsp;<a href="https://www.flickr.com/photos/byebyeempire/80889416/in/photolist-a6rYDC-89zB7-djDJYt-6DhEQ-7m69DE-7m2fke-7m67F5-djDKg8-4pZouH-6DWyh">Brian Pennington</a>)</em></p>
<p>Atlanta’s Marriott Marquis Hotel stands in for the victors’ quarters in the Capitol —&nbsp;Panem&nbsp;—&nbsp;and several scenes feature the hotel’s striking interior courtyard and elevator. The hotel was still receiving guests during filming, which lead to an embarrassing moment for actress Jena Malone. A&nbsp;scene required her to walk out of the elevator topless, and during one take the elevator stopped at the wrong floor, giving some guests quite a surprise.&nbsp;</p>
<p><strong>Brazil&nbsp;(1985)</strong><br><strong>UK &amp; France</strong></p>
<p>Filming locations for Terry Gilliam’s 1985 “dystopian satire” <em>Brazil</em> jump between the United Kingdom and Paris.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8344/image" alt="article-image" width="600" data-kind="article-image"><br><em>Millennium Mills (photograph by <a href="https://www.flickr.com/photos/ed_webster/7859933502">Ed Webster</a>)</em></p>
<p>Scenes set at the “Department of Records,” where everyman Sam Lowry (Jonathan Pryce) works, were filmed at the spooky Millennium Mills in East London, an abandoned flour mill from the early 20th&nbsp;Century.&nbsp;</p>
<p>&nbsp;<img src="https://assets.atlasobscura.com/article_images/8331/image" alt="article-image" width="600" data-kind="article-image"><em><br>Leighton House (photograph by <a href="https://www.flickr.com/photos/europanostra/6964124889">Europa Nostra</a>)</em></p>
<p>In one scene, Lowry’s mother (Katherine Helmond) undergoes a bizarre facelift in a lavish office. Gilliam filmed in the Arab Hall at the Leighton House Museum, the former residence of Victorian-era painter Frederic, Lord Leighton. The Arab Hall is tiled with the ceramics and woodwork Leighton collected on trips to the Middle East. Mrs. Lowry later returns “home” to what once was the Billiard Room of the National Liberal Club, a former clubhouse for members of England’s Liberal Party. The clubhouse has since been taken over by London’s Royal Horseguards Hotel, and the Billiard Room is used for special events.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8333/image" alt="article-image" width="600" data-kind="article-image"><br><em>Espaces d’Abraxas (photograph by <a href="https://www.flickr.com/photos/83318973@N00/117368888/in/photolist-bnxFw-bnxFx-6RHxpR-62df7V-62hthG">Marcus/Flickr user</a>)</em></p>
<p>Scenes in Lowry’s own home were actually filmed in Paris, chiefly in the Noisy-le-Grand commune. His apartment complex is the Espaces d’Abraxas designed by architect Ricardo Bofill and meant to evoke an ancient Greek amphitheater. Portions of the complex&nbsp;also stand in for “Chapel of Our Lady of the Checkout Counter,” which hosts a funeral scene. At film’s end, Lowry finally escapes his restrictive society — sort of — in the lush fields of Cumbria, in England’s Lake District. The exact scene lies somewhere near the village of Askham. &nbsp;</p>
<p><strong>Gattaca&nbsp;(1997)</strong><br><strong>Marin County, California</strong></p>
<p>Several scenes from <em>Gattaca</em>&nbsp;set at the “Gattaca Corporation,” the DNA testing and eugenics center controlling every person’s destiny, were filmed at the Marin County Civic Center just north of San Francisco.</p>
<p><img src="https://assets.atlasobscura.com/article_images/8267/image" alt="article-image" width="600" data-kind="article-image"><br><em>Marin County Civic Center (photograph by&nbsp;<a href="https://www.flickr.com/photos/dwhartwig/7626464530">Daniel Hartwig</a>)</em></p>
<p>The Civic Center was designed by Frank Lloyd Wright, and also appears in another dystopic film — the 1971<em>&nbsp;THX-1138,</em> George Lucas’ very first work.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8269/image" alt="article-image" width="600" data-kind="article-image"><br><em>Los Angeles City Hall (photograph by&nbsp;<a href="https://www.flickr.com/photos/order_in_chaos/5253595984/in/photolist-91f447-965T43-bR8K2M-8i8YCr-8i93QB-bptbb9-8sf3GU-iLcTJx-iLcWX6-a8vG9r-ef4U1L-8tMPwD-hgtbbW-hDvwe4-gQR95V-kxPFUS-k6zUvP-as3Fhh-mcTDBu-bXF49C-kxPDM5-ayFUpq-aEsXoA-bvKZS8-bvKZS4-bvKZSB-bvKZSi-6AWk1V-ap14wq-aN9yWZ-aN9zh6-aN9z5c-aN9zfg-aN9yP4-aN9zc8-aN9yYV-aN9z2k-aN9z6v-aN9ze2-aN9yHT-aN9z3P-aN9yRF-aN9z8a-aN9yLg-aN9za2-aN9yTP-6a9CrX-6adN3s-6adNYm-6adNJm">Ryan Quick</a>)</em></p>
<p>In another scene, the exterior of the Los Angeles City Hall stands in for a concert hall where Vincent (Ethan Hawke) and Irene (Uma Thurman) have their first date, watching a 12-fingered pianist. Afterward, Vincent takes Irene to watch the sunrise at a “solar farm,” filmed at an actual solar power plant in the Mojave desert.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8270/image" alt="article-image" width="600" data-kind="article-image"><br><em>Great Western Forum (photograph by &nbsp;<a href="http://commons.wikimedia.org/wiki/File%3AGreatWesternForum.jpg">Horge/Wikimedia</a>)</em></p>
<p>Later in the film, Irene starts to suspect (correctly) that Vincent is lying about who he is, and secretly takes one of his hairs in for DNA testing. The exterior and box office windows of the&nbsp;Great Western&nbsp;Forum are used for the testing facility.&nbsp;</p>
<p><strong>Escape from New York&nbsp;(1981)</strong><br><strong>East St. Louis, Illinois; St. Louis, Missouri; NYC</strong></p>
<p>Despite its name, much of the filming for this 1981 cult classic took place in East St. Louis, Illinois, as director John Carpenter feared it would be too expensive to make the real New York look sufficiently rundown. East St. Louis had&nbsp;plenty of townhouses which resembled New York, and had also recently suffered a major fire, on top of having an abandoned train station and a bridge, two locations the movie required.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8335/image" alt="article-image" width="600" data-kind="article-image"><em>East St. Louis (photograph by <a href="https://www.flickr.com/photos/binkle76/2463419473">Tyson Blanquart</a>)</em></p>
<p><img src="https://assets.atlasobscura.com/article_images/8271/image" alt="article-image" width="600" data-kind="article-image"><br><em>Liberty Island (photograph by&nbsp;<a href="https://www.flickr.com/photos/photographerglen/5921527642/in/photolist-a2gnSb-6ZhyWW-dNFWsR-d5qEim-5Dsp4M-6SZCb6-6Xtwe6-98SQY-9rxaeG-apjpgQ-8YjrkP-arTK3L-8NLNyf-5sKXSW-9ziM2t-4PHWrK-aySRLr-4SKecC-7WTv7U-7ZeBef-bHxqiv-7Zajwx-7Zdwpq-7ZfyFf-5E8vnd-5tJgyE-4N5piv-5wLPre-6FXJfc-6JZiWE-7gv4Ub-5oLy5u-ddTPfk-8uriJN-ddd1Fk-bceFjt-aySRxF-4TUxxU-ayVwbQ-ddcZMg-aySSgZ-ayVwas-ayVw7Y-ayVwPE-aySRBz-ayVwVL-aySSdD-ayVwk3-5NSnQF-ak1Rwy">Glen Scarborough</a>)</em></p>
<p>One scene was filmed on location in New York. In the film, Manhattan Island has been turned into a maximum-security prison colony, with the guards stationed on Liberty Island, at the foot of the Statue of Liberty. Carpenter was able to persuade federal officials to let the film use the real Liberty Island for the scene — the first film crew ever given access to the site at night. In the scene, guards receive word that Air Force One has crashed into Manhattan, and the President (Donald Pleasance) is now the hostage of the “The Duke Of New York” (Isaac Hayes).&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8272/image" alt="article-image" width="600" data-kind="article-image"><br><em>Fox Theatre (photograph by&nbsp;<a href="https://www.flickr.com/photos/matthewblack/1237121648/in/photolist-76bq1z-37dQqR-kasvbH-2TjzaA-ka1Yyj-r22tA-9EGx1Z-9EGwwX-9EKtzs-9EGy52-9EKthm-9EGxui-doeJHn-dkVGi6-7uxcnX-r22y1-r22AX-r22PG-r22zE-r22Gf-r22MB-r1wb9-r22CN-r22ws-r22Ja-r22RX-r22EJ-r2qKT-r22KU-82abaR-8FMgkx-r22uJ-jzcY7D-bxnPM3-8y7apn-a4tJUw-jRaBz5-jsTonM-jrvfux-r2qGE-9BvbbU-dshxCk-e6oQCH-9BsheR-aq2zLR-eFaYsb-afaMQJ-9BsgbR-eF4RBK-eF242e">Matthew Black</a>)</em></p>
<p>The exterior of St. Louis’ Fox Theater turns up in an early scene where Snake (Kurt Russell), the anti-hero&nbsp;inmate charged with rescuing the President, first goes looking for him.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8275/image" alt="article-image" width="600" data-kind="article-image"><br><em>Union Station’s Grand Hall (photograph by&nbsp;<a href="https://www.flickr.com/photos/pasa/9297491630/in/photolist-cuo4jq-e6csFY-e94uSs-e6csFm-e6csFA-faA5kQ-9oKa7N-f9KDze-73CKzJ-kWDya-kX1S3-6ZAHRs-6ZANRo-a3KD5F-a3NtXJ-a446iU-a446gj-a446mY-a3KCWR-bvbcPT">Paul Sableman</a>)</em></p>
<p>St. Louis’ Union Station stands in for Grand Central Station, where the President is being held captive. The place&nbsp;where Snake fights gladiator-style for his release is also in Union Station, in the Grand Hall.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8273/image" alt="article-image" width="600" data-kind="article-image"><br><em>Old Chain of Rocks Bridge (photograph by&nbsp;<a href="https://www.flickr.com/photos/tfduesing/6983575547">Thomas Duesing</a>)</em></p>
<p>The Chain of Rocks Bridge stands in for New York’s 59<span>th</span>&nbsp;Street Bridge, over which Snake leads the President to ultimate safety. Formerly a bridge on the iconic Route 66, the Old Chain of Rocks Bridge is now closed to vehicular traffic, but hosts biking and pedestrian paths.&nbsp;</p>
<p><strong>Soylent Green (1973)</strong><br><strong>Los Angeles, California</strong></p>
<p>Most of the 1973&nbsp;<em>Soylent Green</em> was filmed in Hollywood sound stages, but two scenes use locations elsewhere in Los Angeles. The exterior of the “euthanasia shop,” where Sol Roth (Edward G. Robinson) goes to subject himself to an assisted suicide, is actually the Los Angeles Memorial Sports Arena.</p>
<p><img src="https://assets.atlasobscura.com/article_images/8277/image" alt="article-image" width="600" data-kind="article-image"><em>Los Angeles Memorial Sports Arena (photograph by&nbsp;<a href="http://commons.wikimedia.org/wiki/File%3ALa_sports_arena.jpg">Pelladon/Wikimedia</a>)</em></p>
<p>Meanwhile, Los Angeles’ Hyperion Sewage Treatment Plant was used for the factory in which Robert Thorn (Charlton Heston) discovers what the film’s “Soylent Green” nutritional wafers are <em>really</em> made of. (Spoiler alert: it’s people.)</p>
<p><img src="https://assets.atlasobscura.com/article_images/8276/image" alt="article-image" width="600" data-kind="article-image"><em>Hyperion Sewage Treatment Plant (photograph by&nbsp;<a href="https://www.flickr.com/photos/docsearls/8480207669/in/photolist-tw4GN-dVnhi4-dVnhei-dVsStY">Doc Searls</a>)</em></p>
<p><strong>12&nbsp;Monkeys (1995)</strong><br><strong>Philadelphia, Pennsylvania; Baltimore, Maryland</strong></p>
<p>This 1995 <em>12&nbsp;Monkeys</em>&nbsp;by Terry Gilliam features John Cole (Bruce Willis) in a time-travel paradox, with several filming locations in Philadelphia and Baltimore. Set in the future after a global plague, the first scenes depict Cole collecting air samples outside Philadelphia’s City Hall.</p>
<p><img src="https://assets.atlasobscura.com/article_images/8279/image" alt="article-image" width="600" data-kind="article-image"><br><em>Philadelphia’s City Hall (photograph by&nbsp;<a href="https://www.flickr.com/photos/draket/6987696291/in/photolist-bDtLEg-4o7qeq-9XBsGE-8p5ME5-8p5Ngh-8p4GLE-8p1uEx-8GnAk3-86Y71s-atJesV-h47uBB-8SegWV-bFA8ca-auHujG-8Sei6B-7nqBh1-aCQPgm-4o7qCS-auHwzy-5f1Dvm-7oXgcX-85XowV-jzXXKt-7sjmGu-4SDot8-85X7zv-7RyLvw-cZamcu-5bmZ4f-cZam6U-9M2YNo-aTpScM-9hJuh2-d6s3a7-7fBACc-545qmj-fM85fW-7WDvuW-7fFuVq-6JXyhj-545qTY-a5BtWF-dsizCW-4aDeEN-9wovq-dsiyVf-dsiB8A-dBfhj4-53dTMb-58sjWg">Ted Drake</a>)</em></p>
<p>Cole is later sent back in time to discover the plague’s origins, where his behavior lands him in an insane asylum, alongside a gloriously demented Jeffrey Goines (Brad Pitt). For the asylum, Gilliam chose Philadelphia’s&nbsp;<a href="https://www.atlasobscura.com/places/eastern-state-penitentiary">Eastern State Penitentiary</a>.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8336/image" alt="article-image" width="600" data-kind="article-image"><em>Eastern State (photograph by Allison Meier/Atlas Obscura)</em></p>
<p><img src="https://assets.atlasobscura.com/article_images/8249/image" alt="article-image" width="600" data-kind="article-image"><br><em>Eastern State (photograph by&nbsp;<a href="http://www.avoidingregret.com/2008/10/satisfyingly-spooked-for-year.html">Sandi Hemmerlein</a>)</em></p>
<p>The penitentiary is itself worth a visit. Built in the 1820s, this spoke wheel-shaped prison represents a major reform in the early American penal system. Closed in 1969, the prison is now open for historic tours, where guests can see the former cells of real-life inmates (such as Al Capone) as well as the <em>12&nbsp;Monkeys</em> film sites.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8280/image" alt="article-image" width="600" data-kind="article-image"><br><em>Philadelphia Convention Center (photograph by&nbsp;<a href="https://www.flickr.com/photos/twobears2/5990287627/in/photolist-a8kMPa-8kuAns-2TYQu9-7nmGkV-5FEyQe-2WYuq-a8kNb4-a8kMZR-a8v77J-a8hU4u-a8f2Vn-a8f32v-a8hU6f-a8f2Qp-a8f2NK-a8f2U2">Tracie Hall</a>)</em></p>
<p>Elsewhere in Philadelphia, scenes from Cole’s recurring dream set at an airport were filmed at Philadelphia’s Convention Center. A scene towards the end of the film, where Kathryn Railly (Madeleine Stowe) buys disguises for herself and for Cole, were shot at Wanamaker’s Department Store.</p>
<p><img src="https://assets.atlasobscura.com/article_images/8281/image" alt="article-image" width="600" data-kind="article-image"><br><em>Senator Theater (photograph by&nbsp;<a href="https://www.flickr.com/photos/seannaber/4353330163/in/photolist-7CFXoB-7CKQ6S-7CKQaW-7CKRdL-7CKRnm-7CKR2A-7CKPLq-7CKSFu-7CFXVX-7CKSRb-7CKRSd-7CFZn2-7CG2fx-7CFXMT-7CKSxh-7CG1bF-7CG2FT-7CG1Ap-7CG2Re-7CFXYT-7CKSGW-7CKRNU-7CKSMY-7CKSoC-7CKSt3-7CKNUh-7CG3cp-7CKQuN-7CKSfE-7CKPi1-7CKQCN-7CKQY9-7CKSkm-7CKSU9-7CFZB8-7CKPsb-7CG1ag-7CFZEe-7CKREu-7CFY4e-7CKSnj-7CKSpQ-7CKRcU-7CG3at-7CFZoV-7CG2vV-7CG3dT-7CG3mg-7CKN5y-7CKNkJ">Sean Naber</a>)</em></p>
<p>In Baltimore, the Garrett-Jacobs Mansion stood&nbsp;in for the home of Dr. Goines (Christopher Plummer), the virologist whose lab inadvertently developed the plague from Willis’ time. Late in the film, Cole and Railly seek some private time in Baltimore’s Senator Theater, an art deco gem.&nbsp;</p>
<p><strong>The Matrix Trilogy (1999-2003)</strong><br><strong>Sydney, Australia</strong></p>
<p>The virtual-reality city inside “the Matrix” is supposed to be a generic any-place. But much of the trilogy was filmed in and around Sydney, Australia.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8283/image" alt="article-image" width="600" data-kind="article-image"><br><em>Metacrotex (photograph by&nbsp;<a href="https://www.flickr.com/photos/acroamatic/286769938/in/photolist-pijHZ-8w2UHk-8w5WZq-rkLHh">Kenneth Pinto</a>)</em></p>
<p>The Metacortex building, where Neo (Keanu Reeves) works as a programmer and first suspects something’s not right with the world, is actually the Metcentre, a shopping mall in Sydney’s Central Business District. The bridge where Neo waits for Trinity (Carrie Ann Moss) to collect him in a car and bring him to meet Morpheus (Laurence Fishburne) is the Adam Street Bridge near Sydney’s Chinatown.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8282/image" alt="article-image" width="600" data-kind="article-image"><br><em>Adams Street Bridge (photograph by <a href="https://www.flickr.com/photos/glutnix/54744291/in/photolist-5Qzz6-phyWS-rkLEm-69h7y-hLB1BA-joR8y6-jRZ6Y6-jC5u26-ipa6hz-6b6ku9">Brett&nbsp;Taylor</a>)</em></p>
<p>Sydney shows up in some of Morpheus’ training programs as well. For example, where Neo is distracted by a woman in a red dress at a fountain and then sees her transformed into one of the film’s “agents”&nbsp;is in a&nbsp;plaza near the corners of Martin Place and Pitt Street.</p>
<p><img src="https://assets.atlasobscura.com/article_images/8284/image" alt="article-image" width="600" data-kind="article-image"><br><em>Martin Place (photograph by&nbsp;<a href="https://www.flickr.com/photos/sg_harrison/3774963876/">SG_Harrison</a>)</em></p>
<p>Nearby is&nbsp;the Westin Hotel on the site of the former General Post Office. This is where Neo sees a black cat and has a moment of “déjà vu,” tipping his companions off to an impending attack. &nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8285/image" alt="article-image" width="600" data-kind="article-image"><br><em>General Post Office (photograph by&nbsp;<a href="http://commons.wikimedia.org/wiki/File%3AGeneral_Post_Office%2C_Sydney%2C_from_Barrack_Street.jpg">Bob Mead</a>)</em></p>
<p>The Colonial State Bank Center is also close by. This is the building where Morpheus is held captive and tortured by agents. The Colonial State may not admit visitors, but you can get into Forty One Restaurant at 2 Chifley Square, where one of Neo’s teammates betrays him to Agent Smith (Hugo Weaving) over a steak.&nbsp;</p>
<p>&nbsp;<img src="https://assets.atlasobscura.com/article_images/8337/image" alt="article-image" width="600" data-kind="article-image"><br><em>White Bay Power Station (photograph by<a href="https://www.flickr.com/photos/mugley/958231748/in/photolist-9KJUuE-9KJUC5-9KG5V2-9KG6g6-9KJUz5-9KG6jK-9KG6bH-9KJTU7-9KG5LD-9KG69r-9KJUqY-9KJUeC-9KG63F-9KG5Hp-9KJU4m-9KJUkf-9KJUih-9KJTY3-9KG5RV-2sFbV3-5HVjzA-5LY6V2-bsT6Bt-bsT42P-bsT6nt-bsT4bK-bsT5rF-bsT5Ng-bsT6cP-bsT3Rc-aBEWKt-aBHBe1-aBHBno-aBEWNi-b49y4p-3PtycU-33FJNu-56NH6Y-imWahX-dfmCCA-bsT574-aBEWii-bsT7U6-bsT5DT-bsT4pr-bsT5f8-bsT4yz-bsT5Zc-bsT4F8-bsT4R4"> Jes/Flickr user</a>)</em></p>
<p>Both sequels also feature Sydney; <em>The Matrix Reloaded</em> uses Sydney’s historic White Bay Power Station for the scenes where the rebels attempt to blow up a power station to kill the electricity to a building, and <em>Matrix Revolutions</em> concludes with a meeting between the Oracle and the Architect at the Royal Botanic Gardens on Sydney Harbor.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8250/image" alt="article-image" width="600" data-kind="article-image"><br><em>Royal Botanic Gardens (via&nbsp;<a href="http://en.wikipedia.org/wiki/File:RoyalBotanicGardensSydney01.jpg">Wikimedia</a>)</em></p>
<p><strong>Minority Report&nbsp;(2002)</strong><br><strong>Los Angeles, California</strong></p>
<p>Like <em>Blade Runner,</em> this is based on a Philip K. Dick story and was shot mainly in Los Angeles, despite the story’s Washington, DC setting. A few DC landmarks do make an appearance, however — the exterior of the Ronald Reagan Trade Center is used for the “Department of PreCrime” building where John Anderton (Tom Cruise) is the star detective.&nbsp;The Willard Washington DC Hotel is the site of a splashy party thrown by Anderton’s superiors just before the film’s “PreCrime” system goes national.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8287/image" alt="article-image" width="600" data-kind="article-image"><br><em>Ronald Reagan Trade Center (photograph by&nbsp;<a href="https://www.flickr.com/photos/iamdavidgaines/4872811981/in/photolist-8qAreF-f9LiRp-f9Lkh2-btLa7x-h9sD8y-aHgPSr-a9mQNX-h9sALS-h9tT5t-h9sBkN-a6bLyQ-a6cG7W-aHgz4v-h9afwz-7nyenU-h9aEcx-h9pV3N-btLa7X-atRGLm-btNGZB">David Gaines</a>)</em></p>
<p>Back in Los Angeles, the Descanso Botanic Gardens stand in for the home of Dr. Iris Hineman (Lois Smith), whom Anderton visits when he suspects the PreCrime system has some flaws. Descanso is home to a wide collection of roses, camellias, lilacs, and native California flora on its 160 acres (but none of the sentient poisonous plants which attack Anderton).&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8338/image" alt="article-image" width="600" data-kind="article-image"><em>Descanso Botanic Gardens (photograph by&nbsp;<a href="https://www.flickr.com/photos/dailymatador/4458160633">dailymatador/Flickr user</a>)</em></p>
<p>The film’s PreCrime system is driven by the visions of three clairvoyant men and women, known as “Precogs.” Anderton kidnaps one, Agatha (Samantha Morton), and brings her to an abandoned shopping mall. The mall is only half abandoned in real life. It’s actually the Hawthorne Plaza Mall, just south of Los Angeles. The entire shopping complex once consisted of an indoor mall with three big-box stores close by, but the mall portion was closed in 1999 after a spate of finance trouble. The three big-box stores are still open, while the mall is now used for a police training center.</p>
<p><img src="https://assets.atlasobscura.com/article_images/8288/image" alt="article-image" width="600" data-kind="article-image"><br><em>Hawthorne Plaza Mall (photograph by&nbsp;<a href="http://commons.wikimedia.org/wiki/File%3AThom_McAn.jpg">Amin Eshaker</a>)</em></p>
<p>Visitors can even also see&nbsp;one of the film&nbsp;sets on a&nbsp;studio backlot. The Warner Brothers VIP Studio Tour includes the street set where Anderton uses a jetpack to escape arrest after being falsely accused of a future murder.&nbsp;</p>
<p><strong>V for Vendetta (2005)</strong><br><strong>England&nbsp;&amp; Berlin</strong></p>
<p>Alan Moore set his fascist dystopia in London, so the 2005&nbsp;adaptation of <em>V for Vendetta</em> features some obvious landmarks like the Old Bailey courthouse and Big Ben. Much of the film was actually shot on sound stages in Berlin, although&nbsp;a handful of scenes were captured&nbsp;at lesser-known English sites. The film’s prologue, which depicts the story of English antihero Guy Fawkes, sets Fawkes’ execution at Hatfield House, a country house in Hertfordshire.</p>
<p><img src="https://assets.atlasobscura.com/article_images/8290/image" alt="article-image" width="600" data-kind="article-image"><br><em>Old Palace at Hatfield House (photograph by&nbsp;<a href="https://www.flickr.com/photos/93448689@N02/9383008399/in/photolist-fi9nuT-fia8DR-f4bXYS-fiozEQ-6jQ1eU-8KRG5w-6jQPnj-ak3n3q-6w18P-5SVd9j-5SQBi2-6RRpDa-7SA82n-8wghc2-8wgkS8-8wiVxm-4F7Ndw-4F3Fhp-4F3HgB-4F7L4N-9C1Mei-9BnkQQ-fi9geX-4EWMh-86rxQM-dSYVvg-4Mq8h-7rj19V-dT5sHY-bAGngq-aoXeWK-apVCYN-fQcVHo-9BjqUX-dT5xhS-hYtu9R-hYszJH-hYsKgE-hYsKE5-hYsXQf-5yEmXV-hYtuxX-hYsHBC-cd43vm-eGwx3n-hYsGJL-eMXxEQ-b4vbcF-b5omQv-aCmyt8">Jason Ballard</a>)</em></p>
<p>Hatfield House was the childhood home of Queen Elizabeth I. Her successor King James I disliked it and gave it to the Earl of Salisbury, who remodeled it extensively into the house that exists today.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8289/image" alt="article-image" width="600" data-kind="article-image"><br><em>Cloth Fair Street (photograph by&nbsp;<a href="https://www.flickr.com/photos/thefixedfactor/5174504322/in/photolist-4w8RLG-cPwLB-63yMrx-8TfFRL-5jBaPh-5jwT56-5jBaFd-8TfMNE-6J1UXu-5qhjKX-b8gkp2-57Ftwd-57FtcS-5iohYy-5qfHCB-5ioh2s-7CLdGJ-9DerHK-9L64Ao-abwFcq-63yLYZ-63yMm8-63yMSR-jAL9P3-bDk1x5-63yLz4-4dEVxR-61Y5xw-4dJUQq-4dEVvz-a64Woi-63D3XJ-5DakDa-63D4q7-4zJgny-63yMfc-63yN96-63yLkt-dgbgPD-63D3Dw-63yLS4-63D3vC-63D39S-63yLHz-63yLu4-63D3LL-63D4is-63D34o-dCQqDs-59mKyF">Simon Wicks</a>)</em></p>
<p>The street where the masked hero V (Hugo Weaving) rescues Evey (Natalie Portman) from a group of corrupt policemen&nbsp;is Cloth Fair, a thoroughfare&nbsp;where medieval textile merchants would set up stalls during summer market fairs.&nbsp;</p>
<p><img src="https://assets.atlasobscura.com/article_images/8291/image" alt="article-image" width="600" data-kind="article-image"><br><em>Chalk Farm Footbridge (photograph by&nbsp;<a href="http://commons.wikimedia.org/wiki/File%3AFootbridge_over_West_Coast_Mainline%2C_Chalk_Farm_-_geograph.org.uk_-_379360.jpg">Danny Robinson</a>)</em></p>
<p>In a catalytic scene, a girl is shot for daring to deface a political poster, and the surrounding crowd revolts. That scene takes place on the Chalk Farm Footbridge on Regent’s Park Road, near the trendy Camden Town district. And at the film’s finale, a mob of rebels, all in Guy Fawkes masks like V, gather at London’s Trafalgar Square before marching on Parliament.</p>
<p><img src="https://assets.atlasobscura.com/article_images/8258/image" alt="article-image" width="600" data-kind="article-image"><em>Trafalgar Square (photo by the author)</em></p>
<hr>
<p><strong><em><a href="https://www.atlasobscura.com/categories/film-locations">Discover more real-world film locations on Atlas Obscura &gt;</a></em></strong></p>
<p><em>Update 3/15/16: An earlier version of this story referred to the 69th Street bridge in New York City, instead of the 59th Street bridge. We regret the error.&nbsp;</em></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An interactive guide to Fourier series (2022) (122 pts)]]></title>
            <link>https://injuly.in/blog/fourier-series/index.html</link>
            <guid>39509414</guid>
            <pubDate>Mon, 26 Feb 2024 09:59:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://injuly.in/blog/fourier-series/index.html">https://injuly.in/blog/fourier-series/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39509414">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

      <hr>

      
      <p>Try drawing something on the first canvas,
and watch two sets of mechanical alien arms retrace your sketch
(no touch screen support yet):</p>

<p>Once you're done with this introduction to Fourier analysis, you'll be capable of making this (and a lot more) yourself.</p>
<p>The satisfying animation is made possible by the subject of this post - an infinite sum called <a href="https://en.wikipedia.org/wiki/Fourier_series">the Fourier series</a>.
The formula is short, and with some effort, you can memorize it.
However, I implore you to understand where the series comes from, and build deeper intuition for it.</p>
<p>To keep you from clicking off this page,
I'll defer the proof and origin of this equation to the second half,
and thread some interactive animations through the body of this write-up.</p>
<p>Before we get to tracing sketches however, let's brush up some basic math.</p>
<h2 id="adding-functions">Adding functions</h2>
<p>Surely, you're familiar with the addition of numbers, vectors and matrices.
Adding functions is not so different.
The addition of two functions \(f\) and \(g\) at input \(x\) is simply \(f(x) + g(x)\).</p>
<p>Put more formally - \((f + g)(x) = f(x) + g(x)\).</p>
<p>Let's visualize this by taking an example.
Assume <code>f</code> is \(2sin(x)\) and <code>g</code> is \(cos(2x)\).</p>
<p>Their sum then, can be given by a function: \(h(x) = 2sin(x) + cos(2x)\).</p>
<p>The graph below plots \(f\) and \(g\) in shades of gray, and their sum, \(h\), in red.</p>

<p>Note how in some places, the values of \(f\) and \(g\) are both positive, and their sum is therefore a larger positive number,
while in other places, \(f\) and \(g\) have opposite signs and their values cancel out to a smaller number.</p>
<p>Through the lens of physics, you could look at the two functions as electromagnetic waves, or visible light rays oscillating in the domain of time.
When two such waves overlap with each other in space, they're said to be in <a href="https://www.britannica.com/science/principle-of-superposition-wave-motion">superposition</a>.
The superposition of two waves results in their sum.</p>
<p>When two points in a wave supplement each other to result in a higher amplitude (the y-value),
their interaction is termed "constructive interference".
When they cancel each other out, it's called "destructive interference".</p>
<p>Go through the last two paragraphs again, and try to digest this idea.
Now, imagine if we had to work our way backwards.
Say we are given a list containing the (x, y) coordinates of all points along the curve of \(h\),
where \(x\) is time and \(y\) is the corresponding output of \(h\) at that point in time.
We have to come up with two simpler periodic functions that sum up to \(h\).</p>
<p>This is exactly what the Fourier series does.</p>
<p>There are several ways to interpret interference in the real world.
If \(f\) and \(g\) were sound waves, their constructive interference would make loud noise, while the destructive interference would produce a quieter sound.
If they were light waves instead, their constructive interference would reveal bright spots on a reflective surface,
and destructive would look like dim patches.</p>
<p>Applications of the Fourier series spill into almost every domain -
signal processing, image compression, shape recognition,
analog transmission, noise cancellation, studying thermodynamic systems
and fitting equations to datasets.</p>
<p>From this wide array of applications, we show our interest in the science of tracing ugly sketches.</p>
<h2 id="decomposing-periodic-functions">Decomposing periodic functions.</h2>
<p>Imagine you had a machine that could scan any food item and display its recipe.
Fourier series does exactly that, except for mathematical functions.</p>
<p>The Fourier series of any periodic function \(f(x)\) with a frequency of \(\omega_0\) is described as:</p>
<p>
   $$
   f(x) = a_0/2 + \sum_{n=1}^{\infty}b_n sin(n\omega_0x) + \sum_{n=1}^{\infty}a_n cos(n\omega_0x)
   $$
</p>
<p>Meaning that for every periodic function \(f\),
there exists a set of coefficients \(a\) and \(b\),
such that \(f(x)\) can be expressed as an infinite sum of sine and cosine terms of increasing frequencies where the
\(nth\) sine term has a coefficient of \(b_n\) and the \(nth\) cosine term has a coefficient of \(a_n\).
The values of these coefficients are given by the following formulae:</p>
<p>$$
a_n = \int_0^T{f(x)cos(nw_0x)}
$$</p>
<p>$$
b_n = \int_0^T{f(x)sin(nw_0x)}
$$</p>
<p>The interval of integration, \(T\), is the fundamental period of the function.
\(T\) and \(\omega_0\) are related by this equation:</p>
<p>$$
\omega_0 = 2\pi/T
$$</p>
<p>If that was too wordy and made little sense to you, that's okay.
We'll prove this equation later in the post.
Until then, an example will help understand this better.</p>
<p>Consider the square wave - a periodic signal that alternates between 1 and -1 depending on its input.
Formally, it is described like so:</p>
<p>$$
f(t) = 4 \lfloor{t}\rfloor - 2\lfloor2t\rfloor + 1
$$</p>
<p>Here's how it looks when graphed out:</p>

<p>If we use the first few terms from \(f\)'s Fourier series, we can closely approximate the behavior of this function.
In the following graph, the gray curve represents the the square wave and the red curve represents our approximation of it.
You can play with the slider to alter the number of terms we take from the series and see how that changes our approximation.</p>

<p>Clearly, our approximation improves as we take more terms from the series.
The Fourier series can be proven to <a href="https://en.wikipedia.org/wiki/Convergent_series">converge</a>.
This means that if we take an infinite number of terms from the series, we can get the <em>exact</em> value of \(f(x)\) for any \(x\).</p>
<p>Of course, it is not possible to add up infinite terms in computers.
Instead, we decide upon a fixed number of terms that approximate our function well enough for most practical purposes.</p>
<p>Whenever I say "Fourier series of a function", I mean a series of simple periodic functions that can be added at any given input to approximate the output of the original function at the same input.
For the remainder of this post our goal with Fourier series is to <strong>approximate periodic functions with sums of simpler sine/cosine functions</strong>.</p>
<h2 id="drawing-with-the-fourier-series">Drawing with the Fourier series</h2>
<p>If you wish to understand how the Fourier series works before seeing it in action,
you can skip this section and read ahead to <a href="#proof">the proof</a>, then come back here.</p>
<p>So, How do we go from decomposing time domain functions to recreating sketches?</p>
<p>Imagine you're drawing a sketch on a square sheet of paper.
You are to draw your sketch, start to finish, without lifting the nib of your pen from the paper's surface.
In other words, your sketch must be <em>continuous</em> with no "breaks" in between.</p>
<p>Assume also that the bottom-left corner of the sheet is its origin.
Once you start drawing, I can delineate the position of the pen's tip using a pair of coordinates \((x, y)\) at any given point in time.</p>
<p>Much like a cartesian plane, the \(x\) coordinate represents the horizontal distance from the origin, and \(y\) the vertical.
Both the x and y coordinates change as the pen moves on the sheet's surface.
Meaning, the position of the x-coordinate of your pen's tip can be written as a function of time.
Say you draw this figure:</p>

<p>If we plot the x and y-coordinates independently as functions of time, they'll form curves that look like this:</p>

<p>The blue curve represents the values of x-coordinates of your sketch.
The vertical axis represents the x-value, and the horizontal axis represents time.
Similarly, the red curve plots the y-coordinates.</p>
<p>Both these curves can be viewed as functions of time.
The blue curve represents a function \(x(t)\) that returns the x-position of the pen's tip at time \(t\),
Similarly, the red curve is a function \(y(t)\) which the same for its y-position.
For each of these functions, we can find a Fourier series that approximates it.</p>
<p>Let \(f_x(t)\) and \(f_y(t)\) be the Fourier approximations for \(x(t)\) and \(y(t)\) respectively.
Then recreating the sketch requires computing the values returned by <code>f_t</code> and <code>y_t</code> over a range of values of t.
then pairing them into <code>(x, y)</code> coordinates and connecting the coordinates with lines.
Here is some pseudo-typescript code that mimics this logic:</p>
<p><code><div><pre><code><span id="1"><span>// The "dt" is our time step.</span></span>
<span id="2"><span>// In the real world, a line is an infinitely long series of points.</span></span>
<span id="3"><span>// In computers, we take a "snapshot" of the pen's position</span></span>
<span id="4"><span>// every dt seconds and join these positions with straight lines to</span></span>
<span id="5"><span>// trace the curve. Smaller values of dt require more computation,</span></span>
<span id="6"><span>// and yield better results.</span></span>
<span id="7"><span>const</span> dt <span>=</span> <span>0.01</span><span>;</span></span>
<span id="8"><span>const</span> f_x <span>=</span> <span>fourier_series</span>(x)<span>;</span> <span>// type of x is (t: number) =&gt; number</span></span>
<span id="9"><span>const</span> f_y <span>=</span> <span>fourier_series</span>(y)<span>;</span> <span>// type of y is (t: number) =&gt; number</span></span>
<span id="10"><span>let</span> prev_point <span>=</span> [<span>f_x</span>(<span>0</span>)<span>,</span> <span>f_y</span>(<span>0</span>)]<span>;</span></span>
<span id="11"><span>for</span> (<span>let</span> t <span>=</span> <span>0.01</span><span>;</span> t <span>&lt;</span> <span>1</span><span>;</span> t <span>+=</span> dt) {</span>
<span id="12">  <span>const</span> current_point <span>=</span> [<span>f_x</span>(t)<span>,</span> <span>f_y</span>(t)]<span>;</span></span>
<span id="13">  <span>draw_line</span>(prev_point<span>,</span> current_point)<span>;</span></span>
<span id="14">  prev_point <span>=</span> current_point<span>;</span></span>
<span id="15">}</span></code></pre></div></code></p><p>The approximation generated by this method is shown below.
Just as before, you can play with the slider to adjust the number of terms used in approximation of the sketch.</p>

<p>Keep in mind that <code>f_x</code> and <code>f_y</code> are really just sums of simpler sine/cosine functions, calculated using Fourier's formulae.</p>
<p>You may be wondering - the functions \(x(t)\) and \(y(t)\) aren't periodic, how come we can still decompose them into sine/cosine sums?
One trick is to set the period to infinity, and compute the series at this limit.</p>
<p>In my code, I just set the period to 1 time unit, and assume that the pen just retraces the drawing again and again.
Meaning that \(x(t + 1) = x(0)\).
This makes the math a lot easier, and certainly doesn't make a difference in the outcome.</p>
<p>To be more clear, when the sketch starts, the time is assumed to be 0, and when it ends, the time is assumed to be 1 second.
Every time point in between is scaled accordingly. This is not necessary of course, you could set the time period to however long it took to draw the first sketch, if that makes things simpler for you.</p>
<h2 id="epicycles">Epicycles</h2>
<p>The final caveat are the epicycles.
It is easy to just plot the values returned by \(f_x\) and \(f_y\) on the cartesian plane.
But how do we animate this using revolving circles?</p>
<p>If you've followed the contents of this article so far, you already know how to recreate sketches.
To animate them, you need to understand <a href="https://en.wikipedia.org/wiki/Polar_coordinate_system">The polar coordinate system</a>.</p>
<p>You can read the wikipedia article, or <a href="https://www.mathsisfun.com/polar-cartesian-coordinates.html">this article</a> to build some intuition for conversion between cartesian and polar coordinates.</p>
<p>In the polar coordinate system, a periodic function with period \(T\) is a vector that rotates around the origin, and completes one full rotation around itself every \(T\) time units.
Look at the graph of \(sin(t)\) in Polar form, for example:</p>

<p>Note how the y-coordinates of the vector's tip traces out a regular sine wave.
You can just as easily plot any periodic function in the polar coordinate system.
To add two periodic functions together, take one rotating vector and center it on the tip of the another rotating vector.
The end result is shown below.
The following animation shows 3 rotating vectors added together, each representing a periodic function:</p>

<p>To convert a sketch to an epicycle animation then, all we need is to convert a term in the Fourier series from cartesian to polar coordinates.
Once we have that, we can add up the terms like in the animation above, and figure out the x and y-coordinates using two sets of epicycles, each representing the Fourier approximation for \(x(t)\) or \(y(t)\).</p>
<p>To do this conversion, we can use the <a href="https://en.wikipedia.org/wiki/Fourier_series#Amplitude-phase_form">polar form of the Fourier series</a>.
Precisely, these are the steps you need to follow:</p>
<ol>
<li>Represent the sketch as a list of points drawn over a period of time.
</li>
<li>Convert the list of points into a two separate lists, one containing the x-coordinates of the sketch, and other the y.
</li>
<li>Convert each list into a function (I use <a href="https://github.com/srijan-paul/fourier-sketch/blob/eb2be0f646f3097c6725ab621461ba59bfba4b6b/src/math/util.ts#L58">this simple helper</a>).
Now, you have the \(x(t)\) and \(y(t)\).
</li>
<li>For each function, find its Fourier series coefficients. <a href="https://github.com/srijan-paul/fourier-sketch/blob/eb2be0f646f3097c6725ab621461ba59bfba4b6b/src/math/fourier.ts#L16">Here</a> is how I do it.
</li>
<li>For each function, <a href="https://github.com/srijan-paul/fourier-sketch/blob/eb2be0f646f3097c6725ab621461ba59bfba4b6b/src/math/util.ts#L93">convert the Fourier series coefficients into a set of polar functions</a>.
</li>
<li>Using a time step of <code>dt</code>, find the final x and y positions of our approximation, and <a href="https://github.com/srijan-paul/fourier-sketch/blob/eb2be0f646f3097c6725ab621461ba59bfba4b6b/src/components/RedrawCanvas.tsx#L21">draw them on a canvas</a>.
</li>
</ol>
<p>If you do everything correctly, you should get something like this:</p>

<p>There is a more novel approach to retracing sketches that involves using only one set of epicycles.
It uses <a href="https://en.wikipedia.org/wiki/Fourier_series#Complex-valued_functions">the complex Fourier Series</a>, and is also fewer lines of code.
When you're new to this concept however, it may throw you off balance, especially if you're not familiar with imaginary numbers and the Argand plane.</p>
<h2 id="proof">Proof</h2>
<p>When I set out to find an "intuitive" proof for the Fourier series,
all I saw were proofs that begin by stating the equation,
and then proving it by finding the coefficients \(a_n\) and \(b_n\) using integrals.
But where did the equation come from?</p>
<p>Did God whisper it to Joseph Fourier in his dreams?</p>
<p>Did he just happen to run into it by chance?</p>
<p>Surprisingly, the answer is "yes".
Of course, he had an unparalleled instinct for math that he whetted with years of practice and research.
There has to be a certain train of thought that he boarded to arrive at this revelation, that any periodic signal can be represented as a sum of simpler harmonics.
But that line of thinking was never publicized, and as you'll see in the next section, there have been people who've thought of this even before Fourier himself did!</p>
<p>The important part is that Fourier asked a question that was mocked as stupid and bizarre until he presented a proof.
And that proof does in fact begin by stating the following hypothesis:</p>
<p>$$
f_o(t) = \sum_{n = 0}^\infty{b_nsin(n\omega_0t)}
$$</p>
<p>Here, \(f_o\) is an odd function with a fundamental period of \(w_0\).
If we can derive a value for \(b_n\) from this equation, we can be convinced that <strong>any odd function can be represented as a sum of sinusoids</strong>.</p>
<p>Now, consider an even function \(f_e\) with a period of \(w_0\):</p>
<p>$$
f_e(t) = \sum_{n=0}^{\infty}a_n cos(n w_0 t)
$$</p>
<p>If we can derive a value for \(a_n\) from this equation, we can be convinced that <strong>any even function can be represented as a sum of co-sinusoids</strong>.</p>
<p>When you combine these two equations with the idea that <a href="https://en.wikipedia.org/wiki/Even_and_odd_functions#Even%E2%80%93odd_decomposition">any periodic function can be represented as a sum of odd and an even function</a>, you get:</p>
<p>$$
f_o(t) + f_e(t) = \sum_{n = 0}^\infty{b_nsin(n\omega_0t)} + \sum_{n=0}^{\infty}a_n cos(n w_0 t)
$$</p>
<p>We can turn the order of this proof, and first say that given any function \(f(t)\), we can find its odd and even parts using the odd-even decomposition rule.
Then, we can represent the odd part as a sum of sinusoids, and the even part as a sum of co-sinusoids.</p>
<p>Now, all that's left is to derive the values for \(a_n\) and \(b_n\) using the two equations stated above.
This is where I save myself the trouble of writing more LaTeX, and defer you to <a href="http://lpsa.swarthmore.edu/Fourier/Series/DerFS.html">this excellent proof</a> by professors from Swarthmore college.
I know I said I'd walk you through the proof, but I can't do a better job of it than the electronics professors at Swarthmore did already.
I'd hate to repeat their work and not give credit.
If you follow the page I linked, you'll realize that the proof only uses basic calculus and trigonometric identities taught in high school.</p>
<h2 id="origins">Origins</h2>
<p>You'll be surprised to learn that the idea behind the series predates Fourier himself.</p>
<p><a href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss">Carl Friedrich Gauss</a>
was one of the many applied mathematicians who wanted to predict the position of Ceres in the night sky.
To attain this goal, he created several algorithms to aid his study of astronomy.
One of them was the <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">Fast Fourier Transform</a> - a function that is very closely related with the Fourier Series.
However, he never published his work because he believed his method to be an unimportant detail in the overall process.</p>
<p>In the 1700s, Euler had found applications for decomposing periodic functions with Fourier Series.</p>
<p>Half a century before Fourier, <a href="https://en.wikipedia.org/wiki/Daniel_Bernoulli">Bernoulli</a> was studying the motion of a string.
He proposed the idea that periodic functions can be represented as sums of harmonics.
Nobody at the time believed this to be a general method, and his ideas were left unexplored.</p>
<p>Things changed in 1807, when a French math wizard named Joseph Fourier found himself studying the heat equation in a metal plate.
In his search for a solution, he sought to ask a seemingly absurd question:</p>
<p><em>Can we represent any periodic function as a sum of simple sine and cosine functions?</em></p>
<p>Precisely, he sought to represent any periodic function \(f(x)\) with a frequency of \(\omega_0\) , in the following form:</p>
<p>
   $$
   f(x) = (a_0 + a_1 cos(\omega_0 t) + a_2 cos(2\omega_0 t) + ... + a_n cos(n\omega_0t)) + (b_1 sin(\omega_0 t) + b_1 sin(2\omega_0 t) + ... + b_n sin(n\omega_0t)
   $$
</p>
<p>Revered mathematicians of the time, including Langrange and Laplace, rejected this idea as informal and hand-wavy.
The panel evaluating his findings said:</p>
<p><em>"The manner in which the author arrives at these equations is not exempt of difficulties and...his analysis to integrate them still leaves something to be desired on the score of generality and even rigour."</em>
Perhaps this was because of a lack of reasoning as to <em>why</em> one should even begin to think of periodic functions this way.</p>
<p>It's not unheard of mathematical ideas to sprout into existence out of seemingly ridiculous places.
Ramanujan attributed some of his major findings to God, and dipped at the age of 32.</p>
<p>After the Fourier Series was accepted by the scientific populace, it spawned a new field of research, called Fourier analysis.
Developments in this field found everyday use in almost every science.</p>
<h2 id="applications">Applications</h2>
<p>By this point, you know enough about Fourier analysis to delve deeper into it yourself.
It would be a shame to blunt the edge of theory by not applying it in practice.</p>
<p>Here a few things you could do:</p>
<ul>
<li>Implement noise reduction in sounds.
</li>
<li>Sharpen images with denoising.
</li>
<li>Write a <a href="https://en.wikipedia.org/wiki/JPEG">JPEG</a> encoder/decoder.
</li>
<li><a href="https://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/">Fit an elephant</a>
</li>
<li>Write basic shape recognizers.
</li>
</ul>
<h2 id="resources-and-further-reading">Resources and further reading</h2>
<ul>
<li>3b1b - <a href="https://www.youtube.com/watch?v=r6sGWTCMz2k">But what is a Fourier series?</a>.
</li>
<li>Swarthmore college - <a href="http://lpsa.swarthmore.edu/Fourier/Series/WhyFS.html">The fourier series</a>.
</li>
<li>Jez Swanson - <a href="https://www.jezzamon.com/fourier/index.html">An interactive introduction to the fourier transform</a>.
</li>
<li>Tony Rosler - <a href="https://www.myfourierepicycles.com/">myFourierEpicycles</a>
</li>
</ul>
<!-- mathjax -->


<!-- lodash -->

<!-- pts.js -->

<!-- script for this post -->



      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Xkcd: Greenhouse Effect (263 pts)]]></title>
            <link>https://xkcd.com/2889/</link>
            <guid>39509379</guid>
            <pubDate>Mon, 26 Feb 2024 09:55:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xkcd.com/2889/">https://xkcd.com/2889/</a>, See on <a href="https://news.ycombinator.com/item?id=39509379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bottom">
<p><img src="https://imgs.xkcd.com/s/a899e84.jpg" width="520" height="100" alt="Selected Comics" usemap="#comicmap"></p><map id="comicmap" name="comicmap">
<area shape="rect" coords="0,0,100,100" href="/150/" alt="Grownups">
<area shape="rect" coords="104,0,204,100" href="/730/" alt="Circuit Diagram">
<area shape="rect" coords="208,0,308,100" href="/162/" alt="Angular Momentum">
<area shape="rect" coords="312,0,412,100" href="/688/" alt="Self-Description">
<area shape="rect" coords="416,0,520,100" href="/556/" alt="Alternative Energy Revolution">
</map>

<p><a href="https://xkcd.com/1732/"><img src="https://imgs.xkcd.com/s/temperature.png" width="520" height="100" alt="Earth temperature timeline"></a></p>
<br>
<div id="comicLinks"><p>
Comics I enjoy:<br>
        <a href="http://threewordphrase.com/">Three Word Phrase</a>,
        <a href="https://www.smbc-comics.com/">SMBC</a>,
        <a href="https://www.qwantz.com/">Dinosaur Comics</a>,
        <a href="https://oglaf.com/">Oglaf</a> (nsfw),
        <a href="https://www.asofterworld.com/">A Softer World</a>,
        <a href="https://buttersafe.com/">Buttersafe</a>,
        <a href="https://pbfcomics.com/">Perry Bible Fellowship</a>,
        <a href="https://questionablecontent.net/">Questionable Content</a>,
        <a href="http://www.buttercupfestival.com/">Buttercup Festival</a>,
        <a href="https://www.homestuck.com/">Homestuck</a>,
	<a href="https://www.jspowerhour.com/">Junior Scientist Power Hour</a>
</p></div>
<br>

<br>
<center>
<p>xkcd.com is best viewed with Netscape Navigator 4.0 or below on a Pentium 3±1 emulated in Javascript on an Apple IIGS<br>at a screen resolution of 1024x1. Please enable your ad blockers, disable high-heat drying, and remove your device<br>from Airplane Mode and set it to Boat Mode. For security reasons, please leave caps lock on while browsing.</p>
</center>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All Aboard the Bureaucracy Train (129 pts)]]></title>
            <link>https://asteriskmag.com/issues/05/all-aboard-the-bureaucracy-train</link>
            <guid>39509156</guid>
            <pubDate>Mon, 26 Feb 2024 09:15:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asteriskmag.com/issues/05/all-aboard-the-bureaucracy-train">https://asteriskmag.com/issues/05/all-aboard-the-bureaucracy-train</a>, See on <a href="https://news.ycombinator.com/item?id=39509156">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<div data-mode="add-marker">
		<p><img id="marker" src="https://asteriskmag.com/assets/img/asterisk_mark.png" title="save highlight"></p><!-- <a href="https://asteriskmag.com/about/#highlights"><img id="help" src="https://asteriskmag.com/assets/img/asterisk_help.png" title="about highlights"></a> -->
		
	</div>

	<section>
					<p>interview</p>
				
		 			<h2>
				   
					<span>Alon Levy</span>
							</h2>
			</section>

	 
		<section id="rangyscope">
					<p>The United States has the most expensive transportation infrastructure in the world. That’s because we refuse to learn from experts, other countries, and our own history.</p>
				<div>
											<div><p><strong>Asterisk:</strong><em> The overarching question in your transit policy career has been: Why is America so bad at building transit systems? Earlier this year, your team at NYU released a massive report comparing transit costs in Istanbul, Italy, Sweden and Boston, and then, of course, New York. It culminates in your fantastic case study of the Second Avenue subway line. And that’s where I wanted to start. What went wrong?</em></p><p><strong>Alon Levy:</strong><em> </em>Let’s go back to John Hylan.</p><p>Hylan was a populist mayor between 1918 and 1925. He ran against various corporate interests, which at the time included private streetcar companies. Hylan was hostile to the private operators. This led to bustitution — using buses to undercut the streetcar companies, though even then it was clear that the buses were providing inferior service that cost more to provide.<sup>
    <!-- <a id="fnref-1" href="#fn-1"> -->
    <span id="fnref-1">
        1    </span>
    <!-- </a> -->
</sup>
</p><p>And he was also against the private companies that were running the subway. So the city started building something called the Independent Subway System, commonly known as IND (because the private companies were called IRT, Interborough Rapid Transit). It was built by the city with the express purpose of driving the privates out of business — so the lines were kind of duplicative of the older lines. Only in two places did they truly expand into new places that were not served before. One is the G train, the Crosstown Line between Long Island City and downtown Brooklyn. The other is the Queens Boulevard line.</p><p>It added value. But it was also redundant on purpose — and it culminated in the city buying out the private operators at Depression prices.&nbsp;</p><p><strong>A:</strong><em> And what they built also had strong impacts on construction costs that continue today.</em></p><p><strong>AL:</strong><em> </em>Yes. The IND was built to very high standards. These standards meant that, for example, the stations were somewhat farther apart. They had more exits because they were planned around longer trains than the IRT.&nbsp;</p><p>They also decided to build full-length mezzanines, which has since become an American tradition, which everyone tells me is necessary because of the fire code. It is not, in fact, necessary, but it’s a tradition, and telling an American that a tradition is wrong and they should learn, for example, from how Germans build, is not that much different from telling an American that they should ceremonially shred the Constitution and the entire body of American law and adopt the governance standards of Nigeria.</p><p><strong>A:</strong><em> So let’s skip ahead to the 21st century and the Second Avenue subway line. How does this history tie into the high construction costs New York is still seeing for contemporary projects?</em></p><p><strong>AL:</strong><em> </em>As I mentioned, the existing standards in New York were very overbuilt. For example, London started building cut and cover subways in the 1860s and then switched to deep boring —&nbsp;</p><p><strong>A:</strong><em> Cut and cover?</em></p><p><strong>AL:</strong><em> </em>Cut and cover means you cut the street open, put down tracks and then cover them up. With deep boring you use a tunnel boring machine. The tunneling machine goes horizontally, and you don’t disturb the surface except to build stations, which are usually still cut and cover. The merchants will hate you, but after a couple of years of this, they get a station. You can even build the stations from underground, but it’s about 50% more expensive. Generally, cut and cover is superior to deep boring because the stations are closer to the surface, so passengers have less time to travel down to them. It’s also cheaper because you don’t remove as much earth and the stations are easier to build. It does create more surface disturbance. But if you can get away with cut and covering everything, you should cut and cover everything.&nbsp;</p><p><strong>A:</strong><em> And the standard set in the 1930s was to use deep boring?</em></p><p><strong>AL:</strong><em> </em>No — into the 1930s the IND used cut and cover, but among their high standards were wider curves, curves that were, in most cases, wider than the turn radius of the streets themselves. And because of that, something had to give. If it was in Brooklyn or in Queens, it didn’t matter very much. But in Manhattan they had to shave corners off of buildings, which meant demolishing buildings in midtown. This made everything much more expensive, and it meant they could not do cut and cover in a way that was cost effective.</p><p>And between that and the fact that cut and cover really is disruptive, the change to deep boring was made — not just the tunnels, which is pretty standard globally, but also the stations at 72nd and 86th Street, which is not standard. This is an example of how you start doing something right, but it’s compromised in a way that makes everything collapse. And as a result, instead of fixing the one bad thing, you replace the paradigm. &nbsp;</p><p>Now, most low-cost places use deep boring, not cut and cover. Cut and cover is cheaper. But if you’re, let’s say, Italy, and you have very low construction costs, you don’t want to deal with the surface disturbance, right? Since surface disturbance scales with income or population density.</p><p><strong>A:</strong><em> And Rome, obviously, is a dense city with a lot of buildings in it.</em></p><p><strong>AL:</strong><em> </em>Yeah. In Rome you can’t do cut and cover because you will cause the collapse of ancient ruins. The parts of the city that are near the classical core are especially expensive because they have to do a lot of deep mining just to protect them. And of course in New York they don’t have to do any of that, but they think that it’s harder.&nbsp;</p><p>The other thing is how to do project delivery. With some variation, in all places with low construction costs and in most places with medium construction costs, the state does planning. They can contract consultants to help, but the state supervises them. The state owns the design. You almost always do two contracts, one for design and one for construction. It’s called, in American terminology, “Design-Bid-Build.”&nbsp;</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/05/all-aboard-the-bureaucracy-train/b41f9f645e-1705605309/9887003_web.jpg" alt="">
  </p>
    <p>
    New York subways. Hagstrom Company, Inc., 1951. Courtesy David Rumsey  </p>
  </figure>
</div>
											<div><p><strong>A:</strong><em> So there’s two different sets of contractors involved here</em> — <em>one group that does the design process, one group that actually builds it.</em></p><p><strong>AL:</strong><em> </em>Yeah. Exactly.</p><p><strong>A:</strong><em> Okay. And then the alternative is Design-Build</em> — <em>where it’s the same people building who are designing.</em></p><p><strong>AL:</strong><em> </em>Right. There’s a belief that Design-Build is more modern and more advanced. But every time a place switches from Design-Bid-Build to Design-Build, their construction costs go up, which is usually taken as evidence that that place is uniquely difficult to build in.&nbsp;</p><p>Now, the right way to do Design-Bid-Build requires a bunch of elements that are not present in the United States. Some of them have never been present. Some of them have atrophied. The first element, that’s the one that used to be present but isn’t anymore, is a large public sector, because you need a large enough bureaucracy that can supervise all the contractors.</p><p><strong>A:</strong><em> I want to zoom in on this point a bit, because it’s not like the U.S. doesn’t have a large public service bureaucracy. But reading your report, it seems like the bureaucracy just doesn’t have the institutional knowledge that it needs to build cost-effectively. One line that hit me over the head was how some of the contractors had worked on projects in D.C. and Boston, but no one had immediate experience managing a project like Second Avenue. These are the people whose job it is to build transit in New York! How do they not have experience building transit in New York?</em></p><p><strong>AL:</strong><em> </em>I mean, New York hadn’t built in a while. And they’re New Yorkers. Do you think they’re really going to try to learn from a village like D.C. or Boston or Paris? It’s actually kind of striking, who learns from whom on this? One of the things that we figured out while doing the database is —&nbsp;</p><p><strong>A:</strong><em> The database, just to clarify for readers —</em></p><p><strong>AL:</strong><em> </em>The database shows the construction costs of, I’m guessing, 80 to 90% of the world’s subways by route length, with construction costs inflated to 2021 dollars. This required getting information in a bunch of different languages. One of our contractors is Arab-Israeli. And so he added a lot of Arab world data for us. And one of the things that he pointed out to us was that it was difficult for him to get information <br>out of Tunisia, Algeria and Morocco because a lot of the work there is done in French. These three countries specifically use French so much that they kind of try to be France. I mean, they’re very proud of their culture. They’re proud of anticolonialism, but they do imitate France in many ways.</p><p><strong>A:</strong><em> This is another interesting point</em> — <em>and I do want to get back to American civil service bureaucracy</em> — <em>but you have written about how former colonies tend to copy transit practices from the metropole.</em></p><p><strong>AL:</strong><em> </em>In Asia a lot of countries are trying to copy Japan, and they’re copying Japan in a very turnkey style. They use methods that require a lot of expensive capital and not a lot of cheap labor, because in Japan, capital is cheap and labor is expensive. It’s very visible in Indian construction costs. The stuff they do domestically, like their electrification project — you know, Indian railways are about to complete electrifying the entire country, right?</p><p><strong>A:</strong><em> Yes.</em></p><p><strong>AL:</strong><em> </em>Yeah, the construction costs for that are rather low, maybe $750,000 per route kilometer, and the Euro norm is probably about $2.5 million. So in India it’s maybe one-third as expensive. And then you look at mega-projects like metros of high-speed rail that those countries outsource to Japanese designers or that they learn from Japanese designers, and their construction costs are rather high. Often, the learning is toxic. That is to say, you learn from the wrong people or you learn from people who are the right people, but not for your circumstance. That’s absolutely a problem.</p><p><strong>A:</strong><em> So even though Japan has built efficiently for Japan, the methods that are efficient in Japan are not methods that are efficient in India.</em></p><p><strong>AL:</strong><em> </em>Exactly. This is a problem throughout the Third World. Now, bear <br>in mind that construction costs per kilometer and country GDP per capita are positively correlated, but that’s entirely because poor countries tend to build more elevated and inferior underground lines, which are cheaper. If you strip that away, the correlation, I think, was very low — 0.04. So that’s fascinating.</p><p><strong>A:</strong><em> You’d expect it to be cheaper because labor costs should be lower.</em></p><p><strong>AL:</strong><em> </em>Right. But labor is not a large proportion of cost. Labor in most places is 25% of the cost.&nbsp;</p><p><strong>A:</strong><em> But not in the U.S.?</em></p><p><strong>AL:</strong><em> </em>Yes, in the U.S., just not in the northeast. Labor issues are northeast specific. They don’t even exist in California.&nbsp;</p><p><strong>A:</strong><em> So bringing us back to Second Avenue —&nbsp;</em></p><p><strong>AL:</strong><em> </em>Yeah. So let’s pop the stack. In New York, they do have an in-house bureaucracy, but it’s atrophied. So what do we mean by “it’s atrophied”? First of all, the United States has, over the generations, developed a parallel civil service, which is an overclass of political appointees. The number that’s stuck in my head is 13,000 state level direct. For example, the head of the MTA is a political appointment of the governor.</p><p><strong>A:</strong><em> Does that mean there are 13,000 political appointees in transit?</em></p><p><strong>AL:</strong><em> </em>No, 13,000 people in the United States work for a state government who are appointed by the governor or lower-level official. The governor may choose to appoint a technocrat, but it’s not expected. And then there’s also the indirect people, the people who these appointees hire as their personal staff. When these people walk into the room, the experienced civil servants lower their heads a little bit. When you tell the civil service that it’s always going to be on the lower end of the class system, you’re going to lose the best people. The best people will pick a career path that lets them be on top, which in this system means they’re doing many different things and never learn one thing well.&nbsp;</p><p><strong>A:</strong><em> Spain and Italy are low-cost countries, right? And when I think of innovative, disruptive organizations, I don’t think of Italian bureaucrats.&nbsp;</em></p><p><strong>AL:</strong><em> </em>Right. Because the social innovations that you see in low-cost countries are really difficult to export.&nbsp;</p><p><strong>A:</strong><em> What features of these bureaucracies make them more effective? What really works in Italy or Turkey or Spain?</em></p><p><strong>AL:</strong><em> </em>Good question. I started complaining about American badness with things like the class system. So let’s look at the mirror image. Southern European bureaucracy is one where, first of all, they all have cultures of respect for engineers. The idea that a public policy grad from Harvard or Oxford gets to tell a scientist what to do — that’s a very American and British thing. In Germany, forget about it. In Italy, forget about it. The same with Spain and Portugal. And this is also carried over to Latin America — not all of which has low construction costs. Brazil kind of sucks at this, but there’s still a culture in which the engineers get a lot of respect. I hear this from students: “Oh yeah, the engineers get the nicest buildings.” The engineer can rise to the top.&nbsp;</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/05/all-aboard-the-bureaucracy-train/56032d7209-1705605416/tokyo_subway_system_map_japanese_beige_web.png" alt="">
  </p>
    <p>
    Tokyo subway map. Wiki commons  </p>
  </figure>
</div>
											<div><p><strong>A:</strong><em> This lack of effective leadership from the public sector comes up over and over again in your discussions of American transit, both internally and in their relationships with other actors. They struggle at cooperating with other agencies</em> — <em>with the Department of Transportation, with the Department of Environmental Protection, with parks, with buildings, with the fire department. And they’re also quite inefficient at managing consultants because they don’t have enough technical in-house knowledge to really understand how to.</em></p><p><strong>AL:</strong><em> </em>Exactly. So then they’ll hire a consultant to manage the consultants — it’s the most ridiculous thing.</p><p><strong>A:</strong><em> They don’t really know what asks to make of other actors. So, as an example of communication issues, the interfacing with public utilities just blew my mind. In New York, nobody knows where utilities are in the street, where an electric line is or a sewage line is. They’re constantly renegotiating to get them moved or get them integrated. In these low-cost countries we’ve been talking about, the transit agencies themselves are more centralized. So instead of having New York City Transit versus the Metropolitan Transit Authority versus, I don’t know, a state agency, there is one central agency that does planning. And obviously this makes things more streamlined internally, but they also seem to handle integration with other non-transit agencies better. And I’m curious how they manage that.</em></p><p><strong>AL:</strong><em> </em>Okay. So first of all, the MTA is actually rather integrated, and the same is true of the MBTA in Boston and SEPTA in Philadelphia.</p><p>However, there’s also an American tradition, which is very difficult to question, that commuter rail is separate. There was this former head of the MBTA, Frank DePaola, I think this was very recently in 2016, who said that commuter rail is not mass transit, it’s its own thing. And so there’s no cooperation. This means that, for example, East Side Access did not have any coordination between the Long Island Rail Road and Metro-North.&nbsp;</p><p>And again, they won’t learn because the U.S. finds it difficult to learn from others, especially if they don’t speak English, especially if they’re not Canada. We see this also with Design-Bid-Build versus Design-Build. To make them work, you need a large in-house bureaucracy. You can just pretend Design-Build works if you don’t have one, because the consultants will do everything. And then when they submit a large bill, they say, “Oh, it’s just hard to build here. It’s because we’re so rich.” But New Zealand and Ireland, which are not especially rich, have British construction costs. But again, you also need to have flexibility. You need to make sure that the builders can be flexible and do variations from design. They’re good at it in Turkey.&nbsp;</p><p><strong>A:</strong><em> This idea of flexibility is also interesting to me, because when you look at these projects, one of the major drivers of increased costs is that something always needs to be redesigned mid-project</em> — <em>a neighbor objects to something about the station or there’s a utility hookup that needs to be changed, and that always adds cost. It seems like American transit agencies are really bad at predicting cost overruns. You’d think that given that this seems to happen every time they’re doing an initial estimate, some of that would be priced in.</em></p><p><strong>AL:</strong><em> </em>This isn’t just an American thing. There’s no difference between cost overruns in the U.S. and Europe.</p><p><strong>A:</strong><em> So the U.S. is just worse on base costs, not overruns?</em></p><p><strong>AL:</strong><em> </em>Yeah, exactly. So actually the U.S. figured out that they had a cost overrun problem in the ’80s. What they started doing is acquiring contingencies. And the problem with contingencies is that the money will pretty much always be spent. A contingency is when — let’s say a project is going to cost $150 million. So instead of saying it’s $150, you say it’s $200 so that when the cost inevitably rises, you can say you went below budget. And this has been pretty deleterious because if you’re optimizing for cost overruns, you will just say that the costs are higher. You will include several layers of contingency. The Federal Transit Administration requires some ridiculously large numbers, I think almost 40%.</p><p><strong>A:</strong><em> So once you’ve said that your budget is $300 million, even if you only expect it to be $200 million, the money will get spent.&nbsp;</em></p><p><strong>AL:</strong><em> </em>I mean, a local neighborhood says they want a signature station, a station with art, where the artist got paid a couple hundred thousand dollars for the entire installation. Or they want to make a space very big, so they need to move the escalators and much more electrical wiring. It’s things like that: the special bespoke designs that don’t actually make the station nicer. Second Avenue Subway had, I think, two different escalator contractors.</p><p><strong>A:</strong><em> Another recurring theme. Every local actor that could be involved in this process has an ask, and it seems like a lot of the skill of managing a transit project is being able to have the requisite engineering knowledge and also the political skill to fend off all of the different requests the different agencies and municipalities and individuals will be making of you.</em></p><p><strong>AL:</strong><em> </em>Yeah, exactly. One of the issues is that if the civil service says, “Oh, these are our designs; they meet the fire code,” and a fire department official says, “No, they don’t,” the civil service is not going to back their own designers.<em>&nbsp;</em></p><p><strong>A:</strong><em> I’m from Los Angeles, and mostly in the 1950s, L.A. standardized plans for schools across the city. And then they just built them a bunch of times, and they’re beautiful buildings and much cheaper than designing from scratch every time, which is what happens now.</em></p><p><strong>AL:</strong><em> </em>That’s the right way to do it. Just build the same thing, and if you need to customize, art does not cost a lot of money. But instead of doing that or painting the bare tunnels like in Stockholm, they have way too many people in the room. These people are typically nongovernmental actors, or from a local government. But I don’t want to make this entirely about federalism because Canada is federal but has a much stronger provincial government than the American state government. Canada has almost the same construction costs as the United States. They kind of imported the American parallel party state, where they’re going to politicize engineering decisions. In New York it’s essentially the same problem. By the way, Italy and Greece both have a lot of clientelism but enough engineering procedure that it doesn’t hit the engineers. Spain and Portugal have much less clientelism than Italy and Greece. Spain and Portugal also have lower construction costs.</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/05/all-aboard-the-bureaucracy-train/d4da2865fe-1704924950/istanbul-train-map_beige_web.png" alt="">
  </p>
    <p>
    Istanbul subway map. istanbulmap360.com  </p>
  </figure>
</div>
											<div><p><strong>A:</strong><em> When you talk about clientelism, is that separate from appointments being politicized?&nbsp;</em></p><p><strong>AL:</strong> Clientelism and political appointments usually come from different traditions, but they’re very similar. Clientelism is when people vote for a party with the expectation that they will get jobs that way. In the United States, before the civil service system, they had the spoils system, where if you wanted to literally be the person digging things to build a road, you needed to be a member of the party that won the election. And so a lot of American civil service dysfunction is people being too rigid because they passed these laws in the 1900s and 1910s and didn’t update them correctly. Instead of, say, letting construction contractors make changes, they always keep the risk in the public sector. The thinking is that if the risk is privatized, the private concessionaire will just raise the prices. Because they assume the civil service means a set of federal regulations for the first third of the 20th century, the errors keep accumulating. Whereas a place like Southern Europe can build.&nbsp;</p><p><strong>A:</strong><em> This complicates the idea of corruption, because I do think of Italy as being more corrupt than the U.S.</em></p><p><strong>AL:</strong><em> </em>Oh yeah. Italy and corruption, it’s like America and racism. Italians are aware of their reputation for corruption. In the ’90s, the entire political system was upended in a series of highly mediagenic prosecutions by nonpolitical prosecutors, who at one point put half of the Italian parliament under indictment. They also passed a lot of anticorruption rules. Italian construction costs in the 1970s were not especially low. They were the same as Britain, same as Germany. They’ve fallen since the ’90s because they cracked down on corruption. There are a lot of remaining corruption issues in Italy, but they’re not about that.&nbsp;</p><p><strong>A:</strong><em> This ties back to something you were saying earlier</em> — <em>there’s learning from people who are doing things right, and then there’s learning the right lessons from your context. Italy’s problem was corruption, and they’ve taken steps to fix it. You talk a lot about the U.S. being resistant to learning from countries in Europe and Asia with low construction costs. But what are the lessons that the U.S. should be learning that are appropriate for our context?</em></p><p><strong>AL:</strong><em> </em>Good question. First of all, I’m going to start with very abstract things before I concretize. Don’t fixate on small differences within the core. Instead, pay attention to things that the core has in common.&nbsp;</p><p><strong>A:</strong><em> Which countries are the core in transit?</em></p><p><strong>AL:</strong><em> </em>Construction and operations are different. For example, Britain is good at operations, at least in London. It’s terrible at construction. In construction the best are Spain and Portugal, and I think also Turkey. Southern Europe in general is good. The Nordic countries used to be excellent. They no longer are. They’ve anglicised a little too much. Germany is decent. France is decent. Parts of Latin America are very good.&nbsp;</p><p><strong>A:</strong><em> Okay.</em></p><p><strong>AL:</strong><em> </em>So the narrow point is, try to learn from as many different places as possible and see what the commonalities are, not the differences. This is where you can talk about the civil service. Scandinavia has a culture of respect for the civil service. The same is true of Germany. The same is true of France, though it’s somewhat more classist. The same is true of Southern Europe, with engineers especially. So you definitely want that. I don’t know enough about Korea to tell you whether that’s how it differs from Taiwan. Taiwan has very high construction costs.&nbsp;</p><p>So first of all, the public sector needs to be empowered to make planning decisions. Again, you can outsource the design to consultants, but the consultants need to be supervised by civil servants, not political appointees and not other consultants. The most complex and the most sensitive projects are the ones that require the most public supervision, not the least. Another place we see this issue in the U.S. is with which contractor to pick. In the United States there is a legal framework for what’s called lowest responsible bid, which means the civil service is required to pick the lowest bid. Usually they can reject a bid if it’s unrealistic. But, for example, in California, they can’t even do that.</p><p>The correct way to do it is have the in-house engineers score every proposal technically for how good it looks to them, and then hire based on a combination of the technical score and the price. In California, they erred by having too little weight on the technical score, only 30%. The norm in the European low- and medium-cost countries that we have seen is that the technical score ranges from 50 to 70%. But this means you need to have an in-house bureaucracy that can technically score things. Another thing low-cost countries do is publicly itemize costs so that whenever you have disputes over changes, they’re prepriced. If someone says, say, “We’ve had a six month delay, and the prices of the concrete walls are higher than we expect because the prices of concrete have risen,” it’s priced in. It means that you want to constantly deprivatize risk as much as possible, because the state can bear the risk much more than a private contractor.</p><p><strong>A:</strong><em> In other words, the more risk that contractors are asked to take on, the more they’ll charge. And this adds up.</em></p><p><strong>AL</strong><em>: </em>Exactly. And this means that the state needs to be less hardline about changes. Again, the changes should be priced in.&nbsp;</p><p><strong>A:</strong><em> These places that have good civil service bureaucracies didn’t always. So how did they build them?&nbsp;</em></p><p><strong>AL:</strong><em> </em>First of all, you stop running against it. You stop calling them a deep state. At this point in the United States, it often happens to the parallel party governance. So Beto O’Rourke in the 2018 Senate race went to, I forget which college, maybe UT Austin, but a very left-leaning electorate to try to get out the vote. And he said the consultants would never let us go to a college. We go here because we didn’t hire any. Now, Beto, like every other American political candidate of note, hires consultants. He hires from the same pool of consultants as everyone else in the Democratic Party. But he runs against his own consultants doing something that is completely anodyne. He’s not an especially populist politician. He’s not an especially outsider politician. And even he ran against his own consultants. Politicians do it all the time, and it cascades downward. The political appointees do not trust the civil service because the civil service is constantly told that they’re not allowed to talk back. Do you think the head of the MTA knows which civil servants to fire? He doesn’t know the difference between them.&nbsp;</p><p><strong>A:</strong><em> And is there anything else concrete that you think the U.S. could implement that would help improve that culture?</em></p><p><strong>AL:</strong><em> </em>Get leaders who don’t talk to the civil service like a king talks to his servants. Andy Byford, head of the New York City Transit Authority from 2017 to 2020, was a political appointee in the sense that his appointment was a political decision by Andrew Cuomo. But he was an outsider to the United States, and he had experience as a professional in the industry his entire life.&nbsp;</p><p>The United States needs to start understanding that you guys are behind countries that you guys look down on. Turkey is the biggest example, not even Italy. Learn from the right places. It’s not like the United States does not have a civil service, okay? It’s not Uganda. There are a lot of people who are, let’s say, my age, give or take a couple years, who have very good ideas. They’re just not being listened to. Stop trying to polarize against the civil service. Hire people who are not generalists or political appointees. If you need to hire an outsider, hire an outsider.&nbsp;</p><p>And over time, I think it would succeed and create a tradition where you have to hire someone who’s a professional because all the successes will come from that. Byford is considered to have been a success in New York, and he became a superstar as a result. But if there are many like him in various parts of the United States, people will notice.&nbsp;</p></div>
										 
				</div>
		
	</section>
	 	<section>
		 		 <p><strong>Alon Levy</strong> is a Fellow in the Transportation and Land Use program of the NYU Marron Institute. They write <a href="https://pedestrianobservations.com/">Pedestrian Observations</a>, a blog about public transit.</p>		 		 		 </section>
	 	<section>            
		<p>
			Published February 2024		</p>
		
		<p>Have something to say? Email us at <a href="mailto:letters@asteriskmag.com">letters@asteriskmag.com</a>.</p>		                        
	</section>	
	
	
	<!--end published content, not coming soon-->

	<!--tags-->
		<section>
		<h4>Further Reading</h4>                
			<p>
				More:  
									<span data-no="tag-1">history</span>
									<span data-no="tag-2">interview</span>
							</p>
			<!--related articles-->
			             
	</section>
	 

	
	            
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Infrastructure Landscape (158 pts)]]></title>
            <link>https://ai-infra.fun/</link>
            <guid>39508950</guid>
            <pubDate>Mon, 26 Feb 2024 08:50:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai-infra.fun/">https://ai-infra.fun/</a>, See on <a href="https://news.ycombinator.com/item?id=39508950">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Early immunotherapy for Crohn’s disease significantly reduces complications (103 pts)]]></title>
            <link>https://newatlas.com/medical/crohns-disease-treatment/</link>
            <guid>39508703</guid>
            <pubDate>Mon, 26 Feb 2024 08:12:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/medical/crohns-disease-treatment/">https://newatlas.com/medical/crohns-disease-treatment/</a>, See on <a href="https://news.ycombinator.com/item?id=39508703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Giving an immunotherapy drug as soon as possible after diagnosis with Crohn’s disease significantly reduces complications, including the need for surgery by a factor of 10, a clinical study has found. Compared to conventional treatment, which favors delaying the use of advanced therapies, the results suggest re-thinking how the condition is treated.</p><p>Falling under the umbrella of inflammatory bowel diseases (IBD), Crohn’s disease is a painful and disabling chronic disease characterized by flare-ups that can cause progressive bowel damage. The standard treatment for acute flareups is steroids to reduce inflammation. Advanced therapy, which includes immunotherapy drugs, isn’t usually given until after conventional treatments have failed. Despite treatment with the current regime, surgical resection of the bowel is still needed in 17% to 25% of patients within five years of diagnosis.</p><p>Led by researchers from the University of Cambridge in the UK, a clinical trial examined the effectiveness of giving the immunotherapy drug infliximab earlier, as soon as possible after diagnosis. They employed what’s called a ‘top-down’ strategy, meaning the drug was given straight after diagnosis regardless of the patient’s symptom severity.</p><p>“As soon as a patient is diagnosed with Crohn’s disease, the clock is ticking – and has likely been ticking for some time – in terms of damage happening to the bowel, so there’s a need to start on advanced therapy such as infliximab as soon as possible,” said Nuru Noor, the study’s lead author.</p><p>Infliximab is a monoclonal antibody that enhances and improves the immune system by blocking tumor necrosis factor alpha (TNF-alpha), a protein cells produce during acute inflammation. The drug is injected intravenously into the bloodstream or under the skin. Although due to historical concerns about infliximab’s cost and side effects – including an increased risk of infection due to suppression of the immune system – the drug is currently only offered later in the disease’s course.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="The study's two treatment arms" width="942" height="429" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/5797ba5/2147483647/strip/true/crop/942x429+0+0/resize/440x200!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6f%2F66%2F1737730947ce818f1cbc353e1469%2Ftwo-trial-arms-crohns.jpeg 440w,https://assets.newatlas.com/dims4/default/a7c1738/2147483647/strip/true/crop/942x429+0+0/resize/800x364!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6f%2F66%2F1737730947ce818f1cbc353e1469%2Ftwo-trial-arms-crohns.jpeg 800w,https://assets.newatlas.com/dims4/default/6388b87/2147483647/strip/true/crop/942x429+0+0/resize/1200x546!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6f%2F66%2F1737730947ce818f1cbc353e1469%2Ftwo-trial-arms-crohns.jpeg 1200w,https://assets.newatlas.com/dims4/default/fd45c62/2147483647/strip/true/crop/942x429+0+0/resize/1920x874!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6f%2F66%2F1737730947ce818f1cbc353e1469%2Ftwo-trial-arms-crohns.jpeg 1920w" data-src="https://assets.newatlas.com/dims4/default/0a28d9c/2147483647/strip/true/crop/942x429+0+0/resize/942x429!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6f%2F66%2F1737730947ce818f1cbc353e1469%2Ftwo-trial-arms-crohns.jpeg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/5797ba5/2147483647/strip/true/crop/942x429+0+0/resize/440x200!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6f%2F66%2F1737730947ce818f1cbc353e1469%2Ftwo-trial-arms-crohns.jpeg 440w,https://assets.newatlas.com/dims4/default/a7c1738/2147483647/strip/true/crop/942x429+0+0/resize/800x364!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6f%2F66%2F1737730947ce818f1cbc353e1469%2Ftwo-trial-arms-crohns.jpeg 800w,https://assets.newatlas.com/dims4/default/6388b87/2147483647/strip/true/crop/942x429+0+0/resize/1200x546!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6f%2F66%2F1737730947ce818f1cbc353e1469%2Ftwo-trial-arms-crohns.jpeg 1200w,https://assets.newatlas.com/dims4/default/fd45c62/2147483647/strip/true/crop/942x429+0+0/resize/1920x874!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6f%2F66%2F1737730947ce818f1cbc353e1469%2Ftwo-trial-arms-crohns.jpeg 1920w" src="https://assets.newatlas.com/dims4/default/0a28d9c/2147483647/strip/true/crop/942x429+0+0/resize/942x429!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F6f%2F66%2F1737730947ce818f1cbc353e1469%2Ftwo-trial-arms-crohns.jpeg">
</p>



    
    

    
        <div><figcaption itemprop="caption">The study's two treatment arms</figcaption><p>Noor et al.</p></div>
    
</figure>

                
            </div><p>The researchers enrolled 386 patients aged 16 to 80 with active Crohn’s disease diagnosed in the previous six months. All patients were given an eight-week course of oral steroids, and then randomized into one of two treatment groups and followed for a year. The ‘top-down’ group received early infliximab combined with an immune system-modulating drug, whereas the ‘accelerated step-up’ group received conventional therapy focused on treating flare-ups.</p><p>The results were striking: at one year, 79% of patients receiving the top-down treatment showed sustained steroid- and surgery-free remission, compared to 15% in the accelerated step-up group. The top-down group had fewer adverse events, including disease flare-ups and serious adverse events, than the accelerated step-up group. Ten participants in the accelerated step-up group required urgent abdominal surgery for their Crohn’s, versus one participant in the top-down group.</p><p>Two-thirds (67%) of top-down patients had no visible ulcers when they underwent an endoscopy camera test at the end of the trial. ‘Endoscopic remission’, as it’s known, is important as it’s consistently been associated with a decreased risk of later Crohn’s disease complications. Most previous clinical trials have considered a therapy ‘highly successful’ if 20% to 30% of patients achieved an endoscopic remission. To top it off, patients in the top-down group also reported a higher quality of life and fewer hospitalizations.</p><p>“Historically, treatment with an advanced therapy like infliximab within two years of diagnosis has been considered ‘early’ and an ‘accelerated step-up’ approach therefore ‘good enough’,” Noor said. “But our findings redefine what should be considered early treatment. We’ve shown that by treating earlier, we can achieve better outcomes for patients than have previously been reported.”</p><p>Contrary to previous concerns, the researchers found no difference in the risk of serious infection between the two treatment groups, suggesting that infliximab was safe to give early. Moreover, infliximab is now available as a cheaper generic or ‘biosimilar’ drug in both the UK and the US. In Australia, treatment with the drug is subsidized through the Pharmaceutical Benefits Scheme (PBS).</p><p>“Up until now, the view has been ‘Why would you use a more expensive treatment strategy and potentially over-treat people if there’s a chance they might do fine anyway?’” said Miles Parkes, corresponding author of the study. “We now know we can prevent the majority of adverse outcomes, including [the] need for urgent surgery, by providing a treatment strategy that is safe and becoming increasingly affordable.”</p><p>While other anti-TNF drugs, such as adalimumab, work similarly to infliximab and are significantly cheaper, more research is needed to establish whether they’re clinically effective. The researchers are now analyzing the health economics to see whether the benefits of infliximab therapy outweigh its cost.</p><p>The study was published in <i><a href="https://www.thelancet.com/journals/langas/article/PIIS2468-1253(24)00034-7/fulltext#seccestitle170" target="_blank" data-cms-ai="0">The Lancet: Gastroenterology &amp; Hepatology</a></i>.</p><p>Source: <a href="https://www.cam.ac.uk/stories/crohns-disease-therapy" target="_blank" data-cms-ai="0">University of Cambridge</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Home Screen Advantage (294 pts)]]></title>
            <link>https://infrequently.org/2024/02/home-screen-advantage/</link>
            <guid>39508257</guid>
            <pubDate>Mon, 26 Feb 2024 06:46:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://infrequently.org/2024/02/home-screen-advantage/">https://infrequently.org/2024/02/home-screen-advantage/</a>, See on <a href="https://news.ycombinator.com/item?id=39508257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      <!-- BLOCK_CONTENT -->

  
  <article>
    <header>
      <h2>
        <a href="https://infrequently.org/2024/02/home-screen-advantage/">Home Screen Advantage</a>
      </h2>
      <h3>Decoding Apple's Desperate Ploy To Scuttle PWAs</h3>
      
    </header>
    <p>After weeks of confusion and intentional chaos, Apple's plan to kneecap the web <a href="https://open-web-advocacy.org/blog/its-official-apple-kills-web-apps-in-the-eu/">has crept into view,</a> menacing a PWApocalypse as the March 6th compliance deadline approaches for the EU's Digital Markets Act (DMA).</p>
<figure>
<picture width="608" height="256">
  <source sizes="(max-width: 1200px) 70vw, 600px" srcset="https://infrequently.org/2024/02/home-screen-advantage/SW4-SE-YavinOrbit3.jpg?nf_resize=fit&amp;w=3600 2400w,
                  https://infrequently.org/2024/02/home-screen-advantage/SW4-SE-YavinOrbit3.jpg?nf_resize=fit&amp;w=2400 1600w,
                  https://infrequently.org/2024/02/home-screen-advantage/SW4-SE-YavinOrbit3.jpg?nf_resize=fit&amp;w=1800 1200w,
                  https://infrequently.org/2024/02/home-screen-advantage/SW4-SE-YavinOrbit3.jpg?nf_resize=fit&amp;w=1200   800w,
                  https://infrequently.org/2024/02/home-screen-advantage/SW4-SE-YavinOrbit3.jpg?nf_resize=fit&amp;w=900   600w,
                  https://infrequently.org/2024/02/home-screen-advantage/SW4-SE-YavinOrbit3.jpg?nf_resize=fit&amp;w=750   500w,
                  https://infrequently.org/2024/02/home-screen-advantage/SW4-SE-YavinOrbit3.jpg?nf_resize=fit&amp;w=600   400w">
<img src="https://infrequently.org/2024/02/home-screen-advantage/SW4-SE-YavinOrbit3.jpg" alt="The view from Cupertino." width="608" height="256" decoding="async">
</picture>

  <figcaption>The view from Cupertino.</figcaption>
</figure>
<p>The DMA requires Apple to open the iPhone to competing app stores, and and its lopsided proposal for "enabling" them is <a href="https://on.ft.com/3wsiYZd">getting most of the press.</a> But Apple knows it has native stores right where it wants them. Cupertino's <a href="https://www.theverge.com/2024/1/25/24050696/epic-games-tim-sweeney-apple-app-store-response">noxious requirements</a> will take years to litigate. Meanwhile, potential competitors are only that.</p>
<p>But Cupertino can't delay the DMA's other mandate: <a href="https://www.theverge.com/2024/1/25/24050478/apple-ios-17-4-browser-engines-eu">real browsers,</a> downloaded from Apple's own app store. Since it can't keep them from emerging, it's trying to raise costs on competitors and lower their potential to disrupt Apple's cozy monopoly. How? By geofencing browser choice and kneecapping web apps, all while gaslighting users about who is breaking their web apps.</p>
<p>The immediate impact of iOS 17.4 in the EU will be broken apps and lost data, affecting schools, governments, startups, gamers, and anyone else with the temerity to look outside the one true app store for even a second.</p>
<p>The data loss will be catastrophic for many, as will the removal of foundational features like <a href="https://web.dev/articles/storage-for-the-web">reliable data storage,</a> <a href="https://web.dev/learn/pwa/app-design">app-like UI,</a> settings integration, <a href="https://web.dev/explore/notifications">Push Notifications,</a> and <a href="https://developer.chrome.com/docs/capabilities/web-apis/badging-api">unread counts</a>. This will <em>just so happen</em> to render PWAs useless to worldwide businesses looking to reach EU users. A <em>regrettable</em> accident, to be sure.</p>
<p>Apple's interpretation of the DMA <em>appears</em> to be that features not available on March 6th don't need to be shared with competitors, and it doesn't want to share web apps. The solution almost writes itself: sabotage PWAs ahead of the deadline and give affected users, businesses, and competitors minimal time to react.</p>
<p>Cupertino's not just trying to vandalise PWAs and critical re-engagement features for Safari; it's working to prevent <em>any</em> browser from <em>ever</em> offering them on iOS. If Apple succeeds in the next two weeks, it will cement a future in which the mobile web will never be permitted to grow beyond marketing pages for native apps.</p>
<p>By hook or by crook, Apple's going to maintain its home screen advantage.</p>
<p>The business goal is obvious: force firms back into the app store Apple taxes and out of the only ecosystem it can't — <a href="https://www.theverge.com/2023/10/26/23933206/google-apple-search-deal-safari-18-billion">at least not directly.</a> Apple's <a href="https://developer.apple.com/support/dma-and-apps-in-the-eu#8">justifications range from unfalsifiable smokescreens to blatant lies,</a> but to know it you have to have a background in browser engineering and the <a href="https://eur-lex.auropa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32022R1925&amp;qid=1708824200885">DMA's legalese.</a> The rest of this post will provide that context. Apologies in advance for the length.</p>
<p>If you'd like to stop reading here, take with you the knowledge that Cupertino's attempt to scuttle PWAs under cover of chaos is exactly what it appears to be: a shocking attempt to keep the web from <em>ever</em> emerging as a true threat to the App Store and <a href="https://www.theregister.com/2024/02/16/apple_web_apps/">blame regulators for Apple's own malicious choices.</a></p>
<p>And they just might get away with it if we don't all <a href="https://open-web-advocacy.org/apple-attempts-killing-webapps/">get involved ASAP.</a></p>
<div><ul><li><a href="#chaos-monkey-business">Chaos Monkey Business </a></li><li><a href="#but-wait!-there's-more!">But Wait! There's More! </a><ul><li><a href="#what's-at-risk%3F">What's At Risk? </a></li><li><a href="#not-with-a-bang%2C-but-with-a-beta">Not With A Bang, But With A Beta </a></li></ul></li><li><a href="#lies%2C-damned-lies%2C-and-%22still%2C-we-regret...%22">Lies, Damned Lies, and "Still, we regret..." </a><ul><li><a href="#the-point">The Point </a></li></ul></li><li><a href="#the-mask-is-off">The Mask Is Off </a></li></ul></div>
<h2 id="chaos-monkey-business">Chaos Monkey Business <a href="#chaos-monkey-business">#</a></h2>
<p>Two weeks ago, Apple sprung its <a href="https://www.wired.com/story/developers-revolt-apple-dma/">EU Digital Markets Act (DMA) compliance plans on the world as a <em>fait accomplis</em>.</a></p>
<p>The last-minute unveil and months of radio silence were calculated to give competitors minimal time to react to the complex terms, conditions, and APIs. This tactic tries to set Apple's proposal as a negotiating baseline, forcing competitors to burn time and money arguing down plainly unacceptable terms before they can enter the market.</p>
<p>For native app store hopefuls, this means years of expensive disputes before they can begin to access an already geofenced market. This was all wrapped in <a href="https://www.apple.com/newsroom/2024/01/apple-announces-changes-to-ios-safari-and-the-app-store-in-the-european-union/#:~:text=the%20best%2C%20most%20secure%20experience%20possible%20for%20EU%20users.-,The%20new%20options%20for%20processing%20payments%20and%20downloading%20apps%20on%20iOS,the%20EU.%20Even%20with%20these%20safeguards%20in%20place%2C%20many%20risks%20remain.,-Developers%20can%20learn%20about%20these%20changes%20on%20the%20Apple">a peevish, belligerant presentation,</a> which the good folks over at <a href="https://theplatformlaw.blog/2024/01/26/when-apple-takes-the-european-commission-for-fools-an-initial-overview-of-apples-new-terms-and-conditions-for-ios-app-distribution-in-the-eu/">The Platform Law Blog have covered in depth.</a></p>
<p>Much of the analysis <a href="https://www.techmeme.com/240125/p30#a240125p30">has focused on the raw deal Apple is offering native app store competitors,</a> missing the forest for the trees: <a href="https://infrequently.org/2024/01/the-web-is-the-app-store/">the threat Apple can't delay by years comes from within.</a></p>
<p>Deep in the sub-basement of Apple's tower of tomfoolery are APIs and policies that purport to enable browser engine choice. If you haven't been working on browsers for 15 years, <a href="https://developer.apple.com/support/alternative-browser-engines/">the terms</a> might seem reasonable, but to these eyes they're <a href="https://techcrunch.com/2024/01/26/apple-dma-webkit/amp/">anything but.</a> OWA <a href="https://open-web-advocacy.org/blog/owa-review-apple-dma-compliance-for-web/">has a lengthy dissection of the tricks Apple's trying to pull.</a></p>
<figure>
<picture width="684" height="538">
  <source sizes="(max-width: 1200px) 70vw, 600px" srcset="https://infrequently.org/2024/02/home-screen-advantage/pray-i-do-not-comply-further.webp?nf_resize=fit&amp;w=3600 2400w,
                  https://infrequently.org/2024/02/home-screen-advantage/pray-i-do-not-comply-further.webp?nf_resize=fit&amp;w=2400 1600w,
                  https://infrequently.org/2024/02/home-screen-advantage/pray-i-do-not-comply-further.webp?nf_resize=fit&amp;w=1800 1200w,
                  https://infrequently.org/2024/02/home-screen-advantage/pray-i-do-not-comply-further.webp?nf_resize=fit&amp;w=1200   800w,
                  https://infrequently.org/2024/02/home-screen-advantage/pray-i-do-not-comply-further.webp?nf_resize=fit&amp;w=900   600w,
                  https://infrequently.org/2024/02/home-screen-advantage/pray-i-do-not-comply-further.webp?nf_resize=fit&amp;w=750   500w,
                  https://infrequently.org/2024/02/home-screen-advantage/pray-i-do-not-comply-further.webp?nf_resize=fit&amp;w=600   400w">
<img src="https://infrequently.org/2024/02/home-screen-advantage/pray-i-do-not-comply-further.webp" alt="Apple's message of hope and optimism for a better web." width="684" height="538" decoding="async" loading="lazy">
</picture>

  <figcaption>Apple's message of hope and optimism for a better web.</figcaption>
</figure>
<p>The proposals are maximally onerous, but you don't have to take my word for it; here's Mozilla:</p>
<blockquote>
<p>We are ... extremely disappointed with Apple’s proposed plan to restrict the newly-announced BrowserEngineKit to EU-specific apps. The effect of this would be to force an independent browser like Firefox to build and maintain two separate browser implementations — a burden Apple themselves will not have to bear.</p>
<p>Apple’s proposals fail to give consumers viable choices by making it as painful as possible for others to provide competitive alternatives to Safari.</p>
<p>This is another example of Apple creating barriers to prevent true browser competition on iOS.</p>
<p>— <a href="https://www.theverge.com/2024/1/26/24052067/mozilla-apple-ios-browser-rules-firefox">Mozilla spokesperson</a></p>
</blockquote>
<p>The obvious strategy is to raise costs and lower the value of porting browsers to iOS. Other browser vendors have cited exactly these concerns when asked about plans to bring their best products to iOS. Apple's play is to engineer an unusable alternative then cite the lack of adoption to <em>other</em> regulators as proof that mandating real engine choice is unwise.</p>
<p>Instead of facilitating worldwide browser choice in good faith, Apple's working to geofence progress; classic "divide and conquer" stuff, justified with <a href="https://www.theregister.com/2021/05/27/safari_webkit_bug/">serially falsified security excuses.</a> Odious, brazen, and likely in violation of the DMA, but to the extent that it will now turn into a legal dispute, that's a feature (not a bug) from Apple's perspective.</p>
<p>When you're the monopolist, delay is winning.</p>
<h2 id="but-wait!-there's-more!">But Wait! There's More! <a href="#but-wait!-there's-more!">#</a></h2>
<p>All of this would be stock <a href="https://infrequently.org/2024/02/home-screen-advantage/2022/06/apple-is-not-defending-browser-engine-choice/#apple's-long-standing-policies-are-anti-diversity">FruitCo doing anti-competitive FruitCo things,</a> but they went further, <a href="https://open-web-advocacy.org/blog/did-apple-just-break-web-apps-in-ios17.4-beta-eu/">attempting to silently shiv PWAs and blame regulators for it.</a> And they did it in the dead of the night, silently disabling important features as close to the DMA compliance deadline as possible.</p>
<p>It's challenging, verging on impossible, to read this as anything but <em>extrordinary</em> bad faith, but Apple's tactics require context to understand. The <a href="https://digital-markets-act.ec.europa.eu/about-dma_en#:~:text=The%20DMA%20entered%20into%20force,and%20provide%20all%20relevant%20information.">DMA came into force in 2022,</a> putting everyone (including Apple) on notice that their biggest platforms and products would probably be "designated", and after designation, they would have six months to "comply". The first set of designation decisions <a href="https://theplatformlaw.blog/2023/09/06/european-commission-adopts-first-dma-designation-decisions/">went out last Sept</a>, obligating Android, Windows, iOS, Chrome, and Safari to comply <em>no later</em> than March 6th, 2024.</p>
<figure>
<a href="https://digital-markets-act.ec.europa.eu/about-dma_en" alt="Apple tried everything to <a href='https://www.theregister.com/2023/11/02/apple_safari_browser/'>shrink the scope of enforcement</a> and <a href='https://www.theverge.com/2024/1/8/23961923/apple-app-store-appeal-european-union-digital-markets-act-core-platform-service-gatekeeper'>delay compliance,</a> but in the end had the same two-years of notice and six-months warning from designation as everyone else." target="_new">
<picture width="712" height="287">
  <source sizes="(max-width: 1200px) 70vw, 600px" srcset="https://infrequently.org/2024/02/home-screen-advantage/digital_markets_act_timeline_0.png?nf_resize=fit&amp;w=3600 2400w,
                  https://infrequently.org/2024/02/home-screen-advantage/digital_markets_act_timeline_0.png?nf_resize=fit&amp;w=2400 1600w,
                  https://infrequently.org/2024/02/home-screen-advantage/digital_markets_act_timeline_0.png?nf_resize=fit&amp;w=1800 1200w,
                  https://infrequently.org/2024/02/home-screen-advantage/digital_markets_act_timeline_0.png?nf_resize=fit&amp;w=1200   800w,
                  https://infrequently.org/2024/02/home-screen-advantage/digital_markets_act_timeline_0.png?nf_resize=fit&amp;w=900   600w,
                  https://infrequently.org/2024/02/home-screen-advantage/digital_markets_act_timeline_0.png?nf_resize=fit&amp;w=750   500w,
                  https://infrequently.org/2024/02/home-screen-advantage/digital_markets_act_timeline_0.png?nf_resize=fit&amp;w=600   400w">
<img src="https://infrequently.org/2024/02/home-screen-advantage/digital_markets_act_timeline_0.png" alt="Apple tried everything to <a href='https://www.theregister.com/2023/11/02/apple_safari_browser/'>shrink the scope of enforcement</a> and <a href='https://www.theverge.com/2024/1/8/23961923/apple-app-store-appeal-european-union-digital-markets-act-core-platform-service-gatekeeper'>delay compliance,</a> but in the end had the same two-years of notice and six-months warning from designation as everyone else." width="712" height="287" decoding="async" loading="lazy">
</picture>

</a>
  <figcaption>Apple tried everything to <a href="https://www.theregister.com/2023/11/02/apple_safari_browser/">shrink the scope of enforcement</a> and <a href="https://www.theverge.com/2024/1/8/23961923/apple-app-store-appeal-european-union-digital-markets-act-core-platform-service-gatekeeper">delay compliance,</a> but in the end had the same two-years of notice and six-months warning from designation as everyone else.</figcaption>
</figure>
<p>A maximally aggressive legal interpretation might try to exploit ambiguity in what it means to comply and when responsibilities <em>actually</em> attach.</p>
<p>Does compliance mean providing open and fair access starting from when iOS and Safari were <em>designated</em>, or does compliance obligation only attach six months later? The DMA's text is not ironclad here:</p>
<blockquote>
<p>10: The gatekeeper shall comply with the obligations laid down in Articles 5, 6 and 7 within 6 months after a core platform service has been listed in the designation decision pursuant to paragraph 9 of this Article.</p>
<p><a href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32022R1925&amp;qid=1708824200885#003.010">DMA Article 3, Clause 10</a></p>
</blockquote>
<p>Firms looking to comply maliciously <em>might</em> try to remove troublesome features <em>just</em> before a compliance deadline, then argue they don't need to share them with competitors becuse they weren't available before the deadline set in. Apple looks set to argue, contra everyone else subject to the DMA, that the moment from which features must be made interoperable is the end of the fair-warning period, not the date of designation.</p>
<p>This appears to be Apple's play, and it stinks to high heavens.</p>
<h3 id="what's-at-risk%3F">What's At Risk? <a href="#what's-at-risk%3F">#</a></h3>
<p>Apple's change isn't merely cosmetic. In addition to immediate data loss, FruitCo's change will destroy:</p>
<ul>
<li><strong>App-like UI:</strong><br>Web apps are no longer going to look or work like apps in the task manager, systems settings, or any other surface. Homescreen web apps will be demoted to tabs in the default browser.</li>
<li><strong>Reliable storage:</strong><br>PWAs were the only exemption to Apple's (frankly silly) <a href="https://developer.mozilla.org/en-US/docs/Web/API/Storage_API/Storage_quotas_and_eviction_criteria#proactive_eviction">seven day storage eviction policy,</a> meaning the last safe harbour for anyone trying to build a serious, offline-first experience just had the rug pulled out from under them.</li>
<li><strong>Push Notifications:</strong><br>Remember how Apple gaslit web developers over Web Push for the best part of a decade? And remember how, when they finally got around to it, <a href="https://webventures.rejh.nl/blog/2024/web-push-ios-one-year/">they did a comically inept job of it?</a> And recall all the fretting and worry about how shite your Push Notifications look and work for iOS users as a result? Well, rest easy, because they're going away and you won't have to worry your pretty little head any more.</li>
<li><strong>App Icon Badging:</strong><br>A kissing cousin of Push, Icon Badging allows PWAs to ambiently notify users of new messages, something iOS native apps have been able to do for nearly 15 years.</li>
</ul>
  <!-- - **Minimally-functional Fullscreen:**<br>Games, in particular, need to maximize phone real estate. Apple hasn't gone off its hobby of gaslighting webdevs. In addition to 8 years of Push Notification excuses, Safari's totally broken [Fullscreen API,](TODO) meant PWAs were the best (if not only) way to get a working UI surface for presenting a `<canvas>` that wouldn't be dismissed at the slightest misgesture. -->
<p>Removal of one would be a crisis. Together? Apple's engineering the PWApocalypse.</p>
<p>You can't build credible mobile experiences without these features. A social network without notifications? A notetaking app that randomly loses data? Businesses will get the message worldwide: if you want to be on the homescreen and deliver services that aren't foundationally compromised, the only game in town is Apple's app store.</p>
<p>Apple understands even the most aggressive legal theories about DMA timing wouldn't support kneecapping PWAs <em>after</em> March 6th. Even if you believe (as I do) their obligations attached back in September, there's at least an argument to be tested. Cupertino's white-shoe litigators would be laughed out of court and <a href="https://ec.europa.eu/commission/presscorner/detail/en/qanda_20_2349#:~:text=In%20case%20a,systemic%20non%2Dcompliance.">Apple would get fined ridiculous amounts</a> for non-compliance after trying to deny these features to other browsers after the end of the fair-warning period. To preserve the argument for litigation, it was necessary to do the dirty deed ahead of the last plausible deadline.</p>
<h3 id="not-with-a-bang%2C-but-with-a-beta">Not With A Bang, But With A Beta <a href="#not-with-a-bang%2C-but-with-a-beta">#</a></h3>
<p>The first indication something was amiss was a conspicuous lack of APIs for PWA support in the documentation for obviously long-baking <a href="https://developer.apple.com/documentation/browserenginekit"><code>BrowserEngineKit</code></a>, released on Feb 1st alongside Apple's <a href="https://www.apple.com/newsroom/2024/01/apple-announces-changes-to-ios-safari-and-the-app-store-in-the-european-union/">peevish, deeply misleading note</a> that attempted to whitewash malicious compliance in a thin coat of security theatre.</p>
<p>Two days later, after developers inside the EU got their hands on the iOS 17.4 Beta, <a href="https://mastodon.social/@mysk/111857830238574891">word started to leak out that PWAs were broken.</a> Nothing about the change was documented in iOS Beta or Safari release notes. Developers filed <a href="https://bugs.webkit.org/show_bug.cgi?id=268643">plaintive bugs</a> and some directly pinged Apple employees, but Cupertino remained shtum. This created panic and confustion as the window for DMA compliance and the inevitable iOS 17.4 final release closed.</p>

<!-- 
beta 1: https://mastodon.social/@mysk/111857830238574891
        https://www.youtube.com/watch?v=AST12aDGf0Q&feature=youtu.be
beta 2: https://mastodon.social/@mysk/111892461059533933
beta 3: https://mastodon.social/@mysk/111926735141742246 
-->
<p><a href="https://mastodon.social/@mysk/111926735141742246">Two more betas followed,</a> but no documentation or acknowledgement of the "bug." Changes to the broken PWA behavior were introduced, but Apple failed to acknowledge the issue or confirm that it was intentional (and therefore likely to persist). After two weeks of <a href="https://open-web-advocacy.org/blog/did-apple-just-break-web-apps-in-ios17.4-beta-eu/">growing panic from web developers</a>, <a href="https://open-web-advocacy.org/blog/its-official-apple-kills-web-apps-in-the-eu/">Apple finally copped to intentionally crippling the only open, tax-free competitor to the app store.</a></p>
<p>Apple's Feb 15th statement is a masterclass in deflection and deceit. To understand why requires a deep understanding of browsers internals and how Apple's closed PWA — sorry, <em>"home screen web app"</em> — system for iOS works.<!-- [^technical-depths-of-depravity] --></p>
<p><em>TL;DR</em>? Apple's cover story is horseshit, stem to stern. Cupertino oughta be ashamed and we web developers are excused for glowing with incandescent rage over being used as pawns; first ignored, then gaslit, and finally betrayed.</p>
<h2 id="lies%2C-damned-lies%2C-and-%22still%2C-we-regret...%22">Lies, Damned Lies, and <em>"Still, we regret..."</em> <a href="#lies%2C-damned-lies%2C-and-%22still%2C-we-regret...%22">#</a></h2>
<p>I really, really hate to do this, but <a href="https://en.wikipedia.org/wiki/Brandolini%27s_law">Brandolini's Law</a> dictates that to refute Apple's <a href="https://en.wikipedia.org/wiki/On_Bullshit">bullshit,</a> I'm going to need to go through <a href="https://developer.apple.com/support/dma-and-apps-in-the-eu#8">their gibberish excuses</a> line-by-line to explain and translate. This is going to hurt me more than it hurts you.</p>
<blockquote>
<p>Q: Why don’t users in the EU have access to Home Screen web apps?</p>
</blockquote>
<p>Translation: <em>"Why did you break functionality that has been a foundational part of iOS since 2007?"</em></p>
<blockquote>
<p>To comply with the Digital Markets Act, Apple has done an enormous amount of engineering work to add new functionality and capabilities for developers and users in the European Union — including more than 600 new APIs and a wide range of developer tools.</p>
</blockquote>
<p>Translation: <em>"We're so very tired, you see. All of this litigating to avoid compliance tuckered us right out. Plus, those big meanies at the EU made us do <em>work</em>. It's all very unfair."</em></p>
<p>It goes without saying, but Apple's burden to add APIs it should have long ago provided for competing native app stores has no bearing whatsoever on its obligation to provide fair access to APIs that browser competitors need. Apple also had the same two years warning as everyone else. It knew this was coming, and special pleading at the 11th hour has big <em>"the dog ate my homework"</em> energy.</p>
<blockquote>
<p>The iOS system has traditionally provided support for Home Screen web apps by building directly on WebKit and its security architecture. That integration means Home Screen web apps are managed to align with the security and privacy model for native apps on iOS, including isolation of storage and enforcement of system prompts to access privacy impacting capabilities on a per-site basis.</p>
</blockquote>
<p>Finally! A recitation of facts.</p>
<p>Yes, iOS <em>has</em> historically forced a pretty busted model on PWAs, but iOS is not unique in providing system settings integration for PWAs. Indeed, many OSes have created the sort of integration infrastructure that Apple describes. These systems leave the question of how PWAs are actually run (and where their storage lives) to the browser that installs them, and the sky has yet to fall. Apple is trying to gussy up their mere preferences as hard requirements without any reasonable justification.</p>
<p>Here we see the outline of a strawman; Apple is insinuating that it can't provide API surface areas to allow the sorts of integrations that others already have. Why? Because it might involve writing a lot of code.</p>
<p><a href="https://support.apple.com/guide/iphone/whats-new-in-ios-17-iphfed2c4091/ios">Bless their hearts.</a></p>
<blockquote>
<p>Without this type of isolation and enforcement, malicious web apps could read data from other web apps and recapture their permissions to gain access to a user’s camera, microphone or location without a user’s consent.</p>
</blockquote>
<!-- This is the bullshit motherlode. The lair of lies. The dungeon of disingenuousness...you get the idea. -->
<p>Keeping one website from abusing permissions or improperly accessing data from another website is what browsers <em>do</em>. It's a browser's one job.</p>
<p>Correctly separating princpals is the very defintion of a "secure" browser. Every single browser vendor (<a href="https://mspoweruser.com/after-ignoring-it-for-6-weeks-apple-starts-testing-fix-for-major-apple-safari-privacy-bug-that-leaks-your-google-id/">save Apple</a>) treats subversion of the <a href="https://en.wikipedia.org/wiki/Same-origin_policy">Same Origin Policy</a> as a showstopping bug to be fixed ASAP. Unbelieveable amounts of engineering go to ensuring browsers overlay stronger sandboxing and more restrictive permissions on top of the universally weaker OS security primitives — iOS very much included.</p>
<p>Browser makers have become masters of origin separation because they run totally untrusted code from all over the internet. Security is paramount because browsers have to be paranoid. They can't just <a href="https://www.darkreading.com/cyberattacks-data-breaches/malicious-apps-millions-downloads-apple-google-app-stores">posture about how store reviews</a> will keep users safe; they have to <em>do the work</em>.</p>
<p>Good browsers separate web apps better than bad ones, which makes it particularly rich that Apple of all vendors is directly misleading this way. Apple's decade+ of under-investment in Safari ensured it was much less prepared for <a href="https://meltdownattack.com/">Spectre and Meltdown</a> and <a href="https://9to5mac.com/2021/07/14/zero-day-exploit-allowed-solarwinds-hackers-to-extract-login-information-from-ios-devices/">Solar Winds</a> that alternaties on other OSes. Competing engines had invested hundreds of engineer years into more advanced <a href="https://www.chromium.org/Home/chromium-security/site-isolation/">Site Isolation</a> technology than Apple had, and the iOS browser engine monoculture has put users at risk over and over again as a result.</p>
<p>With that as background, we can start to unpack Apple's garbled claim. What Cupertino is alluding to here is that it <em>does not want</em> to create APIs for syncing permission state that would enable the thin shim apps every PWA-supporting OS uses to make website "first class" in the OS — including iOS today. Further, it doesn't want to add APIs for attributing storage use, or clearing state, or other common management tasks.</p>
<p>If those APIs existed, Apple would still have a management question, which it's misdirections also allude to. But these aren't a problem in practice either. Every browser that would offer PWA support would <em>happily</em> sign up to security terms that required accurate synchronization of permission state between OS surfaces and web origins, in exactly the same way they'd dutifully treat cross-origin subversion as a fatal bug to be hot-fixed.</p>
<p>Apple's excusemaking is a mirror of Cupertino's years of scaremongering about alternate browser engine security, only to <a href="https://infrequently.org/2021/08/webkit-ios-deep-dive/#the-abandonware-problem">take up my proposal more-or-less wholesale when the rubber hit the road.</a> Nothing about this is monumental to build or challenging to manage; FruitCo's just hoping you don't know better. And why would you? The set of people who understand these details generously number in the low dozens.</p>
<blockquote>
<p>Browsers also could install web apps on the system without a user’s awareness and consent.</p>
</blockquote>
<p>Apple know this is a lie. They retain full control over the system APIs that are called to add icons to the homescreen, install apps, and much else. They can shim in interstitial UI if they feel like doing so. If iOS left this to Safari and did not include these sorts precautions, those are choices Apple has made and has been given two years notice to fix.</p>
<p>Cuptertino seems to be saying <em>"bad things might happen if we continued to do a shit job"</em> and one can't help but agree. However, that's no way out of the DMA's obligations.</p>
<blockquote>
<p>Addressing the complex security and privacy concerns associated with web apps using alternative browser engines would require building an entirely new integration architecture that does not currently exist in iOS and was not practical to undertake given the other demands of the DMA and the very low user adoption of Home Screen web apps.</p>
</blockquote>
<p>[CITATION NEEDED]</p>
<p>Once again, Apple is counting on the opacity of it's own suppression of the web to keep commenters from understanding the game that's afoot. Through an enervating combination of <a href="https://infrequently.org/2022/06/apple-is-not-defending-browser-engine-choice/#choices%2C-choices">strategic underinvestment</a> and coerced monoculture, Apple created (and still maintains) a huge gap in discoverability and friction for installing web apps vs. their native competition. Stacking the deck for native has taken many forms:</p>
<ul>
<li>Preventing web apps from gaining distribution in the app store <a href="https://developer.apple.com/app-store/review/guidelines/#4.2">by explicit policy.</a></li>
<li><a href="https://developer.apple.com/documentation/webkit/promoting_apps_with_smart_app_banners">"Smart Banners"</a> that let sites easily offer installation of their native counterparts.</li>
<li>Steadfast refusal to implement <a href="https://web.dev/articles/promote-install#browser-promotion">analagous features for PWAs</a> or provide competing browsers <a href="https://web.dev/learn/pwa/installation-prompt">the OS and DOM APIs they need to do so.</a></li>
<li><a href="https://www.cdc.gov/niosh/mining/content/hearingloss/installPWA.html">Burying Web App installation behind a "Share Sheet" UI</a> that users and developers complain is incredibly hard to discover, <a href="https://documentation.onesignal.com/docs/getting-your-audience-to-add-to-home-screen">forcing site owners to build clunky intersitials.</a></li>
<li>Denying competitors the necessary API access to offer better install UI and only providing the underwhelming "Share Sheet" option <a href="https://webkit.org/blog/13878/web-push-for-web-apps-on-ios-and-ipados/">last year,</a> a full <em>15 years</em> after Safari could.</li>
</ul>
<p>This campaign of suppression has been widly effective. If users don't know they can install PWAs, it's because <em>Safari never tells them</em>, and until this time last year, neither could any other browser. Developers also struggled to justify building them because Apple's repression extended to neglect of critical features, opening and maininting a substantial capability gap.</p>
<p>If PWAs use on iOS is low, that's a consequence of Apple's own actions. On every other OS where I've seen the data, not only are PWAs a success, they are growing rapidly. Perhaps that's why Apple feels a need to mislead by omission and fail to provide data to back their claim.</p>
<blockquote>
<p>And so, to comply with the DMA’s requirements, we had to remove the Home Screen web apps feature in the EU.</p>
</blockquote>
<p>Bullshit.</p>
<p>Apple's embedded argument expands to:</p>
<ul>
<li>We don't want to comply with the plain-letter language of the law.</li>
<li>To avoid that, we've come up with a legal theory of compliance that's favourable to us.</li>
<li>To comply with <em>that</em> (dubious) theory, and to avoid doing any of the work we don't want to do, we've been <em>forced</em> to bump off the one competitor we can't tax.</li>
</ul>
<p>Neat, tidy, and comprised entirely of bovine excrement.</p>
<blockquote>
<p>EU users will be able to continue accessing websites directly from their Home Screen through a bookmark with minimal impact to their functionality. We expect this change to affect a small number of users. Still, we regret any impact this change — that was made as part of the work to comply with the DMA — may have on developers of Home Screen web apps and our users.</p>
</blockquote>
<p>Translation: <em>"Because fuck you, that's why"</em></p>
<p>Apple is under no obligation by the DMA's terms to nuke PWAs.</p>
<p>Windows and Android will continue supporting PWAs just fine. Apple apparently hopes it can convince you to blame regulators for its own choices. Cupertino's counting the element of surprise plus the press's poorly developed understanding of the situation to keep blowback from snowballing into effective oppostion.</p>
<h3 id="the-point">The Point <a href="#the-point">#</a></h3>
<p>There's no possible way to justify a <a href="https://developer.apple.com/support/core-technology-fee/">"Core Technology Fee"</a> tax on an open, interoperable, standardsized platform that competitors would provide secure implementations of for free. What Apple's attempting isn't just some hand-wavey removal of a "low use" feature ([CITATION NEEDED]), it's sabotage of the only credible alternative to its app store monopoly.</p>
<figure>
<picture width="2134" height="1164">
  <source sizes="(max-width: 1200px) 70vw, 600px" srcset="https://infrequently.org/2021/08/webkit-ios-deep-dive/epic_v_apple.jpg?nf_resize=fit&amp;w=3600 2400w,
                  https://infrequently.org/2021/08/webkit-ios-deep-dive/epic_v_apple.jpg?nf_resize=fit&amp;w=2400 1600w,
                  https://infrequently.org/2021/08/webkit-ios-deep-dive/epic_v_apple.jpg?nf_resize=fit&amp;w=1800 1200w,
                  https://infrequently.org/2021/08/webkit-ios-deep-dive/epic_v_apple.jpg?nf_resize=fit&amp;w=1200   800w,
                  https://infrequently.org/2021/08/webkit-ios-deep-dive/epic_v_apple.jpg?nf_resize=fit&amp;w=900   600w,
                  https://infrequently.org/2021/08/webkit-ios-deep-dive/epic_v_apple.jpg?nf_resize=fit&amp;w=750   500w,
                  https://infrequently.org/2021/08/webkit-ios-deep-dive/epic_v_apple.jpg?nf_resize=fit&amp;w=600   400w">
<img src="https://infrequently.org/2021/08/webkit-ios-deep-dive/epic_v_apple.jpg" alt="A slide from Apple's presentation in Apple v. Epic, attempting to make the claim Epic could have just made a PWA if they didn't like the App Store terms because circa '20 Safari was <em>so</em> capable. <br><br><a href='/2021/04/progress-delayed/'>LOL.</a>" width="2134" height="1164" decoding="async" loading="lazy">
</picture>

  <figcaption>A slide from Apple's presentation in Apple v. Epic, attempting to make the claim Epic could have just made a PWA if they didn't like the App Store terms because circa '20 Safari was <em>so</em> capable. <p><a href="https://infrequently.org/2021/04/progress-delayed/">LOL.</a></p></figcaption>
</figure>
<p>Businesses will get the message: from now on, the only reliable way to get your service under the thumb, or in the notification tray, of the most valuable users in the world is to capitulate to Apple's extortionate App Store taxes.</p>
<p>If the last 15 years are anything to judge by, developers will take longer to understand what's going on, but this is an attempt to pull a <em>"Thoughts on Flash"</em> for the web. Apple's suppression of the web has <a href="https://infrequently.org/2021/04/progress-delayed/">taken many forms</a> over the past decade, but the common thread has been inaction and anti-competitive scuppering of more capable engines. With one of those pillars crumbling, the knives glint a bit more brightly. This is Apple once and for all trying to relegate web development skills to the dustbin of the desktop.</p>
<p>Not only will Apple render web apps unreliable for Safari users, FruitCo is setting up an argument to prevent competitors from ever delivering features that challenge the app store in future. And it doesn't care who it hurts along the way.</p>
<h2 id="the-mask-is-off">The Mask Is Off <a href="#the-mask-is-off">#</a></h2>
<p>This is exactly what it looks like: a single-fingered salute to the web and web developers. The removal of features that <a href="https://en.wikipedia.org/wiki/IPhone_OS_1">allowed the iPhone to exist at all.</a> The end of <a href="https://www.macstories.net/stories/before-the-app-store-the-sweet-solution-of-web-apps-and-developers-relentless-passion/">Steve Jobs' promise that you'd be able to make great apps out of HTML, CSS, and JS.</a></p>
<p>For the past few years Apple has gamely sent $1,600/hr lawyers and <a href="https://www.bloomberg.com/news/articles/2022-09-19/apple-flexes-muscle-as-quiet-power-behind-app-developer-group">astroturf lobbyists</a> to argue it didn't need to be regulated. That Apple was really on the developer's side. That even if it overstepped occasionally, it was all in the best interest of users.</p>
<p>Tell that to the millions of EU PWA users about to lose data. Tell that to the public services built on open technology. Tell it to the businesses that will fold, having sweated to deliver compelling experiences using the shite tools Apple web developers. Apple's rug pull is anti-user, anti-developer, and anti-competition.</p>
<p>Now we see the whole effort in harsh relief. A web Apple can't sandbag and degrade is one it can't abide. FruitCo's fear and loathing of an open platform it can't tax is palpable. The lies told to cover for avarice are ridiculous — literally, <a href="https://www.etymonline.com/word/ridiculous">"worthy of ridicule".</a></p>
<p>It's ok to withhold the benefit of the doubt from Safari and Apple. It's ok to be livid. These lies aren't little or white; they're directly aimed at our future. They're designed to influence the way software will be developed and delivered for decades to come.</p>
<p>If you're as peeved about this as I am, go <a href="https://open-web-advocacy.org/apple-attempts-killing-webapps/">join OWA in the fight</a> and help them create the sort of pressure in the next 10 days that might actually stop a monopolist with money on their mind.</p>


    
    
  </article>
      <!-- END_BLOCK_CONTENT -->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DOOM rendered via console.log() in a web browser (176 pts)]]></title>
            <link>https://github.com/MattCozendey/doom-console-log</link>
            <guid>39508002</guid>
            <pubDate>Mon, 26 Feb 2024 05:43:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/MattCozendey/doom-console-log">https://github.com/MattCozendey/doom-console-log</a>, See on <a href="https://news.ycombinator.com/item?id=39508002">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:MattCozendey/doom-console-log" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="YA4gLCi0FH9nK4y57WtUBorkjJGlAK8Q5tu86x-galo2K5PdBQPHWDYKlBP6z-1mmwSTgPY-NRspeYKonD3Giw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="MattCozendey/doom-console-log" data-current-org="" data-current-owner="MattCozendey" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=exxhf3mjrjWVy0aRGjLZ8EGxmyt2BCByFOQp4W0TSKNKugd%2Bwa%2B%2FtzzEqDdH33F8sNqA363xBFy36%2F8VTaKuPdZ2csJnzPCCgBJRBiVop%2B55Vyrw7LAV3GggNR7kBDbo48WNYBgqWG%2FlHqbkmWBMxXrDLRF4xrkuIq%2BawRaNimqVwjtdaGyMtqQxmQnVb%2Bs7CN14mt0Bj3BeJNM0AUVa%2F4K7v%2B%2FOAfPGxIqtKDlcsF%2B8xlwnan2M50WD91Ijl0nP3gXvX1k6Dvd4Rl7UBisXnHAhFmWuQ%2BDJX%2Bc8g9i1mZMZFdgxTiMLXfh1LQpEW6%2FB3zsO%2BJ462d4Lac3EB0Pz4Eb%2B8a8oueroTMAI8lIvNb7MaxuanRSOCzsheLGL92HhsRJsZN86OlVmFkpslUsSvJwf6uVqVKAB8bmO6CFM60nYbX%2BgTIrteIAqmgnrHYykjL5MTr4ta03eVmz9aDhuaH31%2BxoUQgpBLGMu9NQxnpm8IYzi9hQfMN0sIXd56IAHVGAu99jUoWV%2Bgm%2F6pk7V8drEaQzkcMpbn8r7Ut32--LFvl1HSsGzMcp7%2B8--zZfnlfImwnlpCZZlEGjWhQ%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=MattCozendey%2Fdoom-console-log" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/MattCozendey/doom-console-log&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="5442df8875b02708faf4cda7102bd9037094100dd586d34dce5de7f6af70ec0e" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New car buying guide: the algorithm (2021) (128 pts)]]></title>
            <link>https://github.com/kutinden/buyingacar/blob/main/README.md</link>
            <guid>39507402</guid>
            <pubDate>Mon, 26 Feb 2024 03:36:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kutinden/buyingacar/blob/main/README.md">https://github.com/kutinden/buyingacar/blob/main/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=39507402">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:kutinden/buyingacar" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="pDSJjoPKO3gZ7HKbBHvphMUxOHLK7UYr1-6Pqkdpvmg0CYSN25k8wsf5MdpNWoUzNP7lxktmAPXK-QlI20uxZw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="kutinden/buyingacar" data-current-org="" data-current-owner="kutinden" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=9t1%2B27SDw77oDFUVa3WLJNKvFmDlZtRsC9A9oN6iH%2FZZ6S%2BBF%2BTSstvMApSiebFOGc4FIt7wCshhosLN9sGIQwvmPcj02wrGlnpDMgRqEt9hvJZWjmHNbzPPoCDhxOR%2Fy2WtSl59kB1B%2F410eCfwNbu%2BF1JW%2BtveaYc5s7SewjZKUhoP%2FFf5SLfCyccjDeqNHXy%2Be7KYrQDX64Z8QGOBPrOyw2aO7ofOuTWjW7S6rZO9qB8HN4BnBq3QQY2mz%2BpEnlZFtyEWWYd6yjvmS00ZQBNav5h%2FvlUYA4TJGaLNqpe%2FjMX1JgHrCPd%2FhKeTyC0PMtOD2oMbLmnDyXbBE4EXqOCGxTSwOq635KzOiqqflX2VAdy4RbkQz5ALVjVAb3d2zYjFW95kz49TFSufp09WGkc95SP04o3caANL5wSOgzerjLD14M9%2FV95A8ynU7%2F1uLdG1S6DPQ6ecIL917ht%2FlAgZdqWaNcuXpuKnu0vRJUdS3Nmd1Q%2BhrbOKDrieBU0Jx7Mv7C%2BEijwqucy1ZBXE394NVmzKuhzfQ2rJKb1%2F7NW2VhpqbR4vdePX--XDz6cXEU%2FLE8td%2Fv--HcJtmZTuFfl%2FC7yX6z24ag%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=kutinden%2Fbuyingacar" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/kutinden/buyingacar/blob/main/README.md&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="de8d438443a6fe671795c2573945f8326bdbe63daf2c9c83b7e35dd5953f5296" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A drone that calculates coordinates using a camera and Google Maps (153 pts)]]></title>
            <link>https://twitter.com/ilaffey2/status/1759353732075294766</link>
            <guid>39507267</guid>
            <pubDate>Mon, 26 Feb 2024 03:11:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/ilaffey2/status/1759353732075294766">https://twitter.com/ilaffey2/status/1759353732075294766</a>, See on <a href="https://news.ycombinator.com/item?id=39507267">Hacker News</a></p>
Couldn't get https://twitter.com/ilaffey2/status/1759353732075294766: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[FDA approves first medication to reduce allergic reactions to multiple foods (212 pts)]]></title>
            <link>https://www.fda.gov/news-events/press-announcements/fda-approves-first-medication-help-reduce-allergic-reactions-multiple-foods-after-accidental</link>
            <guid>39507230</guid>
            <pubDate>Mon, 26 Feb 2024 03:02:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fda.gov/news-events/press-announcements/fda-approves-first-medication-help-reduce-allergic-reactions-multiple-foods-after-accidental">https://www.fda.gov/news-events/press-announcements/fda-approves-first-medication-help-reduce-allergic-reactions-multiple-foods-after-accidental</a>, See on <a href="https://news.ycombinator.com/item?id=39507230">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">

                            
                            
                            
                            
                                              
    <div>
    <dl>
        <dt>For Immediate Release:</dt>
                  <dd><time datetime="2024-02-16T16:00:00Z">February 16, 2024</time>
</dd>
                 
                
      </dl>
    </div>
 

  
      
       

  <p><a href="https://www.fda.gov/news-events/press-announcements/la-fda-aprueba-el-primer-medicamento-que-ayuda-reducir-las-reacciones-alergicas-varios-alimentos"><span>Español</span></a></p>

<p>Today, the U.S. Food and Drug Administration approved <a href="https://www.accessdata.fda.gov/drugsatfda_docs/label/2024/103976s5245lbl.pdf">Xolair</a> (omalizumab) injection for immunoglobulin E-mediated food allergy in certain adults and children 1 year or older for the reduction of allergic reactions (Type I), including reducing the risk of anaphylaxis, that may occur with accidental exposure to one or more foods. Patients who take Xolair must continue to avoid foods they are allergic to. Xolair is intended for repeated use to reduce the risk of allergic reactions and is not approved for the immediate emergency treatment of allergic reactions, including anaphylaxis.&nbsp;</p>

<p>Xolair was originally approved in 2003 for the treatment of moderate to severe persistent allergic asthma in certain patients. Xolair is also approved to treat chronic spontaneous urticaria and chronic rhinosinusitis with nasal polyps in certain patients.</p>

<p><strong>“This newly approved use for Xolair will provide a treatment option to reduce the risk of harmful allergic reactions among certain patients with IgE-mediated food allergies,” said Kelly Stone, M.D., Ph.D., associate director of the Division of Pulmonology, Allergy, and Critical Care in the FDA’s Center for Drug Evaluation and Research. “While it will not eliminate food allergies or allow patients to consume food allergens freely, its repeated use will help reduce the health impact if accidental exposure occurs.” &nbsp;</strong></p>

<p>According to the Centers for Disease Control and Prevention, almost 6% of people in the United States in 2021 had a food allergy and exposure to the particular food(s) to which they are allergic can lead to potentially life-threatening allergic reactions (i.e., anaphylaxis). There is currently no cure for food allergy. Current treatment requires strict avoidance of the food(s) the patient is allergic to, and prompt administration of epinephrine to treat anaphylaxis should accidental exposures occur. Palforzia (peanut allergen powder) is an oral immunotherapy product approved in patients 4-17 years of age for the mitigation of allergic reactions, including anaphylaxis, that may occur with accidental exposure to peanut, but its benefits are restricted to peanut allergy. Xolair is the first FDA-approved medication to reduce allergic reactions to more than one type of food after accidental exposure.</p>

<p>Xolair is a drug (in the class of drugs called monoclonal antibodies) that binds to immunoglobulin E (IgE), the antibody type that triggers allergic reactions, and blocks IgE from binding to its receptors.&nbsp;</p>

<p>Xolair’s safety and efficacy in reducing allergic reactions in subjects with food allergies was established in one multi-center, double-blind, placebo-controlled <a href="https://clinicaltrials.gov/study/NCT03881696">study</a> of 168 pediatric and adult subjects (at least 1 year of age or older) who were allergic to peanut and at least two other foods, including milk, egg, wheat, cashew, hazelnut or walnut. Researchers randomly gave subjects either Xolair or placebo treatment for 16 to 20 weeks. The primary measure of Xolair’s efficacy was the percentage of subjects who were able to eat a single dose (600 milligrams or greater) of peanut protein (equivalent to 2.5 peanuts) without moderate to severe allergic symptoms, such as moderate to severe skin, respiratory or gastrointestinal symptoms, at the end of the 16-to-20-week treatment course. Of those who received Xolair, 68% (75 of 110 subjects) were able to eat the single dose of peanut protein without moderate to severe allergic symptoms (e.g., whole body hives, persistent coughing, vomiting), compared to 6% (3 of 55 subjects) who received placebo; these results are statistically significant and clinically meaningful for subjects with food allergy. Of note, however, 17% of subjects receiving Xolair had no significant change in the amount of peanut protein tolerated (could not tolerate 100 mg or more of peanut protein). As a result, continuation of strict allergen avoidance is still necessary, despite treatment with Xolair.</p>

<p>The key secondary measures of efficacy were the percentage of subjects who were able to consume a single dose (1,000 milligrams or greater) of cashew, milk or egg protein without moderate to severe allergic symptoms at the end of the 16-to-20-week treatment course. For cashew, 42% (27 of 64 subjects) who received Xolair achieved this endpoint compared to 3% (1 of 30 subjects) who received placebo. For milk, 66% (25 of 38 subjects) who received Xolair achieved this endpoint, compared to 11% (2 of 19) who received placebo. For egg, 67% (31 of 46 subjects) who received Xolair achieved this endpoint, compared to 0% of the 19 who received placebo. As a result, Xolair treatment is approved for certain patients with one or more IgE-mediated food allergies.</p>

<p>The most common side effects of Xolair observed included injection site reactions and fever. Xolair comes with certain warnings and precautions, such as anaphylaxis, malignancy, fever, joint pain, rash, parasitic (worm) infection and abnormal laboratory tests.</p>

<p>In addition, Xolair comes with a boxed warning for anaphylaxis, which can be life threatening, based on pre-marketing and post-marketing reports of anaphylaxis that occurred after Xolair administration. Anaphylaxis has occurred after the first dose of Xolair, but also has occurred beyond one year after beginning treatment. Xolair should only be started in a healthcare setting equipped to manage anaphylaxis. For selected patients who tolerate initial Xolair treatments in a healthcare setting without anaphylaxis, self-administration (or administration by a caregiver) may be appropriate and should be discussed with a healthcare provider. &nbsp;</p>

<p>Patients should not receive Xolair if they have a history of known severe hypersensitivity to Xolair or any of its components.&nbsp;</p>

<p>Xolair is not approved for the immediate emergency treatment of allergic reactions, including anaphylaxis.&nbsp;</p>

<p>Xolair received <a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="473790c8-9758-497f-838f-5940f6b613a8" href="https://www.fda.gov/patients/fast-track-breakthrough-therapy-accelerated-approval-priority-review/priority-review" title="Priority Review">Priority Review</a> and <a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="81e20fd6-29c4-4bdf-861e-f08b5177c4a0" href="https://www.fda.gov/patients/fast-track-breakthrough-therapy-accelerated-approval-priority-review/breakthrough-therapy" title="Breakthrough Therapy">Breakthrough Therapy</a> designations for this indication.&nbsp;</p>

<p>The FDA granted the approval of Xolair to Genentech.&nbsp;</p>

  <h2>Related Information</h2>
  
  

<p>###</p>
  <div data-quickedit-field-id="node/420830/field_generic_long_text/en/full">
      <p>Boilerplate</p>
      <p>The FDA, an agency within the U.S. Department of Health and Human Services, protects the public health by assuring the safety, effectiveness, and security of human and veterinary drugs, vaccines and other biological products for human use, and medical devices. The agency also is responsible for the safety and security of our nation’s food supply, cosmetics, dietary supplements, radiation-emitting electronic products, and for regulating tobacco products.</p>
    </div>
<hr>

<div>
  <h2>Inquiries</h2>
  
    
        
         
  </div>
 
<br>

<!--BEGIN QUALTRICS WEBSITE FEEDBACK SNIPPET-->


<!--END WEBSITE FEEDBACK SNIPPET-->

<!--BEGIN QUALTRICS WEBSITE FEEDBACK SNIPPET-->



              
                                            
              
            </div></div>]]></description>
        </item>
    </channel>
</rss>