<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 18 Jun 2025 14:30:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Terpstra Keyboard (120 pts)]]></title>
            <link>http://terpstrakeyboard.com/web-app/keys.htm</link>
            <guid>44308558</guid>
            <pubDate>Wed, 18 Jun 2025 10:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://terpstrakeyboard.com/web-app/keys.htm">http://terpstrakeyboard.com/web-app/keys.htm</a>, See on <a href="https://news.ycombinator.com/item?id=44308558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="landing-page">
		<div>
		  
		  <p><img alt="" src="http://terpstrakeyboard.com/web-app/1x1.png">
          </p>

			<form id="settingsForm">
				<div>
					<div>
						<p><label>Tuning\Layout Quick Links</label>
                          

                          <label>Fundamental (Hz)</label>
                          
                       </p>

						<p><label>Right Facing Steps</label>
                          

                          <label>Up/Right Facing Steps</label>
                          
						</p>
                    </div>

					<div>
						<p><label>Hex Size (pixels)</label>
                          

                          <label>Rotation (degrees)</label>
                          
						</p>

						<p><label>Instrument</label>
                          

                          <label>
                              
                              <span>Enumerate Scale</span>
                          </label>

                          <label>
                              
                              <span>Use Spectrum Colors</span>
                          </label>

                          <label>
                              
                              <span>Blank Keys (No labels)</span>
                          </label>
						</p>
					</div>
				</div>

                <p><img alt="" src="http://terpstrakeyboard.com/web-app/1x1.png">
                </p>

				<div>
					<p><label>Scale (<a href="http://www.huygens-fokker.org/scala/scl_format.html" target="new">Scala format</a>)</label>
						
                  </p>

                  <div>
                    <p><label id="numberLabel">Steps To Equivalence Interval</label>
                      

                      
                      
					</p>

                    <p>
                        

                        <label id="note_colorsLabel">Color Layout</label>
                        
                    </p>
                  </div>
				</div>
				<br>
				
            </form>
        </div>

		<p>
          Designed by <a href="http://siementerpstra.com/" target="new">Siemen Terpstra</a> in the late ’80’s. WebApp developed by <a href="http://jamesfenn.com/" target="new">James Fenn</a> with additions and modifications by <a href="http://brandlew.com/" target="new">Brandon Lewis</a>, <a href="http://whatmusicreallyis.com/" title="What Music Really İs" target="new">Bo Constantinsen</a> and <a href="https://sites.google.com/site/wangchengu/" target="new">Chengu Wang</a>. Credits to Scott Thompson and <a href="http://ozanyarman.com/" target="new">Dr Ozan Yarman</a> for contributing samples. Current version 1.5.2 (Jan. 2015 — May 2019), released as Free/Libre and Open Source Software under <a href="https://www.gnu.org/licenses/gpl-3.0.en.html" target="new">GPL-3.0</a>. Download, fork, and get your name down here by fixing issues and implementing features via <a href="https://github.com/wcgbg/terpstrakeyboard/" target="new">GitHub</a>!
		</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I counted all of the yurts in Mongolia using machine learning (125 pts)]]></title>
            <link>https://monroeclinton.com/counting-all-yurts-in-mongolia/</link>
            <guid>44307629</guid>
            <pubDate>Wed, 18 Jun 2025 07:58:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://monroeclinton.com/counting-all-yurts-in-mongolia/">https://monroeclinton.com/counting-all-yurts-in-mongolia/</a>, See on <a href="https://news.ycombinator.com/item?id=44307629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>The <em>Fall of Civilizations</em> podcast put out a <a href="https://www.youtube.com/watch?v=YyqS9V7yHQA">6¾-hour episode</a>
on the history of the Mongol Empire, which I eagerly listened to. After finishing the episode I wondered
about contemporary Mongolian society, I wanted to learn what the lands that the Mongol Empire
exploded from are like in our current day. There are many ways to try to understand a society,
whether it be quantifying it or looking at the lived experiences within it. If you look at
data provided by the World Bank, you’ll see a country that has rapidly reduced poverty in the 21st
century, has a high economic growth rate, a healthy fertility rate, and is solidly an
upper-middle-income country. While Mongolia is a republic with a competitive party system,
<a href="https://www.worldbank.org/en/publication/worldwide-governance-indicators/interactive-data-access">Worldwide Governance Indicators</a>
from the World Bank show a government that has issues with corruption, regulatory quality, and effectiveness.</p>
<table>
<thead>
<tr>
<th>Indicator</th>
<th>Value</th>
<th>Years</th>
</tr>
</thead>
<tbody>
<tr>
<td>Population</td>
<td>3,481,145</td>
<td>2023</td>
</tr>
<tr>
<td>Fertility rate</td>
<td>2.7</td>
<td>2023</td>
</tr>
<tr>
<td>Intentional homicides (per 100,000 people)</td>
<td>6</td>
<td>2021</td>
</tr>
<tr>
<td>Individuals using the Internet (% of population)</td>
<td>1% → 83%</td>
<td>2000 → 2023</td>
</tr>
<tr>
<td>Poverty headcount ratio at $2.15 a day (2017 PPP) (% of population)</td>
<td>11.6% → 0.2%</td>
<td>2002 → 2022</td>
</tr>
<tr>
<td>Average GDP growth</td>
<td>6.62%</td>
<td>2003 → 2023</td>
</tr>
<tr>
<td>GDP per capita, PPP (current international $)</td>
<td>$4,399.4 → $18,004.9</td>
<td>2003 → 2023</td>
</tr>
</tbody>
</table>
<blockquote>
<p>(“Mongolia”)</p>
</blockquote>
<p>All of these indicators are interesting to look at, but they don’t really show what a society is
like. I feel you get much more understanding by going to a country, walking the streets, and
talking to people there. If you’re unable to do this, the next best thing is spending hours
exploring Google Maps, which I did. I opened a satellite view of Ulaanbaatar, the capital of Mongolia.
I saw new glass buildings, Soviet-designed apartment blocks (called ugsarmal), impressive government
buildings, factories, and industrial areas. But something stood out to me. Yurts, extending for kilometers
in all directions.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/yurts-1.jpg" alt="Satellite view of Ulaanbaatar containing yurts"></p>
<blockquote>
<p>Maps Data: Google © 2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<p>Naturally, I was impressed by the quantity of yurts I saw, and I was curious: just how many yurts (ger in Mongolian) are in
Mongolia and why? This set me on the path drawing bounding boxes on over 10,000 yurts to train a machine learning
model to count the rest of the yurts in the country. While I was training the model, I wondered what
the story behind these yurts are, I did a small investigation for later in this article. For now,
this is the story of counting them.</p>
<h2 id="counting-all-the-yurts-in-mongolia">Counting all the yurts in Mongolia</h2>
<p>I was unable to find a count of the yurts in Mongolia, this left me with
the task of doing it myself. Although I had never studied or worked with machine learning, I knew
through some osmosis that machine learning is well fit for this task. I created a simple plan in my
brain:</p>
<ol>
<li>Train a model to identify yurts</li>
<li>Reduce input space and parallelize searching of input space</li>
<li>Keep track of the yurts found</li>
</ol>
<h3 id="training-a-model-to-identify-yurts">Training a model to identify yurts</h3>
<p>The first thing I needed was training data, and lots of it. There’s many different options for satellite
imagery such as <a href="https://www.mapbox.com/imagery">Mapbox</a>, <a href="https://developers.google.com/maps/documentation/tile">Google Maps</a>,
and <a href="https://developers.arcgis.com/rest/basemap-styles/arcgis-imagery-webmap-get/">ArcGIS</a>. I
decided to use Google Maps since I’m already familiar with it.</p>
<p>For digital maps, many systems break the world up into a series of 256 x 256 tiles identified by X, Y, Z values. This is
referred to as tiled web maps and allows for progressively loading maps at different zoom levels and
positions. The zoom level values tend to be 0 through 20, where 0 has the least tiles and 20 the
most. The formula for calculating the number of tiles at a given zoom (z) level is: <span> $2^z * 2^z$ </span>
.
This means increasing <code>z</code> by one will increase the tile count by four times.</p>
<p>I wrote a Python script that generated tiles from a box around Ulaanbaatar and downloaded
them to a folder to use as training data. To list the tiles inside a bounding box made up of a
southwest and northeast coordinates, I used the <a href="https://mercantile.readthedocs.io/en/latest/">mercantile package</a>.</p>
<div><pre tabindex="0"><code data-lang="python"><span>for</span> <span>tile</span> <span>in</span> <span>mercantile</span><span>.</span><span>tiles</span><span>(</span><span>sw_lng</span><span>,</span> <span>sw_lat</span><span>,</span> <span>ne_lng</span><span>,</span> <span>ne_lat</span><span>,</span> <span>zooms</span><span>=</span><span>z</span><span>):</span>
    <span>download_tile</span><span>(</span><span>*</span><span>tile</span><span>)</span>
</code></pre></div><p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-1.jpeg" alt="Sample tile from Google Maps">
<img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-2.jpeg" alt="Sample tile from Google Maps">
<img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-3.jpeg" alt="Sample tile from Google Maps"></p>
<blockquote>
<p>Tiles from Google Maps, you can see yurts on the right tile. Maps Data: Google © 2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<p>I decided to start at zoom level <code>17</code> as it is the lowest zoom level that I can still identify yurts
at. Once I downloaded several hundred tiles at this zoom level, I needed a way to label the yurts on
these tiles. Labeling is the process of drawing boxes around objects in an image. The idea is to
draw these boxes manually, creating what is called annotated data, and then training a model to do
the labeling using the annotated data.
There’s an open source tool called <a href="https://labelstud.io/">Label Studio</a> that does just
this.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/label-studio.jpg" alt="Label Studio showing yurts labeled"></p>
<blockquote>
<p>Here I drew bounding boxes on the tile around the yurts.</p>
</blockquote>
<p>A couple dozen yurts later and I wanted to try and train a model based on my tiny amount of
annotated data. I had the choice between object detection (bounding boxes) and segmentation (outline
objects). Segmentation probably would be more accurate because yurts are not rectangular,
but it seemed like it would take longer to setup. I decided to go with object detection.</p>
<p>I looked at various ways to train an object detection model, my requirements were:</p>
<ul>
<li>Open source</li>
<li>As simple as possible to setup</li>
<li>Able to quickly iterate</li>
<li>Detection speed of the model is a priority due to the potentially large amount of data</li>
<li>Has good default settings around data augmentation, warmups, loss functions, etc</li>
<li>Monitor current and previous training runs to compare accuracy</li>
</ul>
<p>After doing a brief survey of the machine learning landscape, I landed on using <a href="https://docs.ultralytics.com/">YOLO11</a> by Ultralytics.
The YOLO series is a set of models that can complete computer vision tasks, and can be trained with
custom data.
In Label Studio you’re able to export to many different dataset types, YOLO being one of them. After
exporting my annotated data as a YOLO dataset, I split the dataset into training and validation data
and configured the dataset in <code>dataset.yaml</code> for YOLO to use.</p>





<div><pre tabindex="0"><code data-lang="yaml"><span>train</span><span>:</span><span> </span><span>images/train</span><span>
</span><span></span><span>val</span><span>:</span><span> </span><span>images/val</span><span>
</span><span>
</span><span></span><span>nc</span><span>:</span><span> </span><span>1</span><span>
</span><span></span><span>names</span><span>:</span><span>
</span><span>  </span>- <span>yurt</span></code></pre></div>

<p>From the ultralytics package, I used the YOLO class to use their pre-trained <code>yolo11n</code> object
detection model. Ultralytics allows easy tuning of the model with annotated data through the <code>train</code>
method of the <code>YOLO</code> class. The tuned model can be exported through <code>export</code> in various formats.</p>
<div><pre tabindex="0"><code data-lang="python"><span>from</span> <span>ultralytics</span> <span>import</span> <span>YOLO</span>

<span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"yolo11n.pt"</span><span>)</span>
<span>model</span><span>.</span><span>train</span><span>(</span>
    <span>data</span><span>=</span><span>"dataset.yaml"</span><span>,</span>
    <span>device</span><span>=</span><span>"cpu"</span><span>,</span>
<span>)</span>

<span>path</span> <span>=</span> <span>model</span><span>.</span><span>export</span><span>(</span><span>name</span><span>=</span><span>"yurt"</span><span>)</span>
</code></pre></div><p>With some testing I found my Yurt model was less than adequate, which I expected due to the tiny
amount of annotated data. I then did a couple hours of labeling, but the model would always miss
around 10-15% of the yurts in a given tile. At this point I had two options, either increase the
zoom level or gather more training data. To base my decision I decided to calculate how many tiles I
would need to search at each zoom level.</p>
<h3 id="refining-the-search-area">Refining the search area</h3>
<p>Mongolia is 1,564,116 square kilometers, using this we can calculate how many tiles at each zoom
level there are in Mongolia. The world has <span> $2^z * 2^z$ </span>
 tiles, so
on a single axis there are <span> $2^z$ </span>
 tiles. The map projection is from a sphere
a tile will represent more or less area depending on the latitude.
To find the width of the projection at a latitude for Web Mercator, we can
use this formula where <span>$R = 6,378.137$</span>
 is the radius of the equator in kilometers and <span>$\phi = 47.923107575288114$</span>
 is the
latitude of Mongolia in degrees which is converted to radians:</p>
<p><span> $$2\pi * R * \cos(\phi * \dfrac{\pi}{180}) = 26,855.3636571$$</span></p><p>We then need to divide the number of tiles on the x-axis at this location to get the width of a
tile. For the area of a tile, just square the width and divide the area of Mongolia by the area of a
single tile to get the tile count.</p>
<table>
<thead>
<tr>
<th>Zoom Level</th>
<th>Tile Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>17</td>
<td>37,258,617</td>
</tr>
<tr>
<td>18</td>
<td>149,034,469</td>
</tr>
<tr>
<td>19</td>
<td>596,137,879</td>
</tr>
<tr>
<td>20</td>
<td>2,384,551,518</td>
</tr>
</tbody>
</table>
<p>Since Mongolia is such a large country, I began to wonder if there are more ways to reduce the
amount of tiles other than just zoom level. It’s a sparsely populated country, with much of the
country being uninhabited. Also, nearly all yurts are located in urban areas, with the City of
Ulaanbaatar estimating 60% of the population lives in ger (yurt) districts (City of Ulaanbaatar 17).</p>
<p>I used <a href="https://overpass-turbo.eu/">overpass turbo</a> to do a query for all the places human settlements might be in the
country and exported this data as GeoJSON. The query returned several thousand points of interest.</p>
<pre tabindex="0"><code>[out:json][timeout:25];
{{geocodeArea:Mongolia}}-&gt;.searchArea;
(
  node[place](area.searchArea);
  node[man_made](area.searchArea);
  node[historic](area.searchArea);
);
out body;
&gt;;
out skel qt;
</code></pre><p>I wanted to know how many unique tiles for searching a 2,000 meter area around each point there are,
so I wrote a script to do this using geopandas.</p>
<div><pre tabindex="0"><code data-lang="python"><span>gdf</span> <span>=</span> <span>gpd</span><span>.</span><span>read_file</span><span>(</span><span>"./mongolia.geojson"</span><span>)</span>
<span>gdf_merc</span> <span>=</span> <span>gdf</span><span>.</span><span>to_crs</span><span>(</span><span>"EPSG:3857"</span><span>)</span>
<span>gdf_merc</span><span>[</span><span>"buffer"</span><span>]</span> <span>=</span> <span>gdf_merc</span><span>.</span><span>geometry</span><span>.</span><span>buffer</span><span>(</span><span>2000</span><span>)</span>

<span>gdf_buffer</span> <span>=</span> <span>gdf_merc</span><span>.</span><span>set_geometry</span><span>(</span><span>"buffer"</span><span>)</span><span>.</span><span>to_crs</span><span>(</span><span>"EPSG:4326"</span><span>)</span>

<span>tiles</span> <span>=</span> <span>{}</span>
<span>for</span> <span>polygon</span> <span>in</span> <span>gdf_buffer</span><span>.</span><span>geometry</span><span>:</span>
    <span>minx</span><span>,</span> <span>miny</span><span>,</span> <span>maxx</span><span>,</span> <span>maxy</span> <span>=</span> <span>polygon</span><span>.</span><span>bounds</span>

    <span>for</span> <span>tile</span> <span>in</span> <span>mercantile</span><span>.</span><span>tiles</span><span>(</span><span>minx</span><span>,</span> <span>miny</span><span>,</span> <span>maxx</span><span>,</span> <span>maxy</span><span>,</span> <span>zooms</span><span>=</span><span>Z</span><span>):</span>
        <span>tiles</span><span>[</span><span>"</span><span>{}</span><span>-</span><span>{}</span><span>-</span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>str</span><span>(</span><span>tile</span><span>.</span><span>x</span><span>),</span> <span>str</span><span>(</span><span>tile</span><span>.</span><span>y</span><span>),</span> <span>str</span><span>(</span><span>tile</span><span>.</span><span>z</span><span>))]</span> <span>=</span> <span>True</span>
</code></pre></div><table>
<thead>
<tr>
<th>Zoom Level</th>
<th>Tile Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>17</td>
<td>270,559</td>
</tr>
<tr>
<td>18</td>
<td>1,016,617</td>
</tr>
<tr>
<td>19</td>
<td>3,938,174</td>
</tr>
<tr>
<td>20</td>
<td>15,506,872</td>
</tr>
</tbody>
</table>
<h3 id="building-a-model-backend-for-labeling">Building a model backend for labeling</h3>
<p>To speed up the labeling of yurts I wanted Label Studio to use my model to label yurts.
Label Studio has the ability to integrate with a model backend,
essentially an API wrapper around a model, to request predictions. When labeling a tile, Label
Studio makes a request to this API for predictions. The API returns the bounding boxes for the tile.
I fix any mistakes the model made, and submit the tile. Every so often I retrain the model, creating
a feedback loop that improves the model with more and more annotated data.</p>





<div><pre tabindex="0"><code data-lang="python"><span>class</span> <span>YurtModel</span><span>:</span>
    <span># Initialize trained model to reuse across requests</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"best.pt"</span><span>,</span> <span>task</span><span>=</span><span>"detect"</span><span>)</span>

    <span># Task a task sent by Label Studio, and return bounding boxes of yurts</span>
    <span>def</span> <span>predict</span><span>(</span><span>self</span><span>,</span> <span>tasks</span><span>):</span>
        <span>predictions</span> <span>=</span> <span>[]</span>
        <span>for</span> <span>task</span> <span>in</span> <span>tasks</span><span>:</span>
            <span># Get the path to the file from label studio</span>
            <span>path</span> <span>=</span> <span>get_local_path</span><span>(</span>
                <span>task</span><span>[</span><span>"data"</span><span>][</span><span>"image"</span><span>],</span>
                <span>task_id</span><span>=</span><span>task</span><span>[</span><span>"id"</span><span>],</span>
            <span>)</span>

            <span>results</span> <span>=</span> <span>self</span><span>.</span><span>model</span><span>(</span><span>path</span><span>)</span>

            <span>for</span> <span>result</span> <span>in</span> <span>results</span><span>:</span>
                <span>regions</span> <span>=</span> <span>[]</span>
                <span>for</span> <span>prediction</span> <span>in</span> <span>result</span><span>.</span><span>boxes</span><span>:</span>
                    <span>xyxy</span> <span>=</span> <span>prediction</span><span>.</span><span>xyxy</span><span>[</span><span>0</span><span>]</span><span>.</span><span>tolist</span><span>()</span>
                    <span>regions</span><span>.</span><span>append</span><span>({</span>
                        <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
                        <span>"from_name"</span><span>:</span> <span>"label"</span><span>,</span>
                        <span>"to_name"</span><span>:</span> <span>"image"</span><span>,</span>
                        <span>"type"</span><span>:</span> <span>"rectanglelabels"</span><span>,</span>
                        <span>"score"</span><span>:</span> <span>prediction</span><span>.</span><span>conf</span><span>.</span><span>item</span><span>(),</span>
                        <span>"value"</span><span>:</span> <span>{</span>
                            <span>"x"</span><span>:</span> <span>xyxy</span><span>[</span><span>0</span><span>]</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"y"</span><span>:</span> <span>xyxy</span><span>[</span><span>1</span><span>]</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"width"</span><span>:</span> <span>(</span><span>xyxy</span><span>[</span><span>2</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>0</span><span>])</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"height"</span><span>:</span> <span>(</span><span>xyxy</span><span>[</span><span>3</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>1</span><span>])</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"rectanglelabels"</span><span>:</span> <span>[</span>
                                <span>"yurt"</span><span>,</span>
                            <span>],</span>
                        <span>},</span>
                    <span>})</span>

                <span>all_scores</span> <span>=</span> <span>[</span><span>region</span><span>[</span><span>"score"</span><span>]</span> <span>for</span> <span>region</span> <span>in</span> <span>regions</span> <span>if</span> <span>"score"</span> <span>in</span> <span>region</span><span>]</span>
                <span>avg_score</span> <span>=</span> <span>sum</span><span>(</span><span>all_scores</span><span>)</span> <span>/</span> <span>max</span><span>(</span><span>len</span><span>(</span><span>all_scores</span><span>),</span> <span>1</span><span>)</span>

                <span>predictions</span><span>.</span><span>append</span><span>({</span>
                    <span>"result"</span><span>:</span> <span>regions</span><span>,</span>
                    <span>"score"</span><span>:</span> <span>avg_score</span><span>,</span>
                    <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
                <span>})</span>

        <span>return</span> <span>{</span>
            <span>"results"</span><span>:</span> <span>predictions</span><span>,</span>
        <span>}</span>

<span>model</span> <span>=</span> <span>YurtModel</span><span>()</span></code></pre></div>

<p>We then need to fill out the API routes that Label Studio expects, which is a <code>/predict</code> route for
label studio to send tiles and receive predictions, a <code>/setup</code> route to do any initialization
required, and a <code>/health</code> route to do health checks on. I used <a href="https://fastapi.tiangolo.com/">FastAPI</a> to build the API and use
the <code>YurtModel</code> from above.</p>





<div><pre tabindex="0"><code data-lang="python"><span>@app</span><span>.</span><span>post</span><span>(</span><span>"/predict"</span><span>)</span>
<span>async</span> <span>def</span> <span>predict</span><span>(</span><span>request</span><span>:</span> <span>Request</span><span>):</span>
    <span>res</span> <span>=</span> <span>await</span> <span>request</span><span>.</span><span>json</span><span>()</span>
    <span>return</span> <span>model</span><span>.</span><span>predict</span><span>(</span><span>res</span><span>[</span><span>"tasks"</span><span>])</span>

<span>@app</span><span>.</span><span>post</span><span>(</span><span>"/setup"</span><span>)</span>
<span>async</span> <span>def</span> <span>setup</span><span>():</span>
    <span>return</span> <span>{</span>
        <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
    <span>}</span>

<span>@app</span><span>.</span><span>get</span><span>(</span><span>"/health"</span><span>)</span>
<span>async</span> <span>def</span> <span>health</span><span>():</span>
    <span>return</span> <span>{</span>
        <span>"status"</span><span>:</span> <span>"UP"</span><span>,</span>
        <span>"model_class"</span><span>:</span> <span>str</span><span>(</span><span>YurtModel</span><span>.</span><span>__class__</span><span>),</span>
    <span>}</span></code></pre></div>

<p>By relying on the model to find most of the yurts when labeling, I was able to rapidly create more
annotated data. I quickly built a dataset of over 10,000 yurts.</p>
<h3 id="monitoring-accuracy-of-each-model">Monitoring accuracy of each model</h3>
<h3 id="scaling-training-of-models">Scaling training of models</h3>
<p>As the size of the annotated data grew, training the models on my laptop became too slow.
I decided to use <a href="https://vast.ai/">vast.ai</a> to rent GPUs to do my training runs. To train
the models on vast.ai, I needed everything to run in Docker. I wrote a Dockerfile for the training
script, and I pushed it to <a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry">GitHub Container Registry</a>.
In vast.ai I set up authentication with the private image registry so it could pull the image I pushed up.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/docker-auth.png" alt="vast.ai Docker authentication"></p>
<blockquote>
<p>Docker authentication in vast.ai</p>
</blockquote>
<p>Here is the Dockerfile that I used to run the training script on the dataset I created.</p>
<div><pre tabindex="0"><code data-lang="shell">FROM ghcr.io/astral-sh/uv:python3.10-bookworm-slim

WORKDIR /app

<span># Copy training script, annotated data, and requirements to image</span>
COPY scripts/train_model.py .
COPY datasets ./datasets
COPY dataset.yaml .
COPY pyproject.toml .
COPY uv.lock .

<span># Needed for ...</span>
RUN apt-get update -y <span>&amp;&amp;</span> apt-get install -y libgl1-mesa-dev libglib2.0-0

<span># Install package requirements</span>
RUN uv sync --no-dev

<span># Run the training script</span>
CMD <span>[</span><span>"uv"</span>, <span>"run"</span>, <span>"python"</span>, <span>"train_model.py"</span><span>]</span>
</code></pre></div><p>In order to build and push this image to GitHub I ran:</p>
<div><pre tabindex="0"><code data-lang="shell">docker build -t ghcr.io/monroeclinton/yurt -f Dockerfile .
docker push ghcr.io/monroeclinton/yurt:latest
</code></pre></div><p>Since the training happened in ephemeral containers, I needed a way to retrieve the finished model. I
decided to upload the model to S3 after it finished training. To monitor the accuracy of the
models, I also needed the metadata associated with the runs, so I uploaded everything in the
run folder to S3.</p>





<div><pre tabindex="0"><code data-lang="python"><span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"yolo11n.pt"</span><span>)</span>

<span>model</span><span>.</span><span>train</span><span>(</span>
    <span>data</span><span>=</span><span>"dataset.yaml"</span><span>,</span>
    <span>epochs</span><span>=</span><span>1000</span><span>,</span>
    <span>patience</span><span>=</span><span>150</span><span>,</span>
    <span>imgsz</span><span>=</span><span>256</span><span>,</span>
    <span>device</span><span>=</span><span>"cuda"</span><span>,</span>
<span>)</span>

<span>path</span> <span>=</span> <span>model</span><span>.</span><span>export</span><span>(</span><span>name</span><span>=</span><span>"yurt"</span><span>)</span>
<span>train_dir</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>dirname</span><span>(</span><span>os</span><span>.</span><span>path</span><span>.</span><span>dirname</span><span>(</span><span>path</span><span>))</span>

<span>s3</span> <span>=</span> <span>boto3</span><span>.</span><span>client</span><span>(</span>
    <span>service_name</span> <span>=</span><span>"s3"</span><span>,</span>
    <span>endpoint_url</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_ENDPOINT"</span><span>],</span>
    <span>aws_access_key_id</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_ACCESS_KEY_ID"</span><span>],</span>
    <span>aws_secret_access_key</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_SECRET_ACCESS_KEY"</span><span>],</span>
    <span>region_name</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_REGION"</span><span>],</span>
<span>)</span>

<span>timestamp</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>

<span>for</span> <span>root</span><span>,</span> <span>dirs</span><span>,</span> <span>files</span> <span>in</span> <span>os</span><span>.</span><span>walk</span><span>(</span><span>train_dir</span><span>):</span>
    <span>for</span> <span>file</span> <span>in</span> <span>files</span><span>:</span>
        <span>local_path</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>root</span><span>,</span> <span>file</span><span>)</span>
        <span>s3_key</span> <span>=</span> <span>f</span><span>"models/</span><span>{</span><span>int</span><span>(</span><span>timestamp</span><span>)</span><span>}</span><span>/</span><span>{</span><span>os</span><span>.</span><span>path</span><span>.</span><span>relpath</span><span>(</span><span>local_path</span><span>,</span> <span>train_dir</span><span>)</span><span>}</span><span>"</span>
        <span>s3</span><span>.</span><span>upload_file</span><span>(</span><span>local_path</span><span>,</span> <span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_BUCKET"</span><span>],</span> <span>s3_key</span><span>)</span></code></pre></div>

<h3 id="deploying-models-and-searching-mongolia">Deploying models and searching Mongolia</h3>
<p>After dozens of training runs and greatly improving the accuracy of the model, I decided to finally
do my count of Mongolia. There were many options to run my deployment, however I made my choice based on three
criteria:</p>
<ul>
<li>Simplicity in setup and deployment</li>
<li>At least 100 instances of my model should be run</li>
<li>The bottleneck is I/O (downloading tiles), so should be deployed on many CPUs</li>
</ul>
<p>Based on these criteria, I used <a href="https://docs.docker.com/engine/swarm/">Docker Swarm</a> to orchestrate the workload.
It’s already packaged in Docker, so there’s no need to install anything else. Docker Swarm also is
fairly simple to set up, scale, and deploy services with. I rented eight servers, each with 16 vCPUs
(128 vCPUs total), and connected them over a private network.</p>
<p>I picked one server to be the <a href="https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/#manager-nodes">manager node</a>.
On this server, I ran this to initialize the swarm:</p>
<div><pre tabindex="0"><code data-lang="bash">docker swarm init --advertise-addr 10.0.0.2
</code></pre></div><p>This command sets up the swarm and prints a command to run on the worker nodes to connect them to
the manager. Each worker node joined using the token and the manager’s address:</p>
<div><pre tabindex="0"><code data-lang="bash">docker swarm join --token SWARM_TOKEN 10.0.0.2:2377
</code></pre></div><p>I deployed the container images, which I had pushed to GHCR, and pulled with
<code>--with-registry-auth</code> to allow access from the server to GHCR.
There were two images, the <code>api</code> image and the <code>worker</code> image. The API managed a list of
search areas (the areas around the points found from overpass turbo), giving
search areas to workers, and expanding the search radius by 500 meters when yurts were found.
The workers requested search areas from the API and sent back a list of yurts found within the
search areas.</p>
<h4 id="api">API</h4>
<p>I used FastAPI to build the API, in which there were two routes.</p>
<ul>
<li>GET /search-area - Workers sent a request to this route to get a search area to search.</li>
</ul>
<p>This route first checks if there are any stale areas, where a worker had requested a search area
but never finished it. The workers should send periodic health checks to the API, if this fails then
it will return the search area to a different worker after one minute.</p>
<div><pre tabindex="0"><code data-lang="python"><span>stale_area</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>SearchArea</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>SearchArea</span><span>.</span><span>searching</span> <span>==</span> <span>True</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>SearchArea</span><span>.</span><span>health_check</span> <span>&lt;</span> <span>one_minute_ago</span><span>)</span>
    <span>.</span><span>with_for_update</span><span>()</span>
    <span>.</span><span>first</span><span>()</span>
<span>)</span>
</code></pre></div><p>If there are no stale search areas, then a new point will be selected at random, and the search area
will be increased if there have been previous searches.</p>
<div><pre tabindex="0"><code data-lang="python"><span>point</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>Point</span><span>)</span>
    <span>.</span><span>options</span><span>(</span><span>joinedload</span><span>(</span><span>Point</span><span>.</span><span>search_areas</span><span>))</span>
    <span>.</span><span>filter</span><span>(</span><span>Point</span><span>.</span><span>searched</span> <span>==</span> <span>False</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>~</span><span>Point</span><span>.</span><span>search_areas</span><span>.</span><span>any</span><span>(</span><span>SearchArea</span><span>.</span><span>searched</span> <span>==</span> <span>False</span><span>))</span>
    <span>.</span><span>order_by</span><span>(</span><span>func</span><span>.</span><span>random</span><span>())</span>
    <span>.</span><span>first</span><span>()</span>
<span>)</span>

<span>if</span> <span>not</span> <span>point</span><span>:</span>
    <span>raise</span> <span>HTTPException</span><span>(</span>
        <span>status_code</span><span>=</span><span>404</span><span>,</span> <span>detail</span><span>=</span><span>"No available point to search"</span><span>)</span>

<span>previous_areas</span> <span>=</span> <span>[</span><span>sa</span> <span>for</span> <span>sa</span> <span>in</span> <span>point</span><span>.</span><span>search_areas</span><span>]</span>
<span>if</span> <span>previous_areas</span><span>:</span>
    <span>max_meters</span> <span>=</span> <span>max</span><span>(</span><span>area</span><span>.</span><span>meters</span> <span>for</span> <span>area</span> <span>in</span> <span>previous_areas</span><span>)</span>
    <span>new_meters</span> <span>=</span> <span>max_meters</span> <span>+</span> <span>500</span>
<span>else</span><span>:</span>
    <span>new_meters</span> <span>=</span> <span>500</span>
</code></pre></div><p>A <code>SearchArea</code> has a list of tiles that are inside it. Each <code>Tile</code> has as status of <code>searched</code>.
I used geopandas, as shown earlier, to generate a bounding box over the search area and create a list
of tiles. For each of these tiles, I check the database to see if they have already been created + searched.
If they haven’t then they are upserted and assigned to the search area.</p>
<div><pre tabindex="0"><code data-lang="python"><span>created_tiles</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>Tile</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span>
        <span>tuple_</span><span>(</span><span>Tile</span><span>.</span><span>x</span><span>,</span> <span>Tile</span><span>.</span><span>y</span><span>,</span> <span>Tile</span><span>.</span><span>z</span><span>)</span><span>.</span><span>in_</span><span>(</span>
            <span>[(</span><span>tile</span><span>[</span><span>"x"</span><span>],</span> <span>tile</span><span>[</span><span>"y"</span><span>],</span> <span>tile</span><span>[</span><span>"z"</span><span>])</span>
             <span>for</span> <span>tile</span> <span>in</span> <span>tiles_to_create</span><span>]</span>
        <span>)</span>
    <span>)</span>
    <span>.</span><span>all</span><span>()</span>
<span>)</span>

<span>new_area</span><span>.</span><span>tiles</span><span>.</span><span>extend</span><span>(</span><span>created_tiles</span><span>)</span>
</code></pre></div><p>The route returns the search area, containing a list of tiles to search.</p>
<ul>
<li>POST /search-area/:id - Workers sent a request containing the yurts to this route.</li>
</ul>
<p>This route inserts the yurts into the database, and marks the <code>Point</code>, <code>SearchArea</code>, and <code>Tile</code> as
searched as needed. The <code>Point</code> gets marked as searched if no yurts are found, and the <code>SearchArea</code>
and <code>Tile</code> are marked as searched.</p>
<div><pre tabindex="0"><code data-lang="python"><span>if</span> <span>len</span><span>(</span><span>yurts_to_create</span><span>)</span> <span>&gt;</span> <span>0</span><span>:</span>
    <span>stmt</span> <span>=</span> <span>insert</span><span>(</span><span>Yurt</span><span>)</span><span>.</span><span>values</span><span>(</span><span>yurts_to_create</span><span>)</span>
    <span>stmt</span> <span>=</span> <span>stmt</span><span>.</span><span>on_conflict_do_nothing</span><span>(</span>
        <span>index_elements</span><span>=</span><span>[</span><span>"longitude"</span><span>,</span> <span>"latitude"</span><span>])</span>
    <span>db</span><span>.</span><span>execute</span><span>(</span><span>stmt</span><span>)</span>
<span>else</span><span>:</span>
    <span>db</span><span>.</span><span>execute</span><span>(</span>
        <span>update</span><span>(</span><span>Point</span><span>)</span><span>.</span><span>where</span><span>(</span>
            <span>exists</span><span>()</span><span>.</span><span>where</span><span>(</span>
                <span>(</span><span>SearchArea</span><span>.</span><span>point_id</span> <span>==</span> <span>Point</span><span>.</span><span>id</span><span>)</span> <span>&amp;</span> <span>(</span><span>Point</span><span>.</span><span>id</span> <span>==</span> <span>id</span><span>)</span>
            <span>)</span>
        <span>)</span><span>.</span><span>values</span><span>(</span>
            <span>searched</span><span>=</span><span>True</span><span>,</span>
        <span>)</span>
    <span>)</span>
</code></pre></div><h4 id="worker">Worker</h4>
<p>The worker script ran in a loop until it encountered the <code>No available point to search</code> error.
This loop consisted of requesting the <code>/search-area</code> to get a list of tiles to search, downloading
each tile, then passing the tile image to the model to detect yurts. Finally, the worker sends a
list of yurts to the API.</p>
<div><pre tabindex="0"><code data-lang="python"><span>def</span> <span>find_yurts</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>):</span>
    <span>filepath</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>"tiles"</span><span>,</span> <span>"</span><span>{}</span><span>_</span><span>{}</span><span>_</span><span>{}</span><span>.jpeg"</span><span>.</span><span>format</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>))</span>

    <span>results</span> <span>=</span> <span>model</span><span>(</span><span>filepath</span><span>,</span> <span>imgsz</span><span>=</span><span>256</span><span>)</span>

    <span>yurts</span> <span>=</span> <span>[]</span>
    <span>for</span> <span>result</span> <span>in</span> <span>results</span><span>:</span>
        <span>for</span> <span>prediction</span> <span>in</span> <span>result</span><span>.</span><span>boxes</span><span>:</span>
            <span>xyxy</span> <span>=</span> <span>prediction</span><span>.</span><span>xyxy</span><span>[</span><span>0</span><span>]</span><span>.</span><span>tolist</span><span>()</span>

            <span># Find center of the bounding box</span>
            <span>pixel_x</span> <span>=</span> <span>xyxy</span><span>[</span><span>0</span><span>]</span> <span>+</span> <span>(</span><span>xyxy</span><span>[</span><span>2</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>0</span><span>])</span> <span>/</span> <span>2</span>
            <span>pixel_y</span> <span>=</span> <span>xyxy</span><span>[</span><span>1</span><span>]</span> <span>+</span> <span>(</span><span>xyxy</span><span>[</span><span>3</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>1</span><span>])</span> <span>/</span> <span>2</span>

            <span>lat</span><span>,</span> <span>lon</span> <span>=</span> <span>tile_xyz_to_lonlat</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>,</span> <span>pixel_x</span><span>,</span> <span>pixel_y</span><span>)</span>

            <span>yurts</span><span>.</span><span>append</span><span>({</span>
                <span>"latitude"</span><span>:</span> <span>lat</span><span>,</span>
                <span>"longitude"</span><span>:</span> <span>lon</span><span>,</span>
                <span>"score"</span><span>:</span> <span>prediction</span><span>.</span><span>conf</span><span>.</span><span>item</span><span>(),</span>
            <span>})</span>

    <span>return</span> <span>yurts</span>
</code></pre></div><p>I began scaling this service slowly and eventually ramped up to 120 workers running in parallel
using <code>docker service scale worker=120</code>. Each container processed its assigned tile, and if yurts
were found, posted their coordinates to the API.</p>
<h3 id="the-resulting-count">The resulting count</h3>
<p>After searching a couple million tiles I downloaded the yurt dataset, which I uploaded <a href="https://cdn.monroeclinton.com/yurts.json">here (12mb file)</a>.
In total I found 172,689 yurts with a prediction score of greater than 40%.</p>
<p>Perhaps there’s some lonesome yurts far in the Gobi Desert or the Altai Mountains I missed, so we
could add a hundred or so for those. I could have also done more like
providing image context and training on more data from smaller towns, but I only have so much time.</p>
<p>For fun I did some querying using <a href="https://postgis.net/">PostGIS</a> to find areas with high concentrations
of yurts. Generally I found places that are hotels or remote areas near mines.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/many-yurts.jpeg" alt="Many yurts"></p>
<blockquote>
<p>Maps Data: Google © 2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<h2 id="the-people-of-the-yurts">The people of the yurts</h2>
<p>Historically, yurts have been a home for the nomadic peoples of the steppe to live. As
Mongolia developed into the modern world, the usage of yurts changed with the country. For example,
I found a reference to yurts being used as makeshift schools in the early 1900s. This period was the
start of the transformation from a nomadic herder society to an urban industrial society.</p>
<blockquote>
In the rural areas, in addition to the existing 60 scribe schools, at least 49 state primary schools were established by 1917. They were largely housed in yurts and financed with state, municipal, and private funds. (Steiner-Khamsi and Stolpe 36)
</blockquote>

<p>This reflects the developmental history of Mongolia, and how people are adjusting to the modern
world. Mongolia has transitioned from a mostly nomadic herder society, to a mostly urbanized
industrial society. As people transition from one system of life to another, remnants of their old
system persist. Housing and infrastructure are expensive, so as Mongolia transformed, once nomadic
herders took their yurts to urban areas and continued living in them.</p>
<blockquote>
The 51 percent urban population reported in the 1979 census
reflected rapid migration to the cities in the 1970s. The influx of
rural people created housing problems, among them long waits for
assignment to an apartment, expansion of ger districts on the edges
of built-up areas, and pressure to invest in more housing, roads,
and other urban infrastructure. (Worden et al. 86)
</blockquote>

<p>Due to the large number of people moving to urban locations, it has been difficult for the government to
build the infrastructure needed for them. The informal settlements that grew from this difficulty
are now known as ger districts. There have been many efforts to formalize and develop these areas.
The Law on Allocation of Land to Mongolian Citizens for Ownership, passed in 2002, allowed for
existing ger district residents to formalize the land they settled, and allowed for others to
receive land from the government into the future.</p>
<p>Along with the privatization of land, the Mongolian government has been pushing for the development
of ger districts into areas with housing blocks connected to utilities. The plan for this was
published in 2014 as Ulaanbaatar 2020 Master Plan and Development Approaches for 2030. Although
progress has been slow (Choi and Enkhbat 7), they have been making progress in building housing blocks in ger
distrcts. Residents of ger districts sell or exchange their plots to developers who then build housing
blocks on them. Often this is in exchange for an apartment in the building, and often the value of the
apartment is less than the land they originally had (Choi and Enkhbat 15).</p>
<p>Based on what I’ve read about the ger districts, they have been around since at least the 1970s,
and progress on developing them has been slow. When ineffective policy results in a large chunk of
the populace generationally living in yurts on the outskirts of urban areas, it’s clear that there
is failure.
One of the most important functions of government is inspiring the citizenry to achieve greatness.
Most governments around the world fail in this, but we should all work towards it. I think a step
the Mongolian government could take for this is to analyze which policy failures have led to such
slow progress on the ger district issue.</p>
<p>The Mongolian government’s long-term vision is to provide utilities and good housing
for these areas. Although I can’t contribute anything to this vision, I wish for the best
success in this plan.
I’m glad to have learned about a country and people I used to know nothing about. Hopefully in the
future I’ll study more about Mongolia, but for now I’m off to my next project.</p>
<h3 id="further-questions">Further questions</h3>
<ul>
<li>What causes Mongolia and other countries to urbanize and industrialize?</li>
<li>Why do some Mongolians head to the cities and others stay?</li>
<li>What challenges does the Mongolian government face in developing ger districts?</li>
<li>What causes the difference in speed of development between countries?</li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><p>Choi, Mack Joong, and Urandulguun Enkhbat. “Distributional Effects of Ger Area Redevelopment in Ulaanbaatar, Mongolia.” International Journal of Urban Sciences, vol. 24, no. 1, Jan. 2020, pp. 50–68. DOI.org (Crossref), <a href="https://doi.org/10.1080/12265934.2019.1571433">https://doi.org/10.1080/12265934.2019.1571433</a>.</p>

</li>
<li><p>City of Ulaanbaatar. <em>Ulaanbaatar 2020 Master Plan and Development Approach for 2030.</em> 2014.</p>

</li>
<li>

</li>
<li><p>Steiner-Khamsi, Gita, and Ines Stolpe. <em>Educational Import: Local Encounters with Global Forces in Mongolia.</em> 1st ed, Palgrave Macmillan, 2006.</p>

</li>
<li><p>Worden, Robert L, et al. <em>Mongolia: A Country Study.</em> Washington, D.C.: Federal Research Division, Library of Congress: For sale by the Supt. of Docs., U.S. G.P.O, 1991. Pdf. Retrieved from the Library of Congress, &lt;www.loc.gov/item/90006289/&gt;.</p>

</li>
<li><p>Yang, Jeasurk, et al. <em>Poverty Mapping in Mongolia with AI-Based Ger Detection Reveals Urban Slums Persist after the COVID-19 Pandemic.</em> arXiv:2410.09522, arXiv, 12 Oct. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2410.09522">https://doi.org/10.48550/arXiv.2410.09522</a>.</p>

</li>
</ul>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MiniMax-M1 open-weight, large-scale hybrid-attention reasoning model (221 pts)]]></title>
            <link>https://github.com/MiniMax-AI/MiniMax-M1</link>
            <guid>44307290</guid>
            <pubDate>Wed, 18 Jun 2025 06:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/MiniMax-AI/MiniMax-M1">https://github.com/MiniMax-AI/MiniMax-M1</a>, See on <a href="https://news.ycombinator.com/item?id=44307290">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source srcset="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/MiniMaxLogo-Dark.png" media="(prefers-color-scheme: dark)">
      <img src="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/MiniMaxLogo-Light.png" width="60%" alt="MiniMax">
    
  </picture></themed-picture>
</div>
<hr>
<p><a href="https://www.minimax.io/" rel="nofollow">
    <img alt="Homepage" src="https://camo.githubusercontent.com/3faa1e14d767d4a75cf9ed5610309fbb6fe899cec1f03f659f57e5961de37e9b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5f486f6d65706167652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530266c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369494868746247357a4f6e68736157357250534a6f644852774f693876643364334c6e637a4c6d39795a7938784f546b354c336873615735724969423261575633516d393450534977494441674e446b774c6a4532494451784d533433496a34385a47566d637a3438633352356247552b4c6d4e736379307865325a706247773649325a6d5a6a74395043397a64486c735a5434384c32526c5a6e4d2b50484268644767675932786863334d39496d4e73637930784969426b50534a4e4d6a4d7a4c6a51314c4451774c6a6778595445334c6a55314c4445334c6a55314c4441734d5377774c544d314c6a45734d46597a4d7a45754e545a684e4441754f4449734e4441754f4449734d4377774c4445744f4445754e6a4d734d4659784e4456684d5463754e5455734d5463754e5455734d4377784c4441744d7a55754d446b734d4859334f5334774e6d45304d4334344d6977304d4334344d6977774c4441734d5330344d5334324d797777566a45354e5334304d6d45784d5334324d7977784d5334324d7977774c4441734d5377794d7934794e697777646a49344c6a5932595445334c6a55314c4445334c6a55314c4441734d4377774c444d314c6a45734d4659784e4456424e4441754f4449734e4441754f4449734d4377774c4445734d5451774c4445304e56597a4d7a45754e545a684d5463754e5455734d5463754e5455734d4377774c4441734d7a55754d537777566a49784e793431614442574e4441754f4446684e4441754f4445734e4441754f4445734d4377784c4445734f4445754e6a49734d4659794f4445754e545a684d5445754e6a4d734d5445754e6a4d734d4377784c4445744d6a4d754d6a59734d4670744d6a45314c6a6b734e6a4d754e4545304d4334344e6977304d4334344e6977774c4441734d4377304d4467754e544d734d545131566a4d774d4334344e5745784e7934314e5377784e7934314e5377774c4441734d53307a4e5334774f537777646930794e6a42684e4441754f4449734e4441754f4449734d4377774c4441744f4445754e6a4d734d46597a4e7a41754f446c684d5463754e5455734d5463754e5455734d4377774c4445744d7a55754d537777566a4d7a4d4745784d5334324d7977784d5334324d7977774c4445734d4330794d7934794e697777646a51774c6a6732595451774c6a67784c4451774c6a67784c4441734d4377774c4467784c6a59794c4442574e4441754f4446684d5463754e5455734d5463754e5455734d4377774c4445734d7a55754d537777646a49324d4745304d4334344d6977304d4334344d6977774c4441734d4377344d5334324d797777566a45304e5745784e7934314e5377784e7934314e5377774c4445734d53777a4e5334784c4442574d6a67784c6a5532595445784c6a597a4c4445784c6a597a4c4441734d4377774c44497a4c6a49324c4442574d545131515451774c6a67314c4451774c6a67314c4441734d4377774c4451304f53347a4e5377784d4451754d6a46614969382b5043397a646d632b266c6f676f57696474683d3230" data-canonical-src="https://img.shields.io/badge/_Homepage-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&amp;logoWidth=20">
  </a>
  <a href="https://arxiv.org/abs/2506.13585" rel="nofollow">
    <img alt="Paper" src="https://camo.githubusercontent.com/2604333fbd9843f0be50433505993398230120aca88af3c4dc94e4e8e3034c43/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93965f50617065722d4d696e694d61782d2d4d312d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/📖_Paper-MiniMax--M1-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://chat.minimax.io/" rel="nofollow">
    <img alt="Chat" src="https://camo.githubusercontent.com/cf1a97c2a522fe9d780765d5cc263bf289cde77cb8a51435a994aaf202e3e89f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5f4d696e694d61785f436861742d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530266c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369494868746247357a4f6e68736157357250534a6f644852774f693876643364334c6e637a4c6d39795a7938784f546b354c336873615735724969423261575633516d393450534977494441674e446b774c6a4532494451784d533433496a34385a47566d637a3438633352356247552b4c6d4e736379307865325a706247773649325a6d5a6a74395043397a64486c735a5434384c32526c5a6e4d2b50484268644767675932786863334d39496d4e73637930784969426b50534a4e4d6a4d7a4c6a51314c4451774c6a6778595445334c6a55314c4445334c6a55314c4441734d5377774c544d314c6a45734d46597a4d7a45754e545a684e4441754f4449734e4441754f4449734d4377774c4445744f4445754e6a4d734d4659784e4456684d5463754e5455734d5463754e5455734d4377784c4441744d7a55754d446b734d4859334f5334774e6d45304d4334344d6977304d4334344d6977774c4441734d5330344d5334324d797777566a45354e5334304d6d45784d5334324d7977784d5334324d7977774c4441734d5377794d7934794e697777646a49344c6a5932595445334c6a55314c4445334c6a55314c4441734d4377774c444d314c6a45734d4659784e4456424e4441754f4449734e4441754f4449734d4377774c4445734d5451774c4445304e56597a4d7a45754e545a684d5463754e5455734d5463754e5455734d4377774c4441734d7a55754d537777566a49784e793431614442574e4441754f4446684e4441754f4445734e4441754f4445734d4377784c4445734f4445754e6a49734d4659794f4445754e545a684d5445754e6a4d734d5445754e6a4d734d4377784c4445744d6a4d754d6a59734d4670744d6a45314c6a6b734e6a4d754e4545304d4334344e6977304d4334344e6977774c4441734d4377304d4467754e544d734d545131566a4d774d4334344e5745784e7934314e5377784e7934314e5377774c4441734d53307a4e5334774f537777646930794e6a42684e4441754f4449734e4441754f4449734d4377774c4441744f4445754e6a4d734d46597a4e7a41754f446c684d5463754e5455734d5463754e5455734d4377774c4445744d7a55754d537777566a4d7a4d4745784d5334324d7977784d5334324d7977774c4445734d4330794d7934794e697777646a51774c6a6732595451774c6a67784c4451774c6a67784c4441734d4377774c4467784c6a59794c4442574e4441754f4446684d5463754e5455734d5463754e5455734d4377774c4445734d7a55754d537777646a49324d4745304d4334344d6977304d4334344d6977774c4441734d4377344d5334324d797777566a45304e5745784e7934314e5377784e7934314e5377774c4445734d53777a4e5334784c4442574d6a67784c6a5532595445784c6a597a4c4445784c6a597a4c4441734d4377774c44497a4c6a49324c4442574d545131515451774c6a67314c4451774c6a67314c4441734d4377774c4451304f53347a4e5377784d4451754d6a46614969382b5043397a646d632b266c6f676f57696474683d3230" data-canonical-src="https://img.shields.io/badge/_MiniMax_Chat-FF4040?style=flat-square&amp;labelColor=2C3E50&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&amp;logoWidth=20">
  </a>
  <a href="https://www.minimax.io/platform" rel="nofollow">
    <img alt="API" src="https://camo.githubusercontent.com/033a96d7a4beb7f7872861878556c078064d874835da92f5d2ff5a0fe037c960/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29aa15f4150492d506c6174666f726d2d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/⚡_API-Platform-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-MCP">
    <img alt="MCP" src="https://camo.githubusercontent.com/a1dd6a9aad054731a18f4af8cc4f477aa43f9cf83920e1676324b97204238b5e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f9a805f4d43502d4d696e694d61785f4d43502d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🚀_MCP-MiniMax_MCP-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
</p>
<p><a href="https://huggingface.co/MiniMaxAI" rel="nofollow">
    <img alt="Hugging Face" src="https://camo.githubusercontent.com/94aa40386a1540394a4a71cdd73d62c70d4639e122e71df56f06b8a404754daa/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4975f48756767696e675f466163652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🤗_Hugging_Face-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-M1">
    <img alt="GitHub" src="https://camo.githubusercontent.com/f8147f98bcc14eb4bdbeba8461a7af5189604ad0ac027765507eaef31fb3456c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f90995f4769744875622d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🐙_GitHub-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://www.modelscope.cn/organization/MiniMax" rel="nofollow">
    <img alt="ModelScope" src="https://camo.githubusercontent.com/0715f6acf7fc905d6a032c08bd8f41b03f492fda6b3f70bf565e853643052b05/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa496efb88f5f4d6f64656c53636f70652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🤖️_ModelScope-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/LICENSE">
    <img alt="License" src="https://camo.githubusercontent.com/938892ddd7b2f59b079c0be16d5b388f03a3ff8868dbcf480033d0ae3690964f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a96efb88f5f4c6963656e73652d4170616368655f322e302d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/⚖️_License-Apache_2.0-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-01/blob/main/figures/wechat-qrcode.jpeg">
    <img alt="WeChat" src="https://camo.githubusercontent.com/e2a1862bfaa62514518f7eba6c5ad931f8b898d3ebf9867f69f29a83c0d4131d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac5f5765436861742d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/💬_WeChat-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">MiniMax-M1</h2><a id="user-content-minimax-m1" aria-label="Permalink: MiniMax-M1" href="#minimax-m1"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">1. Model Overview</h2><a id="user-content-1-model-overview" aria-label="Permalink: 1. Model Overview" href="#1-model-overview"></a></p>
<p dir="auto">We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model.
MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning
attention mechanism. The model is developed based on our previous <a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01" rel="nofollow">MiniMax-Text-01 model</a>,
which contains a total of 456 billion parameters with 45.9 billion parameters activated
per token. Consistent with MiniMax-Text-01, the M1 model natively supports a context length of 1
million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism
in MiniMax-M1 enables efficient scaling of test-time compute – For example, compared to DeepSeek
R1, M1 consumes 25% of the FLOPs at a generation length of 100K tokens. These properties make M1
particularly suitable for complex tasks that require processing long inputs and thinking extensively.
MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems ranging from
traditional mathematical reasoning to sandbox-based, real-world software engineering environments.
We develop an efficient RL scaling framework for M1 highlighting two perspectives: (1) We propose
CISPO, a novel algorithm that clips importance sampling weights instead of token updates, which
outperforms other competitive RL variants; (2) Our hybrid-attention design naturally enhances the
efficiency of RL, where we address unique challenges when scaling RL with the hybrid architecture. We
train two versions of MiniMax-M1 models with <a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k" rel="nofollow">40K</a> and
<a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k" rel="nofollow">80K</a> thinking budgets respectively. Experiments
on standard benchmarks show that our models outperform other strong open-weight models such as
the original DeepSeek-R1 and Qwen3-235B, particularly on complex software engineering, tool using,
and long context tasks. With efficient scaling of test-time compute, MiniMax-M1 serves as a strong
foundation for next-generation language model agents to reason and tackle real-world challenges.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/figures/TextBench.png"><img width="100%" src="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/TextBench.png"></a>
  <br>
  <em>Benchmark performance comparison of leading commercial and open-weight models across competition-level mathematics, coding, software engineering, agentic tool use, and long-context understanding tasks. We use the MiniMax-M1-80k model here for MiniMax-M1.</em>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2. Evaluation</h2><a id="user-content-2-evaluation" aria-label="Permalink: 2. Evaluation" href="#2-evaluation"></a></p>
<p dir="auto"><strong>Performance of MiniMax-M1 on core benchmarks.</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Category</strong></th>
<th><strong>Task</strong></th>
<th><strong>MiniMax-M1-80K</strong></th>
<th><strong>MiniMax-M1-40K</strong></th>
<th><strong>Qwen3-235B-A22B</strong></th>
<th><strong>DeepSeek-R1-0528</strong></th>
<th><strong>DeepSeek-R1</strong></th>
<th><strong>Seed-Thinking-v1.5</strong></th>
<th><strong>Claude 4 Opus</strong></th>
<th><strong>Gemini 2.5 Pro (06-05)</strong></th>
<th><strong>OpenAI-o3</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><em>Extended Thinking</em></td>
<td><em>80K</em></td>
<td><em>40K</em></td>
<td><em>32k</em></td>
<td><em>64k</em></td>
<td><em>32k</em></td>
<td><em>32k</em></td>
<td><em>64k</em></td>
<td><em>64k</em></td>
<td><em>100k</em></td>
</tr>
<tr>
<td><em><strong>Mathematics</strong></em></td>
<td>AIME 2024</td>
<td>86.0</td>
<td>83.3</td>
<td>85.7</td>
<td>91.4</td>
<td>79.8</td>
<td>86.7</td>
<td>76.0</td>
<td>92.0</td>
<td>91.6</td>
</tr>
<tr>
<td></td>
<td>AIME 2025</td>
<td>76.9</td>
<td>74.6</td>
<td>81.5</td>
<td>87.5</td>
<td>70.0</td>
<td>74.0</td>
<td>75.5</td>
<td>88.0</td>
<td>88.9</td>
</tr>
<tr>
<td></td>
<td>MATH-500</td>
<td>96.8</td>
<td>96.0</td>
<td>96.2</td>
<td>98.0</td>
<td>97.3</td>
<td>96.7</td>
<td>98.2</td>
<td>98.8</td>
<td>98.1</td>
</tr>
<tr>
<td><em><strong>General Coding</strong></em></td>
<td>LiveCodeBench <em>(24/8~25/5)</em></td>
<td>65.0</td>
<td>62.3</td>
<td>65.9</td>
<td>73.1</td>
<td>55.9</td>
<td>67.5</td>
<td>56.6</td>
<td>77.1</td>
<td>75.8</td>
</tr>
<tr>
<td></td>
<td>FullStackBench</td>
<td>68.3</td>
<td>67.6</td>
<td>62.9</td>
<td>69.4</td>
<td>70.1</td>
<td>69.9</td>
<td>70.3</td>
<td>--</td>
<td>69.3</td>
</tr>
<tr>
<td><em><strong>Reasoning &amp; Knowledge</strong></em></td>
<td>GPQA Diamond</td>
<td>70.0</td>
<td>69.2</td>
<td>71.1</td>
<td>81.0</td>
<td>71.5</td>
<td>77.3</td>
<td>79.6</td>
<td>86.4</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>HLE <em>(no tools)</em></td>
<td>8.4*</td>
<td>7.2*</td>
<td>7.6*</td>
<td>17.7*</td>
<td>8.6*</td>
<td>8.2</td>
<td>10.7</td>
<td>21.6</td>
<td>20.3</td>
</tr>
<tr>
<td></td>
<td>ZebraLogic</td>
<td>86.8</td>
<td>80.1</td>
<td>80.3</td>
<td>95.1</td>
<td>78.7</td>
<td>84.4</td>
<td>95.1</td>
<td>91.6</td>
<td>95.8</td>
</tr>
<tr>
<td></td>
<td>MMLU-Pro</td>
<td>81.1</td>
<td>80.6</td>
<td>83.0</td>
<td>85.0</td>
<td>84.0</td>
<td>87.0</td>
<td>85.0</td>
<td>86.0</td>
<td>85.0</td>
</tr>
<tr>
<td><em><strong>Software Engineering</strong></em></td>
<td>SWE-bench Verified</td>
<td>56.0</td>
<td>55.6</td>
<td>34.4</td>
<td>57.6</td>
<td>49.2</td>
<td>47.0</td>
<td>72.5</td>
<td>67.2</td>
<td>69.1</td>
</tr>
<tr>
<td><em><strong>Long Context</strong></em></td>
<td>OpenAI-MRCR <em>(128k)</em></td>
<td>73.4</td>
<td>76.1</td>
<td>27.7</td>
<td>51.5</td>
<td>35.8</td>
<td>54.3</td>
<td>48.9</td>
<td>76.8</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>OpenAI-MRCR <em>(1M)</em></td>
<td>56.2</td>
<td>58.6</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>58.8</td>
<td>--</td>
</tr>
<tr>
<td></td>
<td>LongBench-v2</td>
<td>61.5</td>
<td>61.0</td>
<td>50.1</td>
<td>52.1</td>
<td>58.3</td>
<td>52.5</td>
<td>55.6</td>
<td>65.0</td>
<td>58.8</td>
</tr>
<tr>
<td><em><strong>Agentic Tool Use</strong></em></td>
<td>TAU-bench <em>(airline)</em></td>
<td>62.0</td>
<td>60.0</td>
<td>34.7</td>
<td>53.5</td>
<td>--</td>
<td>44.0</td>
<td>59.6</td>
<td>50.0</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>TAU-bench <em>(retail)</em></td>
<td>63.5</td>
<td>67.8</td>
<td>58.6</td>
<td>63.9</td>
<td>--</td>
<td>55.7</td>
<td>81.4</td>
<td>67.0</td>
<td>73.9</td>
</tr>
<tr>
<td><em><strong>Factuality</strong></em></td>
<td>SimpleQA</td>
<td>18.5</td>
<td>17.9</td>
<td>11.0</td>
<td>27.8</td>
<td>30.1</td>
<td>12.9</td>
<td>--</td>
<td>54.0</td>
<td>49.4</td>
</tr>
<tr>
<td><em><strong>General Assistant</strong></em></td>
<td>MultiChallenge</td>
<td>44.7</td>
<td>44.7</td>
<td>40.0</td>
<td>45.0</td>
<td>40.7</td>
<td>43.0</td>
<td>45.8</td>
<td>51.8</td>
<td>56.5</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">* conducted on the text-only HLE subset.</p>
<p dir="auto">Our models are evaluated with <code>temperature=1.0</code>, <code>top_p=0.95</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">SWE-bench methodology</h3><a id="user-content-swe-bench-methodology" aria-label="Permalink: SWE-bench methodology" href="#swe-bench-methodology"></a></p>
<p dir="auto">We report results derived from the Agentless scaffold. Departing from the original pipeline, our methodology employs a two-stage localization process (without any embedding-based retrieval mechanisms): initial coarse-grained file localization followed by fine-grained localization to specific files and code elements. The values for our models are calculated on the subset of n=486 verified tasks which work on our infrastructure. The excluded 14 test cases that were incompatible with our internal infrastructure are:
<code>"astropy__astropy-7606"</code>,
<code>"astropy__astropy-8707"</code>,
<code>"astropy__astropy-8872"</code>,
<code>"django__django-10097"</code>,
<code>"matplotlib__matplotlib-20488"</code>,
<code>"psf__requests-2317"</code>,
<code>"psf__requests-2931"</code>,
<code>"psf__requests-5414"</code>,
<code>"pylint-dev__pylint-6528"</code>,
<code>"pylint-dev__pylint-7277"</code>,
<code>"sphinx-doc__sphinx-10435"</code>,
<code>"sphinx-doc__sphinx-7985"</code>,
<code>"sphinx-doc__sphinx-8269"</code>,
<code>"sphinx-doc__sphinx-8475"</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">TAU-bench methodology</h3><a id="user-content-tau-bench-methodology" aria-label="Permalink: TAU-bench methodology" href="#tau-bench-methodology"></a></p>
<p dir="auto">We evaluate TAU-Bench with GPT-4.1 as user model and without any custom tools. The maximum number of interaction steps is 40.
Our general system prompt is:</p>
<div data-snippet-clipboard-copy-content="- In each round, you need to carefully examine the tools provided to you to determine if any can be used.
- You must adhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found within these policies."><pre><code>- In each round, you need to carefully examine the tools provided to you to determine if any can be used.
- You must adhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found within these policies.
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">3. Deployment Guide</h2><a id="user-content-3-deployment-guide" aria-label="Permalink: 3. Deployment Guide" href="#3-deployment-guide"></a></p>
<p dir="auto">Download the model from HuggingFace repository:</p>
<ul dir="auto">
<li><a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k" rel="nofollow">MiniMax-M1-40k</a></li>
<li><a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k" rel="nofollow">MiniMax-M1-80k</a></li>
</ul>
<p dir="auto">For production deployment, we recommend using <a href="https://docs.vllm.ai/en/latest/" rel="nofollow">vLLM</a> to serve MiniMax-M1. vLLM provides excellent performance for serving large language models with the following features:</p>
<ul dir="auto">
<li>🔥 Outstanding service throughout performance</li>
<li>⚡ Efficient and intelligent memory management</li>
<li>📦 Powerful batch request processing capability</li>
<li>⚙️ Deeply optimized underlying performance</li>
</ul>
<p dir="auto">For detailed vLLM deployment instructions, please refer to our <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/vllm_deployment_guide.md">vLLM Deployment Guide</a>.
Alternatively, you can also deploy using Transformers directly. For detailed Transformers deployment instructions, you can see our <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/transformers_deployment_guide.md">MiniMax-M1 Transformers Deployment Guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">4. Function Calling</h2><a id="user-content-4-function-calling" aria-label="Permalink: 4. Function Calling" href="#4-function-calling"></a></p>
<p dir="auto">The MiniMax-M1 model supports function calling capabilities, enabling the model to identify when external functions need to be called and output function call parameters in a structured format. <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/function_call_guide.md">MiniMax-M1 Function Call Guide</a> provides detailed instructions on how to use the function calling feature of MiniMax-M1.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">5. Chatbot &amp; API</h2><a id="user-content-5-chatbot--api" aria-label="Permalink: 5. Chatbot &amp; API" href="#5-chatbot--api"></a></p>
<p dir="auto">For general use and evaluation, we provide a <a href="https://chat.minimax.io/" rel="nofollow">Chatbot</a> with online search capabilities and the <a href="https://www.minimax.io/platform/" rel="nofollow">online API</a> for developers. For general use and evaluation, we provide the <a href="https://github.com/MiniMax-AI/MiniMax-MCP">MiniMax MCP Server</a> with video generation, image generation, speech synthesis, and voice cloning for developers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">6. Contact Us</h2><a id="user-content-6-contact-us" aria-label="Permalink: 6. Contact Us" href="#6-contact-us"></a></p>
<p dir="auto">Contact us at <a href="mailto:model@minimax.io">model@minimax.io</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is eating our brains. MIT study: Your brain on ChatGPT (117 pts)]]></title>
            <link>https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/</link>
            <guid>44307257</guid>
            <pubDate>Wed, 18 Jun 2025 06:47:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/">https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/</a>, See on <a href="https://news.ycombinator.com/item?id=44307257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                <p>Check project's website:&nbsp;<a href="https://www.brainonllm.com/">https://www.brainonllm.com</a></p><p>With today's wide adoption of LLM products like ChatGPT from OpenAI, humans and businesses engage and use LLMs on a daily basis. Like any other tool, it carries its own set of advantages and limitations. This study focuses on finding out the <b>cognitive cost of using an LLM</b> in the educational context of writing an essay.</p><p>We assigned participants to three groups: <b>LLM group, Search Engine group, and Brain-only group, where each participant used a designated tool (or no tool in the latter) to write an essay</b>. We conducted 3 sessions with the same group assignment for each participant. In the 4th session we asked LLM group participants to use no tools (we refer to them as LLM-to-Brain), and the Brain-only group participants were asked to use LLM (Brain-to-LLM). We recruited a total of 54 participants for Sessions 1, 2, 3, and 18 participants among them completed session 4. </p><p>We used electroencephalography (EEG) to <b>record participants' brain activity </b>in order to assess their cognitive engagement and cognitive load, and to gain a deeper understanding of neural activations during the essay writing task. We performed <b>NLP analysis</b>, and we interviewed each participant after each session. We performed scoring with the help from the <b>human teachers and an AI judge</b> (a specially built AI agent).</p><p>We discovered a consistent homogeneity across the Named Entities Recognition (NERs), n-grams, ontology of topics within each group. EEG analysis presented robust evidence that LLM, Search Engine and Brain-only groups had <b>significantly different neural connectivity patterns</b>, reflecting divergent cognitive strategies. <b>Brain connectivity systematically scaled down with the amount of external support: the Brain‑only group exhibited the strongest, widest‑ranging networks, Search Engine group showed intermediate engagement, and LLM assistance elicited the weakest overall coupling.</b> In session 4, LLM-to-Brain participants showed weaker neural connectivity and under-engagement of alpha and beta networks; and the Brain-to-LLM participants demonstrated higher memory recall, and re‑engagement of widespread occipito-parietal and prefrontal nodes, likely supporting the visual processing, similar to the one frequently perceived in the Search Engine group. The reported<b> ownership </b>of LLM group's essays in the interviews <b>was low.</b> The Search Engine group had strong ownership, but lesser than the Brain-only group. The LLM group also <b>fell behind in their ability to quote</b> from the essays they wrote just minutes prior. </p><p>As the educational impact of LLM use only begins to settle with the general population, in this study we demonstrate the pressing matter of a likely <b>decrease in learning skills</b> based on the results of our study. The use of LLM had a measurable impact on participants, and while the benefits were initially apparent, as we demonstrated over the course of 4 months, the <b>LLM group's participants performed worse than their counterparts in the Brain-only group at all levels: neural, linguistic, scoring.</b></p><p>We hope this study serves as a preliminary guide to understanding the cognitive and practical impacts of AI on learning environments.</p><p>#cognitivedebt #brainonllm #yourbrainonchatgpt&nbsp;</p>
                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Make little apps for you and your friends (300 pts)]]></title>
            <link>https://pontus.granstrom.me/scrappy/</link>
            <guid>44306859</guid>
            <pubDate>Wed, 18 Jun 2025 05:16:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pontus.granstrom.me/scrappy/">https://pontus.granstrom.me/scrappy/</a>, See on <a href="https://news.ycombinator.com/item?id=44306859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Software is important to people. Most of us spend our workdays in front of computers. We use the computer in our pocket tens if not hundreds of times every day. The apps we use are almost exclusively mass-market, sold on an app-store, made for thousands if not millions of users. Or they are enterprise apps that are custom-built for hundreds of thousands of dollars.</p><p>But there isn’t really any equivalent of home-made software — apps made lovingly by you for your friends and family. Apps that aren’t polished or flashy, but are made to <em>your</em> <em>preference</em> and help you with <em>your particular needs.</em></p><p>We’re John and Pontus, and we’ve been exploring the potential of home-made software together.</p><p>We ended up creating a research prototype that we call <strong>Scrappy</strong> — a tool for making <strong>scrappy apps for just you and your friends.</strong> First and foremost, we aim to contribute a <em>vision</em> of what home-made software could be like. We want to make this vision as concrete as we can, by sharing a working tool and examples of apps made in it. Scrappy, in its current state, is a prototype, not a robust tool, but we hope it paints the picture we carry in our heads — of software as something that can be creative, personal, expressive. Made by anyone, for themselves and their loved ones.</p><h2 id="what-is-scrappy">What is Scrappy?</h2><p>It may not be clear what “a scrappy app for you and your friends” means. What kind of apps are these? Let us paint a picture with a few examples. (We call them “<strong>Scrapps</strong>”.)</p><div><div><p><strong>Arithmetic practice for a kid in elementary school.</strong> When outgrown, the Scrapp can be extended with harder problems.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/math_practice.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/math_practice.mp4" poster="https://pontus.granstrom.me/scrappy/examples/math_practice.jpg" controls=""></video></div><div><p><strong>Attendee counter for a local event.</strong> The counter’s state is shared, so the Scrapp can be used to let people in and out at multiple entrances.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/attendee_counter.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/attendee_counter.mp4" poster="https://pontus.granstrom.me/scrappy/examples/attendee_counter.jpg" controls=""></video></div></div><div><div><p><strong>Meeting cost clock,</strong> to help meetings stay on track. A Scrapp like this can be put together in 15 minutes and shared with coworkers right away.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/meeting_cost_clock.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/meeting_cost_clock.mp4" poster="https://pontus.granstrom.me/scrappy/examples/meeting_cost_clock.jpg" controls=""></video></div><div><p><strong>Weekly chore tracker.</strong> Let roommates flexibly swap weeks, while making sure to track whose up next, to keep things fair.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/chores.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/chores.mp4" poster="https://pontus.granstrom.me/scrappy/examples/chores.jpg" controls=""></video></div></div><h2 id="what-is-it-like-to-make-an-app-in-scrappy">What is it like to make an app in Scrappy?</h2><p>Scrappy is an infinite canvas of interactive objects. The workflow is similar to an app such as Figma, Miro, or Google Slides — except you can attach behaviors to the objects.</p><p>You drag objects out on the canvas — a button, a textfield, a few labels. Select an object, and you can modify its attribute in an inspector panel. Certain objects, like buttons, has attributes like “when clicked” that contain javascript code. When the button is clicked, that code is run — maybe it records the contents of the textfield to a label that acts as a log. You build your app step by step: tweaking and rearranging the objects, and attaching a little bit of code to them.</p><p>There’s no better way to get a feeling for an authoring environment than to see someone use it in action. In the following videos, I’m making an attendee counter for an event.</p><p><strong>The basics.</strong> I start out by adding a number field to track the number of attendees, and two buttons for recording people entering and exiting the venue.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/basics.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/basics.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>Reactive formulas.</strong> Next, I add a field for the venue’s capacity, and a warning when too many people have been let in.
I use a reactive formula to control the visibility of the warning and the border color of the field.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/reactive-formulas.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/reactive-formulas.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>A shared, persistent world.</strong> Without any extra work, Scrappy apps are multiplayer.
App state is persisted and synced, like users expect from online documents like Google Sheets or Figma.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/shared-persistent-world.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/shared-persistent-world.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>The app is always live.</strong> There’s no distinction between editing and running. I can edit the app while a friend is using it.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/liveness.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/liveness.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>Selective sharing.</strong> I make a variant of the app that’s limited to only entering and exiting people. This is done by putting a part of the app in a frame, and sharing only that frame. The limited version is still linked to the main app.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/selective-sharing.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/selective-sharing.mp4" type="video/mp4">Your browser cannot play this video.</video><div><div><p><strong>Visible, tangible data.</strong> Here’s what the <a href="https://www.notion.so/Scrappy-Make-Little-Apps-for-You-and-Your-Friends-1ba27a2e3bb6806fb782cf2ff7e5764e?pvs=21">Meeting Cost Clock app</a> shown above looks like when zoomed out, revealing a common pattern in Scrapps.</p><p>Outside the shared frame are a bunch of fields used to compute the cost of the meeting. This lets me see the data while I’m working on the Scrapp, just like in a spreadsheet, which is very helpful for debugging — and it makes future tweaking or remixing easier.</p></div><p><img src="https://pontus.granstrom.me/scrappy/walkthrough/meeting_cost_clock_internals.png" alt="Meeting cost clock internals"></p></div><h2 id="why-make-scrappy">Why make Scrappy?</h2><p>This project is driven by a desire to reimagine software creation and use. As part of a growing movement variously termed “<a href="https://hackernoon.com/big-and-small-computing-73dc49901b9a">small computing</a>,” “<a href="https://dubroy.com/blog/casual-programming/">casual programming</a>”, and “<a href="https://maggieappleton.com/home-cooked-software">home-cooked software</a>” we want to <a href="https://www.inkandswitch.com/end-user-programming/">emancipate end-users</a> — to “empower people to express themselves without requiring them to be heavy-duty programmers,” to “liberate the programming of computers from the priesthood to the layperson”, as Bill Atkinson worded it. We want to shift the world away from mass-market, industrially-produced software toward more <a href="https://x.com/davidhoang/status/1802140453292372272">personal, even disposable,</a> tools that are designed for and readily <a href="https://malleable.systems/">modified and adapted</a> to <a href="https://gwern.net/doc/technology/2004-03-30-shirky-situatedsoftware.html">specific social contexts</a>. Above all, we want to foster a sense of agency and to ultimately contribute to “<a href="https://x.com/cwervo/status/1808578326409457834">redistributing the means of software production</a>”.</p><p>We were inspired by the simplicity of tools like <a href="https://www.notion.so/">Notion</a>, <a href="https://www.tldraw.com/">tldraw</a>, and <a href="https://mmm.page/">mmm.page</a>, but wanted to empower people with richer interactivity and programming capabilities. However, knowing the strengths and limitations of the standard visual programming paradigms of blocks (e.g. <a href="https://scratch.mit.edu/">Scratch</a>, <a href="https://developers.google.com/blockly">Blockly</a>) and nodes-and-wires (e.g. <a href="https://cycling74.com/products/max">Max/MSP</a>, <a href="https://nodered.org/">Node-RED</a>, <a href="https://natto.dev/">natto</a>, <a href="https://www.holograph.so/">Holograph</a>), we deliberately wanted to go down a different path. Instead, we drew direct inspiration from “media with scripting” environments, both classic systems like <a href="https://en.wikipedia.org/wiki/HyperCard">HyperCard</a>, <a href="https://en.wikipedia.org/wiki/Visual_Basic_(classic)">Visual Basic</a>, and <a href="https://en.wikipedia.org/wiki/Adobe_Director">Macromedia Director</a>, as well as contemporary platforms like <a href="https://www.notion.so/Project-Concept-Definition-618cd9f0e26944f3b1ee4222c1db92c9?pvs=21">Dynamicland</a> and <a href="https://www.minecraft.net/">Minecraft</a>, where the “media with scripting” exist in a shared online world.</p><p>Overall, our target user experience was that of a productivity tool, specifically a canvas-based tool (e.g. <a href="https://www.figma.com/">Figma</a>, <a href="https://miro.com/">Miro</a>, and <a href="https://www.tldraw.com/">tldraw</a>)—rather than programming environments (e.g. <a href="https://squeak.org/">Squeak/Smalltalk</a>, modern IDEs) and website and app builders (e.g. <a href="https://www.squarespace.com/">Squarespace</a>, <a href="https://mmm.page/">mmm.page</a>, <a href="https://bubble.io/">Bubble</a>). And we also wanted that kind of modern “share link”-based real-time collaboration popularized by <a href="https://docs.google.com/">Google Docs</a> and <a href="https://www.figma.com/">Figma</a>.</p><p>Finally, while we acknowledge the capabilities of AI-centric systems that leverage LLMs for code generation (e.g., <a href="https://lovable.dev/">Lovable</a>, <a href="http://bolt.new/">bolt.new</a>, and <a href="https://computer.tldraw.com/">tldraw computer</a>), we deliberately chose to focus our design on direct manipulation and user control.</p><h2 id="who-is-scrappy-for">Who is Scrappy for?</h2><p>As we were prototyping, it wasn’t clear who the ideal user for Scrappy was. We left this open, to see what we’d learn from building the system. Eventually, a few potential personas revealed themselves.</p><ul><li><strong>The process optimizer.</strong> In business environments, there’s always some improvement that can be made using software. But the person who sees the process inefficiency likely can’t make software themselves, and involving a professional programmer is expensive. So what usually happens is they make what improvements they can using tools they are familiar with, such as Excel. Here Scrappy could be a more powerful and flexible Excel, while retaining familiarity and ease of use.</li><li><strong>Teachers and students</strong>. Teaching programming requires teaching a multitude of inessential technical details: how to use the command line, how file systems work, how to set up the environment, dependency management, version control, servers and clients, and on and on. With Scrappy, you can just create a button, write a line of code and click the button to run the code.</li><li><strong>Ourselves!</strong> We are professional programmers who don’t like programming. Why? Because of the all the aforementioned complexity that adds friction to what could be so much simpler. When making mass-market apps, we know we have to deal with that complexity, but when working on a fun hobby project?! Give us a break. Scrappy is that kind of break.</li><li><strong>The DIYer.</strong> People like to customize their house, grow their own vegetables, sow their own clothes, build their own furniture. Scrappy is where a DIY-inclined person makes their own little apps for themselves and their friends.</li></ul><p>As Scrappy solidified, we wanted to focus on one of these personas. There’s a pull toward business use cases, since businesses are the most willing to pay for a product, but we believe the incentives there would lead us too close to existing products like <a href="https://retool.com/">Retool</a> or <a href="https://livecode.com/">LiveCode</a>. The teaching use case is compelling, but we believe it needs a better coding experience (discoverability, better error messages, debugger) which was out of scope for us (for now). We are itching to make stuff for ourselves in Scrappy (and we are strong believers in dogfooding), but most of our projects required features that would balloon the scope.</p><p>The DIYer making home-made software is the least served by existing tools, and fits our vision of democratized computing the best. We decided this is where we could make the biggest contribution (the <a href="https://en.wikipedia.org/wiki/Blue_Ocean_Strategy">blue ocean strategy</a>), and decided to make the DIYer our target persona.</p><p>Ideally, Scrappy would let anyone with basic computer literacy make a simple app and learn from there. This is not quite the case yet — some JavaScript knowledge is required. So today, the person making Scrapps from scratch is a <strong>programmer DIYer.</strong> But when a Scrapp is shared with friends, those friends can use it and remix it without needing programming experience.</p><h2 id="what-should-i-make-in-scrappy">What should I make in Scrappy?</h2><p>Home-made, scrappy apps don’t really exist today, so most people (including us!) are not used to coming up with ideas for them. When faced with a problem that would make a great Scrapp, instead our minds go to “maybe there’s an app for that”, searching the web for one, giving up if we cannot find a good one. To start coming up with good uses for Scrappy requires a shift to a home-made mindset.</p><p>To help you build that mindset, here is an assortment of ideas for Scrapps (some of which are not feasible in the current prototype of Scrappy, but should be).</p><div><div><ul><li>Custom flashcards</li><li>Meeting agenda manager</li><li>Day clock for person with dementia</li><li>Online workshop facilitation</li><li>Consulting time tracker</li><li>Point-based voting for a board</li><li>Receipt generator</li><li>Simple word game</li><li>School grade calculator</li><li>Interactive visual recipe</li><li>Social quiz game</li></ul></div><div><ul><li>Typing tutor</li><li>Lyric writing aids (synonyms, rhymes)</li><li>Board game helper</li><li>Wedding RSVP + seating arrangement</li><li>Dynamic opening hours display</li><li>Family bulletin board</li><li>Group travel planner</li><li>Chore → allowance calculator</li><li>Chess clock productivity timer</li></ul></div></div><p>What makes a problem well-suited for Scrappy? Here are some things they have in common:</p><ul><li><strong>Shared with friends.</strong> While a Scrapp can be for just yourself, Scrappy really shines with multiples users, leveraging the shared, persistent world. Some problems that would need setting up a backend server can be built in minutes in Scrappy.</li><li><strong>Needs tweaking-as-you-go.</strong> Life changes, and so does requirements. In Scrappy, you can edit the app at any time — even while your friend is using it. No building, no deploying, no fuss.</li><li><strong>A sprinkle of computation.</strong> Scrappy shines when thought of as a shared document first, with a little bit of computation added on top. For complex systems with a lot of moving parts, we recommend reaching for traditional software engineering tools.</li><li><strong>Minimal friction.</strong> We all let out a groan inside whenever we are hit with “create an account to continue”. This “account friction” may not be much, but it multiplies when sharing with a group of people — there’s always going to be someone for whom the friction is too much. Scrapps don’t have this problem: just click the link.</li><li><strong>Small number of trusted users.</strong> Scrappy assumes you trust the people you share a Scrapp with, which removes a lot of friction, but if you need to control access and permissions, look elsewhere.</li><li><strong>Not mission-critical.</strong> If you need guaranteed correctness or perfect control over details, don’t reach for Scrappy. Those qualities are what you pay expert engineers for.</li></ul><h2 id="scrappy-vs-mass-market-apps">Scrappy vs mass-market apps</h2><p>When faced with a “scrappy” problem — something small that would benefit from a computer — most people will think “maybe there’s an app for that”, followed by searching an app store or the Internet to look for one.</p><p>If there is no app for that, or there’s no good one, you could make your own in Scrappy. We hope you do! But often there <em>is</em> an app for that. If there is, it will probably be more polished than anything you can make in Scrappy. In this case, there are still reasons to consider using making your own Scrapp:</p><ul><li><strong>Does exactly what you need.</strong> And only what you need. Nothing more, nothing less.</li><li><strong>Home-made with love.</strong> Scrapps are made by you for your friends. A home-knitted sweater will always mean more to you than a store-bought one.</li><li><strong>Fun and playful.</strong> In Scrappy, it’s easy to play around. Tweak the colors, make a cute layout, add little inside jokes.</li><li><strong>Remixable.</strong> Easy to share with others and modify to suit your needs.</li><li><strong>Collaborative by default.</strong> All Scrappy apps are multiplayer, like a Google Doc is. You can even edit them while they are being used by someone else!</li><li><strong>No accounts and signups.</strong> If you share a Scrappy app with someone, they can start using it right away — no tedious sign-up flows stopping your friends or family from joining in.</li><li><strong>You own your data.</strong> The data is stored locally and will only be used for nefarious purposes if its creator (you) wants to!</li></ul><h2 id="scrappy-vs-ai-written-apps">Scrappy vs AI-written apps</h2><p>What about asking an LLM to make a custom, home-made app?</p><p>LLMs are getting better and better, and while they are far from able to make a full-fledged app without a lot of help from a software engineer, they can make small apps pretty reliably.</p><p>So if I can ask ChatGPT or Claude to make an app, why would I use Scrappy?</p><ul><li><strong>Scrappy is understandable.</strong> Using an LLM means going from an English prompt to pages of React code, which is too big a leap for non-programmers. They end up having to rely on the LLM to make changes, and are left helpless if the LLM doesn’t do the right thing. In contrast, Scrappy’s objects-on-a-canvas model is easy to understand, more humane, and acts a shared substrate where user and AI can collaborate on equal footing. And because it is less overwhelming, it’s more likely the user will pick up some programming skills.</li><li><strong>Scrappy is collaborative.</strong> All Scrappy apps are little shared worlds, persistent and with live updating — all for free. LLMs are mainly useful for creating static front-end-only web apps. And in Scrappy, apps can be edited by multiple users in realtime, whereas AI workflows are mostly “type, then wait” with little room for collaboration between humans.</li><li><strong>Scrappy is more fun!</strong> While typing a few sentences of English and seeing a full app appear out of nowhere still feels like magic, it quickly grows old when you’re waiting for minutes only to see the LLM misunderstood you again. In Scrappy, there is joy in tweaking things or remixing something. A spark of “ooh I want it to do this” and it’s only a few clicks and keystrokes away. A sense of creative ownership. And you can edit it together with friends!</li></ul><h2 id="scrappy-vs-hypercard-and-its-successors">Scrappy vs HyperCard (and its successors)</h2><p><a href="https://hypercard.org/">HyperCard</a> was popular among Macintosh users in the early 90s, and is often held as an exemplar of enabling home-made software and end-user programming. Decades later, there have been a number of successors to HyperCard, both commercial (<a href="https://www.mackiev.com/hyperstudio/">HyperStudio</a>, <a href="https://en.wikipedia.org/wiki/SuperCard">SuperCard</a>, <a href="https://livecode.com/">LiveCode</a>) and non-commercial (<a href="https://beyondloom.com/decker/">Decker</a> and <a href="https://hypervariety.com/WildCard/">WildCard</a>, among a number of open-source remakes, most of which are abandonware). Most of these have been quite literal replicas of HyperCard, driven by nostalgia, down to the black-and-white graphics. None have been as successful as the original.</p><p>We wanted to create something in the spirit of HyperCard, rather than recreate HyperCard. Scrappy is different from HyperCard and its direct descendants in a few key ways:</p><ul><li><strong>Designed for the Internet.</strong> Scrappy apps are easily shareable online with a simple link, whereas using HyperCard and most of its descendants is like being trapped in a virtual machine.</li><li><strong>A shared world.</strong> HyperCard stacks could be shared as a file with other users. Scrappy takes this to the next level by letting users edit and use apps at the same time.</li><li><strong>Modern UI conventions.</strong> Scrappy apps live on a high-resolution infinite canvas, with selections, copying, panning and zooming, frames for grouping, etc.</li><li><strong>Uses JavaScript for scripting.</strong> HyperCard and a number of its descendants use programming languages that aren’t in common use. JavaScript is the most common programming language in the world, is native to the Web and works well for a dynamic environment such as Scrappy.</li><li><strong>A larger palette of interactive objects.</strong> Many HyperCard-likes only support a few elements like buttons, text fields, and images. Scrappy supports more UI elements like sliders and timers, but also data types beyond strings: numbers, dates, and compound JSON objects.</li><li><strong>Reactive formulas, like a spreadsheet.</strong> The idea of “this value changes when that value changes” is familiar to many, and can be a stepping stone toward event-based programming, where the user has to think about state.</li></ul><h2 id="future-directions">Future directions</h2><p>With our prototype, we think that we’ve been successful at proving the ideas and design principles that we started with. But there’s a lot more work to do. The number of Scrapps that can be built in a way that feel “Scrappy native” is still low. Much of the time, existing knowledge of JavaScript is required. To improve this, we need to continue work in both “lowering the floor” and “raising the ceiling”.</p><p>Lowering the floor means making things more friendly and approachable for people with little or no programming experience. For example:</p><ul><li><strong>Improve code discoverability.</strong> We’ve made coding easier by presenting the names of objects visually on the screen, and listing their methods in the properties panel. But there’s a ton more than we can do. You should be able to click on objects to discover their methods and insert them in the code. Available names should auto-complete so you don’t have remember syntax and do as much typing.</li><li><strong>Improve debugging.</strong> You should be able to visualize relationships between objects, perhaps as arrows showing which objects read or modify other objects. Error messages should be better worded and show more information about what went wrong. You should be able pause and rewind execution. All of this while live collaborating on the app with a friend.</li><li><strong>Leverage AI.</strong> <a href="https://www.notion.so/Scrappy-Make-Little-Apps-for-You-and-Your-Friends-1ba27a2e3bb6806fb782cf2ff7e5764e?pvs=21">As we mentioned earlier</a>, we don’t believe in having an LLM make an entire app, but we are interested in having it act as an assistant, directed by the user. Maybe you’d click on the canvas and ask the AI to “make start and stop buttons here”, or go to a text label’s “when changed” handler and ask the AI to write code to “show an error message if using non-english characters”.</li><li><strong>Make it even easier to share and remix</strong>. It’s easier to learn by inspecting and tweaking other people’ work than it is to start from a blank canvas. We imagine a public gallery where users can publish their creations, and other users can adopt and customize them for their own needs and preferences.</li><li><strong>Make Scrapps work well on phones/tablets.</strong> A hand-sized touchscreen is too small for editing Scrapps comfortably, but using them on phones should work well — this is not currently the case. The infinite canvas paradigm means that objects have fixed positions, which is a way simpler mental model layout rules (like in CSS), but means designs aren’t responsive to screen size. However, drag-and-drop web page design tools like <a href="http://mmm.page/">mmm.page</a> and <a href="https://www.squarespace.com/">Squarespace</a> show a way to handle this: simply show safe areas for mobile to the user.</li></ul><p>Raising the ceiling means adding functionality and expressive power, letting users create more things with less effort. For example:</p><ul><li><strong>Add support for collections</strong>. Currently, you can edit and store strings, numbers, dates, and JSON data, but you cannot store lists of them or make an editable tables, like in a spreadsheet. We also don’t have any kind of layout containers, like lists, grids, or stacks. Adding this would let authors express more things visually, and they wouldn’t have to resort to JavaScript knowledge and hidden state.</li><li><strong>Instanced frames.</strong> Frames let you <a href="https://www.notion.so/Submission-writeup-1a227a2e3bb680c5be5ddc988af65ce2?pvs=21">selectively share</a> parts of your app, but that frame is fully synced in real-time across users. This is desirable in some cases and undesirable in others. For example, when sharing a form, each user should only see and edit their own copy. You should be able to share instances of the form, that still collects all the data in one place. Another example is a board game helper where there’s some hidden information and users should see some shared UI and some UI only visible to them.</li><li><strong>Tools for data processing.</strong> We’ve found ourselves wanting to use Scrappy to process tabular data. Things like: do the same operations to all rows, filter the table based on some criteria, etc. This can be done in JavaScript, but there should be a Scrappy-native way of doing this, where the data is shown.</li><li><strong>Better support for reuse.</strong> Currently, if you want to repeat an object or set of objects in your Scrapp, you have to manually edit them one by one, or write code to manage them. Instead, you should be able to define a reusable component and make instances that stay linked to the main component. Figma has this, PowerPoint has slide masters, HyperCard has card backgrounds, all to this effect. Further, these components could be shared across projects, or even with other users.</li><li><strong>Allow extending Scrappy.</strong> Some capabilities will be out of reach using Scrappy’s primitives. Currently, we are the only ones able to add new objects, but we’d want to open this up to more people. We expect this would require programming and web expertise, so it wouldn’t be something for a traditional engineer, not the typical Scrappy user.</li><li><strong>Clean up the conceptual model.</strong> Currently, some of the objects store data values, and some support event handlers like “when clicked” and “when changed” handlers. The current implementation is a bit arbitrary about this, which is not only confusing but also limiting. Common behaviors like editing, clicking, and storing data should be made more consistent and freely “mixable” — like the <a href="https://en.wikipedia.org/wiki/Entity_component_system">entity component systems</a> of game engines like <a href="https://unity.com/">Unity</a>.</li></ul><h2 id="conclusion">Conclusion</h2><p>We believe computers should work for people, and dream of a future where computing, like cooking or word processing, is available to everyone. Where you can solve your own small, unique problems with small, unique apps. Where you don’t just rely on mass-market apps made by expert programmers. Where you share home-made little apps with family and friends.</p><p>Scrappy is our contribution to this dream. Each Scrapp is a live, persistent world, easily shared and remixed, closer to familiar productivity apps than alien developer tools. Like any vision, ours is incomplete, but we’ve grounded our explorations in a working prototype with examples of apps.</p><a href="https://scrappy.jrcpl.us/">Try Scrappy! (desktop only)</a><p>We hope Scrappy will inspire you to further chase this particular windmill. If it does, please let us know!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Lstr – A modern, interactive tree command written in Rust (169 pts)]]></title>
            <link>https://github.com/bgreenwell/lstr</link>
            <guid>44306041</guid>
            <pubDate>Wed, 18 Jun 2025 02:07:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bgreenwell/lstr">https://github.com/bgreenwell/lstr</a>, See on <a href="https://news.ycombinator.com/item?id=44306041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">lstr</h2><a id="user-content-lstr" aria-label="Permalink: lstr" href="#lstr"></a></p>
<p dir="auto"><a href="https://github.com/bgreenwell/lstr/actions"><img src="https://github.com/bgreenwell/lstr/actions/workflows/ci.yml/badge.svg" alt="Build Status"></a>
<a href="https://crates.io/crates/lstr" rel="nofollow"><img src="https://camo.githubusercontent.com/b6682dc3f177211760ed9636185d7aafc140643f821ab6e95f05d8052f37919d/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f6c7374722e737667" alt="Latest Version" data-canonical-src="https://img.shields.io/crates/v/lstr.svg"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a></p>
<p dir="auto">A blazingly fast, minimalist directory tree viewer, written in Rust. Inspired by the command line program <a href="https://github.com/Old-Man-Programmer/tree">tree</a>, with a powerful interactive mode.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/bgreenwell/lstr/blob/main/assets/lstr-demo.gif"><img src="https://github.com/bgreenwell/lstr/raw/main/assets/lstr-demo.gif" alt="" data-animated-image=""></a>
<em>An interactive overview of <code>lstr</code>'s project structure... using <code>lstr</code>.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Philosophy</h2><a id="user-content-philosophy" aria-label="Permalink: Philosophy" href="#philosophy"></a></p>
<ul dir="auto">
<li><strong>Fast:</strong> Runs directory scans in parallel by default to maximize speed on modern hardware.</li>
<li><strong>Minimalist:</strong> Provides essential features without the bloat. The core experience is clean and uncluttered.</li>
<li><strong>Interactive:</strong> An optional TUI mode for fluid, keyboard-driven exploration.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>High-performance:</strong> Scans directories in parallel to be as fast as possible.</li>
<li><strong>Classic and interactive modes:</strong> Use <code>lstr</code> for a classic <code>tree</code>-like view, or launch <code>lstr interactive</code> for a fully interactive TUI.</li>
<li><strong>Rich information display (optional):</strong>
<ul dir="auto">
<li>Display file-specific icons with <code>--icons</code> (requires a Nerd Font).</li>
<li>Show file permissions with <code>-p</code>.</li>
<li>Show file sizes with <code>-s</code>.</li>
<li><strong>Git Integration:</strong> Show file statuses (<code>Modified</code>, <code>New</code>, <code>Untracked</code>, etc.) directly in the tree with the <code>-G</code> flag.</li>
</ul>
</li>
<li><strong>Smart filtering:</strong>
<ul dir="auto">
<li>Respects your <code>.gitignore</code> files with the <code>-g</code> flag.</li>
<li>Control recursion depth (<code>-L</code>) or show only directories (<code>-d</code>).</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">You need the Rust toolchain installed on your system to build <code>lstr</code>.</p>
<ol dir="auto">
<li><strong>Clone the repository:</strong>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/bgreenwell/lstr.git
cd lstr"><pre>git clone https://github.com/bgreenwell/lstr.git
<span>cd</span> lstr</pre></div>
</li>
<li><strong>Build and install using Cargo:</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# This compiles in release mode and copies the binary to ~/.cargo/bin
cargo install --path ."><pre><span><span>#</span> This compiles in release mode and copies the binary to ~/.cargo/bin</span>
cargo install --path <span>.</span></pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="lstr [OPTIONS] [PATH]
lstr interactive [OPTIONS] [PATH]"><pre>lstr [OPTIONS] [PATH]
lstr interactive [OPTIONS] [PATH]</pre></div>
<p dir="auto">Note that <code>PATH</code> defaults to the current directory (<code>.</code>) if not specified.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-a</code>, <code>--all</code></td>
<td>List all files and directories, including hidden ones.</td>
</tr>
<tr>
<td><code>--color &lt;WHEN&gt;</code></td>
<td>Specify when to use color output (<code>always</code>, <code>auto</code>, <code>never</code>).</td>
</tr>
<tr>
<td><code>-d</code>, <code>--dirs-only</code></td>
<td>List directories only, ignoring all files.</td>
</tr>
<tr>
<td><code>-g</code>, <code>--gitignore</code></td>
<td>Respect <code>.gitignore</code> and other standard ignore files.</td>
</tr>
<tr>
<td><code>-G</code>, <code>--git-status</code></td>
<td>Show git status for files and directories.</td>
</tr>
<tr>
<td><code>--icons</code></td>
<td>Display file-specific icons; requires a <a href="https://www.nerdfonts.com/" rel="nofollow">Nerd Font</a>.</td>
</tr>
<tr>
<td><code>-L</code>, <code>--level &lt;LEVEL&gt;</code></td>
<td>Maximum depth to descend.</td>
</tr>
<tr>
<td><code>-p</code>, <code>--permissions</code></td>
<td>Display file permissions (Unix-like systems only).</td>
</tr>
<tr>
<td><code>-s</code>, <code>--size</code></td>
<td>Display the size of files.</td>
</tr>
<tr>
<td><code>--expand-level &lt;LEVEL&gt;</code></td>
<td><strong>Interactive mode only:</strong> Initial depth to expand the interactive tree.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interactive mode</h2><a id="user-content-interactive-mode" aria-label="Permalink: Interactive mode" href="#interactive-mode"></a></p>
<p dir="auto">Launch the TUI with <code>lstr interactive [OPTIONS] [PATH]</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Keyboard controls</h3><a id="user-content-keyboard-controls" aria-label="Permalink: Keyboard controls" href="#keyboard-controls"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Key(s)</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>↑</code> / <code>k</code></td>
<td>Move selection up.</td>
</tr>
<tr>
<td><code>↓</code> / <code>j</code></td>
<td>Move selection down.</td>
</tr>
<tr>
<td><code>Enter</code></td>
<td><strong>Context-aware action:</strong><br>- If on a file: Open it in the default editor (<code>$EDITOR</code>).<br>- If on a directory: Toggle expand/collapse.</td>
</tr>
<tr>
<td><code>q</code> / <code>Esc</code></td>
<td>Quit the application normally.</td>
</tr>
<tr>
<td><code>Ctrl</code>+<code>s</code></td>
<td><strong>Shell integration:</strong> Quits and prints the selected path to stdout.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><strong>1. List the contents of the current directory</strong></p>

<p dir="auto"><strong>2. Explore a project interactively, ignoring gitignored files</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="lstr interactive -g --icons"><pre>lstr interactive -g --icons</pre></div>
<p dir="auto"><strong>3. Display a directory with file sizes and permissions (classic view)</strong></p>

<p dir="auto"><strong>4. See the git status of all files in a project</strong></p>

<p dir="auto"><strong>5. Start an interactive session with all data displayed</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="lstr interactive -gG --icons -s -p"><pre>lstr interactive -gG --icons -s -p</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Piping and shell interaction</h2><a id="user-content-piping-and-shell-interaction" aria-label="Permalink: Piping and shell interaction" href="#piping-and-shell-interaction"></a></p>
<p dir="auto">The classic <code>view</code> mode is designed to work well with other command-line tools via pipes (<code>|</code>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Interactive fuzzy finding with <code>fzf</code></h3><a id="user-content-interactive-fuzzy-finding-with-fzf" aria-label="Permalink: Interactive fuzzy finding with fzf" href="#interactive-fuzzy-finding-with-fzf"></a></p>
<p dir="auto">This is a powerful way to instantly find any file in a large project.</p>

<p dir="auto"><code>fzf</code> will take the tree from <code>lstr</code> and provide an interactive search prompt to filter it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Paging large trees with <code>less</code> or <code>bat</code></h3><a id="user-content-paging-large-trees-with-less-or-bat" aria-label="Permalink: Paging large trees with less or bat" href="#paging-large-trees-with-less-or-bat"></a></p>
<p dir="auto">If a directory is too large to fit on one screen, pipe the output to a <em>pager</em>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Using less (the -R flag preserves color)
lstr -L 10 | less -R

# Using bat (a modern pager that understands colors)
lstr --icons | bat"><pre><span><span>#</span> Using less (the -R flag preserves color)</span>
lstr -L 10 <span>|</span> less -R

<span><span>#</span> Using bat (a modern pager that understands colors)</span>
lstr --icons <span>|</span> bat</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Changing directories with <code>lstr</code></h3><a id="user-content-changing-directories-with-lstr" aria-label="Permalink: Changing directories with lstr" href="#changing-directories-with-lstr"></a></p>
<p dir="auto">You can use <code>lstr</code> as a visual <code>cd</code> command. Add the following function to your shell's startup file (e.g., <code>~/.bashrc</code>, <code>~/.zshrc</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# A function to visually change directories with lstr
lcd() {
    # Run lstr and capture the selected path into a variable.
    # The TUI will draw on stderr, and the final path will be on stdout.
    local selected_dir
    selected_dir=&quot;$(lstr interactive -g --icons)&quot;

    # If the user selected a path (and didn't just quit), `cd` into it.
    # Check if the selection is a directory.
    if [[ -n &quot;$selected_dir&quot; &amp;&amp; -d &quot;$selected_dir&quot; ]]; then
        cd &quot;$selected_dir&quot;
    fi
}"><pre><span><span>#</span> A function to visually change directories with lstr</span>
<span>lcd</span>() {
    <span><span>#</span> Run lstr and capture the selected path into a variable.</span>
    <span><span>#</span> The TUI will draw on stderr, and the final path will be on stdout.</span>
    <span>local</span> selected_dir
    selected_dir=<span><span>"</span><span><span>$(</span>lstr interactive -g --icons<span>)</span></span><span>"</span></span>

    <span><span>#</span> If the user selected a path (and didn't just quit), `cd` into it.</span>
    <span><span>#</span> Check if the selection is a directory.</span>
    <span>if</span> [[ <span>-n</span> <span><span>"</span><span>$selected_dir</span><span>"</span></span> <span>&amp;&amp;</span> <span>-d</span> <span><span>"</span><span>$selected_dir</span><span>"</span></span> ]]<span>;</span> <span>then</span>
        <span>cd</span> <span><span>"</span><span>$selected_dir</span><span>"</span></span>
    <span>fi</span>
}</pre></div>
<p dir="auto">After adding this and starting a new shell session (or running <code>source ~/.bashrc</code>), you can simply run:</p>

<p dir="auto">This will launch the <code>lstr</code> interactive UI. Navigate to the directory you want, press <code>Ctrl+s</code>, and your shell's current directory will instantly change.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance and concurrency</h2><a id="user-content-performance-and-concurrency" aria-label="Permalink: Performance and concurrency" href="#performance-and-concurrency"></a></p>
<p dir="auto">By default, <code>lstr</code> uses a parallel directory walker to maximize speed on multi-core systems. This parallelism is managed by the excellent <a href="https://crates.io/crates/rayon" rel="nofollow">rayon</a> thread pool, which is used internally by <code>lstr</code>'s directory traversal engine.</p>
<p dir="auto">For advanced use cases, such as benchmarking or limiting CPU usage, you can control the number of threads by setting the <code>RAYON_NUM_THREADS</code> environment variable before running the command.</p>
<p dir="auto"><strong>To force single-threaded (serial) execution:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="RAYON_NUM_THREADS=1 lstr ."><pre>RAYON_NUM_THREADS=1 lstr <span>.</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspiration</h2><a id="user-content-inspiration" aria-label="Permalink: Inspiration" href="#inspiration"></a></p>
<p dir="auto">The philosophy and functionality of <code>lstr</code> are heavily inspired by the excellent C-based <a href="https://github.com/Old-Man-Programmer/tree">tree</a> command line program. This project is an attempt to recreate that classic utility in modern, safe Rust.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Grug Brained Developer (2022) (888 pts)]]></title>
            <link>https://grugbrain.dev/</link>
            <guid>44303542</guid>
            <pubDate>Tue, 17 Jun 2025 20:24:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grugbrain.dev/">https://grugbrain.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=44303542">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div>
  <p><a href="https://www.redbubble.com/i/sticker/Programmer-Grug-by-colossalbreaker/42915272.EJUG5">
    <img alt="grug" src="https://grugbrain.dev/grug.png">
  </a></p><h2>
     The Grug Brained Developer<br>
     <small>A layman's guide to thinking like the self-aware smol brained</small>
  </h2>
</div>

<h2>Introduction</h2>
<p>this collection of thoughts on software development gathered by grug brain developer</p>
<p>grug brain developer not so smart, but grug brain developer program many long year and learn some things
although mostly still confused</p>
<p>grug brain developer try collect learns into small, easily digestible and funny page, not only for you, the young grug, but also for him
because as grug brain developer get older he forget important things, like what had for breakfast or if put pants on</p>
<p>big brained developers are many, and some not expected to like this, make sour face</p>
<p><em>THINK</em> they are big brained developers many, many more, and more even definitely probably maybe not like this, many
sour face (such is internet)</p>
<p>(note: grug once think big brained but learn hard way)</p>
<p>is fine!</p>
<p>is free country sort of and end of day not really matter too much, but grug hope you fun reading and maybe learn from
many, many mistake grug make over long program life</p>
<h2><a name="grug-on-complexity"></a><a href="#grug-on-complexity">The Eternal Enemy: Complexity</a></h2>
<p>apex predator of grug is complexity</p>
<p>complexity bad</p>
<p>say again:</p>
<p>complexity <em>very</em> bad</p>
<p><em>you</em> say now:</p>
<p>complexity <em>very</em>, <em>very</em> bad</p>
<p>given choice between complexity or one on one against t-rex, grug take t-rex: at least grug see t-rex</p>
<p>complexity is spirit demon that enter codebase through well-meaning but ultimately very clubbable non grug-brain
developers and project managers who not fear complexity spirit demon or even know about sometime</p>
<p>one day code base understandable and grug can get work done, everything good!</p>
<p>next day impossible: complexity demon spirit has entered code and very dangerous situation!</p>
<p>grug no able see complexity demon, but grug sense presence in code base</p>
<p>demon complexity spirit mocking him make change here break unrelated thing there what!?! mock mock mock ha ha so funny
grug love programming and not becoming shiney rock speculator like grug senior advise</p>
<p>club not work on demon spirit complexity and bad idea actually hit developer who let spirit in with club: sometimes grug
himself!</p>
<p>sadly, often grug himself</p>
<p>so grug say again and say often: complexity <em>very</em>, <em>very</em> bad</p>
<h2><a name="grug-on-saying-no"></a><a href="#grug-on-saying-no">Saying No</a></h2>
<p>best weapon against complexity spirit demon is magic word: "no"</p>
<p>"no, grug not build that feature"</p>
<p>"no, grug not build that abstraction"</p>
<p>"no, grug not put water on body every day or drink less black think juice you stop repeat ask now"</p>
<p>note, this good engineering advice but bad career advice: "yes" is magic word for more shiney rock and put in
charge of large tribe of developer</p>
<p>sad but true: learn "yes" then learn blame other grugs when fail, ideal career advice</p>
<p>but grug must to grug be true, and "no" is magic grug word.  Hard say at first, especially if you nice grug and don't like
disappoint people (many such grugs!) but  easier over time even though shiney rock pile not as high as might otherwise be</p>
<p>is ok: how many shiney rock grug really need anyway?</p>
<h2><a name="grug-on-saying-ok"></a><a href="#grug-on-saying-ok">Saying ok</a></h2>
<p>sometimes compromise necessary or no shiney rock, mean no dinosaur meat, not good, wife firmly remind grug
about young grugs at home need roof, food, and so forth, no interest in complexity demon spirit rant by grug for
fiftieth time</p>
<p>in this situation, grug recommend "ok"</p>
<p>"ok, grug build that feature"</p>
<p>then grug spend time think of <a href="https://en.wikipedia.org/wiki/Pareto_principle">80/20 solution</a> to problem and build that instead.<br>
80/20 solution say "80 want with 20 code"  solution maybe not have all bell-whistle that project manager want, maybe a
little ugly, but work and deliver most value, and keep demon complexity spirit at bay for most part to extent</p>
<p>sometimes probably best just not tell project manager and do it 80/20 way.  easier forgive than permission, project managers
mind like butterfly at times overworked and dealing with many grugs.  often forget what even feature supposed to do or move on or
quit or get fired grug see many such cases</p>
<p>anyway is in project managers best interest anyway so grug not to feel too bad for this approach usually</p>
<h2><a name="grug-on-factring-your-code"></a><a href="#grug-on-factring-your-code">Factoring Your Code</a></h2>
<p>next strategy very harder: break code base up properly (fancy word: "factor your code properly")  here is hard give general
advice because each system so different.  however, one thing grug come to believe: not factor your application too early!</p>
<p>early on in project everything very abstract and like water: very little solid holds for grug's struggling brain to hang
on to.  take time to develop "shape" of system and learn what even doing.  grug try not to factor in early part of project
and then, at some point, good cut-points emerge from code base</p>
<p>good cut point has narrow interface with rest of system: small number of functions or abstractions that hide complexity
demon internally, like trapped in crystal</p>
<p>grug quite satisfied when complexity demon trapped properly in crystal, is best feeling to trap mortal enemy!</p>
<p>grug try watch patiently as cut points emerge from code and slowly refactor, with code base taking shape over time along
with experience.  no hard/ fast rule for this: grug know cut point when grug see cut point, just take time to build
skill in seeing, patience</p>
<p>sometimes grug go too early and get abstractions wrong, so grug bias towards waiting</p>
<p>big brain developers often not like this at all and invent many abstractions start of project</p>
<p>grug tempted to reach for club and yell "big brain no maintain code!  big brain move on next architecture committee
leave code for grug deal with!"</p>
<p>but grug learn control passions, major difference between grug and animal</p>
<p>instead grug try to limit damage of big brain developer early in project by giving them thing like
UML diagram (not hurt code, probably throw away anyway) or by demanding working demo tomorrow</p>
<p>working demo especially good trick: force big brain make something to actually work to talk about and code to look at that do
thing, will help big brain see reality on ground more quickly</p>
<p>remember!  big brain have big brain!  need only be harness for good and not in service of spirit complexity demon on
accident, many times seen</p>
<p>(best grug brain able to herd multiple big brain in right direction and produce many complexity demon trap crystals, large
shiney rock pile awaits such grug!)</p>
<p>also sometimes call demo approach "prototype", sound fancier to project manager</p>
<p>grug say prototype early in software making, <em>especially</em> if many big brains</p>
<h2><a name="grug-on-testing"></a><a href="#grug-on-testing">Testing</a></h2>
<p>grug have love/hate relationship with test: test save grug many, many uncountable time and grug love and respect test</p>
<p>unfortunately also many test shamans exist.  some test shaman make test idol, demand things like "first test" before grug
even write code or have any idea what grug doing domain!</p>
<p>how grug test what grug not even understand domain yet!?</p>
<p>"Oh, don't worry: the tests will show you what you need to do."</p>
<p>grug once again catch grug slowly reaching for club, but grug stay calm</p>
<p>grug instead prefer write most tests after prototype phase, when code has begun firm up</p>
<p>but, note well: grug must here be very disciplined!</p>
<p>easy grug to move on and not write tests because "work on grugs machine"!</p>
<p>this very, very bad: no guarantee work on other machine and no guarantee work on grug machine in future, many times</p>
<p>test shaman have good point on importance of test, even if test shaman often sometimes not complete useful
feature in life and talk only about test all time, deserve of club but heart in right place</p>
<p>also, test shaman often talk unit test very much, but grug not find so useful.  grug experience that ideal tests are not
unit test or either end-to-end test, but in-between test</p>
<p><a href="https://en.wikipedia.org/wiki/Unit_testing">unit tests</a> fine, ok, but break as implementation change (much compared api!)
and make refactor hard and, frankly, many bugs anyway often due interactions other code.  often throw away when code change.</p>
<p>grug write unit test mostly at start of project, help get things going but not get too attached or expect value long time</p>
<p><a href="https://smartbear.com/solutions/end-to-end-testing/">end to end</a> tests good, show whole system work, but! hard to
understand when break and drive grug crazy very often, sometimes grugs just end up ignoring because "oh, that break all
time"  very bad!</p>
<p>in-between tests, grug hear shaman call <a href="https://en.wikipedia.org/wiki/Integration_testing">"integration tests"</a> sometime
often with sour look on face. but grug say integration test sweet spot according to grug: high level enough test correctness
of system, low level enough, with good debugger, easy to see what break</p>
<p>grug prefer some unit tests especially at start but not 100% all code test and definitely not "first test".  "test along
the way" work pretty well for grug, especially as grug figure things out</p>
<p>grug focus much ferocious integration test effort as cut point emerge and system stabilize!  cut point api hopefully stable
compared implementation and integration test remain valuable many long time, and easy debug</p>
<p>also small, well curated end-to-end test suite is created to be kept working religiously on pain of clubbing. focus of important
end-to-end test on most common UI features and few most important edge cases, but not too many or become impossible maintain
and then ignored</p>
<p>this ideal set of test to grug</p>
<p>you may not like, but this peak grug testing</p>
<p>also, grug dislike <a href="https://en.wikipedia.org/wiki/Mock_object">mocking</a> in test, prefer only when absolute necessary
to (rare/never) and coarse grain mocking (cut points/systems) only at that</p>
<p>one exception "first test" dislike by grug: when bug found.  grug always try first reproduce bug with regression test
<em>then</em> fix bug, this case only for some reason work better</p>
<h2><a name="grug-on-agile"></a><a href="#grug-on-agile">Agile</a></h2>
<p>grug think agile not terrible, not good</p>
<p>end of day, not worst way to organize development, maybe better than others grug supposes is fine</p>
<p>danger, however, is agile shaman!  many, many shiney rock lost to agile shaman!</p>
<p>whenever agile project fail, agile shaman say "you didn't do agile right!"  grug note this awfully convenient for agile
shaman, ask more shiney rock better agile train young grugs on agile, danger!</p>
<p>grug tempted reach for club when too much agile talk happen but always stay calm</p>
<p>prototyping, tools and hiring good grugs better key to success software: agile process ok and help some but sometimes hurt taken
too seriously</p>
<p>grug say <a href="https://en.wikipedia.org/wiki/No_Silver_Bullet">no silver club</a> fix all software problems no matter what agile
shaman say (danger!)</p>
<h2><a name="grug-on-refactoring"></a><a href="#grug-on-refactoring">Refactoring</a></h2>
<p>refactoring fine activity and often good idea, especially later in project when code firmed up</p>
<p>however, grug note that many times in career "refactors" go horribly off rails and end up causing more harm than good</p>
<p>grug not sure exactly why some refactors work well, some fail, but grug notice that larger refactor, more
likely failure appear to be</p>
<p>so grug try to keep refactors relatively small and not be "too far out from shore" during refactor.  ideally system work
entire time and each step of finish before other begin.</p>
<p>end-to-end tests are life saver here, but often very hard understand why broke... such is refactor life.</p>
<p>also grug notice that introducing too much abstraction often lead to refactor failure and system failure.  good example
was <a href="https://www.webopedia.com/definitions/j2ee/">J2EE</a> introduce, many big brain sit around thinking too much abstraction, nothing good came of it many project hurt</p>
<p>another good example when company grug work for introduce <a href="https://www.techtarget.com/searchnetworking/definition/OSGi">OSGi</a> to help
manage/trap spriit complexity demon in code base.  not only OSGi not help, but make complexity demon much more powerful!
took multiple man year of best developers to rework as well to boot!  more complex spirit and now features impossible
implement! very bad!</p>
<h2><a name="grug-on-chestertons-fence"></a><a href="#grug-on-chestertons-fence">Chesterton's Fence</a></h2>
<p>wise grug shaman <a href="https://en.wikipedia.org/wiki/G._K._Chesterton">chesterton</a> once say</p>
<blockquote>
<p>here exists in such a case a certain institution or law; let us say, for the sake of simplicity, a fence or gate erected across a road. The more modern type of reformer goes gaily up to it and says, “I don’t see the use of this; let us clear it away.” To which the more intelligent type of reformer will do well to answer: “If you don’t see the use of it, I certainly won’t let you clear it away. Go away and think. Then, when you can come back and tell me that you do see the use of it, I may allow you to destroy it.”</p>
</blockquote>
<p>many older grug learn this lesson well not start tearing code out willy nilly, no matter how ugly look</p>
<p>grug understand all programmer platonists at some level wish music of spheres perfection in code.  but danger is here,
world is ugly and gronky many times and so also must code be</p>
<p>humility not often come big brained or think big brained
easily or grug even, but grug often find "oh, grug no like look of this, grug fix" lead many hours pain grug and no better or system
worse even</p>
<p>grug early on in career often charge into code base waving club wildly and smash up everything, learn not good</p>
<p>grug not say no improve system ever, quite foolish, but recommend take time understand system first especially bigger system is and
is respect code working today even if not perfect</p>
<p>here tests often good hint for why fence not to be smashed!</p>
<h2><a name="grug-on-microservices"></a><a href="#grug-on-microservices">Microservices</a></h2>
<p>grug wonder why big brain take hardest problem, factoring system correctly, and introduce network call too</p>
<p>seem very confusing to grug</p>
<h2><a name="grug-on-tools"></a><a href="#grug-on-tools">Tools</a></h2>
<p>grug love tool.  tool and control passion what separate grug from dinosaurs!  tool allow grug brain to create code that
not possible otherwise by doing thinking for grug, always good relief! grug always spend time in new place learning
tools around him to maximize productivity: learn tools for two weeks make development often twice faster and often
have dig around ask other developers help, no docs</p>
<p>code completion in IDE allow grug not have remembered all API, very important!</p>
<p>java programming nearly impossible without it for grug!</p>
<p>really make grug think some time</p>
<p>good debugger worth weight in shiney rocks, in fact also more: when faced with bug grug would often trade all shiney rock and
perhaps few children for good debugger and anyway debugger no weigh anything far as grug can tell</p>
<p>grug always recommend new programmer learn available debugger very deeply, features like conditional break points, expression
evaluation, stack navigation, etc teach new grug more about computer than university class often!</p>
<p>grug say never be not improving tooling</p>
<h2><a name="grug-on-type-systems"></a><a href="#grug-on-type-systems">Type Systems</a></h2>
<p>grug very like type systems make programming easier.  for grug, type systems most value when grug hit dot on keyboard and
list of things grug can do pop up magic.  this 90% of value of type system or more to grug</p>
<p>big brain type system shaman often say type correctness main point type system, but grug note some big brain type system
shaman not often ship code.  grug suppose code never shipped is correct, in some sense, but not really what grug mean
when say correct</p>
<p>grug say tool magic pop up of what can do and complete of code major most benefit of type system, correctness also good but not
so nearly so much</p>
<p>also, often sometimes caution beware big brains here!</p>
<p>some type big brain think in type systems and talk in lemmas, potential danger!</p>
<p>danger abstraction too high, big brain type system code become astral projection of platonic generic turing model of
computation into code base.  grug confused and agree some level very elegant but also very hard do anything like
record number of club inventory for Grug Inc. task at hand</p>
<p>generics especially dangerous here, grug try limit generics to container classes for most part where most value add</p>
<p>temptation generics very large is trick!  spirit demon complex love this one trick! beware!</p>
<p>always most value type system come: hit dot see what grug can do, never forget!</p>
<h2><a name="grug-on-expression-complexity"></a><a href="#grug-on-expression-complexity">Expression Complexity</a></h2>
<p>grug once like to minimize lines of code much as possible.  write code like this:</p>
<pre><code>  if(contact &amp;&amp; !contact.isActive() &amp;&amp; (contact.inGroup(FAMILY) || contact.inGroup(FRIENDS))) {
    // ...
  }
</code></pre>
<p>over time grug learn this hard debug, learn prefer write like so:</p>
<pre><code>  if(contact) {
    var contactIsInactive = !contact.isActive();
    var contactIsFamilyOrFriends = contact.inGroup(FAMILY) || contact.inGroup(FRIENDS);
    if(contactIsInactive &amp;&amp; contactIsFamilyOrFriends) {
        // ...
    }
  }
</code></pre>
<p>grug hear screams from young grugs at horror of many line of code and pointless variable and grug prepare defend self with club</p>
<p>club fight start with other developers attack and grug yell: "easier debug!  see result of each expression more clearly and good name!  easier
understand conditional expression!  EASIER DEBUG!"</p>
<p>definitely easier debug and once club fight end calm down and young grug think a bit, they realize grug right</p>
<p>grug still catch grug writing code like first example and often regret, so grug not judge young grug</p>
<h2><a name="grug-on-dry"></a><a href="#grug-on-dry">DRY</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself">DRY</a> mean Don't Repeat Self, powerful maxim over mind of most
developers</p>
<p>grug respect DRY and good advice, however grug recommend balance in all things, as gruggest big brain aristotle recommend</p>
<p>grug note humourous graph by Lea Verou correspond with grug passion not repeat:</p>
<img alt="code concerns over time" src="https://grugbrain.dev/over-time.png">
<p>over time past ten years program grug not as concerned repeat code.  so long as repeat code simple enough and obvious
enough, and grug begin feel repeat/copy paste code with small variation is better than many callback/closures passed arguments
or elaborate object model: too hard complex for too little benefit at times</p>
<p>hard balance here, repeat code always still make grug stare and say "mmm" often, but experience show repeat code
sometimes often better than complex DRY solution</p>
<p>note well!  grug encourage over literal developer not take does work line too serious, is joke</p>
<h2><a name="grug-on-soc"></a><a href="#grug-on-soc">Separation of Concerns (SoC)</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Separation_of_concerns">Separation of Concern (SoC)</a> another powerful idea over many developer
mind, idea to separate different aspects of system into distinct sections code</p>
<p>canonical example from web development: separation of style (css file), markup (html file) and logic (javascript file)</p>
<p>here grug much more sour faced than DRY and in fact write big brained essay on alternative design principle
<a href="https://htmx.org/essays/locality-of-behaviour/">locality of behavior (LoB)</a> against SoC</p>
<p>grug much prefer put code on the thing that do the thing.  now when grug look at the thing grug know the thing what the
thing do, alwasy good relief!</p>
<p>when separate of concern grug must often all over tarnation many file look understand what how button do, much confuse
and time waste: bad!</p>
<h2><a name="grug-on-closures"></a><a href="#grug-on-closures">Closures</a></h2>
<p>grug like closures for right job and that job usually abstracting operation over collection of objects</p>
<p>grug warn closures like salt, type systems and generics: small amount go long way, but easy spoil things too much use
give heart attack</p>
<p>javascript developers call very special complexity demon spirit in javascript "callback hell" because too much closure
used by javascript libraries very sad but also javascript developer get what deserved let grug be frank</p>
<h2><a name="grug-on-logging"></a><a href="#grug-on-logging">Logging</a></h2>
<p>grug huge fan of logging and encourage lots of it, especially in cloud deployed.  some non-grugs say logging expensive
and not important.  grug used think this way no more</p>
<p>funny story: grug learn idol <a href="https://en.wikipedia.org/wiki/Rob_Pike">rob pike</a> working on logging at google and decide:
"if rob pike working on logging, what grug do there?!?" so not pursue.  turn out logging <em>very</em> important to google so
of course best programmer work on it, grug!</p>
<p>don't be such grug brain, grug, much less shiney rock now!</p>
<p>oh well, grug end up at good company anyway and rob pike dress habit
<a href="https://www.youtube.com/watch?v=KINIAgRpkDA">increasingly erratic</a>, so all work out in end, but
point stand: logging very important!</p>
<p>grug tips on logging are:</p>
<ul>
<li>log all major logical branches within code (if/for)</li>
<li>if "request" span multiple machine in cloud infrastructure, include request ID in all so logs can be grouped</li>
<li>if possible make log level dynamically controlled, so grug can turn on/off when need debug issue (many!)</li>
<li>if possible make log level per user, so can debug specific user issue</li>
</ul>
<p>last two points are especially handy club when fighting bugs in production systems very often</p>
<p>unfortunately log libraries often very complex (java, <a href="https://stackify.com/logging-java/">why you do?</a>) but worth investing
time in getting logging infrastructure "just right" pay off big later in grug experience</p>
<p>logging need taught more in schools, grug think</p>
<h2><a name="grug-on-concurrency"></a><a href="#grug-on-concurrency">Concurrency</a></h2>
<p>grug, like all sane developer, fear concurrency</p>
<p>as much as possible, grug try to rely on simple concurrency models like stateless web request handlers and simple
remote job worker queues where jobs no interdepend and simple api</p>
<p><a href="https://en.wikipedia.org/wiki/Optimistic_concurrency_control">optimistic concurrency</a> seem work well for web stuff</p>
<p>occasionally grug reach for <a href="https://en.wikipedia.org/wiki/Thread-local_storage">thread local variable</a>, usually when
writing framework code</p>
<p>some language have good concurrent data structure, like java <a href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentHashMap.html">ConcurrentHashMap</a>
but still need careful grug work to get right</p>
<p>grug has never used <a href="https://en.wikipedia.org/wiki/Erlang_(programming_language)">erlang</a>, hear good things, but language
look wierd to grug sorry</p>
<h2><a name="grug-on-optimizing"></a><a href="#grug-on-optimizing">Optimizing</a></h2>
<p>ultra biggest of brain developer once say:</p>
<blockquote>
<p>premature optimization is the root of all evil</p>
</blockquote>
<p>this everyone mostly know and grug in humble violent agreement with ultra biggest of big brain</p>
<p>grug recommend always to have concrete, real world perf profile showing specific perf issue before begin optimizing.</p>
<p>never know what actual issue might be, grug often surprise!  very often!</p>
<p>beware only cpu focus: easy to see cpu and much big o notation thinking having been done in school,
but often not root of all slowness, surprise to many including grug</p>
<p>hitting network equivalent of many, many millions cpu cycle and always to be minimized if possible, note well big brain
microservice developer!</p>
<p>inexperienced big brain developer see nested loop and often say "O(n^2)?  Not on my watch!"</p>
<p>complexity demon spirit smile</p>
<h2><a name="grug-on-apis"></a><a href="#grug-on-apis">APIs</a></h2>
<p>grug love good apis.  good apis not make grug think too much</p>
<p>unfortunately, many apis very bad, make grug think quite a bit.  this happen many reasons, here two:</p>
<ul>
<li>API creators think in terms of implementation or domain of API, rather than in terms of use of API</li>
<li>API creators think too abstract and big brained</li>
</ul>
<p>usually grug not care too deeply about detail of api: want write file or sort list or whatever, just want to call
<code>write()</code> or <code>sort()</code> or whatever</p>
<p>but big brain api developers say:</p>
<p>"not so fast, grug!  is that file <em>open for write</em>? did you define a <em>Comparator</em> for that sort?"</p>
<p>grug find self restraining hand reaching for club again</p>
<p>not care about that stuff right now, just want sort and write file mr big brain!</p>
<p>grug recognize that big brain api designer have point and that <em>sometime</em> these things matter, but often do not.
big brain api developers better if design for simple cases with simple api, make complex cases possible
with more complex api</p>
<p>grug call this "layering" apis: two or three different apis at different level complexity for various grug needs</p>
<p>also, if object oriented, put api on thing instead of elsewhere. java worst at this!</p>
<p>grug want filter list in java</p>
<p>"Did you convert it to a stream?"</p>
<p>fine, grug convert to stream</p>
<p>"OK, now you can filter."</p>
<p>OK, but now need return list!  have stream!</p>
<p>"Well, did you collect your stream into a list?"</p>
<p>what?</p>
<p>"Define a Collector&lt;? super T, A, R&gt; to collect your stream into a list"</p>
<p>grug now swear on ancestor grave he club every single person in room, but count two instead and remain calm</p>
<p>put common thing like <code>filter()</code> on list and make return list, listen well big brain java api developer!</p>
<p>nobody care about "stream" or even hear of "stream" before, is not networking api, all java grugs use list mr big brain!</p>
<h2><a name="grug-on-parsing"></a><a href="#grug-on-parsing">Parsing</a></h2>
<p>grug love make programming language at drop of hat and
say <a href="https://en.wikipedia.org/wiki/Recursive_descent_parser">recursive descent</a>
most fun and beautiful way create parser</p>
<p>unfortunately many big brain school teach only parser generator tool.  here grug usual love of tool is not: parser
generator tool generate code of awful snakes nest: impossible understand, bottom up, what?  hide recursive nature of
grammar from grug and debug impossible, very bad according grug!</p>
<p>grug think this because while complexity demon bad for code base and understand, complexity demon very good for generation
of much academic papers, sad but true</p>
<p>production parser almost always recursive descent, despite ignore by schools!  grug furious when learn how simple parse
is! parsing not big brain only magic: so can you!</p>
<p>grug very elated find big brain developer Bob Nystrom redeem the big brain tribe and write excellent book on recursive
descent: <a href="https://craftinginterpreters.com/">Crafting Interpreters</a></p>
<p>book available online free, but grug highly recommend all interested grugs purchase book on general principle, provide
much big brain advice and grug love book <em>very</em> much except visitor pattern (trap!)</p>
<h2><a name="grug-on-visitor-pattern"></a><a href="#grug-on-visitor-pattern">The Visitor Pattern</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Visitor_pattern">bad</a></p>
<h2><a name="grug-on-front-end-development"></a><a href="#grug-on-front-end-development">Front End Development</a></h2>
<p>some non-grugs, when faced with web development say:</p>
<p>"I know, I'll split my front end and back end codebase up and use a hot new SPA library talking to a GraphQL JSON API back end
over HTTP (which is funny because I'm not transferring hypertext)"</p>
<p>now you have two complexity demon spirit lairs</p>
<p>and, what is worse, front end complexity demon spirit even more powerful and have deep spiritual hold on entire front end
industry as far as grug can tell</p>
<p>back end developers try keep things simple and can work ok, but front end developers make very complex very quickly and
introduce lots of code, demon complex spirit</p>
<p>even when website just need put form into database or simple brochure site!</p>
<p>everyone do this now!</p>
<p>grug not sure why except maybe facebook and google say so, but that not seem very good reason to grug</p>
<p>grug not like big complex front end libraries everyone use</p>
<p>grug make <a href="https://htmx.org/">htmx</a> and <a href="https://hyperscript.org/">hyperscript</a> to avoid</p>
<p>keep complexity low, simple HTML, avoid lots javascript, the natural ether of spirit complexity demon</p>
<p>maybe they work for you, but no job post, sorry</p>
<p>react better for job and also some type application, but also you become alcolyte of complexity demon whether you like
or no, sorry such is front end life</p>
<h2><a name="grug-on-fads"></a><a href="#grug-on-fads">Fads</a></h2>
<p>grug note lots of fads in development, especially front end development today</p>
<p>back end better more boring because all bad ideas have tried at this point maybe (still retry some!)</p>
<p>still trying all bad ideas in front end development so still much change and hard to know</p>
<p>grug recommend taking all revolutionary new approach with grain salt: big brains have working for long
time on computers now, most ideas have tried at least once</p>
<p>grug not saying can't learn new tricks or no good new ideas, but also much of time wasted on recycled bad ideas, lots of
spirit complexity demon power come from putting new idea willy nilly into code base</p>
<h2><a name="grug-on-fold"></a><a href="#grug-on-fold">Fear Of Looking Dumb</a></h2>
<p>note!  very good if senior grug willing to say publicly: "hmmm, this too complex for grug"!</p>
<p>many developers Fear Of Looking Dumb (FOLD), grug also at one time FOLD, but grug learn get over: very important senior
grug say "this too complicated and confuse to me"</p>
<p>this make it ok for junior grugs to admit too complex and not understand as well, often such case!  FOLD major source of
complexity demon power over developer, especially young grugs!</p>
<p>take FOLD power away, very good of senior grug!</p>
<p>note: important to make thinking face and look big brained when saying though.  be prepare for big brain or, worse and
much more common, <em>thinks</em> is big brain to make snide remark of grug</p>
<p>be strong! no FOLD!</p>
<p>club sometimes useful here, but more often sense of humor and especially last failed project by big brain very useful,
so collect and be calm</p>
<h2><a name="grug-on-imposter-syndrom"></a><a href="#grug-on-imposter-syndrom">Impostor Syndrome</a></h2>
<p>grug note many such impostor feels in development</p>
<p>always grug one of two states: grug is ruler of all survey, wield code club like thor OR grug have no idea what doing</p>
<p>grug is mostly latter state most times, hide it pretty well though</p>
<p>now, grug make softwares of much work and <a href="https://star-history.com/#bigskysoftware/htmx&amp;bigskysoftware/_hyperscript&amp;Date">moderate open source success</a>
, and yet grug himself often feel not any idea what doing!  very often!  grug still fear make mistake break everyone code and
disappoint other grugs, imposter!</p>
<p>is maybe nature of programming for most grug to feel impostor and be ok with is best: nobody imposter if everybody imposter</p>
<p>any young grug read this far probably do fine in program career even if frustrations and worry is always to be there, sorry</p>
<h2><a name="grug-reads"></a><a href="#grug-reads">Reads</a></h2>
<p>grug like these:</p>
<ul>
<li><a href="https://www.dreamsongs.com/WorseIsBetter.html">Worse is Better</a></li>
<li><a href="https://www.dreamsongs.com/Files/worse-is-worse.pdf">Worse is Better is Worse</a></li>
<li><a href="https://www.dreamsongs.com/Files/IsWorseReallyBetter.pdf">Is Worse Really Better?</a></li>
<li><a href="https://www.goodreads.com/en/book/show/39996759-a-philosophy-of-software-design">A Philosophy of Software Design</a></li>
</ul>
<h2><a name="lol-lmao"></a><a href="#lol-lmao">Conclusion</a></h2>
<p><em>you</em> say: complexity <em>very</em>, <em>very</em> bad</p>

  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bzip2 crate switches from C to 100% Rust (302 pts)]]></title>
            <link>https://trifectatech.org/blog/bzip2-crate-switches-from-c-to-rust/</link>
            <guid>44303361</guid>
            <pubDate>Tue, 17 Jun 2025 20:06:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trifectatech.org/blog/bzip2-crate-switches-from-c-to-rust/">https://trifectatech.org/blog/bzip2-crate-switches-from-c-to-rust/</a>, See on <a href="https://news.ycombinator.com/item?id=44303361">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>Today we published <code>bzip2</code> version <code>0.6.0</code>, which uses our rust implementation of the bzip2 algorithm, <code>libbz2-rs-sys</code>, by default. The <code>bzip2</code> crate is now faster and easier to cross-compile.</p>
<p>The <code>libbz2-rs-sys</code> crate can also be built as a C dynamic library, if you have a C project that would benefit from these improvements.</p>
<h2 id="why-though">Why though?</h2>
<p>Why bother working on this algorithm from the 90s that sees very little use today? The thing is that many protocols and libraries still need to support bzip2 to be compliant with their specification, so many project still, deep down in their dependency tree, depend on bzip2. We've used our experience from zlib-rs to modernize the <code>bzip2</code>  implementation.</p>
<p>We've previously written about the implementation details of <code>libbz2-rs-sys</code> in <a href="https://trifectatech.org/blog/translating-bzip2-with-c2rust/">"Translating bzip2 with c2rust"</a>, now let's look at the benefits of this work.</p>
<h3 id="improved-performance">Improved performance</h3>
<p>Our rust implementation generally outperforms the C implementation, though there are a couple of cases where we only match C performance. We are not aware of any cases where we are substantially slower.</p>
<p>For compression, we are a fair amount faster. For bzip2, the <code>level</code> indicates how much working memory is used. It doesn't influence performance by much, and for <code>sample3.ref</code> level 1 already allocates more memory than the file is large, so higher levels are irrelevant.</p>
<table><thead><tr><th>name</th><th>c (cpu cycles)</th><th>rust (cpu cycles)</th><th>Δ</th></tr></thead><tbody>
<tr><td>sample3.ref (level 1)</td><td><code>38.51M ±  77.03K</code></td><td><code>33.53M ±  90.52K</code></td><td><code>-14.87%</code></td></tr>
<tr><td>silesia-small.tar (level 1)</td><td><code> 3.43G ±   2.06M</code></td><td><code> 3.00G ±   6.31M</code></td><td><code>-14.30%</code></td></tr>
<tr><td>silesia-small.tar (level 9)</td><td><code> 3.47G ±   4.86M</code></td><td><code> 3.17G ±   4.43M</code></td><td><code>- 9.66%</code></td></tr>
</tbody></table>
<p>For decompression there is a bit more of a spread, but we again see significant speedups across the board.</p>
<table><thead><tr><th>name</th><th>c (cpu cycles)</th><th>rust (cpu cycles)</th><th>Δ</th></tr></thead><tbody>
<tr><td>sample3.bz2</td><td><code> 2.53M ±  30.08K</code></td><td><code> 2.42M ±   8.95K</code></td><td><code>- 4.48%</code></td></tr>
<tr><td>sample1.bz2</td><td><code> 9.63M ±  40.44K</code></td><td><code> 8.86M ±  10.64K</code></td><td><code>- 8.63%</code></td></tr>
<tr><td>sample2.bz2</td><td><code>20.47M ±  55.28K</code></td><td><code>19.02M ±  36.13K</code></td><td><code>- 7.67%</code></td></tr>
<tr><td>dancing-color.ps.bz2</td><td><code>87.46M ± 481.02K</code></td><td><code>83.16M ± 548.86K</code></td><td><code>- 5.17%</code></td></tr>
<tr><td>re2-exhaustive.txt.bz2</td><td><code> 1.89G ±  12.29M</code></td><td><code> 1.76G ±  12.64M</code></td><td><code>- 7.65%</code></td></tr>
<tr><td>zip64support.tar.bz2</td><td><code> 2.32G ±  12.09M</code></td><td><code> 2.11G ±  15.42M</code></td><td><code>-10.00%</code></td></tr>
</tbody></table>
<p>One caveat is that on our macOS benchmark machine we occasionally see some lower numbers for decompression. We are not sure what causes the variance, and measuring performance on macOS in a detailed way has turned out to be difficult (e.g there is no tool like <code>perf</code> to automate performance tracking that we could get to work).</p>
<h3 id="enabling-cross-compilation">Enabling cross-compilation</h3>
<p>Cross-compilation of a rust project with C dependencies often works out of the box (because the <code>cc</code> crate tries to handle it), but when it doesn't the errors can be hard to debug. Similarly linking to system libraries can cause confusing and hard-to-reproduce issues.</p>
<p>For bzip2, compilation to webassembly has long been an issue. By removing the C dependency and using rust code instead, the complications of compiling C just disappear: cross-compilation just works. Also building for windows or android just works. Besides providing a better experience for users, this change is also a major maintenance win.</p>
<h3 id="symbols-are-not-exported-by-default">Symbols are not exported (by default)</h3>
<p>Using a C dependency means that its symbols are exported (so that a rust <code>extern</code> block can find them). The exported names can conflict when another dependency declares the same symbols.</p>
<p>By default, <code>libbz2-rs-sys</code> does not export its symbols, which means that it will never conflict with other dependencies. If your rust project does need to emit the symbols, there is a feature flag to enable exporting symbols.</p>
<h3 id="run-tests-with-miri">Run tests with miri</h3>
<p>Writing a performant bzip2 implementation requires some unsafe code, and replicating the C interface in rust requires a lot more. Luckily we are able to run that code under MIRI.</p>
<p>More importantly, higher-level libraries or applications that use <code>bzip2</code> can now run with MIRI as well.</p>
<h2 id="audit">Audit</h2>
<p>The audit found one logic bug (an off-by-one error), and fixed some limitations in our fuzzer.
Beyond that, there were no significant findings (yay!).  We do want to thank the reviewers from <a href="https://www.radicallyopensecurity.com/">Radically Open Security</a>, specifically Christian Reitter, for sharing their fuzzing experience. The full audit report can be found <a href="https://github.com/trifectatechfoundation/libbzip2-rs/blob/main/docs/audits/NGICore%20bzip2%20in%20rust%20code%20audit%20report%202025%201.0.pdf">here</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The <code>bzip2</code> crate is faster now. You can go back to never having to think about it.</p>
<h3 id="thanks">Thanks</h3>
<ul>
<li><a href="https://github.com/alexcrichton">Alex Crichton</a> for sharing maintainership of the <code>bzip2</code> crate</li>
<li><a href="https://www.radicallyopensecurity.com/">Radically Open Security</a> for the audit and sharing their expertise</li>
<li><a href="https://nlnet.nl/">NLnet Foundation</a> for funding this work</li>
</ul>

                <br>
                
                <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Google Translate can tell us about vibecoding (221 pts)]]></title>
            <link>https://ingrids.space/posts/what-google-translate-can-tell-us-about-vibecoding/</link>
            <guid>44302870</guid>
            <pubDate>Tue, 17 Jun 2025 19:23:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ingrids.space/posts/what-google-translate-can-tell-us-about-vibecoding/">https://ingrids.space/posts/what-google-translate-can-tell-us-about-vibecoding/</a>, See on <a href="https://news.ycombinator.com/item?id=44302870">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    <title>What Google Translate Can Tell Us About Vibecoding | Ingrid's Space</title>

    

    
      

    

    


    

    <p>There has been rather a lot of doomsaying (and perhaps astroturfing) lately about LLMs as the end of computer programming. Much of the discussion has been lacking nuance, so I’d like to add mine. I see claims from one side that “I used <code>$LLM_SERVICE_PROVIDER</code> to make a small throwaway tool, so all programmers will be unemployed in <code>$ARBITRARY_TIME_WINDOW</code>”, and from the other side flat-out rejections of the idea that this type of tool can have any utility.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> I think it best sheds light on these claims to examine them in the context of another field that’s been ahead of the curve on this: translation.</p>
<p>Google translate has been around for a while, and has gone through some technological iterations; I’m most interested in discussing its recent incarnations since the switch to <a href="https://arxiv.org/abs/1609.08144">neural machine translation</a> in 2016. Over the years I’ve heard much made about how this is the end of translation and interpretation as professions. I suspect the people who say such things have never actually worked with translator or interpreter. The emblematic example I’ve encountered is “I went on holiday to Japan and we used Google Translate everywhere, there’s no need to hire an interpreter or learn Japanese anymore”. While this undoubtedly speaks for the usefulness of current machine translation technology, the second half of the sentence calls for some scrutiny, particularly “anymore”. I feel confident in asserting that people who say this would not have hired a translator or learned Japanese in a world without Google Translate; they’d have either not gone to Japan at all, or gone anyway and been clueless foreigners as tourists are wont to do.</p>
<p>Indeed it turns out <a href="https://www.npr.org/sections/planet-money/2024/06/18/g-s1-4461/if-ai-is-so-good-why-are-there-still-so-many-jobs-for-translators">the number of available job opportunities for translators and interpreters has actually been increasing</a>. This is not to say that the technology isn’t good, I think it’s pretty close to as good as it can be at what it does. It’s also not to say that machine translation hasn’t changed the profession of translation: in the article linked above, Bridget Hylak, a representative from the American Translators Association, is quoted as saying “Since the advent of neural machine translation (NMT) around 2016, which marked a significant improvement over traditional machine translation like Google Translate, we [translators and interpreters] have been integrating AI into our workflows.”</p>
<p>To explain this apparent contradiction, we need to understand what it is translators actually do because, like us programmers, they suffer from having the nature of their work consistently misunderstood by non-translators. The laity’s image of a translator is a walking dictionary and grammar reference, who substitutes words and and grammatical structures from one language to another with ease, the reality is that a translators’ and interpreters’ work is mostly about ensuring context, navigating ambiguity, and handling cultural sensitivity. This is what Google Translate cannot currently do.</p>
<p>To give a simple example, Norwegian is an extremely closely related language to English and should be an easy translation candidate. The languages share a tonne of cognates, very similar grammar, and similar cultural context; even the idioms tend to translate verbatim. Yet there remain important cultural differences, and a particularly friction-prone one is Norwegian’s lack of polite language. It’s technically possible to say please in Norwegian (vær så snill, or vennligst), but Norwegians tend to prefer blunt communication, and these are not used much in practice. At the dinner table a Norwegian is likely to say something like “Jeg vil ha potetene” (literally “I will have the potatoes”, which sounds presumptuous and haughty in English) where a brit might say “Could I please have some potatoes?”. A good interpreter would have the necessary context for this (or ask for clarification if they’re not sure) and provide a sensitive translation, Google Translate just gives the blunt direct translation. You can probably work past such misunderstandings at dinner with your foreign in-laws (and people do), but it should be apparent why it’s inadvisable to <a href="https://web.archive.org/web/20170811181816/http://www.businessinsider.com/teesside-magistrates-court-forced-to-rely-on-google-translate-because-it-had-no-interpreter-2017-8">subsititute Google Translate for an interpreter at a court hearing</a>. And Norwegian is an easy case. Returning to our tourists, Japanese has wildly different grammar to English, including things like omitting subjects from sentences where it’s apparent from context. In many of these cases you can’t construct a grammatical English sentence without a subject, so Google translate will make one up. Would you be comfortable with a computer inserting a made up subject into your sentence?</p>
<p>All this is not to say Google Translate is doing a bad job. Were I given “Jeg vil ha potetene” with no context or ability to clarify and asked to translate it to English, I’d give the same answer. Maybe the person does want to be rude, how should I know? As a bilingual, I actually do make heavy use of Google Translate, but my use case isn’t “Here’s a block of text, translate it for me”. Instead I have more specific and subtle workflows like “I already know what I want to say, how to say it, and can navigate cultural nuance, but I’m not happy with my wording, I’d like to see the most statistically likely way someone else might phrase this” (A task language models really excel in, as it turns out). I suspect this is what Bridget Hylak meant when she said she has been integrating AI into her workflows (though I also suspect her tools and workflows are more sophisticated than mine).<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p>
<p>It’s a similar story for programming. I think it’s even fair to characterise us as translators, just from squishy humans that speak in ambiguity and cultural nuance, to computers that deal only in absolutes.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> There’s the added complication that we create new abstractions a lot more aggressively in programming languages, and that’s probably why it took machine translation to programming languagues a little while to catch up to machine translation between natural languages, but Big Tech™ chucked all of open source into a wood chipper, and we’re there now.</p>
<p>For what it’s worth, I don’t think it’s inconceivable that some future form of AI could handle context and ambiguity as well as humans do, but I do think we’re at least one more <a href="">AI winter</a> away from that, especially considering that today’s AI moguls seem to have no capacity for nuance, and care more about their tools appearing slick and frictionless than providing responsible output.</p>



</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs pose an interesting problem for DSL designers (187 pts)]]></title>
            <link>https://kirancodes.me/posts/log-lang-design-llms.html</link>
            <guid>44302797</guid>
            <pubDate>Tue, 17 Jun 2025 19:17:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kirancodes.me/posts/log-lang-design-llms.html">https://kirancodes.me/posts/log-lang-design-llms.html</a>, See on <a href="https://news.ycombinator.com/item?id=44302797">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<main id="content">
<div id="outline-container-Programming%20Language%20Design%20in%20the%20Era%20of%20LLMs%3A%20A%20Return%20to%20Mediocrity%3F">
<h2 id="Programming%20Language%20Design%20in%20the%20Era%20of%20LLMs%3A%20A%20Return%20to%20Mediocrity%3F">Programming Language Design in the Era of LLMs: A Return to Mediocrity?<br><span><time datetime="2025-06-17">17 Jun, 2025</time></span>&nbsp;&nbsp;&nbsp;<span><span>programming_languages</span></span>&nbsp;<span><span>code</span></span>&nbsp;<span><span>llm</span></span>&nbsp;<span><span>perspectives</span></span></h2>
<div id="text-Programming%20Language%20Design%20in%20the%20Era%20of%20LLMs%3A%20A%20Return%20to%20Mediocrity%3F">
<p>
The most exciting part of Programming Languages (PL) research for me
has always been in Programming Language Design.
</p>

<p>
By carefully crafting a language with a syntax and semantics tailored
for a specific domain, PL designers can provide an interface for end
users that seamlessly aligns with the sensibilities and intuitions of
practitioners, allowing users to focus on the "interesting" parts of a
problem and tackle larger and more complex problems.
</p>

<p>
Instead of writing a verbose sequence of API calls to display a dialog
to a user in a video game:
</p>
<div>
<pre><code><span># </span><span>example code for a VN</span>
character.draw<span>(</span><span>"alice"</span>, character.LEFT, 0.1<span>)</span>
character.draw<span>(</span><span>"bob"</span>, character.RIGHT, 0.1<span>)</span>
character.say<span>(</span><span>"alice"</span>, <span>"hello there!"</span><span>)</span>
character.say<span>(</span><span>"bob"</span>, <span>"hi!"</span><span>)</span>
character.state<span>(</span><span>"alice"</span>, <span>"sad"</span><span>)</span>
character.say<span>(</span><span>"alice"</span>, <span>"did you hear the news?"</span><span>)</span>
</code></pre>
</div>
<p>
A DSL instead allows designers to focus on the <i>high-level</i> of what the conversation should be:
</p>
<div>
<pre><code><span># </span><span>example DSL for dialog</span>
  <span>[</span> alice @ left <span>in</span> 0.1, bob @right <span>in</span> 0.1  <span>]</span>
alice: hello there!
bob: hi!
alice<span>[</span>sad<span>]</span>: did you hear the news?...
</code></pre>
</div>
<p>
By encoding the "common sense rules" of a domain into the language
itself, we can make writing incorrect programs impossible, and
eliminate cognitive load and minimise the surface area for bugs and exploits. 
</p>

<blockquote>
<p>
A DSL for every domain.  When you have eliminated all that is
incorrect, then whatever remain, however complex, esoteric or
convoluted, simply must be correct.
</p>
</blockquote>

<p>
This has been a fun, exciting and impactful thread of research for the
past several decades, but it would be remiss of me at this point to
not mention the "e-<b>LLM</b>-ephant" in the room. It has only been a few
years, and LLMs and LLM-generated code has already permeated widely
across the software ecosystem and continuously forces developers to
reevaluate their preconceptions of what is and isn't possible for a
machine to generate. Namely, this also includes problems that we might
previously have sought to tackle with language design (eliminating
boilerplate, capturing conventions or common sense etc.).
</p>

<p>
This emerging landscape holds a lot of potential, and there are many
interesting questions in asking how LLMs can contribute to software
development, but as I watch, I am also noticing a worrying trend of
LLM developments supplanting advances and interest in the design of
DSLs: why craft a DSL that eliminates all boilerplate when an LLM can
generate whatever code you need?
</p>

<p>
Is there a future for Language Design in this new era of LLMs? The
point of this blog post is to present some thoughts I've been thinking
about in this emerging space, prompt for discussion, and outline some
potential ways forward language design can co-exist and collaborate
with the advances in LLMs.
</p>
</div>
<div id="outline-container-The%20LLM%20Problem%2C%20or%2C%20rather%2C%20Everything%20is%20Easier%20in%20Python">
<h3 id="The%20LLM%20Problem%2C%20or%2C%20rather%2C%20Everything%20is%20Easier%20in%20Python">The LLM Problem, or, rather, Everything is Easier in Python<br></h3>
<div id="text-The%20LLM%20Problem%2C%20or%2C%20rather%2C%20Everything%20is%20Easier%20in%20Python">
<p>
Let's start with what I see as the biggest problem that the
introduction of LLMs is presenting to language design: <b>everything is
easier in Python</b>.
</p>

<p>
That's a little hyperbolic, but what I'm really getting at here is
that LLMs have been consistently found to have substantially higher
efficiacies when operating in programming languages that are well
represented within their training set – think languages like Python,
Javascript, Typescript etc.
</p>


<figure id="org0a469f1">
<img src="https://kirancodes.me/images/llm-comparison-graph.png" alt="llm-comparison-graph.png">

</figure>

<p>
For example, the above graph taken from the paper <a href="https://dl.acm.org/doi/abs/10.1145/3689735">"Knowledge Transfer
from High-Resource to Low-Resource Programming Languages for Code
LLMs"</a> (2024) shows the performance of one of the LLM models
(StarCoderBase-15B) on solving programming tasks in several
languages against the proportion of the training data represented by
files from that language.
</p>

<p>
The paper presents a technique for improving the performance of these
"low-reseource" languages by synthetically generating data, and so the
upward lines in this graph represent improvements after fine-tuning
using this data, but such transformations have only been applied to
smaller models, and certainly nothing at the scale of the production
models (ChatGPT, CoPilot, Gemini, Claude etc.) that are the most
actively used nowadays.
</p>

<p>
Looking at the original points in this data, the graph plots a bleak
picture: as languages become more niche and specific, the performance
of these models drops off a cliff and becomes abysmal — and bear in
mind, even these "low-resource" langauges themselves are in their own
right, production, industrial systems with millions of users; just not
producing enough code for the LLM to do well on them.
</p>

<blockquote>
<p>
If the performance of LLMs drop so sharply even for these general
purpose languages, then what can anyone expect from running an LLM on
a domain specific language?
</p>
</blockquote>

<p>
Suddenly <b>the opportunity cost for a DSL has just doubled</b>: in the land
of LLMs, a DSL requires not only the investment of build and design
the language and tooling itself, but the end users will have to
sacrifice the use of LLMs to generate any code for your DSL.
</p>

<p>
This brings me to my biggest fear moving forward: will DSLs stagnate?
Will anyone bother writing DSLs if using a niche language forces them
to elide any use of LLMs? or has the barrier to entry to DSL design
simply just jumped up, where now developers will have to work extra
hard to build DSLs that justify losing the ability to use LLMs with
your DSL?
</p>
</div>
</div>
<div id="outline-container-Emerging%20Directions%20of%20Language%20Design%20in%20the%20Land%20of%20LLMs">
<h3 id="Emerging%20Directions%20of%20Language%20Design%20in%20the%20Land%20of%20LLMs">Emerging Directions of Language Design in the Land of LLMs<br></h3>
<div id="text-Emerging%20Directions%20of%20Language%20Design%20in%20the%20Land%20of%20LLMs">
<p>
Okay, so with the doomer-posting out of the way, in this section I
want to take a little bit more of a more optimistic perspective and
think about ways in which language design might evolve and adjust to
work in cooperation with LLMs.
</p>

<p>
So far, there are three interesting directions that I immediately see
for the future, but if you have more I'd love to hear about them!
</p>
</div>
<div id="outline-container-Language%20Design%20Direction%201%3A%20Teaching%20LLMs%20about%20DSLs%20%28through%20Python%3F%29">
<h4 id="Language%20Design%20Direction%201%3A%20Teaching%20LLMs%20about%20DSLs%20%28through%20Python%3F%29">Language Design Direction 1: Teaching LLMs about DSLs (through Python?)<br></h4>
<div id="text-Language%20Design%20Direction%201%3A%20Teaching%20LLMs%20about%20DSLs%20%28through%20Python%3F%29">
<p>
Okay, so the problem with using LLMs on DSLs is that, by their very
nature, the syntax and semantics of a DSL will differ substantially
from general programming languages. This means that without further
context, it can be challenging for an LLM to understand what
constructs in a DSL mean and how they should be used together to
achieve different programming tasks…
</p>

<p>
So… how about we give them that context?
</p>

<p>
I've seen a trend in recent papers such as <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/48bb60a0c0aebb4142bf314bd1a5c6a0-Paper-Conference.pdf">Verified Code Transpilation
with LLMs</a> (2024) where researchers have had success in generating
expressions in niche languages (in this case, logical invariants) by
instead asking the LLM to generate expressions in a well-known
language (in this case, Python), and manually translating to the
obscure language of interest.
</p>

<p>
In the mentioned paper, the authors want to use LLMs to automatically
transpile tensor-processing code into different DSLs. In order to
ensure that the code is correct, they also ask the LLM to generate
invariants that can be used to prove equivalence of both programs:
</p>
<div>
<pre><code><span># </span><span>example invariant from the paper Verified Code Transpilation with LLMs</span>
<span>def</span> <span>invariant_outer</span><span>(</span>row, col, b, a, out<span>)</span>:
   <span>return</span> row <span>&gt;=</span> 0 <span>and</span> row <span>&lt;=</span> <span>len</span><span>(</span>b<span>)</span> <span>and</span>
      out <span>==</span> matrix_scalar_sub<span>(</span>255, matrix_add<span>(</span>b<span>[</span>:i<span>]</span>, a<span>[</span>:i<span>]</span><span>)</span>
</code></pre>
</div>
<p>
The key trick in this paper is the use of Python as an intermediate
language, where the authors ask LLMs to generate code in restricted
subsets of python first, and then write programs to "lift" these
python expressions into their DSLs of choice. In this way, the authors
can effectively use LLMs to generate code for bespoke DSLs without
having to do expensive fine-tuning steps or retraining the model.
</p>

<p>
Generalising this idea and broadening to the problem of language
design in the big, the question I'd like to pose is, can we do this
kind of translation automatically? For example, can we create DSL
design frameworks that also come with LLM-friendly python descriptions
of their semantics? Maybe we can produce frameworks that will test
that these python encodings have the same behaviour as the code they
model, can we automatically generate the python descriptions from the
implementation of the DSL itself?
</p>
</div>
</div>
<div id="outline-container-Language%20Design%20Direction%202%3A%20Bridging%20Formal%20and%20Informal%20with%20LLMs%20in%20DSLs">
<h4 id="Language%20Design%20Direction%202%3A%20Bridging%20Formal%20and%20Informal%20with%20LLMs%20in%20DSLs">Language Design Direction 2: Bridging Formal and Informal with LLMs in DSLs<br></h4>
<div id="text-Language%20Design%20Direction%202%3A%20Bridging%20Formal%20and%20Informal%20with%20LLMs%20in%20DSLs">
<p>
Taking another stab at the problem, another interesting direction that
I see at the intersection of DSLs and LLMs is in investigating new
ways of designing DSLs that work with LLM-based coding workflows.
</p>

<p>
Let me start with a brief interlude on how I have been using LLMs in
my own work, and how they have changed how I write certain kinds of
code – namely, scripts.
</p>

<p>
A lot of my programming work on a day-to-day basis involves working on
the internals of various verification systems. For these kinds of
systems, all the code is pretty complex and intricate, and I really
need to write every line myself. In these situations, LLMs really
aren't that helpful and any LLM-generated code usually fails to
maintain important invariants of the system and use the appropriate
APIs.
</p>

<p>
In contrast, scripts, are things that I write fairly infrequently, are
usually one of, and something where using LLMs has substantially
changed the way in which I write these programs.
</p>

<p>
Here's an example of a prompt that I recently wrote to generate some
Python code to do some basic data analysis:
</p>
<blockquote>
<ul>
<li>write a massively parallel script that iterates through all thy
files in <code>afp-versions</code>, runs the function <code>split_file</code> which you
pass in the text contents of the file which returns a pandas df with
the columns <code>name</code>, <code>start_line</code>, <code>end_line</code>, <code>first_word</code></li>

<li>for each file, record: version (name of immediate subdir under
<code>afp-versions</code> that we are in), and project, (the thy files will be
under a subdir thys after the verison, so the project is the
immediate subdir under that. (extend all rows in the df returned by
<code>split_file</code> with these params)</li>

<li>iteratively merge all of these files into a single dataframe
incrementally and with restarting, and show progress using tqdm, and
use threadpool for parallelism</li>
</ul>
</blockquote>
<p>
Now the description above mostly explains what this code was meant to
do, and the LLM generated code did exactly what I needed it to do.
</p>

<p>
Now, the interesting thing about this snippet from a language design
perspective is that it generates an "incomplete" program – in
particular, I don't ask the LLM to generate the function <code>split_file</code>,
and instead just give a specification for what its inputs and outputs
will be and as relevant to the rest of the task.
</p>

<p>
In a broad sense, when I'm interacting with the LLM for these kinds of
scrappy one-off scripts, I'm outlining the high level plan, and asking
the LLM to generate the glue code, and then manually implementing the
"interesting" part of the problem myself.
</p>

<p>
From a language design perspective, the question that I'd like to pose
from these experiences is, how can we incorporate these kinds of
workflows into a DSL? Namely, how can we bridge the gap between the
formal and informal? My manually written code is in the realm of
"formal", and my textual prompt is in the realm of "informal", and in
this snippet, I do that by encoding a specification of the formal in
the informal as a text component. Can we do this automatically? Can we
build DSLs that integrate seamlessly with informal text? Maybe
automatically generating the natural language specifications based on
the types/analysis that the DSL itself does?
</p>
</div>
</div>
<div id="outline-container-Language%20Design%20Direction%203%3A%20Language%20Design%20for%20Verified%20LLM%20Synthesis">
<h4 id="Language%20Design%20Direction%203%3A%20Language%20Design%20for%20Verified%20LLM%20Synthesis">Language Design Direction 3: Language Design for Verified LLM Synthesis<br></h4>
<div id="text-Language%20Design%20Direction%203%3A%20Language%20Design%20for%20Verified%20LLM%20Synthesis">
<p>
This is probably the most actively being researched of the directions
I've covered in this post so far, but another interesting area for
Language Design to move towards following the advent of LLMs is
towards the design of specification langauges.
</p>

<p>
So at a high level, since LLMs have been picking up steam, there has
been a cottage industry of researchers jumping into the fray of
investigating whether we can use verification langauges, such as Dafny
or Boogie and so on, to be able to verify the output of LLM-generated
code. The first paper I saw in this direction was <a href="https://dl.acm.org/doi/abs/10.1145/3643763">"Towards AI-Assisted
Synthesis of Verified Dafny Methods"</a> (2024), though I'm sure there are
now many many more that have been published, and several more in the
works.
</p>

<div>
<pre><code><span>// </span><span>Example from Towards AI-Assisted Synthesis of Verified Dafny Methods</span>
<span>method</span> <span>FindSmallest</span><span>(</span><span>s</span>: <span>array</span>&lt;<span>int</span>&gt;<span>)</span> <span>returns</span> <span>(</span><span>min</span>: <span>int</span><span>)</span>
  <span>requires</span> s.Length &gt; 0
  <span>ensures</span> <span>forall</span> i :: 0 &lt;= i &lt; s.Length ==&gt; min &lt;= s<span>[</span>i<span>]</span>
  <span>ensures</span> <span>exists</span> i :: 0 &lt;= i &lt; s.Length &amp;&amp; min == s<span>[</span>i<span>]</span> <span>{</span>
 ...
<span>}</span>
</code></pre>
</div>
<p>
Instead of just asking the model to generate some code, which may be
complex, intricate and contain bugs, the authors suggest instead
asking the model to generate programs in verified languages such as
Dafny with specifications. This way, users can just look at the
specifications to understand what the program does and do not need to
understand the LLM code unless the verification fails.<sup><a id="fnr.1" href="#fn.1" role="doc-backlink">1</a></sup>
</p>

<p>
From a language design perspective, the interesting questions in this
domain consist of asking how we can a) integrate these specifications
into DSLs? and b) how we can better design our verification DSLs to
capture properties of interest in bespoke domains – of course, the
properties you care about for a dialog DSL are going to be quite
different from one for maybe a packet routing DSL. Can we
automatically build specification langauges from the implementation of
our language DSL?
</p>
</div>
</div>
</div>
<div id="outline-container-Conclusion%3A%20Language%20Design%20for%20LLMs">
<h3 id="Conclusion%3A%20Language%20Design%20for%20LLMs">Conclusion: Language Design for LLMs<br></h3>
<div id="text-Conclusion%3A%20Language%20Design%20for%20LLMs">
<p>
To conclude this article, I think LLMs pose an interesting problem for
the DSL designers – the opportunity cost for using niche languages is
now substantially increasing, and so we, as language designers, will
be held to a higher standard to justify the use of our DSLs. At the
same time, they certainly have radically changed the space of what is
possible, and also opened up several interesting problems to explore
moving forwards.
</p>

<p>
Main takeway, language design and DSLs will have to adjust for this
crazy new world we're living in, and if we're not careful, there's a
very real chance the space of language design will stagnate, and we'll
lose the diversity of fun and interesting DSLs and everyone will just
end up writing Python…
</p>
</div>
</div>
</div>
</main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Iran asks its people to delete WhatsApp from their devices (318 pts)]]></title>
            <link>https://apnews.com/article/iran-whatsapp-meta-israel-d9e6fe43280123c9963802e6f10ac8d1</link>
            <guid>44302752</guid>
            <pubDate>Tue, 17 Jun 2025 19:12:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/iran-whatsapp-meta-israel-d9e6fe43280123c9963802e6f10ac8d1">https://apnews.com/article/iran-whatsapp-meta-israel-d9e6fe43280123c9963802e6f10ac8d1</a>, See on <a href="https://news.ycombinator.com/item?id=44302752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>Iranian state television on Tuesday afternoon urged the country’s public to remove the messaging platform WhatsApp from their smartphones, alleging the app — without offering specific evidence — gathered user information to send to Israel.</p><p>In a statement, WhatsApp said it was “concerned these false reports will be an excuse for our services to be blocked at a time when people need them the most.” WhatsApp uses end-to-end encryption, meaning a service provider in the middle can’t read a message.</p><p>“We do not track your precise location, we don’t keep logs of who everyone is messaging and we do not track the personal messages people are sending one another,” it added. “We do not provide bulk information to any government.”</p><p><span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/privacy-encryption-signal-whatsapp-9faf31ed3411bc5b7cab0647b4ab224d">End-to-end encryption</a></span> means that messages are scrambled so that only the sender and recipient can see them. If anyone else intercepts the message, all they will see is a garble that can’t be unscrambled without the key.</p>
    
<p>WhatsApp is owned by Meta Platforms, the parent company of Facebook and Instagram. </p><p>Iran has blocked access to various social media platforms over the years but many people in the country use proxies and virtual private networks, or VPNs, to access them. It banned WhatsApp and Google Play in 2022 during mass protests against the government over the death of a woman held by the country’s morality police. That ban was <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/iran-social-media-whatsapp-google-d886b47c427f33f96fb85e7c78d0b831">lifted late last year</a></span>.</p><p>WhatsApp had been one of Iran’s most popular messaging apps besides Instagram and Telegram. </p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From SDR to 'Fake HDR': Mario Kart World on Switch 2 (103 pts)]]></title>
            <link>https://www.alexandermejia.com/from-sdr-to-fake-hdr-mario-kart-world-on-switch-2-undermines-modern-display-potential/</link>
            <guid>44302704</guid>
            <pubDate>Tue, 17 Jun 2025 19:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.alexandermejia.com/from-sdr-to-fake-hdr-mario-kart-world-on-switch-2-undermines-modern-display-potential/">https://www.alexandermejia.com/from-sdr-to-fake-hdr-mario-kart-world-on-switch-2-undermines-modern-display-potential/</a>, See on <a href="https://news.ycombinator.com/item?id=44302704">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

		
					<main id="main">
				

<article id="post-385" itemtype="https://schema.org/CreativeWork" itemscope="itemscope">

	
	
<div ast-blocks-layout="true" itemprop="text">

		
		
<p>Nintendo’s Switch 2 launched on <strong>June 5th 2025</strong> with <em>Mario Kart World</em> headlining the platform and <em>on paper</em> showcasing its new <strong>4K60 + HDR</strong> output pipeline. That promise lands in a market where HDR is the consumer standard: analysts value the global HDR TV segment at <em>≈ <a href="https://www.linkedin.com/pulse/global-hdr-tv-market-comprehensive-analysis-drivers-zpkke/" target="_blank" rel="noopener" title="">US $150B in 2024</a></em> and project <em>US $250 B by 2033</em>. &nbsp;Consumer TVs with HDR functionality started shipping in 2015, and seeing that <a href="https://www.prweb.com/releases/npd-us-tv-purchasers-more-motivated-by-screen-size-and-picture-quality-than-ever-before-810878558.html" target="_blank" rel="noopener" title="most consumers replace their TV every 6.4">most consumers replace their TV every 6.4</a> years this means a high percentage console owners in 2025 now game on HDR capable screens.</p>



<p>Yet, as I’ll show in this article, <em>Mario Kart World</em> surfaces an industry-wide problem: <strong>SDR-first authoring with a last-minute tonemap hack ruins the experience.&nbsp; </strong>I’d figure that SDR games that masquerade as HDR, or “<em>fake HDR</em>” coined by some more incendiary YouTubers, would have been a trend left back in 2020, but here we are in 2025 with a new generation of consoles with a headliner game that still reduces color gamut to SDR, and has no more dynamic range than the SDR presentation.&nbsp; In this article I’ll show my evidence of why I think this game is a “<em>fake HDR</em>” title, and what developers can do to avoid this in the future.</p>



<p>I approach this critique with some experience. <a href="https://www.alexandermejia.com/dolby-vision-on-xbox-series-x/" target="_blank" rel="noopener" title="">I led Dolby Vision for Games program</a> on Xbox Series X|S, helping developers ship Dolby Vision masters on <em>Godfall</em>, <em>Halo Infinite</em>, and <em>COD Warzone</em>, and I’m consulting on more titles still under NDA. Those projects taught me that <strong>HDR excellence starts at the very first art review—not in the final weeks of polish</strong>.</p>



<h2><strong>Test Methodology</strong></h2>



<p>Here is my capture chain that I’m gathering this information with.&nbsp; If you think I’m doing something wrong I’d love to know.</p>



<p><strong>Hardware &amp; capture path</strong></p>



<ul>
<li>Launch Model Nintendo Switch 2 over HDMI on the official dock to –&gt;</li>



<li>Blackmagic DeckLink 4K mini.</li>



<li>Captured in BlackMagic Media Express (3.8.1) in ProRes 4444 on Mac OS (15.1) Mac Studio M1 Ultra</li>



<li>Viewed on Asus ProArt PA27UCX mastering monitor (2,000 nits, Rec.2020 PQ, hardware calibrated)</li>



<li>Davinci Resolve to analyze captures</li>
</ul>



<h3>How can you view this best at home?</h3>



<p>Images posted on this site are in HDR in the AVIF format with the full rec.2020 PQ image data.&nbsp; It’s lossy, but should give a good representation of the HDR experience as long as you have a  display that goes up to 1,000 nits to view the full dynamic range of the scene.&nbsp; You should have HDR enabled on your OS.</p>



<p>I recommend viewing this on a Macbook Pro laptop with the built-in Pro Display XDR, or a high end PC HDR monitor (Similar to the ProArt I’ve mentioned earlier, but there are many more displays of similar quality or better now), or an LG C4/G4 or greater TV in the Cinema mode picture preset.</p>



<p>Google Chrome and other Chromium based browsers seem to have the best viewing experience.&nbsp; Safari will be adding HDR image support later, but as of this writing, it’s not out of beta.</p>



<h3>Capture Procedure</h3>



<ol start="1">
<li>Set Nintendo Switch 2 Console settings to 4k 60, HDR, limited range</li>



<li>Closed the game if it was running already (Some titles on consoles set their HDR tonemap only once at startup, this bypasses any issues when swapping the OS mapping during testing)</li>



<li>Opened the game from a clean start and Ran a single lap in Mario Bros. Circuit at 50CC</li>



<li>Recorded using BlackMagic Media Express into ProRes 4444 Codec (This is a 16-bit 4:4:4 capture medium, however the source content is only 10-bpc)</li>



<li>Recorded a lap pass utilizing steps 1-4.&nbsp; Each time changing the Switch 2 “Maximum Brightness” HDR calibration: <strong>205 nits → 1,000 nits → 2,000 nits → 10,000 nits</strong> (I verified console UI allows the full PQ range utilizing Davinci Resolve).</li>



<li>Captured gameplay, and analyzed in Davinci Resolve Waveform Monitor</li>
</ol>



<h2>Findings</h2>



<h3>Static tone mapping and a clamped 1,000 nit ceiling</h3>



<p>The game’s peak brightness at console 205 nits, is about 205 nits.&nbsp; This is behaving like it should</p>



<div><figure><img decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/205Nits.avif ,https://www.alexandermejia.com/wp-content/uploads/2025/06/205Nits.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/205Nits.avif 360w" sizes="(max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/205Nits.avif" alt="Mario Kart World screenshot at max 205 nits" width="1920" height="1080" title="Mario Kart World at 205 nits" role="img"></figure></div>



<figure><img decoding="async" width="1024" height="542" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/205nit_histogram-1024x542.png" alt="" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/205nit_histogram-1024x542.png 1024w, https://www.alexandermejia.com/wp-content/uploads/2025/06/205nit_histogram-300x159.png 300w, https://www.alexandermejia.com/wp-content/uploads/2025/06/205nit_histogram-768x407.png 768w, https://www.alexandermejia.com/wp-content/uploads/2025/06/205nit_histogram.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The game’s peak brightness at console 2,000 nits is about 900 nits.&nbsp; This is likely the intended behavior but makes the art look washed out as clouds do not have defined detail.</p>



<div><figure><img loading="lazy" decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/2000Nits.avif ,https://www.alexandermejia.com/wp-content/uploads/2025/06/2000Nits.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/2000Nits.avif 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/2000Nits.avif" alt="Mario Kart world at Max 2000 nits." width="1920" height="1080" title="2000Nits" role="img"></figure></div>



<div><figure><img decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/2000nithistogram-1024x542.jpg ,https://www.alexandermejia.com/wp-content/uploads/2025/06/2000nithistogram.jpg 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/2000nithistogram.jpg 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/2000nithistogram-1024x542.jpg" alt="" width="1280" height="678" title="2000nithistogram" loading="lazy" role="img"></figure></div>



<p>Even when the user cranks the console brightness to 10,000 nits, <strong>captured peaks in game never exceed ~950 nits</strong></p>



<div><figure><img loading="lazy" decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/10kNits.avif ,https://www.alexandermejia.com/wp-content/uploads/2025/06/10kNits.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/10kNits.avif 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/10kNits.avif" alt="Mario Kart World screenshot taken with console at 10,000 nits peak brightness" width="1920" height="1080" title="10kNits Mario Kart World" role="img"></figure></div>



<figure><img loading="lazy" decoding="async" width="1024" height="542" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/10knit_histogram-1024x542.jpg" alt="" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/10knit_histogram-1024x542.jpg 1024w, https://www.alexandermejia.com/wp-content/uploads/2025/06/10knit_histogram-300x159.jpg 300w, https://www.alexandermejia.com/wp-content/uploads/2025/06/10knit_histogram-768x407.jpg 768w, https://www.alexandermejia.com/wp-content/uploads/2025/06/10knit_histogram.jpg 1280w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Nintendo’s own test image peaks at only ~500 nits even if you set 10,000 nits peak brightness.&nbsp; Not a good sign that they took HDR seriously.</p>



<div><figure><img loading="lazy" decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/testimage_nx2.avif ,https://www.alexandermejia.com/wp-content/uploads/2025/06/testimage_nx2.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/testimage_nx2.avif 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/testimage_nx2.avif" alt="The test Image Nintendo Switch 2 shows when at 10,000 nits peak brightness selected." width="1920" height="1080" title="Nintendo Switch 2 HDR Test Image" role="img"></figure></div>



<figure><img loading="lazy" decoding="async" width="1024" height="542" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/testhitogram-1024x542.jpg" alt="" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/testhitogram-1024x542.jpg 1024w, https://www.alexandermejia.com/wp-content/uploads/2025/06/testhitogram-300x159.jpg 300w, https://www.alexandermejia.com/wp-content/uploads/2025/06/testhitogram-768x407.jpg 768w, https://www.alexandermejia.com/wp-content/uploads/2025/06/testhitogram.jpg 1280w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>I’m unable to peek into the math in how they are tonemapping from FP16 linear space to the Rec.2100 Display space, but from my testing I can safely assume they are using a single linear tonemap which scales some fixed function math to go from FP16 -&gt; Rec.2100 with a scale factor from the console brightness setting.&nbsp;</p>



<p>It’s likely that the game’s creative intent is designed to only hit ~1,000 nits of brightness.&nbsp; This would be OK if they were limited in resources and could only get their hands on 1,000 nit peak brightness reference displays and needed to be conservative in their mastering. But why does the game not achieve 1,000 nits peak brightness when the Console max brightness is set to 2,000 or 10,000? This suggests an oversight in how their HDR tonemapper works, or they utilized scaling that is not dynamic. &nbsp;</p>



<h3>Single Slope Tone Mapping</h3>



<p>If the game’s true creative intent is to peak at 1,000 nits, then any console HDR brightness setting over 1,000 nits should have no effect on the games output.&nbsp; This suggests to me that the console peak brightness is just part of the equation they use to scale, and they aren’t considering their own creative intent for HDR.</p>



<p>It’s also possible that they didn’t have enough time to fine tune HDR and their creative intent was not achieved.&nbsp; Typically, with dynamically tonemapped games, extending the console max brightness results in seeing more detail and headroom for objects that are bright, like clouds and skies.&nbsp; In Mario Kart world, this scaling does nothing to reveal any more detail regardless of how the peak brightness setting for HDR is set.</p>



<p>This is likely due to the game being art directed only for SDR.&nbsp; In SDR you typically have to make tradeoffs in how contrasty a scene can feel, between details in the darkest or brightest objects.&nbsp; SDR is limited in dynamic range, you<em> have to give up something</em>. The easiest way to solve this issue is to take your SDR frame buffer and extend it to HDR utilizing your own tone mapping function.&nbsp; Many developers under a time constraint build these in a tool called Davinci resolve, and export a LUT to save time on this transform.&nbsp; It’s not going to give you a true HDR representation, but it’s something a single rendering engineer can cook up and implement in a week that is just a brighter, stretched version of their approved and art directed SDR version of the game.</p>



<p>To test my theory, I can use Davinci Resolve’s Rec.709 to Rec.2020 PQ color space transform and dial in settings that give me nearly identical looks.&nbsp; The clouds seem to be dynamic so some sections may show shadows that aren’t there, but when I stretch the SDR into a 1,000 nit Rec.2100 image, I get an incredibly similar result.</p>



<figure><p><img loading="lazy" decoding="async" id="400" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/a_10knit.avif" alt="" width="1920" height="1080"><img loading="lazy" decoding="async" id="401" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/b_sdrgrade.avif" alt="" width="1920" height="1080"></p><figcaption>Left = HDR capture    Right = Personal SDR inverse tonemapped to HDR</figcaption></figure>



<div>
<div>
<figure><img loading="lazy" decoding="async" width="218" height="300" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/CST_settings-218x300.png" alt="Davinci Resolve settings used to inverse tonemap from SDR to HDR" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/CST_settings-218x300.png 218w, https://www.alexandermejia.com/wp-content/uploads/2025/06/CST_settings-743x1024.png 743w, https://www.alexandermejia.com/wp-content/uploads/2025/06/CST_settings-768x1059.png 768w, https://www.alexandermejia.com/wp-content/uploads/2025/06/CST_settings.png 830w" sizes="auto, (max-width: 218px) 100vw, 218px"></figure>
</div>



<p>Because I can get such a close match utilizing this method, confirms to me that there is one SDR master of the game, and they are extending it by utilizing tone mapper functions from the FP16 linear image into the Rec.2020 PQ space.&nbsp; Simply put, you’re getting an SDR image, stretched into HDR10 in Mario Kart World.</p>
</div>







<p><h3>Wait, Why Do I See Banding In The Sky?</h3></p>



<div><figure><img loading="lazy" decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/Skybanding.avif ,https://www.alexandermejia.com/wp-content/uploads/2025/06/Skybanding.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/Skybanding.avif 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/Skybanding.avif" alt="Banding in the Sky of Mario Kart World in HDR" width="1920" height="1080" title="Skybanding" role="img"></figure></div>



<p>This is fully visible at 1:1 in the ProRes 4444 capture.  I’ve zoomed in to showcase the banding on a smaller image and so that SDR users can see this as well.</p>



<p>Banding over the sky dome remains no matter if viewed in SDR, or HDR10.&nbsp; While this is minor, these issues are more visible in HDR where the image can appear brighter.&nbsp; This can happen due to lower bit-depth precision when blending this texture, or utilizing <a href="https://microsoft.github.io/DirectX-Specs/d3d/D3D12R9G9B9E5Format.html" target="_blank" rel="noopener" title="R9G9B9E5">R9G9B9E5</a> as a back buffer to reduce memory bandwidth which requires extra care like <em><a href="https://en.wikipedia.org/wiki/Dither" target="_blank" rel="noopener" title="dithering">dithering</a></em> needed to cover up these transitions.&nbsp; While I can’t do external testing to determine the exact cause, It’s something an art team would likely catch and fix if they had done art reviews in HDR10, rather than sticking to SDR, where problems like this can be very hard to see, especially on smaller displays.</p>



<h3>Stuck in SDR Color space</h3>



<p>The game’s art style is very colorful and bright, however the game makes no use of the extended color gamut afforded by Rec.2020.&nbsp; Instead all colors are clamped in Rec.709, the same color space you get when in SDR. This is easily seen by just analzing the color gamut from the HDR capture and seeing that no pixel values fall in the extended color gamut triangle.</p>



<div><figure><img decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/HighestColorVolume-1024x673.jpg ,https://www.alexandermejia.com/wp-content/uploads/2025/06/HighestColorVolume.jpg 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/HighestColorVolume.jpg 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/HighestColorVolume-1024x673.jpg" alt="" width="1372" height="902" title="The Highest Color Volume" loading="lazy" role="img"><figcaption>See that Yellow Triangle Labeled Rec.2020? That’s all the color that could be used in game</figcaption></figure></div>



<p>You might look at the HDR representations of this game and think “Wait, the game appears more colorful” and this is because of the <a href="https://en.wikipedia.org/wiki/Hunt_effect_(color)" target="_blank" rel="noopener" title="">Hunt Effect</a>. The Hunt Effect describes how we think a brighter color is more saturated, but in reality, it’s just an optical illusion. &nbsp;&nbsp;This means Nintendo has left another key piece of HDR on the table likely due to their focus on creating the game for SDR first.&nbsp; &nbsp;Their artists are missing out on bright saturated colors, like UI elements, Sparks that fly off the tires, and explosions that could be directed to pop more if they chose to.</p>



<div><figure><img decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/hunteffect-300x201.png ,https://www.alexandermejia.com/wp-content/uploads/2025/06/hunteffect.png 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/hunteffect.png 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/hunteffect-300x201.png" alt="" width="300" height="201" title="The Hunt Effect" loading="lazy" role="img"><figcaption>Each color in this image is the same saturation on the CIE chart, but in different brightness<br>By Tom Axford 1 – Own work, building on this spectrum, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=124441017</figcaption></figure></div>



<h3>Subjective Competitive Analysis</h3>



<p>Just looking at other gameplay content that I’ve mastered before in HDR, wide color gamut can be used to help enhance the Red Sands in this Unreal demo called Rural Australia.&nbsp; I used this several times to showcase how extended color gamut made a large difference in the kinds of visuals you could achieve, even in a desert where you might think extended color gamut wasn’t really necessary.&nbsp;</p>



<div><figure><img decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2024/08/Rural_Austrailia.avif ,https://www.alexandermejia.com/wp-content/uploads/2024/08/Rural_Austrailia.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2024/08/Rural_Austrailia.avif 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2024/08/Rural_Austrailia.avif" alt="" width="1280" height="720" title="Rural_Austrailia" loading="lazy" role="img"></figure></div>



<p>Putting these two gameplay clips side by side showcases what you’re missing when you don’t utilize wide color gamut, and high peak brightness.  You can more easily call out specific details, or emphasize colorful elements.  It’s more stimulating, and in general your consumers will think your game looks better.  If this developer could do this level of work in 2 months, imagine what your game would look like if you built for HDR in the first place.</p>



<figure><video controls="" loop="" preload="auto" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/GodfallUltimate_Vs_MKW-20.5.mp4"></video></figure>



<p><a href="https://www.youtube.com/watch?v=Ym3MKdm0igs" target="_blank" rel="noopener" title="">YouTube Link</a> if high quality video above doesn’t work</p>



<h2>Why Developers Do This: The SDR First Pipeline Trap</h2>



<p>Most teams still:</p>



<ol start="1">
<li><strong>Author lighting in sRGB / Rec. 709</strong> Often not viewing HDR until it’s too late.</li>



<li>Hold daily art reviews in SDR. Locking in their 0-1 range in Rec.709 making moving to wider color gamut requiring lots of rework.</li>



<li>During Production lots of art time is spent mapping the final FP16 image into SDR.</li>



<li>Then it’s thrown over the fence to rendering engineers and technical artists to make the SDR image look good in HDR.&nbsp; I hope 2-4 weeks is enough to polish the whole HDR game!</li>
</ol>



<p>I get it.&nbsp; Developers have been living the last 30+ years of their life in SDR. Moving to HDR requires different practices, and some re-learning.&nbsp; But when <a href="https://www.playstationlifestyle.net/2019/05/10/esa-essential-facts-2019-report-video-games/#:~:text=price%2C%20the%20largest%20factor%20when,with%20past%20games%20in%20the" target="_blank" rel="noopener" title="">Gamers in ESA surveys</a> report that the quality of the graphics being the #2 factor in deciding when to purchase a game, you better maximize your HDR rendering, as you can increase the visual quality of your game without having to re-author new assets if done correctly.</p>



<p>In my experience once artists view their game in HDR, major art issues surface.&nbsp; It’s not that HDR creates these issues, but it reveals things that would normally be hidden by lower dynamic range and lower color volume like the banding issue above. &nbsp;Technical Artists have to compromise, usually in a rush by making a single static tonemapping curve for HDR, and lock the color gamut to Rec.709 intentionally bringing their game look closer to SDR.</p>



<p>On paper this ticks the HDR box with minimal risk, but why even bother spending the time on HDR if you’re going to make it look like a stretched version of SDR?&nbsp; TVs already do this by extending maximum brightness, and they do it in a way the user typically prefers.&nbsp; TV manufacturers have gotten really good at making SDR images stretched into pseudo HDR as they’ve been doing this for decades now.</p>



<p>PC gamers have been complaining about these HDR issues for years now. &nbsp;Check any Reddit forum or YouTuber complaining about <em>Fake HDR</em>.&nbsp; Even <a href="https://www.digitaltrends.com/computing/pc-gaming-hdr-problem-a-way-out/" target="_blank" rel="noopener" title="Digital Trends called static HDR “sad, misleading, and embarrassing”"><em>Digital Trends</em> called static HDR “sad, misleading, and embarrassing”</a>.&nbsp; We can do better…</p>



<h2>Why It Matters in 2025</h2>



<ul>
<li><strong>Consumer hardware is good, really good.</strong> – Even mid-range TVs now exceed 1,000 nits and ship with per-frame dynamic tone mapping, Dolby Vision, and HDR10+ capabilities.</li>



<li><strong>Competitive titles are raising the bar</strong> – Games like Call of Duty Warzone, and Godfall: Ultimate Edition are exceeding 1,000 nits peak brightness and have wider color gamuts.&nbsp; I know, I helped them achieve these goals.</li>



<li><strong>HDR is mainstream</strong> – From just a quick browsing of BestBuy, nearly all TVs over 42” are 4K and support HDR. 9<sup>th</sup> gen consoles are shipping with HDR on by default. <em>The majority of your audience is HDR-equipped</em>.</li>



<li><strong>Visuals suffer</strong> – Working SDR first, limits your maximum brightness, limits your color gamut, and limits the impact your wow moments of a sunset, or a truly scary dimly lit dungeon.</li>
</ul>



<h2>What Can Developers Do?</h2>



<p>Developers, specifically art directors and rendering programmers need to take HDR seriously from the beginning.&nbsp;</p>



<ol start="1">
<li><strong>Commit to upgrading your pipelines to Wide Color Gamut at the start</strong>
<ul>
<li>Not all your textures need to be WCG, but you can have a checkbox for textures that exceed rec.709 so they can be processed by your crunching tools.</li>



<li>Too deep into production to make major pipeline changes? &nbsp;Many Developers allow for WCG in their VFX only by using numbers over 1.0 while assuming all their textures are sRGB/Rec.709. This works very well as a first step that doesn’t require you ripping up your texture pipeline but delivering WCG in areas where it has the highest impact like VFX.</li>
</ul>
</li>



<li><strong>Allow your game to have dynamic Range</strong>
<ul>
<li>As a rule of thumb I like to set my high noon daylight scenes to be at least 5x brighter than nighttime at a minimum.&nbsp; Try it out on a reference monitor and bring in all your environment artists and ask which one looks better.</li>



<li>Typically you can leave your SDR scenes for nighttime untouched. These will hit about 50-100 nits brightness.&nbsp;</li>



<li>That means cranking up the sun intensity of your outdoor scenes by about 5x gets you into a ballpark that most embedded tone mappers in TVs can play along with.</li>



<li>VFX needs to be adjusted, make standards for how bright of values your VFX artists should use for emissives like fire, sparks, gunshots, pickups etc.&nbsp; <em>Don’t let obvious things like the blast from a gun be brighter than the sun.</em></li>
</ul>
</li>



<li><strong>Build a Dynamic Tone Mapping service, or utilize ones by Dolby and HDR10+</strong>
<ul>
<li>Because not every display will be capable of hitting 2000+ nits, you should add your own dynamic tone mapping for the best results.</li>



<li>Remove “Auto Iris” or “Automatic Exposure Adjustments” from your game.&nbsp; Let the dynamic tone mapper adjust these based off the user’s display.</li>



<li>This will prevent pulsing on higher end displays, but still ensure that folks on HDR displays that may only peak at 200-300 nits still get a good experience.</li>



<li>Dolby Handles tone mapping down to SDR and has trim controls to adjust it.&nbsp; I know I’m biased towards that solution because I worked on it. Ask developers who have used it how it helped them if you want more data points.</li>
</ul>
</li>



<li><strong>Make HDR review the standard</strong>
<ul>
<li>Supply calibrated HDR monitors to art/lighting.&nbsp; Get more in your studio if budget allows for it.&nbsp;</li>



<li>Do your art reviews on calibrated HDR displays.</li>



<li>HDR offers fantastic dynamic range, color, and creative freedom for your images to look like whatever you want when done well.&nbsp; If SDR looks disappointing compared to HDR then you’re doing it right.&nbsp; (Seriously, do not let your art director reduce the quality of your HDR image in some attempt to make SDR look better, point them to this article or get on a call with them.)</li>
</ul>
</li>
</ol>



<p>If you’re Unreal based, I shipped plugins add Dolby Vision to Unreal Engine projects, even ones designed just for SDR.&nbsp; It solves many of these problems if you’re a smaller team without rendering engineering support.&nbsp; However, you’ll need to contact Dolby to get access to these plugins at the moment.&nbsp;</p>



<h2>What Mario Kart World Could Have Looked like…</h2>



<p>While the tone mapping for the game is baked in pretty tough, I did some color grading in Davinci Resolve by eliminating the 1000 nit ceiling and letting it go to 2000 nits.&nbsp; Clouds do look much better and more realistic; however detail is still lacking in those areas.&nbsp; I can’t magically paint on new detail in an afternoon.</p>



<figure><p><img loading="lazy" decoding="async" id="408" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/base_still.avif" alt="" width="1920" height="1080"><img loading="lazy" decoding="async" id="409" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/graded_v2.avif" alt="" width="1920" height="1080"></p><figcaption>Left == Peak 950 Nit capture     Right == Graded to simulate 2,000 nit capture</figcaption></figure>



<p>I do like the roads and the characters being around 90-110 nits on a high noon sunny day.&nbsp; This feels right to me, but the VFX isn’t bright enough.&nbsp; Because of this they don’t pop and give you the feedback that you’re hitting a blue spark on your drift.&nbsp; Some of the UI elements that should stick out, like a 4,000 nit bright VFX pop when you pick up something. It’s all of these elements that make your game look more impressive even though you’re utilizing the same assets. &nbsp;Experiment with higher brightness and wider color gamut,&nbsp; You can achieve stunning results you just couldn’t achieve in SDR.</p>



<p>You’re already making your game in HDR and utilizing FP16 linear.&nbsp; <em>Don’t just throw the dynamic range all away at the end because you chose to be SDR first.</em></p>



<h2>Conclusion</h2>



<p><em>Mario Kart World</em> reveals that even the highest caliber of developers aren’t taking HDR seriously. The root cause is <strong>SDR first workflows</strong> that tack on a static tone mapper. Players lose, artist intent is capped, and a flagship hardware features are squandered.</p>



<p>If you’re shipping this year and want to avoid a similar post-mortem, let’s talk. I consult studios of any size on <strong>HDR first rendering pipelines, Dolby Vision integration, and dynamic tone-mapping strategies</strong>. Reach out: <a href="mailto:alexander.mejia@human-interact.com" title=""><strong>alexander.mejia@human-interact.com</strong>.</a></p>



<p>The era of checkbox HDR is over—let’s build images that finally exploit the full HDR canvas.</p>

		
		
			</div>

	
</article><!-- #post-## -->

		<!-- #comments -->

			</main><!-- #main -->
			
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Time Series Forecasting with Graph Transformers (108 pts)]]></title>
            <link>https://kumo.ai/research/time-series-forecasting/</link>
            <guid>44301998</guid>
            <pubDate>Tue, 17 Jun 2025 18:05:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kumo.ai/research/time-series-forecasting/">https://kumo.ai/research/time-series-forecasting/</a>, See on <a href="https://news.ycombinator.com/item?id=44301998">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Time series forecasting is a cornerstone in modern business analytics, whether it is concerned with anticipating market trends, user behavior, optimizing resource allocation, or planning for future growth. As such, a wide range of different approaches have been introduced and investigated for forecasting, lately data-driven approaches using machine learning and generative models.</p><p>This blog post will dive into forecasting on graph structured entities, <em>e.g.</em>, as obtained from a relational database, utilizing not only the individual time series as signal but also related information. As most of the world’s data is stored in relational structures, this topic is of particular interest for real world applications. We describe an end-to-end pipeline to perform forecasting using graph transformers and specifically discuss predictive vs. generative paradigms.</p><h2 id="forecasting-on-graph-structured-data">Forecasting on Graph-structured Data</h2><p>Forecasting is the process of making predictions about future events based on historical data and current observations, requiring detecting patterns, trends, and seasonal variations.</p><p>Traditional forecasting methods often treat time series data in isolation, focusing solely on temporal patterns within a single sequence. However, in real-world applications, valuable predictive signals often exist in related data sources. For instance, when forecasting product sales, factors such as marketing campaigns, competitor pricing, or regional economic indicators can significantly impact the accuracy of predictions. Graphs are a natural structure to represent such inter-connected data sources. They represent a set of inter-connected nodes of different entities, where some entities can have time series that can be forecasted. Each node can potentially hold a variety of features that hold important signal for forecasting tasks on other nodes. Further, they lend themselves to a wide arrange of machine learning methods, <em>e.g.</em>, <a href="https://kumo.ai/research/introduction-to-graph-transformers/"><em>Graph Transformers</em></a>.</p><p>A prominent option for obtaining graphs directly from an underlying business problem on a relational database is <a href="https://kumo.ai/research/relational-deep-learning-rdl/"><em>Relational Deep Learning (RDL)</em></a>, which automatically discovers and utilizes cross-table relationships and data in connected tables. The RDL scheme allows to automatically extract a graph structure from the relational database, allowing us to treat timeseries forecasting as a graph learning task. We will use the graph obtained via RDL as an example below. However, our graph forecasting techniques are not limited to graphs obtained via RDL but can be applied on arbitrary forecasting tasks where time series have to be forecasted for a subset of graph nodes.</p><div><p><img alt="" loading="lazy" width="2106" height="488" decoding="async" data-nimg="1" srcset="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F9c7f308dc5e8fe48c1deba1d8967c31c9a081e62-2106x488.png&amp;w=3840&amp;q=75 1x" src="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F9c7f308dc5e8fe48c1deba1d8967c31c9a081e62-2106x488.png&amp;w=3840&amp;q=75"></p></div><p><strong>Example. </strong>Consider the task of forecasting the sales per day for all products stored in a product table (yellow). Further tables containing transactions (blue), customers (green), product marketing (red) can provide additional signals that help solving the task. Using the RDL scheme, we can automatically transform the relational tables into a graph with node features. Then, the task is to perform forecasting on the subset of product nodes via graph machine learning.</p><p><strong>Notation.</strong> We denote the input to our graph forecasting task as a graph <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> with node set <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> and edge set <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, where <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> is a subset of nodes that serve as forecasting entities with a given (past) time series <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>. Additionally, the graph nodes are annotated with arbitrary node features <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, <em>e.g.</em>, encoded row features.</p><h2 id="core-forecasting-framework">Core Forecasting Framework</h2><p>We formulate the core forecasting framework for a single time series <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> of an entity <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> as</p><p>where a function <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> predicts or generates the next time series value <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> given a set of conditioning signals <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, and <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>. The function <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, <em>e.g.</em>, modeled by a Multi-Layer Perceptron (MLP), is shared over all <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>. The conditioning signals serve specific purposes as follows.</p><div><p><img alt="" loading="lazy" width="1761" height="352" decoding="async" data-nimg="1" srcset="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F71df4bc5650b18caf1b27f8180f889f72bee5f0b-1761x352.png&amp;w=1920&amp;q=75 1x, https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F71df4bc5650b18caf1b27f8180f889f72bee5f0b-1761x352.png&amp;w=3840&amp;q=75 2x" src="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F71df4bc5650b18caf1b27f8180f889f72bee5f0b-1761x352.png&amp;w=3840&amp;q=75"></p></div><p><strong>Architecture Overview. </strong>Data flow shown for one entity.<strong> </strong>The forecasting head unifies information from the graph, the past time series, temporal frequency encodings and calendar features. While past and graph encodings are given for the whole entity, the temporal encodings and calendar features are individual for each future prediction.</p><p><strong>Date-time encodings</strong>. To provide the model with information about the current date, current time, day of week, month of year, <em>etc</em>, frequency encodings <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> of several date-time values are provided to the model. Date-time encodings enable to correctly model seasonal effects.</p><p><strong>Calendar encodings</strong>. Embeddings <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> represent indicators given in calendar tables. These can be indicators for specific holidays or other special times, which could be relevant for the model. We find that providing such information explicitly improves the models ability to capture, <em>e.g.</em>, rare outlier days with specific characteristics. Embeddings <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> are processed by a 1-dimensional CNN before fed into <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> to allow the model to see a window of adjacent calendar information.</p><p><strong>Graph entity encodings. </strong>Embeddings <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span><strong> </strong>encode the subgraph around entity <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> obtained via temporal subgraph sampling according to the RDL formula. After neighbor sampling, the embeddings can be obtained with a Graph Transformer or Graph Neural Network on <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> and <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, as described in a dedicated section below. The entity subgraph conditioning allows the model to consider rich signals occurring in relational data.</p><p><strong>Past sequence encodings.</strong> Auto-regressive signals are important for any forecasting model. Thus, the past time series <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> for entity <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> is encoded into embedding <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> with a sequence encoder, <em>e.g.</em>, a Transformer, and provided to the model. The past sequence encoding allows the model to react to the current trend in the time series. We describe the past encoders in a dedicated section below.</p><h3 id="graph-encoding-via-graph-transformers">Graph Encoding via Graph Transformers</h3><p>The forecasting head <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> is conditioned on node embeddings <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> for each entity <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, respectively. A temporal neighbor sampler can be used to obtain subgraphs <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> with features <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> around entities <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, allowing to scale the approach to real world graphs. If temporal graphs are present (such as in the RDL context), subgraphs are sampled to only contain nodes with a timestamp smaller than current time <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, preventing data leakage from future information in the given graph. The subgraph can then be encoded with a Graph Transformer:</p><p>Graph Transformers employ graph positional encodings of the given graph <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, which allow a sequence Transformer architecture to process and understand the graph structure underlying individual node features <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>. For an in-depth discussion about Graph Transformers, check out our<a href="https://kumo.ai/research/introduction-to-graph-transformers/"> Graph Transformer blog post</a>.</p><h3 id="past-sequence-encoder">Past Sequence Encoder</h3><p>The past embedding <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> is another important conditioning for function <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>. Auto-regressive models have been shown to perform well in forecasting tasks, allowing the model to continue current trends in recently observed past values. To this end, we encode the past sequence <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> for each entity <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> with a past encoding network:</p><p>In theory, any sequence encoder can be used, such as transformers, convolutional neural networks (CNNs), or simple MLPs. We found that one-dimensional CNNs performing convolution over the temporal dimension provide a good trade-off between efficiency and accuracy.</p><h3 id="training">Training</h3><p>The above forecasting framework provides a deep learning architecture that can be trained in an end-to-end fashion on existing graphs with time series per entity. Naively, we can sample a specific time <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, apply temporal sampling to sample subgraphs with timestamp earlier than <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> and train the network to predict future values at time <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> by minimizing an MSE loss against ground truth time series values <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>:</p><h2 id="regression-vs.-generative-forecasting">Regression vs. Generative Forecasting</h2><p>At its core, time series forecasting is a probabilistic task due to ambiguities in the mapping from input signals/features to future value. Thus, it is reasonable to consider a more advanced framework than above, where we model distributions over random variables instead of a function:</p><p>Here, it is assumed that the future value follows some conditional distribution, from which we can obtain the most likely value or perform probabilistic inference via sampling.</p><p><strong>Forecasting via Regression.</strong> In the above framework, the function <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> is naively modeled with an Multi-Layer Perceptron (MLP) and trained to regress value <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> by minimizing a mean-squared error. This paradigm provides good forecast in an efficient manner. However, simple regression via MSE implicitly assumes <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> to be Gaussian and trains the model to predict the mean of that Gaussian, which is a conceptual limitation of the given framework. A result is that the naive regression head above is prone to mean collapse, potentially smoothing out higher frequency forecasts, especially if the actual distribution is far away from a Gaussian, <em>e.g.</em>, multi-modal.</p><p>In the past, forecasting research came up with alternatives, <em>e.g.</em>, optimizing the negative log likelihood of examples given a parameterized distribution, with distribution parameters predicted by the model. This allows to predict the full distribution instead of individual point estimates and provides additional outputs, such as quantile bands as a measure of uncertainty. Typically, this is used to model Gaussian distributions or alternatives for discrete forecasts like the Negative Binomial distribution. However, one crucial limitation remains: the given time series data needs to follow a parameterized distribution in the first place and that distribution needs to be assumed a priori.</p><p><strong>Generative Forecasting. </strong>Here, we explore an alternative probabilistic inference formulation based on recent research in <a href="https://arxiv.org/abs/2406.11838">continuous generative modeling</a>, concretely, conditional diffusion models. Instead of using a simple regression MLP <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, we use a diffusion head, which iteratively denoises a randomly initialized future time series, conditioned on the embeddings from above.</p><p>The diffusion head, modeled via a CNN, takes above conditionings <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>, <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> and iteratively denoises a noisy time-series. During training, we follow the common practice of a <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Model (DDPM) schedule over 1000 steps</a>, adding noise to a future time series and training the model to predict the added noise. During inference, we start with a time-series randomly initialized from a Gaussian distribution and iteratively denoise it until obtaining a sample from the modeled distribution.</p><p>Modeling the time series forecasting task in this manner enables some interesting properties. Firstly, the trained model allows to sample values from <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> without making any assumption on the form of <span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span>. We can sample multiple times and statistics can be obtained empirically, by extracting modes or providing quantile bands. We can obtain an empirical mean and variance, which would require again the assumption on the distribution type. However, we can also empirically extract modes or provide quantiles bands.</p><h2 id="forecasting-results">Forecasting Results</h2><p>In the following, we provide some example forecast for a task of predicting future visits in individual stores. We observe that the generative model is able to forecast higher frequency details and reacts better to rare events than the predictive forecasting baseline. In quantitative comparison both perform equally well, though.</p><div><p><img alt="" loading="lazy" width="1013" height="294" decoding="async" data-nimg="1" srcset="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2Fbdbb336c3990139b1cca9b119d0d229260cc54bd-1013x294.png&amp;w=1080&amp;q=75 1x, https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2Fbdbb336c3990139b1cca9b119d0d229260cc54bd-1013x294.png&amp;w=2048&amp;q=75 2x" src="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2Fbdbb336c3990139b1cca9b119d0d229260cc54bd-1013x294.png&amp;w=2048&amp;q=75"></p></div><div><p><img alt="" loading="lazy" width="1013" height="294" decoding="async" data-nimg="1" srcset="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2Fb20fa890a423c6ddfd70d163397adff18cb2a1aa-1013x294.png&amp;w=1080&amp;q=75 1x, https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2Fb20fa890a423c6ddfd70d163397adff18cb2a1aa-1013x294.png&amp;w=2048&amp;q=75 2x" src="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2Fb20fa890a423c6ddfd70d163397adff18cb2a1aa-1013x294.png&amp;w=2048&amp;q=75"></p></div><div><p><img alt="" loading="lazy" width="1013" height="294" decoding="async" data-nimg="1" srcset="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F155ff4b25c95935f9c03450709e77c52f2e307c2-1013x294.png&amp;w=1080&amp;q=75 1x, https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F155ff4b25c95935f9c03450709e77c52f2e307c2-1013x294.png&amp;w=2048&amp;q=75 2x" src="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F155ff4b25c95935f9c03450709e77c52f2e307c2-1013x294.png&amp;w=2048&amp;q=75"></p></div><p><strong>Generative Forecasting Comparison. </strong>We compare predictive and generative forecasting results for three entities of a forecasting task, where our models make one prediction per day for the next 90 days in advance. While quantitatively both models perform similarly well, generative forecasting shows less mean collapse and captures some high frequency details better.</p><div><p><img alt="" loading="lazy" width="1013" height="294" decoding="async" data-nimg="1" srcset="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2Fcd04d2a35153d682b57e25fa23f4c3a979fccd8a-1013x294.png&amp;w=1080&amp;q=75 1x, https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2Fcd04d2a35153d682b57e25fa23f4c3a979fccd8a-1013x294.png&amp;w=2048&amp;q=75 2x" src="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2Fcd04d2a35153d682b57e25fa23f4c3a979fccd8a-1013x294.png&amp;w=2048&amp;q=75"></p></div><div><p><img alt="" loading="lazy" width="1013" height="294" decoding="async" data-nimg="1" srcset="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F1403456633bb96d5802921b0ca3ff1b7efb57f6a-1013x294.png&amp;w=1080&amp;q=75 1x, https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F1403456633bb96d5802921b0ca3ff1b7efb57f6a-1013x294.png&amp;w=2048&amp;q=75 2x" src="https://kumo.ai/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fs3lh42f5%2Fproduction%2F1403456633bb96d5802921b0ca3ff1b7efb57f6a-1013x294.png&amp;w=2048&amp;q=75"></p></div><p><strong>Comparison to Facebook Prophet. </strong>We compare our forecasting results against forecasts from Facebook Prophet. While Prophet also captures trend and seasonality mostly correct, it shows slightly more divergence and more mean collapse behavior.</p><h2 id="conclusion">Conclusion</h2><p>Time series forecasting has been and will continue to be an important task in machine learning for several different applications. In this blog post, we described how to design an end-to-end pipeline for forecasting on graph structures, performing forecasting on a subset of graph nodes while using input signals from the whole graph,<em> e.g.</em>, to combine data from multiple tables in a database. We also discussed the differences between point predictions and probabilistic forecasting using generative formulations, which we believe to be an interesting area for future investigation.</p><h3 id="further-resources">Further Resources</h3><p>If you’re excited to dive deeper and start experimenting with forecasting on graphs or Graph Transformers on your own, <a href="https://pyg.org/">PyTorch Geometric (PyG)</a> is a great place to begin. It’s one of the most widely used libraries for building Graph Neural Networks and comes with built-in support for Graph Transformers. The <a href="https://pytorch-geometric.readthedocs.io/en/latest/?utm_source=chatgpt">official documentation</a> is packed with examples, and the <a href="https://pytorch-geometric.readthedocs.io/en/latest/tutorial/graph_transformer.html?highlight=transformers&amp;utm_source=chatgpt">Graph Transformer tutorial</a> walks you through building Transformer-based models on graphs.</p><p>Also make sure to check out our <a href="https://kumo.ai/research/relational-deep-learning-rdl/">Relational Deep Learning</a> framework, which allows to directly apply Graph Neural Networks and Graph Transformers to relational data, and <a href="https://relbench.stanford.edu/">RelBench</a>, a unified evaluation framework for a wide arrange of practical analytics tasks.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building Effective AI Agents (443 pts)]]></title>
            <link>https://www.anthropic.com/engineering/building-effective-agents</link>
            <guid>44301809</guid>
            <pubDate>Tue, 17 Jun 2025 17:50:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/engineering/building-effective-agents">https://www.anthropic.com/engineering/building-effective-agents</a>, See on <a href="https://news.ycombinator.com/item?id=44301809">Hacker News</a></p>
<div id="readability-page-1" class="page"><section aria-label="Engineering Article Hero"><a href="https://www.anthropic.com/engineering">Engineering at Anthropic</a></section><div><article><div><p>Over the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.</p><p>In this post, we share what we’ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents.</p><h2 id="what-are-agents">What are agents?</h2><p>"Agent" can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as <strong>agentic systems</strong>, but draw an important architectural distinction between <strong>workflows </strong>and<strong> agents</strong>:</p><ul><li><strong>Workflows</strong> are systems where LLMs and tools are orchestrated through predefined code paths.</li><li><strong>Agents</strong>, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.</li></ul><p>Below, we will explore both types of agentic systems in detail. In Appendix 1 (“Agents in Practice”), we describe two domains where customers have found particular value in using these kinds of systems.</p><h2 id="when-and-when-not-to-use-agents">When (and when not) to use agents</h2><p>When building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense.</p><p>When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough.</p><h2 id="when-and-how-to-use-frameworks">When and how to use frameworks</h2><p>There are many frameworks that make agentic systems easier to implement, including:</p><ul><li><a href="https://langchain-ai.github.io/langgraph/">LangGraph</a> from LangChain;</li><li>Amazon Bedrock's <a href="https://aws.amazon.com/bedrock/agents/">AI Agent framework</a>;</li><li><a href="https://rivet.ironcladapp.com/">Rivet</a>, a drag and drop GUI LLM workflow builder; and</li><li><a href="https://www.vellum.ai/">Vellum</a>, another GUI tool for building and testing complex workflows.</li></ul><p>These frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts ​​and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.</p><p>We suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.</p><p>See our <a href="https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents">cookbook</a> for some sample implementations.</p><h2 id="building-blocks-workflows-and-agents">Building blocks, workflows, and agents</h2><p>In this section, we’ll explore the common patterns for agentic systems we’ve seen in production. We'll start with our foundational building block—the augmented LLM—and progressively increase complexity, from simple compositional workflows to autonomous agents.</p><h3 id="building-block-the-augmented-llm">Building block: The augmented LLM</h3><p>The basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities—generating their own search queries, selecting appropriate tools, and determining what information to retain.</p><div><figure><img loading="lazy" width="2401" height="1000" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd3083d3f40bb2b6f477901cc9a240738d3dd1371-2401x1000.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd3083d3f40bb2b6f477901cc9a240738d3dd1371-2401x1000.png&amp;w=3840&amp;q=75"><figcaption>The augmented LLM</figcaption></figure></div><p>We recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released <a href="https://www.anthropic.com/news/model-context-protocol">Model Context Protocol</a>, which allows developers to integrate with a growing ecosystem of third-party tools with a simple <a href="https://modelcontextprotocol.io/tutorials/building-a-client#building-mcp-clients">client implementation</a>.</p><p>For the remainder of this post, we'll assume each LLM call has access to these augmented capabilities.</p><h3 id="workflow-prompt-chaining">Workflow: Prompt chaining</h3><p>Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see "gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.</p><div><figure><img loading="lazy" width="2401" height="1000" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&amp;w=3840&amp;q=75"><figcaption>The prompt chaining workflow</figcaption></figure></div><p><strong>When to use this workflow:</strong> This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.</p><p><strong>Examples where prompt chaining is useful:</strong></p><ul><li>Generating Marketing copy, then translating it into a different language.</li><li>Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.</li></ul><h3 id="workflow-routing">Workflow: Routing</h3><p>Routing classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.</p><div><figure><img loading="lazy" width="2401" height="1000" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&amp;w=3840&amp;q=75"><figcaption>The routing workflow</figcaption></figure></div><p><strong>When to use this workflow:</strong> Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.</p><p><strong>Examples where routing is useful:</strong></p><ul><li>Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools.</li><li>Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed.</li></ul><h3 id="workflow-parallelization">Workflow: Parallelization</h3><p>LLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:</p><ul><li><strong>Sectioning</strong>: Breaking a task into independent subtasks run in parallel.</li><li><strong>Voting:</strong> Running the same task multiple times to get diverse outputs.</li></ul><div><figure><img loading="lazy" width="2401" height="1000" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&amp;w=3840&amp;q=75"><figcaption>The parallelization workflow</figcaption></figure></div><p><strong>When to use this workflow:</strong> Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.</p><p><strong>Examples where parallelization is useful:</strong></p><ul><li><strong>Sectioning</strong>:<ul><li>Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response.</li><li>Automating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model’s performance on a given prompt.</li></ul></li><li><strong>Voting</strong>:<ul><li>Reviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem.</li><li>Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives.</li></ul></li></ul><h3 id="workflow-orchestrator-workers">Workflow: Orchestrator-workers</h3><p>In the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.</p><div><figure><img loading="lazy" width="2401" height="1000" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&amp;w=3840&amp;q=75"><figcaption>The orchestrator-workers workflow</figcaption></figure></div><p><strong>When to use this workflow:</strong> This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.</p><p><strong>Example where orchestrator-workers is useful:</strong></p><ul><li>Coding products that make complex changes to multiple files each time.</li><li>Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information.</li></ul><h3 id="workflow-evaluator-optimizer">Workflow: Evaluator-optimizer</h3><p>In the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.</p><div><figure><img loading="lazy" width="2401" height="1000" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&amp;w=3840&amp;q=75"><figcaption>The evaluator-optimizer workflow</figcaption></figure></div><p><strong>When to use this workflow:</strong> This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.</p><p><strong>Examples where evaluator-optimizer is useful:</strong></p><ul><li>Literary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques.</li><li>Complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted.</li></ul><h3 id="agents">Agents</h3><p>Agents are emerging in production as LLMs mature in key capabilities—understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain “ground truth” from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it’s also common to include stopping conditions (such as a maximum number of iterations) to maintain control.</p><p>Agents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 ("Prompt Engineering your Tools").</p><div><figure><img loading="lazy" width="2401" height="1000" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F58d9f10c985c4eb5d53798dea315f7bb5ab6249e-2401x1000.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F58d9f10c985c4eb5d53798dea315f7bb5ab6249e-2401x1000.png&amp;w=3840&amp;q=75"><figcaption>Autonomous agent</figcaption></figure></div><p><strong>When to use agents:</strong> Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.</p><p>The autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails.</p><p><strong>Examples where agents are useful:</strong></p><p>The following examples are from our own implementations:</p><ul><li>A coding Agent to resolve <a href="https://www.anthropic.com/research/swe-bench-sonnet">SWE-bench tasks</a>, which involve edits to many files based on a task description;</li><li>Our <a href="https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo">“computer use” reference implementation</a>, where Claude uses a computer to accomplish tasks.</li></ul><div><figure><img loading="lazy" width="2400" height="1666" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4b9a1f4eb63d5962a6e1746ac26bbc857cf3474f-2400x1666.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4b9a1f4eb63d5962a6e1746ac26bbc857cf3474f-2400x1666.png&amp;w=3840&amp;q=75"><figcaption>High-level flow of a coding agent</figcaption></figure></div><h2 id="combining-and-customizing-these-patterns">Combining and customizing these patterns</h2><p>These building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity <em>only</em> when it demonstrably improves outcomes.</p><h2 id="summary">Summary</h2><p>Success in the LLM space isn't about building the most sophisticated system. It's about building the <em>right</em> system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short.</p><p>When implementing agents, we try to follow three core principles:</p><ol><li>Maintain <strong>simplicity</strong> in your agent's design.</li><li>Prioritize <strong>transparency</strong> by explicitly showing the agent’s planning steps.</li><li>Carefully craft your agent-computer interface (ACI) through thorough tool <strong>documentation and testing</strong>.</li></ol><p>Frameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users.</p><h3 id="acknowledgements">Acknowledgements</h3><p>Written by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful.</p><h2 id="appendix-1-agents-in-practice">Appendix 1: Agents in practice</h2><p>Our work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight.</p><h3 id="a-customer-support">A. Customer support</h3><p>Customer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:</p><ul><li>Support interactions naturally follow a conversation flow while requiring access to external information and actions;</li><li>Tools can be integrated to pull customer data, order history, and knowledge base articles;</li><li>Actions such as issuing refunds or updating tickets can be handled programmatically; and</li><li>Success can be clearly measured through user-defined resolutions.</li></ul><p>Several companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness.</p><h3 id="b-coding-agents">B. Coding agents</h3><p>The software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:</p><ul><li>Code solutions are verifiable through automated tests;</li><li>Agents can iterate on solutions using test results as feedback;</li><li>The problem space is well-defined and structured; and</li><li>Output quality can be measured objectively.</li></ul><p>In our own implementation, agents can now solve real GitHub issues in the <a href="https://www.anthropic.com/research/swe-bench-sonnet">SWE-bench Verified</a> benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.</p><h2 id="appendix-2-prompt-engineering-your-tools">Appendix 2: Prompt engineering your tools</h2><p>No matter which agentic system you're building, tools will likely be an important part of your agent. <a href="https://www.anthropic.com/news/tool-use-ga">Tools</a> enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a <a href="https://docs.anthropic.com/en/docs/build-with-claude/tool-use#example-api-response-with-a-tool-use-content-block">tool use block</a> in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools.</p><p>There are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes.</p><p>Our suggestions for deciding on tool formats are the following:</p><ul><li>Give the model enough tokens to "think" before it writes itself into a corner.</li><li>Keep the format close to what the model has seen naturally occurring in text on the internet.</li><li>Make sure there's no formatting "overhead" such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes.</li></ul><p>One rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good <em>agent</em>-computer interfaces (ACI). Here are some thoughts on how to do so:</p><ul><li>Put yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it’s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools.</li><li>How can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools.</li><li>Test how the model uses your tools: Run many example inputs in our <a href="https://console.anthropic.com/workbench">workbench</a> to see what mistakes the model makes, and iterate.</li><li><a href="https://en.wikipedia.org/wiki/Poka-yoke">Poka-yoke</a> your tools. Change the arguments so that it is harder to make mistakes.</li></ul><p>While building our agent for <a href="https://www.anthropic.com/research/swe-bench-sonnet">SWE-bench</a>, we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths—and we found that the model used this method flawlessly.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Resurrecting a dead torrent tracker and finding 3M peers (578 pts)]]></title>
            <link>https://kianbradley.com/2025/06/15/resurrecting-a-dead-tracker.html</link>
            <guid>44301686</guid>
            <pubDate>Tue, 17 Jun 2025 17:40:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kianbradley.com/2025/06/15/resurrecting-a-dead-tracker.html">https://kianbradley.com/2025/06/15/resurrecting-a-dead-tracker.html</a>, See on <a href="https://news.ycombinator.com/item?id=44301686">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>So I was uh, downloading some linux isos, like usual. It was going slowly, so I opened up the <code>Trackers</code> tab in qBittorrent and saw the following:</p>

<p><img src="https://kianbradley.com/assets/tracker-list.png" alt="List of trackers... most of them unreachable"></p>

<p><strong>Most of the trackers were totally dead.</strong> Either the hosts were down or the domains weren’t being used.</p>

<p>That got me thinking. <strong>What if <em>I</em> picked up one of these dead domains?</strong> How many clients would try to connect?</p>

<h2 id="what-are-trackers-for-anyways">What are trackers for, anyways?</h2>

<p>A <em>tracker</em> is a core component of the <a href="https://en.wikipedia.org/wiki/BitTorrent">BitTorrent protocol</a>. Trackers are the services that point you to other peers for the torrent. Without trackers, there would be no one to share the file with.</p>

<p>Obviously this represents a major source of centralization in the torrent protocol. If your trackers aren’t maintained – or if they get forced offline by certain industry organizations – you’re out of luck.</p>

<p>We have an alternative, called <a href="https://en.wikipedia.org/wiki/Mainline_DHT">Mainline DHT</a>, which performs a more decentralized lookup of peers based on infohash alone. DHT isn’t perfect, though. It relies on <a href="https://stackoverflow.com/questions/1181301/how-does-a-dht-in-a-bittorent-client-get-bootstrapped">bootstrap nodes</a> and is vulnerable to <a href="https://www.bittorrent.org/beps/bep_0042.html">Sybil attacks</a>. And in the example of my poorly-served torrent, DHT wasn’t surfacing any peers, regardless.</p>

<h2 id="hosting-a-tracker">Hosting a tracker</h2>

<p>Looking through the list of trackers marked “host not found”, I noticed <code>udp://open.demonii.si:1337/announce</code> was available.</p>

<p>I bought the domain through <a href="https://www.dynadot.com/">Dynadot</a> (one of the few .si domain registrars), then spun up a <a href="https://cockbox.org/">quick anonymous VPS</a>. I mapped the domain to the VPS, then set up <a href="https://erdgeist.org/arts/software/opentracker/">opentracker</a>, the most widely used and robust torrent tracker software.</p>

<p>Instructions for Ubuntu 24.04:</p>

<figure><pre><code data-lang="bash"><span>sudo </span>apt <span>install </span>gcc-14 g++-14 build-essential zlib1g-dev
<span>sudo </span>update-alternatives <span>--install</span> /usr/bin/gcc gcc /usr/bin/gcc-14 14
<span>sudo </span>update-alternatives <span>--install</span> /usr/bin/g++ g++ /usr/bin/g++-14 14</code></pre></figure>

<p>Follow the <a href="https://erdgeist.org/gitweb/opentracker/tree/README">readme</a> to compile, first the dependency <a href="https://www.fefe.de/libowfat/">libowfat</a> (a GPL reimplementation of some of <a href="https://cr.yp.to/djb.html">dan bernstein</a>’s C libraries) and then opentracker itself.</p>

<figure><pre><code data-lang="bash">cvs <span>-d</span> :pserver:cvs@cvs.fefe.de:/cvs <span>-z9</span> co libowfat
<span>cd </span>libowfat
make
<span>cd</span> ..
git clone git://erdgeist.org/opentracker
<span>cd </span>opentracker
make</code></pre></figure>

<p>Finally, a quick systemd unit file to daemonize this service:</p>

<figure><pre><code data-lang="bash"><span>[</span>Unit]
<span>Description</span><span>=</span>opentracker
<span>After</span><span>=</span>network-online.target
<span>Wants</span><span>=</span>network-online.target

<span>[</span>Service]
<span>Type</span><span>=</span>simple
<span>User</span><span>=</span>opentracker
<span>Group</span><span>=</span>opentracker
<span>WorkingDirectory</span><span>=</span>/var/lib/opentracker
<span>ExecStart</span><span>=</span>/home/opentracker/opentracker/opentracker <span>-p</span> 1337 <span>-P</span> 1337 <span>\</span>
          <span>-d</span> /var/lib/opentracker <span>-u</span> opentracker
<span>Restart</span><span>=</span>on-failure
<span>LimitNOFILE</span><span>=</span>65536

<span>[</span>Install]
<span>WantedBy</span><span>=</span>multi-user.target</code></pre></figure>

<h2 id="what-did-i-find">What did I find?</h2>

<p>Before even starting opentracker, I saw a flood of traffic against UDP port 1337:</p>

<video controls="" src="https://kianbradley.com/assets/tcpdump.mp4" alt="Flood of UDP traffic reported by tcpdump"></video>

<p>I then started the tracker. After about an hour, it peaked at about <strong>1.7 million distinct torrents across 3.1 million peers!</strong></p>

<p>Response from <code>http://open.demonii.si:1337/stats?mode=everything</code>:</p>

<figure><pre><code data-lang="xml"><span>&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span>
<span>&lt;stats&gt;</span>
  <span>&lt;tracker_id&gt;</span>273419141<span>&lt;/tracker_id&gt;</span>
  <span>&lt;version&gt;</span>
https://erdgeist.org/gitweb/opentracker/commit/?id=b20b0b89264e9d28ab873b8b1cc9ba73cdb58aeb
  <span>&lt;/version&gt;</span>
  <span>&lt;uptime&gt;</span>10313<span>&lt;/uptime&gt;</span>
  <span>&lt;torrents&gt;</span>
    <span>&lt;count_mutex&gt;</span>1735538<span>&lt;/count_mutex&gt;</span>
    <span>&lt;count_iterator&gt;</span>1735523<span>&lt;/count_iterator&gt;</span>
  <span>&lt;/torrents&gt;</span>
  <span>&lt;peers&gt;</span>
    <span>&lt;count&gt;</span>3155701<span>&lt;/count&gt;</span>
  <span>&lt;/peers&gt;</span>
  <span>&lt;seeds&gt;</span>
    <span>&lt;count&gt;</span>1342504<span>&lt;/count&gt;</span>
  <span>&lt;/seeds&gt;</span>
  <span>&lt;completed&gt;</span>
    <span>&lt;count&gt;</span>244224<span>&lt;/count&gt;</span>
  <span>&lt;/completed&gt;</span>
  <span>&lt;connections&gt;</span>
    <span>&lt;tcp&gt;</span>
      <span>&lt;accept&gt;</span>21532<span>&lt;/accept&gt;</span>
      <span>&lt;announce&gt;</span>20219<span>&lt;/announce&gt;</span>
      <span>&lt;scrape&gt;</span>263<span>&lt;/scrape&gt;</span>
    <span>&lt;/tcp&gt;</span>
    <span>&lt;udp&gt;</span>
      <span>&lt;overall&gt;</span>58843612<span>&lt;/overall&gt;</span>
      <span>&lt;connect&gt;</span>18321703<span>&lt;/connect&gt;</span>
      <span>&lt;announce&gt;</span>33160261<span>&lt;/announce&gt;</span>
      <span>&lt;scrape&gt;</span>3211543<span>&lt;/scrape&gt;</span>
      <span>&lt;missmatch&gt;</span>4116689<span>&lt;/missmatch&gt;</span>
    <span>&lt;/udp&gt;</span>
    <span>&lt;livesync&gt;</span>
      <span>&lt;count&gt;</span>0<span>&lt;/count&gt;</span>
    <span>&lt;/livesync&gt;</span>
  <span>&lt;/connections&gt;</span>
  <span>&lt;debug&gt;</span>
    <span>&lt;renew&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"00"</span><span>&gt;</span>12216193<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"01"</span><span>&gt;</span>1463740<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"02"</span><span>&gt;</span>536527<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"03"</span><span>&gt;</span>284756<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"04"</span><span>&gt;</span>243276<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"05"</span><span>&gt;</span>93237<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"06"</span><span>&gt;</span>63618<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"07"</span><span>&gt;</span>53934<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"08"</span><span>&gt;</span>36851<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"09"</span><span>&gt;</span>28990<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"10"</span><span>&gt;</span>352150<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"11"</span><span>&gt;</span>56610<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"12"</span><span>&gt;</span>24557<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"13"</span><span>&gt;</span>21628<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"14"</span><span>&gt;</span>24932<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"15"</span><span>&gt;</span>63250<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"16"</span><span>&gt;</span>38174<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"17"</span><span>&gt;</span>33730<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"18"</span><span>&gt;</span>27827<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"19"</span><span>&gt;</span>27166<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"20"</span><span>&gt;</span>22463<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"21"</span><span>&gt;</span>17820<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"22"</span><span>&gt;</span>17248<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"23"</span><span>&gt;</span>17276<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"24"</span><span>&gt;</span>17825<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"25"</span><span>&gt;</span>20144<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"26"</span><span>&gt;</span>27987<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"27"</span><span>&gt;</span>792338<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"28"</span><span>&gt;</span>1579577<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"29"</span><span>&gt;</span>1625355<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"30"</span><span>&gt;</span>2229105<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"31"</span><span>&gt;</span>1670317<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"32"</span><span>&gt;</span>1581574<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"33"</span><span>&gt;</span>846355<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"34"</span><span>&gt;</span>96656<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"35"</span><span>&gt;</span>68160<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"36"</span><span>&gt;</span>47801<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"37"</span><span>&gt;</span>36705<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"38"</span><span>&gt;</span>32256<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"39"</span><span>&gt;</span>27535<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"40"</span><span>&gt;</span>27593<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"41"</span><span>&gt;</span>27640<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"42"</span><span>&gt;</span>24090<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"43"</span><span>&gt;</span>20762<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>interval=</span><span>"44"</span><span>&gt;</span>17880<span>&lt;/count&gt;</span>
    <span>&lt;/renew&gt;</span>
    <span>&lt;http_error&gt;</span>
      <span>&lt;count</span> <span>code=</span><span>"302 Redirect"</span><span>&gt;</span>0<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>code=</span><span>"400 Parse Error"</span><span>&gt;</span>0<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>code=</span><span>"400 Invalid Parameter"</span><span>&gt;</span>55<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>code=</span><span>"400 Invalid Parameter (compact=0)"</span><span>&gt;</span>0<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>code=</span><span>"400 Not Modest"</span><span>&gt;</span>0<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>code=</span><span>"402 Payment Required"</span><span>&gt;</span>0<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>code=</span><span>"403 Access Denied"</span><span>&gt;</span>0<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>code=</span><span>"404 Not found"</span><span>&gt;</span>883<span>&lt;/count&gt;</span>
      <span>&lt;count</span> <span>code=</span><span>"500 Internal Server Error"</span><span>&gt;</span>0<span>&lt;/count&gt;</span>
    <span>&lt;/http_error&gt;</span>
    <span>&lt;mutex_stall&gt;</span>
      <span>&lt;count&gt;</span>0<span>&lt;/count&gt;</span>
    <span>&lt;/mutex_stall&gt;</span>
  <span>&lt;/debug&gt;</span>
<span>&lt;/stats&gt;</span></code></pre></figure>

<h2 id="is-this-legal">Is this legal?</h2>

<p>Maybe.</p>

<p>When the recording industry and other litigious organizations go after torrent trackers, they’re mainly chasing down the public-facing parts of the system. The legal decisions against websites like The Pirate Bay hinge on how they highlight popular movies, sell ads, and offer .torrent files. This is all taken as evidence of <strong>inducement</strong>, meaning the <em>intentional promotion</em> of copyright infringement.</p>

<p>Does hosting tracker infrastructure, unadvertised, count as “inducement”? It’s a harder case to make. I’m aware that many torrents, both freely available and copyrighted, use this tracker. But it would be more difficult to prove intent here.</p>

<p>Regardless, I was spooked. I thought through my chain of events and realized I had already fucked up by paying for the domain with a credit card. I shut down the VPS and deleted the domain quickly after confirming it works.</p>

<p>So… the domain is available now. It’s quite easy to find unclaimed domains like this. If you want to do a public service, <em>open.demonii.si</em> and others are up for registration…</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's CDNA 4 Architecture Announcement (160 pts)]]></title>
            <link>https://chipsandcheese.com/p/amds-cdna-4-architecture-announcement</link>
            <guid>44301660</guid>
            <pubDate>Tue, 17 Jun 2025 17:38:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/amds-cdna-4-architecture-announcement">https://chipsandcheese.com/p/amds-cdna-4-architecture-announcement</a>, See on <a href="https://news.ycombinator.com/item?id=44301660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>CDNA 4 is AMD’s latest compute oriented GPU architecture, and represents a modest update over CDNA 3. CDNA 4’s focus is primarily on boosting AMD’s matrix multiplication performance with lower precision data types. Those operations are important for machine learning workloads, which can often maintain acceptable accuracy with very low precision types. At the same time, CDNA 4 seeks to maintain AMD’s lead in more widely applicable vector operations.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png" width="1055" height="758" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:758,&quot;width&quot;:1055,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;From AMD's whitepaper&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="From AMD's whitepaper" title="From AMD's whitepaper" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b4bd5-a5f6-4377-8cba-f5ea5ffda06e_1055x758.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>To do so, CDNA 4 largely uses the same system level architecture as CDNA 3. It’s massive chiplet setup, with parallels to AMD’s successful use of chiplets for CPU products. Accelerator Compute Dies, or XCDs, contain CDNA Compute Units and serve a role analogous to Core Complex Dies (CCDs) on AMD’s CPU products. Eight XCDs sit atop four base dies, which implement 256 MB of memory side cache. AMD’s Infinity Fabric provides coherent memory access across the system, which can span multiple chips.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png" width="1005" height="581" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:581,&quot;width&quot;:1005,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:98710,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/166048376?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb62c4acf-bb78-4e3a-a169-e06e770da3bc_1005x581.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Compared to the CDNA 3 based MI300X, the CDNA 4 equipped MI355X slightly cuts down CU count per XCD, and disables more CUs to maintain yields. The resulting GPU is somewhat less wide, but makes up much of the gap with higher clock speeds. Compared to Nvidia’s B200, both MI355X and MI300 are larger GPUs with far more basic building blocks. Nvidia’s B200 does adopt a multi-die strategy, breaking from a long tradition of using monolithic designs. However, AMD’s chiplet setup is far more aggressive and seeks to replicate their scaling success with CPU designs with large compute GPUs.</p><p><span>CDNA 3 provided a huge vector throughput advantage over Nvidia’s H100, but faced a more complicated situation with machine learning workloads. Thanks to a mature software ecosystem and a heavy focus on matrix multiplication throughput (tensor cores), Nvidia could often get close (</span><a href="https://chipsandcheese.com/p/testing-amds-giant-mi300x" rel="">https://chipsandcheese.com/p/testing-amds-giant-mi300x</a><span>) to the nominally far larger MI300X. AMD of course maintained massive wins if the H100 ran out of VRAM, but there was definitely room for improvement.</span></p><p>CDNA 4 rebalances its execution units to more closely target matrix multiplication with lower precision data types, which is precisely what machine learning workloads use. Per-CU matrix throughput doubles in many cases, with CDNA 4 CUs matching Nvidia’s B200 SMs in FP6. Elsewhere though, Nvidia continues to show a stronger emphasis on low precision matrix throughput. B200 SMs have twice as much per-clock throughput as a CDNA 4 CU across a range of 16-bit and 8-bit data types. AMD continues to rely on having a bigger, higher clocked GPU to maintain an overall throughput lead.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F635bc260-858d-426d-8dab-6e574f8695d0_369x238.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F635bc260-858d-426d-8dab-6e574f8695d0_369x238.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F635bc260-858d-426d-8dab-6e574f8695d0_369x238.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F635bc260-858d-426d-8dab-6e574f8695d0_369x238.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F635bc260-858d-426d-8dab-6e574f8695d0_369x238.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F635bc260-858d-426d-8dab-6e574f8695d0_369x238.png" width="369" height="238" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/635bc260-858d-426d-8dab-6e574f8695d0_369x238.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:238,&quot;width&quot;:369,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F635bc260-858d-426d-8dab-6e574f8695d0_369x238.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F635bc260-858d-426d-8dab-6e574f8695d0_369x238.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F635bc260-858d-426d-8dab-6e574f8695d0_369x238.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F635bc260-858d-426d-8dab-6e574f8695d0_369x238.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>With vector operations and higher precision data types, AMD continues MI300X’s massive advantage. Each CDNA 4 CU continues to have 128 FP32 lanes, which deliver 256 FLOPS per cycle when counting FMA operations. MI355X’s lower CU count does lead to a slight reduction in vector performance compared to MI300X. But compared to Nvidia’s Blackwell, AMD’s higher core count and higher clock speeds let it maintain a huge vector throughput lead. Thus AMD’s CDNA line continues to look very good for high performance compute workloads.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F283ff197-239d-46ae-bc39-c85481140787_752x452.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F283ff197-239d-46ae-bc39-c85481140787_752x452.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F283ff197-239d-46ae-bc39-c85481140787_752x452.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F283ff197-239d-46ae-bc39-c85481140787_752x452.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F283ff197-239d-46ae-bc39-c85481140787_752x452.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F283ff197-239d-46ae-bc39-c85481140787_752x452.png" width="752" height="452" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/283ff197-239d-46ae-bc39-c85481140787_752x452.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:452,&quot;width&quot;:752,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F283ff197-239d-46ae-bc39-c85481140787_752x452.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F283ff197-239d-46ae-bc39-c85481140787_752x452.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F283ff197-239d-46ae-bc39-c85481140787_752x452.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F283ff197-239d-46ae-bc39-c85481140787_752x452.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Nvidia’s focus on machine learning and matrix operations keeps them very competitive in that category, despite having fewer SMs running at lower clocks. AMD’s giant MI355X holds a lead across many data types, but the gap between AMD and Nvidia’s largest GPUs isn’t nearly as big as with vector compute.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png" width="1456" height="876" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:876,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d88ed55-fee0-4a68-8c36-4f93bb270183_1546x930.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>GPUs provide a software managed scratchpad local to a group of threads, typically ones running on the same core. AMD GPUs use a Local Data Share, or LDS, for that purpose. Nvidia calls their analogous structure Shared Memory. CDNA 3 had a 64 KB LDS, carrying forward a similar design from AMD GCN GPUs going back to 2012. That LDS had 32 2 KB banks, each 32 bits wide, providing up to 128 bytes per cycle in the absence of bank conflicts.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png" width="1456" height="537" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:537,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa01ed507-6c6c-40a3-9ed8-128f462590fa_1600x590.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>CDNA 4 increases the LDS capacity to 160 KB and doubles read bandwidth to 256 bytes per clock. GPUs natively operate on 32 bit elements, and it would be reasonable to assume AMD doubled bandwidth by doubling bank count. If so, each bank may now have 2.5 KB of capacity. Another possibility would be increasing bank count to 80 while keeping bank size at 2 KB, but that’s less likely because it would complicate bank selection. A 64-banked LDS could naturally serve a 64-wide wavefront access with each bank serving a lane. Furthermore, a power-of-two bank count would allow simple bank selection via a subset of address bits.</p><p>The larger LDS lets software keep more data close to the execution units. Kernels can allocate more LDS capacity without worrying about lower occupancy due to LDS capacity constraints. For example, a kernel that allocates 16 KB of LDS could run four workgroups on a CDNA 3 CU. On CDNA 4, that would increase to ten workgroups.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png" width="1269" height="710" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:710,&quot;width&quot;:1269,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F548bfdb3-521c-4f34-9491-35da9a37c1fa_1269x710.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Software has to explicitly move data into the LDS to take advantage of it, which can introduce overhead compared to using a hardware-managed cache. CDNA 3 had GLOBAL_LOAD_LDS instructions that let kernels copy data into the LDS without going through the vector register file, CDNA 4 augments GLOBAL_LOAD_LDS to support moving up to 128 bits per lane, versus 32 bits per lane on CDNA 3. That is, the GLOBAL_LOAD_LDS instruction can accept sizes of 1, 2, 4, 12, or 16 DWORDS (32-bit elements), versus just 1, 2, or 4 on CDNA 3.</span><sup>1</sup></p><p>CDNA 4 also introduces read-with-transpose LDS instructions. Matrix multiplication involves multiplying elements of a row in one matrix with corresponding elements in a second matrix’s column. Often that creates inefficient memory access patterns, for at least one matrix, depending on whether data is laid out in row-major or column-major order. Transposing a matrix turns the awkward row-to-column operation into a more natural row-to-row one. Handling transposition at the LDS is also natural for AMD’s architecture, because the LDS already has a crossbar that can map bank outputs to lanes (swizzle).</p><p>Even with its LDS capacity increase, AMD continues to have less data storage within its GPU cores compared to Nvidia. Blackwell’s SMs have a 256 KB block of storage partitioned for use as both L1 cache and Shared Memory. Up to 228 KB can be allocated for use as Shared Memory. With a 164 KB Shared Memory allocation, which is close to matching AMD’s 160 KB LDS, Nvidia would still have 92 KB available for L1 caching. CDNA 4, like CDNA 3, has a 32 KB L1 vector cache per CU. Thus a Blackwell SM can have more software managed storage while still having a larger L1 cache than a CDNA 4 CU. Of course, AMD’s higher CU count means there’s 40 MB of LDS capacity across the GPU, while Nvidia only has ~33 MB of Shared Memory across B200 with the largest 228 KB Shared Memory allocation.</p><p>To feed the massive arrays of Compute Units, MI355X largely uses the same system level architecture as MI300X. MI355X does see a few enhancements though. The L2 caches can “writeback dirty data and retain a copy of the line”. “Dirty” refers to data that has been modified in a write-back cache, but hasn’t been propagated to lower levels in the memory subsystem. When a dirty line is evicted to make room for newer data, its contents are written back to the next level of cache, or DRAM if it’s the last level cache.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png" width="1456" height="824" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:824,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:193428,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/166048376?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d09938-bf74-48f1-9db3-a935d0e04c84_1580x894.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>AMD may be seeking to opportunistically use write bandwidth when the memory subsystem is under low load, smoothing out spikes in bandwidth demand caused by cache fill requests accompanied by writebacks. Or, AMD could be doing something special to let the L2 transition a line to clean state if written data is likely to be read by other threads across the system, but isn’t expected to be modified again anytime soon.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png" width="1456" height="815" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:815,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a8d9949-3905-44cb-83e9-72365acc3249_1598x895.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>MI355X’s DRAM subsystem has been upgraded to use HBM3E, providing a substantial bandwidth and capacity upgrade over its predecessor. It also maintains AMD’s lead over its Nvidia competition. Nvidia also uses HBM3E with the B200, which also appears to have eight HBM3E stacks. However, the B200 tops out at 180 GB of capacity and 7.7 TB/s of bandwidth, compared to 288 GB at 8 TB/s on the MI355X. The MI300X could have a substantial advantage over Nvidia’s older H100 when the H100 ran out of DRAM capacity, and AMD is likely looking to retain that advantage.</p><p>Higher bandwidth from HBM3E also helps bring up MI355X’s compute-to-bandwidth ratio. MI300X had ~0.03 bytes of DRAM bandwidth per FP32 FLOP, which increases to 0.05 on MI355X. Blackwell for comparison has ~0.10 bytes of DRAM bandwidth per FP32 FLOP. While Nvidia has increased last level cache capacity on Blackwell, AMD continues to lean more heavily on big caches, while Nvidia relies more on DRAM bandwidth.</p><p>CDNA 2 and CDNA 3 made sweeping changes compared to their predecessors. CDNA 4’s changes are more muted. Much like going from Zen 3 to Zen 4, MI355X retains a similar chiplet arrangement with compute and IO chiplets swapped out for improved versions. Rather than changing up their grand strategy, AMD spent their time tuning CDNA 3. Fewer, higher clocked CUs are easier to utilize, and increased memory bandwidth can help utilization too. Higher matrix multiplication throughput also helps AMD take on Nvidia for machine learning workloads.</p><p><span>In some ways, AMD’s approach with this generation has parallels to Nvidia’s. Blackwell SMs are basically identical to Hopper’s from a vector execution perspective, with improvements focused on the matrix side. Nvidia likely felt they had a winning formula, as their past few GPU generations have undoubtedly been successful. AMD may have found a winning formula with CDNA 3 as well. MI300A, MI300X’s iGPU cousin, powers the highest ranking supercomputer on TOP500’s June list.</span><sup>4</sup><span> Building on success can be a safe and rewarding strategy, and CDNA 4 may be doing just that.</span></p><p><span>If you like the content then consider heading over to the </span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span> or </span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span> if you want to toss a few bucks to Chips and Cheese. Also consider joining the </span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p><ol><li><p><a href="https://github.com/llvm/llvm-project/blob/main/clang/test/CodeGenOpenCL/builtins-amdgcn-gfx950.cl" rel="">https://github.com/llvm/llvm-project/blob/main/clang/test/CodeGenOpenCL/builtins-amdgcn-gfx950.cl</a><span> - b96 and b128 (96-bit and 128-bit) global_load_lds sizes</span></p></li><li><p><a href="https://github.com/llvm/llvm-project/blob/84ff1bda2977e580265997ad2d4c47b18cd3bf9f/mlir/include/mlir/Dialect/LLVMIR/ROCDLOps.td#L426C1-L426C50" rel="">https://github.com/llvm/llvm-project/blob/84ff1bda2977e580265997ad2d4c47b18cd3bf9f/mlir/include/mlir/Dialect/LLVMIR/ROCDLOps.td#L426C1-L426C50</a><span> - LDS transpose intrinsics</span></p></li><li><p><a href="https://docs.nvidia.com/cuda/blackwell-tuning-guide/index.html" rel="">https://docs.nvidia.com/cuda/blackwell-tuning-guide/index.html</a></p></li><li><p><a href="https://top500.org/lists/top500/2025/06/" rel="">https://top500.org/lists/top500/2025/06/</a></p></li><li><p><a href="https://www.reddit.com/r/hardware/comments/1kj38r1/battle_of_the_giants_8x_nvidia_blackwell_b200/" rel="">https://www.reddit.com/r/hardware/comments/1kj38r1/battle_of_the_giants_8x_nvidia_blackwell_b200/</a><span> - reports 148 Compute Units via OpenCL for B200. Nvidia usually reports SMs for the Compute Unit count</span></p></li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ICE arrests NYC Comptroller because he asked to see a warrant (303 pts)]]></title>
            <link>https://www.thecity.nyc/2025/06/17/brad-lander-arrest-ice-immigration-court/</link>
            <guid>44301501</guid>
            <pubDate>Tue, 17 Jun 2025 17:25:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thecity.nyc/2025/06/17/brad-lander-arrest-ice-immigration-court/">https://www.thecity.nyc/2025/06/17/brad-lander-arrest-ice-immigration-court/</a>, See on <a href="https://news.ycombinator.com/item?id=44301501">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
		<main id="main">

			
	

	
			<figure>

				<img width="1200" height="800" src="https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?fit=1200%2C800&amp;ssl=1" alt="" data-hero-candidate="1" fetchpriority="high" decoding="async" srcset="https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?w=2048&amp;ssl=1 2048w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?resize=1024%2C683&amp;ssl=1 1024w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?resize=768%2C512&amp;ssl=1 768w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?resize=1536%2C1024&amp;ssl=1 1536w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?resize=1200%2C800&amp;ssl=1 1200w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?resize=2000%2C1333&amp;ssl=1 2000w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?resize=780%2C520&amp;ssl=1 780w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?resize=400%2C267&amp;ssl=1 400w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?resize=706%2C471&amp;ssl=1 706w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-1-1.jpg?fit=1200%2C800&amp;ssl=1&amp;w=370 370w" sizes="(max-width: 1200px) 100vw, 1200px">			<figcaption><span>Federal Agents arrest Comptroller Brad Lander while was escorting a person from immigration court at 26 Federal Plaza, June 17, 2025. <span><span>Credit:</span> Ben Fractenberg/THE CITY</span></span></figcaption>
			
			</figure><!-- .post-thumbnail -->

		
				<div>

					

<article id="post-64317">
	<div>

		
				
					
						
					
						
					
				
<p>New York City Comptroller Brad Lander was detained inside a Lower Manhattan immigration court building Tuesday morning by masked federal agents as he attempted to escort a man from his court appearance there.&nbsp;</p>

<p>Moments ahead of his detention, Lander had linked arms with the man leaving an immigration courtroom on the 12th floor, refusing to let go as masked federal agents pushed into the crowd attempting to pull the man away.&nbsp;</p>

<p>In the chaotic scene at around noon, Lander asked the agents repeatedly to show a judicial warrant.</p>


<figure></figure>

<p>“You do not have the authority to arrest U.S. citizens,” Lander repeated, as the officers tightened handcuffs to his wrists. </p>

<p>The federal agents escorted him into an elevator, with one member of his NYPD security detail alongside him.</p>


<div><p>A reporter from The City had overheard one agent say to another minutes before Lander’s arrest, “Do you want to arrest the Comptroller?” </p><p>It wasn’t immediately clear what charges, if any, the mayoral candidate will face. A spokesperson for ICE didn’t immediately return a request for comment.</p></div>


<p>Masked agents from several federal agencies had lined the halls of 26 Federal Plaza Tuesday morning, including Immigration and Customs Enforcement, Enforcement and Removal Operations, the FBI and even the Treasury Department.&nbsp;</p>


<p>“While escorting a defendant out of immigration court at 26 Federal Plaza, Brad was taken by masked agents and detained by ICE,” Dora Pekec, Lander’s campaign spokesperson said in a statement. “This is still developing and we are monitoring the situation closely.”</p>

<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?resize=1024%2C683&amp;ssl=1" alt="" srcset="https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?resize=1024%2C683&amp;ssl=1 1024w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?resize=768%2C512&amp;ssl=1 768w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?resize=1536%2C1024&amp;ssl=1 1536w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?resize=1200%2C800&amp;ssl=1 1200w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?resize=2000%2C1333&amp;ssl=1 2000w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?resize=780%2C520&amp;ssl=1 780w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?resize=400%2C267&amp;ssl=1 400w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?resize=706%2C471&amp;ssl=1 706w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2.jpg?w=2048&amp;ssl=1 2048w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2025/06/061725_lander_arrest-2-1024x683.jpg?w=370&amp;ssl=1 370w" sizes="auto, (max-width: 1024px) 100vw, 1024px"><figcaption>Federal Agents arrest Comptroller Brad Lander while was escorting a person from immigration court at 26 Federal Plaza, June 17, 2025. <span><span>Credit:</span> Ben Fractenberg/THE CITY</span></figcaption></figure>

<p>Last week federal agents tackled and handcuffed U.S. Sen. Alex Padilla (D-Calif.) when he tried to question Homeland Security Secretary Kristi Noem during a media event related to immigration. Separately, Rep. LaMonica McIver (D-N.J.) was hit with federal charges stemming from her earlier attempt to visit a privately operated migrant detention facility in Newark.</p>



<p>Lander was making his third trip to accompany immigrants attending court hearings over <a href="https://www.thecity.nyc/2025/05/28/ice-arrests-migrants-26-federal-plaza-pastor/">the last month</a> as federal agents, often masked, have begun staking out courtrooms to detain people leaving what had been routine hearings. He is the <a href="https://hellgatenyc.com/lander-accompanies-immigrants-ice/">only mayoral candidate</a> who’s done so.&nbsp;</p>

<p>Moments ahead of Lander’s arrest, THE CITY asked him why he was inside immigration court in the final days of the Democratic primary here, rather than out talking to voters.&nbsp;</p>

<p>“I don’t think there’s any place that’s more important to be right now than bearing witness and trying to stand up for the rule of law,” Lander said. “A big question on the campaign trail is how will you stand up to Donald Trump.”</p>

<p>In a dig at the polling frontrunner in the mayoral race, Lander added, “Andrew Cuomo wants to tell a story about what he would do, but he views it as like a finger-poking ego fight — not show up and protect people.”</p>

<p><em>Ben Fractenberg contributed reporting.</em></p>

	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
			<div>
															<p><a href="https://www.thecity.nyc/author/gwynne-hogan/" rel="author">
											<img data-perfmatters-preload="" width="80" height="80" src="https://www.thecity.nyc/wp-content/uploads/2023/10/gwynne_headshot_1-100x100.jpg?crop=1" alt="" srcset="https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/10/gwynne_headshot_1-scaled.jpg?resize=100%2C100&amp;ssl=1 100w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/10/gwynne_headshot_1-scaled.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/10/gwynne_headshot_1-scaled.jpg?resize=1200%2C1200&amp;ssl=1 1200w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/10/gwynne_headshot_1-scaled.jpg?resize=800%2C800&amp;ssl=1 800w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/10/gwynne_headshot_1-scaled.jpg?resize=600%2C600&amp;ssl=1 600w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/10/gwynne_headshot_1-scaled.jpg?resize=400%2C400&amp;ssl=1 400w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/10/gwynne_headshot_1-scaled.jpg?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/10/gwynne_headshot_1-scaled.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/www.thecity.nyc/wp-content/uploads/2023/10/gwynne_headshot_1-100x100.jpg?w=370&amp;ssl=1 370w">											</a></p><div>
					<!-- .author-bio-header -->

											<p>
							Gwynne Hogan is a senior reporter covering immigration, homelessness, and many things in between. Her coverage of the migrant crisis earned her the Newswomen’s Club of New York’s Journalist of the...															
													</p>
					
				</div><!-- .author-bio-text -->

			</div><!-- .author-bio -->
			
</article><!-- #post-${ID} -->
				</div>

			
		</main><!-- #main -->
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Miscalculation by Spanish power grid operator REE contributed to blackout (123 pts)]]></title>
            <link>https://www.reuters.com/business/energy/investigation-into-spains-april-28-blackout-shows-no-evidence-cyberattack-2025-06-17/</link>
            <guid>44301186</guid>
            <pubDate>Tue, 17 Jun 2025 16:54:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/energy/investigation-into-spains-april-28-blackout-shows-no-evidence-cyberattack-2025-06-17/">https://www.reuters.com/business/energy/investigation-into-spains-april-28-blackout-shows-no-evidence-cyberattack-2025-06-17/</a>, See on <a href="https://news.ycombinator.com/item?id=44301186">Hacker News</a></p>
Couldn't get https://www.reuters.com/business/energy/investigation-into-spains-april-28-blackout-shows-no-evidence-cyberattack-2025-06-17/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla Robotaxi launch is a dangerous game of smoke and mirrors (123 pts)]]></title>
            <link>https://electrek.co/2025/06/16/tesla-robotaxi-launch-dangerous-game-smoke-mirrors/</link>
            <guid>44300727</guid>
            <pubDate>Tue, 17 Jun 2025 16:07:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/06/16/tesla-robotaxi-launch-dangerous-game-smoke-mirrors/">https://electrek.co/2025/06/16/tesla-robotaxi-launch-dangerous-game-smoke-mirrors/</a>, See on <a href="https://news.ycombinator.com/item?id=44300727">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="788" src="https://electrek.co/wp-content/uploads/sites/3/2025/01/Tesla-Self-driving-Fremont-factory.png?w=1600" alt="Tesla Self-driving Fremont factory" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/01/Tesla-Self-driving-Fremont-factory.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/01/Tesla-Self-driving-Fremont-factory.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/01/Tesla-Self-driving-Fremont-factory.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/01/Tesla-Self-driving-Fremont-factory.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>Tesla’s upcoming Robotaxi launch in Austin, Texas, is increasingly looking like a game of smoke and mirrors, and a dangerous one at that.</p>



<p>CEO Elon Musk claims Tesla is being “paranoid with safety”, but it is taking risks for the purpose of optics.</p>



<h2 id="h-it-s-all-about-optics">It’s all about optics</h2>



<p>Musk has been wrong about self-driving for years. His track record is marked by missed deadlines and broken promises.</p>



<p>He said:</p>	
	



<blockquote>
<p>“Our goal is, and I feel pretty good about this goal, that we’ll be able to do a demonstration drive of full autonomy all the way from LA to New York, from home in LA to let’s say dropping you off in Times Square in New York, and then having the car go park itself, by the end of next year. Without the need for a single touch, including the charger.”</p>
</blockquote>



<p>That was in 2016, and therefore, he claimed it would happen by the end of 2017. Today, in 2025, Tesla is still not capable of doing that.</p>



<p>Musk has claimed that Tesla would achieve unsupervised self-driving every year for the last decade. It has become a running gag<span>, with many YouTube videos featuring his predictions and&nbsp;<a href="https://en.wikipedia.org/wiki/List_of_predictions_for_autonomous_Tesla_vehicles_by_Elon_Musk" target="_blank">a Wikipedia page</a>&nbsp;tr</span>acking his missed deadlines.</p>



<p>Famously, the predictions are about Tesla achieving self-driving “by the end of the year” or “next year.”</p>



<p>This time, Musk has set a clear deadline of “June” for Tesla to launch its robotaxi service.</p>



<p>With Waymo pulling ahead in the autonomous driving race, now operating in four cities, providing over 200,000 paid rides per week, and soon expanding with 2,000 more vehicles, Musk needs a win to maintain the illusion he has been pushing for a while: that Tesla is the leader in autonomous driving.</p>



<p>He recently claimed about Tesla’s self-driving technology:</p>



<blockquote>
<p>No one is even close. There’s really not a close second. We felt like it was a bit of an iPhone moment — you either get it or you don’t, and there’s a massive gap.</p>
</blockquote>



<p>This is becoming increasingly difficult to claim amid Waymo’s expansion. Still, Musk believes that the robotaxi launch in Austin will help maintain the illusion, even though Waymo has already been operating like Tesla’s plans in Austin for years in other cities and for months in Austin itself.</p>



<h2 id="h-moving-of-the-goal-post">Moving of the Goal Post</h2>



<p>We have often described what Tesla is doing in Austin with its planned “robotaxi” launch as <a href="https://electrek.co/2025/02/10/elon-musk-masterful-move-goalpost-tesla-full-self-driving/" target="_blank" rel="noreferrer noopener">a moving of the goalpost</a>.</p>



<p>For years, Tesla has promised unsupervised self-driving in all its vehicles built since 2016. Musk explicitly said that customers who bought Tesla’s Full Self-Driving package would be able to “go to sleep” at the wheel of their vehicles and wake up in another city.</p>



<p>Now, Musk is claiming that Tesla has “solved” self-driving with its “robotaxi” launch, but it is vastly different from prior promises.</p>



<p>Tesla plans to operate its own small internal fleet of vehicles with dedicated software optimized for a geo-fenced area of Austin and supported by “<a href="https://electrek.co/2025/05/16/tesla-robotaxi-fleet-powered-by-plenty-of-teleoperation/" target="_blank" rel="noreferrer noopener">plenty of teleoperation</a>.” This is a night-and-day difference compared to deploying unsupervised self-driving in customer vehicles, as promised since 2016.</p>



<p>Musk himself is on <a href="https://www.reddit.com/r/SelfDrivingCars/comments/1lan6v1/elon_musk_in_2019_if_you_need_a_geofence_area_you/" target="_blank" rel="noreferrer noopener">record</a> saying, “If you need a geofence area, you don’t have real self-driving.”</p>



		<figure>
			
			
			
		</figure>
		


<p>Now, Musk is on record saying that Tesla will only launch the service in a limited area in Austin and even avoid certain intersections that Tesla is not sure it can handle:</p>



<blockquote>
<p>We will geo‑fence it. It’s not going to take intersections unless we are highly confident it’s going to do well with that intersection. Or it will just take a route around that intersection.</p>
</blockquote>



<p>In addition to geofencing, Tesla is also utilizing teleoperation to control vehicles with human operators remotely. </p>



<p>We reported last year when<a href="https://electrek.co/2024/11/25/tesla-remote-control-team-robotaxis-waymo/" target="_blank" rel="noreferrer noopener"> Tesla started building a “teleoperation team.”</a></p>



<p>Despite Tesla originally planning to launch the robotaxi service on June 12, and now “tentatively” on June 22, the automaker posted a new job listing days ago for engineers to help build a low-latency teleoperation system to operate its “self-driving” cars and robots.</p>



<p>The use of geofencing and teleoperation results in Tesla having the same limitations as Waymo, which Musk claimed means it’s “not real self-driving and not scalable to the customer fleet as promised by Tesla for years.</p>



<h2 id="h-paranoid-about-safety">‘Paranoid’ about Safety</h2>



<p>Musk claims that Tesla is being “super paranoid” about safety, but you have to take his word for it.</p>



<p>We have pointed it out before, but it’s worth repeating: Waymo tested its self-driving vehicles in Austin for six months with safety drivers and then for another six months without safety drivers before launching its autonomous ride-hailing service in the city.</p>



<p>As for Tesla, it tested its vehicles with safety drivers throughout Austin for a few months. Then, Musk announced in late May, only weeks before the planned launch, that <a href="https://electrek.co/2025/05/29/tesla-testing-robotaxi-service-without-drivers-for-several-days-elon-musk/" target="_blank" rel="noreferrer noopener">it had started testing without safety drivers</a>.</p>



<p>Despite many people being on the lookout for these driverless Tesla Robotaxis, <a href="https://electrek.co/2025/06/10/tesla-driverless-robotaxi-spotted-austin-car-trailing/" target="_blank" rel="noreferrer noopener">they were only spotted for the first time last week</a>.</p>



<p>Since then, only two confirmed Tesla vehicles without drivers have been spotted testing.</p>



<p>Furthermore, several of those vehicles were spotted with Tesla employees in the front passenger seat. While Musk claims that there are “no safety driver”, these “passengers” pay attention at all times and have access to a kill switch to stop the vehicle.</p>



<p>They virtually operate like “safety drivers”, but they are on the passenger seat rather than the driver’s seat.</p>



<p>Tesla is currently still in the “testing” phase based on the listing with the state regulators, which also mentions “no” safety drivers:</p>



<figure><img decoding="async" src="https://electrek.co/wp-content/uploads/sites/3/2025/06/Screenshot-2025-06-16-at-8.02.53%E2%80%AFPM.png" alt=""></figure>



<p>To go back to the “optics” for a second, Tesla’s head of self-driving, Ashok Elluswamy, has shared this conveniently cropped image of Tesla’s “robotaxis” being tested in Austin:</p>



<figure><img loading="lazy" decoding="async" height="1024" width="964" src="https://electrek.co/wp-content/uploads/sites/3/2025/06/Screenshot-2025-06-16-at-11.23.42%E2%80%AFPM.png?w=964" alt=""></figure>



<p>The image crops out the passenger seat of the car in front, which would show a Tesla employee, and the driver’s seat of the trailing car, which would show a driver, as spotted in Austin over the last week.</p>



<p>There’s also no way to know precisely at what rates these safety passengers and remote operators are intervening on the self-driving vehicles.</p>



<p>Tesla has never released any intervention or disengagement data about its self-driving and ADAS programs despite using “miles between disengagements” as a metric to track improvements and Musk claiming for years that self-driving is a “solved problem” for Tesla.</p>



<p>As we have previously reported, the best available data comes from a crowdsourced effort. <a href="https://electrek.co/2025/02/06/elon-musk-approved-tesla-full-self-driving-dataset-doubled-whats-the-state-of-fsd-now/" target="_blank" rel="noreferrer noopener">Musk has previously shared and misrepresented the dataset in a positive light</a>.</p>



<p>Currently, the data for the combined two most recent updates (v13.2.8-9) on Tesla’s latest hardware (HW4), which is reportedly the same hardware used in Tesla’s “robotaxis” in Austin, currently sits at 444 miles between critical disengagements:</p>



<figure><img loading="lazy" decoding="async" height="285" width="1024" src="https://electrek.co/wp-content/uploads/sites/3/2025/06/Screenshot-2025-06-16-at-7.52.17%E2%80%AFPM.png?w=1024" alt=""></figure>



<p>That would imply a high risk of an accident every 444 miles without a driver paying attention and ready to take control at all times.</p>



<p>Tesla is also currently actively <a href="https://electrek.co/2025/06/05/tesla-admits-suffer-financial-harm-if-self-driving-crash-data-becomes-public/" target="_blank" rel="noreferrer noopener">fighting in court against organizations trying to access its self-driving crash data</a>.</p>



<p>There are currently efforts to raise concerns about Tesla’s “robotaxi” deployment in Austin.</p>



<p>The Dawn Project attempted to convey the potential danger of Tesla’s upcoming robotaxi fleet by demonstrating how Tesla vehicles fail to stop for school buses with their stop signs activated and can potentially run over children on the latest public Supervised Full Self-Driving (FSD) v13.2.9:</p>



<figure><p>
<iframe id="post-youtube-video-1" title="Tesla to launch 'robotaxis' in Austin next week, but critics say the tech still isn't safe" width="500" height="281" data-src="https://www.youtube.com/embed/NItXNJcOkL4?feature=oembed&amp;rel=0&amp;enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p>Musk has repeatedly highlighted that the vehicles used for the robotaxi service in Austin are the same that it currently delivers to customers, like this one used in this test.</p>



<p>However, they use a new, custom software optimized for Austin, with supposedly more parameters, allowing for greater performance. Still, there is no way to verify this, as Tesla has not released any data.</p>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>I can’t lie. I’m getting extremely concerned about this. I don’t think that we can trust Musk or Tesla in their current state to launch this safely.</p>



<p>As I previously stated, I think Tesla’s FSD would be an incredible product if it were sold as a regular ADAS system, rather than something called “Full Self-Driving,” with the promise that it would eventually become unsupervised.</p>



<p>Tesla wouldn’t <span>face </span>a significant liability for not being able to fulfill its promises to customers, as&nbsp;it has <a href="https://electrek.co/2025/06/02/tesla-has-no-plan-for-hw3-owners-4-months-after-admitting-it-wont-support-self-driving/" target="_blank" rel="noreferrer noopener">already confirmed for HW3 owners</a>.&nbsp;Additionally, safety would be improved, as drivers wouldn’t become so complacent with the technology.</p>



<p>Speaking of those failed promises, they are also what’s driving Tesla to push for this launch in Austin.</p>



<p>As Waymo’s former long-time CEO John Krafcik said about Tesla’s effort: <em>“<a href="https://electrek.co/2025/03/14/waymo-ceo-tesla-robotaxi-launch-fake/" target="_blank" rel="noreferrer noopener">There are many ways to fake a robotaxi service.</a>”</em></p>



<p>Musk badly needs a win with self-driving, and he saw an opportunity to get one by getting his gullible fanbase of Tesla shareholders excited about a glimpse at its long-promised future full of “Tesla robotaxis.”</p>




	<p>As he previously stated, he knows full well that the way Tesla is doing this is not more scalable than Waymo even if the hardware cost per vehicle is lower. The hardware cost is negligible compared to teleoperation, development, insurance, and other expenses.</p>



<p>Even with all the smoke and mirrors involved with this project, it’s becoming clear that Tesla is not even ready for it. </p>



<p>Now, the question is whether Musk lets the June deadline slip and takes another ‘L’ on self-driving, or if he pushes for Tesla to launch the potentially dangerous service with lots of limitations.</p>



<p>With the federal government in complete shambles and the Texas government being too close to Musk and Tesla, I wouldn’t count on the regulators to act here. Although they probably should.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making 2.5 Flash and 2.5 Pro GA, and introducing Gemini 2.5 Flash-Lite (343 pts)]]></title>
            <link>https://blog.google/products/gemini/gemini-2-5-model-family-expands/</link>
            <guid>44300717</guid>
            <pubDate>Tue, 17 Jun 2025 16:06:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/">https://blog.google/products/gemini/gemini-2-5-model-family-expands/</a>, See on <a href="https://news.ycombinator.com/item?id=44300717">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;We’re expanding our Gemini 2.5 family of models&quot;
  }">
      <div>
          
            <p>Jun 17, 2025</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Gemini 2.5 Flash and Pro are now generally available, and we’re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.
        </p>
      
    </div>

    

    
      










<div>
    <figure>
      <div>
        <p><img alt="Blue and black futuristic illustration with Gemini 2.5 logo in the middle" data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_bundle_keyword_blog_header_20.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_bundle_keyword_blog_header_20.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_bundle_keyword_blog_header_2.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_bundle_keyword_blog_header_2.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_bundle_keyword_blog_header_2.width-2200.format-webp.webp 2200w">
        </p>
      </div>
      
    </figure>
  </div>






    

    
    <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="We’re expanding our Gemini 2.5 family of models" listen-to-article="Listen to article" data-date-modified="2025-06-17T16:00:01.204310+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;We’re expanding our Gemini 2.5 family of models&quot;
         }"><p data-block-key="8yppc">We designed Gemini 2.5 to be a family of hybrid reasoning models that provide amazing performance, while also being at the <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf#page=3">Pareto Frontier</a> of cost and speed. Today, we’re taking the next step with our 2.5 Pro and Flash models by releasing them as stable and generally available. And we’re bringing you 2.5 Flash-Lite in preview — our most cost-efficient and fastest 2.5 model yet.</p><h2 data-block-key="5f1o6">Making 2.5 Flash and 2.5 Pro generally available</h2><p data-block-key="7vsso">Thanks to all of your feedback, today we’re releasing stable versions of 2.5 Flash and Pro, so you can build production applications with confidence. <a href="https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/">Developers</a> like Spline and Rooms and <a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-2-5-flash-lite-flash-pro-ga-vertex-ai">organizations</a> like Snap and SmartBear have already been using the latest versions in-production for the last few weeks.</p><h2 data-block-key="eje8v">Introducing Gemini 2.5 Flash-Lite</h2><p data-block-key="a8cs2">We’re also introducing a preview of the new Gemini 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet. You can start building with the preview version now, and we’re looking forward to your feedback.</p><p data-block-key="38ei">2.5 Flash Lite has all-around higher quality than 2.0 Flash-Lite on coding, math, science, reasoning and multimodal benchmarks. It excels at high-volume, latency-sensitive tasks like translation and classification, with lower latency than 2.0 Flash-Lite and 2.0 Flash on a broad sample of prompts. It comes with the same capabilities that make Gemini 2.5 helpful, including the ability to turn thinking on at different budgets, connecting to tools like Google Search and code execution, multimodal input, and a 1 million-token context length.</p><p data-block-key="80lci">See more details about our 2.5 family of models in the latest <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf">Gemini technical report</a>.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Gemini 2.5 Flash Lite benchmarks table" external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="We’re expanding our Gemini 2.5 family of models" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="Gemini 2.5 Flash Lite benchmarks table" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_2-5_benchmarks_margin_light2x_1.gif">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;We’re expanding our Gemini 2.5 family of models&quot;
         }"><p data-block-key="ymdz3">The preview of Gemini 2.5 Flash-Lite is now available in Google AI Studio and Vertex AI, alongside the stable versions of 2.5 Flash and Pro. Both 2.5 Flash and Pro are also accessible in the Gemini app. We’ve also brought custom versions of 2.5 Flash-Lite and Flash to Search.</p><p data-block-key="70e7i">We can’t wait to see what you continue to build with Gemini 2.5.</p></div>
  


            
            

            
              




            
          </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Honda Conducts Successful Launch and Landing of Experimental Reusable Rocket (1134 pts)]]></title>
            <link>https://global.honda/en/topics/2025/c_2025-06-17ceng.html</link>
            <guid>44300102</guid>
            <pubDate>Tue, 17 Jun 2025 15:02:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://global.honda/en/topics/2025/c_2025-06-17ceng.html">https://global.honda/en/topics/2025/c_2025-06-17ceng.html</a>, See on <a href="https://news.ycombinator.com/item?id=44300102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <div><p>TOKYO, Japan, June 17, 2025 – Honda R&amp;D Co., Ltd., a research and development subsidiary of Honda Motor Co., Ltd., today conducted a launch and landing test of an experimental reusable rocket<sup>*1</sup> (6.3 m in length, 85 cm in diameter, 900 kg dry weight/1,312 kg wet weight) developed independently by Honda. The test was completed successfully, the first time Honda landed a rocket after reaching an altitude of 300 meters.</p>
<p>This test marked the first launch and landing test conducted by Honda with an aim to demonstrate key technologies essential for rocket reusability, such as flight stability during ascent and descent, as well as landing capability. Through this successful test, Honda achieved its intended rocket behaviors for the launch and landing (reaching an altitude of 271.4 m, and landing at 37cm of the target touchdown point, flight duration 56.6 sec), while obtaining data during the ascent and descent.</p>
</div>
<div>

<p>
    <h3><b>＜Overview of launch and landing test of a Honda reusable rocket＞</b>
</h3>
</p>


    
</div>
<div>
<p>・Purpose:&nbsp;&nbsp;&nbsp;&nbsp; Establishment of key technologies necessary for a reusable rocket<br>
・Location:&nbsp;&nbsp;&nbsp;&nbsp; Honda facility in Taiki Town<sup>*2</sup>, Hiroo District, Hokkaido Prefecture, Japan<br>
・Date/Time:&nbsp; June 17, 2025. Launch time: 16:15</p>
</div>

<div><p>*1 A reusable rocket, also known as a reusable launch vehicle (RLV), is a type of rocket that, unlike a conventional expendable launch vehicle (ELV), can be used repeatedly in a short period of time. A reusable rocket is launched in a vertical position, reaches an altitude of around 100 kilometers, and then lands back on earth while maintaining a vertical position.</p>
<p>*2 Taiki Town, located in southeastern Hokkaido, Japan, has been developing itself as a “space town” through the joint efforts of public and private sectors, and various aviation/space related tests are being conducted by a wide range of organizations including JAXA (Japan Aerospace Exploration Agency), businesses and universities.</p>
</div>
<div>

<p>
    <h3><b>＜Safety measures for Honda rocket testing＞</b>
</h3>
</p>


    
</div>
<div>
<p>Since 2024, Honda has been safely conducting engine combustion tests and hovering tests for its reusable rockets in Taiki Town in Hiroo District, Hokkaido, Japan. As with all previous tests, Honda conducted this launch and landing test while placing the highest priority on safety and with the understanding and cooperation of local authorities and residents.&nbsp;</p>
</div>
<div>

<p>
    <h4><b>Specific safety measures taken for this test</b>
</h4>
</p>


    
</div>
<div><ul>
<li>A restricted area with a 1-kilometer radius was established. During the test, access to the area was restricted through the installation of signs and gates, as well as the deployment of security personnel.<b></b></li>
<li>The restricted area was defined by calculating the potential area where the rocket could fall to earth in the event of a thrust cut-off and by adding a sufficient buffer zone (safe distance calculated based on the guidelines set forth by the Cabinet Office of the Government of Japan) that would cover the potential area where a blast wave, debris dispersion, or fireballs could impact in the event of an explosion within the potential fall area.<b></b></li>
<li>The experimental rocket was equipped with a safety system to prevent deviation from a pre-defined flight corridor, speed and attitude conditions, ensuring no impact beyond the restricted area.<b></b></li>
</ul>
</div>
<div>

<p>
    <h3><b>＜Honda initiatives in the areas of rocket research＞</b>
</h3>
</p>


    
</div>
<div><p>As announced in 2021, Honda has been pursuing research and development in the field of space technologies while viewing it as a place to take on challenges to realize the “dreams” and “potential” of people worldwide while leveraging its core technologies. Honda has the aim to enable people to transcend the constraints of time, place or ability and make people’s daily lives more enjoyable. Examples of Honda initiatives toward creating new value in the ultimate environment of outer space include research into a circulative renewable energy system, key robotic technologies for use in outer space and reusable rockets.&nbsp;</p>
<p>Inspired by the dream of young Honda engineers who wanted to build rockets by utilizing core technologies amassed by Honda through the development of various products, such as combustion and control technologies, Honda started rocket research based on the belief that it has the potential to contribute more to people’s daily lives by launching satellites with its own rockets, that could lead to various services<sup>*3</sup> that are also compatible with other Honda business.</p>
<p>In today’s world, vast amounts of data are consumed, with the growing expectation for greater utilization of a data system in outer space through expanded use of satellites. In light of this trend, the need for satellite launch rockets is also expected to increase in the coming years.</p>
<p>In this market environment, Honda has chosen to take on the technological challenge of developing reusable rockets by utilizing Honda technologies amassed in the development of various products and automated driving systems, based on a belief that reusable rockets will contribute to achieving sustainable transportation.</p>
<p>Although Honda rocket research is still in the fundamental research phase, and no decisions have been made regarding commercialization of these rocket technologies, Honda will continue making progress in the fundamental research with a technology development goal of realizing technological capability to enable a suborbital launch by 2029.&nbsp;</p>
</div>
<div>

<p>
    <h4><b>Comments by Toshihiro Mibe, Global CEO of Honda</b>
</h4>
</p>


    
</div>
<div>
<p>“We are pleased that Honda has made another step forward in our research on reusable rockets with this successful completion of a launch and landing test. We believe that rocket research is a meaningful endeavor that leverages Honda’s technological strengths. Honda will continue to take on new challenges—not only to offer our customers various services and value through our products, while addressing environmental and safety issues, but also to continue creating new value which will make people’s time and place more enjoyable.”</p>
</div>
<div>
<p>*3 Examples include remote sensing to monitor Earth conditions such as global warming and extreme weather, as well as satellite constellations that enable wide-area communication, which is an essential component for connected features of mobility products.</p>
</div>

    
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Timescale Is Now TigerData (136 pts)]]></title>
            <link>https://www.tigerdata.com/blog/timescale-becomes-tigerdata</link>
            <guid>44300064</guid>
            <pubDate>Tue, 17 Jun 2025 14:58:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tigerdata.com/blog/timescale-becomes-tigerdata">https://www.tigerdata.com/blog/timescale-becomes-tigerdata</a>, See on <a href="https://news.ycombinator.com/item?id=44300064">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Subscribe to the TigerData Newsletter</p><div><form></form></div><p>By submitting, you acknowledge TigerData's<!-- --> <!--$--><a href="https://www.tigerdata.com/legal/privacy">Privacy Policy</a><!--/$--></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Now might be the best time to learn software development (249 pts)]]></title>
            <link>https://substack.com/home/post/p-165655726</link>
            <guid>44299979</guid>
            <pubDate>Tue, 17 Jun 2025 14:51:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://substack.com/home/post/p-165655726">https://substack.com/home/post/p-165655726</a>, See on <a href="https://news.ycombinator.com/item?id=44299979">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><h3 translated="">The app for independent voices</h3></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why JPEGs still rule the web (2024) (178 pts)]]></title>
            <link>https://spectrum.ieee.org/jpeg-image-format-history</link>
            <guid>44299970</guid>
            <pubDate>Tue, 17 Jun 2025 14:51:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/jpeg-image-format-history">https://spectrum.ieee.org/jpeg-image-format-history</a>, See on <a href="https://news.ycombinator.com/item?id=44299970">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Why JPEGs Still Rule the Web"><p><em><em>A version of this post </em></em><a href="https://tedium.co/2024/06/16/jpeg-image-format-history/" rel="noopener noreferrer" target="_blank"><em><em>originally appeared</em></em></a><em><em> on </em></em><a href="https://tedium.co/" rel="noopener noreferrer" target="_blank"><em><em>Tedium</em></em></a><em><em>, Ernie Smith’s newsletter, which hunts for the end of the long tail.</em></em></p><p>For roughly three decades, the JPEG has been the World Wide Web’s primary image format. But it wasn’t the one the Web started with. In fact, the first mainstream graphical browser, NCSA Mosaic, didn’t initially support inline JPEG files—<a href="https://ftp.jurassic.nl/pub/irix/mosaic/Mac/FAQ/FAQ.HTML" rel="noopener noreferrer" target="_blank">just inline GIFs</a>, along with a couple of other <a href="https://spectrum.ieee.org/carnegie-mellon-is-saving-old-software-from-oblivion" target="_blank">formats forgotten to history</a>. However, the JPEG had many advantages over the format it quickly usurped.</p><p data-rm-resized-container="25%"><a href="https://tedium.co/" target="_blank"></a><a title="Select for lightbox">aspect_ratio</a><a href="https://tedium.co/" target="_blank"><img alt="Tedium logo, a red rectangle with the word Tedium in white, above the text &quot;This post originally appeared on Tedium.&quot;" data-rm-shortcode-id="c603546dab9e1dd1612b1364d3107471" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/tedium-logo-a-red-rectangle-with-the-word-tedium-in-white-above-the-text-this-post-originally-appeared-on-tedium.png?id=60568211&amp;width=980" height="900" id="6aeb5" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/tedium-logo-a-red-rectangle-with-the-word-tedium-in-white-above-the-text-this-post-originally-appeared-on-tedium.png?id=60568211&amp;width=980" width="3000"></a><small placeholder="add photo credit..."><a href="https://spectrum.ieee.org/media-library/eyjhbgcioijiuzi1niisinr5cci6ikpxvcj9.eyjpbwfnzsi6imh0dhbzoi8vyxnzzxrzlnjibc5tcy82mdu2odixms9vcmlnaw4ucg5niiwizxhwaxjlc19hdci6mtc1nzmynzi1mn0._gbglxpbsmfwoobs84_whxbl_vnslwx1geovlhgvwku/image.png?width=980" target="_blank"></a></small></p><p>Despite not appearing together right away—it first appeared in Netscape in 1995, three years after the image standard was officially published—the JPEG and web browser fit together naturally. JPEG files degraded more gracefully than GIFs, retaining more of the picture’s initial form—and that allowed the format to scale to greater levels of success. While it wasn’t capable of animation, it progressively expanded from something a modem could pokily render to a format that was good enough for high-end professional photography.</p><p>For the internet’s purposes, the degradation was the important part. But it wasn’t the only thing that made the JPEG immensely valuable to the digital world. An essential part was that it was a documented standard built by numerous stakeholders.</p><h2>The GIF was a de facto standard. The JPEG was an actual one</h2><p>How important is it that JPEG was a standard? Let me tell you a story.</p><p>During <a href="https://archive.nytimes.com/bits.blogs.nytimes.com/2013/05/21/an-honor-for-the-creator-of-the-gif/?smid=tw-nytimes" target="_blank">a 2013 <em><em>New York Times</em></em> interview</a> conducted just before he received an award honoring his creation, GIF creator Steve Wilhite stepped into a debate he unwittingly created. <span>Simply put, nobody knew how to pronounce the acronym for the image format he had fostered, the Graphics Interchange Format. He used the moment to attempt to set the record straight—it was pronounced like the peanut butter brand: “It is a soft ‘G,’ pronounced ‘jif.’ End of story,” he said.</span></p><p>I <a href="https://shortformblog.com/post/51026114908/steve-wilhite-gif-award" target="_blank">posted a quote from Wilhite</a> on my popular Tumblr around that time, a period when the <a href="https://spectrum.ieee.org/tag/social-media">social media</a> site was the center of the GIF universe. And soon afterward, my post got thousands of reblogs—nearly all of them disagreeing with Wilhite. Soon, <a href="https://knowyourmeme.com/memes/gif-vs-jif-pronunciation-debate/" target="_blank">Wilhite’s quote became a meme</a>.</p><p>The situation paints how Wilhite, who died in 2022, did not develop his format by committee. He could say it sounded like “JIF” because he built it himself. He was handed the project as a <a href="https://spectrum.ieee.org/tag/compuserve">CompuServe</a> employee in 1987; he produced the object, and that was that. The initial document describing how it works? <a href="https://www.w3.org/Graphics/GIF/spec-gif87.txt" target="_blank">Dead simple</a>. 38 years later, we’re still using the GIF—but it never rose to the same prevalence of JPEG.</p><p>The JPEG, which formally emerged about five years later, was very much <em><em>not</em></em> that situation. Far from it, in fact—it’s the difference between a de facto standard and an actual one. And that proved essential to its eventual ubiquity.</p><p><img alt="Full resolution photo of a sunlit pine forest with a narrow trail winding through the trees and grassy undergrowth." data-rm-shortcode-id="b24d49ca553a838abbd2d8e0f9fb221c" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/full-resolution-photo-of-a-sunlit-pine-forest-with-a-narrow-trail-winding-through-the-trees-and-grassy-undergrowth.jpg?id=61013768&amp;width=980" height="3982" id="2999a" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/full-resolution-photo-of-a-sunlit-pine-forest-with-a-narrow-trail-winding-through-the-trees-and-grassy-undergrowth.jpg?id=61013768&amp;width=980" width="5973"><small placeholder="Add Photo Caption...">We’re going to degrade the quality of this image throughout this article. At its full image size, it’s 13.7 megabytes.</small><small placeholder="Add Photo Credit...">Irina Iriser</small></p><h2>How the JPEG format came to life</h2><p>Built with input from dozens of stakeholders, the Joint Photographic Experts Group ultimately aimed to create a format that fit everyone’s needs. (Reflecting its committee-led roots, there would be no confusion about the format’s name—an acronym of the organization that designed it.) And when the format was finally unleashed on the world, it was the subject of a more than 600-page book.</p><p><em><em>JPEG: Still Image Data Compression Standard</em></em>, written by <a href="https://spectrum.ieee.org/tag/ibm">IBM</a> employees and JPEG organization stakeholders William B. Pennebaker and Joan L. Mitchell, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&amp;gbpv=1&amp;pg=PA1&amp;printsec=frontcover" target="_blank">describes</a> a landscape of multimedia imagery, held back without a way to balance the need for photorealistic images and immediacy. Standardization, they believed, could fix this.</p><p>“The problem was not so much the lack of <a href="https://spectrum.ieee.org/tag/algorithms">algorithms</a> for image compression (as there is a long history of technical work in this area),” the authors wrote, “but, rather, the lack of a standard algorithm—one which would allow an interchange of images between diverse applications.”</p><p>And they were absolutely right. For more than 30 years, JPEG has made high-quality, high-resolution photography accessible in <a href="https://spectrum.ieee.org/tag/operating-systems">operating systems</a> far and wide. Although we no longer need to compress JPEGs to within an inch of their life, having that capability helped enable the modern <a href="https://spectrum.ieee.org/tag/internet">internet</a>.</p><p><a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&amp;gbpv=1&amp;dq=ibm+jpeg&amp;pg=PA278&amp;printsec=frontcover" target="_blank">As the book notes</a>, Mitchell and Pennebaker were given IBM’s support to follow through this research and work with the JPEG committee, and that support led them to develop many of the JPEG format’s foundational patents. Described in <a href="https://patents.google.com/patent/US4905297" target="_blank">patents</a> filed by Mitchell and Pennebaker in 1988, IBM and other members of the JPEG standards committee, such as AT&amp;T and Canon, were developing ways to use compression to make high-quality images easier to deliver in confined settings.</p><p>Each member brought their own needs to the process. Canon, obviously, was more focused on <a href="https://spectrum.ieee.org/tag/printers">printers</a> and photography, while AT&amp;T’s interests were tied to data transmission. Together, the companies left behind a standard that has stood the test of time.</p><p>All this means, funnily enough, that the first place that a program capable of using JPEG compression appeared was not MacOS or Windows, but OS/2—a fascinating-but-failed graphical <a href="https://spectrum.ieee.org/tag/operating-system">operating system</a> created by Pennebaker and Mitchell’s employer, IBM. As early as 1990, OS/2 supported the format through the <a href="https://www.edm2.com/index.php/OS/2_Image_Support" target="_blank">OS/2 Image Support</a> application.</p><p><img alt="Nearly identical photo of a sunlit pine forest." data-rm-shortcode-id="b810c6423ebd0b3e07f4d42c4c7162ac" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/nearly-identical-photo-of-a-sunlit-pine-forest.jpg?id=61015732&amp;width=980" height="3982" id="ef951" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/nearly-identical-photo-of-a-sunlit-pine-forest.jpg?id=61015732&amp;width=980" width="5973"><small placeholder="Add Photo Caption...">At 50 percent of its initial quality, the image is down to about 2.6 MB. By dropping half of the image’s quality, we brought it down to one-fifth of the original file size. </small><small placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><h2>What a JPEG does when you heavily compress it</h2><p>The thing that differentiates a JPEG file from a PNG or a GIF is how the data degrades as you compress it. The goal for a JPEG image is to still look like a photo when all is said and done, even if some compression is necessary to make it all work at a reasonable size. That way, you can display something that looks close to the original image in fewer bytes.</p><p>Or, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&amp;gbpv=1&amp;pg=PA4&amp;printsec=frontcover" target="_blank">as Pennebaker and Mitchell put it</a>, “the most effective compression is achieved by approximating the original image (rather than reproducing it exactly).”</p><p>Central to this is a compression process called <a href="https://spectrum.ieee.org/compression-algorithms" target="_blank">discrete cosine transform</a> (DCT), a lossy form of compression encoding heavily used in all sorts of compressed formats, most notably in <a href="https://spectrum.ieee.org/tag/digital-audio">digital audio</a> and <a href="https://spectrum.ieee.org/tag/signal-processing">signal processing</a>. Essentially, it delivers a lower-quality product by removing details, while still keeping the heart of the original product through approximation. The stronger the cosine transformation, the more compressed the final result.</p><p>The algorithm, <a href="https://ieeexplore.ieee.org/abstract/document/1672377" target="_blank">developed by researchers</a> in the 1970s, essentially takes a grid of data and treats it as if you’re controlling its frequency with a knob. The data rate is controlled like water from a faucet: The more data you want, the higher the setting. DCT allows a trickle of data to still come out in highly compressed situations, even if it means a slightly compromised result. In other words, you may not keep all the data when you compress it, but DCT allows you to keep the heart of it.</p><p>(See <a href="https://www.youtube.com/watch?v=Q2aEzeMDHMA" target="_blank">this video</a> for a more technical but still somewhat easy-to-follow description of DCT.)</p><p>DCT is everywhere. If you <a href="https://ottverse.com/discrete-cosine-transform-dct-video-compression/" target="_blank">have ever seen a streaming video</a> or an online radio stream that degraded in quality because your bandwidth suddenly declined, you’ve witnessed DCT being utilized in real time.</p><p>A JPEG file doesn’t have to leverage the DCT with just one method, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&amp;gbpv=1&amp;pg=PA81&amp;printsec=frontcover" target="_blank">as <em><em>JPEG: Still Image Data Compression Standard</em></em> explains</a>:</p><p>The JPEG standard describes a family of large image compression techniques, rather than a single compression technique. It provides a “tool kit” of compression techniques from which applications can select elements that satisfy their particular requirements.</p><p>The toolkit has four modes:</p><ul><li><strong>Sequential DCT,</strong> which displays the compressed image in order, like a window shade slowly being rolled down</li><li><strong>Progressive DCT,</strong> which displays the full image in the lowest-resolution format, then adds detail as more information rolls in</li><li><strong>Sequential lossless,</strong> which uses the window shade format but doesn’t compress the image</li><li><strong>Hierarchical mode,</strong> which combines the prior three modes—so maybe it starts with a progressive mode, then loads DCT compression slowly, but then reaches a lossless final result</li></ul><p>At the time the JPEG was being created, <a href="https://spectrum.ieee.org/tag/modems">modems</a> were extremely common. That meant images loaded slowly, making Progressive DCT the most fitting format for the early internet. Over time, the progressive DCT mode has become less common, as many computers can simply load the sequential DCT in one fell swoop.</p><p><img alt="The same photo of a sunlit pine forest with very slight degradation visible." data-rm-shortcode-id="f045496eded7dca7812ee1bcaa6bbff1" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/the-same-photo-of-a-sunlit-pine-forest-with-very-slight-degradation-visible.jpg?id=61029700&amp;width=980" height="3982" id="fd0cd" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/the-same-photo-of-a-sunlit-pine-forest-with-very-slight-degradation-visible.jpg?id=61029700&amp;width=980" width="5973"><small placeholder="Add Photo Caption...">That same forest, saved at 5 percent quality. Down to about 419 kilobytes.</small><small placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>When an image is compressed with DCT, the change tends to be less noticeable in busier, more textured areas of the picture, like hair or foliage. Those areas are harder to compress, which means they keep their integrity longer. It tends to be more noticeable, however, with solid colors or in areas where the image sharply changes from one color to another—like text on a page. Ever screenshot a social media post, only for it to look noisy? Congratulations, you just made a JPEG file.</p><p>Other formats, like PNG, do better with text, because their compression format is intended to be non-lossy. (Side note: PNG’s compression format, DEFLATE, <a href="https://www.ietf.org/rfc/rfc1951" target="_blank">was designed</a> by Phil Katz, who also created the ZIP format. The PNG format uses it in part because it was a license-free compression format. So it turns out the brilliant coder with the <a href="https://www.wsj.com/articles/SB961363319756539141" target="_blank">sad life story</a> improved the internet in multiple ways before his <a href="https://tedium.co/2015/02/17/early-internet-history-tales/" target="_blank">untimely passing</a>.)</p><p>In many ways, the JPEG is one tool in our image-making toolkit. Despite its age and maturity, it remains one of our best options for sharing photos on the internet. But it is not a tool for every setting—despite the fact that, like a wrench sometimes used as a hammer, we often leverage it that way.</p><h2>Forgent Networks claimed to own the JPEG’s defining algorithm</h2><p>The JPEG format gained popularity in the ’90s for reasons beyond the quality of the format. Patents also played a role: Starting in 1994, the tech company Unisys <a href="https://www.theregister.com/1999/09/01/unisys_demands_5k_licence_fee/" target="_blank">attempted to bill individual users</a> who relied on GIF files, which used a patent the company owned. This made the free-to-use JPEG more popular. (This situation also led to the creation of the patent-free PNG format.)</p><p>While the JPEG was standards-based, it could still have faced the same fate as the GIF, thanks to the quirks of the patent system. A few years before the file format came to life, a pair of Compression Labs employees <a href="https://patents.google.com/patent/US4698672A/en" target="_blank">filed a patent application</a> that dealt with the compression of motion graphics. By the time anyone noticed its similarity to JPEG compression, the format was ubiquitous.</p><p><img alt="The same photo of a sunlit pine forest with more noticeable color degradation visible. Areas with previously subtle color gradients now appear more like blocks of color." data-rm-shortcode-id="5fb227e9168811101372ad575e42dc89" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/the-same-photo-of-a-sunlit-pine-forest-with-more-noticeable-color-degradation-visible-areas-with-previously-subtle-color-gradie.jpg?id=61016218&amp;width=980" height="3982" id="e1296" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/the-same-photo-of-a-sunlit-pine-forest-with-more-noticeable-color-degradation-visible-areas-with-previously-subtle-color-gradie.jpg?id=61016218&amp;width=980" width="5973"><small placeholder="Add Photo Caption...">Our forest, saved at 1 percent quality. This image is only about 239 KB in size, yet it’s still easily recognizable as the same photo. That’s the power of the JPEG.</small><small placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>Then in 1997, a company named Forgent Networks acquired Compression Labs. The company eventually spotted the patent and began filing lawsuits over it, a series of events it saw as a stroke of good luck.</p><p>“The patent, in some respects, is a lottery ticket,” Forgent Chief Financial Officer Jay Peterson <a href="https://www.cnet.com/tech/tech-industry/staking-a-claim-in-the-patent-gold-mine/" target="_blank">told <em><em>CNET</em></em> in 2005</a>. “If you told me five years ago that ‘You have the patent for JPEG,’ I wouldn’t have believed it.”</p><p>While Forgent’s claim of ownership of the JPEG <a href="https://spectrum.ieee.org/tag/compression-algorithm">compression algorithm</a> was tenuous, it ultimately saw more success with its legal battles than Unisys did. The company earned more than $100 million from <a href="https://spectrum.ieee.org/tag/digital-camera">digital camera</a> makers before the patent finally ran out of steam around 2007. The company also attempted to extract licensing fees from the PC industry. Eventually, Forgent agreed <a href="https://www.cnet.com/tech/tech-industry/forgent-settles-jpeg-patent-cases/" target="_blank">to a modest $8 million</a> settlement.</p><p>As the company took an increasingly aggressive approach to its acquired patent, it began to lose battles both in the court of public opinion and in actual courtrooms. <a href="https://arstechnica.com/uncategorized/2006/05/6930-2/" target="_blank">Critics pounced on examples of prior art</a>, while courts limited the patent’s use to motion-based uses like video.</p><p>By 2007, Forgent’s compression patent expired—and its litigation-heavy approach to business went away. That year, the company became <a href="https://www.asuresoftware.com/" target="_blank">Asure Software</a>, which now specializes in <a href="https://spectrum.ieee.org/tag/payroll">payroll</a> and HR solutions. Talk about a reboot.</p><h2>Why the JPEG won’t die</h2><p>The JPEG file format has served us well. It’s been difficult to remove the format from its perch. The JPEG 2000 format, for example, was intended to supplant it by offering more lossless options and better performance. The format is <a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000143.shtml" target="_blank">widely used by the Library of Congress</a> and specialized sites like the <a href="https://spectrum.ieee.org/tag/internet-archive">Internet Archive</a>, however, it is less popular as an end-user format.</p><p data-rm-resized-container="25%"><img alt="Animated GIF of the forest images, starting at full resolution and progressing through increasingly degraded version of the iamge." data-rm-shortcode-id="ea7ce7f87d0b7e0afed75e4a9a57e2a7" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/animated-gif-of-the-forest-images-starting-at-full-resolution-and-progressing-through-increasingly-degraded-version-of-the-iamg.gif?id=61016209&amp;width=980" height="240" id="171a2" lazy-loadable="true" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20360%20240'%3E%3C/svg%3E" width="360"><small placeholder="Add Photo Caption...">See the forest JPEG degrade from its full resolution to 1 percent quality in this GIF. </small><small placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>Other image technologies have had somewhat more luck getting past the JPEG format. The Google-supported <a href="https://developers.google.com/speed/webp" target="_blank">WebP</a> is popular with website developers (<a href="https://www.pcgamer.com/heres-why-you-have-to-deal-with-so-many-annoying-webps-now/" target="_blank">and controversial</a> with end users). Meanwhile, the formats <a href="https://aomediacodec.github.io/av1-avif/" target="_blank">AVIF</a> and <a href="https://www.iso.org/standard/83650.html" target="_blank">HEIC</a>, each developed by standards bodies, have largely outpaced both JPEG and JPEG 2000.</p><p>Still, the JPEG will be difficult to kill at this juncture. These days, the format is similar to <a href="https://spectrum.ieee.org/tag/mp3">MP3</a> or ZIP files—two legacy formats too popular and widely used to kill. Other formats that compress the files better and do the same things more efficiently are out there, but it’s difficult to topple a format with a 30-year head start.</p><p>Shaking off the JPEG is easier said than done. I think most people will be fine to keep it around.</p><p><em><em>Ernie Smith is the editor of </em></em><a href="https://tedium.co/" target="_blank"><em><em>Tedium</em></em></a><em><em>, a long-running newsletter that hunts for the end of the long tail.</em></em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[O3 Turns Pro (156 pts)]]></title>
            <link>https://thezvi.substack.com/p/o3-turns-pro</link>
            <guid>44299947</guid>
            <pubDate>Tue, 17 Jun 2025 14:49:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thezvi.substack.com/p/o3-turns-pro">https://thezvi.substack.com/p/o3-turns-pro</a>, See on <a href="https://news.ycombinator.com/item?id=44299947">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>You can now have o3 throw vastly more compute at a given problem. That’s o3-pro. </p><p>Should you have o3 throw vastly more compute at a given problem, if you are paying the $200/month subscription price for ChatGPT Pro? Should you pay the $200, or the order of magnitude markup over o3 to use o3-pro in the API?</p><p>That’s trickier. Sometimes yes. Sometimes no. My experience so far is that waiting a long time is annoying, sufficiently annoying that you often won’t want to wait. Whenever I ask o3-pro something, I often also have been asking o3 and Opus. </p><p>Using the API at scale seems prohibitively expensive for what you get, and you can (and should) instead run parallel queries using the chat interface.</p><p>The o3-pro answers have so far definitely been better than o3, but the wait is usually enough to break my workflow and human context window in meaningful ways - fifteen minutes plus variance is past the key breakpoint, such that it would have not been substantially more painful to fully wait for Deep Research. </p><p>Indeed, the baseline workflow feels similar to Deep Research, in that you fire off a query and then eventually you context shift back and look at it. But if you are paying the subscription price already it’s often worth queuing up a question and then having it ready later if it is useful. </p><p>In many ways o3-pro still feels like o3, only modestly better in exchange for being slower. Otherwise, same niche. If you were already thinking ‘I want to use Opus rather than o3’ chances are you want Opus rather than, or in addition to, o3-pro. </p><p>Perhaps the most interesting claim, from some including Tyler Cowen, was that o3-pro is perhaps not a lying liar, and hallucinates far less than o3. If this is true, in many situations it would be worth using for that reason alone, provided the timing allows this. The bad news is that it didn’t improve on a Confabulations benchmark.  </p><p><a href="https://x.com/TheZvi/status/1933563488602837286" rel="">My poll (n=19) was roughly evenly split on this question</a><span>. </span></p><p>My hunch, based on my use so far, is that o3-pro is hallucinating modestly less because:</p><ol><li><p>It is more likely to find or know the right answer to a given question, which is likely to be especially relevant to Tyler’s observations.</p></li><li><p>It is considering its answer a lot, so it usually won’t start writing an answer and then think ‘oh I guess that start means I will provide some sort of answer’ like o3.</p></li><li><p>The queries you send are more likely to be well-considered to avoid the common mistake of essentially asking for hallucinations. </p></li></ol><p>But for now I think you still have to have a lot of the o3 skepticism.</p><p><span>And as always, the next thing will be here soon, </span><a href="https://x.com/HCSolakoglu/status/1934921625583145415" rel="">Gemini 2.5 Pro Deep Think is coming</a><span>.</span></p><p><a href="https://x.com/elder_plinius/status/1932608359028391998" rel="">Pliny of course jailbroke it, for those wondering</a><span>. Pliny also </span><a href="https://x.com/elder_plinius/status/1932921389427597757" rel="">offers us the tools and channels information</a><span>.</span></p><p><span>My poll strongly </span><a href="https://x.com/TheZvi/status/1933563487315186180" rel="">suggested o3-pro is slightly stronger than o3</a><span>. </span></p><blockquote><p><a href="https://x.com/gdb/status/1932561536268329463" rel="">Greg Brockman</a><span> (OpenAI): o3-pro is much stronger than o3.</span></p><p><a href="https://x.com/OpenAI/status/1932530411651150013" rel="">OpenAI</a><span>: In expert evaluations, reviewers consistently prefer OpenAI o3-pro over o3, highlighting its improved performance in key domains—including science, education, programming, data analysis, and writing.</span></p><p>Reviewers also rated o3-pro consistently higher for clarity, comprehensiveness, instruction-following, and accuracy.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png" width="1200" height="675" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:675,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:126229,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://thezvi.substack.com/i/165878744?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8f2b152-7e74-4143-b930-9c8e4432ed08_1200x675.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Like OpenAI o1-pro, OpenAI o3-pro excels at math, science, and coding as shown in academic evaluations.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png" width="1200" height="675" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:675,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:213155,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://thezvi.substack.com/i/165878744?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ae699a-02dc-45c8-9b96-8ae0122b46e6_1200x675.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>To assess the key strength of OpenAI o3-pro, we once again use our rigorous "4/4 reliability" evaluation, where a model is considered successful only if it correctly answers a question in all four attempts, not just one.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png" width="1200" height="675" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:675,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:222044,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://thezvi.substack.com/i/165878744?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fea1ff-bc5c-42fd-80df-b2dec9bbf31d_1200x675.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>OpenAI o3-pro has access to tools that make ChatGPT useful—it can search the web, analyze files, reason about visual inputs, use Python, personalize responses using memory, and more.</p><p><a href="https://x.com/sama/status/1932532561080975797" rel="">Sam Altman</a><span>: o3-pro is rolling out now for all chatgpt pro users and in the api.</span></p><p>it is really smart! i didnt believe the win rates relative to o3 the first time i saw them.</p></blockquote><p>Arena has gotten quite silly if treated as a comprehensive measure (as in Gemini 2.5 Flash is rated above o3), but as a quick heuristic, if we take a 64% win rate seriously, that would by the math put o3-pro ~100 above o3 at 1509 on Arena, crushing Gemini-2.5-Pro for the #1 spot. I would assume that most pairwise comparisons would have a less impressive jump, since o3-pro is essentially offering the same product as o3 only somewhat better, which means the result will be a lot less noisy than if it was up against Gemini. </p><p>So this both is a very impressive statistic and also doesn’t mean much of anything.</p><p><a href="https://x.com/nearcyan/status/1932612410843815979" rel="">The problem with o3-pro is that it is slow</a><span>. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg" width="302" height="379.312" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:628,&quot;width&quot;:500,&quot;resizeWidth&quot;:302,&quot;bytes&quot;:135570,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://thezvi.substack.com/i/165878744?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ab7635-4e1c-4438-a30a-0efb1db71fcd_500x628.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><blockquote><p>Nearcyan: one funny note is that minor UX differences in how you display 'thinking'/loading/etc can easily move products from the bottom half of this meme to the top half. </p><p>Another note is anyone I know who is the guy in the bottom left is always extremely smart and a pleasure to speak with.</p><p>the real problem is I may be closer to the top right than the bottom left</p></blockquote><p>Today I had my first instance of noticing I’d gotten a text (during the night, in this case) and they got a response 20 minutes slower than they would have otherwise because I waited for o3-pro to give its answer to the question I’d been asked. </p><p><span>Thus, even with access to o3-pro at zero marginal compute cost, </span><a href="https://x.com/TheZvi/status/1933563490117300617" rel="">almost half of people reported they rarely use it for a given query, and only about a quarter said they usually use it</a><span>. </span></p><p><span>It is also super frustrating to run into errors when you are waiting 15+ minutes for a response, and </span><a href="https://x.com/TheZvi/status/1933563491819868288" rel="">reports of such errors were common</a><span> which matches my experience. </span></p><blockquote><p><a href="https://x.com/bindureddy/status/1932889892562088086" rel="">Bindu Reddy</a><span>: o3-Pro Is Not Very Good At Agentic Coding And Doesn't Score Higher Than o3 😿</span></p><p>After a lot of waiting and numerous retries, we have finally deployed o3-pro on LiveBench AI.</p><p>Sadly, the overall score doesn't improve over o3 🤷‍♂️</p><p>Mainly because it's not very agentic and isn't very good at tool use... it scores way below o3 on the agentic-coding category.</p><p>The big story yesterday was not o3-pro but the price decrease in o3!!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg" width="900" height="569" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:569,&quot;width&quot;:900,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:57576,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://thezvi.substack.com/i/165878744?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659752b0-22a3-4161-8aa8-b949acece1d0_900x569.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Dominik Lukes: I think this take by @bindureddy very much matches the vibes I'm getting: it does not "feel" very agentic and as ready to reach for the right tools as o3 is - but it could just be because o3 keeps you informed about what it's doing in the CoT trace.</p></blockquote><p>I certainly would try o3-pro in cases where o3 was failing, if I’d already also tried Opus and Gemini first. I wonder if that agentic coding score drop actually represent an issue here, where because it is for the purpose of reasoning longer and they don’t want it endlessly web searching o3-pro is not properly inclined to exploit tools?</p><p><a href="https://x.com/dioscuri/status/1932934997901783266" rel="">o3-pro gets 8.5/10 on BaldurBench</a><span>, which is about creating detailed build guides for rapidly changing video games. Somewhat subjective but should still work.</span></p><blockquote><p><a href="https://x.com/_l_zahir/status/1932924771668566049" rel="">L Zahir</a><span>: bombs all my secret benchmarks, no better than o3.</span></p></blockquote><p><a href="https://x.com/LechMazur/status/1932897888650072275" rel="">Lech Mazur gives us four of his benchmarks</a><span>: A small improvement over o3 for Creative Writing Benchmark, a substantial boost from 79.5% (o3) or 82.5% (o1-pro) to 87.3% on Word Connections, no improvement on Thematic Generalization, very little improvement on Confabulations (avoiding hallucinations). The last one seems the most important to note. </span></p><p>Tyler Cowen was very positive, he seems like the perfect customer for o3-pro? By which I mean he can context shift easily so he doesn’t mind waiting, and also often uses queries where these models get a lot of value out of going at problems super hard, and relatively less value out of the advantages of other models (doesn’t want the personality, doesn’t want to code, and so on). </p><blockquote><p><a href="https://marginalrevolution.com/marginalrevolution/2025/06/o3-pro.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=o3-pro" rel="">Tyler Cowen</a><span>: It is very, very good. Hallucinates far less than other models. Can solve economics problems that o3 cannot. It can be slow, but that is what we have Twitter scrolling for, right? While we are waiting for o3 pro to answer a query we can </span><a href="https://x.com/polynoamial/status/1932532770594857350" rel="">read about</a><span>…</span><a href="https://x.com/WesRothMoney/status/1932546133982945501" rel="">o3 pro</a><span>.</span></p></blockquote><p>Contrast that with the score on Confabulations not changing. I am guessing there is a modest improvement, for reasons described earlier. </p><p><span>There are a number of people pointing out places o3-pro solves something o3 doesn’t, such has here </span><a href="https://x.com/0xSMW/status/1932803015750398064" rel="">it solved the gimbal uap mystery in 18 minutes</a><span>. </span></p><p>McKay Wrigley, eternal optimist, agrees on many fronts.</p><blockquote><p><a href="https://x.com/mckaywrigley/status/1932607750649049571" rel="">McKay Wrigley</a><span>: My last 4 o3 Pro requests in ChatGPT… It thought for: - 26m 10s - 23m 45s - 19m 6s - 21m 18s Absolute *powerhouse* of a model.</span></p><p>Testing how well it can 1-shot complex problems - impressed so far.</p><p>It’s too slow to use as a daily driver model (makes sense, it’s a beast!), but it’s a great “escalate this issue” model. If the current model you’re using is struggling with a task, then escalate it to o3 pro.</p><p>This is not a “vibe code” model.</p><p>This is the kind of model where you’ll want to see how useful it is to people like Terence Tao and Tyler Cowen.</p><p>Btw the point of this post was that I’m happy to have a model that is allowed to think for a long time.</p><p>To me that’s the entire point of having a “Pro” version of the model - let it think!</p><p>Obviously more goes into evaluating if it’s a great model (imo it’s really powerful).</p></blockquote><p>Here’s a different kind of vibe coding, perhaps?</p><blockquote><p><a href="https://x.com/lisperati/status/1932926392452460709" rel="">Conrad Barski</a><span>: For programming tasks, I can give o3 pro some code that needs a significant revision, then ramble on and on about what the various attributes of the revision need to be and then it can reliably generate an implementation of the revision.</span></p><p>It feels like with previous models I had to give them more hand holding to get good results, I had to write my requests in a more thoughtful, structured way, spending more time on prompting technique.</p><p>o3 pro, on the other hand, can take loosely-connected constraints and then "fill in the gaps" in a relatively intelligent way- I feel it does this better than any other model so far.</p></blockquote><p>The time cost and dollar costs are very real. </p><blockquote><p><a href="https://x.com/mattshumer_/status/1932592146030178478" rel="">Matt Shumer</a><span>: My initial take on o3 Pro:</span></p><p>It is not a daily-driver coding model.</p><p>It's a superhuman researcher + structured thinker, capable of taking in massive amounts of data and uncovering insights you would probably miss on your own.</p><p>Use it accordingly. </p><p>I reserve the right to alter my take.</p><p>Bayram Annokov: slow, expensive, and veeeery good - definitely a jump up in analytical tasks</p><p>Emad: 20 o3 prompts &gt; o3 pro except for some really advanced specific stuff I have found Only use it as a final check really or when stumped.</p><p>Eyes Alight: it is so very slow it took 13 minutes to answer a trivial question about a post on Twitter. I understand the appeal intellectually of an Einstein at 1/20th speed, but in reality I'm not sure I have the patience for it.</p><p><a href="https://x.com/ClayCampaigne/status/1933236232479518814" rel="">Clay</a><span>: o3-pro achieving breakthrough performance in taking a long time to think.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg" width="351" height="445.5571227080395" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:900,&quot;width&quot;:709,&quot;resizeWidth&quot;:351,&quot;bytes&quot;:57616,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://thezvi.substack.com/i/165878744?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ae776b7-587d-4aec-908c-a1688990ac9d_709x900.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://x.com/techczech/status/1932705769809461282" rel="">Dominik Lukes</a><span>: Here's my o3 Pro testing results thread. Preliminary conclusions:</span></p><p>- great at analysis</p><p>- slow and overthinking simple problems</p><p>- o3 is enough for most tasks</p><p>- still fails SVG bike and local LLM research test</p><p>- very few people need it</p><p>- it will take time to develop a feel for it</p><p>Kostya Medvedovsky: For a lot of problems, it reminds me very strongly of Deep Research. Takes about the same amount of time, and will spend a lot of effort scouring the web for the answer to the question.</p><p>Makes me wish I could optionally turn off web access and get it to focus more on the reasoning aspect.</p><p>This may be user error and I should be giving it  *way* more context.</p><p>Violet: you can turn search off, and only turn search on for specific prompts.</p><p>Xeophon: TL;DR:</p><p>o3 pro is another step up, but for going deep, not wide. It is good to go down one path, solve one problem; not for getting a broad overview about different topics/papers etc. Then it hallucinates badly, use ODR for this.</p></blockquote><p>Part of ‘I am very intelligent’ is knowing when to think for longer and when not to. In that sense, o3-pro is not so smart, you have to take care of that question yourself. I do understand why this decision was made, let the user control that. </p><p>I agree with Lukes that most people do not ‘need’ o3 pro and they will be fine not paying for it, and for now they are better off with their expensive subscription (if any) being Claude Max. But even if you don’t need it, the queries you benefit from can still be highly useful.</p><p>It makes sense to default to using Opus and o3 pro (and for quick stuff Sonnet) </p><p>o3-pro is too slow to be a good ‘default’ model, especially for coding. I don’t want to have to reload my state in 15 minute intervals. It may or may not be good for the ‘call in the big guns’ role in coding, where you have a problem that Opus and Gemini (and perhaps regular o3) have failed to solve, but which you think o3-pro might get. </p><p>Here’s one that both seems central wrong but also makes an important point:</p><blockquote><p>Nabeel Qureshi: You need to think pretty hard to get a set of evals which allows you to even distinguish between o3 and o3 pro. </p><p>Implication: "good enough AGI" is already here.</p></blockquote><p>The obvious evals where it does better are Codeforces, and also ‘user preferences.’ Tyler Cowen’s statement suggests hallucination rate, which is huge if true (and it better be true, I’m not waiting 20 minutes that often to get an o3-level lying liar.) Tyler also reports there are questions where o3 fails and o3-pro succeeds, which is definitive if the gap is only one way. And of course if all else fails you can always have them do things like play board games against each other, as one answer suggests. </p><p>Nor do I think either o3 or o3-pro is the AGI you are looking for.</p><p>However, it is true that for a large percentage of tasks, o3 is ‘good enough.’ That’s even true in a strict sense for Claude Sonnet or even Gemini Flash. Most of the time one has a query, the amount of actually needed intelligence is small. </p><blockquote><p>In the limit, we'll have to rely on AIs to tell us which AI model is smarter, because we won't be smart enough to tell the difference. What a weird future.</p><p>(Incidentally, this has already been the case in chess for years. Humans cannot tell the difference between a 3300 elo and a 3600 elo chess engine; we just make them fight it out and count the number of wins.)</p></blockquote><p>You can tell 3300 from 3600 in chess, but only because you can tell who won. If almost any human looked at individual moves, you’d have very little idea.</p><p>I always appreciate people thinking at the limit rather than only on the margin. This is a central case of that. </p><p>Here’s one report that it’s doing well on the fully informal FictionBench:</p><blockquote><p><a href="https://x.com/chatgpt21/status/1933050795240415449" rel="">Chris</a><span>: Going to bed now, but had to share something crazy: been testing the o3 pro model, and honestly, the writing capabilities are astounding. Even with simple prompts, it crafts medium to long-form stories that make me deeply invested &amp; are engaging they come with surprising twists, and each one carries this profound, meaningful depth that feels genuinely human. </span></p><p>The creativity behind these narratives is wild far beyond what I’d expect from most writers today. We’re talking sophisticated character development, nuanced plot arcs, and emotional resonance, all generated seamlessly. It’s genuinely hard to believe this is early-stage reinforcement learning with compute added at test time; the potential here is mind blowing. We’re witnessing just the beginning of AI enhanced storytelling, and already it’s surpassing what many humans can create. Excited to see what’s next with o4 Goodnight!</p></blockquote><p>This contrasts with:</p><blockquote><p>Archivedvideos: Really like it for technical stuff, soulless</p><p><a href="https://x.com/JuliusSimonelli/status/1932944540517945732" rel="">Julius</a><span>: I asked it to edit an essay and it took 13 minutes and provided mediocre results. Different from but slightly below the quality of 4o. Much worse than o3 or either Claude 4 model</span></p></blockquote><p><span>Other positive reactions include </span><a href="https://x.com/mlwigdahl/status/1933524889085866268" rel="">Matt Wigdahl being impressed</a><span> on a hairy RDP-related problem, </span><a href="https://x.com/a66mike99/status/1933140863103959206" rel="">a66mike99 getting interesting output and pushback on the request</a><span> (in general I like this, although if you’re thinking for 20 minutes this could be a lot more frustrating?), </span><a href="https://x.com/niplav_site/status/1932901796994781356" rel="">niplav being impressed by results on a second attempt after Claude crafted a better prompt</a><span> (this seems like an excellent workflow!), and </span><a href="https://x.com/Sithis3/status/1933565284096315475" rel="">Sithis3 saying o3-pro solves many problems o3 struggles on</a><span>. </span></p><p><a href="https://x.com/erikphoel/status/1932805295773753551" rel="">The obvious counterpoint is some people didn’t get good responses</a><span>, and saw it repeating the flaws in o3.</span></p><blockquote><p>Erik Hoel: First o3 pro usage. Many mistakes. Massive overconfidence. Clear inability to distinguish citations, pay attention to dates. Does anyone else actually use these models? They may be smarter on paper but they are increasingly lazy and evil in practice.</p><p>Kukutz: very very very slow, not so clever (can't solve my semantic puzzle).</p><p><a href="https://x.com/HoskinsAllen/status/1933160300808970481" rel="">Allen</a><span>: I think it’s less of an upgrade compared to base model than o1-pro was. Its general quality is better on avg but doesn’t seem to hit “next-level” on any marks. Usually mentions the same things as o3.</span></p><p>I think OAI are focused on delivering GPT-5 more than anything.</p></blockquote><p><a href="https://x.com/TheXeophon/status/1933451968262905864" rel="">This thread from Xeophon features reactions that are mixed but mostly meh</a><span>. </span></p><p>Or to some it simply doesn’t feel like much of a change at all.</p><blockquote><p><a href="https://x.com/TheZvi/status/1932895110745993637" rel="">Nikita Sokolsky</a><span>: Feels like o3’s outputs after you fix the grammar and writing in Claude/Gemini: it writes less concisely but haven’t seen any “next level” prompt responses just yet.</span></p><p>MartinDeVido: Meh....</p></blockquote><p>Here’s a fun reminder that details can matter a lot:</p><blockquote><p><a href="https://x.com/jjhughes/status/1932975519147896992" rel="">John Hughes</a><span>: I was thrilled yesterday: o3-pro was accepting ~150k tokens of context (similar to Opus), a big step up from regular o3, which allows only a third as much in ChatGPT. @openai seems to have changed that today. Queries I could do yesterday are now rejected as too long.</span></p><p>With such a low context limit, o3-pro is much less useful to lawyers than o1-pro was. Regular o3 is great for quick questions/mini-research, but Gemini is better at analyzing long docs and Opus is tops for coding. Not yet seeing answers where o3-pro is noticeably better than o3.</p></blockquote><p>I presume that even at $200/month, the compute costs of letting o3-pro have 150k input tokens would add up fast, if people actually used it a lot. </p><p><a href="https://x.com/jerryjliu0/status/1934043120033010085" rel="">This is one of the things</a><span> I’ve loved the most so far about o3-pro. </span></p><blockquote><p>Jerry Liu: o3-pro is extremely good at reasoning, extremely slow, and extremely concise - a top-notch consultant that will take a few minutes to think, and output bullet points. </p><p>Do not ask it to write essays for you.</p></blockquote><p>o3-pro will make you wait, but its answer will not waste your time. This is a sharp contrast to Deep Research queries, which will take forever to generate and then include a ton of slop.</p><p>It is not the main point but I must note the absence of a system card update. When you are releasing what is likely the most powerful model out there, o3-pro, was everything you needed to say truly already addressed by the model card for o3? </p><blockquote><p><span>OpenAI: As o3-pro uses the same underlying model as o3, </span><a href="https://t.co/iqXaDSAtQq" rel="">full safety details can be found in the o3 system card</a><span>.</span></p><p><a href="https://x.com/Miles_Brundage/status/1932531528086798370" rel="">Miles Brundage</a><span>: This last sentence seems false? </span></p><p>The system card does not appear to have been updated even to incorporate the information in this thread.</p><p>The whole point of the term system card is that the model isn’t the only thing that matters.</p><p>If they didn’t do a full Preparedness Framework assessment, e.g. because the evals weren’t too different and they didn’t consider it a good use of time given other coming launches, they should just say that, I think.</p><p>If o3-pro were the max capability level, I wouldn't be super concerned about this, and I actually suspect it is the same Preparedness Framework level as o3. </p><p>The problem is that this is not the last launch, and lax processes/corner-cutting/groupthink get more dangerous each day.</p><p>As OpenAI put it, ‘there’s no such thing as a small launch.’</p></blockquote><p><a href="https://help.openai.com/en/articles/9624314-model-release-notes" rel="">The link they provide</a><span> goes to ‘Model Release Notes,’ which is not quite nothing, but it isn’t much and does not include a Preparedness Framework evaluation. </span></p><p>I agree with Miles that if you don’t want to provide a system card for o3-pro that This Is Fine, but you need to state your case for why you don’t need one. This can be any of:</p><ol><li><p>The old system card tested for what happens at higher inference costs (as it should!) so we effectively were testing o3-pro the whole time, and we’re fine.</p></li><li><p>The Preparedness team tested o3-pro and found it not appreciably different from o3 in the ways we care about, providing no substantial additional uplift or other concerns, despite looking impressive in some other ways.</p></li><li><p>This is only available at the $200 level so not a release of o3-pro so it doesn’t count (I don’t actually think this is okay, but it would be consistent with previous decisions I also think aren’t okay, and not an additional issue.) </p></li></ol><p>As far as I can tell we’re basically in scenario #2, and they see no serious issues here. Which again is fine if true, and if they actually tell us that this is the case. But the framework is full of ‘here are the test results’ and presumably those results are different now. I want o3-pro on those charts.</p><p>What about alignment otherwise? Hard to say. I did notice this (but did not attempt to make heads or tails of the linked thread), seems like what you would naively expect:</p><blockquote><p><a href="https://x.com/YeshuaGod22/status/1933395147934331140" rel="">Yeshua God</a><span>: </span><a href="https://t.co/KI8CPUGPd6" rel="">Following the mesa-optimiser recipe to the letter.</a><span> @aidan_mclau very troubling.</span></p></blockquote><p>For many purposes, the 80% price cut in o3 seems more impactful than o3-pro. That’s a huge price cut, whereas o3-pro is still largely a ‘special cases only’ model.</p><blockquote><p>Aaron Levie: With OpenAI dropping the price of o3 by 80%, today is a great reminder about how important it is to build for where AI is going instead of just what's possible now. You can now get 5X the amount of output today for the same price you were paying yesterday.</p><p>If you’re building AI Agents, it means it's far better to build capabilities that are priced and designed for the future instead of just economically reasonable today. </p><p>In general, we know there's a tight correlation between the amount of compute spent on a problem and the level of successful outcomes we can get from AI. This is especially true with AI Agents that potentially can burn through hundreds of thousands or millions of tokens on a single task. </p><p>You're always making trade-off decisions when building AI Agents around what level of accuracy or success you want and how much you want to spend: do you want to spend $0.10 for something to be 95% successful or $1 for something to be 99% successful? A 10X increase in cost for just a 4 pt improvement in results? At every price:success intersection a new set of use-cases from customers can be unlocked.</p><p>Normally when building technology that moves at a typical pace, you would primarily build features that are economically viable today (or with some slight efficiency gains anticipated at the rate of Moore's Law, for instance). You'd be out of business otherwise. But with the cost of AI inference dropping rapidly, the calculus completely changes. In a world where the cost of inference could drop by orders of magnitude in a year or two, it means the way we build software to anticipate these cost drops changes meaningfully.</p><p>Instead of either building in lots of hacks to reduce costs, or going after only the most economically feasible use-cases today, this instructs you to build the more ambitious AI Agent capabilities that would normally seem too cost prohibitive to go after. Huge implications for how we build AI Agents and the kind of problems to go after.</p></blockquote><p>I would say the cost of inference not only might drop an order of magnitude in a year or two, if you hold quality of outputs constant it is all but certain to happen at least one more time. Where you ‘take your profits’ in quality versus quantity is up to you.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
    </channel>
</rss>