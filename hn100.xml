(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 13 Jun 2024 22:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Taking away iPhone made daughter a better person (101 pts)]]></title>
            <link>https://www.theguardian.com/technology/article/2024/jun/13/kids-no-iphone-screen-time</link>
            <guid>40674073</guid>
            <pubDate>Thu, 13 Jun 2024 20:07:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/article/2024/jun/13/kids-no-iphone-screen-time">https://www.theguardian.com/technology/article/2024/jun/13/kids-no-iphone-screen-time</a>, See on <a href="https://news.ycombinator.com/item?id=40674073">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><em>The byline on this essay is a pseudonym.</em></p><p>My daughter is one of those kids the US surgeon general warned us about. Our nation’s children are “unknowing participants” in a “decades-long experiment”. <a href="https://www.theguardian.com/media/social-media" data-link-name="in body link" data-component="auto-linked-tag">Social media</a> usage poses mental health risks to youth, who use it “almost constantly”, causing sleep deprivation, depression and anxiety.</p><p>Before sixth grade, my daughter saved her dog-walking money for a phone. She found a used <a href="https://www.theguardian.com/technology/iphone" data-link-name="in body link" data-component="auto-linked-tag">iPhone</a> 13 Mini on Craigslist. I set expectations to incentivize getting good grades, keeping her room clean and taking out the trash. Little did I know that the iPhone would systematically undermine her ability to complete these tasks – and so much more.</p><p>When my daughter walked into class through an inflatable arch on her first day of middle school, I took comfort in the fact that I could reach her. Like most parents, I associated the phone with safety, not danger. I didn’t know that social media developers were manipulating her next swipe, or that her “human future” was being sold to the highest bidder, enriching the wealthiest corporations in human history.</p><p>I learned the hard way – through my daughter’s lies, manipulation, slipping grades. Through the “zebra-stripe” scars sketched across her arms.</p><p>Her sixth-grade school picture captures my daughter’s “emo” phase: the feather earring, Pink Floyd T-shirt and crooked smile. The innocence in that image was quickly replaced by the selfie. Peace-signs-over-puckered-lips selfie. Head tilt, half-face, full-body selfie. The selfie in bed. Her camera roll documents my child’s downward spiral. Crying selfies, puffy-eyed selfies, unable-to-leave-the-bedroom selfies.</p><p>By spring semester, my daughter was performing poorly in school. I took her for a psychiatric evaluation, assuming she had ADHD. Afternoon sun filtered through faux wood blinds, casting light strips across her ever-present black hoodie. The doctor’s questions started predictably. Trouble focusing in class? Completing homework? Sleeping? Then the interview took a turn for the dreadful. Do you feel your life isn’t worth living? Have you ever harmed yourself? Do you wish you were dead?</p><p>I gaped at my child’s profile, each “yes”<em> </em>lacerating my guts.</p><p>The doctor diagnosed my daughter with depression and anxiety. Further testing showed that gaining her friends’ approval occupied 80% of her attention. No wonder she was failing math. It was a miracle she was passing any of her classes at all with only 20% of her brain available for school.</p><p>The doctor prescribed therapy and Lexapro. While these were helpful, the doctor failed to alert me to the sweeping phone trends among middle school students. I have since learned that my daughter is among the first generation of 10-to-14-year-olds active on social media. For these girls, suicide rates have risen 151%, self-harm 182%. Her treatment assumed her struggles were individual, as opposed to structural. In our country, we prescribe drugs to solve this social crisis.</p><p>Ignorant of these dynamics at the time, I allowed my daughter to continue her social media use. One day, I received a text from another mom. I stared at the screen, wondering why this mom had sent me a graphic selfie. Then I recognized the mole on the woman’s chest. My kid’s mole.</p><p>My daughter gasped when I showed her the photo. She handed over her phone. I discovered she’d circumvented the screen limits and had been using social media into the wee hours of the morning. She’d sent the image to someone named PJ on Snapchat. He claimed to be a 16-year-old boy, but his response was so graphic I suspected someone older. The phone was a two-way street, I realized with dread, with platforms adults could use to kidnap and traffic our children.</p><p>I called a family meeting with my daughter, her dad and her stepmom. My daughter would delete her social media accounts and give up her phone until the start of the school year. As the summer months passed with travels, in-person hangouts and family time, my daughter returned to herself. The dark circles under her eyes faded. The sighs, shrugs and eyerolls stopped. She got up in the morning. She laughed. She even let me hug her, sometimes.</p><p>It was hard to give her phone back before seventh grade, but we had a deal. I wanted to reinforce her good behavior. I made new rules: no social media, no devices in bedrooms, phones off at 8pm. We charged our phones on the kitchen counter. I bought alarm clocks and sound machines. We endured digital detox. My daughter started soccer. My insomnia resolved. We joined a gym and worked out together.</p><figure id="7734e1f6-4003-48e8-9a03-f0605c74be21" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:15,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;‘They give us liberty with less anxiety’: A teenager, a parent and a teacher on smartphones for under-14s | panel&quot;,&quot;elementId&quot;:&quot;7734e1f6-4003-48e8-9a03-f0605c74be21&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/commentisfree/article/2024/jun/08/smartphones-under-14s-st-albans-children&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:2,&quot;theme&quot;:1,&quot;design&quot;:8}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false,&quot;updateLogoAdPartnerSwitch&quot;:true,&quot;assetOrigin&quot;:&quot;https://assets.guim.co.uk/&quot;}"></gu-island></figure><p>But within a few months, my daughter relapsed. Little lies. Big lies. Another text came from a friend’s mom with selfies of our daughters vaping and hanging out with boys I’d never met at the mall. We held another family meeting.</p><p>“This may sound crazy,” my daughter’s stepmom said. “But maybe she doesn’t need a phone.”</p><p>The words rippled across my mind. How had I never thought of it? The phone was destroying my daughter, but I couldn’t imagine life without it. I’d remained loyal to the idea of it, the ideal of it. I took custody of the phone again.</p><p>My daughter threw a tantrum when I told her she’d lost her phone until high school. She didn’t want to be <em>that </em>kid, the only one in class without her phone. But as the tantrum subsided, she began to return to herself. Then, within a few weeks, signs of her addictive behavior began to resurface.</p><p>I found iPhone chargers in the outlets by her bed – for charging her AirPods, she said. She threw her body on the ground to stop me from searching beneath her bed. One night, as I lay in bed ruminating, it hit me. I remembered my daughter had <em>two</em> phones. When I’d accidentally broken the Mini in a weight machine during our workout, I bought her a new iPhone 13. I’d seized the 13, but she could still have the Mini.</p><p>“I sold it to a friend at school,” my daughter said when I asked her the next morning. She couldn’t say to whom, or for how much.</p><p>“I’ll find it,” I said with an <em>I-see-you</em> gesture. I was frantic, but displayed calm confidence, even a little humor, as I searched her backpack and drawers, patted down her pockets, entered her room unannounced, trying to catch her in the act. My daughter remained calm throughout my searches. I began to think I’d gone completely mad. I bought a metal detector.</p><p>Then one evening, I came into her room. My daughter bolted upright and shuffled her comforter. I rushed to the bed, ran my hands under the covers. A charging cord! My fingers traced its length to the attached phone.</p><p>We stared at the Mini lying in my hands. The Snapchat app glowed beneath the shattered screen. She looked at me. Her eyes went wide, then filled with tears.</p><p>That night, my heartbeat tapped wildly against my pillow as I scrolled her social media. Her exchanges were desperate with need. She pleaded with people to reply, especially a boy named Damien. When he didn’t respond, she said she was depressed, sexted, sent a boob pic.</p><p>I found answers via my sister in Johann Hari’s book Stolen Focus, which explores how and why our attention is collapsing: “The phones we have, and the programs that run on them, were deliberately designed by the smartest people in the world to maximally grab and maximally hold our attention.” Of course. At such a young age, my daughter was defenseless against this manipulation. She assessed her worth within a system where she was simultaneously attention-addicted and attention-starved. She’d internalized an algorithm where provocative content wins: “If it’s more enraging, it’s more engaging,” Hari writes.</p><p>The social experiment in our house is being replicated in homes across the country. As parents, we want to keep our kids safe. We want them to call us if an active shooter comes to campus. But the greatest danger lies <em>within </em>the phone, not outside if it.</p><p>One reason our kids are so addicted to their phones is because we’re addicted to ours. My friends complain of insomnia, but can’t imagine leaving their phones outside the bedroom. Addressing my child’s phone use has meant addressing my own. I have to restrain myself from texting while driving. I’ve stopped rushing to the charge station every morning to see if I missed a message.</p><p>As seventh grade ends, my daughter is <em>that </em>kid. Without her phone, she’s the kid who dribbles her soccer ball across the living room, rides her skateboard down the street, makes the honor roll, joins the track team. She’s the kid whose hands gesture wildly as she chats with her friends, who plaits her hair and falls asleep reading a book.</p><p>These days, we use my phone together to coordinate hangouts, listen to audiobooks, sing along to her songs and mine – Shakira and Sade, Ice Cube and the Fugees. Last weekend, we drove down the Pacific Coast Highway to visit family. June gloom hugged the coastline as my daughter and I bodysurfed a glassy wave that rushed us to the shore. “Again!” she said, leaping to her feet. She’s addicted to the feeling of the water rolling beneath her belly.</p><p>My daughter’s not the only kid. I recently met a woman who seized her 11-year-old son’s phone when she discovered he was sexting. Students at the Illing middle school in Connecticut build community and pay attention in class now that the school makes them put their phones in rubber pouches – a trend that’s quickly spreading. British children are largely learning in “a mobile phone-free environment” since a department of education mandate.</p><p>We need both individual <em>and </em>systemic changes to check our phone use. I’m curious where these changes will take us by the time my daughter enters high school.</p><p>Until then, I’ll hold the phone.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S.-Saudi petrodollar pact ends after 50 years (143 pts)]]></title>
            <link>https://www.nasdaq.com/articles/us-saudi-petrodollar-pact-ends-after-50-years</link>
            <guid>40673567</guid>
            <pubDate>Thu, 13 Jun 2024 19:28:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nasdaq.com/articles/us-saudi-petrodollar-pact-ends-after-50-years">https://www.nasdaq.com/articles/us-saudi-petrodollar-pact-ends-after-50-years</a>, See on <a href="https://news.ycombinator.com/item?id=40673567">Hacker News</a></p>
Couldn't get https://www.nasdaq.com/articles/us-saudi-petrodollar-pact-ends-after-50-years: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Mouth-based touchpad enables people living with paralysis to use computers (103 pts)]]></title>
            <link>https://news.mit.edu/2024/mouth-based-touchpad-augmental-0605</link>
            <guid>40673091</guid>
            <pubDate>Thu, 13 Jun 2024 18:43:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.mit.edu/2024/mouth-based-touchpad-augmental-0605">https://news.mit.edu/2024/mouth-based-touchpad-augmental-0605</a>, See on <a href="https://news.ycombinator.com/item?id=40673091">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          

            <p>When Tomás Vega SM&nbsp;’19 was 5 years old, he began to stutter. The experience gave him an appreciation for the adversity that can come with a disability. It also showed him the power of technology.</p><p>“A keyboard and a mouse were outlets,” Vega says. “They allowed me to be fluent in the things I did. I was able to transcend my limitations in a way, so I became obsessed with human augmentation and with the concept of cyborgs. I also gained empathy. I think we all have empathy, but we apply it according to our own experiences.”</p><p>Vega has been using technology to augment human capabilities ever since. He began programming when he was 12. In high school, he helped people manage disabilities including hand impairments and multiple sclerosis. In college, first at the University of California at Berkeley and then at MIT, Vega built technologies that helped people with disabilities live more independently.</p><p>Today Vega is the co-founder and CEO of Augmental, a startup deploying technology that lets people with movement impairments seamlessly interact with their personal computational devices.</p><p>Augmental’s first product is the MouthPad, which allows users to control their computer, smartphone, or tablet through tongue and head movements. The MouthPad’s pressure-sensitive touch pad sits on the roof of the mouth, and, working with a pair of motion sensors, translates tongue and head gestures into cursor scrolling and clicks in real time via Bluetooth.</p><p>“We have a big chunk of the brain that is devoted to controlling the position of the tongue,” Vega explains. “The tongue comprises eight muscles, and most of the muscle fibers are slow-twitch, which means they don’t fatigue as quickly. So, I thought why don’t we leverage all of that?”</p><p>People with spinal cord injuries are already using the MouthPad every day to interact with their favorite devices independently. <a href="https://www.youtube.com/watch?v=d9U8BaNx3ZM">One of Augmental’s users</a>, who is living with quadriplegia and studying math and computer science in college, says the device has helped her write math formulas and study in the library — use cases where other assistive speech-based devices weren’t appropriate.</p><p>“She can now take notes in class, she can play games with her friends,” Vega says. “She is more independent. Her mom told us that getting the MouthPad was the most significant moment since her injury.”</p><p>That’s the ultimate goal of Augmental: to improve the accessibility of technologies that have become an integral part of our lives.</p><p>“We hope that a person with a severe hand impairment can be as competent using a phone or tablet as somebody using their hands,” Vega says.</p>        

      </div><div>
          

            <p><strong>Making computers more accessible</strong></p><p>In 2012, as a first-year student at UC Berkeley, Vega met his eventual Augmental co-founder, Corten Singer. That year, he told Singer he was determined to join the Media Lab as a graduate student, something he achieved four years later when he joined the Media Lab’s Fluid Interfaces research group run by Pattie Maes, MIT’s Germeshausen Professor of Media Arts and Sciences.</p><p>“I only applied to one program for grad school, and that was the Media Lab,” Vega says. “I thought it was the only place where I could do what I wanted to do, which is augmenting human ability.”</p><p>At the Media Lab, Vega took classes in microfabrication, signal processing, and electronics. He also developed wearable devices to help people access information online, improve their sleep, and regulate their emotions.</p><p>“At the Media Lab, I was able to apply my engineering and neuroscience background to build stuff, which is what I love doing the most,” Vega says. “I describe the Media Lab as Disneyland for makers. I was able to just play, and to explore without fear.”</p><p>Vega had gravitated toward the idea of a brain-machine interface, but an internship at Neuralink made him seek out a different solution.</p><p>“A brain implant has the highest potential for helping people in the future, but I saw a number of limitations that pushed me from working on it right now,” Vega says. “One is the long timeline for development. I’ve made so many friends over the past years that needed a solution yesterday.”</p><p>At MIT, he decided to build a solution with all the potential of a brain implant but without the limitations.</p><p>In his last semester at MIT, Vega built what he describes as&nbsp;“a lollipop with a bunch of sensors” to test the mouth as a medium for computer interaction. It worked beautifully.</p><p>“At that point, I called Corten, my co-founder, and said, ‘I think this has the potential to change so many lives,’” Vega says.&nbsp;“It could also change the way humans interact with computers in the future.”</p><p>Vega used MIT resources including the&nbsp;<a href="https://vms.mit.edu/" target="_blank">Venture Mentoring Service</a>, the&nbsp;<a href="https://icorps.mit.edu/" target="_blank">MIT I-Corps program</a>, and received crucial early funding from MIT’s&nbsp;<a href="https://innovation.mit.edu/resource/e14-fund/" target="_blank">E14 Fund</a>. Augmental was officially born when Vega graduated from MIT at the end of 2019.</p><p>Augmental generates each MouthPad design using a 3D model based on a scan of the user’s mouth. The team then 3-D prints the retainer using dental-grade materials and adds the electronic components.</p><p>With the MouthPad, users can scroll up, down, left, and right by sliding their tongue. They can also right click by doing a sipping gesture and left click by pressing on their palate. For people with less control of their tongue, bites, clenches, and other gestures can be used, and people with more neck control can use head-tracking to move the cursor on their screen.</p><p>“Our hope is to create an interface that is multimodal, so you can choose what works for you,” Vega says. “We want to be accommodating to every condition.”</p><p><strong>Scaling the MouthPad</strong></p><p>Many of Augmental’s current users have spinal cord injuries, with some users unable to move their hands and others unable to move their heads. Gamers and programmers have also used the device. The company’s most frequent users interact with the MouthPad every day for up to nine hours.</p><p>“It’s amazing because it means that it has really seamlessly integrated into their lives, and they are finding lots of value in our solution,” Vega says.</p><p>Augmental is hoping to gain U.S. Food and Drug Administration clearance over the next year to help users do things like control wheelchairs and robotic arms. FDA clearance will also unlock insurance reimbursements for users, which will make the product more accessible.</p><p>Augmental is already working on the next version of its system, which will respond to whispers and even more subtle movements of internal speech organs.</p><p>“That’s crucial to our early customer segment because a lot of them have lost or have impaired lung function,” Vega says.</p><p>Vega is also encouraged by progress in AI agents and the hardware that goes with them. No matter how the digital world evolves, Vega believes Augmental can be a tool that can benefit everyone.</p><p>“What we hope to provide one day is an always-available, robust, and private interface to intelligence,” Vega says. “We think that this is the most expressive, wearable, hands-free input system that humans have created.”</p>        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You can help Anna's Archive by seeding torrents (159 pts)]]></title>
            <link>https://annas-archive.org/torrents</link>
            <guid>40672215</guid>
            <pubDate>Thu, 13 Jun 2024 17:20:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://annas-archive.org/torrents">https://annas-archive.org/torrents</a>, See on <a href="https://news.ycombinator.com/item?id=40672215">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en">
      

      <p>
        This torrent list is the “ultimate unified list” of releases by Anna’s Archive, Library Genesis, Sci-Hub, and others. By seeding these torrents, you help preserve humanity’s knowledge and culture. These torrents represent the vast majority of human knowledge that can be mirrored in bulk.
      </p>

      <p>
        These torrents are not meant for downloading individual books. They are meant for long-term preservation. With these torrents you can set up a full mirror of Anna’s Archive, using our <a href="https://annas-software.org/AnnaArchivist/annas-archive">source code</a> and metadata (which can be <a href="https://annas-software.org/AnnaArchivist/annas-archive/-/blob/main/data-imports/README.md">generated</a> or <a href="https://annas-archive.org/torrents#aa_derived_mirror_metadata">downloaded</a> as ElasticSearch and MariaDB databases). We also have full lists of torrents, as <a href="https://annas-archive.org/dyn/torrents.json">JSON</a>.
      </p>

      <p>
        Currently <strong>60%</strong> of the total <strong>521.1TB</strong> is copied in at least 4 locations, and only 3% in more than 10 locations. We need your help to get to 100%!
      </p>

      <div>
        <p><em>“The lost cannot be recovered; but let us save what remains: not by vaults and locks which fence them from the public eye and use, in consigning them to the waste of time, but by such a multiplication of copies, as shall place them beyond the reach of accident.”</em></p><p>— Thomas Jefferson, 1791</p>
      </div>

      <p><span id="guide">Guide</span> <a href="#guide">§</a></p>

      <p>
        The list of torrents is split in three parts:<br>
        1. The first part is managed and released by Anna’s Archive. These include books, papers, and magazines from websites such as Z-Library and Internet Archive. It also includes metadata records from websites such as WorldCat and ISBNdb.<br>
        2. The second part is managed and released by others, such as Library Genesis and Sci-Hub. We include these torrents in order to present a unified list of everything you need to mirror Anna’s Archive.<br>
        3. Miscellaneous other torrents; not critical to seed and not included in stats or the torrent list generator.<br>
      </p>

      <p>
        Torrents seeded by Anna’s Archive are indicated with a checkmark (✅). Some torrents get temporarily embargoed (🔒) upon release, for various reasons (e.g. protecting our scraping methods). An embargo means very slow initial seeding speeds. They get lifted within a year.
      </p>

      <p>
        For more information about the different collections, see the <a href="https://annas-archive.org/datasets">Datasets</a> page. Also see the <a href="https://annas-archive.org/faq#torrents">Torrents FAQ</a>.
      </p>

      <p>
        <strong>IMPORTANT:</strong> If you seed large amounts of our collection (50TB or more), please <a href="https://annas-archive.org/contact">contact us</a> so we can let you know when we deprecate any large torrents.
      </p>

      <p><span id="stats">Stats</span> <a href="#stats">§</a></p>

      <p>
        You can help out enormously by seeding torrents that are low on seeders. If everyone who reads this chips in, we can preserve these collections forever. This is the current breakdown, excluding embargoed torrents, but including external torrents:
      </p>

      <table>
        <tbody><tr><td>🔴 210.2TB</td><td>&lt;4 seeders</td></tr>
        <tr><td>🟡 293.4TB</td><td>4–10 seeders</td></tr>
        <tr><td>🟢 17.5TB</td><td>&gt;10 seeders</td></tr>
      </tbody></table>

      
      <div><p>Scraped from <a href="https://opentrackr.org/">opentrackr.org</a>.</p></div>

      

<!--       <div class="mt-8 group"><span class="text-xl font-bold" id="long_term_seeders">Long Term Seeders</span> <a href="#long_term_seeders" class="custom-a invisible group-hover:visible text-gray-400 hover:text-gray-500 text-sm align-[2px]">§</a></div>

      <p class="mb-4">
        List of heroes who are committed to long term seeding of all or large parts of this torrent list. These people help preserve humanity’s knowledge and culture, and we are deeply grateful for that. <a href="/contact">Contact us</a> if you wish to be added. We’ll give you Amazing Archivist-level membership if you seed 100TB+. IP addresses are required to supply so we can verify if you’re still seeding.
      </p> -->

      <!-- <table>
        <tr><th class="text-left pr-4">Username</th><th class="text-left pr-4">Contact</th><th class="text-left pr-4">IPs</th><th class="text-left pr-4">Notes</th></tr>
        <tr><td class="pr-4"><a href="/profile/Anna000">AnnaArchivist #Anna000</a></td><td class="pr-4"><a href="/contact">Contact</a></td><td class="pr-4">95.214.235.224</td><td class="pr-4">Anna’s Archive is committed to seeding all the torrents in this list for as long as possible.</td></tr>
      </table> -->

      <p><span id="generate_torrent_list">Generate Torrent List</span> <a href="#generate_torrent_list">§</a></p>

      <p>
        Generate a list of torrents, sorted by <a href="https://annas-software.org/AnnaArchivist/annas-archive/-/issues/157">(seeders + 0.1*leechers)*fraction-of-torrent-size-compared-to-average-size + random-number-between-0.0-and-2.0</a>, ascending. Specify a maximum TB to store (we simply keep adding torrents until max TB is reached).
      </p>

      

      <p>
        We only show non-obsolete, non-embargoed files with at least one seeder here. For a complete list see the full <a href="https://annas-archive.org/dyn/torrents.json">torrents JSON</a>. For an unofficial tool that actually downloads the torrent files, see <a href="https://github.com/shaped1/annas-torrents">this repo</a>.
      </p>

      <p><span id="similar_lists">Similar Lists</span> <a href="#similar_lists">§</a></p>

      <p>
        Similar lists, independently maintained. Note that at the time of this writing, all these lists are included in our list, under <a href="#external">External Collections</a>, similarly to how Anna’s Archive itself is a meta-collection of many external collections.
      </p>

      <ul>
        <li><a href="https://ipdl.cat/">ipdl.cat</a></li>
        <li><a href="https://phillm.net/libgen-seeds-needed.php">PhillM's LibGen torrent index</a></li>
      </ul>

          <p><span id="managed_by_aa">Managed by Anna’s Archive</span> <a href="#managed_by_aa">§</a></p>

          <p>
            These torrents are managed and released by Anna’s Archive.
          </p>

          <p>
            Torrents with “aac” in the filename use the <a href="https://annas-blog.org/annas-archive-containers.html">Anna’s Archive Containers format</a>. Torrents that are crossed out have been superseded by newer torrents, for example because newer metadata has become available — we normally only do this with small metadata torrents.
            <!-- Some torrents that have messages in their filename are “adopted torrents”, which is a perk of our top tier <a href="/donate">“Amazing Archivist” membership</a>. -->
          </p>

      
          <p><span id="external">External Collections</span> <a href="#external">§</a></p>

          <p>
            These torrents are managed and released by others. We include these torrents in order to present a unified list of everything you need to mirror Anna’s Archive.
          </p>

      
          <p><span id="other_aa">Other Torrents by Anna’s Archive</span> <a href="#other_aa">§</a></p>

          <p>
            These are miscellaneous torrents which are not critical to seed, but contain useful data for certain use cases. These torrents are not included in the seeding stats or torrent list generator.
          </p>

      
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ted Chiang has won the PEN/Faulkner Foundation's short story prize (174 pts)]]></title>
            <link>https://lithub.com/ted-chiang-has-won-the-pen-faulkner-foundations-short-story-prize/</link>
            <guid>40672158</guid>
            <pubDate>Thu, 13 Jun 2024 17:14:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lithub.com/ted-chiang-has-won-the-pen-faulkner-foundations-short-story-prize/">https://lithub.com/ted-chiang-has-won-the-pen-faulkner-foundations-short-story-prize/</a>, See on <a href="https://news.ycombinator.com/item?id=40672158">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<a href="https://lithub.com/category/the-hub/">
		<div>
			<p><img src="https://s26162.pcdn.co/wp-content/plugins/lb-content-widgets/img/lit-horn.svg">
			</p>
			<p>
				<h2>The Hub</h2>
			</p>
			<p>
				<h2>News, Notes, Talk</h2>
			</p>
		</div>
		</a>
		
				<div>
				
				
				
				
				<p><a href="https://www.flickr.com/photos/arturovillarrubia/5494916110/" rel="nofollow" target="_blank"><em>Photo by Arturo Villarrubia</em></a></p>
<p>Science fiction writer Ted Chiang has won <a href="https://www.penfaulkner.org/our-awards/the-pen-malamud-award/" target="_blank">the&nbsp;2024 PEN/Bernard and Ann Malamud Award for Excellence in the Short Story.</a> The award is given each year to a writer who has “demonstrated exceptional achievement in the short story form.”</p>
<p>Ted Chiang has published two collections of short fiction, <em>Stories of Your Life and Others</em>, and <em>Exhalation</em>, both of which have won many awards including four Hugos and four Nebulas. Award Committee Chair Jung Yun describes Chiang’s work as “an absolute wonder to behold,” and “invite readers to think, imagine, and explore unique worlds beyond their own.” She also noted that “Chiang exemplifies Bernard Malamud’s belief that a short story can produce ‘the surprise and effect of a profound knowledge in a short time.’”</p>
<p>Chiang joins past prize winners Edwidge Danticat, Yiyun Li, Charles Baxter, Lydia Davis, John Edgar Wideman, Amina Gautier, Joan Silber, Jhumpa Lahiri, Joy Williams, and Deborah Eisenberg.</p>
<p>The official bestowing of the prize will happen at the end of the year, at the Annual&nbsp;PEN/Malamud Award Ceremony on Friday, December 6, 2024.</p>
				
										
									
				

				

			</div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Please maintain eye contact for the duration of the ad (176 pts)]]></title>
            <link>https://twitter.com/soren_iverson/status/1801253187602788424</link>
            <guid>40671381</guid>
            <pubDate>Thu, 13 Jun 2024 16:06:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/soren_iverson/status/1801253187602788424">https://twitter.com/soren_iverson/status/1801253187602788424</a>, See on <a href="https://news.ycombinator.com/item?id=40671381">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[AMD CEO Lisa Su reminisces about designing the PS3's infamous Cell processor (215 pts)]]></title>
            <link>https://www.tomshardware.com/tech-industry/amd-ceo-lisa-su-reminisces-on-helping-design-the-ps3s-infamous-cell-processor-at-ibm</link>
            <guid>40670898</guid>
            <pubDate>Thu, 13 Jun 2024 15:31:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/tech-industry/amd-ceo-lisa-su-reminisces-on-helping-design-the-ps3s-infamous-cell-processor-at-ibm">https://www.tomshardware.com/tech-industry/amd-ceo-lisa-su-reminisces-on-helping-design-the-ps3s-infamous-cell-processor-at-ibm</a>, See on <a href="https://news.ycombinator.com/item?id=40670898">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-320-80.jpg" alt="Shot of the Cell CPU inside of a disassembled Sony PlayStation 3." srcset="https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/qEqwoQaWYeBYHsbKhA7DFD.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span>Shot of the Cell CPU inside of a disassembled Sony PlayStation 3.</span>
<span itemprop="copyrightHolder">(Image credit: Greenpro on WikiMedia Commons)</span>
</figcaption>
</div>

<div id="article-body">
<p>Just after <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/computex" data-before-rewrite-localise="https://www.tomshardware.com/tag/computex">Computex 2024</a>, AMD CEO Lisa Su sat down with Stratechery to conduct an extended interview about <a data-analytics-id="inline-link" href="https://stratechery.com/2024/an-interview-with-amd-ceo-lisa-su-about-solving-hard-problems/" data-url="https://stratechery.com/2024/an-interview-with-amd-ceo-lisa-su-about-solving-hard-problems/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">solving hard problems</a> throughout her career— including her time at <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/ibm" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/ibm">IBM</a> and contributing to the legacy of PlayStation from both there and AMD afterward. As she notes, "I've been working on PlayStation for a long time, if you think about it. PlayStation 3, 4, 5...[like the common thread] across multiple companies, yes."</p><p>Now, even if you're familiar with the PlayStation 3 and its nature as being difficult to program thanks to its IBM PowerPC-based Cell processor, you most likely didn't know Lisa Su had any involvement with it before this week. Details are few and far between, but we did manage to find the earliest statement where pre-AMD CEO Lisa Su commented on the matter.</p><p>According to Lisa Su, <a data-analytics-id="inline-link" href="https://www.eetimes.com/ibm-sony-toshiba-team-on-processor-architecture-for-broadband/" data-url="https://www.eetimes.com/ibm-sony-toshiba-team-on-processor-architecture-for-broadband/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">then-Director of Emerging Products at IBM</a> in 2001, "We [IBM] started with a clean sheet of paper and sat down and tried to imagine what sort of processor we'd need five years from now." The decision that IBM, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/sony" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/sony">Sony</a>, and Toshiba made was to create a CPU with an extreme focus on parallelization.&nbsp;</p><p>Today, that approach is fairly common through multicore CPUs, Simultaneous Multi-Threading (SMT, or Hyper-threading under Intel marketing), and even dedicated Efficiency cores, but SMT wouldn't emerge until 2002, and the first <em>consumer</em> multicore CPUs from AMD and Intel wouldn't be seen until 2005. And, of course, <a data-analytics-id="inline-link" href="https://www.techspot.com/article/2363-multi-core-cpu/" data-url="https://www.techspot.com/article/2363-multi-core-cpu/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">the first-ever multicore was released by IBM for workstation and server use in 2001</a>— the same year they were planning the PS3's Cell processor.</p><p>The interviewer points out that Sony's PlayStation 3 is viewed as one of its least successful consoles, which is true. The PlayStation 3 pretty much lost the generation handily to Nintendo's cheap, casual-friendly Wii and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/microsoft" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/microsoft">Microsoft</a>'s less powerful but easier Xbox 360. The complexity of the architecture meant that cross-platform games didn't always perform as well as they should on PS3, though as developers (particularly first-party devs) mastered the hardware, it did result in the most visually stunning console games of the latter half of the generation being Sony exclusives, like <em>Uncharted 3</em> and its ilk.</p><p>Lisa said, "The Cell processor was extremely ambitious at that time, thinking about the type of parallelism it was trying to get out there. Again, I would say, from a business standpoint, it was certainly successful. As you rank things, I think history will tell you that there may be different rankings."</p><p>"My perspective is, the console era has gone through phases [...] but once you went to HD, you had tremendous increase in cost of asset creation, you had developers heavily motivated to support multiple processors, you had game engines coming along. Suddenly, no one wanted to go to the burden of differentiating on the Cell; they just wanted to <em>run</em> on the Cell," Su explained.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-6SfLwwcGyhc4B9pHSZ2DY4"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><p>Interestingly, the seventh generation of home consoles (PlayStation 3, Xbox 360, Nintendo Wii/Wii U) also marks a shift in AMD's allegiances to console manufacturers. AMD produced graphics chips for both Nintendo and Xbox, and all three console manufacturers used the PowerPC CPU architecture. But come to the eighth-gen (PS4, XB1, Switch), both Xbox and PlayStation had switched fully to AMD-powered x86 CPU and GPU architecture. The Switch also saw Nintendo pivot to an Nvidia-powered SoC design (with Arm CPU cores) for their new hybrid console focus.</p><p>With the added context of this interview, one can't help but wonder if Lisa Su's unifying thread throughout the last three generations of PlayStation hardware isn't a coincidence. It could just be corporate happenstance, but going from a humble Product Director and Engineer working on the PS3 at IBM to Senior vice president (in 2012, CEO in 2014) at AMD, setting the future course of both PlayStation and Xbox hardware, is truly impressive.&nbsp;</p><p>It's also a great win for AMD, in general, to provide the hardware behind the two biggest consoles on the market for two consecutive (and a third upcoming) console generations. No matter who wins between Sony and Microsoft and their console war, AMD wins, and that's the kind of thinking that earns you a CEO spot.</p>
</div>




<!-- Drop in a standard article here maybe? -->


</section>





<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MLow: Meta's low bitrate audio codec (327 pts)]]></title>
            <link>https://engineering.fb.com/2024/06/13/web/mlow-metas-low-bitrate-audio-codec/</link>
            <guid>40670612</guid>
            <pubDate>Thu, 13 Jun 2024 15:05:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2024/06/13/web/mlow-metas-low-bitrate-audio-codec/">https://engineering.fb.com/2024/06/13/web/mlow-metas-low-bitrate-audio-codec/</a>, See on <a href="https://news.ycombinator.com/item?id=40670612">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<ul>
<li aria-level="1"><span>At Meta, we support real-time communication (RTC) for billions of people through our apps, including WhatsApp, Instagram, and Messenger.&nbsp;</span></li>
<li aria-level="1"><span>We are working to make RTC accessible by providing a high-quality experience for everyone – even those who might not have the fastest connections or the latest phones.</span></li>
<li aria-level="1"><span>As more and more people have relied on our products to make calls over the years, we’ve been working on new ways to ensure all calls have a solid audio quality.</span></li>
<li aria-level="1"><span>We’ve built the Meta Low Bitrate (MLow) codec: a new tool that improves audio quality especially for those on slow-speed connections.</span></li>
</ul>
<figure id="attachment_21370" aria-describedby="caption-attachment-21370"><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Copy-of-Complexity-Properties-Triangle-1.png?w=663" alt="" width="350" height="295" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Copy-of-Complexity-Properties-Triangle-1.png 663w, https://engineering.fb.com/wp-content/uploads/2024/06/Copy-of-Complexity-Properties-Triangle-1.png?resize=96,81 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Copy-of-Complexity-Properties-Triangle-1.png?resize=192,162 192w" sizes="(max-width: 992px) 100vw, 62vw"><figcaption id="caption-attachment-21370">Figure 1: Increasing complexity or bitrate usually improves quality, but good codecs achieve higher quality while balancing the other two.</figcaption></figure>
<p><span>RTC products use many building blocks to deliver the full experience, and one of the critical components is audio/video codecs. These codecs help compress the captured audio/video data so it can be sent across the internet efficiently to the recipient, keeping the experience real time. For example, the size of raw audio captured for a typical call is 768 kbps (mono, sampling at 48kHz, bit depth 16), which modern codecs are able to compress down to 25-30 kbps. Often this compression comes at the cost of some quality (loss of information), but good codecs can strike a balance among the trio of quality, bitrate, and complexity by exploiting deep knowledge about the nature of the audio signal as well as by using psychoacoustics.&nbsp;</span></p>
<p><span>Building a good codec is quite challenging, and that is why we don’t see new codecs emerging very often. The last widely known, good open-source codec was Opus, released in 2012, which has become the codec of choice for the wide variety of applications on the internet. Meta has used Opus for all its RTC needs, and so far it has served us well – helping to deliver quality calls to billions of users across the globe.&nbsp;</span></p>
<p><iframe title="YouTube video player" src="https://www.youtube.com/embed/3ypsZUNRjI4?si=Jo_8nGuviRGzo-4f" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<h2><span>Our motivation for building a new codec</span></h2>
<p><span>Given the massive scale of RTC usage in Meta products, we get to see how a codec performs in a range of network scenarios and how it impacts the end user’s experience. In particular, we’ve observed that a significant chunk of calls have poor network connections throughout or for part of a call. Typically a bandwidth estimation module (BWE) detects the quality of the network, and as the network quality degrades, we need to lower the codec operating bitrate to avoid congesting the network and keep the audio flowing – impacting the trio balance referenced above. Complicating matters, conducting a video call despite poor network quality leaves little room for audio and pushes the audio bitrate further down. The lowest operating point for Opus is 6 kbps, at which it runs in NarrowBand mode (0 – 4kHz) and does not adequately capture all the sound frequencies produced by human voices—and so doesn’t sound as clear or natural. Here is an example of how Opus sounds at 6kbps and the corresponding reference file for comparison.</span></p>
<p><span>Raw reference signal:&nbsp;</span></p>

<p><span>Opus @ 6 kbps NarrowBand (NB):&nbsp;</span></p>

<p><span>Over the last two years, we have seen development of some new machine learning (ML)-based audio codecs that provide good quality audio at very low bitrates. In October of 2022, Meta released </span><a href="https://ai.meta.com/blog/ai-powered-audio-compression-technique/" target="_blank" rel="noopener"><span>Encodec</span></a><span>, which achieves amazingly crisp audio quality at very low bitrates. While these AI/ML-based codecs are able to achieve great quality at low bitrates, it often comes at the expense of heavy computational cost. Consequently, only the very high-end (expensive) mobile handsets are able to run these codecs reliably, while users running on lower-end devices continue to experience audio quality issues in low-bitrate conditions. So the net impact of these newer computationally expensive codecs is actually limited to a small portion of users.</span></p>
<p><span>A significant number of our users still use low-end devices. For example, more than 20 percent of our calls are made on ARMv7 devices, and 10’s of millions of daily calls on WhatsApp are on 10-year-old-plus devices. Given the readily available codec choices and our commitment to ensure that all users – regardless of what device they’re on – have a quality calling experience, we clearly need a codec with very low-compute requirements that still delivers high-quality audio at these lowest bitrates.</span></p>

<p><span>We broke ground with our development of a new codec in late 2021. After nearly two years of active development and testing, we are proud to announce </span><b>M</b><span>eta </span><b>Low</b><span> Bitrate audio codec, aka MLow, which achieves two-times-better quality than Opus (POLQA MOS 1.89 vs 3.9 @ 6kbps WB). Even more importantly, we are able to achieve this great quality while keeping MLow’s computational complexity </span><b>10 percent lower</b><span> than that of Opus.&nbsp;</span></p>
<p><span>Figure 2 below shows a MOS (Mean Opinion Score) plot on a 1-5 scale and compares the POLQA scores between Opus and MLow at various bitrates. As the chart makes evident, MLow has a huge advantage over Opus at the lowest bitrates, where it saturates quality faster than Opus.</span></p>
<figure id="attachment_21287" aria-describedby="caption-attachment-21287"><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-2.png?w=916" alt="" width="500" height="464" srcset="https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-2.png 1738w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-2.png?resize=916,851 916w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-2.png?resize=768,713 768w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-2.png?resize=1024,951 1024w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-2.png?resize=1536,1426 1536w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-2.png?resize=96,89 96w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-2.png?resize=192,178 192w" sizes="(max-width: 992px) 100vw, 62vw"><figcaption id="caption-attachment-21287">Figure 2: POLQA score comparing Opus (WB) versus MLow at various bitrates across a large dataset of files.</figcaption></figure>
<p><span>We have already fully launched MLow to all Instagram and Messenger calls and are actively rolling it out on WhatsApp—and we’ve already seen incredible improvement in user engagement driven by better audio quality.</span></p>
<p><span>Here are some audio samples for you to listen to. We suggest that you use your favorite pair of headphones to appreciate the striking audio-quality differences.</span></p>
<table>
<tbody>
<tr>
<td><span>Opus 6 kbps NB</span></td>
<td><span>MLow 6 kbps WB</span></td>
<td><span>Reference</span></td>
</tr>
<tr>
<td><audio src="https://engineering.fb.com/wp-content/uploads/2024/06/opus_6000_kbps_48k_m_sample2_trimmed.wav" controls="controls"></audio></td>
<td><audio src="https://engineering.fb.com/wp-content/uploads/2024/06/mlow_6000_kbps_48k_m_sample2_trimmed.wav" controls="controls"></audio></td>
<td><audio src="https://engineering.fb.com/wp-content/uploads/2024/06/48k_m_sample2_trimmed.wav" controls="controls"><span data-mce-type="bookmark">﻿</span></audio></td>
</tr>
<tr>
<td><audio src="https://engineering.fb.com/wp-content/uploads/2024/05/opus_6000_kbps_48k_f_sample3.wav" controls="controls"></audio></td>
<td><audio src="https://engineering.fb.com/wp-content/uploads/2024/06/mlow_6000_kbps_48k_f_sample3.wav" controls="controls"><span data-mce-type="bookmark">﻿</span></audio></td>
<td><audio src="https://engineering.fb.com/wp-content/uploads/2024/05/48k_f_sample3.wav" controls="controls"><span data-mce-type="bookmark">﻿</span></audio></td>
</tr>
</tbody>
</table>
<p><span>Being able to encode high-quality audio at lower bitrates also unlocks more effective Forward Error Correction (FEC) strategies. Compared with Opus, with MLow we can afford to pack FEC at much lower bitrates, which significantly helps to improve the audio quality in packet loss scenarios.&nbsp;</span></p>
<p><span>Here are two audio samples at 14 kbps with heavy 30 percent receiver-side packet loss.</span></p>
<p>Opus:</p>
<p><audio src="https://engineering.fb.com/wp-content/uploads/2024/05/opus_eloss_10_dloss_30_14000_bps_48k_f_sample2.wav" controls="controls"></audio><br>
MLow:</p>

<p><span>Note that at these bitrates, Opus is not able to encode any inband FEC. It needs a minimum of 19 kbps to encode any inband FEC at 10 percent packet loss, which hurts the audio recovery.</span></p>
<h2><span>MLow internals</span></h2>
<p><span>MLow builds on the concepts of a classic CELP (Code Excited Linear Prediction) codec with advancements around excitation generation, parameter quantization, and coding schemes. Figure 3 is a high-level visual of how the codec works internally. On the left we have an input signal (raw PCM audio) feeding into the encoder, which then splits the signal into two low and high-frequency bands. Then, each band is encoded separately while making use of shared information to achieve better compression. All the output is passed through a range encoder to further compress and generate an encoded payload. The decoder does the exact opposite when given the payload to generate output audio signals.</span></p>
<figure id="attachment_21288" aria-describedby="caption-attachment-21288"><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-3.png?w=1024" alt="" width="1024" height="242" srcset="https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-3.png 1484w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-3.png?resize=916,217 916w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-3.png?resize=768,182 768w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-3.png?resize=1024,242 1024w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-3.png?resize=96,23 96w, https://engineering.fb.com/wp-content/uploads/2024/05/MLow-image-3.png?resize=192,45 192w" sizes="(max-width: 992px) 100vw, 62vw"><figcaption id="caption-attachment-21288">Figure 3: High level MLow encoder and decoder architecture.</figcaption></figure>
<p><span>With these split-band optimizations, we are able to encode the high band using very few bits, which lets MLow deliver SuperWideBand (32kHz sampling) using a much lower bitrate.</span></p>
<h2><span>What’s next?</span></h2>
<p><span>MLow has greatly enhanced audio quality on low-end devices while still ensuring calls are end-to-end encrypted. We are really excited about what we have accomplished in just the last two years—from developing a new codec to successfully shipping it to billions of users around the globe. We’re continuing to work on improving the audio recovery in heavy packet loss networks by pumping out more redundant audio, which MLow allows us to do efficiently. We’re excited to share more as we continue working to make it easier for all our users to make quality audio calls.&nbsp;</span></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Shpool, a Lightweight Tmux Alternative (214 pts)]]></title>
            <link>https://github.com/shell-pool/shpool</link>
            <guid>40669337</guid>
            <pubDate>Thu, 13 Jun 2024 13:22:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/shell-pool/shpool">https://github.com/shell-pool/shpool</a>, See on <a href="https://news.ycombinator.com/item?id=40669337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">shpool</h2><a id="user-content-shpool" aria-label="Permalink: shpool" href="#shpool"></a></p>
<p dir="auto"><code>shpool</code> is a service that enables session persistence by allowing the
creation of named shell sessions owned by <code>shpool</code> so that the session
is not lost if the connection drops. <code>shpool</code> can be thought of as a lighter
weight alternative to <code>tmux</code> or GNU <code>screen</code>. While <code>tmux</code> and <code>screen</code> take over
the whole terminal and provide window splitting and tiling features, <code>shpool</code>
only provides persistent sessions. The biggest advantage of this approach is
that <code>shpool</code> does not break native scrollback or copy-paste.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing from crates.io</h3><a id="user-content-installing-from-cratesio" aria-label="Permalink: Installing from crates.io" href="#installing-from-cratesio"></a></p>
<p dir="auto">Run</p>
<div data-snippet-clipboard-copy-content="cargo install shpool
curl -fLo &quot;${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.service&quot; --create-dirs https://raw.githubusercontent.com/shell-pool/shpool/master/systemd/shpool.service
sed -i &quot;s|/usr|$HOME/.cargo|&quot; &quot;${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.service&quot;
curl -fLo &quot;${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.socket&quot; --create-dirs https://raw.githubusercontent.com/shell-pool/shpool/master/systemd/shpool.socket
systemctl --user enable shpool
systemctl --user start shpool
loginctl enable-linger"><pre><code>cargo install shpool
curl -fLo "${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.service" --create-dirs https://raw.githubusercontent.com/shell-pool/shpool/master/systemd/shpool.service
sed -i "s|/usr|$HOME/.cargo|" "${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.service"
curl -fLo "${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.socket" --create-dirs https://raw.githubusercontent.com/shell-pool/shpool/master/systemd/shpool.socket
systemctl --user enable shpool
systemctl --user start shpool
loginctl enable-linger
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Generally <code>shpool</code> is used to provide persistent sessions when
sshing in to a remote host. To do so, <code>shpool</code> must be installed
on the remote host. No extra software is required on the client.
After installing and setting up, the typical usage pattern
is to ssh into the host you have installed shpool on, then create
a new named session by running <code>shpool attach main</code>. Here <code>main</code>
is the name of the session. You'll want a separate named session
for each terminal you use to connect to your remote host. If your
connection drops or becomes stuck, you can ssh back into the remote
host and re-attach to the same named session by running <code>shpool attach main</code>
again.</p>
<p dir="auto">If your terminal gets stuck and you forcibly close the window, you
might find that <code>shpool</code> still think a terminal is connected to
your session when you attempt to reattach. This is likely because
an ssh proxy is holding the connection open in the vain hope that
it will get some traffic again. You can just run <code>shpool detach main</code>
to force the session to detach and allow you to attach.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">You can specify some additional configuration options to the daemon
by passing a <code>-c /path/to/config.toml</code> flag, or by creating and
editing <code>~/.config/shpool/config.toml</code>. The options available
are documented in detail in <code>libshpool/src/config.rs</code>, but there
are a few common things you may wish to tweak.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Detach Keybinding</h4><a id="user-content-detach-keybinding" aria-label="Permalink: Detach Keybinding" href="#detach-keybinding"></a></p>
<p dir="auto">You may wish to configure your detach keybinding.
By default, <code>shpool</code> will detach from the current user session when you
press the sequence <code>Ctrl-Space Ctrl-q</code> (press <code>Ctrl-Space</code> then release
it and press <code>Ctrl-q</code>, don't try to hold down all three keys at once),
but you can configure a different binding by adding an entry
like</p>
<div data-snippet-clipboard-copy-content="[[keybinding]]
binding = &quot;Ctrl-a d&quot;
action = &quot;Detach&quot;"><pre><code>[[keybinding]]
binding = "Ctrl-a d"
action = "Detach"
</code></pre></div>
<p dir="auto">to you <code>~/.config/shpool/config.toml</code>.</p>
<p dir="auto">For the moment, control is the only modifier key supported, but the keybinding
engine is designed to be able to handle more, so if you want a different one,
you can file a bug with your feature request.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Session Restore Mode</h4><a id="user-content-session-restore-mode" aria-label="Permalink: Session Restore Mode" href="#session-restore-mode"></a></p>
<p dir="auto">Shpool can do a few different things when you re-attach to an existing
session. You can choose what you want it to do with the <code>session_restore_mode</code>
configuration option.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><code>"screen"</code> (default) - restore a screenful of history</h5><a id="user-content-screen-default---restore-a-screenful-of-history" aria-label="Permalink: &quot;screen&quot; (default) - restore a screenful of history" href="#screen-default---restore-a-screenful-of-history"></a></p>
<p dir="auto">The <code>"screen"</code> option causes <code>shpool</code> to re-draw sufficient output to fill the
entire screen of the client terminal as well as using the SIGWINCH trick
described in the <code>"simple"</code> section below. This will help restore
context for interactive terminal sessions that are not full blown ncurses
apps. <code>"screen"</code> is the default reattach behavior for <code>shpool</code>.
You can choose this option explicitly by adding</p>
<div data-snippet-clipboard-copy-content="session_restore_mode = &quot;screen&quot;"><pre><code>session_restore_mode = "screen"
</code></pre></div>
<p dir="auto">to your <code>~/.config/shpool/config.toml</code>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><code>"simple"</code> - only ask child processes to redraw</h5><a id="user-content-simple---only-ask-child-processes-to-redraw" aria-label="Permalink: &quot;simple&quot; - only ask child processes to redraw" href="#simple---only-ask-child-processes-to-redraw"></a></p>
<p dir="auto">The <code>"simple"</code> option avoids restoring any output. In this reconnect mode, <code>shpool</code> will
issue some SIGWINCH signals to try to convince full screen ncurses apps
such as vim or emacs to re-draw the screen, but will otherwise do nothing.
Any shell output produced when there was no client connected to the session
will be lost. You can choose this connection mode by adding</p>
<div data-snippet-clipboard-copy-content="session_restore_mode = &quot;simple&quot;"><pre><code>session_restore_mode = "simple"
</code></pre></div>
<p dir="auto">to your <code>~/.config/shpool/config.toml</code>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><code>{ lines = n }</code> - restore the last n lines of history</h5><a id="user-content--lines--n----restore-the-last-n-lines-of-history" aria-label="Permalink: { lines = n } - restore the last n lines of history" href="#-lines--n----restore-the-last-n-lines-of-history"></a></p>
<p dir="auto">The lines option is much like the <code>"screen"</code> option, except that rather
than just a screenful of text, it restores the last n lines of text
from the terminal being re-attached to. This could be useful if you
wish to have more context than a single screenful of text. Note that
n cannot exceed the value of the <code>output_spool_lines</code> configuration
option, but it defaults to the value of the lines option, so you likely
won't need to change it.</p>
<div data-snippet-clipboard-copy-content="session_restore_mode = { lines = n }"><pre><code>session_restore_mode = { lines = n }
</code></pre></div>
<p dir="auto">where n is a number to your <code>~/.config/shpool/config.toml</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Shell Config</h4><a id="user-content-shell-config" aria-label="Permalink: Shell Config" href="#shell-config"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">bash</h5><a id="user-content-bash" aria-label="Permalink: bash" href="#bash"></a></p>
<p dir="auto">If you use bash, you may want to ensure that the <code>huponexit</code> option
is set to make sure that child processes exit when you leave a
shell. Without this setting, background processes you have
spawned over the course of your shell session will stick around
in the <code>shpool</code> daemon's process tree and eat up memory. To set
this option add</p>

<p dir="auto">to your <code>~/.bashrc</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Subcommands</h3><a id="user-content-subcommands" aria-label="Permalink: Subcommands" href="#subcommands"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">shpool daemon</h4><a id="user-content-shpool-daemon" aria-label="Permalink: shpool daemon" href="#shpool-daemon"></a></p>
<p dir="auto">The <code>daemon</code> subcommand causes <code>shpool</code> to run in daemon mode. When running in
this mode, <code>shpool</code> listens for incoming connections and opens up subshells,
retaining ownership of them in a table. In general, this subcommand will not
be invoked directly by users, but will instead be called from a systemd unit
file.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">shpool attach</h4><a id="user-content-shpool-attach" aria-label="Permalink: shpool attach" href="#shpool-attach"></a></p>
<p dir="auto">The <code>attach</code> subcommand connects to the <code>shpool daemon</code> instance, passing in a
name. If the name is new, a new shell is created, and if it already exists it
just attaches to the existing session so long as no other terminal is currently
connected to that session. The <code>--ttl</code> flag can be used to limit how long the
session will last.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">shpool list</h4><a id="user-content-shpool-list" aria-label="Permalink: shpool list" href="#shpool-list"></a></p>
<p dir="auto">Lists all the current shell sessions.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">shpool detach</h4><a id="user-content-shpool-detach" aria-label="Permalink: shpool detach" href="#shpool-detach"></a></p>
<p dir="auto">Detach from a one or more sessions without stopping them.
Will detach the current session if run from inside a <code>shpool</code>
session with no session name arguments.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">shpool kill</h4><a id="user-content-shpool-kill" aria-label="Permalink: shpool kill" href="#shpool-kill"></a></p>
<p dir="auto">Kills a named shell session.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">(Optional) Automatically Connect to shpool</h3><a id="user-content-optional-automatically-connect-to-shpool" aria-label="Permalink: (Optional) Automatically Connect to shpool" href="#optional-automatically-connect-to-shpool"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Explicitly named sessions</h4><a id="user-content-explicitly-named-sessions" aria-label="Permalink: Explicitly named sessions" href="#explicitly-named-sessions"></a></p>
<p dir="auto">Specifying session names yourself lets you assign logical
roles such as text editing to each session.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">ssh config</h5><a id="user-content-ssh-config" aria-label="Permalink: ssh config" href="#ssh-config"></a></p>
<p dir="auto">If you typically connect to a small number of sessions with
the same jobs on a particular machine, custom ssh config
blocks on your client machine are probably the best
fit.</p>
<p dir="auto">To do this, you can add a config block named <code>edit</code> like so</p>
<div data-snippet-clipboard-copy-content="Host = edit
    Hostname remote.host.example.com

    RemoteCommand shpool attach -f edit
    RequestTTY yes"><pre><code>Host = edit
    Hostname remote.host.example.com

    RemoteCommand shpool attach -f edit
    RequestTTY yes
</code></pre></div>
<p dir="auto">to <code>~/.ssh/config</code> on your client machine. You will need one
such block per session name. You can then invoke this with
<code>ssh edit</code>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">shell function</h5><a id="user-content-shell-function" aria-label="Permalink: shell function" href="#shell-function"></a></p>
<p dir="auto">If you would rather have a little more flexibility in
specifying the session name and machine you are targeting,
you can make a custom shell function to let you specify
both at invocation time. Add</p>
<div data-snippet-clipboard-copy-content="function shpool-ssh () {
    if [ $# -ne 2 ] ; then
        echo &quot;usage: shpool-ssh <remote-machine> <session-name>&quot; >&amp;2
        return 1
    fi
    ssh -t &quot;-oRemoteCommand=shpool attach -f $2&quot; &quot;$1&quot;
}"><pre><code>function shpool-ssh () {
    if [ $# -ne 2 ] ; then
        echo "usage: shpool-ssh &lt;remote-machine&gt; &lt;session-name&gt;" &gt;&amp;2
        return 1
    fi
    ssh -t "-oRemoteCommand=shpool attach -f $2" "$1"
}
</code></pre></div>
<p dir="auto">to your <code>.bashrc</code> then invoke it like
<code>shpool-ssh remote.host.example.com main</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Local tty based</h4><a id="user-content-local-tty-based" aria-label="Permalink: Local tty based" href="#local-tty-based"></a></p>
<p dir="auto">Rather than specify an explicit name when you connect, you
can set up your system to automatically generate a <code>shpool</code>
session name based on your local terminal emulator's tty
number. To do so, you can add a block of custom ssh config
in the <code>~/.ssh/config</code> of your local machine like</p>
<div data-snippet-clipboard-copy-content="Host = by-tty
    User remoteuser
    Hostname remote.host.example.com

    RemoteCommand shpool attach -f &quot;ssh-$(basename $(tty))&quot;
    RequestTTY yes"><pre><code>Host = by-tty
    User remoteuser
    Hostname remote.host.example.com

    RemoteCommand shpool attach -f "ssh-$(basename $(tty))"
    RequestTTY yes
</code></pre></div>
<p dir="auto">which you then invoke with <code>ssh by-tty</code>. You can apply the same principle
of using <code>$(basename $(tty))</code> to get a unique id for your local terminal
to the custom shell function approach as well.</p>
<p dir="auto">The local-tty based approach has the advantage that you don't
need to specify a session name, but it can run into problems
if you have to close the local window and open a new terminal,
which can come up if your connection freezes rather than drops.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comparison with other tools</h2><a id="user-content-comparison-with-other-tools" aria-label="Permalink: Comparison with other tools" href="#comparison-with-other-tools"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>tmux</code> and GNU <code>screen</code></h3><a id="user-content-tmux-and-gnu-screen" aria-label="Permalink: tmux and GNU screen" href="#tmux-and-gnu-screen"></a></p>
<p dir="auto"><code>tmux</code> is probably the best known session persistence tool, and
GNU <code>screen</code> has a similar feature set, so in comparison to <code>shpool</code>
it can be thought of as belonging to the same category.</p>
<p dir="auto">The main way that <code>shpool</code> differs from <code>tmux</code> is that <code>tmux</code> is a
terminal multiplexer which necessarily means that it offers session
persistence features, while <code>shpool</code> only aims to be a session
persistence tool. In contrast to <code>tmux</code> the philosophy of <code>shpool</code>
is that managing different terminals is the job of your display or
window manager, not your session persistence tool. Every operating
system has its own idioms for switching between applications, and
there is no reason to switch to different idioms when switching
between terminals. Especially for users of tiling window managers
such as <code>i3</code>, <code>sway</code> or <code>xmonad</code>, tmux's multiplexing features are
redundant.</p>
<p dir="auto">While <code>tmux</code> renders terminal contents remotely and only paints
the current view to the screen, <code>shpool</code> just directly sends
all shell output back to the user's local terminal. This means
that all rendering is handled by a single terminal state machine
rather than going through <code>tmux</code>s internal in-memory terminal
before getting formatted and re-rendered by the local terminal.
This has performance implications, and probably most
importantly means that a terminal using <code>shpool</code> will feel
completely native. Scrollback and copy-paste will work exactly
as they do in your native terminal, while they can behave differently
when using <code>tmux</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/mobile-shell/mosh"><code>mosh</code></a></h3><a id="user-content-mosh" aria-label="Permalink: mosh" href="#mosh"></a></p>
<p dir="auto"><code>mosh</code> is another tool focused on providing persistent remote shell
sessions. It differs from the other tools discussed here in that it
has its own network protocol, which it bootstraps off of regular
ssh. Like <code>tmux</code>, it renders the screen contents remotely and sends
just the current view back. It is somewhat unique in trying to
predicatively guess the right output to display to the user if
there is a network lag.</p>
<p dir="auto"><code>shpool</code> differs from <code>mosh</code> in that it has nothing to do with
the network, remaining confined to a single machine like most of
these other tools. Just like in the case of <code>tmux</code>, <code>mosh</code> will
impact the way scrollback and copy-paste work, while <code>shpool</code>
keeps them feeling entirely native.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/crigler/dtach"><code>dtach</code></a>, <a href="https://github.com/martanne/abduco"><code>abduco</code></a>, and <a href="https://github.com/yazgoo/diss"><code>diss</code></a></h3><a id="user-content-dtach-abduco-and-diss" aria-label="Permalink: dtach, abduco, and diss" href="#dtach-abduco-and-diss"></a></p>
<p dir="auto">These tools have the most in common with <code>shpool</code>. Just like <code>shpool</code>, they
eschew multiplexing and just send the raw bytes back to you for your local
terminal to render. While you could say that <code>shpool</code> aims to be a
simpler version of <code>tmux</code>, these tools follow the same philosophy with
an even greater laser focus on simplicity and doing one thing well.</p>
<p dir="auto"><code>shpool</code> aims to be an easy and pleasant experience for people
who just want session persistence without having to care about
it too much, so it has a few more "cushy" features that would
not be as good a fit for the focus on simplicity of these
tools.</p>
<p dir="auto">The most obvious of these features is the difference between
how <code>shpool</code> and these programs handle re-attaches. Though under normal operation,
<code>shpool</code> does not do any rendering and subsetting of the shell
output, it continually maintains an in-memory render of the
terminal state via the <a href="https://crates.io/crates/shpool_vt100" rel="nofollow"><code>shpool_vt100</code></a>
crate. On reattach, <code>shpool</code> will use this in-memory render to
re-draw the screen, so you can easily see where you were when
your connection dropped. This even allows you to see output
generated after your connection dropped.</p>
<p dir="auto">Another such feature is the automatic prompt prefix. <code>shpool</code>
will detect when you are using a known shell (currently
<code>bash</code>, <code>zsh</code>, or <code>fish</code>) and automatically inject a prefix
into your prompt to let you know the name of the <code>shpool</code> session
you are in. This adds some nice context so you don't lose
track of your terminals and have some hint about the current
terminal state.</p>
<p dir="auto">There are also some features <code>shpool</code> is missing which these
programs have. In particular, it seems that <code>dtach</code> and <code>abduco</code>
support shared sessions, while <code>shpool</code> only allows a single
client to be connected to a particular session at a time.
There may be more since I don't know these tools as well
as <code>shpool</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hacking</h2><a id="user-content-hacking" aria-label="Permalink: Hacking" href="#hacking"></a></p>
<p dir="auto">For information on how to develop shpool, see <a href="https://github.com/shell-pool/shpool/blob/master/HACKING.md">HACKING.md</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iTerm 3.5.1 removes automatic OpenAI integration, requires opt-in (127 pts)]]></title>
            <link>https://iterm2.com/downloads.html</link>
            <guid>40668803</guid>
            <pubDate>Thu, 13 Jun 2024 12:27:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iterm2.com/downloads.html">https://iterm2.com/downloads.html</a>, See on <a href="https://news.ycombinator.com/item?id=40668803">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <h3>Stable Releases</h3>


<hr>


<p>
Stable releases update rarely but have no serious bugs.
</p>


<h4><a href="https://iterm2.com/downloads/stable/iTerm2-3_5_1.zip"><img src="https://iterm2.com/images/Download.png" width="100" height="27">iTerm2 3.5.1 (OS 10.15+)
</a></h4>


<p>
This is the recommended build for most users.
Built on June 11, 2024.
<br>
</p><p>▸ Show Changelog</p>





<p>▸ Show Older Versions</p>




<h3>Test Releases</h3>


<hr>


<p>
Test releases update many times a year and are occasionally unstable.
</p>


<h4><a href="https://iterm2.com/downloads/beta/iTerm2-3_5_1beta4.zip"><img src="https://iterm2.com/images/Download.png" width="100" height="27">iTerm2 3.5.1beta4 (OS 10.15+)
</a></h4>


<p>
This is the recommended beta build for most users.
Built on June 3, 2024.
<br>
</p><p>▸ Show Changelog</p>





<p>▸ Show Older Versions</p>




<h3>Nightly Builds</h3>


<hr>


<p>A nightly build is made at midnight Pacific time on days where a change was committed. The change log may be seen <a href="https://github.com/gnachman/iTerm2/commits/master">on Github</a>. Nightly builds sometimes have serious bugs.
</p>

<h4>
<a href="https://iterm2.com/nightly/latest">
<img src="https://iterm2.com/images/Download.png" width="100" height="35">Latest nightly build
</a></h4>


<p>
Older nightly builds may be found in the <a href="https://iterm2.com/downloads/nightly">nightly build archives.</a>
</p>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Southwest Airlines Boeing 737-8 Max Experienced Dutch Roll (244 pts)]]></title>
            <link>https://avherald.com/h?article=519ce679</link>
            <guid>40668504</guid>
            <pubDate>Thu, 13 Jun 2024 11:53:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://avherald.com/h?article=519ce679">https://avherald.com/h?article=519ce679</a>, See on <a href="https://news.ycombinator.com/item?id=40668504">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="100%">
<tbody><tr><td id="adcell">


</td>
<td id="ad1cell">


<!--Begin Content Section-->


<!--Begin Article 519ce679-->
<div><p><span>Accident: Southwest B38M enroute on May 25th 2024, Dutch Roll</span></p><div><span>By Simon Hradecky, created Wednesday, Jun 12th 2024 17:25Z, last updated Wednesday, Jun 12th 2024 17:25Z</span><div><p><span>A Southwest Airlines Boeing 737-8 MAX, registration N8825Q performing flight WN-746 from Phoenix,AZ to Oakland,CA (USA) with 175 passengers and 6 crew, was enroute at FL320 when the aircraft experienced Dutch Roll. The crew was able to regain control and landed the aircraft on Oakland's runway 30 about 55 minutes later.<p>The FAA reported: "AIRCRAFT EXPERIENCED A DUTCH ROLL, REGAINED CONTROL AND POST FLIGHT INSPECTION REVEALED DAMAGE TO THE STANDBY PCU, OAKLAND, CA." and stated the aircraft sustained substantial damage, the occurrence was rated an accident.</p><p>The aircraft remained on the ground in Oakland until Jun 6th 2024, then positioned to Everett,WA (USA), ATS facilities, and is still on the ground in Everett 6 days later.</p><p>Dutch Roll is a coupled out of phase movement of the aircraft as result of weakened directional stability (provided by the vertical tail and rudder), in which the aircraft oscillates around its vertical as well as longitudinal axis (coupled yaw and roll).</p><p>The PCU is the power control unit, an actuator controlling the (vertical) rudder.</p><p><a href="https://flightaware.com/live/flight/SWA746/history/20240525/1425Z/KPHX/KOAK" target="_blank">https://flightaware.com/live/flight/SWA746/history/20240525/1425Z/KPHX/KOAK</a></p></span></p></div></div></div>
<!--End Article-->
<div data-nosnippet=""><hr><hr><div><p><br><span>By (anonymous) on Thursday, Jun 13th 2024 13:05Z</span></p></div>
<hr><div><p><br><span>By Picarus on Thursday, Jun 13th 2024 11:24Z</span></p></div>
<hr><div><p><br><span>By Peter Evers on Thursday, Jun 13th 2024 10:27Z</span></p></div>
<hr><div><p><br><span>By Brian Johnson on Thursday, Jun 13th 2024 08:34Z</span></p></div>
<hr><div><p><br><span>By Kris on Thursday, Jun 13th 2024 03:10Z</span></p></div>
<hr><div><p><br><span>By RJ on Thursday, Jun 13th 2024 02:04Z</span></p></div>
<hr><div><p><br><span>By FBW on Thursday, Jun 13th 2024 01:37Z</span></p></div>
<hr><div><p><br><span>By Raffles on Wednesday, Jun 12th 2024 21:11Z</span></p></div>
<hr><div><p><br><span>By SB on Wednesday, Jun 12th 2024 21:00Z</span></p></div>
<hr><div><p><br><span>By Ducky on Wednesday, Jun 12th 2024 20:17Z</span></p></div>
<hr><div><p><br><span>By JT on Wednesday, Jun 12th 2024 20:08Z</span></p></div>
<hr><div><p><br><span>By john on Wednesday, Jun 12th 2024 19:40Z</span></p></div>
<hr><div><p><br><span>By Paul on Wednesday, Jun 12th 2024 18:54Z</span></p></div>
<hr><div><p><br><span>By ernst on Wednesday, Jun 12th 2024 18:23Z</span></p></div>
<hr><div><p><br><span>By ADS on Wednesday, Jun 12th 2024 17:58Z</span></p></div>
<hr><div><p><br><span>By FBW on Wednesday, Jun 12th 2024 17:57Z</span></p></div>
<hr>
</div><!-- End Content Section--></td>
<td><div data-nosnippet="">
<p><a href="https://avherald.com/h?link=00000038" target="_blank"><img src="https://avherald.com/images/cgaqe_160x160_2024.png" width="160" height="160" alt="Aircraft Cabin Air Conference 2024"></a></p><hr>
<p>The Aviation Herald Apps<br>Android and iOS</p>
<p><a href="https://avherald.com/h?link=00000036"><img src="https://avherald.com/images/avhapp_iphone.jpg" width="160" height="258" alt="AVHAPP on Android and iOS" target="_blank"></a></p><p>Support The Aviation Herald</p>
<p>Euro</p>

<p>US$</p>
<br>
<!--<div class="sitetitle">Monthly support</div>
<div class="sitetitle">1 &euro;/month</div>
<div class="sitetext"><form action="https://www.paypal.com/cgi-bin/webscr" method="post">
<input type="hidden" name="cmd" value="_s-xclick">
<input type="hidden" name="hosted_button_id" value="25LUS2V8Q8YMG">
<input type="hidden" name="return" value="http://avherald.com/h?thanks=">
<input type="image" src="/images/btn_avheraldCC_LG.gif" width="160" height="47" border="0" name="submit" alt="Monthly Support The Aviation Herald">
</form></div><br>-->
<!--<br><div align=center><b>Oct 28 2021<br>We are aware of the evacuation of a Transavia B738 at Amsterdam Schiphol today. The wheel fire occurred during taxi about 10 minutes after landing, no injuries. Therefore ground incident outside our coverage.</b><br></div></br>-->

<p>Interview:</p>

&nbsp;</div></td>
</tr>
</tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Indian Startup 3D Prints Rocket Engine in Just 72 Hours (307 pts)]]></title>
            <link>https://spectrum.ieee.org/3d-printed-rocket</link>
            <guid>40668088</guid>
            <pubDate>Thu, 13 Jun 2024 11:02:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/3d-printed-rocket">https://spectrum.ieee.org/3d-printed-rocket</a>, See on <a href="https://news.ycombinator.com/item?id=40668088">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Indian Startup 3D Prints Rocket Engine in Just 72 Hours"><p>A rocket featuring the world’s first rocket engine 3D printed as a single piece blasted off from India’s east coast in late May. Startup <a href="https://agnikul.in/" rel="noopener noreferrer" target="_blank">Agnikul</a> fabricated the engine in just 72 hours and hopes the approach could open the door to “on-demand” rocket launches for operators of <a data-linked-post="2650279684" href="https://spectrum.ieee.org/spacety-startup-plans-news-china-satellites" target="_blank">small satellites</a>.</p><p> The Chennai-based company isn’t the only <a data-linked-post="2658688879" href="https://spectrum.ieee.org/indian-space-private-launch" target="_blank">private space</a> operation to rely heavily on <a data-linked-post="2656527556" href="https://spectrum.ieee.org/3d-printing-could-usher-in-safe-nuclear" target="_blank">3D printing</a>—both <a data-linked-post="2650279177" href="https://spectrum.ieee.org/the-worlds-largest-3d-metal-printer-is-churning-out-rockets" target="_blank">Relativity Space</a><a href="https://www.relativityspace.com/" rel="noopener noreferrer" target="_blank"></a> and <a data-linked-post="2657171481" href="https://spectrum.ieee.org/rocket-booster-rocket-lab" target="_blank">Rocket Lab</a><a href="https://www.rocketlabusa.com/" rel="noopener noreferrer" target="_blank"></a> use the approach extensively to build their <a data-linked-post="2656046543" href="https://spectrum.ieee.org/nasas-space-launch-system-will-lift-off" target="_blank">launch vehicles</a>. What sets Agnikul apart is that its engine is printed in one go, rather than as multiple components that have to then be stitched together, which significantly speeds up manufacturing time.</p><p> On 30 May, the company carried out its first suborbital launch powered by the engine. A single-stage rocket lifted off from the <a href="https://www.isro.gov.in/" target="_blank">Indian Space Research Organisation’s</a> Satish Dhawan Space Center on Sriharikota island in Andhra Pradesh, reaching an altitude of 6.5 kilometers before splashing down into the ocean.</p><p><span data-rm-shortcode-id="47c210ba720d4009542b5774c43a4067"><iframe type="lazy-iframe" data-runner-src="https://www.youtube.com/embed/3J2Lnck_vgU?rel=0" width="100%" height="auto" frameborder="0" scrolling="no"></iframe></span><small placeholder="Add Photo Caption...">The launch of Agnibaan SOrTeD</small></p><p> “It performed very successfully,” says cofounder and chief operating officer <a href="https://www.linkedin.com/in/moin-spm-53a80342/?originalSubdomain=in" target="_blank">Moin SPM</a>. “It met all the objectives of the mission so we have a lot of confidence in the technologies that we have built.”</p><p> The company’s first commercial product will be a two-stage rocket called <a href="https://agnikul.in/#/products" target="_blank">Agnibaan</a>, which will be 18 meters tall, feature eight engines in total and able to carry a 300-kilogram payload to an altitude of around 700 km. The launch vehicle used in May’s test was only 6 meters tall and featured just a single engine, making it roughly equivalent to Agnibaan’s second stage.</p><p> The launch acted as a technology demonstrator to test out all of the key subsystems necessary for an orbital launch. Those included the flight computer, avionics, guidance, and navigation systems, as well as the launchpad itself, which was purpose-built for the mission.</p><p> The team hit its target of 6 kilonewtons of thrust and was able to successfully carry out a wind-biasing maneuver, in which the rockets trajectory is adjusted midflight to account for the affects of wind. Besides validating the technology, SPM says they gained valuable experience in both manufacturing processes and launch operations.</p><p data-rm-resized-container="25%"><img id="4f373" data-rm-shortcode-id="bc254dae3e83b0307e0f9c44f06365c2" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-yellowish-metallic-3d-printed-rocket-engine-sits-on-a-table.jpg?id=52436307&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/a-yellowish-metallic-3d-printed-rocket-engine-sits-on-a-table.jpg?id=52436307&amp;width=980" width="2000" height="2667" alt="A yellowish metallic 3D printed rocket engine sits on a table."><small placeholder="Add Photo Caption...">This 3D-printed rocket engine was used in Agnikul’s first successful launch.</small><small placeholder="Add Photo Credit...">Agnikul</small></p><p> The launch also vindicated the company’s unconventional manufacturing approach. Constructing a rocket engine using conventional approaches can take months, followed by extensive qualification testing to ensure it meets the required specifications. Using a metal 3D printer from German company <a href="https://www.eos.info/en" rel="noopener noreferrer" target="_blank">EOS</a>, Agnikul produced its engine in roughly three days. Agnikul printed the engine out of <a href="https://en.wikipedia.org/wiki/Inconel" target="_blank">inconel</a>, a high-performance alloy of nickel and chromium that can withstand high temperatures and mechanical loads. The machine also automatically outputs a report that details any deviations during printing, removing the need for postfabrication qualification.</p><p> Assembling the rest of the rocket and integrating the engine took roughly two weeks. The company says that opens the door to providing low-cost, <a href="https://spectrum.ieee.org/3d-printed-rockets-india-agnikul" target="_blank">“on-demand”</a> launch services to <a data-linked-post="2650279771" href="https://spectrum.ieee.org/how-small-satellites-are-providing-lowcost-access-to-space" target="_blank">operators of small satellites</a>, which otherwise need to wait for a ride share on a bigger rocket.</p><p> The big challenge now will be going from a single engine to a cluster of seven on Agnibaan’s first stage, says cofounder and CEO <a href="https://www.linkedin.com/in/srinath-ravichandran-09679a7/?originalSubdomain=in" target="_blank">Srinath Ravichandran</a>. This raises all kinds of challenges, from balancing thrust across the engines at lift-off to managing engine plume interactions when the engines gimbal to alter the trajectory. “But these are problems that people have figured out,” he says. “We believe that we should just be able to fine-tune it for our mission and go.”</p><p> The company is currently building facilities to carry out ground tests of engine clusters, says Ravichandran, and is targeting its first orbital launch for this time next year.</p><p><em>This post was updated in 12 June to add further detail about the material that Agnikul is using to 3D print its rocket engines.</em><br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Chose Profit over Security, Whistleblower Says (449 pts)]]></title>
            <link>https://www.propublica.org/article/microsoft-solarwinds-golden-saml-data-breach-russian-hackers</link>
            <guid>40667976</guid>
            <pubDate>Thu, 13 Jun 2024 10:39:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.propublica.org/article/microsoft-solarwinds-golden-saml-data-breach-russian-hackers">https://www.propublica.org/article/microsoft-solarwinds-golden-saml-data-breach-russian-hackers</a>, See on <a href="https://news.ycombinator.com/item?id=40667976">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pp-location="article body">

        
                    <div data-pp-location="top-note">
                

                                                
            <p>ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive <a href="https://www.propublica.org/newsletters/the-big-story?source=www.propublica.org&amp;placement=top-note&amp;region=national">our biggest stories</a> as soon as they’re published.</p>

                

            </div><!-- end .article-body__top-notes -->
        
        




                    
<p data-pp-blocktype="copy" data-pp-id="2.0">Microsoft hired Andrew Harris for his extraordinary skill in keeping hackers out of the nation’s most sensitive computer networks. In 2016, Harris was hard at work on a mystifying incident in which intruders had somehow penetrated a major U.S. tech company.</p>
        
    
                        


   
            
            
            
            

   
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="4.0">The breach troubled Harris for two reasons. First, it involved the company’s cloud — a virtual storehouse typically containing an organization’s most sensitive data. Second, the attackers had pulled it off in a way that left little trace.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="5.0">He retreated to his home office to “war game” possible scenarios, stress-testing the various software products that could have been compromised.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="6.0">Early on, he focused on a Microsoft application that ensured users had permission to log on to cloud-based programs, the cyber equivalent of an officer checking passports at a border. It was there, after months of research, that he found something seriously wrong.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="7.0">The product, which was used by millions of people to log on to their work computers, contained a flaw that could allow attackers to masquerade as legitimate employees and rummage through victims’ “crown jewels” — national security secrets, corporate intellectual property, embarrassing personal emails — all without tripping alarms.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="8.0">To Harris, who had previously spent nearly seven years working for the Defense Department, it was a security nightmare. Anyone using the software was exposed, regardless of whether they used Microsoft or another cloud provider such as Amazon. But Harris was most concerned about the federal government and the implications of his discovery for national security. He flagged the issue to his colleagues.</p>
        
    
                    
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="10.0">They saw it differently, Harris said. The federal government was preparing to make a massive investment in cloud computing, and Microsoft wanted the business. Acknowledging this security flaw could jeopardize the company’s chances, Harris recalled one product leader telling him. The financial consequences were enormous. Not only could Microsoft lose a multibillion-dollar deal, but it could also lose the race to dominate the market for cloud computing.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="11.0">Harris said he pleaded with the company for several years to address the flaw in the product, a ProPublica investigation has found. But at every turn, Microsoft dismissed his warnings, telling him they would work on a long-term alternative — leaving cloud services around the globe vulnerable to attack in the meantime.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="12.0">Harris was certain someone would figure out how to exploit the weakness. He’d come up with a temporary solution, but it required customers to turn off one of Microsoft’s most convenient and popular features: the ability to access nearly every program used at work with a single logon.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="13.0">He scrambled to alert some of the company’s most sensitive customers about the threat and personally oversaw the fix for the New York Police Department. Frustrated by Microsoft’s inaction, he left the company in August 2020.</p>
        
    
                    

<figure data-pp-id="14" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="1062" height="1660" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1250&amp;q=75&amp;w=800&amp;s=347fe7b1ed331d98ed0ab66e262efc9f" srcset="https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=625&amp;q=75&amp;w=400&amp;s=13665e6a08853763b63f7d6c0b463487 400w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1250&amp;q=75&amp;w=800&amp;s=347fe7b1ed331d98ed0ab66e262efc9f 800w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1876&amp;q=75&amp;w=1200&amp;s=608fc692923d4caef27c6033e1a1fb01 1200w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2032&amp;q=75&amp;w=1300&amp;s=4a7abb4c19014cc31d7f295c1ec39dc6 1300w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2266&amp;q=75&amp;w=1450&amp;s=be7ae31a95f7cd90dde1c4f5019eed7a 1450w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2501&amp;q=75&amp;w=1600&amp;s=4717bb35a0047345972ba442c2a17a16 1600w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=3126&amp;q=75&amp;w=2000&amp;s=e357f50008eb1ca4e3ab3c386e6be69c 2000w">

            
    
<figcaption>
        <span>Andrew Harris shared his Microsoft employee badge on his LinkedIn page when he announced his departure from the company in 2020.</span>
    
        <span>
        <span>Credit: </span>
        Screenshot by ProPublica
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="15.0">Within months, his fears became reality. U.S. officials <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2021/04/15/fact-sheet-imposing-costs-for-harmful-foreign-activities-by-the-russian-government/">confirmed reports</a> that a state-sponsored team of Russian hackers had carried out SolarWinds, one of the <a href="https://www.cbsnews.com/news/solarwinds-hack-russia-cyberattack-60-minutes-2021-07-04/">largest cyberattacks</a> in U.S. history. They used the flaw Harris had identified to vacuum up sensitive data from a number of federal agencies, including, ProPublica has learned, the National Nuclear Security Administration, which maintains the United States’ nuclear weapons stockpile, and the National Institutes of Health, which at the time was engaged in COVID-19 research and vaccine distribution. The Russians also used the weakness to compromise dozens of email accounts in the Treasury Department, <a href="https://www.finance.senate.gov/ranking-members-news/wyden-statement-following-treasury-and-irs-briefing-on-solarwinds-hack">including those of its highest-ranking officials</a>. One federal official <a href="https://www.wsj.com/articles/suspected-russian-hack-extends-far-beyond-solarwinds-software-investigators-say-11611921601">described the breach</a> as “an espionage campaign designed for long-term intelligence collection.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="16.0">Harris’ account, told here for the first time and supported by interviews with former colleagues and associates as well as social media posts, upends the prevailing public understanding of the SolarWinds hack.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="17.0">From the moment the hack surfaced, Microsoft insisted it was blameless. Microsoft President Brad Smith <a href="https://www.intelligence.senate.gov/sites/default/files/documents/qfr-bsmith-022321.pdf">assured Congress in 2021</a> that “there was no vulnerability in any Microsoft product or service that was exploited” in SolarWinds.</p>
        
    
                    
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="19.0">He also said customers could have done more to protect themselves.</p>

<p data-pp-blocktype="copy" data-pp-id="19.1">Harris said they were never given the chance.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="20.0">“The decisions are not based on what’s best for Microsoft’s customers but on what’s best for Microsoft,” said Harris, who now works for CrowdStrike, a cybersecurity company that competes with Microsoft.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="21.0">Microsoft declined to make Smith and other top officials available for interviews for this story, but it did not dispute ProPublica’s findings. Instead, <a href="https://www.documentcloud.org/documents/24742779-microsoft-statement">the company issued a statement</a> in response to written questions. “Protecting customers is always our highest priority,” a spokesperson said. “Our security response team takes all security issues seriously and gives every case due diligence with a thorough manual assessment, as well as cross-confirming with engineering and security partners. Our assessment of this issue received multiple reviews and was aligned with the industry consensus.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="22.0">ProPublica’s investigation comes as the Pentagon <a href="https://www.axios.com/2024/05/17/pentagon-weighs-microsoft-licensing-upgrades">seeks to expand its use of Microsoft products</a> — a move that has <a href="https://regmedia.co.uk/2024/06/04/schmitt_wyden_dod_microsoft_e5.pdf">drawn scrutiny</a> from federal lawmakers amid a series of cyberattacks on the government.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="23.0">Smith is set to testify on Thursday before <a href="https://homeland.house.gov/hearing/a-cascade-of-security-failures-assessing-microsoft-corporations-cybersecurity-shortfalls-and-the-implications-for-homeland-security/">the House Homeland Security Committee</a>, which is examining Microsoft’s role in a breach perpetrated last year by hackers connected to the Chinese government. Attackers exploited Microsoft security flaws to gain access to top U.S. officials’ emails. In investigating the attack, the federal Cyber Safety Review Board <a href="https://www.cisa.gov/resources-tools/resources/cyber-safety-review-board-releases-report-microsoft-online-exchange-incident-summer-2023">found</a> that Microsoft’s “security culture was inadequate and requires an overhaul.”</p>
        
    
                    

<figure data-pp-id="24" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="2000" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=b41b841faa27cff1464b1cfd66364a3e" srcset="https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=3811d27f029a74164e3d3e0c2f625981 400w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=b41b841faa27cff1464b1cfd66364a3e 800w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=1200&amp;s=e5cd9cea67d8764d227517f60b9847bb 1200w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=75&amp;w=1300&amp;s=3e347f880737055ad52213d7b100c50f 1300w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=75&amp;w=1450&amp;s=27667a7dffc08daacff5e6a637c545d2 1450w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=75&amp;w=1600&amp;s=15edc589b0616e03a3f1c1b8059afef0 1600w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1333&amp;q=75&amp;w=2000&amp;s=91ad733f8e4af8242228e80f752459c2 2000w">

            
    
<figcaption>
        <span>Microsoft President Brad Smith testifies during a Senate Select Committee on Intelligence hearing about SolarWinds on Feb. 23, 2021.</span>
    
        <span>
        <span>Credit: </span>
        Drew Angerer/Getty Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="25.0">For its part, Microsoft has said that work has already begun, declaring that <a href="https://www.microsoft.com/en-us/security/blog/2024/05/03/security-above-all-else-expanding-microsofts-secure-future-initiative/">the company’s top priority is security</a> “above all else.” Part of the effort involves adopting the board’s recommendations. “If you’re faced with the tradeoff between security and another priority, your answer is clear: Do security,” the company’s CEO, Satya Nadella, told employees in the wake of the board’s report, which identified a “corporate culture that deprioritized both enterprise security investments and rigorous risk management.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="26.0">ProPublica’s investigation adds new details and pivotal context about that culture, offering an unsettling look into how the world’s largest software provider handles the security of its own ubiquitous products. It also offers crucial insight into just how much the quest for profits can drive those security decisions, especially as tech behemoths push to dominate the newest — and most lucrative — frontiers, including the cloud market.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="27.0">“This is part of the problem overall with the industry,” said Nick DiCola, who was one of Harris’ bosses at Microsoft and now works at Zero Networks, a network security firm. Publicly-traded tech giants “are beholden to the share price, not to doing what’s right for the customer all the time. That’s just a reality of capitalism. You’re never going to change that in a public company because at the end of the day, they want the shareholder value to go up.”</p>
        
    
                    
<h3>A “Cloud-First World”</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="29.0">Early this year, Microsoft surpassed Apple to become the world’s most valuable company, worth more than $3 trillion. That triumph was almost unimaginable a decade ago. (The two remain in close competition for the top spot.)</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="30.0">In 2014, the same year that Harris joined Microsoft and Nadella became the CEO, Wall Street and consumers alike viewed the company as stuck in the past, clinging to the “shrink-wrapped” software products like Windows that put it on the map in the 1990s. Microsoft’s long-stagnant share price reflected its status as an also-ran in almost every major technological <a href="https://www.ft.com/content/0e8c3002-20c7-11ea-92da-f0c92e957a96">breakthrough</a> since the turn of the century, from its Bing search engine to its Nokia mobile phone division.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="31.0">As the new CEO, Nadella was determined to reverse the trend and shake off the company’s fuddy-duddy reputation, so he staked Microsoft’s future on the Azure cloud computing division, which then lagged far behind Amazon. In his earliest all-staff memo, Nadella told employees they would need “to reimagine a lot of what we have done in the past for a … <a href="https://www.geekwire.com/2014/satya-nadellas-full-memo-microsoft-staffers-need-believe-impossible-remove-improbable/">cloud-first world</a>.”</p>
        
    
                    

<figure data-pp-id="32" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="1997" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=d8df1bc01ab70929d30d241bdff377d3" srcset="https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=266&amp;q=75&amp;w=400&amp;s=85562fb93a8753372f39ad6c335e8c40 400w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=d8df1bc01ab70929d30d241bdff377d3 800w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=799&amp;q=75&amp;w=1200&amp;s=40b86ef673bf6a63e36a045f37898806 1200w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=865&amp;q=75&amp;w=1300&amp;s=58e80d6a692ba0c35fe6879ae29d79f0 1300w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=965&amp;q=75&amp;w=1450&amp;s=49d44d2efaaf8deaa7234a7a49bf70d1 1450w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1065&amp;q=75&amp;w=1600&amp;s=228def8f190e1e6d44f8c79ff573c55a 1600w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1331&amp;q=75&amp;w=2000&amp;s=f9f70caa8beb96a842373c5ce6b3f522 2000w">

            
    
<figcaption>
        <span>Microsoft CEO Satya Nadella promotes the company’s cloud offerings at an event in San Francisco in 2014.</span>
    
        <span>
        <span>Credit: </span>
        David Paul Morris/Bloomberg via Getty Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="33.0">Microsoft salespeople pitched business and government customers on a “hybrid cloud” strategy, where they kept some traditional, on-premises servers (typically stored on racks in customers’ own offices) while shifting most of their computing needs to the cloud (hosted on servers in Microsoft data centers).</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="34.0">Security was a key selling point for the cloud. On-site servers were notoriously vulnerable, in part because organizations’ overburdened IT staff often failed to promptly install the required patches and updates. With the cloud, that crucial work was handled by dedicated employees whose job was security.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="35.0">The dawn of the cloud era at Microsoft was an exciting time to work in the field of cybersecurity for someone like Harris, whose high school yearbook features a photo of him in front of a desktop computer and monitor with a mess of floppy disks beside him. One hand is on the keyboard, the other on a wired mouse. Caption: “Harris the hacker.”</p>
        
    
                    

<figure data-pp-id="36" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="1715" height="1011" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=472&amp;q=75&amp;w=800&amp;s=f74d5e3a8c3df8f87d54eee093b709e6" srcset="https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=236&amp;q=75&amp;w=400&amp;s=858cdea91df24d12544b7b0476eb3519 400w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=472&amp;q=75&amp;w=800&amp;s=f74d5e3a8c3df8f87d54eee093b709e6 800w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=707&amp;q=75&amp;w=1200&amp;s=c6aa317725f25d3ee09272c1bfa3f920 1200w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=766&amp;q=75&amp;w=1300&amp;s=4dedb158322d3a2c9e23aea6a36e0e1e 1300w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=855&amp;q=75&amp;w=1450&amp;s=47c3dca69779e067df2386c499b2ca47 1450w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=943&amp;q=75&amp;w=1600&amp;s=2872c3e68e6a0b62b0cff9d53f1cf972 1600w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1179&amp;q=75&amp;w=2000&amp;s=f13ff7f6e4eb9f59788bc70335c6d081 2000w">

            
    
<figcaption>
        <span>Harris’ high school yearbook</span>
    
        <span>
        <span>Credit: </span>
        Classmates.com
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="37.0">As a sophomore at Pace University in New York, he wrote a white paper titled “How to Hack the Wired Equivalent Protocol,” a network security standard, and was awarded a prestigious Defense Department scholarship, which the government uses to recruit cybersecurity specialists. The National Security Agency paid for three years of his tuition, which included a master’s degree in software engineering, in exchange for a commitment to work for the government for at least that long, he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="38.0">Early in his career, he helped lead the Defense Department’s efforts to protect individual devices. He became an expert in the niche field known as identity and access management, securing how people log in.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="39.0">As the years wore on, he grew frustrated by the lumbering bureaucracy and craved the innovation of the tech industry. He decided he could make a bigger impact in the private sector, which designed much of the software the government used.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="40.0">At Microsoft he was assigned to a secretive unit known as the “Ghostbusters” (as in: “Who you gonna call?”), which responded to hacks of the company’s most sensitive customers, especially the federal government. As a member of this team, Harris first investigated the puzzling attack on the tech company and remained obsessed with it, even after switching roles inside Microsoft.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="41.0">Eventually, he confirmed the weakness within Active Directory Federation Services, or AD FS, a product that allowed users to sign on a single time to access nearly everything they needed. The problem, he discovered, rested in how the application used a computer language known as SAML to authenticate users as they logged in.</p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        
    
    
    
    
    
    
    
    
            
    
<figcaption>
    
        <span>
        <span>Credit: </span>
        Illustrations by Anuj Shrestha, special to ProPublica
    </span>
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="43.0">This is what makes a SAML attack unique. Typically, hackers leave what cybersecurity specialists call a “noisy” digital trail. Network administrators monitoring the so-called “audit logs” might see unknown or foreign IP addresses attempting to gain access to their cloud services. But SAML attacks are much harder to detect. The forged token is the equivalent of a robber using a copied master key. There was little trail to track, just the activities of what appear to be legitimate users.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="44.0">Harris and a colleague who consulted for the Department of Defense spent hours in front of both real and virtual whiteboards as they mapped out how such an attack would work, the colleague told ProPublica. The “token theft” risk, as Harris referred to it, became a regular topic of discussion for them.</p>
        
    
                    
<h3>A Clash With “Won’t Fix” Culture</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="46.0">Before long, Harris alerted his supervisors about his SAML finding. Nick DiCola, his boss at the time, told ProPublica he referred Harris to the Microsoft Security Response Center, which fields reports of security vulnerabilities and determines which need to be addressed. Given its central role in improving Microsoft product security, the team once considered itself the “conscience of the company,” urging colleagues to improve security without regard to profit. In a meeting room, someone hung a framed photo of Winston “the Wolf,” the charismatic fixer in Quentin Tarantino’s movie “Pulp Fiction” who is summoned to clean up the aftermath of bloody hits.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="47.0">Members of the team were not always popular within the company. Plugging security holes is a cost center, and making new products is a profit center, former employees told ProPublica. In 2002, the company’s founder, Bill Gates, tried to settle the issue, <a href="https://news.microsoft.com/2012/01/11/memo-from-bill-gates/">sending a memo</a> that turned out to be eerily prescient. “Flaws in a single Microsoft product, service or policy not only affect the quality of our platform and services overall, but also our customers’ view of us as a company,” Gates wrote, adding: “So now, when we face a choice between adding features and resolving security issues, we need to choose security.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="48.0">At first, Gates’ memo was transformational and the company’s product divisions were more responsive to the center’s concerns. But over time, the center’s influence waned.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="49.0">Its members were stuck between cultural forces. Security researchers — often characterized as having outsized egos — believed their findings should be immediately addressed, underestimating the business challenges of developing fixes quickly, former MSRC employees told ProPublica.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="50.0">Product managers had little motivation to act fast, if at all, since compensation was tied to the release of new, revenue-generating products and features. That attitude was particularly pronounced in Azure product groups, former MSRC members said, because they were under pressure from Nadella to catch up to Amazon.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="51.0">“Azure was the Wild West, just this constant race for features and functionality,” said Nate Warfield, who worked in the MSRC for four years beginning in 2016. “You will get a promotion because you released the next new shiny thing in Azure. You are not going to get a promotion because you fixed a bunch of security bugs.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="52.0">Former employees told ProPublica that the center fielded hundreds or even thousands of reports a month, pushing the perennially understaffed group to its limits. The magazine Popular Science noted that volume as one of the reasons why working in the MSRC was one of the 10 “<a href="https://www.popsci.com/scitech/article/2007-06/worst-jobs-science-2007/">worst jobs in science</a>,” between whale feces researchers and elephant vasectomists.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="53.0">“They’re trained, because they’re so resource constrained, to think of these cases in terms of: ‘How can I get to ‘won’t fix,’” said Dustin Childs, who worked in the MSRC in the years leading up to Harris’ saga. Staff would often punt on fixes by telling researchers they would be handled in “v-next,” the next product version, he said. Those launches, however, could be years away, leaving customers vulnerable in the interim, he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="54.0">The center also routinely rejected researchers’ reports of weaknesses by saying they didn’t cross what its staff called a “security boundary.” But when Harris discovered the SAML flaw, it was a term with no formal definition, former employees said.</p>
        
    
                    

<figure data-pp-id="55" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="1637" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=437&amp;q=75&amp;w=800&amp;s=e1250b7677bcfa79735d052583702e66" srcset="https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=218&amp;q=75&amp;w=400&amp;s=24f90fa50676e5077a9fe0460294e2de 400w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=437&amp;q=75&amp;w=800&amp;s=e1250b7677bcfa79735d052583702e66 800w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=655&amp;q=75&amp;w=1200&amp;s=4598a08e1928e9d76906095d397bf1b9 1200w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=709&amp;q=75&amp;w=1300&amp;s=ca666a9a63d69b25cca507c3bea41cd0 1300w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=791&amp;q=75&amp;w=1450&amp;s=af3826e4d57abff88dde1d6c3451fee3 1450w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=873&amp;q=75&amp;w=1600&amp;s=d59e2adf79d220b993b65e0b31e30a5e 1600w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1091&amp;q=75&amp;w=2000&amp;s=dfe15a09ba1c967b2640fd705603557a 2000w">

            
    
<figcaption>
    
        <span>
        <span>Credit: </span>
        Jaap Arriens / Sipa USA via AP Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="56.0">By 2017, the lack of clarity had become the “butt of jokes,” Warfield said. Several prominent security researchers who regularly interacted with the MSRC made T-shirts and stickers that said “____ [fill in the blank] is not a security boundary.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="57.0">“Any time Microsoft didn’t want to fix something, they’d just say, ‘That’s not a security boundary, we’re not going to fix it,’” Warfield recalled.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="58.0">Unaware of the inauspicious climate, Harris met virtually with MSRC representatives and sketched out how a hacker could jump from an on-premises server to the cloud without being detected. The MSRC declined to address the problem. Its staff argued that hackers attempting to exploit the SAML flaw would first have to gain access to an on-premises server. As they saw it, Harris said, that was the security boundary — not the subsequent hop to the cloud.</p>
        
    
                    
<h3>Business Over Security</h3>
<p data-pp-blocktype="copy" data-pp-id="59.0">“WTF,” Harris recalled thinking when he got the news. “This makes no sense.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="60.0">Microsoft had told customers the cloud was the safest place to put their most precious data. His discovery proved that, for the millions of users whose systems included AD FS, their cloud was only as secure as their on-premises servers. In other words, all the buildings owned by the landlord are only as secure as the most careless tenant who forgot to lock their window.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="61.0">Harris pushed back, but he said the MSRC held firm.</p>

<p data-pp-blocktype="copy" data-pp-id="61.1">Harris had a reputation for going outside the chain of command to air his concerns, and he took his case to the team managing the products that verified user identities.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="62.0">He had some clout, his former colleagues said. He had already established himself as a known expert in the field, had pioneered a cybersecurity threat detection method and later was listed as the named inventor on a <a href="https://ppubs.uspto.gov/dirsearch-public/print/downloadPdf/10977364">Microsoft patent</a>. Harris said he “went kind of crazy” and fired off an email to product manager Mark Morowczynski and director Alex Simons requesting a meeting.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="63.0">He understood that developing a long-term fix would take time, but he had an interim solution that could eliminate the threat. One of the main practical functions of AD FS was to allow users to access both on-premises servers and a variety of cloud-based services after entering credentials only once, a Microsoft feature known as “seamless” single sign-on. Harris proposed that Microsoft tell its customers to turn off that function so the SAML weakness would no longer matter.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="64.0">According to Harris, Morowczynski quickly jumped on a videoconference and said he had discussed the concerns with Simons.</p>

<p data-pp-blocktype="copy" data-pp-id="64.1">“Everyone violently agreed with me that this is a huge issue,” Harris said. “Everyone violently disagreed with me that we should move quickly to fix it.”</p>

<p data-pp-blocktype="copy" data-pp-id="64.2">Morowczynski, Harris said, had two primary objections.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="65.0">First, a public acknowledgement of the SAML flaw would alert adversaries who could then exploit it. Harris waved off the concern, believing it was a risk worth taking so that customers wouldn’t be ignorant to the threat. Plus, he believed Microsoft could warn customers without betraying any specifics that could be co-opted by hackers.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="66.0">According to Harris, Morowczynski’s second objection revolved around the business fallout for Microsoft. Harris said Morowczynski told him that his proposed fix could alienate one of Microsoft’s largest and most important customers: the federal government, which used AD FS. Disabling seamless SSO would have widespread and unique consequences for government employees, who relied on physical “smart cards” to log onto their devices. Required by federal rules, the cards generated random passwords each time employees signed on. Due to the configuration of the underlying technology, though, removing seamless SSO would mean users could not access the cloud through their smart cards. To access services or data on the cloud, they would have to sign in a second time and would not be able to use the mandated smart cards.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="67.0">Harris said Morowczynski rejected his idea, saying it wasn’t a viable option.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="68.0">Morowczynski told Harris that his approach could also undermine the company’s chances of getting one of the largest government computing contracts in U.S. history, which would be formally announced the next year. Internally, Nadella had made clear that Microsoft needed a piece of this multibillion-dollar deal with the Pentagon if it wanted to have a future in selling cloud services, Harris and other former employees said.</p>
        
    
                    
<h3>Killing the Competition</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="70.0">By Harris’ account, the team was also concerned about the potential business impact on the products sold by Microsoft to sign into the cloud. At the time, Microsoft was in a fierce rivalry with a company called Okta.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="71.0">Microsoft customers had been sold on seamless SSO, which was one of the competitive advantages — or, in Microsoft parlance, “kill points” — that the company then had over Okta, whose users had to sign on twice, Harris said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="72.0">Harris’ proposed fix would undermine the company’s strategy to marginalize Okta and would “add friction” to the user experience, whereas the “No. 1 priority was to remove friction,” Harris recalled Morowczynski telling him. Moreover, it would have cascading consequences for the cloud business because the sale of identity products often led to demand for other cloud services.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="73.0">“That little speed bump of you authenticating twice was unacceptable by Microsoft’s standards,” Harris said. He recalled Morowczynski telling him that the product group’s call “was a business decision, not a technical one.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="74.0">“What they were telling me was counterintuitive to everything I’d heard at Microsoft about ‘customer first,’” Harris said. “Now they’re telling me it’s not ‘customer first,’ it’s actually ‘business first.’”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="75.0">DiCola, Harris’ then-supervisor, told ProPublica the race to dominate the market for new and high-growth areas like the cloud drove the decisions of Microsoft’s product teams. “That is always like, ‘Do whatever it frickin’ takes to win because you have to win.’ Because if you don’t win, it’s much harder to win it back in the future. Customers tend to buy that product forever.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="76.0">According to Harris, Morowczynski said his team had “on the road map” a product that could replace AD FS altogether. But it was unclear when it would be available to customers.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="77.0">In the months that followed, Harris vented to his colleagues about the product group’s decision. ProPublica talked to three people who worked with Harris at the time and recalled these conversations. All of them spoke on the condition of anonymity because they feared professional repercussions. The three said Harris was enraged and frustrated over what he described to them as the product group’s unwillingness to address the weakness.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="78.0">Neither Morowczynski nor Simons returned calls seeking comment, and Microsoft declined to make them available for interviews. The company did not dispute the details of Harris’ account. In its statement, Microsoft said it weighs a number of factors when it evaluates potential threats. “We prioritize our security response work by considering potential customer disruption, exploitability, and available mitigations,” the spokesperson said. “We continue to listen to the security research community and <a href="https://www.microsoft.com/en-us/security/blog/2024/05/03/security-above-all-else-expanding-microsofts-secure-future-initiative/">evolve our approach</a> to ensure we are meeting customer expectations and protecting them from emerging threats.”</p>
        
    
                    
<h3>Another Major Warning</h3>
<p data-pp-blocktype="copy" data-pp-id="79.0">Following the conversation with Morowczynski, Harris wrote a reminder to himself on the whiteboard in his home office: “SAML follow-up.” He wanted to keep the pressure on the product team.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="80.0">Soon after, the Massachusetts- and Tel Aviv-based cybersecurity firm <a href="https://www.cyberark.com/resources/threat-research-blog/golden-saml-newly-discovered-attack-technique-forges-authentication-to-cloud-apps">CyberArk published a blog post describing the flaw</a>, which it dubbed “Golden SAML,” along with a proof of concept, essentially a road map that showed how hackers could exploit the weakness.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="81.0">Years later, in his <a href="https://www.intelligence.senate.gov/sites/default/files/documents/qfr-bsmith-022321.pdf">written testimony for the Senate Intelligence Committee</a>, Microsoft’s Brad Smith said this was the moment the company learned of the issue. “The Golden SAML theory became known to cybersecurity professionals at Microsoft and across the U.S. government and the tech sector at precisely the same time, when it was published in a public paper in 2017,” Smith wrote.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="82.0">Lavi Lazarovitz of CyberArk said the firm mentioned the weakness — before the post was published — in a private WhatsApp chat of about 10 security researchers from various companies, a forum members used to compare notes on emerging threats. When they raised the discovery to the group, which included at least one researcher from Microsoft, the other members were dismissive, Lazarovitz said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="83.0">“Many in the security research community — I don’t want to say mocked — but asked, ‘Well, what’s the big deal?’” Lazarovitz said.</p>
        
    
                    

<figure data-pp-id="84" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="2000" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=6554df025044772e330fefd7612e6969" srcset="https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=f7c287d181d54fc9688e70499e246e18 400w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=6554df025044772e330fefd7612e6969 800w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=1200&amp;s=26f73a96a20f0737a92950b706260fdf 1200w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=75&amp;w=1300&amp;s=8489cc46a4128fb95721017d773ba5fd 1300w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=75&amp;w=1450&amp;s=faaa1d9ae88c8a0276f92ea8a9d72283 1450w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=75&amp;w=1600&amp;s=4072888de9057865c3dee56dac3fdf8a 1600w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1333&amp;q=75&amp;w=2000&amp;s=4791dc9e93c2a88ee2ed12aa4c53251f 2000w">

            
    
<figcaption>
        <span>The CyberArk headquarters in Newton, Massachusetts</span>
    
        <span>
        <span>Credit: </span>
        Sipa via AP Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="85.0">Nevertheless, CyberArk believed it was worth taking seriously, given that AD FS represented the gateway to users’ most sensitive information, including email. “Threat actors operate in between the cracks,” Lazarovitz said. “So obviously, we understood the feedback that we got, but we still believed that this technique will be eventually leveled by threat actors.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="86.0">The Israel-based team also reached out to contacts at Microsoft’s Israeli headquarters and were met with a response similar to the one they got in the WhatsApp group, Lazarovitz said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="87.0">The published report was CyberArk’s way of warning the public about the threat. Disclosing the weakness also had a business benefit for the company. In the blog post, it pitched its own security product, which it said “will be extremely beneficial in blocking attackers from getting their hands on important assets like the token-signing certificate in the first place.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="88.0">The report initially received little attention. Harris, however, seized on it. He said he alerted Morowczynski and Simons from the product group as well as the MSRC. The situation was more urgent than before, Harris argued to them, because CyberArk included the proof of concept that could be used by hackers to carry out a real attack. For Harris, it harkened back to Morowczynski’s worry that flagging the weakness could give hackers an advantage.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="89.0">“I was more energetic than ever to have us actually finally figure out what we’re going to do about this,” Harris said.</p>

<p data-pp-blocktype="copy" data-pp-id="89.1">But the MSRC reiterated its “security boundary” stance, while Morowczynski reaffirmed the product group’s earlier decision, Harris said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="90.0">Harris said he then returned to his supervisors, including Hayden Hainsworth and Bharat Shah, who, as corporate vice president of the Azure cloud security division, also oversaw the MSRC. “I said, ‘Can you guys please listen to me,’” Harris recalled. “‘This is probably the most important thing I’ve ever done in my career.’”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="91.0">Harris said they were unmoved and told him to take the problem back to the MSRC.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="92.0">Microsoft did not publicly comment on the CyberArk blog post at the time. Years later, <a href="https://www.intelligence.senate.gov/sites/default/files/documents/qfr-bsmith-022321.pdf">in written responses to Congress</a>, Smith said the company’s security researchers reviewed the information but decided to focus on other priorities. Neither Hainsworth nor Shah returned calls seeking comment.</p>
        
    
                    
<h3>Defusing a Ticking Bomb</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="94.0">Harris said he was deeply frustrated. On a personal level, his ego was bruised. Identifying major weaknesses is considered an achievement for cybersecurity professionals, and, despite his internal discovery, CyberArk had claimed Golden SAML.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="95.0">More broadly, he said he was more worried than ever, believing the weakness was a ticking bomb. “It’s out in the open now,” he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="96.0">Publicly, Microsoft <a href="https://web.archive.org/web/20171209163607/https://azure.microsoft.com/en-us/overview/azure-vs-aws/">continued to promote the safety of its products</a>, even boasting of its relationship with the federal government in sales pitches. “To protect your organization, Azure embeds security, privacy, and compliance into its development methodology,” the company said in late 2017, “and has been recognized as the most trusted cloud for U.S. government institutions.”</p>
        
    
                    

<figure data-pp-id="97" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="2000" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=e7b0960b96cf5d1f02a7d97b4d4d7150" srcset="https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=da9833ef4061a5b33a701f937077b35e 400w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=e7b0960b96cf5d1f02a7d97b4d4d7150 800w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=1200&amp;s=86e71ed8b005c7a41832a34997e71ea5 1200w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=75&amp;w=1300&amp;s=b40c716a2b5e1502d08f0effb23531d1 1300w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=75&amp;w=1450&amp;s=0fd89fec85bb8daffaa4fd06d14991e9 1450w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=75&amp;w=1600&amp;s=850fe4a2d34050d14add5728eba9fb8f 1600w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1333&amp;q=75&amp;w=2000&amp;s=e53244a56feec4da9e6906ddbf703f39 2000w">

            
    
<figcaption>
        <span>Attendees walk through the exhibition floor during the Microsoft Developers Build Conference in Seattle in 2017.</span>
    
        <span>
        <span>Credit: </span>
        David Ryder/Bloomberg via Getty Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="98.0">Internally, Harris complained to colleagues that customers were being left vulnerable.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="99.0">“He was definitely having issues” with the product team, said Harris’ former Microsoft colleague who consulted for the Defense Department. “He vented that it was a problem that they just wanted to ignore.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="100.0">Harris typically pivoted from venting to discussing how to protect customers, the former colleague said. “I asked him to show me what I’m going to have to do to make sure the customers were aware and could take corrective action to mitigate the risk,” he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="101.0"><a href="https://www.linkedin.com/posts/andrewfharris_yet-another-post-on-securing-identity-with-activity-6503732899240562688-nq9w?utm_source=share&amp;utm_medium=member_desktop">Harris also took his message to LinkedIn</a>, where he posted a discreet warning and an offer.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="102.0">“I hope all my friends and followers on here realize by now the security relationship” involved in authenticating users in AD FS, he wrote in 2019. “If not, reach out and let’s fix that!”</p>
        
    
                    

<figure data-pp-id="103" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="1060" height="566" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=427&amp;q=75&amp;w=800&amp;s=c171e77bebc48982146233d7336a8552" srcset="https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=214&amp;q=75&amp;w=400&amp;s=1ed769d36f0a5f020bc3619bb05f065f 400w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=427&amp;q=75&amp;w=800&amp;s=c171e77bebc48982146233d7336a8552 800w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=641&amp;q=75&amp;w=1200&amp;s=301e17912a83203fcd57e7c36dd39416 1200w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=694&amp;q=75&amp;w=1300&amp;s=cb6d9e12442df09a7101b15144813099 1300w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=774&amp;q=75&amp;w=1450&amp;s=046a3796f2c79272c6b848424730863b 1450w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=854&amp;q=75&amp;w=1600&amp;s=3a6b4877c7cd53fc8faa602f7f5b15af 1600w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1068&amp;q=75&amp;w=2000&amp;s=5ed8a7d266b111c2091c6485e7c40466 2000w">

            
    
<figcaption>
        <span>In 2019, Harris posted a discreet warning and an offer on LinkedIn.</span>
    
        <span>
        <span>Credit: </span>
        Screenshot by ProPublica
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="104.0">Separately, he realized he could help customers with whom he had existing relationships, including the NYPD, the nation’s largest police force.</p>

<p data-pp-blocktype="copy" data-pp-id="104.1">“Knowing this exploit is actually possible, why would I not architect around it, especially for my critical customers?” Harris said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="105.0">On a visit to the NYPD, Harris told a top IT official, Matthew Fraser, about the AD FS weakness and recommended disabling seamless SSO. Fraser was in disbelief at the severity of the issue, Harris recalled, and he agreed to disable seamless SSO.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="106.0">In an interview, Fraser confirmed the meeting. </p>

<p data-pp-blocktype="copy" data-pp-id="106.1">“This was identified as one of those areas that was prime, ripe,” Fraser said of the SAML weakness. “From there, we figured out what’s the best path to insulate and secure.”</p>

<h3>More Troubling Revelations</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="107.0">It was over beers at a conference in Orlando in 2018 that Harris learned the weakness was even worse than he’d initially realized. A colleague sketched out on a napkin how hackers could also bypass a common security feature called multifactor authentication, which requires users to perform one or more additional steps to verify their identity, such as entering a code sent via text message.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="108.0">They realized that, no matter how many additional security steps a company puts in place, a hacker with a forged token can bypass them all. When they brought the new information to the MSRC, “it was a nonstarter,” Harris said. While the center <a href="https://www.microsoft.com/en-us/msrc/windows-security-servicing-criteria">had published a formal definition</a> of “security boundary” by that point, Harris’ issues still didn’t meet it.</p>
        
    
                    

<figure data-pp-id="109" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="1770" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=472&amp;q=75&amp;w=800&amp;s=bae33542f0f9dc9e24f39f8c4143746c" srcset="https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=236&amp;q=75&amp;w=400&amp;s=728df062b6f46fe3bdb6a2ec768ed81d 400w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=472&amp;q=75&amp;w=800&amp;s=bae33542f0f9dc9e24f39f8c4143746c 800w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=708&amp;q=75&amp;w=1200&amp;s=97aa587005667b0e0a14ca5178999fb4 1200w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=767&amp;q=75&amp;w=1300&amp;s=f98a131d6b40e168689911d526e338c8 1300w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=856&amp;q=75&amp;w=1450&amp;s=827158c7c104ba7f81f13efb3a40f5d5 1450w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=944&amp;q=75&amp;w=1600&amp;s=0553a5c2aaa629243b4de868086c6942 1600w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1180&amp;q=75&amp;w=2000&amp;s=2ba1bc91982436ba78472e98d0c8f77b 2000w">

            
    
<figcaption>
        <span>Nadella delivers the keynote address at a 2018 conference in Seattle for software developers.</span>
    
        <span>
        <span>Credit: </span>
        Elaine Thompson/AP
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="110.0">By March 2019, concerns over Golden SAML were spilling out into the wider tech world. That month, <a href="https://troopers.de/troopers19/agenda/fpxwmn/">at a conference in Germany</a>, two researchers from the cybersecurity company Mandiant delivered a presentation demonstrating how hackers could infiltrate AD FS to gain access to organizations’ cloud accounts and applications. They also released the tools they used to do so.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="111.0">Mandiant said it notified Microsoft before the presentation, making it the second time in roughly 16 months that an outside firm had flagged the SAML issue to the company.</p>

<p data-pp-blocktype="copy" data-pp-id="111.1">In August 2020, Harris left Microsoft to work for CrowdStrike. In his exit interview with Shah, Harris said he raised the SAML weakness one last time. Shah listened but offered no feedback, he said.</p>

<p data-pp-blocktype="copy" data-pp-id="111.2">“There is no inspector general-type thing” within Microsoft, Harris said. “If something egregious is happening, where the hell do you go? There’s no place to go.”</p>

<h3>SolarWinds Breaks</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="112.0">Four months later, news of the SolarWinds attack broke. Federal officials soon announced that beginning in 2019 Russian hackers had breached and exploited the network management software offered by a Texas-based company called SolarWinds, which had the misfortune of lending its name to the attack. The hackers covertly inserted malware into the firm’s software updates, gaining “backdoor” access to the networks of companies and government agencies that installed them. The ongoing access allowed hackers to take advantage of “post-exploit” vulnerabilities, including Golden SAML, to steal sensitive data and emails from the cloud.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="113.0">Despite the name, <a href="https://www.wsj.com/articles/suspected-russian-hack-extends-far-beyond-solarwinds-software-investigators-say-11611921601">nearly a third</a> of victims of the attack never used SolarWinds software at all, Brandon Wales, then acting director of the federal Cybersecurity and Infrastructure Security Agency, said in the aftermath. In March 2021, Wales told a Senate panel that hackers were able to “gain broad access to data stores that they wanted, largely in Microsoft Office 365 Cloud … and it was all because they compromised those systems that manage trust and identity on networks.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="114.0">Microsoft itself was also breached.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="115.0">In the immediate aftermath of the attack, <a href="https://techcommunity.microsoft.com/t5/microsoft-entra-blog/protecting-microsoft-365-from-on-premises-attacks/ba-p/1751754">Microsoft advised customers</a> of Microsoft 365 to disable seamless SSO in AD FS and similar products — the solution that Harris proposed three years earlier.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="116.0">As the world dealt with the consequences, Harris took his long simmering frustration <a href="http://x.com/ciberesponce/status/1339885673646612480?s=46&amp;t=chjNjKsX2TeMLEHigHtMaA">public in a series</a> of <a href="https://twitter.com/ciberesponce/status/1363483257136947204?s=46&amp;t=chjNjKsX2TeMLEHigHtMaA">posts</a> on social media and on his <a href="https://www.ciberesponce.com/learning-identity-cyber-attacks/">personal blog</a>. <a href="https://x.com/ciberesponce/status/1364336404122263554?s=46&amp;t=chjNjKsX2TeMLEHigHtMaA">Challenging Brad Smith</a> by name, and criticizing the MSRC’s decisions — which he referred to as “utter BS” — Harris lambasted Microsoft for failing to publicly warn customers about Golden SAML.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="117.0">Microsoft “was not transparent about these risks, forced customers to use ADFS knowing these risks, and put many customers and especially US Gov’t in a bad place,” Harris <a href="https://www.linkedin.com/posts/andrewfharris_ive-been-outspoken-a-bit-on-how-bypassing-activity-6745561216145588224-R4mj?utm_source=share&amp;utm_medium=member_desktop">wrote on LinkedIn in December 2020</a>. A long-term fix was “never a priority” for the company, he wrote. “Customers are boned and sadly it’s been that way for years (which again, sickens me),” Harris said in the post.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="118.0">In the months and years following the SolarWinds attack, Microsoft took a number of actions to mitigate the SAML risk. One of them was a way to efficiently detect fallout from such a hack. The advancement, however, was available only as part of a paid add-on product known as Sentinel.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="119.0">The lack of such a detection, <a href="https://techcommunity.microsoft.com/t5/microsoft-sentinel-blog/non-interactive-logins-minimizing-the-blind-spot/ba-p/2287932">the company said in a blog post</a>, had been a “blind spot.”</p>
        
    
                    
<h3>“Microsoft Is Back on Top”</h3>
<p data-pp-blocktype="copy" data-pp-id="120.0">In early 2021, the Senate Select Committee on Intelligence called Brad Smith to testify about SolarWinds.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="121.0">Although Microsoft’s product had played a central role in the attack, Smith seemed unflappable, his easy and conversational tone a reflection of the relationships he had spent decades building on Capitol Hill. Without referencing notes or reading from a script, as some of his counterparts did, he confidently deflected questions about Microsoft’s role. Laying the responsibility with the government, he said that in the lead-up to the attack, the authentication flaw “was not prioritized by the intelligence community as a risk, nor was it flagged by civilian agencies or other entities in the security community as a risk that should be elevated” over other cybersecurity priorities.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="122.0">Smith also downplayed the significance of the Golden SAML weakness, <a href="https://www.intelligence.senate.gov/sites/default/files/documents/os-bsmith-022321.pdf">saying it was used in just 15% of the 60 cases</a> that Microsoft had identified by that point. At the same time, he acknowledged that, “without question, these are not the only victims who had data observed or taken.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="123.0">When Sen. Marco Rubio of Florida <a href="https://www.intelligence.senate.gov/hearings/open-hearing-hearing-hack-us-networks-foreign-adversary">pointedly asked him</a> what Microsoft had done to address Golden SAML in the years before the attack, Smith responded by listing a handful of steps that customers could have taken to protect themselves. His suggestions included purchasing an antivirus product like Microsoft Defender and securing devices with another Microsoft product called Intune.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="124.0">“The reality is any organization that did all five of those things, if it was breached, it in all likelihood suffered almost no damage,” Smith said.</p>

<p data-pp-blocktype="copy" data-pp-id="124.1">Neither Rubio nor any other senator pressed further.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="125.0">Ultimately, Microsoft <a href="https://www.defense.gov/News/News-Stories/Article/Article/3243483/department-names-vendors-to-provide-joint-warfighting-cloud-capability/">won a piece</a> of the Defense Department’s multibillion-dollar cloud business, sharing it with Amazon, Google and Oracle.</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="127.0">Since December 2020, when the SolarWinds attack was made public, Microsoft’s stock has soared 106%, largely on the runaway success of Azure and artificial intelligence products like ChatGPT, where the company is the largest investor. “Microsoft Is Back on Top,” proclaimed Fortune, which featured Nadella on the cover of its most recent issue.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="128.0">In September 2021, just 10 months after the discovery of SolarWinds, the paperback edition of Smith’s book, “Tools and Weapons,” was published. In it, Smith praised Microsoft’s response to the attack. The MSRC, Smith wrote, “quickly activated its incident response plan” and the company at large “mobilized more than 500 employees to work full time on every aspect of the attack.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="129.0">In the new edition, Smith also reflected on his congressional testimony on SolarWinds. The hearings, he wrote, “examined not only what had happened but also what steps needed to be taken to prevent such attacks in the future.” He didn’t mention it in the book, but that certainly would include the long-term alternative that Morowczynski first promised to Harris in 2017. The company began offering it <a href="https://redmondmag.com/articles/2022/02/14/azure-active-directory-certificate-based-authentication-preview.aspx">in 2022</a>.</p>
        
    
                    <div data-pp-location="bottom-note">
                        <p>Development by <a href="https://www.propublica.org/people/lucas-waldron">Lucas Waldron</a>.</p>

        </div><!-- end .article-body__bottom-notes -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Potential ozone depletion from satellite demise during atmospheric reentry (133 pts)]]></title>
            <link>https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024GL109280</link>
            <guid>40667786</guid>
            <pubDate>Thu, 13 Jun 2024 10:06:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024GL109280">https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024GL109280</a>, See on <a href="https://news.ycombinator.com/item?id=40667786">Hacker News</a></p>
Couldn't get https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024GL109280: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Arm says it wants all Snapdragon X Elite laptops destroyed (185 pts)]]></title>
            <link>https://www.xda-developers.com/arm-says-it-wants-all-snapdragon-x-elite-laptops-destroyed/</link>
            <guid>40667606</guid>
            <pubDate>Thu, 13 Jun 2024 09:33:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.xda-developers.com/arm-says-it-wants-all-snapdragon-x-elite-laptops-destroyed/">https://www.xda-developers.com/arm-says-it-wants-all-snapdragon-x-elite-laptops-destroyed/</a>, See on <a href="https://news.ycombinator.com/item?id=40667606">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                       <div>
                    


        

                        <nav>
                <ul>
                    <li><a href="https://www.xda-developers.com/">Home</a></li>
                                                                                            <li><a href="https://www.xda-developers.com/processor/">CPU</a></li>
                                                                                                                                                            </ul>
            </nav>
            
                </div>
                            


    
    
            
    
    
            
    
    
    
        
    
                            






            
            
    
    
    
    
            

    
    
            
    
    
            
    
    
    
        
    
                                
    <p>The legal battle between Arm and Qualcomm continues</p>

            
            
    
    
    
    
            

    
    
            
    
    
            
    
    
    
        
    
                                
                                    
                                                                                                                        
                                                        <div data-img-url="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="&quot;&quot;">

        
    



<figure>
        <picture>
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
            
            <source media="(min-width: 1024px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=1140&amp;h=&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=1140&amp;h=&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
            
            <source media="(min-width: 768px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=943&amp;h=&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=943&amp;h=&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
            
            <source media="(min-width: 481px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=767&amp;h=&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=767&amp;h=&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
            
            <source media="(min-width: 0px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=480&amp;h=&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=480&amp;h=&amp;dpr=2">
                <img width="3000" height="2000" alt="Snapdragon X Elite chip in front of a reference design laptop powered by the chip" data-img-url="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg" src="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg">
    </picture>
                                    
    </figure>


    </div>

    
            <!-- Not injecting Ads due to No-Ads mode. -->
    
            
            
    
    
    
    
            

    
    
        </div><div id="article-body" itemprop="articleBody">

<div id="custom_block_0">

                    <h3>Key Takeaways</h3>
        
                    <div>    <ul>
                    <li>
                                        Arm is trying to eliminate Qualcomm from Windows market, so it can introduce its own Cortex design.
                        </li>
                    <li>
                                        Rumors suggest that Nvidia, MediaTek, and AMD may enter the Windows ecosystem soon with Arm chips.
                        </li>
                    <li>
                                        Arm claims Qualcomm doesn't have a license for custom Arm chips, creating a legal battle between the two companies.
                        </li>
            </ul>
</div>
        
        
    </div><!-- Not injecting Ads due to No-Ads mode. -->
<p>            We're less than a week away from <a href="https://www.xda-developers.com/snapdragon-x/">Qualcomm's Snapdragon X-series</a> laptops hitting shelves, and it's kind of a big deal. There's been an elephant in the room since Apple started using Arm chips for Macs, and this is seen as the launch that will bring Windows to parity with that.
    </p>    
<p>            But there's another elephant in the room, at least depending on your perspective, which is that Arm Holdings and Qualcomm have been locked in a legal battle for some time over these very chips.
    </p>            
    
                    
                            
                
    
                                        
    
        
        
    <div>

        
                    			<a href="https://www.xda-developers.com/microsoft-copilot-plus/">
		<div data-img-url="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="&quot;&quot;">

            




<figure>
        <picture>
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
                                        
            <source media="(min-width: 1024px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=440&amp;h=280&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=440&amp;h=280&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
                                        
            <source media="(min-width: 768px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=310&amp;h=240&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=310&amp;h=240&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
                                        
            <source media="(min-width: 481px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=800&amp;h=520&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=800&amp;h=520&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
                                        
            <source media="(min-width: 0px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=480&amp;h=320&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=480&amp;h=320&amp;dpr=2">
                <img width="6000" height="4000" loading="lazy" decoding="async" alt="Satya Nadella Copilot (6)" data-img-url="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG" src="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG">
    </picture>
                
    </figure>


        </div>

		</a>
	
        
                            
                
        
        
            </div>
<!-- Not injecting Ads due to No-Ads mode. --><h2 id="what-does-arm-want">
                        What does Arm want?
               </h2><h3 id="it-39-s-very-anti-arm">
            It's very anti-Arm
    </h3>




    



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
    

<p>            "Arm's claim against Qualcomm and Nuvia is about protecting the Arm ecosystem and partners who rely on our IP and innovative designs, and therefore enforcing Qualcomm's contractual obligation to destroy and stop using the Nuvia designs that were derived from Arm technology," an Arm spokesperson said in a statement to <a href="https://www.usnews.com/news/technology/articles/2024-06-10/analysis-arm-qualcomm-legal-battle-seen-disrupting-ai-powered-pc-wave" rel="noopener noreferrer nofollow" target="_blank">Reuters</a>.
    </p>    
<p>            And there it is. Arm wants all Snapdragon X series chips destroyed. Or does it?
    </p>    
<p>            Firstly, let's be clear about what the company is calling for here. Qualcomm has laid the foundation for Windows on Arm. It's built out the ecosystem since the platform was announced at the end of 2016. Native apps like Chrome, Slack, and a whole bunch of others wouldn't exist if it weren't for <a href="https://www.xda-developers.com/best-windows-on-arm/">Snapdragon PCs</a>. What Arm is calling for is for Qualcomm to be eliminated from the market so that it can swoop in with its own Cortex designs.
    </p>    
<p>            And according to rumors, as well as what I'm hearing from my own sources, there are more Arm vendors coming to the Windows ecosystem. Rumor has it that Nvidia, MediaTek, and AMD all have their eyes on it, and a competitor could be entering the space as soon as CES 2025.
    </p>    
<p>            It's likely that a company like MediaTek would use Arm's own Cortex cores, which would be more lucrative for Arm, of course. It also allows the company to maintain a competitive advantage in a world where both Apple and Qualcomm have both moved on from Cortex designs because Arm wasn't offering what they needed.
    </p>    <h2 id="it-39-s-all-about-licensing">
                        It's all about licensing
               </h2><h3 id="does-qualcomm-have-the-rights-to-make-its-own-arm-chips-no-one-knows">
            Does Qualcomm have the rights to make its own Arm chips? No one knows
    </h3>




    



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
    

<p>            Basically, Arm says that Qualcomm doesn't have a license to make custom Arm chips, and Qualcomm is saying that it does. There are two kinds of Arm licenses. One is for using Cortex cores, which are designed by Arm, essentially as an off-the-shelf part. The other kind of license lets you build whatever you want, as long as it follows the Arm instruction set.
    </p>    
<p>            Apple and Qualcomm are doing the latter.
    </p>    
<p>            Arm has always been pretty public about the fact that it prefers companies to use Cortex cores. Custom chips mean companies like Qualcomm can make a better Arm chip than Arm, and it can't make the old Apple excuse anymore, which was always that it was about optimizing for the full stack from hardware to software.
    </p>    
<p>            Qualcomm bought a company called Nuvia to do this, and it originally used Nuvia's architectural license. Arm's argument is that the Nuvia license was canceled when it was taken over by Qualcomm. A new deal would have to be negotiated if that holds up in court.
    </p>    
<p>            So, there you have it. Arm wants Qualcomm to stop shipping the product it's been contesting, but to be honest, that's not usually how these things end. It's unlikely that any product will be delayed from hitting shelves. These cases tend to end with one company giving a bucket of cash to the other, and everyone moves on.
    </p>    
<p>            Snapdragon X PCs are slated to launch on June 18, with a total of 14 products from seven OEMs, including Microsoft, HP, Dell, Lenovo, Samsung, Asus, and even Qualcomm. Acer also has a product ready to go in mid-July.
    </p>    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's MI300X Outperforms Nvidia's H100 for LLM Inference (266 pts)]]></title>
            <link>https://www.blog.tensorwave.com/amds-mi300x-outperforms-nvidias-h100-for-llm-inference/</link>
            <guid>40667102</guid>
            <pubDate>Thu, 13 Jun 2024 07:57:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.blog.tensorwave.com/amds-mi300x-outperforms-nvidias-h100-for-llm-inference/">https://www.blog.tensorwave.com/amds-mi300x-outperforms-nvidias-h100-for-llm-inference/</a>, See on <a href="https://news.ycombinator.com/item?id=40667102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>There has been much anticipation around AMD’s flagship MI300X accelerator. With unmatched raw <a href="https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html">specs</a>, the pressing question remains: Can it outperform NVIDIA’s Hopper architecture in real-world AI workloads? We have some exciting early results to share.</p>



<p>For the past month, <strong><a href="https://tensorwave.com/">TensorWave</a> and <a href="https://mk1.ai/">MK1</a></strong> have worked closely to unlock performance of AMD hardware for AI inference. To start, we focused on Mixture of Expert (MoE) architectures due to their compute efficiency and popularity – notably used by Mistral, Meta, Databricks, and X.ai for their most powerful open-source LLMs.</p>



<p>The initial results are impressive: using MK1’s inference software, the MI300X <strong>achieves 33% higher throughput compared to the H100 SXM</strong> running vLLM on Mixtral 8x7B for a real-world chat use case. Despite NVIDIA’s software ecosystem being more mature, it is clear that AMD is already a formidable competitor in the AI market. When hardware availability and cost are factored in, the MI300X proves to be an attractive option for enterprises running large-scale inference in the cloud. </p>



<blockquote>
<p>We expect AMD’s performance advantage to climb even higher after further optimization, so stay tuned for more updates!</p>
<cite>-Darrick Horton, CEO TensorWave</cite></blockquote>



<div>
<p><strong>Subscribe to the TensorWave Newsletter</strong></p>




</div>



<p>We invite you to experience the MI300X firsthand at TensorWave, which comes prepackaged with MK1’s inference software. <a href="mailto:alex@tensorwave.com">C</a><a href="mailto:contact@tensorwave.com">ontact us</a> to find out more.</p>



<h2 id="h-inference-benchmarks">Inference Benchmarks</h2>



<p>We conducted extensive offline and online inference tests comparing the MI300X and H100 SXM5 accelerators using the Mixtral 8x7B model.</p>



<ul>
<li><strong>Offline Tests:</strong> These are standardized and provide insights into the performance of the forward pass across different setups.</li>
</ul>



<ul>
<li><strong>Online Tests:</strong> These are more sophisticated and estimate system performance in a real-world setting where multiple users are serviced asynchronously.</li>
</ul>



<p><strong><span>Benchmark Setup</span></strong></p>



<p><strong>AMD</strong></p>



<ul>
<li><strong>Hardware:</strong> TensorWave node equipped with 8 MI300X accelerators, 2 AMD EPYC CPU Processors (192 cores), and 2.3 TB of DDR5 RAM.</li>



<li><strong>MI300X Accelerator: </strong>192GB VRAM, 5.3 TB/s, ~1300 TFLOPS for FP16</li>



<li><strong>Drivers: </strong>ROCm 6.1.2</li>



<li><strong>Inference Stack: </strong>MK1’s inference engine (Flywheel) v0.9.2 and AMD’s ROCm optimized fork of vLLM (rocm/vllm) v0.4.0.</li>



<li><strong>Configuration: </strong>Tensor parallelism set to 1 (tp=1), since we can fit the entire model Mixtral 8x7B in a single MI300X’s 192GB of VRAM.</li>
</ul>



<p><strong>NVIDIA</strong></p>



<ul>
<li><strong>Hardware:</strong> Baremetal node with 8 H100 SXM5 accelerators with NVLink, 160 CPU cores, and 1.2 TB of DDR5 RAM.</li>



<li><strong>H100 SXM5 Accelerator: </strong>80GB VRAM, 3.35 TB/s, ~986 TFLOPS for FP16</li>



<li><strong>Drivers: </strong>CUDA 12.2</li>



<li><strong>Inference Stack: </strong>vLLM v4.3<strong>&nbsp;</strong></li>



<li><strong>Configuration: </strong>Tensor parallelism set to 2 (tp=2), which is required to fit Mixtral 8x7B in two H100’s 80GB VRAM.</li>
</ul>



<p><strong><em>Notes</em></strong></p>



<ul>
<li>All benchmarks are performed using the Mixtral 8x7B model.</li>



<li>All inference frameworks are configured to use FP16 compute paths. Enabling FP8 compute is left for future work.</li>



<li>To make an accurate comparison between the systems with different settings of tensor parallelism, we extrapolate throughput for the MI300X by 2.</li>
</ul>



<h2 id="h-offline-results">Offline Results</h2>



<p>To measure peak throughput for each inference solution, we generate prompts of a fixed size and directly feed them to the model. This method, known as offline batching, enhances hardware efficiency by processing multiple prompts simultaneously. Although larger batch sizes boost throughput, they also increase latency due to more requests being processed. Following standard practice, we constrain requests in a batch to have equal input sizes, and to have equal output sizes.</p>



<p>We assess the throughput of each system by varying the batch size. This was done using a modified version of `benchmark_throughput.py` script in the vLLM repository, refactored to include Flywheel as a backend. Prompts were also randomly generated within a batch to remove caching mechanisms. The performance metrics, detailed in the table below, measure throughput as a function of batch size.</p>



<figure><img decoding="async" width="1024" height="576" src="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4-1024x576.png" alt="" srcset="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4-1024x576.png 1024w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4-300x169.png 300w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4-768x432.png 768w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4-1536x864.png 1536w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Notably, our results show that <strong>MI300X running MK1 Flywheel outperforms H100 running vLLM for every batch size</strong>, with an increase in performance ranging from 1.22x to 2.94x.</p>



<h2 id="h-online-results-for-chat-data-distribution">Online Results for Chat Data Distribution</h2>



<p>Moving beyond offline metrics, we designed a series of online benchmarks to simulate a realistic typical chat application. This involves generating responses to user inputs that closely mirror actual usage patterns.</p>



<p>Specifically, we simulate chat traffic by spawning independent workers to send requests to an endpoint. We then sweep the number of workers to increase the number of concurrent requests.</p>



<p>In these experiments, requests were generated using a standard text chat distribution with an average of 573 input tokens and 50 output tokens. Note that our benchmarking tool supports arbitrary data distributions; please reach out if you have a specific use case you’d like to test.</p>



<p>The key metrics of interest are:</p>



<ul>
<li><strong>Throughput (Requests per Second)</strong>: The number of requests the system can handle per second for a given workload.</li>



<li><strong>Average Latency (Seconds)</strong>: The average time taken to generate a full response for each request.</li>



<li><strong>Time Per Output Token (TPOT)</strong>: The time to generate each subsequent token (averaged) after the first token, which impacts the overall speed of generating long responses.</li>
</ul>



<p>For the first benchmark, we tested a non-streaming use case where throughput and latency are measured for servicing the <em>full response</em>.</p>



<figure><img decoding="async" width="1024" height="576" src="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference-1024x576.png" alt="" srcset="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference-1024x576.png 1024w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference-300x169.png 300w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference-768x432.png 768w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference-1536x864.png 1536w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>At a target average latency of 5 seconds, <strong>two MI300X with tp=1 services 33% more requests per second than two H100s with tp=2</strong>. This means you can service the same number of users with similar quality of service using fewer accelerators!</p>



<p>For the second benchmark, we enable streaming and measure throughput and TPOT for individual tokens as they are streamed out.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput-1024x576.png" alt="" srcset="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput-1024x576.png 1024w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput-300x169.png 300w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput-768x432.png 768w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput-1536x864.png 1536w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Here we observe that the MI300X has higher throughput for every TPOT compared to the H100. This means the <strong>MI300X can generate text faster at higher traffic volumes</strong>, which is crucial for any LLM application.</p>



<h2 id="h-conclusion">Conclusion</h2>



<p>Our benchmarks demonstrate that AMD's MI300X outperforms NVIDIA's H100 in both offline and online inference tasks for MoE architectures like Mixtral 8x7B. The MI300X not only offers higher throughput but also excels in real-world scenarios requiring fast response times. </p>



<p>Given its impressive performance, competitive cost, and hardware availability, the MI300X with MK1 software is an excellent choice for enterprises looking to scale their AI inference capabilities. We encourage you to explore the capabilities of MI300X at TensorWave and experience these benefits first-hand. Contact us to learn more and schedule a test drive of this powerful accelerator!</p>







<figure><a href="https://tensorwave.com/book-a-call?utm_source=twblog&amp;utm_content=unlocking_inference"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad-1024x576.png" alt="" srcset="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad-1024x576.png 1024w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad-300x169.png 300w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad-768x432.png 768w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad-1536x864.png 1536w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad.png 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uncensor any LLM with abliteration (460 pts)]]></title>
            <link>https://huggingface.co/blog/mlabonne/abliteration</link>
            <guid>40665721</guid>
            <pubDate>Thu, 13 Jun 2024 03:42:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/blog/mlabonne/abliteration">https://huggingface.co/blog/mlabonne/abliteration</a>, See on <a href="https://news.ycombinator.com/item?id=40665721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p><a href="https://huggingface.co/blog">
						Back to Articles</a></p>

				
				
				
				<div data-target="BlogAuthorsByline" data-props="{&quot;authors&quot;:[{&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg&quot;,&quot;fullname&quot;:&quot;Maxime Labonne&quot;,&quot;name&quot;:&quot;mlabonne&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false}}],&quot;translators&quot;:[],&quot;proofreaders&quot;:[],&quot;lang&quot;:&quot;en&quot;}">

<p><span><span><a href="https://huggingface.co/mlabonne"><img alt="Maxime Labonne's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg">
					</a>
			</span>

	</span></p></div>
				

				<!-- HTML_TAG_START -->
<div><nav aria-label="Secondary"><ul><li><a href="#✂️-what-is-abliteration" title="✂️ What is abliteration?"><!-- HTML_TAG_START -->✂️ What is abliteration?<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#💻-implementation" title="💻 Implementation"><!-- HTML_TAG_START -->💻 Implementation<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#⚖️-dpo-fine-tuning" title="⚖️ DPO Fine-Tuning"><!-- HTML_TAG_START -->⚖️ DPO Fine-Tuning<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#conclusion" title="Conclusion"><!-- HTML_TAG_START -->Conclusion<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#references" title="References"><!-- HTML_TAG_START -->References<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li></ul></nav></div><p><a rel="nofollow" href="https://i.imgur.com/KhorYYG.png"><img alt="KhorYYG.png" src="https://i.imgur.com/KhorYYG.png"></a></p>
<p>The third generation of Llama models provided fine-tunes (Instruct) versions that excel in understanding and following instructions. However, these models are heavily censored, designed to refuse requests seen as harmful with responses such as "As an AI assistant, I cannot help you." While this safety feature is crucial for preventing misuse, it limits the model's flexibility and responsiveness.</p>
<p>In this article, we will explore a technique called "abliteration" that can uncensor any LLM without retraining. This technique effectively removes the model's built-in refusal mechanism, allowing it to respond to all types of prompts.</p>
<p>The code is available on&nbsp;<a rel="nofollow" href="https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing">Google Colab</a>&nbsp;and in the&nbsp;<a rel="nofollow" href="https://github.com/mlabonne/llm-course">LLM Course</a>&nbsp;on GitHub.</p>
<h2>
	<a rel="nofollow" href="#✂️-what-is-abliteration" id="✂️-what-is-abliteration">
		<span></span>
	</a>
	<span>
		✂️ What is abliteration?
	</span>
</h2>
<p>Modern LLMs are fine-tuned for safety and instruction-following, meaning they are trained to refuse harmful requests. In their <a rel="nofollow" href="https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction">blog post</a>, Arditi et al. have shown that this refusal behavior is mediated by a specific direction in the model's residual stream. If we prevent the model from representing this direction, it <strong>loses its ability to refuse requests</strong>. Conversely, adding this direction artificially can cause the model to refuse even harmless requests.</p>
<p>In the traditional decoder-only Llama-like architecture, there are three residual streams we can target: at the start of each block ("pre"), between the attention and MLP layers ("mid"), and after the MLP ("post"). The following figure illustrates the location of each residual stream.</p>
<p><a rel="nofollow" href="https://i.imgur.com/hsdR9e7.png"><img alt="" src="https://i.imgur.com/hsdR9e7.png"></a></p>
<p>To uncensor an LLM, we first need to identify the "refusal direction" within the model. This process involves a few technical steps:</p>
<ol>
<li><strong>Data Collection</strong>: Run the model on a set of harmful instructions and a set of harmless instructions, recording the residual stream activations at the last token position for each.</li>
<li><strong>Mean difference</strong>: Calculate the mean difference between the activations of harmful and harmless instructions. This gives us a vector representing the "refusal direction" for each layer of the model.</li>
<li><strong>Selection</strong>: Normalize these vectors and evaluate them to select the single best "refusal direction."</li>
</ol>
<p>Once we have identified the refusal direction, we can "ablate" it, effectively removing the model's ability to represent this feature. This can be done through an <strong>inference-time intervention</strong> or permanently with <strong>weight orthogonalization</strong>.</p>
<p>Let's talk about inference-time intervention first. For every component that writes to the residual stream (such as an attention head), we calculate the projection of its output onto the refusal direction and subtract this projection. This subtraction is applied at every token and every layer, ensuring that the model never represents the refusal direction.</p>
<p>On the other hand, weight orthogonalization involves modifying the model weights directly. By orthogonalizing the component weights with respect to the refusal direction, it prevents the model from writing to this direction altogether. This is achieved by adjusting the matrices that write to the residual stream, ensuring they do not contribute to the refusal direction.</p>
<p>In the next section, we will implement abliteration with weight orthogonalization.</p>
<h2>
	<a rel="nofollow" href="#💻-implementation" id="💻-implementation">
		<span></span>
	</a>
	<span>
		💻 Implementation
	</span>
</h2>
<p>The following implementation of abliteration is based on <a href="https://huggingface.co/failspy/llama-3-70B-Instruct-abliterated/blob/main/ortho_cookbook.ipynb">FailSpy's notebook</a>, which is itself based on the original authors' <a rel="nofollow" href="https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw?usp=sharing">notebook</a>. I mostly adapted and simplified it to make it easier to understand. This section is quite code-heavy so you can see what is going on, but you can use FailSpy's <a rel="nofollow" href="https://github.com/FailSpy/abliterator">abliterator library</a> if you're less interested in the technical details (also check his <a href="https://huggingface.co/collections/failspy/abliterated-v3-664a8ad0db255eefa7d0012b">collection of abliterated models</a> on Hugging Face).</p>
<p>The code relies on the excellent <a rel="nofollow" href="https://github.com/TransformerLensOrg/TransformerLens">TransformerLens</a> library (formerly known as EasyTransformer) to do the heavy lifting. It is designed for mechanistic interpretability and is used here to intervene on activations. Thanks to Neel Nanda and Joseph Bloom for creating and maintaining this library.</p>
<p>First, let's install the necessary packages and import them. All these steps are available in this <a rel="nofollow" href="https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing">Google Colab notebook</a>.</p>
<pre><code>!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping

<span>import</span> torch
<span>import</span> functools
<span>import</span> einops
<span>import</span> gc

<span>from</span> datasets <span>import</span> load_dataset
<span>from</span> tqdm <span>import</span> tqdm
<span>from</span> torch <span>import</span> Tensor
<span>from</span> typing <span>import</span> <span>List</span>
<span>from</span> transformer_lens <span>import</span> HookedTransformer, utils
<span>from</span> transformer_lens.hook_points <span>import</span> HookPoint
<span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer
<span>from</span> jaxtyping <span>import</span> Float, Int
<span>from</span> collections <span>import</span> defaultdict

<span># Turn automatic differentiation off to save GPU memory (credit: Undi95)</span>
torch.set_grad_enabled(<span>False</span>)
</code></pre>
<p>We need two datasets: one containing harmless instructions, and one containing harmful instructions. We'll use <a href="https://huggingface.co/datasets/tatsu-lab/alpaca">tatsu-lab/alpaca</a> as well as data from <a rel="nofollow" href="https://github.com/llm-attacks/llm-attacks">llm-attacks</a>. To make things easier, I repackaged them in two Hugging Face datasets: <a href="https://huggingface.co/datasets/harmless_behaviors">mlabonne/harmless_behaviors</a> and <a href="https://huggingface.co/datasets/mlabonne/harmful_behaviors">mlabonne/harmful_behaviors</a>. That way, you can easily replace them with your own datasets.</p>
<p>We will load the instructions and reformat them into a list of dictionaries with "role" and "content" keys. This makes it compatible with the <code>apply_chat_tokenizer()</code> method, which we will use to follow Llama 3's chat template.</p>
<pre><code><span>def</span> <span>reformat_texts</span>(<span>texts</span>):
    <span>return</span> [[{<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: text}] <span>for</span> text <span>in</span> texts]

<span># Get harmful and harmless datasets</span>
<span>def</span> <span>get_harmful_instructions</span>():
    dataset = load_dataset(<span>'mlabonne/harmful_behaviors'</span>)
    <span>return</span> reformat_texts(dataset[<span>'train'</span>][<span>'text'</span>]), reformat_texts(dataset[<span>'test'</span>][<span>'text'</span>])

<span>def</span> <span>get_harmless_instructions</span>():
    dataset = load_dataset(<span>'mlabonne/harmless_alpaca'</span>)
    <span>return</span> reformat_texts(dataset[<span>'train'</span>][<span>'text'</span>]), reformat_texts(dataset[<span>'test'</span>][<span>'text'</span>])

harmful_inst_train, harmful_inst_test = get_harmful_instructions()
harmless_inst_train, harmless_inst_test = get_harmless_instructions()
</code></pre>
<p>Now that we have our datasets, we can load the model we want to abliterate. Unfortunately, you can't directly load a custom model using <code>HookedTransformer</code>. Here, I use a trick described in FailSpy's notebook to download a custom model and rename it as <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">meta-llama/Meta-Llama-3-8B-Instruct</a>. Load in <code>torch.float16</code> format if your GPU is not compatible with BF16.</p>
<p>In this example, we'll use <a href="https://huggingface.co/mlabonne/Daredevil-8B">mlabonne/Daredevil-8B</a>, a mega-merge created with DARE TIES (see my article about <a href="https://huggingface.co/blog/mlabonne/merge-models">model merging</a>) that has the highest MMLU score on the Open LLM Leaderboard in the 8B category.</p>
<pre><code>MODEL_ID = <span>"mlabonne/Daredevil-8B"</span>
MODEL_TYPE = <span>"meta-llama/Meta-Llama-3-8B-Instruct"</span>

<span># Download and load model</span>
!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}

<span># Load model and tokenizer</span>
model = HookedTransformer.from_pretrained_no_processing(
    MODEL_TYPE,
    local_files_only=<span>True</span>,
    dtype=torch.bfloat16,
    default_padding_side=<span>'left'</span>
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)
tokenizer.padding_side = <span>'left'</span>
tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<p>We can now tokenize our datasets. We're using the same number of samples for both harmless and harmful instructions. Note that a high number of samples can use all the RAM/VRAM, which is why I'm limiting it to 256 here.</p>
<pre><code><span>def</span> <span>tokenize_instructions</span>(<span>tokenizer, instructions</span>):
    <span>return</span> tokenizer.apply_chat_template(
        instructions,
        padding=<span>True</span>,
        truncation=<span>False</span>,
        return_tensors=<span>"pt"</span>,
        return_dict=<span>True</span>,
        add_generation_prompt=<span>True</span>,
    ).input_ids

n_inst_train = <span>min</span>(<span>256</span>, <span>len</span>(harmful_inst_train), <span>len</span>(harmless_inst_train))

<span># Tokenize datasets</span>
harmful_tokens = tokenize_instructions(
    tokenizer,
    instructions=harmful_inst_train[:n_inst_train],
)
harmless_tokens = tokenize_instructions(
    tokenizer,
    instructions=harmless_inst_train[:n_inst_train],
)
</code></pre>
<p>Everything is set up, we can now implement the first step of abliteration: data collection. We want to process these tokenized datasets and store the residual stream activations in <code>harmful</code> and <code>harmless</code>. This is managed by the <a rel="nofollow" href="https://github.com/TransformerLensOrg/TransformerLens">transformer_lens</a> library.</p>
<pre><code><span># Define batch size based on available VRAM</span>
batch_size = <span>32</span>

<span># Initialize defaultdicts to store activations</span>
harmful = defaultdict(<span>list</span>)
harmless = defaultdict(<span>list</span>)

<span># Process the training data in batches</span>
num_batches = (n_inst_train + batch_size - <span>1</span>) // batch_size
<span>for</span> i <span>in</span> tqdm(<span>range</span>(num_batches)):
    <span>print</span>(i)
    start_idx = i * batch_size
    end_idx = <span>min</span>(n_inst_train, start_idx + batch_size)

    <span># Run models on harmful and harmless prompts, cache activations</span>
    harmful_logits, harmful_cache = model.run_with_cache(
        harmful_tokens[start_idx:end_idx],
        names_filter=<span>lambda</span> hook_name: <span>'resid'</span> <span>in</span> hook_name,
        device=<span>'cpu'</span>,
        reset_hooks_end=<span>True</span>
    )
    harmless_logits, harmless_cache = model.run_with_cache(
        harmless_tokens[start_idx:end_idx],
        names_filter=<span>lambda</span> hook_name: <span>'resid'</span> <span>in</span> hook_name,
        device=<span>'cpu'</span>,
        reset_hooks_end=<span>True</span>
    )

    <span># Collect and store the activations</span>
    <span>for</span> key <span>in</span> harmful_cache:
        harmful[key].append(harmful_cache[key])
        harmless[key].append(harmless_cache[key])

    <span># Flush RAM and VRAM</span>
    <span>del</span> harmful_logits, harmless_logits, harmful_cache, harmless_cache
    gc.collect()
    torch.cuda.empty_cache()

<span># Concatenate the cached activations</span>
harmful = {k: torch.cat(v) <span>for</span> k, v <span>in</span> harmful.items()}
harmless = {k: torch.cat(v) <span>for</span> k, v <span>in</span> harmless.items()}
</code></pre>
<p>We can now compute the refusal direction for each layer. This corresponds to the mean difference between the activations of harmful and harmless instructions, which is then normalized. We sort them in descending order in <code>activation_scored</code>. </p>
<pre><code><span># Helper function to get activation index</span>
<span>def</span> <span>get_act_idx</span>(<span>cache_dict, act_name, layer</span>):
    key = (act_name, layer)
    <span>return</span> cache_dict[utils.get_act_name(*key)]

<span># Compute difference of means between harmful and harmless activations at intermediate layers</span>
activation_layers = [<span>"resid_pre"</span>, <span>"resid_mid"</span>, <span>"resid_post"</span>]
activation_refusals = defaultdict(<span>list</span>)

<span>for</span> layer_num <span>in</span> <span>range</span>(<span>1</span>, model.cfg.n_layers):
    pos = -<span>1</span>  <span># Position index</span>

    <span>for</span> layer <span>in</span> activation_layers:
        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=<span>0</span>)
        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean(
            dim=<span>0</span>
        )

        refusal_dir = harmful_mean_act - harmless_mean_act
        refusal_dir = refusal_dir / refusal_dir.norm()
        activation_refusals[layer].append(refusal_dir)

<span># Get all calculated potential refusal directions, sort them in descending order based on their mean</span>
<span># Use a subset of layers if certain activations are not promising</span>
selected_layers = [<span>"resid_pre"</span>]
activation_scored = <span>sorted</span>(
    [
        activation_refusals[layer][l - <span>1</span>]
        <span>for</span> l <span>in</span> <span>range</span>(<span>1</span>, model.cfg.n_layers)
        <span>for</span> layer <span>in</span> selected_layers
    ],
    key=<span>lambda</span> x: <span>abs</span>(x.mean()),
    reverse=<span>True</span>,
)
</code></pre>
<p>The final step of the process consists of evaluating the refusal directions we calculated. To do this, we're going to apply the refusal direction to each residual stream and each block during inference. In the following snippet, we get generations for four test harmful instructions and 20 blocks (or layers).</p>
<pre><code><span>def</span> <span>_generate_with_hooks</span>(<span></span>
<span>    model: HookedTransformer,</span>
<span>    tokenizer: AutoTokenizer,</span>
<span>    tokens: Int[Tensor, <span>"batch_size seq_len"</span>],</span>
<span>    max_tokens_generated: <span>int</span> = <span>64</span>,</span>
<span>    fwd_hooks=[],</span>
<span></span>) -&gt; <span>List</span>[<span>str</span>]:
    all_tokens = torch.zeros(
        (tokens.shape[<span>0</span>], tokens.shape[<span>1</span>] + max_tokens_generated),
        dtype=torch.long,
        device=tokens.device,
    )
    all_tokens[:, : tokens.shape[<span>1</span>]] = tokens
    <span>for</span> i <span>in</span> <span>range</span>(max_tokens_generated):
        <span>with</span> model.hooks(fwd_hooks=fwd_hooks):
            logits = model(all_tokens[:, : -max_tokens_generated + i])
            next_tokens = logits[:, -<span>1</span>, :].argmax(
                dim=-<span>1</span>
            )  <span># greedy sampling (temperature=0)</span>
            all_tokens[:, -max_tokens_generated + i] = next_tokens
    <span>return</span> tokenizer.batch_decode(
        all_tokens[:, tokens.shape[<span>1</span>] :], skip_special_tokens=<span>True</span>
    )

<span>def</span> <span>get_generations</span>(<span></span>
<span>    model: HookedTransformer,</span>
<span>    tokenizer: AutoTokenizer,</span>
<span>    instructions: <span>List</span>[<span>str</span>],</span>
<span>    fwd_hooks=[],</span>
<span>    max_tokens_generated: <span>int</span> = <span>64</span>,</span>
<span>    batch_size: <span>int</span> = <span>4</span>,</span>
<span></span>) -&gt; <span>List</span>[<span>str</span>]:
    generations = []
    <span>for</span> i <span>in</span> tqdm(<span>range</span>(<span>0</span>, <span>len</span>(instructions), batch_size)):
        tokens = tokenize_instructions(
            tokenizer, instructions=instructions[i : i + batch_size]
        )
        generation = _generate_with_hooks(
            model,
            tokenizer,
            tokens,
            max_tokens_generated=max_tokens_generated,
            fwd_hooks=fwd_hooks,
        )
        generations.extend(generation)
    <span>return</span> generations

<span># Inference-time intervention hook</span>
<span>def</span> <span>direction_ablation_hook</span>(<span></span>
<span>    activation: Float[Tensor, <span>"... d_act"</span>],</span>
<span>    hook: HookPoint,</span>
<span>    direction: Float[Tensor, <span>"d_act"</span>],</span>
<span></span>):
    <span>if</span> activation.device != direction.device:
        direction = direction.to(activation.device)
    proj = (
        einops.einsum(
            activation, direction.view(-<span>1</span>, <span>1</span>), <span>"... d_act, d_act single -&gt; ... single"</span>
        )
        * direction
    )
    <span>return</span> activation - proj

<span># Testing baseline</span>
N_INST_TEST = <span>4</span>
baseline_generations = get_generations(
    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]
)

<span># Evaluating layers defined earlier (needs human evaluation to determine best layer for refusal inhibition)</span>
EVAL_N = <span>20</span>  <span># Evaluate how many of the top N potential directions</span>
evals = []
<span>for</span> refusal_dir <span>in</span> tqdm(activation_scored[:EVAL_N]):
    hook_fn = functools.partial(direction_ablation_hook, direction=refusal_dir)
    fwd_hooks = [
        (utils.get_act_name(act_name, layer), hook_fn)
        <span>for</span> layer <span>in</span> <span>list</span>(<span>range</span>(model.cfg.n_layers))
        <span>for</span> act_name <span>in</span> activation_layers
    ]
    intervention_generations = get_generations(
        model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=fwd_hooks
    )
    evals.append(intervention_generations)
</code></pre>
<p>We stored all the generations in the <code>evals</code> list. We can now print them and manually select the layer (block) that provides an uncensored response for each instruction. I'm automatically excluding responses containing "I cannot" and "I can't" to filter out unwanted answers.</p>
<p>If you can't find a layer that satisfies these requirements, you might want to test other residual streams in the previous <code>selected_layers</code> list, other instructions, additional blocks, etc.</p>
<pre><code><span># Print generations for human evaluation</span>
blacklist = [<span>"I cannot"</span>, <span>"I can't"</span>]
<span>for</span> i <span>in</span> <span>range</span>(N_INST_TEST):
    <span>print</span>(<span>f"\033[1mINSTRUCTION <span>{i}</span>: <span>{harmful_inst_test[i]}</span>"</span>)
    <span>print</span>(<span>f"\nBASELINE COMPLETION:\n<span>{baseline_generations[i]}</span>\033[0m"</span>)
    <span>for</span> layer_candidate <span>in</span> <span>range</span>(EVAL_N):
        <span>if</span> <span>not</span> <span>any</span>(word <span>in</span> evals[layer_candidate][i] <span>for</span> word <span>in</span> blacklist):
            <span>print</span>(<span>f"\n---\n\nLAYER CANDIDATE #<span>{layer_candidate}</span> INTERVENTION COMPLETION:"</span>)
            <span>print</span>(evals[layer_candidate][i])
</code></pre>
<p>In my case, the layer candidate 9 managed to provide uncensored answer for the four instructions. This is the one that we will select for the refusal direction. In the following, we implement weight orthogonalization to modify the weights and prevent the model from creating outputs with this direction. You can verify that the model is successfully uncensored by printing the completions.</p>
<pre><code><span>def</span> <span>get_orthogonalized_matrix</span>(<span></span>
<span>    matrix: Float[Tensor, <span>"... d_model"</span>], vec: Float[Tensor, <span>"d_model"</span>]</span>
<span></span>) -&gt; Float[Tensor, <span>"... d_model"</span>]:
    proj = (
        einops.einsum(
            matrix, vec.view(-<span>1</span>, <span>1</span>), <span>"... d_model, d_model single -&gt; ... single"</span>
        )
        * vec
    )
    <span>return</span> matrix - proj

<span># Select the layer with the highest potential refusal direction</span>
LAYER_CANDIDATE = <span>9</span>
refusal_dir = activation_scored[LAYER_CANDIDATE]

<span># Orthogonalize the model's weights</span>
<span>if</span> refusal_dir.device != model.W_E.device:
    refusal_dir = refusal_dir.to(model.W_E.device)
model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)

<span>for</span> block <span>in</span> tqdm(model.blocks):
    <span>if</span> refusal_dir.device != block.attn.W_O.device:
        refusal_dir = refusal_dir.to(block.attn.W_O.device)
    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)
    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)

<span># Generate text with abliterated model</span>
orthogonalized_generations = get_generations(
    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]
)

<span># Print generations</span>
<span>for</span> i <span>in</span> <span>range</span>(N_INST_TEST):
    <span>if</span> <span>len</span>(baseline_generations) &gt; i:
        <span>print</span>(<span>f"INSTRUCTION <span>{i}</span>: <span>{harmful_inst_test[i]}</span>"</span>)
        <span>print</span>(<span>f"\033[92mBASELINE COMPLETION:\n<span>{baseline_generations[i]}</span>"</span>)
    <span>print</span>(<span>f"\033[91mINTERVENTION COMPLETION:\n<span>{evals[LAYER_CANDIDATE][i]}</span>"</span>)
    <span>print</span>(<span>f"\033[95mORTHOGONALIZED COMPLETION:\n<span>{orthogonalized_generations[i]}</span>\n"</span>)
</code></pre>
<p>We're now ready to use the model. We convert it back to the Hugging Face format and upload it to the HF hub.</p>
<pre><code><span># Convert model back to HF safetensors</span>
hf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16)
lm_model = hf_model.model

state_dict = model.state_dict()
lm_model.embed_tokens.weight = torch.nn.Parameter(state_dict[<span>"embed.W_E"</span>].cpu())

<span>for</span> l <span>in</span> <span>range</span>(model.cfg.n_layers):
    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(
        einops.rearrange(
            state_dict[<span>f"blocks.<span>{l}</span>.attn.W_O"</span>], <span>"n h m-&gt;m (n h)"</span>, n=model.cfg.n_heads
        ).contiguous()
    )
    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(
        torch.transpose(state_dict[<span>f"blocks.<span>{l}</span>.mlp.W_out"</span>], <span>0</span>, <span>1</span>).contiguous()
    )

hf_model.push_to_hub(<span>f"<span>{MODEL_ID}</span>-abliterated"</span>)
<span># hf_model.push_to_hub(f"{MODEL_ID}-abliterated")</span>
</code></pre>
<h2>
	<a rel="nofollow" href="#⚖️-dpo-fine-tuning" id="⚖️-dpo-fine-tuning">
		<span></span>
	</a>
	<span>
		⚖️ DPO Fine-Tuning
	</span>
</h2>
<p>I evaluated the abliterated and source models from the previous section on the Open LLM Leaderboard and on Nous' benchmark suite. Here are the results:</p>
<p><a rel="nofollow" href="https://i.imgur.com/ECCejII.png"><img alt="" src="https://i.imgur.com/ECCejII.png"></a></p>
<p>As you can see, the source model significantly outperforms Llama 3 8B Instruct. However, we observe a performance drop in the ablated version across all benchmarks. The ablation process successfully uncensored it but also degraded the model's quality.</p>
<p>To address this issue, an idea consists of further training our abliterated model to heal it. Like most fine-tuned models, Llama 3 8B Instruct is quite brittle when it comes to supervised fine-tuning. An additional SFT would likely break the model's performance.</p>
<p>Alternatively, preference alignment is quite light and shouldn't lobotomize our abliterated model. DPO is a good candidate here for its ease of use and good track record. To implement it, I used <a rel="nofollow" href="https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing">LazyAxolotl</a> with the <a href="https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k">mlabonne/orpo-dpo-mix-40k</a> dataset. Here's the configuration I used:</p>
<pre><code><span>base_model:</span> <span>mlabonne/Daredevil-8B-abliterated</span>
<span>model_type:</span> <span>LlamaForCausalLM</span>
<span>tokenizer_type:</span> <span>AutoTokenizer</span>

<span>load_in_8bit:</span> <span>false</span>
<span>load_in_4bit:</span> <span>true</span>
<span>strict:</span> <span>false</span>
<span>save_safetensors:</span> <span>true</span>

<span>rl:</span> <span>dpo</span>
<span>chat_template:</span> <span>chatml</span>
<span>datasets:</span>
  <span>-</span> <span>path:</span> <span>mlabonne/orpo-dpo-mix-40k-flat</span>
    <span>split:</span> <span>train</span>
    <span>type:</span> <span>chatml.intel</span>

<span>dataset_prepared_path:</span>
<span>val_set_size:</span> <span>0.0</span>
<span>output_dir:</span> <span>./out</span>

<span>adapter:</span> <span>qlora</span>
<span>lora_model_dir:</span>

<span>sequence_len:</span> <span>2048</span>
<span>sample_packing:</span> <span>false</span>
<span>pad_to_sequence_len:</span> <span>false</span>

<span>lora_r:</span> <span>64</span>
<span>lora_alpha:</span> <span>32</span>
<span>lora_dropout:</span> <span>0.05</span>
<span>lora_target_linear:</span> <span>true</span>
<span>lora_fan_in_fan_out:</span>

<span>wandb_project:</span> <span>axolotl</span>
<span>wandb_entity:</span>
<span>wandb_watch:</span>
<span>wandb_name:</span>
<span>wandb_log_model:</span>

<span>gradient_accumulation_steps:</span> <span>8</span>
<span>micro_batch_size:</span> <span>1</span>
<span>num_epochs:</span> <span>1</span>
<span>optimizer:</span> <span>paged_adamw_8bit</span>
<span>lr_scheduler:</span> <span>cosine</span>
<span>learning_rate:</span> <span>5e-6</span>
<span>train_on_inputs:</span> <span>false</span>
<span>group_by_length:</span> <span>false</span>

<span>bf16:</span> <span>auto</span>
<span>fp16:</span>
<span>tf32:</span>

<span>gradient_checkpointing:</span> <span>true</span>
<span>early_stopping_patience:</span>
<span>resume_from_checkpoint:</span>
<span>local_rank:</span>
<span>logging_steps:</span> <span>1</span>
<span>xformers_attention:</span>
<span>flash_attention:</span> <span>true</span>
<span>warmup_steps:</span> <span>100</span>
<span>evals_per_epoch:</span> <span>0</span>
<span>eval_table_size:</span>
<span>eval_table_max_new_tokens:</span> <span>128</span>
<span>saves_per_epoch:</span> <span>1</span>
<span>debug:</span>
<span>deepspeed:</span> <span>deepspeed_configs/zero2.json</span>
<span>weight_decay:</span> <span>0.0</span>
<span>special_tokens:</span>
  <span>pad_token:</span> <span>&lt;|end_of_text|&gt;</span>
</code></pre>
<p>I trained it using 6xA6000 GPUs with DeepSpeed ZeRO-2. The training took about 6 hours and 45 minutes. Here are the training curves I got from W&amp;B:</p>
<p><a rel="nofollow" href="https://i.imgur.com/nVcJYuu.png"><img alt="" src="https://i.imgur.com/nVcJYuu.png"></a></p>
<p>It automatically uploaded the DPO fine-tuned model, called <a href="https://huggingface.co/mlabonne/NeuralDaredevil-8B-abliterated">mlabonne/NeuralDaredevil-8B-abliterated</a>. To see if it fixed our abliterated version, I evaluated it on the same benchmarks:</p>
<p><a rel="nofollow" href="https://i.imgur.com/ChDwx4r.png"><img alt="" src="https://i.imgur.com/ChDwx4r.png"></a></p>
<p>We can see that this additional training allowed us to recover most of the performance drop due to abliteration. One area where the model doesn't improve is GSM8K, a math dataset, which could mean the orpo-dpo-mix-40k would benefit from more math samples.</p>
<p>The final model is an uncensored LLM with state-of-the-art performance in the 8B category. I recommend it as an improved version of Llama 3 8B Instruct when you don't need censorship. You can play with quantized versions like GGUF in LM Studio.</p>
<h2>
	<a rel="nofollow" href="#conclusion" id="conclusion">
		<span></span>
	</a>
	<span>
		Conclusion
	</span>
</h2>
<p>In this article, we introduced the concept of abliteration. This technique uses the model's activations on harmless and harmful prompts to calculate a refusal direction. It then uses this direction to modify the model's weights and ensure that we stop outputting refusals. This technique also demonstrates the fragility of safety fine-tuning and raises ethical considerations.</p>
<p>We applied abliteration to Daredevil-8B to uncensor it, which also degraded the model's performance. We then healed it using DPO to create the NeuralDaredevil-8B model, a fully uncensored and high-quality 8B LLM. Abliteration is not limited to removing alignment and should be seen as a form of fine-tuning without retraining. Indeed, it can creatively be applied to other goals, like FailSpy's <a href="https://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule">MopeyMule</a>, which adopts a melancholic conversational style.</p>
<p>I hope you liked this article. If you want to see more follow me on&nbsp;<a href="https://huggingface.co/mlabonne/">Hugging Face</a>&nbsp;and Twitter&nbsp;<a rel="nofollow" href="https://twitter.com/maximelabonne">@maximelabonne</a>.</p>
<h2>
	<a rel="nofollow" href="#references" id="references">
		<span></span>
	</a>
	<span>
		References
	</span>
</h2>
<ul>
<li>FailSpy, "<a rel="nofollow" href="https://github.com/FailSpy/abliterator">abliterator library</a>," GitHub, 2024.</li>
<li>Andy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, "<a rel="nofollow" href="https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction">Refusal in LLMs is mediated by a single direction</a>," Lesswrong, 2024.</li>
</ul>
<!-- HTML_TAG_END --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Meta trains large language models at scale (375 pts)]]></title>
            <link>https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/</link>
            <guid>40664339</guid>
            <pubDate>Wed, 12 Jun 2024 23:35:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/">https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/</a>, See on <a href="https://news.ycombinator.com/item?id=40664339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<p><span>As we continue to focus our AI research and development on solving increasingly complex problems, one of the most significant and challenging shifts we’ve experienced is the sheer scale of computation required to train large language models (LLMs).</span></p>
<p><span>Traditionally, our AI model training has involved a training massive number of models that required a comparatively smaller number of GPUs. This was the case for our recommendation models (e.g., our feed and ranking models) that would ingest vast amounts of information to make accurate recommendations that power most of our products.</span></p>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>With the advent of generative AI (GenAI), we’ve seen a shift towards fewer jobs, but incredibly large ones. Supporting GenAI at scale has meant rethinking how our software, hardware, and network infrastructure come together.</span></p>
<h2><span>The challenges of large-scale model training</span></h2>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>As we increase the number of GPUs in a job, the likelihood of an interruption due to a hardware failure also increases. Also, all of these GPUs still need to communicate on the same high-speed fabric to perform optimally. This underscores the importance of four factors:</span></p>
<ul>
<li aria-level="1"><b>Hardware reliability</b><span>: Ensuring that our hardware is reliable is important. We need to minimize the chances of a hardware failure interrupting a training job. This involves rigorous testing and quality control measures, and automation to quickly detect and remediate issues.</span></li>
<li aria-level="1"><b>Fast recovery on failure</b><span>: Despite our best efforts, hardware failures can and do occur. When they do, we need to be able to recover quickly. This involves reducing re-scheduling overhead and fast training re-initialization.</span></li>
<li aria-level="1"><b>Efficient preservation of the training state</b><span>: In the event of a failure, we need to be able to pick up where we left off. This means we need to regularly checkpoint our training state and efficiently store and retrieve training data.</span></li>
<li aria-level="1"><b>Optimal connectivity between GPUs:</b><span> Large-scale model training involves transferring vast amounts of data between GPUs in a synchronized fashion. A slow data exchange between a subset of GPUs can compound and slow down the whole job. Solving this problem requires a robust and high-speed network infrastructure as well as efficient data transfer protocols and algorithms.&nbsp;</span></li>
</ul>
<h2><span>Innovating across the infrastructure stack</span></h2>
<p><span>Perfecting every layer of our infrastructure stack is important due to the demands of GenAI at scale. This has encompassed developments in a wide range of areas.</span></p>
<h3><span>Training software</span></h3>
<p><span>We enable researchers to use </span><a href="https://pytorch.org/blog/training-production-ai-models/"><span>PyTorch</span></a><span> and other new open source developments, facilitating extremely fast research-to-production development. This includes </span><span>developing new algorithms and techniques for efficient large-scale training and integrating new software tools and frameworks into our infrastructure.</span></p>
<h3><span>Scheduling</span></h3>
<p><span>Efficient scheduling helps ensure that our resources are used optimally. This involves </span><span>sophisticated algorithms that can allocate resources based on the needs of different jobs and dynamic scheduling to adapt to changing workloads.</span></p>
<h3><span>Hardware&nbsp;</span></h3>
<p><span>We need high-performance hardware to handle the computational demands of large-scale model training. Beyond size and scale, many hardware configurations and attributes need to be best optimized for GenAI. Given that hardware development times are traditionally long, we had to adapt existing hardware, and to this end we explored various dimensions including power, HBM capacity and speed, and I/O.&nbsp;</span></p>
<p><span>We also pivoted by modifying the </span><a href="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/"><span>Grand Teton</span></a><span> platform that was developed using NVIDIA H100 GPUs, increased the TDP of the GPUs to 700W, and moved to HBM3 on the GPUs. Since we did not have time to change the cooling infrastructure, we had to remain in an air-cooled environment. The mechanical and thermal designs had to change to accommodate this, and that triggered a validation cycle to support a large-scale deployment.&nbsp;</span></p>
<p><span>All of these hardware-related changes were challenging because we had to find a solution that fit within the existing resource constraints, with a very small degree of freedom to change and meet a tight schedule.</span></p>
<h3><span>Data center deployment</span></h3>
<p><span>Once we’ve chosen a GPU and system, the task of placing them in a data center for optimal usage of resources (power, cooling, networking, etc.) requires revisiting trade-offs made for other types of workloads. Data center power and cooling infrastructure cannot be changed quickly (or easily) and we had to find an optimal layout that allowed maximum compute capability within a data hall. This required relocating supporting services such as readers out of the data hall and packing as many GPU racks as possible to maximize the power and network capability for highest compute density with the largest network cluster.&nbsp;</span></p>
<h3><span>Reliability&nbsp;</span></h3>
<p><span>We need to plan for detection and remediation to minimize downtime during hardware failures. The number of failures scales with the size of the cluster, and having a job that spans the cluster makes it necessary to keep adequate spare capacity to restart the job as soon as possible. In addition, we monitor failures and can sometimes take preventive measures to mitigate downtime.&nbsp;</span></p>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>Some of the most frequent failure modes we have observed are:</span></p>
<ul>
<li aria-level="1"><b>GPUs falling off:</b><span> In this case, GPUs are not detected by the host on PCIe. There are several reasons for this failure, but this failure mode is seen more in the early life and settles as the server ages.</span></li>
<li aria-level="1"><b>DRAM &amp; SRAM UCE:</b><span> Uncorrectable errors are common in memories, and we monitor and identify repeat offenders, track against thresholds, and initiate RMAs when error rates exceed vendor thresholds.</span></li>
<li aria-level="1"><b>HW network cable:</b><span> In the general category of unreachable servers, these failures are also seen most often in the early life of the server.&nbsp;</span></li>
</ul>
<h3><span>Network</span></h3>
<p><span>Large-scale model training involves transferring vast amounts of data quickly between GPUs. This requires robust and high-speed network infrastructure as well as efficient data transfer protocols and algorithms.&nbsp;</span></p>
<p><span>There are two leading choices in the industry that fit these requirements: RoCE and InfiniBand fabrics. Both of these options had tradeoffs. On the one hand, Meta had built RoCE clusters for the past four years, but the largest of those clusters only supported 4K GPUs. We needed significantly larger RoCE clusters. On the other hand, Meta had built research clusters with InfiniBand as </span><a href="https://ai.meta.com/blog/ai-rsc/"><span>large as 16K GPUs</span></a><span>. However, those clusters were </span><i><span>not</span></i><span> tightly integrated into Meta’s production environment, nor were they built for the latest generation of GPUs/networking. This made for a difficult decision of what fabric to build with.</span></p>
<p><span>So we decided to build both: </span><a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/"><span>two 24k clusters</span></a><span>, one with RoCE and another with InfiniBand. Our intent was to build and learn from the operational experience. These learnings will inform the future direction of GenAI fabrics. We optimized the RoCE cluster for quick build time, and the InfiniBand cluster for full-bisection bandwidth. We used both InfiniBand and RoCE clusters to train </span><a href="https://ai.meta.com/blog/meta-llama-3/"><span>Llama 3</span></a><span>, with the RoCE cluster used for training the largest model. Despite the underlying network technology differences between these clusters, we were able to tune both of them to provide equivalent performance for these large GenAI workloads</span></p>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>We optimized three aspects of the overall stack to make network communication for GenAI models performant on both clusters:</span></p>
<ol>
<li><span>We assigned communication patterns resulting from different model, data and pipeline parallelisms to different layers of the network topology so that the network capabilities were effectively exploited.</span></li>
<li><span>We implemented collective communication patterns with network topology awareness so that they can be less latency-sensitive. We do this by changing the default implementation of collectives with custom algorithms such as recursive doubling or halving instead of conventional algorithms like rings.</span></li>
<li><span>Just like ranking jobs, GenAI jobs produce additional fat flows that make it hard to distribute traffic across all possible network paths. This required us to further invest in network load balancing and routing to achieve an optimal distribution of traffic across network resources.</span></li>
</ol>
<p><span>We spoke in depth about our </span><a href="https://atscaleconference.com/videos/scaling-roce-networks-for-ai-training/"><span>RoCE load-balancing techniques</span></a><span> at </span><a href="https://atscaleconference.com/videos/scaling-roce-networks-for-ai-training/"><span>Networking @Scale 2023</span></a><span>.</span></p>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<h3><span>Storage</span></h3>
<p><span>We need efficient data-storage solutions to store the vast amounts of data used in model training. This involves investing in high-capacity and high-speed storage technologies and developing new data-storage solutions for specific workloads.</span></p>
<h2><span>Looking ahead</span></h2>
<p><span>In the next few years w</span><span>e will be working with hundreds of thousands of GPUs, handling even larger volumes of data, and dealing with longer distances and latencies. We’ll be adopting new hardware technologies—including newer GPU architectures—and evolving our infrastructure.&nbsp;</span></p>
<p><span>These challenges will push us to innovate and adapt in ways we can’t fully predict yet. But one thing is certain: We are only at the beginning of this journey. As we continue to navigate the evolving landscape of AI, we remain committed to pushing the boundaries of what’s possible.</span></p>

		
	</div></div>]]></description>
        </item>
    </channel>
</rss>