<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 15 Oct 2024 12:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Web Browser Engineering (166 pts)]]></title>
            <link>https://browser.engineering/index.html</link>
            <guid>41846780</guid>
            <pubDate>Tue, 15 Oct 2024 09:42:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://browser.engineering/index.html">https://browser.engineering/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41846780">Hacker News</a></p>
<div id="readability-page-1" class="page">


<header>


<a href="https://twitter.com/browserbook">Twitter</a> ·
<a href="https://browserbook.substack.com/">Blog</a> ·
<a href="https://patreon.com/browserengineering">Patreon</a> ·
<a href="https://github.com/browserengineering/book/discussions">Discussions</a>
</header>




<nav id="toc">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#part-1-loading-pages" id="toc-part-1-loading-pages">Part
1: Loading Pages</a></li>
<li><a href="#part-2-viewing-documents" id="toc-part-2-viewing-documents">Part 2: Viewing Documents</a></li>
<li><a href="#part-3-running-applications" id="toc-part-3-running-applications">Part 3: Running
Applications</a></li>
<li><a href="#part-4-modern-browsers" id="toc-part-4-modern-browsers">Part 4: Modern Browsers</a></li>
</ul>
</nav>

<p>Web browsers are ubiquitous, but how do they work? This book
explains, building a basic but complete web browser, from networking to
JavaScript, in a couple thousand lines of Python.</p>

<div>
<figure>
<img src="https://browser.engineering/im/cover.jpg" alt="The cover for Web Browser Engineering, from Oxford University Press">

</figure>
<section id="pre-order-web-browser-engineering">
<h2>Pre-order <em>Web Browser Engineering</em></h2>
<p><em>Web Browser Engineering</em> will be published by Oxford
University Press before the end of the year. To get it as soon as it’s
out, <a href="https://global.oup.com/academic/product/web-browser-engineering-9780198913863">pre-order
now!</a></p>
</section>
</div>
<p>Follow this book’s <a href="https://browserbook.substack.com/archive">blog</a> or <a href="https://twitter.com/browserbook">Twitter</a> for updates. You can
also talk about the book with others in our <a href="https://github.com/browserengineering/book/discussions">discussion
forum</a>.</p>
<p>If you are enjoying the book, consider supporting us on <a href="https://patreon.com/browserengineering">Patreon</a>.</p>
<p>Or just <a href="mailto:author@browser.engineering">send us an
email</a>!</p>
<section id="introduction">
<h2>Introduction</h2>
<ol type="1">
<li><a href="https://browser.engineering/preface.html">Preface</a></li>
<li><a href="https://browser.engineering/intro.html">Browsers and the Web</a></li>
<li><a href="https://browser.engineering/history.html">History of the Web</a></li>
</ol>
</section>
<h2 id="part-1-loading-pages">Part 1: Loading Pages</h2>
<ol type="1">
<li><a href="https://browser.engineering/http.html">Downloading Web Pages</a><br>
URLs and HTTP requests</li>
<li><a href="https://browser.engineering/graphics.html">Drawing to the Screen</a><br>
Creating windows and drawing to a canvas</li>
<li><a href="https://browser.engineering/text.html">Formatting Text</a><br>
Word wrapping and line spacing</li>
</ol>
<h2 id="part-2-viewing-documents">Part 2: Viewing Documents</h2>
<ol start="4" type="1">
<li><a href="https://browser.engineering/html.html">Constructing an HTML Tree</a><br>
Parsing and fixing HTML</li>
<li><a href="https://browser.engineering/layout.html">Laying Out Pages</a><br>
Inline and block layout</li>
<li><a href="https://browser.engineering/styles.html">Applying Author Styles</a><br>
Parsing and applying CSS</li>
<li><a href="https://browser.engineering/chrome.html">Handling Buttons and Links</a><br>
Hyperlinks and browser chrome</li>
</ol>
<h2 id="part-3-running-applications">Part 3: Running Applications</h2>
<ol start="8" type="1">
<li><a href="https://browser.engineering/forms.html">Sending Information to Servers</a><br>
Form submission and web servers</li>
<li><a href="https://browser.engineering/scripts.html">Running Interactive Scripts</a><br>
Changing the DOM and reacting to events</li>
<li><a href="https://browser.engineering/security.html">Keeping Data Private</a><br>
Cookies and logins, XSS and CSRF</li>
</ol>
<h2 id="part-4-modern-browsers">Part 4: Modern Browsers</h2>
<ol start="11" type="1">
<li><a href="https://browser.engineering/visual-effects.html">Adding Visual Effects</a><br>
Blending, clipping, and compositing</li>
<li><a href="https://browser.engineering/scheduling.html">Scheduling Tasks and Threads</a><br>
The event loop and the rendering pipeline</li>
<li><a href="https://browser.engineering/animations.html">Animating and Compositing</a><br>
Smooth animations using the GPU</li>
<li><a href="https://browser.engineering/accessibility.html">Making Content Accessible</a><br>
Keyboard input, zooming, and the accessibility tree</li>
<li><a href="https://browser.engineering/embeds.html">Supporting Embedded Content</a><br>
Images, iframes, and scripting</li>
<li><a href="https://browser.engineering/invalidation.html">Reusing Previous Computation</a><br>
Invalidation, editing, and correctness</li>
</ol>













</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Retreat to Muskworld (114 pts)]]></title>
            <link>https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/</link>
            <guid>41845596</guid>
            <pubDate>Tue, 15 Oct 2024 06:34:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/">https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/</a>, See on <a href="https://news.ycombinator.com/item?id=41845596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<figure><img loading="lazy" data-attachment-id="277" data-permalink="https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/screenshot-2024-10-11-174447/" data-orig-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png" data-orig-size="847,644" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2024-10-11 174447" data-image-description="" data-image-caption="" data-medium-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=300" data-large-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=847" tabindex="0" role="button" width="847" height="644" src="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=847" alt="A Tesla Cybercab driving past a small town movie set, during Tesla's We, Robot event"><figcaption><em>A totally real robotaxi, driving in an extremely normal town</em></figcaption></figure>



<p>Almost eight years ago, Elon Musk announced that every Tesla made from that moment forward would be capable of Level 5 autonomous driving with nothing more than a software update. It was a pivotal moment in Tesla’s history, committing the company to not just succeed as an electric automaker, but solve one of the most ambitious AI and robotics challenges possible. To create confidence in that staggering aspiration, Tesla released a video of a Model X driving around Palo Alto autonomously to the Rolling Stones’ “Paint it Black,” claiming that the driver behind the wheel was only there “for legal purposes.”</p>



<p>Eight long and hype-filled years later, Tesla is still looking for ways to build confidence in its ability to deliver a “general solution to self-driving” through hype and spectacle, even as companies like Waymo deliver <a href="https://techcrunch.com/2024/08/20/waymo-is-now-giving-100000-robotaxi-rides-week/">the reality of 100,000 driverless taxi rides per week</a>. Rather than meeting the competitive challenge from Waymo with real driverless rides on real public streets, Tesla’s latest ploy for credibility sees the firm retreating ever deeper into fantasy, building what can only be described as a temporary theme park on a movie studio lot for its first ever “driverless” demonstration.</p>



<p>This contrast is instructive. The “Paint It Black” video of eight years ago was no more “real” or “fake” than yesterday’s “We, Robot” demonstration, but at least it had the pretense of reality: it depicted a real car on real roads. Tesla’s latest spectacle likely cost orders of magnitude more to produce, but it didn’t even purport to show any actual real-world capability. The entire thing was pure fantasy, in a contained fantasy world, built on a movie theater lot that exists for the sole purpose of producing such spectacles.</p>



<p>This trajectory, from simulating future capability on public roads to creating a fantasy world for fantasy cars to show off fantasy capabilities, should worry Tesla’s supporters. We can already see Musk retreating into a misinformation-fueled fantasy world every day on Twitter, and the jarring divisiveness of the Cybertruck suggests that his runaway ego is already making Tesla’s products less palatable. If Musk’s retreat into a self-soothing fantasy bubble is also making his hype game less effective, and the 8% drop in Tesla’s stock price suggests that it is, his most important skill set is on the line.</p>



<p>Of course, with Wall Street analysts almost universally declaring themselves “underwhelmed,” it has to be asked: what else could they have possibly expected? Did they really believe that now, after eight years of empty hype, fake statistics, and blown deadlines, Tesla would actually start providing credible evidence to back the litany of bullshit? Having made no effort to explain his chronic inability to meet (let alone stop making) his self-imposed deadlines, and facing no real consequences for nearly a decade of what is either unprecedented public delusion or deception, why on earth would Elon Musk make a serious play for credibility now?</p>



<p>Having drawn a poor hand eight years ago, <a href="https://niedermeyer.io/2024/04/22/no-more-rebuys-mr-musk/">Elon Musk is playing poker the only way he knows how: going all-in on every hand</a>. This strategy has created a confidence game of unprecedented proportions in our financial markets, as every gambler in the casino wants to put a chip on the guy going all-in and winning every time, and only the $13 billion of hung debt for the Twitter deal suggests his hot streak might ever end. All Musk has to do to keep the music playing is to project confidence, which is infinitely easier to do in a studio lot setpiece than out on public roads.</p>



<p>For everyone not locked into this financial-cognitive nightmare, it’s hard to imagine anyone seriously believing that a night of delusional Disney Adult cringe might actually inflate Tesla’s stock beyond the current ~$680 billion valuation. Given that Tesla’s “Full Self-Driving” is already the subject of a multi-year <a href="https://apnews.com/article/tesla-investigations-justice-department-musk-self-driving-29a68864f75c9fabbd04f7a87d169444">federal investigation</a> into <a href="https://www.reuters.com/business/autos-transportation/tesla-autopilot-probe-us-prosecutors-focus-securities-wire-fraud-2024-05-08/">securities and wire fraud</a>, We, Robot’s blatant fantasy-mongering is downright shocking. If anything, showing a low-speed, closed-course theme park ride in order to build confidence around Tesla’s progress toward actual real-world driverless capability is almost too childish to call a fraud. </p>



<p>Ultimately, Musk’s increasingly-degenerate gambling run is slouching toward one last big coinflip: the 2024 presidential election. With <a href="https://www.nytimes.com/2024/10/11/us/politics/elon-musk-donald-trump-pennsylvania.html">Musk going “all-in” on Donald Trump</a>, and <a href="https://nypost.com/2024/10/07/us-news/elon-musk-suggests-hell-be-thrown-in-prison-if-harris-beats-trump-if-he-loses-im-fed/">musing that he will end up in a prison cell if Kamala Harris is elected</a>, it’s clear that his main political issue is his freedom to keep rolling over his endless confidence game without legal consequences. If Trump wins and delivers Musk the impunity he craves, the line between amusement park fantasy and $700 billion self-driving juggernaut will all but disappear, and we will all find ourselves living in Muskworld’s house of mirrors.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Superstitious Users and the FreeBSD Logo (104 pts)]]></title>
            <link>https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html</link>
            <guid>41845427</guid>
            <pubDate>Tue, 15 Oct 2024 06:10:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html">https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html</a>, See on <a href="https://news.ycombinator.com/item?id=41845427">Hacker News</a></p>
<div id="readability-page-1" class="page">
   
<!--htdig_noindex-->
    <b>Brett Glass</b> 
    <a href="mailto:freebsd-chat%40freebsd.org?Subject=Superstitious%20users%20and%20the%20FreeBSD%20logo&amp;In-Reply-To=" title="Superstitious users and the FreeBSD logo">brett at lariat.net
       </a><br>
    <i>Wed Nov 30 02:30:04 UTC 2011</i>
    <ul>
        <li>Previous message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006641.html">Query about about freebsd-chat digestt, Vol 398, Issue 1
</a></li>
        <li>Next message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006643.html">Superstitious users and the FreeBSD logo
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/date.html#6642">[ date ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/thread.html#6642">[ thread ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/subject.html#6642">[ subject ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/author.html#6642">[ author ]</a>
         </li>
       </ul>
    <hr>  
<!--/htdig_noindex-->
<!--beginarticle-->
<pre>Everyone:

I just got a call from the owner of a hotel for which we provide 
hotspot service. She says that a guest spotted the "Powered by 
FreeBSD" logo at the bottom of the login page, and was offended; 
the guest was convinced that either we or the hotel management 
"worshipped the Devil" and refused to stay at the hotel unless the 
logo was removed. The owner could make no headway by explaining 
that the besneakered mascot was a cartoon character and was a 
daemon, not the Devil. And she feared upsetting the guest even more 
if she said that large portions of the same software are inside 
every Mac and iPad. The hotel stands to lose more than $1000 if the 
guest, who had originally planned to stay for a long period, moves out.

One of our tech support people also got a call directly from the 
hotel guest, who claimed that having the logo on the page 
constituted "abuse." The guest also claimed to be "losing money" 
because she wouldn't use the hotspot if there was a "devil" on the 
splash page. He didn't even realize what she was talking about at 
first.... He couldn't imagine why on Earth this person was calling 
him and going on about devils.

Attempts at misguided religious censorship notwithstanding, I don't 
want to see one of my  ISP's customers lose business. And I'd like 
to keep a FreeBSD logo on our hotspot page. Is there artwork that 
doesn't include horned creatures that might offend the ignorant or 
superstitious?

--Brett Glass

</pre>


<!--endarticle-->
<!--htdig_noindex-->
    <hr>
    <ul>
        <!--threads-->
	<li>Previous message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006641.html">Query about about freebsd-chat digestt, Vol 398, Issue 1
</a></li>
	<li>Next message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006643.html">Superstitious users and the FreeBSD logo
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/date.html#6642">[ date ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/thread.html#6642">[ thread ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/subject.html#6642">[ subject ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/author.html#6642">[ author ]</a>
         </li>
       </ul>

<hr>
<a href="http://lists.freebsd.org/mailman/listinfo/freebsd-chat">More information about the freebsd-chat
mailing list</a><br>
<!--/htdig_noindex-->

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cheating alleged after men's world conker champion found with steel chestnut (292 pts)]]></title>
            <link>https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut</link>
            <guid>41844545</guid>
            <pubDate>Tue, 15 Oct 2024 03:12:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut">https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut</a>, See on <a href="https://news.ycombinator.com/item?id=41844545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The World Conker Championships is investigating cheating allegations after the men’s winner was found to have a steel chestnut in his pocket.</p><p>David Jakins won the annual title in Southwick, <a href="https://www.theguardian.com/uk-news/northamptonshire" data-link-name="in body link" data-component="auto-linked-tag">Northamptonshire</a>, on Sunday for the first time after competing since 1977.</p><p>But the 82-year-old was found to have a metal replica in his pocket when he was searched by organisers after his victory.</p><p>The retired engineer has denied using the metal variety in the tournament.</p><p>Jakins was responsible for drilling and inserting strings into other competitors’ chestnuts as the competition’s top judge, known as the “King Conker”.</p><p>Alastair Johnson-Ferguson, who lost in the men’s final against Jakins, said he suspected “foul play”, the Telegraph reported.</p><p>The 23-year-old said: “My conker disintegrated in one hit, and that just doesn’t happen … I’m suspicious of foul play and have expressed my surprise to organisers.”</p><p>Kelci Banschbach, 34, from Indianapolis, defeated the men’s champion in the grand final to become the first American to win the competition. More than 200 people took part.</p><p>Jakins said: “I was found with the steel conker in my pocket, but I only carry [it] around with me for humour value and I did not use it during the event.</p><p>“Yes, I did help prepare the conkers before the tournament. But this isn’t cheating or a fix, and I didn’t mark the strings.”</p><p>St John Burkett, a spokesperson for the World Conker Championships, said the cheating claims were being investigated.</p><p>“Allegations of foul play have been received that somehow King Conker swapped his real conker for the metal one later found in his pocket.</p><p>“Players select conkers from a sack before each round.</p><p>“There are also suggestions that King Conker had marked the strings of harder nuts. We can confirm he was involved in drilling and lacing the nuts before the event.</p><p>“We are investigating.”</p><p>More than 2,000 conkers had been prepared prior to the event.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zamba2-7B (258 pts)]]></title>
            <link>https://www.zyphra.com/post/zamba2-7b</link>
            <guid>41842975</guid>
            <pubDate>Mon, 14 Oct 2024 22:45:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zyphra.com/post/zamba2-7b">https://www.zyphra.com/post/zamba2-7b</a>, See on <a href="https://news.ycombinator.com/item?id=41842975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-animation="default" data-collapse="tiny" data-duration="400" data-easing="ease" data-easing2="ease" role="banner"><p><a href="https://www.zyphra.com/"><img width="123" sizes="(max-width: 479px) 123px, (max-width: 767px) 22vw, 123px" alt="" src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch.png" loading="lazy" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch.png 1505w"></a></p></div><div><p><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash.jpg" loading="lazy" width="726" height="Auto" alt="" sizes="(max-width: 479px) 90vw, (max-width: 1279px) 80vw, 90vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-500.jpg 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-800.jpg 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-1080.jpg 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-1600.jpg 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-2000.jpg 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-2600.jpg 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-3200.jpg 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash.jpg 5760w"></p><p><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b.png" loading="lazy" width="648" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b.png 1470w"></p></div><div><p>October 14, 2024</p><p>PALO ALTO, CALIFORNIA</p><p>Zyphra is excited to release Zamba2-7B, a state-of-the-art small language model. At the 7B scale, we outperform the leading models of Mistral, Google’s Gemma and Meta’s Llama3 series in both quality and performance. We believe Zamba2-7B is the leading model for running on-device and on consumer GPUs as well as for many enterprise applications which require a powerful but compact and efficient model for natural-language tasks.</p><p>Authors</p><p>Zyphra Team</p><p>Collaborators</p><p>Daniel A Roberts (Sequoia Capital &amp; MIT), Andrey Gromov (Meta FAIR), Kushal Tirumala (Meta FAIR) and Hassan Shapourian (Cisco)</p></div><div id="introduction"><div><div id="zamba1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zamba2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="Quality vs. Inference Speed" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zamba3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section></div><div><div id="zyda1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zyda2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zyda3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="zyda4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section></div><div><section id="small1"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><div><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div></section><section id="small2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="small3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="small4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section><section id="small5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div></section></div><div><div id="rag1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="rag2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="rag3"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="rag4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><div id="rag5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><div id="tree1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="tree2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="tree3"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="674" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="586" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div id="layer1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="layer2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="619" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="658" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="680" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div></section><div id="layer3"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><section id="edge1"><div><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="455" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><div id="edge2"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><section id="edge3"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><div id="edge4"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><div id="edge5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><div id="hop1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="hop2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="hop3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="581" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="hop4"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><div id="hop5"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><section id="hop6"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="367" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="hop7"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="479" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="hop8"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="458" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a></section><section id="hop9"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" width="360" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section></div><div><div target="_blank" id="cook1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="cook2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="cook3"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a></section><div id="cook4"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><div id="cook5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div><section id="cook7"><p>What is Annealing?</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a></section><section id="cook8"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="hop9"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section></div><div><div target="_blank" id="mini1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="mini2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="mini3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a></section><section id="mini4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section><section id="mini5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" width="384" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section></div><div><section id="noc1"><div target="_blank"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="429" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="noc2"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="267" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" width="319" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="noc3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="400" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section></div><div><div target="_blank" id="longrag1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="longrag2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><div target="_blank" id="longrag3"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><section id="longrag4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="longrag5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div target="_blank" id="zamba2_1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zamba2_2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div target="_blank"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" width="319" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="zamba2_3"><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="571" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" width="571" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div target="_blank" id="zyda2_1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zyda2_2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zyda2_3"><div target="_blank"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="zyda2_4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="zyda2_5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section><section id="zyda2_7"><h3>Analysis of Global Duplicates</h3><p>We present histograms depicting distribution of cluster sizes in all the datasets (see Fig. 7-11). Please, note that all the figures are in log-log scale. We see a significant drop in the number of clusters starting from the size of around 100. This drop is present both in DCLM and FineWeb-Edu2 (see Fig. 8 and 9 respectively), and most likely is explained by a combination of&nbsp; the deduplication strategy and quality when creating both datasets: DCLM deduplication was done individually within 10 shards, while FineWeb-Edu2 was deduplicated within every Common Crawl snapshot. We find that large clusters usually contain low quality material (repeated advertisements, license agreements templates, etc), so it’s not surprising that such documents were removed. Notably, DCLM still contained one cluster with the size close to 1 million documents, containing low quality documents seemingly coming from the advertisements (see Appendix).We find both Zyda-1and Dolma-CC contain a small amount of duplicates, which is expected, since both datasets were deduplicated globally by their authors. Remaining duplicates are likely false negatives from the initial deduplication procedure. Note, that distribution of duplicates clusters sizes of these two datasets (Fig. 10 and 11) don’t contain any sharp drops, but rather hyper exponentially decreases with cluster size.&nbsp;</p><a href="#"><img src="https://cdn.prod.website-files.com/img/placeholder-thumb.svg" loading="lazy" alt=""></a><h5><em>Figure 7: Distribution of cluster sizes of duplicates in global dataset (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9.png 1600w" alt=""></a><h5><em>Figure 8: Distribution of cluster sizes of duplicates in DCLM (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10.png 1600w" alt=""></a><h5><em>Figure 9: Distribution of cluster sizes of duplicates in FineWeb-Edu2 (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11.png 1600w" alt=""></a><h5><em>Figure 10: Distribution of cluster sizes of duplicates in Zyda-1 (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12.png 1600w" alt=""></a><h5><em>Figure 11: Distribution of cluster sizes of duplicates in Dolma-CC (log-log scale).</em></h5><h3>Largest cluster in DCLM</h3><p>Below is an example of the document from the largest cluster (~1M documents) of duplicates in DCLM (quality score 0.482627):<br>‍<em>Is safe? Is scam?<br>Is safe for your PC?<br>Is safe or is it scam?<br>Domain is SafeSafe score: 1</em>‍<br>‍<em>The higher the number, the more dangerous the website.Any number higher than 1 means DANGER.</em>‍<br>‍<em>Positive votes:<br>Negative votes:<br>Vote Up Vote Down review</em>‍<br>‍<em>Have you had bad experience with Warn us, please!</em></p><h3>Examples of varying quality score in DCLM in a cluster</h3><p>Below one will find a few documents with different quality scores from DCLM coming from the same duplicates cluster. Quality score varies from ~0.2 to ~0.04.</p></section></div></div>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Physics of Magic Windows (2021) (125 pts)]]></title>
            <link>https://mattferraro.dev/posts/caustics-engineering</link>
            <guid>41842775</guid>
            <pubDate>Mon, 14 Oct 2024 22:25:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mattferraro.dev/posts/caustics-engineering">https://mattferraro.dev/posts/caustics-engineering</a>, See on <a href="https://news.ycombinator.com/item?id=41842775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p><time datetime="2021-08-18">August 18, 2021</time></p><p>I recently made a physical object that defies all intuition. It's a square of acrylic, smooth on both sides, totally transparent. A tiny window.</p><div><p><img alt="Clear Acrylic" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwindow_clear.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwindow_clear.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwindow_clear.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>But it has the magic property that if you shine a flashlight on it, it forms an image:</p><div><p><img alt="2D Image of Cat" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2F2D_image.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2F2D_image.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2F2D_image.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>And if you take it out in the sun, it produces this 3D hologram:</p><video width="100%" height="auto" autoplay="" muted="" controls="" loop=""><source src="https://mattferraro.dev/images/caustics-engineering/3dcat.mp4" type="video/mp4"></video><p>This post describes the math that went into making the object, and how you can create your own.</p><h2 id="but-first-how-is-this-even-possible">But first: How is this even possible?</h2><p>Let's focus on the 2D image before talking about the hologram.</p><p>The physical phenomenon we're looking at is called a <em>caustic</em>.</p><div><p><img alt="Example Caustic" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fglass_caustic_pd.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fglass_caustic_pd.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fglass_caustic_pd.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>Caustics are the bright patches of light we see when illuminating a transparent object. All the photons that don't pass directly through the object are what form the object's shadow. All those photons still have to go somewhere; they contribute to the caustic pattern.</p><p>The most interesting aspect of caustics is that they arise from even the tiniest of variations in surface flatness. Even the gentlest waves on the surface of a pool form powerful lenses that cast intense caustics on the floor below.</p><div><p><img alt="Water Caustics" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwater_caustics_cc.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwater_caustics_cc.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwater_caustics_cc.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>The reason my acrylic square can form an image is because I've distributed just the right amount of concavity and convexity into the surface so that the refracted light forms a caustic image.</p><p>To gain some intuition for how it is done, consider a traditional convex lens:</p><div><p><img alt="Parabolic Lens" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ftraditional_lens_2.svg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ftraditional_lens_2.svg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ftraditional_lens_2.svg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>This lens forms the simplest possible caustic. If all the incoming light is from a single, very distant light source like the Sun, this lens focuses all of its incoming light into a single point. The caustic image from this lens is dark everywhere with one very bright spot in the center.</p><p>Zooming in on one small section of the lens we notice a few properties:</p><ol><li>The overall thickness of the lens does not have a direct impact on the outgoing ray angle. We could add material to the left side of this lens and nothing would change. The first transition, from air to glass, can be entirely ignored.</li><li>The angle between the incoming light rays and the glass-air boundary has a strong effect on the refracted ray angle.</li><li>Whether two rays converge or diverge is controlled by how <em>curved</em> the lens is where the glass meets the air</li></ol><p>In other words, the height of the glass <span>h(x)</span> is not on its own important. But the slope of the glass, <span>\frac{\mathrm{d}h}{\mathrm{d}x}</span>, gives us the outgoing ray angle via Snell's law. Where rays converge the image is brighter than the light source. Where rays diverge the image is darker. Therefore the brightness of the image (at that point, where the rays fall) is related to <span>\frac{\mathrm{d}^2h}{\mathrm{d}x^2}</span>.</p><p>The thickness of my acrylic slab varies across the entire <span>xy</span> plane, so I'll call it <span>h(x,y)</span> and we'll think of it as a <strong>heightmap</strong>.</p><p>By controlling <span>\nabla h = (\frac{\partial h}{\partial x}, \frac{\partial h}{\partial y}</span>), and <span>\nabla ^2 h = (\frac{\partial ^2 h}{\partial x^2} + \frac{\partial ^2 h}{\partial y^2})</span>, we can steer all of our incoming light to the correct locations in the image, while contributing the right brightness to make it recognizable. By making some simplifying assumptions we can guarantee that the resulting heightmap will be smooth and continuous.</p><p>For the Magic Window shown above, the total height variation over the <span>10cm \times 10cm</span> surface is about <span>2.0mm</span>.</p><div><p><img alt="Slight Refraction" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fslight_refraction.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fslight_refraction.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fslight_refraction.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>See how the slight variations in surface height distort the straight line of the floor moulding? Our Magic Window works like any other lens—by bending light.</p><h2 id="table-of-contents">Table of Contents</h2><ul><li><a href="#formulating-the-problem">Formulating the Problem</a></li><li><a href="#steps-to-a-solution">Steps to a Solution</a></li><li><a href="#morphing-the-cells">Morphing the Cells</a><ul><li><a href="#computing-the-loss">Computing the Loss</a></li><li><a href="#stepping-to-reduce-loss">Stepping to Reduce Loss</a></li></ul></li><li><a href="#snells-law-and-normal-vectors">Snell's Law and Normal Vectors</a></li><li><a href="#finding-the-heightmap">Finding the Heightmap</a></li><li><a href="#manufacturing">Manufacturing</a></li><li><a href="#acknowledgements">Acknowledgements</a></li><li><a href="#my-code">My Code</a></li><li><a href="#licensing">Licensing</a></li><li><a href="#contact-me">Contact me</a></li><li><a href="#one-last-thing">One Last Thing</a></li></ul><h2 id="formulating-the-problem">Formulating the Problem</h2><p>We want to find a heightmap <span>h(x,y)</span> whose caustic image has brightness <span>b(u,v)</span>, equal to some input image. To achieve this we can imagine a grid of cells, akin to pixels, on the surface of the acrylic lens. Here each "pixel" on the lens corresponds to a pixel in the image. Image pixels and their corresponding lens-space "pixels" are labeled with shared <span>(i, j)</span> coordinates.</p><div><p><img alt="Diagram" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Remember that <span>(i, j)</span> are integers labeling the column and row of the pixel, whereas <span>(x, y)</span> and <span>(u, v)</span> are real numbers measured in something like meters or inches.</p><h2 id="steps-to-a-solution">Steps to a Solution</h2><p><strong>Step 1:</strong> We morph the cells on the lens, making them bigger or smaller, so that the area of lens cell <span>(i, j)</span> is proportional to the brightness of image cell <span>(i, j)</span>. The resulting lens grid is no longer square—lots of warping and skew have to be introduced to maintain continuity. This step is by far the hardest part and must be solved iteratively.</p><p><strong>Step 2:</strong> For each cell <span>(i, j)</span> we need to find the angle from the lens cell to image cell and use Snell's law to find the required surface normal. This step is straightforward geometry.</p><p><strong>Step 3:</strong> Integrate all the surface normals to find a continuous heightmap <span>h(x,y)</span>. We're back to iterative methods here, but if we apply certain contraints to how we solve step 1, this step is actually fast and easy.</p><h2 id="morphing-the-cells">Morphing the Cells</h2><p>For an image with <span>n \times n</span> pixels, the lens grid will need <span>(n+1) \times (n+1)</span> points, so that each cell in the lens grid is defined by four points. Technically we should adopt yet another coordinate system to label the <em>points</em> in the lens grid since they are distinct from the <em>cells</em> in the lens grid, but I think it's easier to just reuse <span>(i, j)</span> and we can say that for grid cell <span>(i, j)</span>, the point in the upper left is defined as grid point <span>(i, j)</span>.</p><div><p><img alt="Diagram 2" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram2.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram2.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram2.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>This leaves us with one row and one column of extra grid points along the bottom and right edges, but that will be trivial to deal with when it comes up.</p><p>Each <em>point</em> in the lens grid <span>(i,j)</span> has an <span>(x, y)</span> coordinate. A point's <span>(i, j)</span> coordinates never change but the <span>(x, y)</span> coordinates will change as we morph the cells more and more.</p><h2 id="computing-the-loss">Computing the Loss</h2><p>Given the <span>(x, y)</span> locations of all the lens grid points, simple geometry lets us calculate the area of each lens grid cell. Of course at first every cell has the same area, but that will change as soon as we start morphing things.</p><p>The condition we want is that every lens grid <em>cell</em> <span>(i, j)</span> has an <em>area</em> which scales with the <em>brightness</em> of image pixel <span>b(i, j)</span>.</p><p>Area and brightness are not compatible units so it is helpful to normalize cell area by the full window area, and pixel brightness by total image brightness, so that each is measured in a unitless "percentage".</p><p>\tag{1.0}
\frac{A_{ij}}{\Sigma A} = \frac{b_{ij}}{\Sigma b}</p><p>Intuitively, this means:</p><blockquote><p>If a single pixel contributes <span>x\%</span> of the brightness of the entire image, the corresponding window cell should take up <span>x\%</span> of the area of the entire window.</p></blockquote><p>Equation <span>(1.0)</span> is the goal, but it will not be not be true until after we've morphed the window grid. Until we've done that, we need to compute a loss function which tells us how badly we're missing our target. Something like:</p><p>\tag{1.1}
L = \frac{b_{ij}}{\Sigma b} - \frac{A_{ij}}{\Sigma A}</p><p>In code:</p><pre><code><span># In Julia-flavored psuedocode</span>
img <span>=</span> read_image<span>(</span><span>"cat.png"</span><span>)</span>
brightness <span>=</span> convert_to_grayscale<span>(</span>img<span>)</span>
total_brightness <span>=</span> sum<span>(</span>brightness<span>)</span>
brightness <span>=</span> brightness <span>.</span><span>/</span> total_brightness

w <span>=</span> <span>.1</span> <span># meters</span>
h <span>=</span> <span>.1</span> <span># meters</span>
area_total <span>=</span> w <span>*</span> h
loss <span>=</span> compute_pixel_area<span>(</span>grid<span>)</span> <span>.</span><span>/</span> area_total <span>-</span> brightness
</code></pre><div><p><img alt="Image and Loss Function" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fimage_and_loss.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fimage_and_loss.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fimage_and_loss.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Where I've colorized the loss function so that red areas indicate regions where our grid cells need to grow and blue regions indicate where our grid cells need to shrink.</p><p>This image is the loss function <span>L</span> and I'll refer to it a lot. </p><h2 id="stepping-to-reduce-loss">Stepping to Reduce Loss</h2><p>The loss image can be thought of as a scalar field <span>L(x, y)</span>. The gradient of a scalar field yields a vector field, which we could call <span>\nabla L(x,y)</span>. We can step each grid point slowly in the direction of the gradient field, and in doing so the cells that are too small will get bigger and the cells that are too big will get smaller. Our loss will shrink, and we'll create our image!</p><p>The first thing to do is compute <span>\nabla L</span> and look at the vector field:</p><div><p><img alt="Gradient of L as a vector field" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fgrad_L.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fgrad_L.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fgrad_L.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Crap.</p><p><span>\nabla L</span> is a very poorly behaved vector field. It is noisy, discontinuous, and in many places equal to zero.</p><p>Almost everywhere, neighboring points need to step in drastically different directions. This creates a situation where improving one cell's loss will necessarily worsen its neighbor's losses, which means that in practice this method can never converge. It's a dead end.</p><hr><p>Instead let's draw an analogy to Computational Fluid Dynamics. We need to dilate certain cells and shrink others according to a brightness function. This is similar to modeling compressible air flow where each cell has pressure defined as a pressure function.</p><p>If every cell in a 2D grid has some initial pressure, how does the system relax over time? The regions with high pressure expand and the regions of low pressure contract, with regions of middling pressure getting shoved around in a sort of global tug-of-war. Clearly, our problem is analogous.</p><p>So, how is this problem solved in CFD simulations? A standard approach is to define a <strong>Velocity Potential</strong> called <span>\Phi</span> (read: <em>phi</em>). The Velocity Potential <span>\Phi</span> is a scalar field defined at each cell. Its units are <span>meters^2 / second</span> which at first glance is not very easy to interpret. But the reason <span>\Phi</span> is convenient is that its spatial derivatives are measured in <span>meters/second</span>. In other words, the gradient of <span>\Phi</span> gives a vector whose units are velocity:</p><p>\tag{1.2}
\nabla \Phi = \left( \frac{\partial{\Phi}}{\partial{x}}, \frac{\partial{\Phi}}{\partial{y}} \right) = \vec{v}</p><div><p><img alt="Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Here is an example <span>\Phi</span>. It is just some scalar field best viewed as a heightmap.</p><div><p><img alt="Gradient of Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>And here is the gradient of that same <span>\Phi</span>. These vectors are velocity vectors that point uphill. If we were performing Computational Fluid Dynamics, these vectors would indicate how fluid might flow from regions of high pressure to regions of low pressure. </p><p>Notice how well behaved this vector field is! There is gentle variation across the field but any two neighbors are very similar to each other. None of the arrows pierce the boundary.</p><p>In our case we don't have fluid pressure, we have light pressure. Regions in our image which are too bright have high light pressure, which is quantified in our loss function <span>L</span>.</p><p>If we can somehow use <span>L</span> to find a <span>\Phi</span> that describes our light pressure distribution, all we need to do is calculate <span>\vec{v} = \nabla \Phi</span> and we'll be able to morph all of our lens grid points according to <span>\vec{v}</span> to decrease our loss!</p><p>So how do we find a suitable <span>\Phi</span>? Well, the property we know about each cell is its loss, which encodes how much that cell needs to grow or shrink. </p><blockquote><p>This property, how much a cell grows or shrinks over time as it moves with a velocity field, is called the <strong>divergence</strong> of that field.</p></blockquote><p>Divergence is written as <span>\nabla \cdot</span>, so in our case, we know that we need to find a velocity field <span>\vec{v}</span> whose divergence equals the loss:</p><p>\tag{1.3}
\nabla \cdot \vec{v} = L(x, y)</p><p>Unfortunately there is no "inverse divergence" operator so we cannot easily invert this equation to find <span>\vec{v}</span> directly. But we <em>can</em> plug equation <span>(1.2)</span> in to equation <span>(1.3)</span> to yield:</p><p>\tag{1.4}
\nabla \cdot \nabla \Phi = L(x, y)</p><p>Which we read as <em>The divergence of the gradient of the potential field <span>\Phi</span> equals the loss</em>.</p><p>This equation comes up surprisingly frequently in many branches of physics and math. It is usually written in a more convenient shorthand:</p><p>\tag{1.5}
\nabla ^2 \Phi = L</p><p>Which you may recognize as <a href="https://mattferraro.dev/posts/poissons-equation">Poisson's Equation</a>!</p><p>This is fantastic news because Poisson's equation is <a href="https://mattferraro.dev/posts/poissons-equation#how-do-i-solve-it">extremely easy</a> to solve! If you aren't familiar with it, just think of this step like inverting a big matrix, or numerically integrating an ODE, or finding the square root of a real number. It's an intricate, tedious task that would be painful to do with a paper and pencil, but it's the kind of thing computers are <em>really</em> good at.</p><p>Now that we've written down the problem as Poisson's Equation, it is as good as solved. We can use any off the shelf solver, plug in our known <span>L(x, y)</span> using Neumann boundary conditions and boom, and out pops <span>\Phi(x,y)</span> as if by magic.</p><div><p><img alt="Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fclearly_cat_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fclearly_cat_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fclearly_cat_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Can you figure out why the cat appears so clearly in this 3D rendering of <span>\Phi</span>? What controls the brightness of each pixel in a render like this?</p><p>We plug <span>\Phi</span> in to Equation <span>(1.2)</span> to find <span>\vec{v}</span> and we take a look at the vector field:</p><div><p><img alt="Gradient of Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Disappointingly, it does not look like a kitty to me.</p><p>And technically we need to march our points in the direction of <em>negative</em> <span>\nabla L</span> if we want to <em>decrease</em> <span>L</span>. Here's <span>-\nabla L</span>:</p><div><p><img alt="Negative Gradient of Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fnegative_grad_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fnegative_grad_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fnegative_grad_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>But the good news is that this vector field is smooth and well-behaved. We simply march the grid points along this vector field and we'll get exactly what we need. </p><p>If you squint you can almost see how the bright background will expand and the cat's dark fur will shrink.</p><div><p><img alt="Image and Vector Field" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>We step all the lens grid points forward some small amount in the direction of <span>-\vec{v}</span>. After morphing the grid a tiny amount we recompute the loss function <span>L</span>, find a new <span>\Phi</span> and new <span>-\vec{v}</span>, and take another small step.</p><pre><code><span># In Julia-flavored psuedocode</span>
image <span>=</span> read_image<span>(</span><span>"cat.png"</span><span>)</span>
gray <span>=</span> convert_to_grayscale<span>(</span>image<span>)</span>
grid <span>=</span> create_initial_grid<span>(</span>gray<span>.</span>size <span>+</span> <span>1</span><span>)</span>

L <span>=</span> compute_loss<span>(</span>gray<span>,</span> grid<span>)</span>

<span>while</span> max<span>(</span>L<span>)</span> <span>&gt;</span> <span>0.01</span>
    ϕ <span>=</span> poisson_solver<span>(</span>L<span>,</span> <span>"neumann"</span><span>,</span> <span>0</span><span>)</span>
    v <span>=</span> compute_gradient<span>(</span>ϕ<span>)</span>
    grid <span>=</span> step_grid<span>(</span>grid<span>,</span> <span>-</span>v<span>)</span>
    L <span>=</span> compute_loss<span>(</span>gray<span>,</span> grid<span>)</span>
<span>end</span>
</code></pre><p>After three or four iterations the loss gets very small and we've got our morphed cells!</p><div><p><img alt="Grid After Warping" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side_warped.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side_warped.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side_warped.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Look at how this cat's chin ballooned out but her nose and forehead shrunk. Her left ear is noticably longer and thinner because the bright background had to grow to take up more light. Her pupils went from oblong to sharp.</p><p>Note that image on the right is just a screenshot of Fusion360's default mesh rendering with the wireframe turned on:</p><div><p><img alt="Screenshot of Fusion360" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2FFusion360.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2FFusion360.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2FFusion360.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>The reason it is darker in some areas is because the mesh is more tightly packed in those areas. Let's zoom in on the eye:</p><div><p><img alt="Zoom in on the Eye" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_eye.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_eye.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_eye.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Look at how detailed that is! We've managed to capture even the bright reflections in her eyes. Zooming in further to just the pupil:</p><div><p><img alt="Zoom in on the Pupil" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_pupil.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_pupil.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_pupil.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>We can see the fine structure of the grid cells. Our formulation of the problem is only concerned with cells as quadralaterals. The triangles you see are just an artifact of converting our quadralateral grid into a triangle mesh more suitable for other software to deal with.</p><p>So again, in summary:</p><div><p><img alt="Overall Flow" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Foverall_flow.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Foverall_flow.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Foverall_flow.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>If we follow these steps we will successfully morph our grid points. Now we've got to do some geometry!</p><h2 id="snells-law-and-normal-vectors">Snell's Law and Normal Vectors</h2><p>Snell's law tells us how light bends when passing from one material to another. </p><div><p><img alt="Snell's Law" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsnells_law_2.svg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsnells_law_2.svg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsnells_law_2.svg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>\tag{2.0}
\frac{\sin(\theta_2)}{\sin(\theta_1)} = \frac{n_1}{n_2}</p><p>Where <span>n_1 = 1.49</span> is the <a href="https://en.wikipedia.org/wiki/Refractive_index">Refractive Index</a> of acrylic and <span>n_2 = 1</span> is the refractive index of air. If we know <span>\theta_2</span>, Snell's Law gives us <span>\theta_1</span>.</p><p>Snell's law is not some arbitrary axiom of physics. It is a direct consequence of Fermat's <a href="https://en.wikipedia.org/wiki/Fermat%27s_principle">Principle of Least Time</a>, which is a fascinating and critical link between ray optics and wave optics. But that's a topic for another day.</p><p>In our case, each lens cell <span>(i, j)</span> has migrated to position <span>(x, y)</span>, and it needs to send its light to the image plane at <span>(u, v)</span>, which sits some distance away <span>d</span>.</p><p>We start by defining a 3D normal vector <span>\vec{N}(x, y)</span> which everywhere points normal to our heightmap <span>h(x, y)</span>.</p><div><p><img alt="Example Surface Normals" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsurface_normals.svg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsurface_normals.svg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsurface_normals.svg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Normal vectors always point perpendicular to the surface they start on. They generally encode meaning in their direction, not their length, so we're free to scale them to any length that is convenient for our purposes. Very often people choose to make their Normal vectors of length <span>1</span>.</p><p>But if we normalize <span>\vec{N}</span> so that its <span>z</span> coordinate is <span>-1</span>, we can write it:</p><p>\tag{2.1}
\vec{N} = (\frac{\partial{h}}{\partial{x}}, \frac{\partial{h}}{\partial{y}}, -1)</p><p>If you consider just the <span>x</span> and <span>y</span> components, we recognize that</p><p>\tag{2.2}
\vec{N}_{xy} = \nabla h</p><p>Which is a property often used in computer graphics applications, as well as geospatial applications involving <a href="https://en.wikipedia.org/wiki/Digital_elevation_model">Digital Elevation Models</a>.</p><p>Using Snell's Law, a small angle approximation, and a lot of tedious geometry, we find the <span>x</span> and <span>y</span> components of the normal vector <span>\vec{N}</span>:</p><p>\tag{2.3}
N_x(i, j) = \tan \frac{\tan^{-1} \left( \frac{u - x} {d} \right)} {(n_1 - n_2)}</p><p>\tag{2.4}
N_y(i, j) = \tan \frac{\tan^{-1} \left( \frac{v - y} {d} \right)} {(n_1 - n_2)}</p><p>There is nothing interesting about this derivation so I've skipped it here.</p><h2 id="finding-the-heightmap">Finding the Heightmap</h2><p>At this point we have our morphed grid cells and we've found all our surface normals. All we have to do is find a heightmap <span>h(x,y)</span> that has the required surface normals.</p><p>Unfortunately, this is not a problem that is solvable in the general case.</p><p>We could try to integrate the normals manually, starting at one corner and working our way down the grid, but this method does not usually result in a physically realizable object. </p><p>If the integral of the normals running left to right pulls your surface up, but the integral of the normals running top to bottom pulls your surface down, there is just no solution that results in a solid, unbroken surface.</p><p>A much better approach is to reach back to equation <span>(2.2)</span>, repeated here:</p><p>\tag{2.2}
\vec{N}_{xy} = \nabla h</p><p>And to take the divergence of both sides:</p><p>\tag{2.5}
\nabla \cdot \vec{N}_{xy} = \nabla \cdot \nabla h</p><p>Do you recognize the form of this equation? Adopting shorthand and swapping sides:</p><p>\tag{2.6}
\nabla ^2 h = \nabla \cdot \vec{N}_{xy}</p><p>We arrive at yet another instance of <a href="https://mattferraro.dev/posts/poissons-equation">Poisson's Equation</a>! We found <span>\vec{N}_{xy}</span> in the previous section, and calculating the divergence of a known vector field is easy:</p><p>\tag{2.7}
\nabla \cdot \vec{N}_{xy} = \left( \frac{\partial}{\partial{x}}, \frac{\partial}{\partial{y}} \right) \cdot (\vec{N}_x, \vec{N}_y) = \frac{\partial{\vec{N}_x}}{\partial{x}} + \frac{\partial{\vec{N}_y}}{\partial{y}}</p><p>In code it looks like:</p><pre><code>δx <span>=</span> <span>(</span>Nx<span>[</span>i<span>+</span><span>1</span><span>,</span> j<span>]</span> <span>-</span> Nx<span>[</span>i<span>,</span> j<span>]</span><span>)</span>
δy <span>=</span> <span>(</span>Ny<span>[</span>i<span>,</span> j<span>+</span><span>1</span><span>]</span> <span>-</span> Ny<span>[</span>i<span>,</span> j<span>]</span><span>)</span>
divergence<span>[</span>i<span>,</span> j<span>]</span> <span>=</span> δx <span>+</span> δy
</code></pre><p>All that's left is to plug our known <span>\nabla \cdot \vec{N}_{xy}</span> in to a Poisson solver with Neumann boundary conditions and out pops <span>h(x, y)</span>, ready to use!</p><p>Well, there's one thing left to improve. By modifying the height of each point we've actually changed the distance from each lens point to the image, so the lens-image distance is no longer a constant <span>d</span> it is actually a function <span>D(x,y)</span>. With our heightmap in hand we can easily calculate:</p><p>\tag{2.8}
D(x,y) = d - h(x,y)</p><p>And repeat the process by calculating new normals using <span>D(x,y)</span> instead of <span>d</span>, which lets us create a new heightmap.</p><p>We can loop this process and measure changes to ensure convergence, but in practice just 2 or 3 iterations is all you need:</p><pre><code><span># In Julia-flavored psuedocode</span>
d <span>=</span> <span>.2</span> <span># meters</span>
D <span>=</span> d <span>.</span><span>*</span> array_of_ones<span>(</span>n<span>,</span> n<span>)</span>

<span>for</span> i <span>in</span> <span>1</span><span>:</span><span>3</span>
    Nx<span>,</span> Ny <span>=</span> compute_normals<span>(</span>grid<span>,</span> D<span>)</span>
    divergence <span>=</span> compute_divergence<span>(</span>Nx<span>,</span> Ny<span>)</span>
    h <span>=</span> poisson_solver<span>(</span>divergence<span>,</span> <span>"neumann"</span><span>,</span> <span>0</span><span>)</span>
    D <span>=</span> copy<span>(</span>h<span>)</span>
<span>end</span>
</code></pre><p>The resulting heightmap can be converted to a solid object by adopting a triangular grid and closing off the back surface.</p><div><p><img alt="Final Object" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_1.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_1.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_1.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Note that the image looks mirrored when looking at it head on. That's because the heightmap forms the <em>back</em> surface of the Magic Window. The front surface is factory flat.</p><div><p><img alt="Final Object" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_2.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_2.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_2.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>The height differences are subtle but certainly enough to get the job done.</p><div><p><img alt="Finished Product" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinished_product.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinished_product.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinished_product.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><h2 id="manufacturing">Manufacturing</h2><p>The process of manufacturing our Magic Window is identical to carving any other 2.5D object.</p><p>We bring our object into Fusion360 or any other CAM software. We set up a roughing toolpath left to right, and a finishing toolpath top to bottom just like you find in most tutorials.</p><p>Any old CNC router or mill will work. I designed and built my own router last year. If you want to do the same I recommend you start <a href="https://mattferraro.dev/posts/cnc-router">here</a>.</p><p>I used a <span>\frac{1}{4}</span> inch diameter, ball-nosed, carbide bit for both roughing and finishing passes, which took 10 minutes and 90 minutes respectively.</p><div><p><img alt="On the Router" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fon_the_router.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fon_the_router.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fon_the_router.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>After carving the surface finish is rough and transluscent. We need to wet sand using <span>200, 400, 600, 1000</span> and <span>1500</span> grit sandpapers, then finish with a soft rag and some automotive polish. Sanding and polishing takes about half an hour for a <span>10 cm \times 10 cm</span> Magic Window.</p><div><p><img alt="After Sanding" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fafter_sanding.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fafter_sanding.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fafter_sanding.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><h2 id="acknowledgements">Acknowledgements</h2><p>All of the math for this post came from <a href="http://nishitalab.org/user/egaku/tog14/yue-continuous-caustics-lens.pdf">Poisson-Based Continuous Surface Generation for Goal-Based Caustics</a>, a phenomenal 2014 paper by Yue et al. If you continue this work in some way, please cite them.</p><h2 id="my-code">My Code</h2><p>My source code is available <a href="https://github.com/MattFerraro/causticsEngineering">here</a>. I am a novice at programming in Julia so if you have suggestions for how to improve this code, please reach out or make a pull request!</p><p><strong>Caveats</strong>: There are a lot of issues with my code. I confuse <span>x</span> and <span>y</span> in several places. I have extra negative signs that I inserted that make the code work but I don't know why. My units and notation are inconsistent throughout. The original paper suggests a better way of calculating loss but I didn't implement it because the naive way was easier, yet I rolled my own mesh utilities and Poisson solver because I enjoyed the challenge.</p><p>In short: To me this code is a fun side project. If you want to build a business off of this code you should probably hire someone who knows how to program professionally in Julia.</p><h2 id="licensing">Licensing</h2><p>I've posted all my code under the MIT license. Please feel free to use this code for anything you want, including hobbyist, educational, and commercial uses. I only ask that if you make something, please show me!</p><p>Except where otherwise attributed, all images in this blog post and the blog post itself are my own work that I license as <a href="https://creativecommons.org/licenses/by/2.0/">CC-BY</a>.</p><p>The cat in this post is named Mitski and she approves of you using her image as the new standard reference image for image processing papers. It's time to let Lenna <a href="https://en.wikipedia.org/wiki/Lenna#Criticism">retire</a>.</p><p>If you use my code to make your own Magic Windows, I'd love to see them! I'm on Twitter at <a href="https://twitter.com/mferraro89">@mferraro89</a>. Email me at <a href="mailto:mattferraro.dev@gmail.com">mattferraro.dev@gmail.com</a> and I will gladly help if you get stuck!</p><h2 id="one-last-thing">One Last Thing</h2><p>I know what you're thinking. <em>What about the hologram?!</em></p><p>Does the math above imply that a hologram will always be created, or is this one cat hologram just an incredible coincidence?</p><p>Well you see, I've discovered a truly marvelous proof of this, which this website's margin is unfortunately too narrow to contain :)</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Routine dental X-rays are not backed by evidence–experts want it to stop (200 pts)]]></title>
            <link>https://arstechnica.com/health/2024/10/do-you-really-need-those-routine-dental-x-rays-probably-not/</link>
            <guid>41842294</guid>
            <pubDate>Mon, 14 Oct 2024 21:37:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/health/2024/10/do-you-really-need-those-routine-dental-x-rays-probably-not/">https://arstechnica.com/health/2024/10/do-you-really-need-those-routine-dental-x-rays-probably-not/</a>, See on <a href="https://news.ycombinator.com/item?id=41842294">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2056298">
  
  <header>
  <div>
    <div>
      

      

      <p>
        The actual recommendations might surprise you—along with the state of modern dentistry.
      </p>

      
    </div>

          <div>
        <p><img width="1000" height="1000" src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-1000x1000.jpg" alt="" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-1000x1000.jpg 1000w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-150x150.jpg 150w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-500x500.jpg 500w" sizes="(max-width: 1000px) 100vw, 1000px">
        </p>
        <div>
    
    <p>
      An expert looking at a dental X-ray and saying "look at that unnecessary X-ray," probably.

              <span>
          Credit:

                      <a href="https://www.gettyimages.com/detail/photo/dentist-showing-a-patient-her-x-ray-royalty-free-image/907343114?phrase=dental+x+rays&amp;adppopup=true">
          
          Getty | MilanEXPO

                      </a>
                  </span>
          </p>
  </div>
      </div>
      </div>
</header>

  

  
      
    
    <div>
                      
                      
          
<p>Has your dentist ever told you that it's recommended to get routine dental X-rays every year? My (former) dentist's office did this year—in writing, even. And they claimed that the recommendation came from the American Dental Association.</p>
<p>It's a common refrain from dentists, but it's false. The American Dental Association <em>does not</em> recommend annual routine X-rays. And this is not new; it's been that way for well over a decade.</p>
<p><a href="https://www.ada.org/-/media/project/ada-organization/ada/ada-org/files/resources/library/oral-health-topics/dental_radiographic_examinations_2012.pdf?rev=fd33893f4d634cbaab92733c2313c354&amp;hash=45F728CEF900B5B654539635A9147AA9">The association's guidelines from 2012</a> recommended that adults who don't have an increased risk of dental caries (myself included) need only bitewing X-rays of the back teeth every two to three years. Even people with a higher risk of caries can go as long as 18 months between bitewings. The guidelines also note that X-rays should not be preemptively used to look for problems: "Radiographic screening for the purpose of detecting disease before clinical examination should not be performed," the guidelines read. In other words, dentists are supposed to examine your teeth <em>before</em> they take any X-rays.</p>
<p>But, of course, the 2012 guidelines are outdated—the latest ones go further. In <a href="https://jada.ada.org/article/S0002-8177(23)00734-1/fulltext">updated guidance published in April</a>, the ADA doesn't recommend any specific time window for X-rays at all. Rather, it emphasizes that patient exposure to X-rays should be minimized, and any X-rays should be clinically justified.</p>
<p>There's a good chance you're surprised. Dentistry's overuse of X-rays is a problem dentists do not appear eager to discuss—and would likely prefer to skirt. My former dentist declined to comment for this article, for example. And other dentists have been doing that for years. Nevertheless, the problem is well-established. A New York Times article from 2016, titled "<a href="https://www.nytimes.com/2016/07/26/upshot/you-probably-dont-need-dental-x-rays-every-year.html">You Probably Don’t Need Dental X-Rays Every Year</a>," quoted a dental expert noting the exact problem:</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>"Many patients of all ages receive bitewing X-rays far more frequently than necessary or recommended. And adults in good dental health can go a decade between full-mouth X-rays."</p>
<h2>Data is lacking</h2>
<p>The problem has bubbled up again in a series of commentary pieces published in JAMA Internal Medicine today. The pieces were all sparked by a viewpoint that Ars reported on in May, in which three dental and health experts highlighted that <a href="https://arstechnica.com/science/2024/05/do-you-need-a-dentist-visit-every-6-months-that-filling-the-data-is-weak/">many routine aspects of dentistry, including biannual cleanings, are not evidence-based</a> and that the industry is rife with overdiagnosis and overtreatment. That viewpoint, titled "<a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2818193">Too Much Dentistry</a>," also appeared in JAMA Internal Medicine.</p>
<p>The new pieces take a more specific aim at dental radiography. But, as in the May viewpoint, experts also blasted dentistry more generally for being out of step with modern medicine in its lack of data to support its practices—practices that continue amid financial incentives to overtreat and little oversight to stop it, they note.</p>
<p>In a piece titled "<a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2825067">Too Much Dental Radiography</a>," Sheila Feit, a retired medical expert based in New York, pointed out that using X-rays for dental screenings is not backed by evidence. "Data are lacking about outcomes," she wrote. If anything, the weak data we have makes it look ineffective. For instance, <a href="https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD014545/full">a 2021 systemic review</a> of 77 studies that included data on a total of 15,518 tooth sites or surfaces found that using X-rays to detect early tooth decay led to a high degree of false-negative results. In other words, it led to missed cases.</p>
<p>Feit called for gold-standard randomized clinical trials to evaluate the risks and benefits of X-ray screenings for patients, particularly adults at low risk of caries. "Financial aspects of dental radiography also deserve further study," Feit added. Overall, Feit called the May viewpoint "a timely call for evidence to support or refute common clinical dental practices."</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<h2>Dentistry without oversight</h2>
<p><a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2825065">In a response</a> published simultaneously in JAMA Internal Medicine, oral medicine expert Yehuda Zadik championed Feit's point, calling it "an essential discussion about the necessity and risks of routine dental radiography, emphasizing once again the need for evidence-based dental care."</p>
<p>Zadik, a professor of dental medicine at The Hebrew University of Jerusalem, noted that the overuse of radiography in dentistry is a global problem, one aided by dentistry's unique delivery:</p>
<p>"Dentistry is among the few remaining health care professions where clinical examination, diagnostic testing including radiographs, diagnosis, treatment planning, and treatment are all performed in place, often by the same care practitioner" Zadik wrote. "This model of care delivery prevents external oversight of the entire process."</p>
<p>While routine X-rays continue at short intervals, Zadik notes that current data "favor the reduction of patient exposure to diagnostic radiation in dentistry," while advancements in dentistry dictate that X-rays should be used at "longer intervals and based on clinical suspicion."</p>
<p>Though the digital dental X-rays often used today provide smaller doses of radiation than the film X-rays used in the past, radiation's harms are cumulative. Zadik emphasizes that with the primary tenet of medicine being "First, do no harm," any unnecessary X-ray is an unnecessary harm. Further, other technology can sometimes be used instead of radiography, including <a href="https://en.wikipedia.org/wiki/Electronic_apex_locator">electronic apex locators</a> for root canal procedures.</p>
<p>"Just as it is now unimaginable that, in the past, <a href="https://wi101.wisc.edu/the-rise-and-fall-of-shoe-fitting-fluroscopes/">shoe fittings for children were conducted using X-rays</a>, in the future it will be equally astonishing to learn that the fit of dental crowns was assessed using radiographic imaging," Zadik wrote.</p>

          
                  </div>
                    
        
          
    
    <div>
        
        
        
        <div>
          
          
<h2>X-rays do more harm than good in children</h2>
<p>Feit's commentary also prompted a reply from the three authors of the original May viewpoint: Paulo Nadanovsky, Ana Paula Pires dos Santos, and David Nunan. The three followed up on Feit's point that data is weak on whether X-rays are useful for detecting early decay, specifically white spot lesions. The experts raise the damning point that even if dental X-rays were shown to be good at doing that, there's still no evidence that that's good for patients.</p>
<p>"[T]here is no evidence that detecting white spot lesions, with or without radiographs, benefits patients," the researchers wrote. "Most of these lesions do not progress into dentine cavities," and there's no evidence that early treatments make a difference in the long run.</p>
<p>To bolster the point, the three note that data from children suggest that X-ray screening does more harm than good. <a href="https://bmcoralhealth.biomedcentral.com/articles/10.1186/s12903-021-01528-w">In a randomized clinical trial published in 2021</a>, 216 preschool children were split into two groups: one that received only a visual-tactile dental exam, while the others received both a visual-tactile exam and X-rays. The study found that adding X-rays caused more harm than benefit because the X-rays led to false positives and overdiagnosis of cavitated caries needing restorative treatment. The authors of the trial concluded that "visual inspection should be conducted alone in regular clinical practice."</p>
<p>Like Zadik, the three researchers note that screenings for decay and cavities are not the only questionable use of X-rays in dental practice. Other common dental and orthodontic treatments involving radiography—practices often used in children and teens—might also be unnecessary harms. They raise the argument against the <a href="https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD003879.pub5/full">preventive removal of wisdom teeth</a>, which is also <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1963310/">not backed by evidence</a>.</p>
<p>Like Feit, the three researchers reiterate the call for well-designed trials to back up or refute common dental practices.</p>


          
                  </div>

                  
          <div>
  <div>
          <p><a href="https://arstechnica.com/author/beth/"><img src="https://arstechnica.com/wp-content/uploads/2016/05/b.mole-45957.jpg" alt="Photo of Beth Mole"></a></p>
  </div>

  <div>
    

    <p>
      Beth is Ars Technica’s Senior Health Reporter. Beth has a Ph.D. in microbiology from the University of North Carolina at Chapel Hill and attended the Science Communication program at the University of California, Santa Cruz. She specializes in covering infectious diseases, public health, and microbes.
    </p>
  </div>
</div>


  


  


  
              </div>
  </article>


<div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/starship_flight5_catch1-768x432.jpg" alt="Listing image for first story in Most Read: SpaceX catches returning rocket in mid-air, turning a fanciful idea into reality" decoding="async" loading="lazy">
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla Optimus Bots Were Remotely Operated at Cybercab Event (210 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-10-14/tesla-s-optimus-robots-were-remotely-operated-at-cybercab-event</link>
            <guid>41842060</guid>
            <pubDate>Mon, 14 Oct 2024 21:10:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-10-14/tesla-s-optimus-robots-were-remotely-operated-at-cybercab-event">https://www.bloomberg.com/news/articles/2024-10-14/tesla-s-optimus-robots-were-remotely-operated-at-cybercab-event</a>, See on <a href="https://news.ycombinator.com/item?id=41842060">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Project Euler (153 pts)]]></title>
            <link>https://projecteuler.net/problem=912</link>
            <guid>41841581</guid>
            <pubDate>Mon, 14 Oct 2024 20:27:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://projecteuler.net/problem=912">https://projecteuler.net/problem=912</a>, See on <a href="https://news.ycombinator.com/item?id=41841581">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="problem">
<p>
Let $s_n$ be the $n$-th positive integer that does not contain three consecutive ones in its binary representation.<br>
For example, $s_1 = 1$ and $s_7 = 8$.
</p>
<p>
Define $F(N)$ to be the sum of $n^2$ for all $n\leq N$ where $s_n$ is odd. You are given $F(10)=199$.
</p>
<p>
Find $F(10^{16})$ giving your answer modulo $10^9+7$.
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Splitting engineering teams into defense and offense (124 pts)]]></title>
            <link>https://www.greptile.com/blog/how-we-engineer</link>
            <guid>41841366</guid>
            <pubDate>Mon, 14 Oct 2024 20:07:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.greptile.com/blog/how-we-engineer">https://www.greptile.com/blog/how-we-engineer</a>, See on <a href="https://news.ycombinator.com/item?id=41841366">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>October 14, 2024<!-- --> <span>(<!-- -->1<!-- -->d ago)</span></span></p><p>Author: <!-- -->Daksh Gupta</p></div><article><p>I’m Daksh, one of the co-founders of <u><a target="_blank" rel="noopener noreferrer" href="https://www.greptile.com/">Greptile</a></u>. We build AI that understands large codebases, which you can query via an API. Large software teams use it for things like AI-powered code reviews and diagnosing root causes for outages.</p>
<p>We are a team of 4 engineers. Our customers are often surprised to learn this, since they assume we must be much larger given the breadth of our product. While this is flattering, the truth is that our product is covered in warts, and our “lean” team is more a product of our inability to identify and hire great engineers, rather than an insistence on superhuman efficiency.</p>
<p>The result is that our product breaks more often than we’d like. The core functionality may remain largely intact but the periphery is often buggy, something we expect will improve only as our engineering headcount catches up to our product scope. Nevertheless, the reason we get anything done at all in these circumstances has to do with a specific way in which we structure our engineering team.</p>
<h2 id="event-driven-vs-long-running-processes"><a href="#event-driven-vs-long-running-processes"></a>Event-driven vs. long-running processes</h2>
<p>15 years ago, Paul Graham wrote about the “<u><a target="_blank" rel="noopener noreferrer" href="https://www.paulgraham.com/makersschedule.html">maker vs. manager schedule</a></u>”, the idea that makers, such as software developers, were different from managers in that they need long, uninterrupted hours to build great things. This essay resonated with engineers around the world who had been trying to squeeze their work in between endless mandatory meetings, and probably led to some sweeping changes at least at software-driven companies in favor of creating a “maker-schedule” for engineers.</p>
<p>Small startups don’t suffer from an excess of meetings and instead have a whole different problem. Customers!</p>
<p>Without dedicated technical support teams and usually with immature products, engineers take on a lot of support work - from hotfixes to building small features for large customers, to just helping customers navigate their products. With enough customers, there is very little time to build new features and make ambitious, complex changes to the codebase.</p>
<p>The engineering work that comes from customers, whether it is general support, bug fixes, or small modifications can be considered “event-driven” engineering.</p>
<p>The engineering work that includes longer-term (more than a week), ambitious projects, can be considered “long-running” engineering.</p>
<p>These two are at odds.</p>
<h2 id="the-fortress"><a href="#the-fortress"></a>The fortress</h2>
<p>Our solution to this problem has been simple, but so far, effective. This is not meant to be prescriptive. Every engineering team is different.</p>
<p>We instruct half the team (2 engineers) at a given point to work on long-running tasks in 2-4 week blocks. This could be refactors, big features, etc. During this time, they don’t have to deal with any support tickets or bugs. Their only job is to focus on getting their big PR out.</p>
<p>The other half of engineers must simply protect the first two from any support work, bugs, etc. Their job is to maintain a fortress around the long-running processes, by catching all the event-driven engineering work. At the end of the cycle, we swap.</p>
<h2 id="why-this-works"><a href="#why-this-works"></a>Why this works</h2>
<p>Remarkable things happen when you take distractions away from a craftsperson. They can spend more time in flow and keep a large amount of context on the “client-side” of their brains.</p>
<p>Critically, it takes only 1-2 short interruptions to dramatically reduce the amount of work an engineer can do in a day. This chart sums it up well.</p>
<p><img src="https://www.greptile.com/5-min.png" alt="Productivity chart"></p>
<div><p>Impact of interruptions on developer productivity</p></div>
<p>It follows then that it’s far more useful to isolate interruptions to a few people rather than disperse them to “keep everyone productive”. If you’re spending some amount of time on support, incrementally more time spent on support will not affect your productivity much.</p>


<h2 id="defenseoffense-engineering"><a href="#defenseoffense-engineering"></a>Defense/Offense engineering</h2>
<p>A mental model that I have found useful is to view event-driven engineering as “defensive” and long-running processes as “offensive”. This tracks nicely to the effect that each has.</p>
<p>Defensive engineering exists to maintain your product, whereas offensive engineering exists to expand it.</p>
<p>Defensive engineering more strongly correlates with your retention and customer satisfaction, whereas offensive engineering arguably correlates a little more strongly with your ability to acquire new customers.</p>
<h2 id="disclaimer"><a href="#disclaimer"></a>Disclaimer</h2>
<p>Not only am I not a professional engineering manager, this is also a very specific and usually ephemeral situation - a small team running a disproportionately fast growing product in a hyper-competitive and fast-evolving space. This is not advice, but rather an observation about how we run our engineering team.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bike Manufacturers Are Making Bikes Less Repairable (189 pts)]]></title>
            <link>https://www.ifixit.com/News/101675/bike-manufacturers-are-making-bikes-less-repairable</link>
            <guid>41840971</guid>
            <pubDate>Mon, 14 Oct 2024 19:27:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ifixit.com/News/101675/bike-manufacturers-are-making-bikes-less-repairable">https://www.ifixit.com/News/101675/bike-manufacturers-are-making-bikes-less-repairable</a>, See on <a href="https://news.ycombinator.com/item?id=41840971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
<p>The bicycle is probably the canonical example of something that anyone can fix. Spares from all brands are mostly interchangeable, and you can do most repairs with wrenches, screwdrivers, and Allen keys, or some fairly standard tools for bottom brackets and chainrings. But that’s all changing.</p>



<p>Just like cars, tractors, computers, and seemingly every other product category, bikes—and especially e-bikes—are going all black box on us. Instead of using standard parts that can easily be swapped or upgraded, bike makers are using more and more proprietary parts. At the same time,<a href="https://www.vice.com/en/article/mechanics-ask-walmart-major-bike-manufacturers-to-stop-making-and-selling-built-to-fail-bikes/"> cheap bikes are getting worse</a> and are designed to fail, or rather, they are not designed to last, which is pretty much the same thing.</p>


  <div>
      <p><span>Featured Guide</span></p><div>
          <div>
            <h3>How to Replace a Bike Chain With a Master Link</h3>
            <p>Bicycle chains sometimes need to be removed for…</p>
            <p><a href="https://www.ifixit.com/Guide/How+to+Replace+a+Bike+Chain+With+a+Master+Link/140441">Follow this Guide</a></p>
          </div>
          <p><a href="https://www.ifixit.com/Guide/How+to+Replace+a+Bike+Chain+With+a+Master+Link/140441"><img decoding="async" loading="lazy" src="https://guide-images.cdn.ifixit.com/igi/DdhLc1N4sRZfdMOW.medium" alt="How to Replace a Bike Chain With a Master Link"></a>
          </p>
        </div>
          </div>



<h3>Riding Away From Standardization</h3>



<p>For example, the bottom bracket—the tubular bearing assembly at the bottom of the frame that the pedal axle threads through—has long been a fairly standard part.</p>



<p>Over the years, and on different continents, there may have been a few thread sizes, but a cyclist could easily buy the right part for a surprisingly reasonable price. Just as important, you’ve been able to remove the bottom bracket with one of a few simple tools. Now, though, a bike shop has to keep 20+ tools on hand to deal with all the proprietary fittings.</p>



<figure><img fetchpriority="high" loading="lazy" decoding="async" width="4000" height="2672" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12.jpg 4000w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-1536x1026.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-2048x1368.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-1347x900.jpg 1347w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-450x300.jpg 450w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-768x512.jpg 768w" sizes="(max-width: 4000px) 100vw, 4000px"><figcaption>Standard parts make modding and repair easier. Photo Charlie Sorrel.</figcaption></figure>



<p>On electric bikes, things are even worse. Batteries are as non-standard as they are on cell phones. Instead of creating a standard, a kind of giant li-ion AA-equivalent for all bikes, you’re stuck buying non-standard sizes that you won’t be able to use on a new bike. This creates its own kind of lock-in, like the batteries on power tools, perhaps making you more likely to stick with the same brand within a family.</p>



<p>Then there are the apps. If you’re considering an e-bike that requires an app to function, or to change settings, do not buy that bike. When (not if) that app is abandoned, the bike will become at best a hobbled version of itself.</p>



<p>The result is that possibly the greenest, most efficient form of transport is turning into yet another source of landfill and e-waste. But why?</p>



<p>The cynical—and probably correct—take is that it boosts sales. By using proprietary parts, a bike manufacturer guarantees you have to go back to them for spares. And if those spares are not readily available, or are too expensive, then maybe you’ll just give up and buy a new bike instead.</p>



<p>Couple this with the explosion in new bike tech in recent years, which is itself designed to drive the desire to “upgrade” a perfectly good bike by replacing it with a new one, and you can see the attraction for the bean counters. Electronic, wireless gear shifters. Carbon-fiber seat posts. Active, self-adjusting suspension. Proprietary apps for changing key features like the power mode. All of these are superfluous for most riders, and add complexity to what is essentially a very simple, and pretty much perfect, machine.</p>



<h3>Buy Cheap, Buy Twice</h3>



<p>Bikes are getting ever more popular, in large part thanks to e-bikes, which make riding easy for people who would not otherwise consider cycling. That’s good news! Alas, to service this popularity, cheap and crappy bikes have proliferated.</p>



<p>“Budget bikes from ‘big box’ stores […] cost little ($150 to $250) because manufacturers cut corners. These bikes are built to fail: badly engineered, constructed from low-grade materials and fabricated in countries with inhumane labor standards,” writes cycling advocate <a href="https://nyc.streetsblog.org/2021/03/24/opinion-how-the-budget-bike-trap-creates-inequality-in-nyc">Josh Bicker on StreetsBlog NYC</a>.</p>



<p>These bikes are often broken out of the box. That’s bad if the buyer is a bike shop, and possibly deadly if the buyer is an inexperienced rider buying off the internet. Buying a used bike is a much better way to get a well-made machine for a good price. The downside of that is that you need to know what to look for, and how to bring that bike up to correct working order so that it is reliable and safe.</p>



<figure><img loading="lazy" decoding="async" width="4256" height="2832" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733.jpg 4256w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-1536x1022.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-2048x1363.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-1353x900.jpg 1353w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-450x300.jpg 450w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-768x512.jpg 768w" sizes="(max-width: 4256px) 100vw, 4256px"></figure>



<p>Fortunately, that’s possible with local bike kitchens and co-ops, or by asking your bike shop to look over the bike for you. This works best if that bike isn’t using a bunch of proprietary parts. You can’t reach into the spare parts bin for a brake caliper if your bike uses a proprietary disk-brake design, whether that’s a super-high-end model, or a closed unit with more plastic than metal.</p>



<p>Ideally all bikes would continue to draw on a pool of standard part-types, but manufacturers seem set on the opposite. This makes it all the more important that we have legislation to force them to make proprietary parts available for riders to buy themselves, not just selling to repair shops (if at all). And with the increase of technology in bikes, public repair information is also essential. You’re definitely not going to find powered-hub servicing guides on <a href="https://sheldonbrown.com/">Sheldon Brown</a>. Fingers crossed, but that legislation may indeed be on the way.</p>


  <div>
      <p><span>Featured Guide</span>

              <a href="https://www.ifixit.com/Guide/How+to+Repair+Electric+Bikes/75971"><img decoding="async" loading="lazy" src="https://guide-images.cdn.ifixit.com/igi/Od2deYTwnAFriE1g.medium" alt="How to Repair Electric Bikes"></a></p><h3>How to Repair Electric Bikes</h3>
        <p>What to inspect and do when looking to resolve…</p>
        <p><a href="https://www.ifixit.com/Guide/How+to+Repair+Electric+Bikes/75971">Follow this Guide</a></p>
          </div>



<h3>Batteries Should Be Interoperable</h3>



<p>Let’s get to batteries. After tires, tubes, cables and chains, the one thing on an electric bike that will 100% wear out and need replacing is the battery. Unlike most laptops, you can easily remove the battery from the bike. But forget about ordering up a standard replacement, because there isn’t one. The batteries are often shaped to fit the bike, but even those that clip into a section below the rear rack, or are otherwise independently-mounted vary in capacity, voltage, and current delivery.</p>



<p>That keeps replacement costs higher, but it also means that you are stuck if the manufacturer discontinues your battery. A bike that is otherwise in perfect working order might end up prematurely useless, or you will end up in the world of shonky spares from Amazon or another unreliable source.</p>



<figure><img loading="lazy" decoding="async" width="4000" height="3000" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4.jpg 4000w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4-1536x1152.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4-2048x1536.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4-1200x900.jpg 1200w" sizes="(max-width: 4000px) 100vw, 4000px"><figcaption>Sure, why not? Photo Charlie Sorrel.</figcaption></figure>



<p>The standard excuses apply. Original parts are designed to work safely together. Using non-official parts can be dangerous, etc. That may be true, but if so, it’s only because the parts were designed that way. The blame is with the manufacturer. It’s totally possible to design around standard batteries. Ask anyone who’s ever made a device that runs on AA batteries, or swapped a new 12-Volt lead-acid battery into a car.</p>



<p>We are 100% against this trend. A bike is an almost perfect machine, and e-bikes combined with public transit are probably the best way to get cars out of cities, and to make personal transport sustainable.</p>



<p>“There’s no machine known that is more efficient than a human on a bicycle,” Bill Nye, the science guy,<a href="https://bigthink.com/articles/bill-nye-the-city-of-the-future/"> told Big Think</a>. “Bowl of oatmeal, 30 miles — you can’t come close to that.”</p>



<p>And yet all that is being ruined in an effort to make us buy a new bike every few years, instead of repairing the ones we have. Newer, more exotic specs and components encourage us to “upgrade,” just like with smartphones, laptops, and cameras, and they also turn the perfect machine into an unknowable black box that is often not worth the cost of repair.</p>



<figure><img loading="lazy" decoding="async" width="695" height="460" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11084604/zcqutape5vm2kvphcxlu.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11084604/zcqutape5vm2kvphcxlu.jpg 695w, https://valkyrie.cdn.ifixit.com/media/2024/10/11084604/zcqutape5vm2kvphcxlu-300x200.jpg 300w" sizes="(max-width: 695px) 100vw, 695px"><figcaption>The Infinite Battery is endlessly repairable, and even looks cool.</figcaption></figure>



<p>One ray of hope here is the Infinite Battery by Gouach, <a href="https://www.indiegogo.com/projects/infinite-the-repairable-universal-ebike-battery#/">currently seeking development funding via Indiegogo</a>. It’s compatible with all major brands’ setups, and offers the usual power capacity and safety features, but it is totally user serviceable. All parts can be swapped out individually, and when the cells inside start to wear out, you can replace them individually, almost as if your bike ran on around 30 AA cells</p>



<p>If you can repair a bike, and use standard spares, either new or harvested from dead bikes, then a bike can essentially live forever. If the growing anti-repair practices of the bike industry are allowed to threaten that, then we no longer own our machines. We are essentially renting disposable gadgets instead.</p>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I Experience Web Today (374 pts)]]></title>
            <link>https://how-i-experience-web-today.com</link>
            <guid>41840931</guid>
            <pubDate>Mon, 14 Oct 2024 19:22:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://how-i-experience-web-today.com">https://how-i-experience-web-today.com</a>, See on <a href="https://news.ycombinator.com/item?id=41840931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>
                    https://example.com
                </p>
                <p><a href="https://how-i-experience-web-today.com/detail">
                    Then it shows me something
                </a></p><p>
                    Example Domain. This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for ...
                </p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Play 3.0 mini – A lightweight, reliable, cost-efficient Multilingual TTS model (207 pts)]]></title>
            <link>https://play.ht/news/introducing-play-3-0-mini/</link>
            <guid>41840872</guid>
            <pubDate>Mon, 14 Oct 2024 19:16:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://play.ht/news/introducing-play-3-0-mini/">https://play.ht/news/introducing-play-3-0-mini/</a>, See on <a href="https://news.ycombinator.com/item?id=41840872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Today we’re releasing our most capable and conversational voice model that can speak in 30+ languages using any voice or accent, with industry leading speed and accuracy. We’re also releasing 50+ new conversational AI voices across languages.</p>
<p>Our mission is to make voice AI accessible, personal and capable for all. Part of that mission is to advance the current state of interactive voice technology in conversational AI and elevate user experience.</p>
<p>When you’re building real time applications using TTS, a few things really matter – latency, reliability, quality and naturalness of speech. While we’ve been leading on latency and naturalness of speech with our previous generation models, Play 3.0 mini makes significant improvements to reliability and audio quality while still being the fastest and most conversational voice model.</p>
<p>Play3.0 mini is the first in a series of efficient multi-lingual AI text-to-speech models we plan to release over the coming months. Our goal is to make the models smaller and cost-efficient so they can be run on devices and at scale.</p>
<h2>Play 3.0 mini is our fastest, most conversational speech model yet</h2>
<p>3.0 mini achieves a mean latency of 189 milliseconds for TTFB, making it our fastest AI Text to Speech model. It supports text-in streaming from LLMs and audio-out streaming, and can be used via our HTTP REST API, websockets API or SDKs. 3.0 mini is also more efficient than Play 2.0, and runs inference 28% faster.</p>
<figure><p>
<iframe title="Introducing Play 3.0 mini - a new compact Text to Speech model for realtime Voice AI" width="640" height="360" src="https://www.youtube.com/embed/DusTj5NLC9w?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>
<h2>Play 3.0 mini supports 30+ languages across any voice</h2>
<p>Play 3.0 mini now supports more than 30+ languages, many with multiple male and female voice options out of the box.&nbsp; Our English, Japanese, Hindi, Arabic, Spanish, Italian, German, French, and Portuguese voices are available now for production use cases, and are available through our <a href="https://docs.play.ht/reference/api-getting-started">API</a> and on our <a href="https://www.play.ht/playground">playground</a>.&nbsp; Additionally, Afrikaans, Bulgarian, Croatian, Czech, Hebrew, Hungarian, Indonesian, Malay, Mandarin, Polish, Serbian, Swedish, Tagalog, Thai, Turkish, Ukrainian, Urdu, and Xhosa are available for testing.</p>
<h2>Play 3.0 mini is more accurate</h2>
<p>Our goal with Play 3.0 mini was to build the best TTS model for conversational AI. To achieve this, the model had to outperform competitor models in latency and accuracy while generating speech in the most conversational tone.</p>
<p>LLMs hallucinate and voice LLMs are no different. Hallucinations in voice LLMs can be in the form of extra or missed words or numbers in the output audio not part of the input text. Sometimes they can just be random sounds in the audio. This makes it difficult to use generative voice models reliably.</p>
<p>Here are some challenging text prompts that most TTS models struggle to get right –</p>
<blockquote>
<p><em>“Okay, so your flight UA2390 from San Francisco to Las Vegas on November 3rd is confirmed. And, your ticket number is F X 2, 3 9 A, 7 R T. The flight is scheduled to depart at 2:45 p.m. Is there anything else I can assist you with?”</em></p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t1.wav"></audio></figure>
<blockquote>
<p><em>“Now, when people RSVP, they can call the event coordinator at&nbsp;<strong>555 342 1234</strong>, but if they need more details, they can also call the backup number, which is&nbsp;<strong>416 789 0123</strong>.”</em></p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t2+(1).mp3"></audio></figure>
<blockquote>
<p>“<em>I’ve successfully processed your order and I’d like to confirm your product ID. It is A as in Alpha, 1, 2, 3, B as in Bravo, 5, 6, 7,&nbsp; Z as in Zulu, 8, 9, 0,&nbsp; X as in X-ray.</em>“</p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t3.mp3"></audio></figure>
<p>3.0 mini was finetuned specifically on a diverse dataset of alpha-numeric phrases to make it reliable for critical use cases where important information such as phone numbers, passport numbers, dates, currencies, etc. can’t be misread.</p>
<h2>Play 3.0 mini reads alphanumeric sequences more naturally</h2>
<p>We’ve trained the model to read numbers and acronyms just like humans do. The model adjusts its pace and slows down any alpha-numeric characters. Phone numbers for instance are read out with more natural pacing, and similarly all acronyms and abbreviations. This makes the overall conversational experience more natural.</p>
<blockquote>
<p><em>“Alright, let’s troubleshoot your laptop issue. First, let’s confirm your device’s ID so we’re on the same page. The I D is 894-d94-774-496-438-9b0-d2. Did I get that right?</em>“</p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t4.mp3"></audio></figure>
<h2>Play 3.0 mini achieves the best voice similarity for voice cloning</h2>
<p>When cloning voices, close often isn’t good enough.&nbsp; Play 3.0 voice cloning achieves state-of-the-art performance when cloning voices, ensuring accurate reproduction of accent, tone, and inflection of cloned voices.&nbsp; In benchmarking using a popular open source embedding model, we lead competitor models by a wide margin for similarity to the original voice.&nbsp; Try it for yourself by cloning your own voice, and talking to yourself on <a href="https://play.ai/" target="_blank" rel="noopener">https://play.ai</a>&nbsp;</p>
<h2>Websockets API Support</h2>
<p>3.0 mini’s API now supports websockets, which significantly reduces the overhead of opening and closing HTTP connections, and makes it easier than ever to enable text-in streaming from LLMs or other sources.</p>
<h2>Play 3.0 mini is a cost efficient model</h2>
<p>We’re happy to announce reduced pricing for our higher volume Startup and Growth tiers, and have now introduced a new Pro tier at $49 a month for businesses with more modest requirements.&nbsp; Check out our new pricing table <a href="https://play.ht/pricing/?planType=api">here</a>.</p>
<p>We look forward to seeing what you build with us!&nbsp; If you’ve custom, high volume requirements, feel free to <a href="https://play.ht/contact-us/">contact our sales team</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Funding Construction of Seven U.S. Nuclear Reactors (511 pts)]]></title>
            <link>https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624</link>
            <guid>41840769</guid>
            <pubDate>Mon, 14 Oct 2024 19:06:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624">https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624</a>, See on <a href="https://news.ycombinator.com/item?id=41840769">Hacker News</a></p>
Couldn't get https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[The three-page paper that shook philosophy: Gettiers in software engineering (214 pts)]]></title>
            <link>https://jsomers.net/blog/gettiers</link>
            <guid>41840390</guid>
            <pubDate>Mon, 14 Oct 2024 18:28:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jsomers.net/blog/gettiers">https://jsomers.net/blog/gettiers</a>, See on <a href="https://news.ycombinator.com/item?id=41840390">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              <p>In 1963, the philosopher Edmund Gettier published a three-page <a href="http://www-bcf.usc.edu/~kleinsch/Gettier.pdf">paper</a> in the journal <em>Analysis</em> that quickly became a classic in the field. Epistemologists going back to the Greeks had debated what it meant to know something, and in the Enlightenment, a definition was settled upon: to know something is to have a <em>justified true belief</em> about it:</p>

<ul>
<li><strong>justified</strong> in the sense of deriving from evidence</li>
<li><strong>true</strong>, because it doesn't make sense to "know" a falsehoood</li>
<li><strong>belief</strong>, i.e., a proposition in your head</li>
</ul>

<p>Gettier, in his tiny paper, upended the consensus. He asked "Is Justified True Belief Knowledge?" and offered three cases---soon to be known as "the Gettier cases"---that suggested you could have a JTB about something and yet still we would want to say you didn't <em>know</em> it. For that, he earned lasting fame, and his paper generated a literature <a href="https://en.wikipedia.org/wiki/Gettier_problem">all its own</a>.</p>

<h2>A Gettier case</h2>

<p>Supppose you're standing in a field and off in the distance you see a cow. But suppose that what you're actually looking at isn't a cow, it's just a convincingly lifelike model of a cow made out of papier-mâché. You're not seeing a cow, you're seeing the model. But then finally suppose that right behind the papier-mâché cow is a real cow!</p>

<p>On the one hand, you have a justified true belief that "there is a cow in the field": (1) you believe there's a cow in the field; (2) that belief didn't come from nowhere, but is justified by your seeing something that looks exactly like a cow; (3) and there is, in fact, a cow in the field. Still, we wouldn't want to say that you <em>know</em> there's a cow in the field, because in a sense you got lucky: by a strange coincidence, there happened to be a real cow there---a cow you knew nothing about.</p>

<h2>In software engineering</h2>

<p>At my old company, <a href="http://genius.com/">Genius</a>, the CTO---who'd studied philosophy as an undergrad---was obsessed with these Gettier cases. He called them "gettiers" for short. So we used to talk about gettiers all the time, no doubt in part just because it felt clever to talk about them, but also because when you're a programmer, you run into things that feel like Gettier cases with unusual frequency. And once you have a name for them, you start seeing them everywhere.</p>

<p>Here's a recent example. I was working on a web application that used a client-side framework that had been developed in-house. My app was a little search engine, and in my latest pull request, I'd made it so that when you hit Enter in the search field, the field lost focus, so that folks who like to browse the web via their keyboard wouldn't have to manually escape from the input box.</p>

<p>When I released the new version, I noticed that I'd broken the autofocusing of the search field that was supposed to happen on pageload. I started poking around, only to discover that I couldn't seem to get the correct behavior back. No matter what code I changed, which lines I commented out, how many times I hard-refreshed the browser, etc., I couldn't get the autofocus to work.</p>

<p>What had actually happened is that a coworker of mine had made a change to the framework itself, which changed how certain events were bound to the root DOM element, and as a result broke the "autofocus" attribute. At some point, I did a routine rebase on top of this change (and many other unrelated changes). Which meant that when I deployed my little pull request, I was <em>also</em> deploying a bug I had nothing to do with---one that ended up breaking autofocus. It only appeared as though my changes caused the problem, because I'd edited some code having to do with focus in the search field.</p>

<p>Note that I had a justified belief that "the pull request I just deployed broke autofocus on the production site," and in fact my change <em>did</em> break it---making the belief true. But the break actually happened for a completely different reason!</p>

<p>(Yes, I should have caught the bug in testing, and in fact I did notice some odd behavior. But making software is hard!)</p>

<p>Here's another example. (This one's from a long time ago, so the details might be a bit off.) A user once reported that on-site messages were no longer generating email notifications, and I was asked to investigate. Soon, I discovered that someone had recently pushed a change to the code that handled emails in our web app; the change seemed to introduce a bug that was responsible for the broken behavior. But---gettier!---the email service that the code relied on had itself gone down, at almost the exact same time that the change was released. I could have had a JTB that the code change had caused the emails to stop delivering, but still we wouldn't want to say I "knew" this was the cause, because it was actually the service outage that was directly responsible.</p>

<h2>A new term of art</h2>

<p>A philosopher might say that these aren't bona fide Gettier cases. True gettiers are rare. But it's still a useful idea, and it became something of a term of art at Genius---and has stuck with me since---because it's a good name for one of the trickiest situations you can get into as a programmer: a problem has multiple potential causes, and you have every reason to believe in one of them, even though another is secretly responsible.</p>

<p>Having a term for these tricky cases allows you, I think, to be ever-so-slightly more alert to them. You can be a better developer this way. As I've spent more time writing software, I've gotten better at sensing when my assumptions are probably wrong---when something gettieresque might be going on: have I forgotten to clear the cache? Am I working off the wrong branch? Am I even hitting this code path?</p>

<p>Software is a complex and ephemeral business. More than most people, developers are daily faced with bizarre epistemological problems. It helps to be able to distinguish a cow in the field from, well, a gettier.</p>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Commonly used arm positions can overestimate blood pressure readings: study (264 pts)]]></title>
            <link>https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html</link>
            <guid>41840023</guid>
            <pubDate>Mon, 14 Oct 2024 17:57:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html">https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html</a>, See on <a href="https://news.ycombinator.com/item?id=41840023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/johns-hopkins-medicine-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/johns-hopkins-medicine-1.jpg" data-sub-html="Researchers say their study findings underscore the importance of adhering to clinical guidelines calling for firm arm support on a desk or other surface when measuring blood pressure. Credit: Tammy Brady">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/johns-hopkins-medicine-1.jpg" alt="Johns Hopkins Medicine study finds commonly used arm positions can substantially overestimate blood pressure readings" title="Researchers say their study findings underscore the importance of adhering to clinical guidelines calling for firm arm support on a desk or other surface when measuring blood pressure. Credit: Tammy Brady" width="800" height="529">
             <figcaption>
                Researchers say their study findings underscore the importance of adhering to clinical guidelines calling for firm arm support on a desk or other surface when measuring blood pressure. Credit: Tammy Brady
            </figcaption>        </figure>
    </div><p>A study led by Johns Hopkins Medicine researchers concludes that commonly used ways of positioning the patient's arm during blood pressure (BP) screenings can substantially overestimate test results and may lead to a misdiagnosis of hypertension.</p>

                                        
                                                                                  
                                         

                                                                                                                                    <p>In a report on the <a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/10.1001/jamainternmed.2024.5213" target="_blank">study</a>, which was published Oct. 7 in <i>JAMA Internal Medicine</i>, investigators examined the effects of three different arm positions: an arm supported on a desk, arm supported on a lap, and an unsupported arm hanging at the patient's side.</p>
<p>Researchers found that lap support overestimated <a href="https://medicalxpress.com/tags/systolic+pressure/" rel="tag">systolic pressure</a> (the top number in a BP reading) by nearly 4 mmHg, and an unsupported arm hanging at the side overestimated systolic pressure by nearly 7 mmHg.</p>
<p>The findings confirm that arm position makes a "huge difference" when it comes to an accurate <a href="https://medicalxpress.com/tags/blood+pressure/" rel="tag">blood pressure</a> measurement, says Tammy Brady, M.D., Ph.D., vice chair for <a href="https://medicalxpress.com/tags/clinical+research/" rel="tag">clinical research</a> in the Department of Pediatrics at the Johns Hopkins University School of Medicine, medical director of the pediatric hypertension program at Johns Hopkins Children's Center, deputy director of the Welch Center for Prevention, Epidemiology, and Clinical Research and senior author of the study.</p>
<p>And they underscore the importance of adhering to clinical guidelines calling for firm support on a desk or other surface when measuring blood pressure, the investigators add.</p>

                                                                                                                                                         
                                                                                                                                                                                                <p>According to the American Heart Association, nearly half of U.S. adults have elevated blood pressure, a diagnosis made when the measured force of blood flowing through blood vessels is higher than what is generally considered normal, on average 120/80.*</p>
<p>Untreated, <a href="https://medicalxpress.com/tags/high+blood+pressure/" rel="tag">high blood pressure</a> increases the risk of stroke, heart attack and other serious cardiovascular conditions. Because hypertension may cause minimal or no symptoms, early and frequent screening during routine checkups is considered the cornerstone of hypertension management.</p>
<p>In most cases, lifestyle changes such as weight loss, healthy diets and exercise, as well as therapy with any of a variety of medications, can keep BP under control.</p>
<p>The latest clinical practice guidelines from the American Heart Association emphasize several key steps for an <a href="https://medicalxpress.com/tags/accurate+measurement/" rel="tag">accurate measurement</a>—including <a href="https://clinicalconnection.hopkinsmedicine.org/videos/cuff-size-matters-for-blood-pressure-monitoring-and-device-accuracy" target="_blank">appropriate cuff size</a>, back support, feet flat on the floor with legs uncrossed, and an appropriate arm position, in which the middle of an adjustable BP cuff is positioned at mid-heart level on an arm supported on a desk or table.</p>
<p>Despite these recommendations, the researchers say BP is too often measured with patients seated on an exam table without any, or inadequate, arm support. In some cases, a clinician holds the arm, or the patient holds an arm in their lap.</p>

                                                                                                                                            <p>In the new Johns Hopkins study, the researchers recruited 133 adult participants (78% Black, 52% female) between Aug. 9, 2022, and June 1, 2023. Study participants, who ranged from age 18 to 80, were sorted at random into one of six possible groups that differed by order of the three seated arm positions.</p>
<p>Measurements were taken during a single visit between 9 a.m. and 6 p.m. Before BP measures were taken, all participants first emptied their bladders and then walked for two minutes to mimic a typical clinical scenario in which people walk into a clinic or office before screening takes place. They then underwent a five-minute, seated rest period with their backs and feet supported.</p>
<p>Each person, wearing an upper arm BP cuff selected and sized based on their upper arm size, had three sets of triplicate measurements taken with a digital blood pressure device 30 seconds apart.</p>
<p>Upon completion of each set of three measurements, the cuff was removed, participants walked for two minutes and rested for five minutes. On the same visit, they then underwent a fourth set of triplicate measurements with their arm supported on a desk, a set used to account for well-known variations in BP readings. All of the measurements were conducted in a quiet and private space, and participants were asked not to talk to researchers or use their phones during the screening.</p>
<p>Researchers found that BP measurements obtained with arm positions frequently used in clinical practice—an arm on the lap or unsupported at the side—were markedly higher than those obtained when the arm was supported on a desk, the standard, recommended arm position.</p>
<p>Supporting the arm on the lap overestimated systolic BP—the top number of a reading, or the force of blood flow when pumped out of the heart, by 3.9 mmHg and diastolic blood pressure—the bottom number, or the pressure in the arteries when the heart rests between beats, by 4.0 mmHg. An unsupported arm at the side overestimated systolic by 6.5 mmHg and diastolic by 4.4 mmHg.</p>
<p>"If you are consistently measuring blood pressure with an unsupported arm, and that gives you an overestimated BP of 6.5 mmHg, that's a potential difference between a systolic BP of 123 and 130, or 133 and 140—which is considered stage 2 hypertension," says Sherry Liu, M.H.S., an epidemiology research coordinator at the Welch Center for Prevention, Epidemiology, and Clinical Research, Department of Epidemiology, at Johns Hopkins Bloomberg School of Public Health and study author.</p>
<p>Investigators caution that their study results may only apply during screenings with automated BP devices, and may not apply to readings done with other BP devices.</p>
<p>However, Brady says, the findings suggest that clinicians need to pay better attention to best practice guidelines, and that patients "must advocate for themselves in the clinical setting and when measuring their BP at home."</p>
<h2>*Current blood pressure guidelines and variability</h2>
<p>According to the 2017 guidelines from the American Heart Association, normal blood pressure is less than 120/80 mmHg. Blood pressure readings between 120-129/80 mmHg are classified as elevated, with hypertension being diagnosed at 130/80 mmHg or higher.</p>
<p>It's important to note that blood pressure can vary due to factors such as stress, diet, caffeine intake, and smoking. Therefore, to obtain an accurate reading, it is crucial to measure blood pressure under consistent conditions, following clinical guidelines.</p>

                                                                                                                                                                            
                                        											<div>
												                                                    <p><strong>More information:</strong>
                                                    Hairong Liu et al, Arm Position and Blood Pressure Readings, <i>JAMA Internal Medicine</i> (2024). DOI: 10.1001/jamainternmed.2024.5213 , <a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/10.1001/jamainternmed.2024.5213" target="_blank">jamanetwork.com/journals/jamai … ainternmed.2024.5213</a>
																								
																								</p>
																							</div>
                                        											
																					
                                                                                                                        
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Commonly used arm positions can substantially overestimate blood pressure readings, study finds (2024, October 7)
                                                 retrieved 14 October 2024
                                                 from https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PayloadCMS: Open-Source, Fullstack Next.js Framework (162 pts)]]></title>
            <link>https://github.com/payloadcms/payload</link>
            <guid>41839994</guid>
            <pubDate>Mon, 14 Oct 2024 17:55:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/payloadcms/payload">https://github.com/payloadcms/payload</a>, See on <a href="https://news.ycombinator.com/item?id=41839994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><p><a href="https://payloadcms.com/" rel="nofollow"><img width="100%" src="https://github.com/payloadcms/payload/raw/main/packages/payload/src/admin/assets/images/github-banner-alt.jpg?raw=true" alt="Payload headless CMS Admin panel built with React"></a></p></div>
<p dir="auto">
  <a href="https://github.com/payloadcms/payload/actions"><img alt="GitHub Workflow Status" src="https://camo.githubusercontent.com/aa40d8228c4f55ab9abc8872111aa72bfafd26727ae1d051c244bc243029e109/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f7061796c6f6164636d732f7061796c6f61642f6d61696e2e796d6c3f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/github/actions/workflow/status/payloadcms/payload/main.yml?style=flat-square"></a>
  &nbsp;
  <a href="https://discord.gg/payload" rel="nofollow"><img alt="Discord" src="https://camo.githubusercontent.com/e9df3d6912b718b05305520e56871d24ffcbc53fd43df018b74822e0c48f0be4/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3936373039373538323732313537323933343f6c6162656c3d446973636f726426636f6c6f723d373238396461267374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/discord/967097582721572934?label=Discord&amp;color=7289da&amp;style=flat-square"></a>
  &nbsp;
  <a href="https://www.npmjs.com/package/payload" rel="nofollow"><img alt="npm" src="https://camo.githubusercontent.com/aa42c043a19ec19a1ffd1579b6ec06392ffc7e5929490f8310af0178218c867f/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f7061796c6f61643f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/npm/v/payload?style=flat-square"></a>
  &nbsp;
  <a href="https://twitter.com/payloadcms" rel="nofollow"><img src="https://camo.githubusercontent.com/ef66e825f06cba28732876c654ca76f0f2f59ac6b97ab8fab70229394244a5f1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f666f6c6c6f772d7061796c6f6164636d732d3144413146323f6c6f676f3d74776974746572267374796c653d666c61742d737175617265" alt="Payload Twitter" data-canonical-src="https://img.shields.io/badge/follow-payloadcms-1DA1F2?logo=twitter&amp;style=flat-square"></a>
</p>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">
<a href="https://payloadcms.com/docs/getting-started/what-is-payload" rel="nofollow"><strong>Explore the Docs</strong></a>&nbsp;·&nbsp;<a href="https://payloadcms.com/community-help" rel="nofollow"><strong>Community Help</strong></a>&nbsp;·&nbsp;<a href="https://demo.payloadcms.com/" rel="nofollow"><strong>Try Live Demo</strong></a>&nbsp;·&nbsp;<a href="https://github.com/payloadcms/payload/discussions/1539" data-hovercard-type="discussion" data-hovercard-url="/payloadcms/payload/discussions/1539/hovercard"><strong>Roadmap</strong></a>&nbsp;·&nbsp;<a href="https://www.g2.com/products/payload-cms/reviews#reviews" rel="nofollow"><strong>View G2 Reviews</strong></a>
</h4><a id="user-content-explore-the-docscommunity-helptry-live-demoroadmapview-g2-reviews" aria-label="Permalink: Explore the Docs&nbsp;·&nbsp;Community Help&nbsp;·&nbsp;Try Live Demo&nbsp;·&nbsp;Roadmap&nbsp;·&nbsp;View G2 Reviews" href="#explore-the-docscommunity-helptry-live-demoroadmapview-g2-reviews"></a></p>
<hr>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">🎉 <strong>Payload 3.0 beta released!</strong> You can now deploy Payload fully in any Next.js app folder. Read more in the <a href="https://payloadcms.com/blog/30-beta-install-payload-into-any-nextjs-app-with-one-line" rel="nofollow"><strong>announcement post</strong></a>.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Benefits over a regular CMS</h3><a id="user-content-benefits-over-a-regular-cms" aria-label="Permalink: Benefits over a regular CMS" href="#benefits-over-a-regular-cms"></a></p>
<ul dir="auto">
  <li>Don’t hit some third-party SaaS API, hit your own API</li>
  <li>Use your own database and own your data</li>
  <li>It's just Express - do what you want outside of Payload</li>
  <li>No need to learn how Payload works - if you know JS, you know Payload</li>
  <li>No vendor lock-in</li>
  <li>Avoid microservices hell - get everything (even auth) in one place</li>
  <li>Never touch ancient WP code again</li>
  <li>Build faster, never hit a roadblock</li>
  <li>Both admin and backend are 100% extensible</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">☁️ Deploy instantly with Payload Cloud.</h2><a id="user-content-️-deploy-instantly-with-payload-cloud" aria-label="Permalink: ☁️ Deploy instantly with Payload Cloud." href="#️-deploy-instantly-with-payload-cloud"></a></p>
<p dir="auto">Create a cloud account, connect your GitHub, and <a href="https://payloadcms.com/new" rel="nofollow">deploy in minutes</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Get started by self-hosting completely free, forever.</h2><a id="user-content--get-started-by-self-hosting-completely-free-forever" aria-label="Permalink: 🚀 Get started by self-hosting completely free, forever." href="#-get-started-by-self-hosting-completely-free-forever"></a></p>
<p dir="auto">Before beginning to work with Payload, make sure you have all of the <a href="https://payloadcms.com/docs/getting-started/installation" rel="nofollow">required software</a>.</p>
<div data-snippet-clipboard-copy-content="npx create-payload-app@latest"><pre lang="text"><code>npx create-payload-app@latest
</code></pre></div>
<p dir="auto">Alternatively, it only takes about five minutes to <a href="https://payloadcms.com/docs/getting-started/installation#from-scratch" rel="nofollow">create an app from scratch</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🖱️ One-click templates</h2><a id="user-content-️-one-click-templates" aria-label="Permalink: 🖱️ One-click templates" href="#️-one-click-templates"></a></p>
<p dir="auto">Jumpstart your next project by starting with a pre-made template. These are production-ready, end-to-end solutions designed to get you to market as fast as possible.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/payloadcms/payload/tree/main/templates/ecommerce">🛒 E-Commerce</a></h3><a id="user-content--e-commerce" aria-label="Permalink: 🛒 E-Commerce" href="#-e-commerce"></a></p>
<p dir="auto">Eliminate the need to combine Shopify and a CMS, and instead do it all with Payload + Stripe. Comes with a beautiful, fully functional front-end complete with shopping cart, checkout, orders, and much more.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/payloadcms/payload/tree/main/templates/website">🌐 Website</a></h3><a id="user-content--website" aria-label="Permalink: 🌐 Website" href="#-website"></a></p>
<p dir="auto">Build any kind of website, blog, or portfolio from small to enterprise. Comes with a beautiful, fully functional front-end complete with posts, projects, comments, and much more.</p>
<p dir="auto">We're constantly adding more templates to our <a href="https://github.com/payloadcms/payload/tree/main/templates">Templates Directory</a>. If you maintain your own template, consider adding the <code>payload-template</code> topic to your GitHub repository for others to find.</p>
<ul dir="auto">
<li><a href="https://github.com/payloadcms/payload/tree/main/templates">Official Templates</a></li>
<li><a href="https://github.com/topics/payload-template">Community Templates</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ Features</h2><a id="user-content--features" aria-label="Permalink: ✨ Features" href="#-features"></a></p>
<ul dir="auto">
<li>Completely free and open-source</li>
<li><a href="https://payloadcms.com/docs/graphql/overview" rel="nofollow">GraphQL</a>, <a href="https://payloadcms.com/docs/rest-api/overview" rel="nofollow">REST</a>, and <a href="https://payloadcms.com/docs/local-api/overview" rel="nofollow">Local</a> APIs</li>
<li><a href="https://payloadcms.com/docs/admin/overview" rel="nofollow">Easily customizable ReactJS Admin</a></li>
<li><a href="https://payloadcms.com/docs/production/deployment" rel="nofollow">Fully self-hosted</a></li>
<li><a href="https://payloadcms.com/docs/authentication/overview" rel="nofollow">Extensible Authentication</a></li>
<li><a href="https://payloadcms.com/docs/upload/overview" rel="nofollow">Local file storage &amp; upload</a></li>
<li><a href="https://payloadcms.com/docs/versions/overview" rel="nofollow">Version History and Drafts</a></li>
<li><a href="https://payloadcms.com/docs/configuration/localization" rel="nofollow">Field-based Localization</a></li>
<li><a href="https://payloadcms.com/docs/fields/blocks" rel="nofollow">Block-based Layout Builder</a></li>
<li><a href="https://payloadcms.com/docs/fields/rich-text" rel="nofollow">Extensible SlateJS rich text editor</a></li>
<li><a href="https://payloadcms.com/docs/fields/array" rel="nofollow">Array field type</a></li>
<li><a href="https://payloadcms.com/docs/fields/overview#conditional-logic" rel="nofollow">Field conditional logic</a></li>
<li>Extremely granular <a href="https://payloadcms.com/docs/access-control/overview" rel="nofollow">Access Control</a></li>
<li><a href="https://payloadcms.com/docs/hooks/overview" rel="nofollow">Document and field-level hooks</a> for every action Payload provides</li>
<li>Built with Typescript &amp; very Typescript-friendly</li>
<li>Intensely fast API</li>
<li>Highly secure thanks to HTTP-only cookies, CSRF protection, and more</li>
</ul>
<p dir="auto"><a href="https://github.com/payloadcms/payload/discussions"><strong>Request Feature</strong></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🗒️ Documentation</h2><a id="user-content-️-documentation" aria-label="Permalink: 🗒️ Documentation" href="#️-documentation"></a></p>
<p dir="auto">Check out the <a href="https://payloadcms.com/docs/getting-started/what-is-payload" rel="nofollow">Payload website</a> to find in-depth documentation for everything that Payload offers.</p>
<p dir="auto">Migrating from v1 to v2? Check out the <a href="https://github.com/payloadcms/payload/releases/tag/v2.0.0">2.0 Release Notes</a> on how to do it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙋 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🙋 Contributing" href="#-contributing"></a></p>
<p dir="auto">If you want to add contributions to this repository, please follow the instructions in <a href="https://github.com/payloadcms/payload/blob/main/CONTRIBUTING.md">contributing.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📚 Examples</h2><a id="user-content--examples" aria-label="Permalink: 📚 Examples" href="#-examples"></a></p>
<p dir="auto">The <a href="https://github.com/payloadcms/payload/blob/main/examples">Examples Directory</a> is a great resource for learning how to setup Payload in a variety of different ways, but you can also find great examples in our blog and throughout our social media.</p>
<p dir="auto">If you'd like to run the examples, you can either copy them to a folder outside this repo or run them directly by (1) navigating to the example's subfolder (<code>cd examples/your-example-folder</code>) and (2) using the <code>--ignore-workspace</code> flag to bypass workspace restrictions (e.g., <code>pnpm --ignore-workspace install</code> or <code>pnpm --ignore-workspace dev</code>).</p>
<p dir="auto">You can see more examples at:</p>
<ul dir="auto">
<li><a href="https://github.com/payloadcms/payload/blob/main/examples">Examples Directory</a></li>
<li><a href="https://payloadcms.com/blog" rel="nofollow">Payload Blog</a></li>
<li><a href="https://www.youtube.com/@payloadcms" rel="nofollow">Payload YouTube</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔌 Plugins</h2><a id="user-content--plugins" aria-label="Permalink: 🔌 Plugins" href="#-plugins"></a></p>
<p dir="auto">Payload is highly extensible and allows you to install or distribute plugins that add or remove functionality. There are both officially-supported and community-supported plugins available. If you maintain your own plugin, consider adding the <code>payload-plugin</code> topic to your GitHub repository for others to find.</p>
<ul dir="auto">
<li><a href="https://github.com/orgs/payloadcms/repositories?q=topic%3Apayload-plugin">Official Plugins</a></li>
<li><a href="https://github.com/topics/payload-plugin">Community Plugins</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚨 Need help?</h2><a id="user-content--need-help" aria-label="Permalink: 🚨 Need help?" href="#-need-help"></a></p>
<p dir="auto">There are lots of good conversations and resources in our Github Discussions board and our Discord Server. If you're struggling with something, chances are, someone's already solved what you're up against. 👇</p>
<ul dir="auto">
<li><a href="https://github.com/payloadcms/payload/discussions">GitHub Discussions</a></li>
<li><a href="https://github.com/payloadcms/payload/issues">GitHub Issues</a></li>
<li><a href="https://t.co/30APlsQUPB" rel="nofollow">Discord</a></li>
<li><a href="https://payloadcms.com/community-help" rel="nofollow">Community Help</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">⭐ Like what we're doing? Give us a star</h2><a id="user-content--like-what-were-doing-give-us-a-star" aria-label="Permalink: ⭐ Like what we're doing? Give us a star" href="#-like-what-were-doing-give-us-a-star"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/38d5e6db344c163aaaae2a559865ff61aca0b9832f4e83bb54012434be974127/68747470733a2f2f636d732e7061796c6f6164636d732e636f6d2f6d656469612f7061796c6f61642d6769746875622d737461722e676966"><img src="https://camo.githubusercontent.com/38d5e6db344c163aaaae2a559865ff61aca0b9832f4e83bb54012434be974127/68747470733a2f2f636d732e7061796c6f6164636d732e636f6d2f6d656469612f7061796c6f61642d6769746875622d737461722e676966" alt="payload-github-star" data-animated-image="" data-canonical-src="https://cms.payloadcms.com/media/payload-github-star.gif"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">👏 Thanks to all our contributors</h2><a id="user-content--thanks-to-all-our-contributors" aria-label="Permalink: 👏 Thanks to all our contributors" href="#-thanks-to-all-our-contributors"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f296954b37ac59ab7f397c85901c87a419bba668dbcef88e624ceb7f8218d7a8/68747470733a2f2f636f6e7472696275746f72732d696d672e7765622e6170702f696d6167653f7265706f3d7061796c6f6164636d732f7061796c6f6164"><img src="https://camo.githubusercontent.com/f296954b37ac59ab7f397c85901c87a419bba668dbcef88e624ceb7f8218d7a8/68747470733a2f2f636f6e7472696275746f72732d696d672e7765622e6170702f696d6167653f7265706f3d7061796c6f6164636d732f7061796c6f6164" data-canonical-src="https://contributors-img.web.app/image?repo=payloadcms/payload"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Response to DHH (155 pts)]]></title>
            <link>https://ma.tt/2024/10/on-dhh/</link>
            <guid>41839864</guid>
            <pubDate>Mon, 14 Oct 2024 17:42:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ma.tt/2024/10/on-dhh/">https://ma.tt/2024/10/on-dhh/</a>, See on <a href="https://news.ycombinator.com/item?id=41839864">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">

			
				
<article id="post-128140">
	<!-- .entry-header -->

		<div>
		
<p>I’ll just remind everyone at the start that this is a respectful debate, and DHH and I tried to get on a call but couldn’t because we were both traveling.</p>



<p>However, “<a href="https://world.hey.com/dhh/automattic-is-doing-open-source-dirty-b95cf128">Automattic is doing open source dirty</a>” is an abomination of a headline, and David’s second post <a href="https://world.hey.com/dhh/open-source-royalty-and-mad-kings-a8f79d16">Open source royalty and mad kings</a>, is just sloppy. So I’m forced to reply publicly:</p>



<p>DHH claims to be an expert on open source, but his toxic personality and inability to scale teams means that although he has invented about half a trillion dollars worth of good ideas, most of the value has been captured by others. Let’s look at 37signals portfolio:</p>



<ul>
<li><a href="https://www.hey.com/">Hey</a>, proprietary, some sort of email / calendar / blogging thing that almost no one uses. It’s trying to be Gmail/Workspace and <a href="https://medium.com/">Medium</a> at the same time. And you can arbitrarily cut off anyone publishing with Hey, they have no open source rights.</li>



<li><a href="https://once.com/campfire">Campfire</a>, proprietary, you invented <a href="https://slack.com/">Slack</a> but they took the idea and built a $900M/ARR business with it, while you are trying to make shrinkwrap licensing a “thing” with <a href="https://once.com/">Once</a>.</li>



<li><a href="https://once.com/writebook">Writebook</a>, proprietary. Pretty cool.</li>



<li><a href="https://basecamp.com/">Basecamp</a>, proprietary. Great software. You invented the ideas <a href="https://www.atlassian.com/">Atlassian</a> ran with and built a $4.4B/revenue and growing business.</li>



<li><a href="https://rubyonrails.org/">Rails</a>, finally some open source! Looks like ~943k lines of code, 143k from Basecamp org. Automattic publishes 6.58M lines of open source code, 6.9x more than you. Yet, we’re “doing open source dirty”? <a href="https://www.shopify.com/">Shopify</a> used Rails to build a $7B/revenue and growing business, why didn’t you?</li>
</ul>



<p>David, perhaps it would be good to explore with a therapist or coach why you keep having these great ideas but cannot scale them beyond a handful of niche customers. I will give full credit and respect. 37signals inspired tons of what Automattic does! We’re now half a billion in revenue. Why are you still so small?</p>



<p>I was surprised someone as smart as DHH would fall for WP Engine’s lame deferral to make this about “GPL code” or forking, rather than trademarks. We have no problem with their use of GPL code, our beef is with their trademark abuse.</p>



<p>Let’s talk about trademarks! I don’t own the WordPress trademark personally, it belongs to a foundation on which I’m one of three votes. Rails? </p>



<blockquote>
<p><strong>“Rails”</strong>,&nbsp;<strong>“Ruby on Rails”</strong>, and the&nbsp;<strong>Rails logo</strong>&nbsp;are registered trademarks of&nbsp;<strong>David Heinemeier Hansson</strong>, but are under exclusive license to&nbsp;<a href="https://rubyonrails.org/foundation">The Rails Foundation</a>, which is responsible for administering their use and permission. You may not use these trademarks in a commercial setting to imply that your product or service is endorsed or associated with Ruby on Rails without permission. You may use these marks to refer to Ruby on Rails in a way where it’s clear that you’re simply referring to the project, not claiming endorsement or association.</p>
</blockquote>



<p>Huh, sounds like if I wanted to start RailsEngine I would need a trademark license. You are  ignoring WP Engine’s trademark abuse while retaining the same for your Rails trademark. The same as Drupal, where “Drupal is a registered trademark of Dries Buytaert, who retains sole ownership and control of this policy and any trademark licensing.” (<a href="https://dri.es/solving-the-maker-taker-problem">Dries has also decided to drop in on this debate</a>.)</p>



<p>Dries or David could arbitrarily withdraw their trademarks from the foundations / etc. at any time and for any reason or no reason. If they die, it’s not clear what happens to the trademarks. Their communities should look into that and consider a different name or taking over the trademark into a Foundation with multiple board members. </p>



<p>David, perhaps instead of spending <a href="https://www.autoblog.com/news/pagani-zonda-hh-commissioner-revealed-as-30-year-old-chicago-sof">$2M on a race car</a>, you should do some <a href="https://ma.tt/2024/09/charitable-contributions/">philanthropy</a>.</p>



<p>Instead of <a href="https://world.hey.com/dhh/how-it-started-how-it-s-going-baefaf09">bragging about your beautiful office in the clouds</a>, you should question why you can’t scale teams.</p>



<p>When you did a (less generous) buy-out offer <a href="https://www.theverge.com/2021/4/30/22412714/basecamp-employees-memo-policy-hansson-fried-controversy">33% of your team left</a>, vs <a href="https://ma.tt/2024/10/alignment/">8.4% of mine</a>.</p>



<p>I’m unsure why you felt you had to insert yourself into this fight with Silver Lake / WP Engine and take their side, but here we are.</p>



<p>Respectfully,<br>Matt</p>

	</div><!-- .entry-content -->
	
	<!-- .entry-meta -->
</article><!-- #post -->
						<nav>
		<h2>
			Post navigation		</h2>
		<!-- .nav-links -->
	</nav><!-- .navigation -->
						
<!-- #comments -->
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Vortex – a high-performance columnar file format (198 pts)]]></title>
            <link>https://github.com/spiraldb/vortex</link>
            <guid>41839773</guid>
            <pubDate>Mon, 14 Oct 2024 17:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/spiraldb/vortex">https://github.com/spiraldb/vortex</a>, See on <a href="https://news.ycombinator.com/item?id=41839773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Vortex</h2><a id="user-content-vortex" aria-label="Permalink: Vortex" href="#vortex"></a></p>
<p dir="auto"><a href="https://github.com/spiraldb/vortex/actions"><img src="https://github.com/fulcrum-so/vortex/actions/workflows/ci.yml/badge.svg" alt="Build Status"></a>
<a href="https://crates.io/crates/vortex-array" rel="nofollow"><img src="https://camo.githubusercontent.com/5e4c7b2bac2564f0b28006fdc5832127a05ecfe4c324a299deca48c113f2539a/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f766f727465782d61727261792e737667" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/vortex-array.svg"></a>
<a href="https://docs.rs/vortex-array" rel="nofollow"><img src="https://camo.githubusercontent.com/bb82ef82b46a0d868cb2eaa855fb1724ffd3d9d614d5ff26be508aec779e9792/68747470733a2f2f646f63732e72732f766f727465782d61727261792f62616467652e737667" alt="Documentation" data-canonical-src="https://docs.rs/vortex-array/badge.svg"></a>
<a href="https://pypi.org/project/vortex-array/" rel="nofollow"><img src="https://camo.githubusercontent.com/506d8e3db54d4420bdf98ef197df095cb9bf0b76bf29d032704d00cb6e4f4198/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f766f727465782d6172726179" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/pypi/pyversions/vortex-array"></a></p>
<p dir="auto">Vortex is a toolkit for working with compressed Apache Arrow arrays in-memory, on-disk, and over-the-wire.</p>
<p dir="auto">Vortex is designed to be to columnar file formats what Apache DataFusion is to query engines (or, analogously,
what LLVM + Clang are to compilers): a highly extensible &amp; extremely fast <em>framework</em> for building a modern
columnar file format, with a state-of-the-art, "batteries included" reference implementation.</p>
<p dir="auto">Vortex is an aspiring successor to Apache Parquet, with dramatically faster random access reads (100-200x faster)
and scans (2-10x faster), while preserving approximately the same compression ratio and write throughput. It will also support very wide
tables (at least 10s of thousands of columns) and (eventually) on-device decompression on GPUs.</p>
<div dir="auto"><p dir="auto">Caution</p><p dir="auto">This library is still under rapid development and is a work in progress!</p>
<p dir="auto">Some key features are not yet implemented, both the API and the serialized format are likely to change in breaking ways,
and we cannot yet guarantee correctness in all cases.</p>
</div>
<p dir="auto">The major features of Vortex are:</p>
<ul dir="auto">
<li><strong>Logical Types</strong> - a schema definition that makes no assertions about physical layout.</li>
<li><strong>Zero-Copy to Arrow</strong> - "canonicalized" (i.e., fully decompressed) Vortex arrays can be zero-copy converted to/from Apache Arrow arrays.</li>
<li><strong>Extensible Encodings</strong> - a pluggable set of physical layouts. In addition to the builtin set of Arrow-compatible encodings,
the Vortex repository includes a number of state-of-the-art encodings (e.g., FastLanes, ALP, FSST, etc.) that are implemented
as extensions. While arbitrary encodings can be implemented as extensions, we have intentionally chosen a small set
of encodings that are highly data-parallel, which in turn allows for efficient vectorized decoding, random access reads,
and (in the future) decompression on GPUs.</li>
<li><strong>Cascading Compression</strong> - data can be recursively compressed with multiple nested encodings.</li>
<li><strong>Pluggable Compression Strategies</strong> - the built-in Compressor is based on BtrBlocks, but other strategies can trivially be used instead.</li>
<li><strong>Compute</strong> - basic compute kernels that can operate over encoded data (e.g., for filter pushdown).</li>
<li><strong>Statistics</strong> - each array carries around lazily computed summary statistics, optionally populated at read-time.
These are available to compute kernels as well as to the compressor.</li>
<li><strong>Serialization</strong> - Zero-copy serialization of arrays, both for IPC and for file formats.</li>
<li><strong>Columnar File Format (in progress)</strong> - A modern file format that uses the Vortex serde library to store compressed array data.
Optimized for random access reads and extremely fast scans; an aspiring successor to Apache Parquet.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview: Logical vs Physical</h2><a id="user-content-overview-logical-vs-physical" aria-label="Permalink: Overview: Logical vs Physical" href="#overview-logical-vs-physical"></a></p>
<p dir="auto">One of the core design principles in Vortex is strict separation of logical and physical concerns.</p>
<p dir="auto">For example, a Vortex array is defined by a logical data type (i.e., the type of scalar elements) as well as a physical encoding
(the type of the array itself). Vortex ships with several built-in encodings, as well as several extension encodings.</p>
<p dir="auto">The built-in encodings are primarily designed to model the Apache Arrow in-memory format, enabling us to construct
Vortex arrays with zero-copy from Arrow arrays. There are also several built-in encodings (e.g., <code>sparse</code> and
<code>chunked</code>) that are useful building blocks for other encodings. The included extension encodings are mostly designed
to model compressed in-memory arrays, such as run-length or dictionary encoding.</p>
<p dir="auto">Analogously, <code>vortex-serde</code> is designed to handle the low-level physical details of reading and writing Vortex arrays. Choices
about which encodings to use or how to logically chunk data are left up to the <code>Compressor</code> implementation.</p>
<p dir="auto">One of the unique attributes of the (in-progress) Vortex file format is that it encodes the physical layout of the data within the
file's footer. This allows the file format to be effectively self-describing and to evolve without breaking changes to
the file format specification.</p>
<p dir="auto">In fact, the format is designed to support forward compatibility by optionally embedding WASM decoders directly into the files
themselves. This should help avoid the rapid calcification that has plagued other columnar file formats.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Components</h2><a id="user-content-components" aria-label="Permalink: Components" href="#components"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Logical Types</h3><a id="user-content-logical-types" aria-label="Permalink: Logical Types" href="#logical-types"></a></p>
<p dir="auto">The Vortex type-system is still in flux. The current set of logical types is:</p>
<ul dir="auto">
<li>Null</li>
<li>Bool</li>
<li>Integer(8, 16, 32, 64)</li>
<li>Float(16, b16, 32, 64)</li>
<li>Binary</li>
<li>UTF8</li>
<li>Struct</li>
<li>List (partially implemented)</li>
<li>Date/Time/DateTime/Duration (implemented as an extension type)</li>
<li>Decimal: TODO</li>
<li>FixedList: TODO</li>
<li>Tensor: TODO</li>
<li>Union: TODO</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Canonical/Flat Encodings</h3><a id="user-content-canonicalflat-encodings" aria-label="Permalink: Canonical/Flat Encodings" href="#canonicalflat-encodings"></a></p>
<p dir="auto">Vortex includes a base set of "flat" encodings that are designed to be zero-copy with Apache Arrow. These are the
canonical representations of each of the logical data types. The canonical encodings currently supported are:</p>
<ul dir="auto">
<li>Null</li>
<li>Bool</li>
<li>Primitive (Integer, Float)</li>
<li>Struct</li>
<li>VarBin (Binary, UTF8)</li>
<li>VarBinView (Binary, UTF8)</li>
<li>Extension</li>
<li>...with more to come</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compressed Encodings</h3><a id="user-content-compressed-encodings" aria-label="Permalink: Compressed Encodings" href="#compressed-encodings"></a></p>
<p dir="auto">Vortex includes a set of highly data-parallel, vectorized encodings. These encodings each correspond to a compressed
in-memory array implementation, allowing us to defer decompression. Currently, these are:</p>
<ul dir="auto">
<li>Adaptive Lossless Floating Point (ALP)</li>
<li>BitPacked (FastLanes)</li>
<li>Constant</li>
<li>Chunked</li>
<li>Delta (FastLanes)</li>
<li>Dictionary</li>
<li>Fast Static Symbol Table (FSST)</li>
<li>Frame-of-Reference</li>
<li>Run-end Encoding</li>
<li>RoaringUInt</li>
<li>RoaringBool</li>
<li>Sparse</li>
<li>ZigZag</li>
<li>...with more to come</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compression</h3><a id="user-content-compression" aria-label="Permalink: Compression" href="#compression"></a></p>
<p dir="auto">Vortex's default compression strategy is based on the
<a href="https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf" rel="nofollow">BtrBlocks</a> paper.</p>
<p dir="auto">Roughly, for each chunk of data, a sample of at least ~1% of the data is taken. Compression is then attempted (
recursively) with a set of lightweight encodings. The best-performing combination of encodings is then chosen to encode
the entire chunk. This sounds like it would be very expensive, but given basic statistics about a chunk, it is
possible to cheaply prune many encodings and ensure the search space does not explode in size.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compute</h3><a id="user-content-compute" aria-label="Permalink: Compute" href="#compute"></a></p>
<p dir="auto">Vortex provides the ability for each encoding to specialize the implementation of a compute function to avoid
decompressing where possible. For example, filtering a dictionary-encoded UTF8 array can be more cheaply performed by
filtering the dictionary first.</p>
<p dir="auto">Note--as mentioned above--that Vortex does not intend to become a full-fledged compute engine, but rather to implement
basic compute operations as may be required for efficient scanning &amp; pushdown.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Statistics</h3><a id="user-content-statistics" aria-label="Permalink: Statistics" href="#statistics"></a></p>
<p dir="auto">Vortex arrays carry lazily-computed summary statistics. Unlike other array libraries, these statistics can be populated
from disk formats such as Parquet and preserved all the way into a compute engine. Statistics are available to compute
kernels as well as to the compressor.</p>
<p dir="auto">The current statistics are:</p>
<ul dir="auto">
<li>BitWidthFreq</li>
<li>TrailingZeroFreq</li>
<li>IsConstant</li>
<li>IsSorted</li>
<li>IsStrictSorted</li>
<li>Max</li>
<li>Min</li>
<li>RunCount</li>
<li>TrueCount</li>
<li>NullCount</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Serialization / Deserialization (Serde)</h3><a id="user-content-serialization--deserialization-serde" aria-label="Permalink: Serialization / Deserialization (Serde)" href="#serialization--deserialization-serde"></a></p>
<p dir="auto">The goals of the <code>vortex-serde</code> implementation are:</p>
<ul dir="auto">
<li>Support scanning (column projection + row filter) with zero-copy and zero heap allocation.</li>
<li>Support random access in constant or near-constant time.</li>
<li>Forward statistical information (such as sortedness) to consumers.</li>
<li>Provide IPC format for sending arrays between processes.</li>
<li>Provide an extensible, best-in-class file format for storing columnar data on disk or in object storage.</li>
</ul>
<p dir="auto">TODO: insert diagram here</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Integration with Apache Arrow</h2><a id="user-content-integration-with-apache-arrow" aria-label="Permalink: Integration with Apache Arrow" href="#integration-with-apache-arrow"></a></p>
<p dir="auto">Apache Arrow is the de facto standard for interoperating on columnar array data. Naturally, Vortex is designed to
be maximally compatible with Apache Arrow. All Arrow arrays can be converted into Vortex arrays with zero-copy,
and a Vortex array constructed from an Arrow array can be converted back to Arrow, again with zero-copy.</p>
<p dir="auto">It is important to note that Vortex and Arrow have different--albeit complementary--goals.</p>
<p dir="auto">Vortex explicitly separates logical types from physical encodings, distinguishing it from Arrow. This allows
Vortex to model more complex arrays while still exposing a logical interface. For example, Vortex can model a UTF8
<code>ChunkedArray</code> where the first chunk is run-length encoded and the second chunk is dictionary encoded.
In Arrow, <code>RunLengthArray</code> and <code>DictionaryArray</code> are separate incompatible types, and so cannot be combined in this way.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">For best performance we recommend using <a href="https://github.com/microsoft/mimalloc">MiMalloc</a> as the application's
allocator.</p>
<div dir="auto" data-snippet-clipboard-copy-content="#[global_allocator]
static GLOBAL_ALLOC: MiMalloc = MiMalloc;"><pre><span>#<span>[</span>global_allocator<span>]</span></span>
<span>static</span> <span>GLOBAL_ALLOC</span><span>:</span> <span>MiMalloc</span> = <span>MiMalloc</span><span>;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Please see <a href="https://github.com/spiraldb/vortex/blob/develop/CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto">In order to build vortex, you may also need to install the flatbuffer compiler (flatc):</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac</h3><a id="user-content-mac" aria-label="Permalink: Mac" href="#mac"></a></p>

<p dir="auto">This repo uses rye to manage the combined Rust/Python monorepo build. First, make sure to run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install Rye from https://rye-up.com, and setup the virtualenv
rye sync"><pre><span><span>#</span> Install Rye from https://rye-up.com, and setup the virtualenv</span>
rye sync</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Licensed under the Apache License, Version 2.0 (the "License").</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Governance</h2><a id="user-content-governance" aria-label="Permalink: Governance" href="#governance"></a></p>
<p dir="auto">Vortex is and will remain an open-source project. Our intent is to model its governance structure after the
<a href="https://substrait.io/governance/" rel="nofollow">Substrait project</a>, which in turn is based on the model of the Apache Software Foundation.
Expect more details on this in Q4 2024.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgments 🏆</h2><a id="user-content-acknowledgments-" aria-label="Permalink: Acknowledgments 🏆" href="#acknowledgments-"></a></p>
<p dir="auto">This project is inspired by and--in some cases--directly based upon the existing, excellent work of many researchers
and OSS developers.</p>
<p dir="auto">In particular, the following academic papers greatly influenced the development:</p>
<ul dir="auto">
<li>Maximilian Kuschewski, David Sauerwein, Adnan Alhomssi, and Viktor Leis.
2023. <a href="https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf" rel="nofollow">BtrBlocks: Efficient Columnar Compression
for Data Lakes</a>. Proc. ACM Manag. Data 1,
2,
Article 118 (June 2023), 14 pages. <a href="https://doi.org/10.1145/3589263" rel="nofollow">https://doi.org/10.1145/3589263</a></li>
<li>Azim Afroozeh and Peter
Boncz. <a href="https://www.vldb.org/pvldb/vol16/p2132-afroozeh.pdf" rel="nofollow">The FastLanes Compression Layout: Decoding &gt;100 Billion Integers per Second with Scalar
Code</a>. PVLDB, 16(9): 2132 - 2144, 2023.</li>
<li>Peter Boncz, Thomas Neumann, and Viktor Leis. <a href="https://www.vldb.org/pvldb/vol13/p2649-boncz.pdf" rel="nofollow">FSST: Fast Random Access String
Compression</a>.
PVLDB, 13(11): 2649-2661, 2020.</li>
<li>Azim Afroozeh, Leonardo X. Kuffo, and Peter Boncz. 2023. <a href="https://ir.cwi.nl/pub/33334/33334.pdf" rel="nofollow">ALP: Adaptive Lossless floating-Point
Compression</a>. Proc. ACM
Manag. Data 1, 4 (SIGMOD), Article 230 (December 2023), 26 pages. <a href="https://doi.org/10.1145/3626717" rel="nofollow">https://doi.org/10.1145/3626717</a></li>
</ul>
<p dir="auto">Additionally, we benefited greatly from:</p>
<ul dir="auto">
<li>the existence, ideas, &amp; implementation of <a href="https://arrow.apache.org/" rel="nofollow">Apache Arrow</a>.</li>
<li>likewise for the excellent <a href="https://github.com/apache/datafusion">Apache DataFusion</a> project.</li>
<li>the <a href="https://github.com/jorgecarleitao/parquet2">parquet2</a> project by <a href="https://github.com/jorgecarleitao">Jorge Leitao</a>.</li>
<li>the public discussions around choices of compression codecs, as well as the C++ implementations thereof,
from <a href="https://github.com/duckdb/duckdb">duckdb</a>.</li>
<li>the <a href="https://github.com/facebookincubator/velox">Velox</a> and <a href="https://github.com/facebookincubator/nimble">Nimble</a> projects,
and discussions with their maintainers.</li>
</ul>
<p dir="auto">Thanks to all of the aforementioned for sharing their work and knowledge with the world! 🚀</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Furilabs Linux Phone (114 pts)]]></title>
            <link>https://furilabs.com/shop/flx1/</link>
            <guid>41839326</guid>
            <pubDate>Mon, 14 Oct 2024 16:52:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://furilabs.com/shop/flx1/">https://furilabs.com/shop/flx1/</a>, See on <a href="https://news.ycombinator.com/item?id=41839326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-elementor-type="product" data-elementor-id="2206" data-elementor-post-type="elementor_library">
					<div data-id="44bb64ec" data-element_type="section" data-settings="{&quot;stretch_section&quot;:&quot;section-stretched&quot;}">
					<div data-id="2a270102" data-element_type="column" data-widget_type="shortcode.default">
					<p><a id="flx1" data-magic360-options="columns:73;rows:1; images: https://furilabs.com/wp-content/uploads/2024/03/flx1_001.png https://furilabs.com/wp-content/uploads/2024/03/flx1_011.png https://furilabs.com/wp-content/uploads/2024/03/flx1_021.png https://furilabs.com/wp-content/uploads/2024/03/flx1_031.png https://furilabs.com/wp-content/uploads/2024/03/flx1_041.png https://furilabs.com/wp-content/uploads/2024/03/flx1_051.png https://furilabs.com/wp-content/uploads/2024/03/flx1_061.png https://furilabs.com/wp-content/uploads/2024/03/flx1_071.png https://furilabs.com/wp-content/uploads/2024/03/flx1_081.png https://furilabs.com/wp-content/uploads/2024/03/flx1_091.png https://furilabs.com/wp-content/uploads/2024/03/flx1_101.png https://furilabs.com/wp-content/uploads/2024/03/flx1_111.png https://furilabs.com/wp-content/uploads/2024/03/flx1_121.png https://furilabs.com/wp-content/uploads/2024/03/flx1_131.png https://furilabs.com/wp-content/uploads/2024/03/flx1_141.png https://furilabs.com/wp-content/uploads/2024/03/flx1_151.png https://furilabs.com/wp-content/uploads/2024/03/flx1_161.png https://furilabs.com/wp-content/uploads/2024/03/flx1_171.png https://furilabs.com/wp-content/uploads/2024/03/flx1_181.png https://furilabs.com/wp-content/uploads/2024/03/flx1_191.png https://furilabs.com/wp-content/uploads/2024/03/flx1_201.png https://furilabs.com/wp-content/uploads/2024/03/flx1_211.png https://furilabs.com/wp-content/uploads/2024/03/flx1_221.png https://furilabs.com/wp-content/uploads/2024/03/flx1_231.png https://furilabs.com/wp-content/uploads/2024/03/flx1_241.png https://furilabs.com/wp-content/uploads/2024/03/flx1_251.png https://furilabs.com/wp-content/uploads/2024/03/flx1_261.png https://furilabs.com/wp-content/uploads/2024/03/flx1_271.png https://furilabs.com/wp-content/uploads/2024/03/flx1_281.png https://furilabs.com/wp-content/uploads/2024/03/flx1_291.png https://furilabs.com/wp-content/uploads/2024/03/flx1_301.png https://furilabs.com/wp-content/uploads/2024/03/flx1_311.png https://furilabs.com/wp-content/uploads/2024/03/flx1_321.png https://furilabs.com/wp-content/uploads/2024/03/flx1_331.png https://furilabs.com/wp-content/uploads/2024/03/flx1_341.png https://furilabs.com/wp-content/uploads/2024/03/flx1_351.png https://furilabs.com/wp-content/uploads/2024/03/flx1_361.png https://furilabs.com/wp-content/uploads/2024/03/flx1_371.png https://furilabs.com/wp-content/uploads/2024/03/flx1_381.png https://furilabs.com/wp-content/uploads/2024/03/flx1_391.png https://furilabs.com/wp-content/uploads/2024/03/flx1_401.png https://furilabs.com/wp-content/uploads/2024/03/flx1_411.png https://furilabs.com/wp-content/uploads/2024/03/flx1_421.png https://furilabs.com/wp-content/uploads/2024/03/flx1_431.png https://furilabs.com/wp-content/uploads/2024/03/flx1_441.png https://furilabs.com/wp-content/uploads/2024/03/flx1_451.png https://furilabs.com/wp-content/uploads/2024/03/flx1_461.png https://furilabs.com/wp-content/uploads/2024/03/flx1_471.png https://furilabs.com/wp-content/uploads/2024/03/flx1_481.png https://furilabs.com/wp-content/uploads/2024/03/flx1_491.png https://furilabs.com/wp-content/uploads/2024/03/flx1_501.png https://furilabs.com/wp-content/uploads/2024/03/flx1_511.png https://furilabs.com/wp-content/uploads/2024/03/flx1_521.png https://furilabs.com/wp-content/uploads/2024/03/flx1_531.png https://furilabs.com/wp-content/uploads/2024/03/flx1_541.png https://furilabs.com/wp-content/uploads/2024/03/flx1_551.png https://furilabs.com/wp-content/uploads/2024/03/flx1_561.png https://furilabs.com/wp-content/uploads/2024/03/flx1_571.png https://furilabs.com/wp-content/uploads/2024/03/flx1_581.png https://furilabs.com/wp-content/uploads/2024/03/flx1_591.png https://furilabs.com/wp-content/uploads/2024/03/flx1_601.png https://furilabs.com/wp-content/uploads/2024/03/flx1_611.png https://furilabs.com/wp-content/uploads/2024/03/flx1_621.png https://furilabs.com/wp-content/uploads/2024/03/flx1_631.png https://furilabs.com/wp-content/uploads/2024/03/flx1_641.png https://furilabs.com/wp-content/uploads/2024/03/flx1_651.png https://furilabs.com/wp-content/uploads/2024/03/flx1_661.png https://furilabs.com/wp-content/uploads/2024/03/flx1_671.png https://furilabs.com/wp-content/uploads/2024/03/flx1_681.png https://furilabs.com/wp-content/uploads/2024/03/flx1_691.png https://furilabs.com/wp-content/uploads/2024/03/flx1_701.png https://furilabs.com/wp-content/uploads/2024/03/flx1_711.png https://furilabs.com/wp-content/uploads/2024/03/flx1_721.png; large-images: https://furilabs.com/wp-content/uploads/2024/03/flx1_001.png https://furilabs.com/wp-content/uploads/2024/03/flx1_011.png https://furilabs.com/wp-content/uploads/2024/03/flx1_021.png https://furilabs.com/wp-content/uploads/2024/03/flx1_031.png https://furilabs.com/wp-content/uploads/2024/03/flx1_041.png https://furilabs.com/wp-content/uploads/2024/03/flx1_051.png https://furilabs.com/wp-content/uploads/2024/03/flx1_061.png https://furilabs.com/wp-content/uploads/2024/03/flx1_071.png https://furilabs.com/wp-content/uploads/2024/03/flx1_081.png https://furilabs.com/wp-content/uploads/2024/03/flx1_091.png https://furilabs.com/wp-content/uploads/2024/03/flx1_101.png https://furilabs.com/wp-content/uploads/2024/03/flx1_111.png https://furilabs.com/wp-content/uploads/2024/03/flx1_121.png https://furilabs.com/wp-content/uploads/2024/03/flx1_131.png https://furilabs.com/wp-content/uploads/2024/03/flx1_141.png https://furilabs.com/wp-content/uploads/2024/03/flx1_151.png https://furilabs.com/wp-content/uploads/2024/03/flx1_161.png https://furilabs.com/wp-content/uploads/2024/03/flx1_171.png https://furilabs.com/wp-content/uploads/2024/03/flx1_181.png https://furilabs.com/wp-content/uploads/2024/03/flx1_191.png https://furilabs.com/wp-content/uploads/2024/03/flx1_201.png https://furilabs.com/wp-content/uploads/2024/03/flx1_211.png https://furilabs.com/wp-content/uploads/2024/03/flx1_221.png https://furilabs.com/wp-content/uploads/2024/03/flx1_231.png https://furilabs.com/wp-content/uploads/2024/03/flx1_241.png https://furilabs.com/wp-content/uploads/2024/03/flx1_251.png https://furilabs.com/wp-content/uploads/2024/03/flx1_261.png https://furilabs.com/wp-content/uploads/2024/03/flx1_271.png https://furilabs.com/wp-content/uploads/2024/03/flx1_281.png https://furilabs.com/wp-content/uploads/2024/03/flx1_291.png https://furilabs.com/wp-content/uploads/2024/03/flx1_301.png https://furilabs.com/wp-content/uploads/2024/03/flx1_311.png https://furilabs.com/wp-content/uploads/2024/03/flx1_321.png https://furilabs.com/wp-content/uploads/2024/03/flx1_331.png https://furilabs.com/wp-content/uploads/2024/03/flx1_341.png https://furilabs.com/wp-content/uploads/2024/03/flx1_351.png https://furilabs.com/wp-content/uploads/2024/03/flx1_361.png https://furilabs.com/wp-content/uploads/2024/03/flx1_371.png https://furilabs.com/wp-content/uploads/2024/03/flx1_381.png https://furilabs.com/wp-content/uploads/2024/03/flx1_391.png https://furilabs.com/wp-content/uploads/2024/03/flx1_401.png https://furilabs.com/wp-content/uploads/2024/03/flx1_411.png https://furilabs.com/wp-content/uploads/2024/03/flx1_421.png https://furilabs.com/wp-content/uploads/2024/03/flx1_431.png https://furilabs.com/wp-content/uploads/2024/03/flx1_441.png https://furilabs.com/wp-content/uploads/2024/03/flx1_451.png https://furilabs.com/wp-content/uploads/2024/03/flx1_461.png https://furilabs.com/wp-content/uploads/2024/03/flx1_471.png https://furilabs.com/wp-content/uploads/2024/03/flx1_481.png https://furilabs.com/wp-content/uploads/2024/03/flx1_491.png https://furilabs.com/wp-content/uploads/2024/03/flx1_501.png https://furilabs.com/wp-content/uploads/2024/03/flx1_511.png https://furilabs.com/wp-content/uploads/2024/03/flx1_521.png https://furilabs.com/wp-content/uploads/2024/03/flx1_531.png https://furilabs.com/wp-content/uploads/2024/03/flx1_541.png https://furilabs.com/wp-content/uploads/2024/03/flx1_551.png https://furilabs.com/wp-content/uploads/2024/03/flx1_561.png https://furilabs.com/wp-content/uploads/2024/03/flx1_571.png https://furilabs.com/wp-content/uploads/2024/03/flx1_581.png https://furilabs.com/wp-content/uploads/2024/03/flx1_591.png https://furilabs.com/wp-content/uploads/2024/03/flx1_601.png https://furilabs.com/wp-content/uploads/2024/03/flx1_611.png https://furilabs.com/wp-content/uploads/2024/03/flx1_621.png https://furilabs.com/wp-content/uploads/2024/03/flx1_631.png https://furilabs.com/wp-content/uploads/2024/03/flx1_641.png https://furilabs.com/wp-content/uploads/2024/03/flx1_651.png https://furilabs.com/wp-content/uploads/2024/03/flx1_661.png https://furilabs.com/wp-content/uploads/2024/03/flx1_671.png https://furilabs.com/wp-content/uploads/2024/03/flx1_681.png https://furilabs.com/wp-content/uploads/2024/03/flx1_691.png https://furilabs.com/wp-content/uploads/2024/03/flx1_701.png https://furilabs.com/wp-content/uploads/2024/03/flx1_711.png https://furilabs.com/wp-content/uploads/2024/03/flx1_721.png;" href="https://bunny-wp-pullzone-nrxogzjcpx.b-cdn.net/wp-content/uploads/2024/03/flx1_011.png"><img width="425" height="800" alt="" src="https://bunny-wp-pullzone-nrxogzjcpx.b-cdn.net/wp-content/uploads/2024/03/flx1_011.png"></a></p>
				</div>
				<div data-id="29071f2" data-element_type="column">
						
				<div data-id="531cddd" data-element_type="widget" data-widget_type="woocommerce-product-etheme_title.default">
				<p>
			<h2>FLX1</h2>		</p>
				</div>
				<div data-id="46830f0" data-element_type="widget" data-widget_type="woocommerce-product-etheme_price.default">
				<p> <span>Original price was: $550.00.</span><span>Current price is: $499.00.</span></p>
				</div>
				<div data-id="41a954a" data-element_type="widget" data-widget_type="woocommerce-product-etheme_short_description.default">
	<p>Fast, performant and cheap. You wanted all 3? Now you got it! The FLX1 from Furi Labs runs a fully optimized system called Furi OS, packing a lightning fast user interface, tons of storage, and a privacy centric approach like no other.</p>
<p>FuriLabs is ready to protect your data and keep you connected and secure at all times. Long term support, removable battery, IP68 and an unmatched price point. Get your hands on one today. FuriLabs: planned permanence!</p>
<p>Dimensions &amp; Weights:<br>
Phone: 171mm x 82mm x 12mm : 280g<br>
Box: 180mm x 90mm x 28mm : 76g<br>
Total: 180mm x 90mm x 28mm : 356g</p>
<p>See our <a href="https://furilabs.com/shipping/">shipping details</a>.</p>
<p>Read some reviews in the below tabs or <a href="https://furilabs.com/furilabs-in-the-media/">some external reviews</a>.</p>

</div>
				
				
				
				
				<div data-id="74b5f00" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h2>Share:</h2>		</p>
				</div>
				
					</div>
					</div>
				<div data-title="Tabs" data-id="32646211" data-element_type="section" data-widget_type="woocommerce-product-etheme_tabs.default" data-settings="{&quot;stretch_section&quot;:&quot;section-stretched&quot;}">
		
		
		
			<div data-id="ff3df52" data-element_type="container" data-widget_type="text-editor.default" data-elementor-type="product-post" data-elementor-id="77" data-elementor-post-type="product" id="tab-description" role="tabpanel" aria-labelledby="tab-title-description">
							<p>The FLX1 runs FuriOS, which is an operating system based on Debian, designed and oriented for mobile use without any artificial limitations.</p><p>Custom camera app with high quality video recording, picture capabilities and native QR code reading and Wifi hotspot joing via QR code.</p><p>VoLTE, MMS, SMS and the ability to use Android apps in a customized container are some of the additions we have built into FuriOS.</p>						</div>

		
			<div id="tab-et_custom_tab_01" role="tabpanel" aria-labelledby="tab-title-et_custom_tab_01"><table>
<tbody>
<tr>
<td colspan="2"><strong>Motherboard</strong></td>
</tr>
<tr>
<td>Chipset</td>
<td>Mediatek Dimensity 900</td>
</tr>
<tr>
<td>Memory</td>
<td>6GB LPDDR4X</td>
</tr>
<tr>
<td>Storage</td>
<td>128GB UFS2.1</td>
</tr>
<tr>
<td>CPU</td>
<td>2x Cortex-A78 2.4Ghz &amp;&amp; 6x Cortex A55 2.0Ghz</td>
</tr>
<tr>
<td>GPU</td>
<td>Mali G68 MC4</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Camera</strong></td>
</tr>
<tr>
<td>Front Camera</td>
<td>16MP</td>
</tr>
<tr>
<td>Back Camera</td>
<td>50MP with Optical Image stabilization</td>
</tr>
<tr>
<td>Macro Camera</td>
<td>2MP</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Battery</strong></td>
</tr>
<tr>
<td>Charging</td>
<td>Wired/Wireless and NFC combo</td>
</tr>
<tr>
<td>Battery Type</td>
<td>Li-Po Removable battery</td>
</tr>
<tr>
<td>Battery capacity</td>
<td>5000mAh</td>
</tr>
<tr>
<td>USB</td>
<td>Type C 3.0 waterproof</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Connectivity</strong></td>
</tr>
<tr>
<td>Modem</td>
<td>2G/3G/4G/5G/5G ENDC</td>
</tr>
<tr>
<td>SIM Slots</td>
<td>Dual</td>
</tr>
<tr>
<td>WiFi</td>
<td>WiFi 6.0 (a/b/g/n/ac/ax)</td>
</tr>
<tr>
<td>Bluetooth</td>
<td>5.2, A2DP, LE</td>
</tr>
<tr>
<td>ESIM</td>
<td>N/a</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Bands</strong></td>
</tr>
<tr>
<td>GSM</td>
<td>2/3/5/8</td>
</tr>
<tr>
<td>UMTS</td>
<td>B1/8</td>
</tr>
<tr>
<td>TD-LTE</td>
<td>B38/40</td>
</tr>
<tr>
<td>FDD-LTE</td>
<td>B1/3/5/7/8/20/28A/28B</td>
</tr>
<tr>
<td>5G</td>
<td>NR N1, 2, 3, 4, 5, 7, 8, 12, 20, 28, 38, 40, 41, 60, 66, 77, 78 SA/NSA</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Screen</strong></td>
</tr>
<tr>
<td>Resolution</td>
<td>6.59" FHD+ IPS Display 10 point multi touch</td>
</tr>
<tr>
<td>Refresh rate</td>
<td>60Hz/120Hz panel</td>
</tr>
<tr>
<td>Glass type</td>
<td>Gorilla Glass 5</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Fingerprint</strong></td>
</tr>
<tr>
<td>Fingerprint</td>
<td>Side Mounted, on power button</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Peripherals</strong></td>
</tr>
<tr>
<td>Micro SD</td>
<td>Up to 1TB</td>
</tr>
<tr>
<td>Headphone jack</td>
<td>3.5mm waterproof</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Material</strong></td>
</tr>
<tr>
<td>Back cover</td>
<td>Polycarbonate</td>
</tr>
<tr>
<td>Mid frame</td>
<td>Polycarbonate and TPU</td>
</tr>
<tr>
<td>Keys</td>
<td>Metal</td>
</tr>
<tr>
<td>Water proof</td>
<td>IP68</td>
</tr>
</tbody>
</table></div>

		
			<div id="tab-additional_information" role="tabpanel" aria-labelledby="tab-title-additional_information">
						

<table aria-label="Product Details">
			<tbody><tr>
			<th scope="row">Weight</th>
			<td>0.356 kg</td>
		</tr>
			<tr>
			<th scope="row">Dimensions</th>
			<td>18 × 9 × 2.80 cm</td>
		</tr>
	</tbody></table>
					</div>

		
			<div id="tab-reviews" role="tabpanel" aria-labelledby="tab-title-reviews">

    
    <div id="comments">

        
        <h2>
            3 reviews for <span>FLX1</span>        </h2>

                    <ol>
                <li id="li-comment-304">

	<div id="comment-304">

		<p><img alt="" src="https://secure.gravatar.com/avatar/45e1d2b07a0aee59c602933bcb7d5b78?s=80&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/45e1d2b07a0aee59c602933bcb7d5b78?s=160&amp;d=mm&amp;r=g 2x" height="80" width="80" decoding="async" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2080%2080'%3E%3C/svg%3E"></p><div>

			<p><span>Rated <strong>5</strong> out of 5</span></p>
	<p>
		<strong>alaraajavamma </strong>
		<em>(verified owner)</em> 		<span>–</span> <time datetime="2024-07-26T13:47:28+10:00">July 26, 2024</time>
	</p>

	<div><p>By an incredible margin, the best linux phone I’ve had – and I’ve had practically all of them (Pinephones, Librem 5, Vollas etc.). For the first time this truly competes in the same world as the billion dollar Android / ios devices. </p>
<p>Imagine a full-blooded Phosh desktop environment with fully working camera &amp; gps and a device that is truly lightning fast and has battery protection according to today’s phone standards (without suspend 24 hours)</p>
</div>
		</div>
	</div>
</li><!-- #comment-## -->
<li id="li-comment-369">

	<div id="comment-369">

		<p><img alt="" src="https://secure.gravatar.com/avatar/71644a78a55e90778258c566472776a6?s=80&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/71644a78a55e90778258c566472776a6?s=160&amp;d=mm&amp;r=g 2x" height="80" width="80" decoding="async" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2080%2080'%3E%3C/svg%3E"></p><div>

			<p><span>Rated <strong>5</strong> out of 5</span></p>
	<p>
		<strong>Brian Aberts </strong>
		<em>(verified owner)</em> 		<span>–</span> <time datetime="2024-09-06T09:54:17+10:00">September 6, 2024</time>
	</p>

	<div><p>This phone isn’t ready YET to hand to grandma and say “Use this!”. But I think it’s quickly heading in that direction. The amount of progress the dev team has made on it in the past month and a half has been an incredible journey. I bought this not knowing if the phone would work or not, as I had never heard of Furilabs… But I took the chance and ordered one on July 19, 2024.</p>
<p>When I first got it, I found out that the phone didn’t work at all in the USA. It would display a 2G or 2.75G symbol, but wouldn’t make calls or browse the web, as it didn’t have support for US cellular bands, only Europe and the rest of the world. I offered to give SSH access to my device to one of the developers, and have been letting him test modem builds on my device with a Ting MVNO Sim. At first it seemed kind of unbelievable that it would work and I’ll be honest, I also wasn’t confident in the developers to get it working. I’m glad I did though, because as of today the latest modem build the developer has put on the device has allowed me to place and receive calls, and browse the web on 4G LTE. 5G doesn’t work yet, and only band 4 of 4G LTE is enabled currently on the test build I’m running, but I can use the phone now!</p>
<p>I will continue to offer my device to run test builds, and I’m excited to see the progress. The phone’s software is extremely liberating compared to android. It’s fast, it runs a familiar debian-based Linux environment, and it already has some really unique features I have been wanting in a phone! For example, there is a button on the left side of the phone that you can program from within the settings to run shell scripts, take screenshots, open the camera or take pictures, and toggle the flashlight. This is the first Linux phone I’ve had with such a feature, and I think it’s a fantastic addition.</p>
<p>With my thoughts on the camera, it takes great quality pictures, and even records video! It might sound odd I am counting recording video as a feature, but other Linux phones like the PinePhone and Librem 5 do not support video recording with the camera, unless you use hacky scripts. Aside from that it’s fully water resistant, has a removable microSD slot, dual sim capability (at least in hardware, as currently that’s not enabled in software), a LED indicator for notifications, a fully working fingerprint sensor (as in you can enroll each finger on your hand and use it to login to the phone – at least after you login once from a cold boot with your pin or password), full disk encryption, Android app support with Waydroid (but amped up as it supports the phone’s sensors and GPS and runs android apps as if they’re native apps), and volume toggle in the accesibility settings menu that lets you set the speaker volume to 150%.</p>
<p>You might be wondering why you wouldn’t just buy a PinePhone, Librem 5, or a phone that can run Ubuntu Touch instead of this device. My answer to that is that the FLX1 while yes, it does use Halium, has far better support than a typical Ubuntu Touch or Droidian capable device. The developers involved with the project have been involved with the Droidian project, and are the one of very people who did a lot of the work to get good Waydroid integration on those systems originally. With the FLX1 they’re amping it up even more, with further improvements, fixes, and features that other devices haven’t seen and probably won’t. A big one for me being VoLTE and VoNR support, something even the mature Ubuntu Touch project still doesn’t have publicly available. To my knowledge, this is the first Linux phone that has 5G fully working, and notably 4G VoLTE working globally with hopefully soon to follow global 5G VoNR support. As I’ve said, the US band support for the rest of the 4G range and 5G is still something that they’re working on, and I hope to soon report progress on that front. Very exciting!</p>
<p>There’s tons of additional settings to dive into, such as USB settings for MTP, USB state, CD-ROM settings, a NFC support toggle, GPS SUPL server setting you can set to a custom server URL, a printer setting panel for adding and managing printers, and a slew of accesibility and privacy/security settings such as controlling screen lock, location access, file history, camera access, and more. Some of those things have already existed in phosh, but a number of them have been enabled or modified, offering things you don’t usually get on mobile Linux. A quick note on the Waydroid settings, it has a full panel for controlling starting/stopping of android apps, clearing app data, as well as controlling NFC access to Waydroid and enabling a shared folder. It also gives you the IP of the Android container and gives you information on the Android version running in the container. The developers have mentioned to me that they have big plans to rework phosh to be a bit cleaner, add more fine grained controls and settings, and add features such as RCS support for messaging and context-aware text suggestions in the phosh keyboard. </p>
<p>With that said, the new features like RCS likely won’t arrive for a bit as they have the cellular stuff to sort, as well as some bug fixes and other features they want to polish first. The device does have some bugs in Firefox and with video acceleration in video players such as MPV, and I’ve personally noticed that compass bearing in Waydroid is currently bugged although GPS itself does work otherwise. That alongside the fact that 4G LTE band support is currently limited to Band 4 in the USA means that I realize this phone isn’t currently for everyone, but I think it’s important to note that the developers are incredibly active on telegram and respond immediately to problems you might have. There has only been one device so far, to the best of my knowledge, that was defective from the manufacturer they use, but Furilabs was quick to replace it and apologize to the person for the inconvience. I believe they even overnighted it to him before taking a look at the device to see what hardware had failed.</p>
<p>The device has a fantastic custom recovery system. You can SSH in to copy files or try to fix a software issue, and it has a factory reset system to easily roll back to the original software, or you could even reflash the phone completely using a computer. Another big thing to note is that the developers also are on a roll at releasing bug fixes and offering workarounds for issues. Infact, they’re currently testing a new software release in their beta-tester telegram channel with a couple of volunteers to see if there’s any bugs that may have slipped through before release. Currently they aim to release a new update every month, and they release the full changelog on their website for everyone to see. When I asked about automated testing, one of the developers mentioned they would like to get to implementing that when they get a chance. </p>
<p>I highly recommend you take a look at the changelogs, web forum, and telegram as there’s tons of awesome information available. Not just to mention the fact that the phone is open source, with source code avaialable on github for everything except the modem and some of the recovery I believe (Which has intulectual property bits that mediatek wouldn’t be happy to have public). It’s not perfect right now, but nothing is perfect, let’s be real! What it is however, is incredibly promising. I’ve never had more hope in a Linux phone than this one. I’ve gotten up at 6AM some days to excitedly work with the developer to test out new modem firmware because it’s been a steady march of improvements each day, and that’s just the modem! I am highly confident within the next month, mauybe two, that this device is something I will be able to give to my mother to use.</p>
<p>With the open source stack, and the GTK framework powering applications, Android app support in a pinch, a powerful Debian-based stack with a proper Linux terminal and libraries, bash scripting, and powerful hardware, it has a lot to offer. I will update this review as it progresses to add more information and expand on where the software stands as it improves, but while this phone isn’t for everyone just year, I hope some of you reading this might be willing to pull the trigger and support the project by grabbing one of these, and my reasoning is simple: In order to survive and continue to improve on the device, Furilabs is going to need money coming in from sales. This is a small company, and every little thing will count. I’ve never had more hope for a Linux phone project than this one, and I’ve tussled with a lot over the years. And I’m sure if you’re interested you can reach out on the Telegram to maybe test out the latest modem builds for USA support and get it polished up quicker!</p>
<p>Posted 9/5/24</p>
</div>
		</div>
	</div>
</li><!-- #comment-## -->
<li id="li-comment-370">

	<div id="comment-370">

		<p><img alt="" src="https://secure.gravatar.com/avatar/3913dac94a176be1fd1a0fefe07cb7c6?s=80&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/3913dac94a176be1fd1a0fefe07cb7c6?s=160&amp;d=mm&amp;r=g 2x" height="80" width="80" decoding="async" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2080%2080'%3E%3C/svg%3E"></p><div>

			<p><span>Rated <strong>5</strong> out of 5</span></p>
	<p>
		<strong>Hman </strong>
		<em>(verified owner)</em> 		<span>–</span> <time datetime="2024-09-06T10:35:03+10:00">September 6, 2024</time>
	</p>

	<div><p>From iPhone to Furiphone</p>
<p>I bought this phone because I care about my data and personal privacy. I didn’t realize that such a phone would radically alter my relationship to my phone: they could have just called it a “myPhone” because the phone (not to mention your data) belong to YOU. It feels like having something unique and that is YOURS in your pocket, which is an experience that I didn’t even realize was possible until getting it. You can also tell that the development team has made this a labor of LOVE. It is always a pleasure being able to talk to them about the phone and feel like you’re helping to develop a piece of linux history. (The first fully functional linux phone!)</p>
<p>You can install all the linux apps you want, while having access to all traditional apps through the excellent waydroid service installed on the phone. The phone is accessible through terminal on my computer and it has been great using a familiar interface to install apps and manage files. The latest updates are always something to look forward to, making the phone better and better with each iteration. The camera app runs buttery smooth and takes excellent pictures, and hard as it is to believe there will only be more improvements!</p>
<p>I was an iPhone user before and find myself missing the iOS ecosystem very little with this phone. It has all the apps and features that I need, they run well, and delivers a personal experience with great technical support staff available to talk about any problems that you have. It does a lot of things better than my iPhone, like provide instant access to all my folders and files on other devices through setting up a single app like Syncthing.</p>
<p>If you love Linux, having your own experience with technology and a great community attached to it, then you will love this phone. It’s worth it even beyond the added privacy and peace of mind that only linux can afford. So what are you waiting for? Go out and BUY your own now!</p>
</div>
		</div>
	</div>
</li><!-- #comment-## -->
            </ol>

                        </div>

            <p>Only logged in customers who have purchased this product may leave a review.</p>
    
    
</div>

					</div>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek: Advancing theorem proving in LLMs through large-scale synthetic data (163 pts)]]></title>
            <link>https://arxiv.org/abs/2405.14333</link>
            <guid>41838589</guid>
            <pubDate>Mon, 14 Oct 2024 15:44:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2405.14333">https://arxiv.org/abs/2405.14333</a>, See on <a href="https://news.ycombinator.com/item?id=41838589">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2405.14333">View PDF</a>
    <a href="https://arxiv.org/html/2405.14333v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Proof assistants like Lean have revolutionized mathematical proof verification, ensuring high accuracy and reliability. Although large language models (LLMs) show promise in mathematical reasoning, their advancement in formal theorem proving is hindered by a lack of training data. To address this issue, we introduce an approach to generate extensive Lean 4 proof data derived from high-school and undergraduate-level mathematical competition problems. This approach involves translating natural language problems into formal statements, filtering out low-quality statements, and generating proofs to create synthetic data. After fine-tuning the DeepSeekMath 7B model on this synthetic dataset, which comprises 8 million formal statements with proofs, our model achieved whole-proof generation accuracies of 46.3% with 64 samples and 52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at 23.0% with 64 samples and a tree search reinforcement learning method at 41.0%. Additionally, our model successfully proved 5 out of 148 problems in the Lean 4 Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4 failed to prove any. These results demonstrate the potential of leveraging large-scale synthetic data to enhance theorem-proving capabilities in LLMs. Both the synthetic dataset and the model will be made available to facilitate further research in this promising field.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Huajian Xin [<a href="https://arxiv.org/show-email/a0e31718/2405.14333">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 23 May 2024 09:03:42 UTC (4,425 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA's Europa Clipper Launch (201 pts)]]></title>
            <link>https://www.youtube.com/watch?v=lQToTWKwtuw</link>
            <guid>41838420</guid>
            <pubDate>Mon, 14 Oct 2024 15:28:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=lQToTWKwtuw">https://www.youtube.com/watch?v=lQToTWKwtuw</a>, See on <a href="https://news.ycombinator.com/item?id=41838420">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Busy Status Bar from Flipper Devices (1090 pts)]]></title>
            <link>https://busy.bar/?hn</link>
            <guid>41838337</guid>
            <pubDate>Mon, 14 Oct 2024 15:19:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://busy.bar/?hn">https://busy.bar/?hn</a>, See on <a href="https://news.ycombinator.com/item?id=41838337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-artboard-recid="784896582" data-artboard-screens="320,420,1200,1440,1720,1920" data-artboard-height="1111" data-artboard-valign="center" data-artboard-upscale="grid" data-artboard-height-res-320="2140" data-artboard-height-res-420="2490" data-artboard-height_vh-res-420="100" data-artboard-upscale-res-420="window" data-artboard-height-res-1200="831" data-artboard-height-res-1440="871" data-artboard-height-res-1720="1091" id="rec784896582" data-animationappear="off" data-record-type="396">             <div data-elem-id="1719228202659" data-elem-type="image" data-field-top-value="265" data-field-left-value="668" data-field-width-value="1127" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-filewidth-value="3000" data-field-fileheight-value="1066" data-field-top-res-320-value="454" data-field-left-res-320-value="-163" data-field-width-res-320-value="646" data-field-top-res-420-value="515" data-field-left-res-420-value="-201" data-field-width-res-420-value="802" data-field-top-res-1200-value="213" data-field-left-res-1200-value="413" data-field-width-res-1200-value="726" data-field-top-res-1440-value="223" data-field-left-res-1440-value="493" data-field-width-res-1440-value="881" data-field-top-res-1720-value="289" data-field-left-res-1720-value="590" data-field-width-res-1720-value="1016"> <p><img data-original="img/tild6163-3961-4233-a438-663566386561__bv_orange_3_1.png" alt="" imgfield="tn_img_1719228202659" src="https://busy.bar/img/tild6163-3961-4233-a438-663566386561__bv_orange_3_1.png"> </p> </div> <div data-elem-id="1722857618734" data-elem-type="text" data-field-top-value="905" data-field-left-value="372" data-field-width-value="198" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1692" data-field-top-res-420-value="1682" data-field-left-res-420-value="-612" data-field-width-res-420-value="331" data-field-top-res-1200-value="655" data-field-left-res-1200-value="197" data-field-width-res-1200-value="152" data-field-top-res-1440-value="740" data-field-left-res-1440-value="249" data-field-width-res-1440-value="159" data-field-top-res-1720-value="864" data-field-left-res-1720-value="333" data-field-width-res-1720-value="178"> <p>Cloud-based Python/JavaScript/Go apps</p> </div>     <div data-elem-id="1722514507053" data-elem-type="text" data-field-top-value="836" data-field-left-value="1174" data-field-width-value="235" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1899" data-field-left-res-320-value="32" data-field-width-res-320-value="123" data-field-top-res-420-value="2192" data-field-left-res-420-value="38" data-field-width-res-420-value="157" data-field-top-res-1200-value="616" data-field-left-res-1200-value="741" data-field-width-res-1200-value="183" data-field-top-res-1440-value="680" data-field-left-res-1440-value="892" data-field-width-res-1440-value="189" data-field-top-res-1720-value="802" data-field-left-res-1720-value="1051" data-field-width-res-1720-value="211"> <p>&gt; Free JavaScript apps SDK</p> </div> <p data-elem-id="1722857660392" data-elem-type="text" data-field-top-value="798" data-field-left-value="665" data-field-width-value="331" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1601" data-field-left-res-320-value="34" data-field-width-res-320-value="251" data-field-top-res-420-value="1851" data-field-left-res-420-value="40" data-field-width-res-420-value="303" data-field-top-res-1200-value="578" data-field-left-res-1200-value="397" data-field-width-res-1200-value="271" data-field-top-res-1440-value="650" data-field-left-res-1440-value="483" data-field-width-res-1440-value="265" data-field-top-res-1720-value="766" data-field-left-res-1720-value="595" data-field-width-res-1720-value="339"> <h3 field="tn_text_1722857660392">Time management technique based on short intervals of focused work broken by five-minute breaks.</h3> </p> <div data-elem-id="1722514534198" data-elem-type="text" data-field-top-value="862" data-field-left-value="1174" data-field-width-value="253" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1937" data-field-left-res-320-value="32" data-field-width-res-320-value="109" data-field-top-res-420-value="2239" data-field-left-res-420-value="38" data-field-width-res-420-value="137" data-field-top-res-1200-value="636" data-field-left-res-1200-value="741" data-field-width-res-1200-value="209" data-field-top-res-1440-value="702" data-field-left-res-1440-value="892" data-field-width-res-1440-value="230" data-field-top-res-1720-value="826" data-field-left-res-1720-value="1051" data-field-width-res-1720-value="243"> <p>&gt; Libs for Python/JavaScript/Go</p> </div>  <div data-elem-id="1722857675736" data-elem-type="text" data-field-top-value="905" data-field-left-value="859" data-field-width-value="197" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1704" data-field-left-res-320-value="43" data-field-width-res-320-value="245" data-field-top-res-420-value="1974" data-field-left-res-420-value="53" data-field-width-res-420-value="348" data-field-top-res-1200-value="655" data-field-left-res-1200-value="526" data-field-width-res-1200-value="130" data-field-top-res-1440-value="740" data-field-left-res-1440-value="639" data-field-width-res-1440-value="158" data-field-top-res-1720-value="864" data-field-left-res-1720-value="769" data-field-width-res-1720-value="177"> <p>Integration with hourly payment time trackers</p> </div> <div data-elem-id="1722514566656" data-elem-type="text" data-field-top-value="914" data-field-left-value="1174" data-field-width-value="261" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="2013" data-field-left-res-320-value="32" data-field-width-res-320-value="125" data-field-top-res-420-value="2333" data-field-left-res-420-value="38" data-field-width-res-420-value="146" data-field-top-res-1200-value="676" data-field-left-res-1200-value="741" data-field-width-res-1200-value="211" data-field-top-res-1440-value="746" data-field-left-res-1440-value="892" data-field-width-res-1440-value="210" data-field-top-res-1720-value="872" data-field-left-res-1720-value="1051" data-field-width-res-1720-value="252"> <p>&gt; Self-hosted cloud provisioning</p> </div>  <div data-elem-id="1722857692064" data-elem-type="text" data-field-top-value="905" data-field-left-value="681" data-field-width-value="150" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1678" data-field-left-res-320-value="43" data-field-width-res-320-value="260" data-field-top-res-420-value="1939" data-field-left-res-420-value="53" data-field-width-res-420-value="348" data-field-top-res-1200-value="655" data-field-left-res-1200-value="410" data-field-width-res-1200-value="100" data-field-top-res-1440-value="740" data-field-left-res-1440-value="497" data-field-width-res-1440-value="121" data-field-top-res-1720-value="864" data-field-left-res-1720-value="609" data-field-width-res-1720-value="134"> <p>Configure your own focus intervals</p> </div>  <div data-elem-id="1722514579043" data-elem-type="text" data-field-top-value="809" data-field-left-value="1461" data-field-width-value="222" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1860" data-field-left-res-320-value="168" data-field-width-res-320-value="114" data-field-top-res-420-value="2161" data-field-left-res-420-value="220" data-field-width-res-420-value="153" data-field-top-res-1200-value="595" data-field-left-res-1200-value="961" data-field-width-res-1200-value="207" data-field-top-res-1440-value="657" data-field-left-res-1440-value="1122" data-field-width-res-1440-value="178" data-field-top-res-1720-value="778" data-field-left-res-1720-value="1308" data-field-width-res-1720-value="199"> <p>&gt; Serial COM port over USB</p> </div>  <div data-elem-id="1722514579041" data-elem-type="text" data-field-top-value="835" data-field-left-value="1461" data-field-width-value="338" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1898" data-field-left-res-320-value="168" data-field-width-res-320-value="130" data-field-top-res-420-value="2208" data-field-left-res-420-value="220" data-field-width-res-420-value="174" data-field-top-res-1200-value="615" data-field-left-res-1200-value="961" data-field-width-res-1200-value="233" data-field-top-res-1440-value="679" data-field-left-res-1440-value="1122" data-field-width-res-1440-value="271" data-field-top-res-1720-value="802" data-field-left-res-1720-value="1308" data-field-width-res-1720-value="330"> <p>&gt; IoT integrations: IFTTT, HomeAssistant</p> </div>  <div data-elem-id="1722514579035" data-elem-type="text" data-field-top-value="887" data-field-left-value="1461" data-field-width-value="364" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1960" data-field-left-res-320-value="168" data-field-width-res-320-value="123" data-field-top-res-420-value="2285" data-field-left-res-420-value="220" data-field-width-res-420-value="146" data-field-top-res-1200-value="667" data-field-left-res-1200-value="961" data-field-width-res-1200-value="189" data-field-top-res-1440-value="723" data-field-left-res-1440-value="1122" data-field-width-res-1440-value="292" data-field-top-res-1720-value="848" data-field-left-res-1720-value="1308" data-field-width-res-1720-value="326"> <p>&gt; Offline API (no internet required)</p> </div>    <p data-elem-id="1722857591333" data-elem-type="text" data-field-top-value="798" data-field-left-value="152" data-field-width-value="374" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1362" data-field-left-res-320-value="33" data-field-width-res-320-value="246" data-field-top-res-420-value="1585" data-field-left-res-420-value="39" data-field-width-res-420-value="329" data-field-top-res-1200-value="578" data-field-left-res-1200-value="63" data-field-width-res-1200-value="268" data-field-top-res-1440-value="650" data-field-left-res-1440-value="72" data-field-width-res-1440-value="300" data-field-top-res-1720-value="766" data-field-left-res-1720-value="136" data-field-width-res-1720-value="360"> <h3 field="tn_text_1722857591333">Built-in apps: clock, weather, social media metrics, currency chart, pixel art wallpapers, and more.</h3> </p> <div data-elem-id="1722857608514" data-elem-type="text" data-field-top-value="905" data-field-left-value="171" data-field-width-value="164" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1441" data-field-left-res-320-value="44" data-field-width-res-320-value="260" data-field-top-res-420-value="1674" data-field-left-res-420-value="53" data-field-width-res-420-value="348" data-field-top-res-1200-value="655" data-field-left-res-1200-value="75" data-field-width-res-1200-value="113" data-field-top-res-1440-value="740" data-field-left-res-1440-value="87" data-field-width-res-1440-value="141" data-field-top-res-1720-value="864" data-field-left-res-1720-value="147" data-field-width-res-1720-value="146"> <p>Install JavaScript apps from community</p> </div>     <div data-elem-id="1722872865168" data-elem-type="text" data-field-top-value="1038" data-field-left-value="-100" data-field-width-value="160" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1465" data-field-left-res-320-value="43" data-field-width-res-320-value="247" data-field-top-res-420-value="1707" data-field-left-res-420-value="53" data-field-width-res-420-value="331" data-field-top-res-1440-value="888" data-field-left-res-1440-value="229" data-field-width-res-1440-value="159" data-field-top-res-1720-value="982" data-field-left-res-1720-value="-89" data-field-width-res-1720-value="143"> <p>Remote hosted Python/Javascript/Go apps via API</p> </div> <p data-elem-id="1722514020064" data-elem-type="text" data-field-top-value="386" data-field-left-value="146" data-field-width-value="398" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1013" data-field-left-res-320-value="31" data-field-width-res-320-value="260" data-field-top-res-420-value="1208" data-field-left-res-420-value="37" data-field-width-res-420-value="296" data-field-top-res-1200-value="266" data-field-left-res-1200-value="59" data-field-width-res-1200-value="259" data-field-top-res-1440-value="314" data-field-left-res-1440-value="68" data-field-width-res-1440-value="326" data-field-top-res-1720-value="372" data-field-left-res-1720-value="130" data-field-width-res-1720-value="326"> <h3 field="tn_text_1722514020064">Customizable busy status message to match your own workflow.</h3> </p> <div data-elem-id="1722514063649" data-elem-type="text" data-field-top-value="462" data-field-left-value="164" data-field-width-value="453" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1072" data-field-left-res-320-value="41" data-field-width-res-320-value="215" data-field-top-res-420-value="1271" data-field-left-res-420-value="50" data-field-width-res-420-value="288" data-field-top-res-1200-value="344" data-field-left-res-1200-value="71" data-field-width-res-1200-value="278" data-field-top-res-1440-value="374" data-field-left-res-1440-value="82" data-field-width-res-1440-value="364" data-field-top-res-1720-value="440" data-field-left-res-1720-value="147" data-field-width-res-1720-value="342"> <p>Set any busy message, expiry timer and activation trigger</p> </div> <div data-elem-id="1722857497787" data-elem-type="text" data-field-top-value="500" data-field-left-value="164" data-field-width-value="453" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1116" data-field-left-res-320-value="41" data-field-width-res-320-value="253" data-field-top-res-420-value="1326" data-field-left-res-420-value="50" data-field-width-res-420-value="338" data-field-top-res-1200-value="385" data-field-left-res-1200-value="71" data-field-width-res-1200-value="300" data-field-top-res-1440-value="404" data-field-left-res-1440-value="82" data-field-width-res-1440-value="364" data-field-top-res-1720-value="501" data-field-left-res-1720-value="147" data-field-width-res-1720-value="405"> <p>Upload custom busy graphics or choose from gallery</p> </div> <div data-elem-id="1722857506371" data-elem-type="text" data-field-top-value="535" data-field-left-value="164" data-field-width-value="371" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1160" data-field-left-res-320-value="41" data-field-width-res-320-value="232" data-field-top-res-420-value="1361" data-field-left-res-420-value="50" data-field-width-res-420-value="310" data-field-top-res-1200-value="408" data-field-left-res-1200-value="71" data-field-width-res-1200-value="245" data-field-top-res-1440-value="433" data-field-left-res-1440-value="82" data-field-width-res-1440-value="298" data-field-top-res-1720-value="534" data-field-left-res-1720-value="147" data-field-width-res-1720-value="333"> <p>Activate manually from device or remotely from PC, Mobile App or via API</p> </div> <div data-elem-id="1722857515571" data-elem-type="text" data-field-top-value="594" data-field-left-value="164" data-field-width-value="371" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1204" data-field-left-res-320-value="41" data-field-width-res-320-value="220" data-field-top-res-420-value="1417" data-field-left-res-420-value="50" data-field-width-res-420-value="295" data-field-top-res-1200-value="450" data-field-left-res-1200-value="71" data-field-width-res-1200-value="245" data-field-top-res-1440-value="483" data-field-left-res-1440-value="82" data-field-width-res-1440-value="298" data-field-top-res-1720-value="587" data-field-left-res-1720-value="147" data-field-width-res-1720-value="333"> <p>Automatic activation by Zoom, Discord, Microsoft Teams, Google Calendar</p> </div>      </div><div data-artboard-recid="796388741" data-artboard-screens="320,420,1200,1440,1720,1920" data-artboard-height="1010" data-artboard-valign="center" data-artboard-upscale="grid" data-artboard-height-res-320="1080" data-artboard-height-res-420="1250" data-artboard-height_vh-res-420="100" data-artboard-upscale-res-420="window" data-artboard-height-res-1200="740" data-artboard-height-res-1440="790" id="rec796388741" data-animationappear="off" data-record-type="396">    <div data-elem-id="1723051146284" data-elem-type="image" data-field-top-value="162" data-field-left-value="2238" data-field-width-value="200" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-filewidth-value="1600" data-field-fileheight-value="1107" data-field-top-res-320-value="199" data-field-left-res-320-value="-85" data-field-width-res-320-value="500" data-field-top-res-420-value="242" data-field-left-res-420-value="-90" data-field-width-res-420-value="640"> <p><img data-original="img/tild3262-3138-4461-a632-346233363732__mobile_headset.jpg" alt="" imgfield="tn_img_1723051146284" src="https://busy.bar/img/tild3262-3138-4461-a632-346233363732__mobile_headset.jpg"> </p> </div>  <p data-elem-id="1723037385419" data-elem-type="text" data-field-top-value="208" data-field-left-value="111" data-field-width-value="713" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-animate-prx="scroll" data-field-top-res-320-value="60" data-field-left-res-320-value="25" data-field-top-res-420-value="80" data-field-left-res-420-value="71" data-field-width-res-420-value="270" data-field-axisy-res-420-value="top" data-field-top-res-1200-value="153" data-field-left-res-1200-value="40" data-field-top-res-1440-value="160" data-field-left-res-1440-value="41" data-field-width-res-1440-value="641"> <h2 field="tn_text_1723037385419">Live Busy status</h2> </p>  <p data-elem-id="1723037385412" data-elem-type="text" data-field-top-value="306" data-field-left-value="111" data-field-width-value="622" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="546" data-field-left-res-320-value="15" data-field-width-res-320-value="288" data-field-top-res-420-value="656" data-field-left-res-420-value="15" data-field-width-res-420-value="383" data-field-top-res-1200-value="225" data-field-left-res-1200-value="40" data-field-width-res-1200-value="468" data-field-top-res-1440-value="241" data-field-left-res-1440-value="42" data-field-width-res-1440-value="524"> <h3 field="tn_text_1723037385412">Busy Status Bar can integrate with desktop software and automatically activate when you’re on a call, live on stream, recording audio or when a certain program is active.</h3> </p> <div data-elem-id="1723037476748" data-elem-type="text" data-field-top-value="488" data-field-left-value="191" data-field-width-value="361" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="713" data-field-left-res-320-value="68" data-field-width-res-320-value="263" data-field-top-res-420-value="855" data-field-left-res-420-value="80" data-field-width-res-420-value="352" data-field-top-res-1200-value="345" data-field-left-res-1200-value="92" data-field-width-res-1200-value="278" data-field-top-res-1440-value="374" data-field-left-res-1440-value="102" data-field-width-res-1440-value="356"> <p>Automatic “On Call” status</p> </div>  <div data-elem-id="1723037523349" data-elem-type="text" data-field-top-value="530" data-field-left-value="191" data-field-width-value="375" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="746" data-field-left-res-320-value="68" data-field-width-res-320-value="239" data-field-top-res-420-value="891" data-field-left-res-420-value="80" data-field-width-res-420-value="303" data-field-top-res-1200-value="377" data-field-left-res-1200-value="92" data-field-width-res-1200-value="336" data-field-top-res-1440-value="409" data-field-left-res-1440-value="102" data-field-width-res-1440-value="305"> <p>When the microphone is activated on the computer, the device will automatically display an “on call” status.</p> </div> <div data-elem-id="1723037779285" data-elem-type="text" data-field-top-value="656" data-field-left-value="191" data-field-width-value="451" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="865" data-field-left-res-320-value="68" data-field-width-res-320-value="244" data-field-top-res-420-value="1018" data-field-left-res-420-value="80" data-field-width-res-420-value="299" data-field-top-res-1200-value="471" data-field-left-res-1200-value="92" data-field-width-res-1200-value="375" data-field-top-res-1440-value="507" data-field-left-res-1440-value="102" data-field-width-res-1440-value="370"> <p>When you are streaming through any software like OBS (Open Broadcaster Software), Busy Status Bar will automatically turn on.</p> </div>    <div data-elem-id="1723037822598" data-elem-type="text" data-field-top-value="777" data-field-left-value="191" data-field-width-value="368" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1005" data-field-left-res-320-value="68" data-field-width-res-320-value="263" data-field-top-res-420-value="1148" data-field-left-res-420-value="80" data-field-width-res-420-value="352" data-field-top-res-1200-value="567" data-field-left-res-1200-value="92" data-field-width-res-1200-value="278" data-field-top-res-1440-value="609" data-field-left-res-1440-value="102" data-field-width-res-1440-value="356"> <p>Supports Windows / macOS / Linux.</p> </div>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists successfully breed corals to improve their heat tolerance (133 pts)]]></title>
            <link>https://phys.org/news/2024-10-scientists-successfully-corals-tolerance.html</link>
            <guid>41837925</guid>
            <pubDate>Mon, 14 Oct 2024 14:34:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2024-10-scientists-successfully-corals-tolerance.html">https://phys.org/news/2024-10-scientists-successfully-corals-tolerance.html</a>, See on <a href="https://news.ycombinator.com/item?id=41837925">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/scientists-have-succes.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/scientists-have-succes.jpg" data-sub-html="Credit: Dr. James Guest">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/scientists-have-succes.jpg" alt="Scientists have successfully bred corals to improve their heat tolerance" title="Credit: Dr. James Guest" width="800" height="530">
             <figcaption>
                Credit: Dr. James Guest
            </figcaption>        </figure>
    </div><p>A new study has shown that selective breeding can lead to a modest rise in coral heat tolerance.</p>


										      
																																	<p>Led by experts at Newcastle University's Coralassist Lab, the study documents the world's first effort to selectively breed adult corals for enhanced <a href="https://phys.org/tags/heat+tolerance/" rel="tag">heat tolerance</a>, i.e. the ability of adult corals to survive intense marine heat waves. The breeding effort was a success, showing that it is possible to improve the heat tolerance of adult coral offspring, even in a single generation.</p>
<p>However, the improvement was modest in comparison to future marine heat waves expected under climate change. The authors stress that rapid reductions of global greenhouse gas emissions are an absolute requirement to mitigate warming and give corals an opportunity to adapt.</p>
<p>The study, published in the journal <i>Nature Communications</i>, was carried out in partnership with the University of Victoria, Horniman Museum and Gardens, Palau International Coral Reef Center, University of Derby, and the University of Exeter.</p>
<p>The publication is the result of a five-year project which was launched by Dr. James Guest.</p>
<h2>Not a silver bullet solution</h2>
<p>"This work shows that <a href="https://phys.org/tags/selective+breeding/" rel="tag">selective breeding</a> is feasible but not a silver bullet solution and that more research is needed to maximize breeding outcomes," says study lead author, Liam Lachs, a Postdoctoral Research Associate at Newcastle University. He continues, reflecting that "in parallel, rapid reductions of global greenhouse gas emissions are an absolute requirement to mitigate warming and give corals an opportunity to adapt.</p>
<p>Dr. Guest, Reader in Coral Reef Ecology at Newcastle University's School of Natural and Environmental Sciences, explains that "the results show that selective breeding could be a viable tool to improve population resilience. Yet, there are still many challenges that need to be overcome.</p>
<p>"How many corals need to outplanted to benefit wild populations? Can we ensure there are no trade-offs (evidence so far suggests this is not a large risk)? How can we avoid dilution of selected traits once added to the wild? How can we maximize responses to selection?</p>
<p>"Given the moderate levels of enhancement we achieved in this study, the effectiveness of such interventions will also depend on urgent climate action."</p>

																																						
																																			<h2>Successful breeding trial</h2>
<p>Selective breeding has been practiced by humans for thousands of years to produce animals and plants with desirable characteristics. Now it is being considered as a tool for nature conservation, particularly for coral reefs.</p>
<p>These <a href="https://phys.org/tags/marine+ecosystems/" rel="tag">marine ecosystems</a> are at the forefront of climate change impacts, as reef-building corals are highly sensitive to marine heat waves. These can trigger mass coral bleaching and mortality events which have already led to considerable reef declines globally.</p>
<p>The experts conducted selective breeding trials for two different traits, either the tolerance to a short intense heat exposure (10 days, reaching +3.5°C) or a less-intense but long-term exposure more typical of natural marine heat waves (1 month, reaching +2.5°C).</p>
<p>The team found that selecting parent colonies for high rather than low heat tolerance increased the tolerance of adult offspring. This result held for the response to both 10-day and one-month exposures. Heat tolerance could in theory be enhanced by approximately 1°C/week within one generation. However, this level of enhancement is likely insufficient to keep pace with unabated warming.</p>
<h2>What's next?</h2>
<p>Selectively breeding for short-stress tolerance did not show evidence of enhancing the ability of offspring to survive the long heat stress exposure. With no genetic correlation detected, it is plausible that these traits are under independent genetic controls.</p>
<p>This would have important implications, as interventions would benefit from cheap and rapid assays that can effectively identify heat tolerant colonies for breeding. However, if these assays do not predict adult colony survival to natural marine heat waves, it presents a serious challenge for management interventions.</p>
<p>Study lead author, Dr. Adriana Humanes, Postdoctoral Research Associate at the Coralassist Lab, Newcastle University, highlights that "considerable work remains before selective breeding can be successfully implemented. A deeper understanding is needed to determine which traits to prioritize and how these traits are genetically correlated."</p>
<h2>Take home message</h2>
<p>The authors say that this work is an important proof of concept: selective breeding corals for adult heat wave survival is possible. Now, they call for more research and development to understand how to operationalize breeding interventions and maximize outcomes to hopefully keep pace with the lower levels of warming that can be achieved with concurrent climate action.</p>

																																																					
																				<div>
																						<p><strong>More information:</strong>
												Selective breeding enhances coral heat tolerance to marine heatwaves, <i>Nature Communications</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1038/s41467-024-52895-1" target="_blank">DOI: 10.1038/s41467-024-52895-1</a>
																						
																						</p>
																					</div>
                               											
																					
                              										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Scientists successfully breed corals to improve their heat tolerance (2024, October 14)
												retrieved 14 October 2024
												from https://phys.org/news/2024-10-scientists-successfully-corals-tolerance.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Stallman Report (147 pts)]]></title>
            <link>https://stallman-report.org/</link>
            <guid>41837782</guid>
            <pubDate>Mon, 14 Oct 2024 14:19:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stallman-report.org/">https://stallman-report.org/</a>, See on <a href="https://news.ycombinator.com/item?id=41837782">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h3>October 14th, 2024</h3>
<p>Richard Stallman (aka “RMS”) is the founder of <a href="https://www.gnu.org/">GNU</a> and the
<a href="https://www.fsf.org/">Free Software Foundation</a> and present-day voting member of the Free Software
Foundation (FSF) board of directors and “Chief GNUisance” of the GNU project. He
is responsible for innumerable contributions to the free software movement,
setting its guiding principles, organizing political action, and directly
contributing to a flourishing free software ecosystem. The majority of
Stallman’s political activity has been of priceless value to society at large.</p>
<p>However, Stallman has been the subject of numerous allegations of misconduct.
Stallman has also incited numerous controversies for advancing a political
agenda which normalizes sexual misconduct and advocates for reforming our social
and legal understanding of sexual conduct in a manner which benefits the
perpetrators of abuse.</p>
<p>On the basis that Stallman has not demonstrated an understanding of his
misconduct; has not apologized for allegations of misconduct, alleged or
corroborated; continues to publish his harmful political program; and does not
acknowledge or apologize for harm done in the course of this program, this
report reiterates the position that Stallman should be removed from the board of
directors at the Free Software Foundation.</p>
<p>To support this case, we have catalogued the following:</p>
<ol>
<li>Primary sources documenting Stallman’s political advocacy for:
<ul>
<li>The normalization of sexual relations between adults and minors
<a href="#topicref-1">[1]</a>
</li>
<li>Defense of individuals both accused and convicted of sexual crimes,
including the rape of minors, sexual assault, and sexual harassment
<a href="#topicref-2">[2]</a>
</li>
<li>Dismissal of legal norms regarding sexual assault
<a href="#topicref-3">[3]</a>
</li>
<li>Dismissal of legal norms regarding sexual harassment
<a href="#topicref-4">[4]</a>
</li>
<li>Support for the possession of child sexual abuse material
<a href="#topicref-5">[5]</a>
</li>
<li>Legal and social normalization of sex between humans and animals
<a href="#topicref-6">[6]</a>
</li>
<li>Legal and social normalization of sex with corpses (necrophilia)
<a href="#topicref-7">[7]</a>
</li>
</ul>
</li>
<li>Credible allegations of sexual misconduct regarding Stallman
<a href="#topicref-8">[8]</a>
</li>
<li>Misconduct of the Free Software Foundation board of directors
<a href="#topicref-9">[9]</a>
</li>
<li>Calls from the free software community for Stallman’s removal
<a href="#topicref-10">[10]</a>
</li>
<li>Recommendations for reconciliation and closure
<a href="#topicref-11">[11]</a>
</li>
</ol>
<div>
	<p><strong>Content warning</strong>: This report catalogues and directly quotes hundreds of
statements from Richard Stallman of an extremely offensive nature on subjects
including rape, sexual assault, child sexual abuse, sexual exploitation of
children, and more. Sensitive readers are <strong>strongly advised</strong> to proceed with
caution.</p>
<p>If you or someone you know has been the victim of sexual violence, help is
available. The United States National Sexual Assault Hotline can be reached at
800-656-HOPE (4673), and is available for <a href="https://hotline.rainn.org/online">live chat
online</a> 24/7. You can speak to a trained
expert for confidential support at any time.</p>
<p>International readers are directed to
<a href="https://www.hotpeachpages.net/">HotPeachPages</a> for resources in your location
and your language, and <a href="https://childhelplineinternational.org/helplines/">Child Helpline International</a>
provides international resources specifically aimed at the needs of children and
young people.</p>

</div>

<h2 id="statement-regarding-stallmans-medical-situation">
	Statement regarding Stallman’s medical situation
	
	<a href="#statement-regarding-stallmans-medical-situation"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>We understand that Richard Stallman was diagnosed with follicular
lymphoma in 2023.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> We are pleased to hear that his prognosis is good and
his cancer is in remission. We wish Stallman good health, a peaceful recovery,
and many more years of health.</p>
<p>We are not of the opinion that Stallman’s cancer diagnosis absolves him of
responsibility for his actions, past and present. We urge Stallman to reconsider
his controversial political positions and issue retractions and/or apologies to
the extent that his health permits him to do so, and draw attention to the fact
that Stallman continues to forward his controversial views following his
diagnosis. We also urge the free software community to hold Stallman accountable
for his actions and to contend with our history of sexism and tolerating abuse.</p>
<h2 id="why-publish-this-report">
	Why publish this report?
	
	<a href="#why-publish-this-report"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>Richard Stallman has a profound influence on the free software community and our
movement. He is responsible for defining the four freedoms that steer us, he has
written all of our principal philosophy, he founded our foundational software
projects, and he is venerated as our ideological leader.</p>
<p>Richard Stallman has also embarked upon a decades-long political project to
normalize sexual violence. Under his ideological leadership, the free software
movement is unsafe, particularly for women. Women represent just 3% of the free
software community,<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> compared to 23% of industry
programmers generally.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> This is no accident. There is a
pervasive culture of sexism and a stark lack of accountability in free software,
and it begins with Stallman’s unchallenged and reprehensible behavior.</p>
<p>The case against Stallman is clear, and yet the free software community has
failed to act, in particular at the level of institutions and leadership but
also in the form of grassroots support for Stallman. Many defenses of Stallman
rely on a comfortable ignorance: ignorance of the scope and depth of Stallman’s
political campaign against women and victims of sexual violence, or a
comfortable belief that Stallman ceased his problematic behavior following his
2021 re-instatement in the Free Software Foundation. Some believe that
Stallman’s speech has not caused material harm, or that his fringe views are not
taken seriously; we provide evidence to dismiss all of these arguments in this
report.</p>
<p>Ignorance of the case against Stallman is due in part to the scattered and
disorganized nature of information regarding Stallman’s misconduct. Many of
those who raise a defense of Stallman have heard one or two uncorroborated
allegations of misconduct or one or two examples of years-old problematic
quotes, and understandably find it easier to excuse it as such. Furthermore,
those most directly accountable for Stallman’s behavior are the members of the
Free Software Foundation board of directors, and their misconduct in handling
the case is not widely known; this report brings this misconduct to light. By
carefully organizing information about Stallman’s misconduct and the misconduct
of the FSF board of directors into a single, comprehensive and exhaustively
cited report, the appeal to ignorance is no longer applicable.</p>
<p>This report collects hundreds of primary sources from 2003 to 2024 which clearly
demonstrate Stallman’s harmful political program and misconduct, meticulously
cataloged, analyzed, and subject to factual rebuttals. If the free software
community cannot address the blatant misconduct of Richard Stallman in the face
of overwhelming evidence, the free software community is not safe, and cannot be
made safe. Our institutions and our community must act. We have made several
recommendations for such actions at the end of the report.</p>
<p>First, we will justify our unqualified condemnation of Richard Stallman.</p>
<h2 id="stallmans-political-statements">
	Stallman’s political statements
	
	<a href="#stallmans-political-statements"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>Richard Stallman maintains a collection of political notes on his
website.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> He frequently publishes short political opinions on
his website here on a wide variety of topics. In this report we draw attention
to his political program on sex, drawing from his political notes as a primary
source.</p>
<p>Note that all quotes sourced from Stallman’s website for this report are direct
quotes of material publicly available at the time this report was prepared in
September 2024.<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> You may click any citation to
view it on Stallman’s website.</p>

<p>We have catalogued comments published by Stallman, mostly in the political notes
section of his personal website, and categorized comments of interest. Each link
leads to a page which provides a complete list of comments applicable to each
category. Some comments have been reproduced in multiple categories.</p>
<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Dates applicable</th>
      <th colspan="3">Occurrences</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th>Total</th>
      <th colspan="2">Retracted</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th>Yes</th>
      <th>No</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://stallman-report.org/on-children">Distinction between "children" and other minors</a></td>
      <td>2003-2024</td>
      <td>124</td>
      <td colspan="2">n/a</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-child-pornography">Support of child sexual abuse material</a></td>
      <td>2003-2019</td>
      <td>55</td>
      <td>0</td>
      <td>55</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/defense-of-sexual-misconduct">Defense of sexual misconduct</a></td>
      <td>2006-2023</td>
      <td>37</td>
      <td>1</td>
      <td>36</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-sex-with-minors">Support of sex between adults and minors</a></td>
      <td>2006-2019</td>
      <td>34</td>
      <td>5</td>
      <td>29</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-sexual-assault">Misrepresentation of sexual assault</a></td>
      <td>2015-2024</td>
      <td>24</td>
      <td>1</td>
      <td>23</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-sexual-harassment">Misrepresentation of sexual harassment</a></td>
      <td>2014-2018</td>
      <td>13</td>
      <td>0</td>
      <td>13</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-bestiality/">Support of bestiality</a></td>
      <td>2003-2018</td>
      <td>12</td>
      <td>0</td>
      <td>12</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-necrophilia">Support of necrophilia</a></td>
      <td>2003-2013</td>
      <td>3</td>
      <td>0</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<h3 id="normalization-of-sexual-relations-between-adults-and-minors">
	Normalization of sexual relations between adults and minors
	
	<a href="#normalization-of-sexual-relations-between-adults-and-minors"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>Richard Stallman has consistently advanced the political position that minors
can consent to sex with adults. Stallman’s position on minors having sex with
adults is the only position addressed in this report for which Stallman has
issued a retraction:</p>
<blockquote>
<p>Many years ago I posted that I could not see anything wrong about sex between
an adult and a child, if the child accepted it.</p>
<p>Through personal conversations in recent years, I’ve learned to understand how
sex with a child can harm per<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> psychologically. This changed my mind
about the matter: I think adults should not do that. I am grateful for the
conversations that enabled me to understand why.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2019-jul-oct.html#14_September_2019_(Sex_between_an_adult_and_a_child_is_wrong)">stallman.org, 14 September 2019 “Sex between an adult and a child is wrong”</a></p>
<p>However, as we will show, Stallman’s retraction is misleading and does not cover
the majority of his past statements on the subject. In short, we will show that
Stallman’s 2019 retraction only addresses his views regarding sex between adults
and pre-pubescent minors, and this retraction does not account for minors above
the age of 12 or 13.</p>
<p>Our report catalogues 34 political comments from Stallman making a political
case for sexual relationships between adults and minors. A strict reading of the
retraction applies it only to the singular political note that it references,
which Stallman has updated accordingly:</p>
<details>
	<summary>This quote is covered by Stallman's 2019 retraction. Click to show.</summary>
	<blockquote>
<p>Dutch pedophiles have formed a political party to campaign for legalization.</p>
<p>I am skeptical of the claim that voluntarily pedophilia harms children. The
arguments that it causes harm seem to be based on cases which aren’t
voluntary, which are then stretched by parents who are horrified by the idea
that their little baby is maturing.</p>
<p>[Many years after posting this note, I had conversations with people who had
been sexually abused as children and had suffered harmful effects. These
conversations eventually convinced me that the practice is harmful and adults
should not do it.]</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2006-mar-jun.html#05%20June%202006%20(Dutch%20paedophiles%20form%20political%20party)">stallman.org, 05 June 2006 “Dutch paedophiles form political party”</a></p>

</details>

<p>Stallman has not updated similar comments with the same retraction. However, in
good faith we have assumed that the retraction applies to any comments which
would apply to minors under the age of 12 or 13, or explicitly refers to
“children”. Accounting for this, our catalog of Stallman’s 34 comments in
support of sex between adults and minors indicates that four have been fully
retracted and one has been partially retracted.</p>
<p><a href="https://stallman-report.org/on-sex-with-minors">Appendix: Stallman on sexual relations between adults and minors</a></p>
<hr>
<p>We justify our interpretation by citing 124 primary sources in which Stallman
insists on a distinction between “children” and other minors, in particular
teenagers. Our sources are dated from 2003 to 2024; Stallman has emphasized that
teenagers are distinct from “children”, on average, once every 9 weeks since
2003. Stallman has made this distinction 42 times following his 2019 retraction,
an average of once every 6½ weeks since the retraction.</p>
<p><a href="https://stallman-report.org/on-children">Appendix: Stallman’s idiolectical use of “child”</a></p>
<p>To understand Stallman’s remarks on sexual relationships between adults and
minors, we must show how Stallman distinguishes between “children” and other
minors.</p>
<p>Stallman is particular about his use of language, and a reading of his political
notes must be paired with an understanding of his idiolect. Stallman uses
numerous unconventional definitions and terminology in his political notes and
does so with rigour and consistency. In order to assist the reader in
interpreting Stallman’s political notes, he provides two resources on his
website: a glossary<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup> and an “anti-glossary”<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>.</p>
<p>Consider the following political note:</p>
<blockquote>
<p>The company Dataminr tries to scan all posted tweets to find anything
suggestive of possible violent intent, and report it to the thugs.</p>
</blockquote>
<p>– <a href="https://stallman.org/notes/2020-jul-oct.html#26_October_2020_(BIBO:_company_reports_tweets_to_thugs)">stallman.org, 16 October 2020, “BIBO: company reports tweets to thugs”</a></p>
<p>Out of context, the reader may be confused as to who the “thugs” are. The
glossary answers:</p>
<blockquote>
<p><strong>Thugs</strong>: the armed, usually uniformed marauders that attack protesters and
blacks, and make false accusations against them.</p>
<p>Usually only a few thugs commit the physical violence, but when one thug makes
a false accusation, the rest lie to support it. That’s why they deserve the
term “thugs” as a group. They are so habituated to perjury that they have
their own word for it: “testilying”.</p>
<p>A few members of thug departments are upright and refuse to support the
others’ lies. They are the honorable exceptions, and I express my respect for
them by calling them “police officers”.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/glossary.html#thug">stallman.org, “Glossary”</a></p>
<p>It follows that when Stallman says the word “thug”, he is referring to the
police, and indeed his use of the word “thug” is consistent with this in
thousands of his political notes. Many banal words and phrases are given a
similar treatment in his political notes and the two glossaries serve as a guide
for readers to interpret and understand his political notes as such.</p>
<p>Notably, the following definition also appears in Stallman’s anti-glossary:</p>
<blockquote>
<p><strong>Children</strong>: Humans up to age 12 or 13 are children. After that, they become
adolescents or teenagers. Let’s resist the practice of infantilizing
teenagers, by not calling them “children”.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/antiglossary.html#children">stallman.org, “Anti-Glossary”</a></p>
<p>That Stallman’s use of the word “child” is consistent with this idiolectical
definition and is re-enforced throughout Stallman’s political notes. He has
drawn a distinction between children and teenagers numerous times, which this
report catalogues in an appendix.</p>
<p><a href="https://stallman-report.org/on-children">Appendix: Stallman’s idiolectical use of “child”</a></p>
<p>Stallman has made this distinction most recently in June 2024:</p>
<blockquote>
<p>Please do not use the word “children” or “child” to refer to anyone under age
18. A 17-year-old is not a child. A 13-year-old is a teenager.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2024-mar-jun.html#16_June_2024_(Online_addictive-feeds_law,_NY)">stallman.org, 16 June 2024 “Online addictive-feeds law NY”</a></p>
<p>Stallman’s insistence on distinguishing children from other minors, in
particular teenagers, is often made with sexual overtures. For example, in
December 2023:</p>
<blockquote>
<p>The intended purpose of that law is to prevent minors from accessing porn
sites. To exclude everyone under 18 is unreasonably strict. They try to
justify this by referring to all minors as “children”. Even a person of age
17 is a “child” according to them.</p>
<p>To exclude only children – real children – from porn sites might be ok in
principle. But how to determine whether a given user is under the specified
age? The methods mentioned in the article either directly require a user to
identify perself, or indirectly require per to make perself vulnerable to
being identified.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2023-sep-dec.html#6_December_2023_(UK_age_verification_for_porn_sites)">stallman.org, 6 December 2023 “UK age verification for porn sites”</a></p>
<p>Or in April 2018, explicitly making this distinction in support of sexual
activity between adults and minors (denying the experience of two rape victims
in the process, see <a href="#defense-of-sexual-misconduct">defense of sexual misconduct</a>):</p>
<blockquote>
<p>It sounds horrible: “UN peacekeepers accused of child rape in South Sudan.”
But the article makes it pretty clear that the “children” involved were not
children. They were teenagers.</p>
<p>What about “rape”? Was this really rape? Or did they have sex willingly, and
prudes want to call it “rape” to make it sound like an injustice? We can’t
tell from the article which one it is.</p>
<p>Rape means coercing someone to have sex. Precisely because that is a grave and
clear wrong, using the same name for something much less grave is a
distortion.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2018-mar-jun.html#30_April_2018_(UN_peacekeepers_in_South_Sudan)">stallman.org, 30 April 2018 “UN peacekeepers in South Sudan”</a></p>
<p>This report considers Stallman’s idiolectical use of “child”, established in his
“anti-glossary” and re-enforced throughout his political notes, supports the
interpretation that his retraction only addresses sexual relationships between
adults and children up to the age of “12 or 13”, and that political notes which
remark upon sexual relationships between minors above the age of 12 or 13 are
not covered by his 2019 retraction.</p>
<hr>

<p>We now offer a rebuttal of Stallman’s political position regarding sexual
relations between adults and minors.</p>
<p><a href="https://stallman-report.org/on-sex-with-minors">Appendix: Stallman on sexual relations between adults and minors</a></p>
<p>Sexual relationships between adults and minors are prohibited by social and
legal norms because a differential of life and sexual experiences between adults
and minors enables adults to manipulate minors for the purpose of sexual
gratification. This bears out in statistics that highlight the risks sexual
relationships with adults impose on young girls in particular.</p>
<p>Older men often manipulate minors into unsafe sexual practices, leading to
undesirable outcomes for their victims. Young girls are often unprepared to
negotiate the use of contraception with an older partner, resulting in teenage
girls having unprotected sex at a rate that increases by 11% for each year older
their partner is. (Manlove, Ryan, Franzetta 2007)<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>
Minors who have sexual relationships with
partners 5 or more years older are 3.7 times more likely to experience an
unwanted pregnancy (Planned Parenthood, 2004; Darroch et al.,
1999)<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup> <sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup> and twice as likely to acquire a
sexually transmitted infection (STI) than peers who have partners similar in age
(Ryan, Franzetta 2008).<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup></p>
<p>Young women aged 15 to 17 who have had a relationship with a partner five years
or older than themselves have been forced to have sex at twice the rate of young
women who have only had similar-age relationships (Darroch 1999).<sup id="fnref1:11"><a href="#fn:11" role="doc-noteref">11</a></sup>
Minors who experience these rapes have poorer life outcomes than their peers;
women who are raped before the age of 18 are twice as likely to be raped in
adulthood (Tjaden, Thoennes 2000)<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup> and experience
significantly higher incidences of domestic abuse, mental health problems, low
self-esteem, and long-term intimacy problems in adulthood (Flemming et al
1999).<sup id="fnref:14"><a href="#fn:14" role="doc-noteref">14</a></sup> Abuse involving sexual intercourse increases this risk by
a factor of two (Flemming et al 1999).<sup id="fnref1:14"><a href="#fn:14" role="doc-noteref">14</a></sup></p>
<hr>
<p><em>Note on the sexual abuse of young men:</em></p>
<p><em>This report does not deny the experiences of young men who are victims of
sexual abuse. However, young heterosexual women are at a much higher risk of
exploitation than young heterosexual men (about 5× higher). The academic
literature tends to focus on the experiences of heterosexual young women as a
result, creating a gender bias that is unfortunately reproduced in our report
due to a lack of reliable sources. However, it is noted by Manlove et al. that
young boys who have sex before the age of 16 with an older partner are more than
twice as likely to father a child as a teen than young boys with similar aged
partners.</em><sup id="fnref1:9"><a href="#fn:9" role="doc-noteref">9</a></sup></p>
<hr>
<p>Stallman often makes the claim that it is normal for adults to be sexually
attracted to minors. On one occasion he has likened condemnation of this
attraction to homosexual conversion therapy:</p>
<blockquote>
<p>Research found that men generally find females of age 18 the most attractive.</p>
<p>This accords with the view that Stendhal reported in France in the 1800s, that
a woman’s most beautiful years were from 16 to 20.</p>
<p>Although this attitude on men’s part is normal, the author still wants to
present it as wrong or perverted, and implicitly demands men somehow control
their attraction to direct it elsewhere. Which is as absurd, and as
potentially oppressive, as claiming that homosexuals should control their
attraction and direct it towards to the other sex. Will men be pressured to
undergo “age conversion therapy” intended to brainwash them to feel attracted
mainly to women of their own age?</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2018-jul-oct.html#21_August_2018_(Age_and_attraction)">stallman.org, 21 August 2018 “Age and attraction”</a></p>
<p>The presumption that adult attraction to minors is “normal” is difficult to
characterize. The prevalence of adults with an attraction to post-pubescent
minors is unknown. The prevalence of adults with an attraction to pre-pubescent
minors is better studied, but poorly estimated; estimates for men are generally
around 5% (Seto 2009).<sup id="fnref:15"><a href="#fn:15" role="doc-noteref">15</a></sup> However, the editors note that pedophilia is
understood as a psychological pathology by the medical literature and is noted
as such by its inclusion in the DSM-5 (American Psychiatric Association, 2013).<sup id="fnref:16"><a href="#fn:16" role="doc-noteref">16</a></sup></p>
<p>Additionally, it is factually incorrect to assume that sexual abuse of minors is
motivated by a sexual attraction to minors. Studies show that only about 50% of
sexual exploitation of minors is motivated by sexual attraction.</p>
<blockquote>
<p>Although this preference increases the risk of engaging in CSA, only about 50%
of all individuals who do sexually abuse children are pedophilic (Blanchard et
al., 2001; Schaefer et al., 2010) and not every pedophilic individual actually
has abused children. The other 50% of individuals that have abused children
are those who do so without a sexual attraction to children; i.e., they lack
the necessary social skills to develop and maintain emotional and sexual
relationships with appropriately aged peers and look to “replacement partners”
in children as a kind of “surrogate” (Beier, 1998; Seto, 2008; Mokros et al.,
2012b). (Tenbergen et al, 2015)<sup id="fnref:17"><a href="#fn:17" role="doc-noteref">17</a></sup></p>
</blockquote>

<h3 id="defense-of-sexual-misconduct">
	Defense of sexual misconduct
	
	<a href="#defense-of-sexual-misconduct"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>Stallman’s political notes frequently respond to news articles about sexual
crimes by downplaying the severity of the crime and advocating on behalf of the
offender. Stallman’s political notes consistently contribute to a broader
harmful discourse which silences the experiences of victims of sexual violence.</p>
<p>This report catalogues 37 examples of Stallman expressing a defense of
individuals accused of or convicted of sexual harassment, sexual assault, or
rape (statutory or otherwise). The report only considers occasions where
Stallman acknowledges or assumes that the sexual act took place, or presents his
arguments as if it had taken place. We have omitted other occasions where
Stallman does not presume the act had taken place, for instance occasions where
Stallman emphasizes the presumption of innocence in legal proceedings.</p>
<p><a href="https://stallman-report.org/defense-of-sexual-misconduct">Appendix: Stallman’s defense of sexual misconduct</a></p>
<p>Among these sources we have identified at least 567 separate victims of sexual
misconduct whose experience was downplayed or dismissed by
Stallman.<sup id="fnref:18"><a href="#fn:18" role="doc-noteref">18</a></sup></p>

<p>It is demonstrable that Stallman’s defenses of sexual misconduct cause material
harm to victims. Rhetoric which denies or downplays a victim’s experience of
sexual misconduct causes harm:</p>
<blockquote>
<p>One of the most important factors that predicts severity of post-trauma
symptomatology in any rape victim is the post-trauma response received from
the environment. For example, where a victims’ experience of rape is ignored
(deliberately or as a result of people simply not knowing), not recognised,
minimised, or both; and where victims are blamed, judged as culpable, met with
further violence, violation, or both. Lack of empathy and understanding can,
therefore, reduce the prospects for a recovery. (Mason, Lodrick 2013)<sup id="fnref:19"><a href="#fn:19" role="doc-noteref">19</a></sup></p>
</blockquote>
<p>The symptoms of complex post-traumatic stress disorder (CPTSD) in victims of
sexual assault which are exacerbated by rhetoric similar to Stallman’s are
severe. It is also noted that the symptoms of post-traumatic stress disorder in
sexual assault victims are exacerbated if the victim is very young.</p>
<blockquote>
<p>Post-traumatic stress disorder is an extremely distressing and disabling
condition. Intrusive symptoms such as ﬂashbacks, nightmares and feeling as
though the assault is reoccurring are profoundly upsetting to individuals who
experience them. Their psychological response is often to become avoidant of
thoughts, feelings, places and other reminders of the assault.
(Mason, Lodrick 2013)<sup id="fnref1:19"><a href="#fn:19" role="doc-noteref">19</a></sup></p>
</blockquote>
<p>Exposure to Stallman’s rhetoric not only harms the victims of the incidents to
which he refers, but also harms victims of similar experiences which are exposed
to his remarks.</p>
<p>Stallman’s defenses of sexual misconduct rely on a number of recurring premises.
A common defense relies on Stallman’s insistence that minors over the age of 12
or 13 are sexually mature and can meaningfully consent to having sex with an
adult. Consider Stallman’s remarks on the case of Cody Wilson:</p>
<blockquote>
<p>Cody Wilson has been charged with “sexual assault” on a “child” after a
session with a sex worker of age 16. (…)</p>
<p>The article refers to the sex worker as a “child”, but that is not so.
Elsewhere it has been published that she is 16 years old. That is late
adolescence, not childhood.</p>
<p>Calling teenagers “children” encourages treating teenagers as children, a
harmful practice which retards their development into capable adults.</p>
<p>In this case, the effect of that mislabeling is to smear Wilson. It is rare,
and considered perverse, for adults to be physically attracted to children.
However, it is normal for adults to be physically attracted to adolescents.
Since the claim sbout[<em>sic</em>] Wilson is the latter, it is wrong to present it
as the former.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2018-jul-oct.html#23_September_2018_(Cody_Wilson)">stallman.org, 23 September 2018 “Cody Wilson”</a></p>
<p>Laws regarding rape are more general than cases of outwardly apparent coercion.
We have provided a general rebuttal of Stallman’s political position on sexual
relations between adults and minors <a href="#minor-adult-sex-rebuttal">elsewhere in the
report</a>.</p>
<p>Other defenses of sexual misconduct by Stallman focus on an insistence on
appropriate use of language in order to establish the “gravity” of the crime in
order to determine how the public should “judge” an offender. This defense is
often associated with Stallman’s fixation on the term “sexual assault”, which we
cover <a href="#dismissal-of-legal-norms-regarding-sexual-assault">elsewhere in this report</a>.</p>
<blockquote>
<p>Jelani Maraj (who I had never heard of) could be imprisoned for a long time
for “sexual assault”. What does that concretely mean?</p>
<p>Due to the vagueness of the term “sexual assault” together with the dishonest
law that labels sex with adolescents as “rape” even if they are willing, we
cannot tell from this article what sort of acts Maraj was found to have
committed. So we can’t begin to judge whether those acts were wrong.</p>
<p>I see at least three possibilities. Perhaps those acts really constituted
rape — it is a possibility. Or perhaps the two had sex willingly, but her
parents freaked out and demanded prosecution. Or, intermediate between those
two, perhaps he pressured her into having sex, or got her drunk.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2017-nov-feb.html#13_November_2017_(Jelani_Maraj)">stallman.org, 13 November 2017 “Jelani Maraj”</a></p>
<p>In this comment, Stallman belittles the experience of a rape
victim<sup id="fnref:20"><a href="#fn:20" role="doc-noteref">20</a></sup> and argues for the following positions:</p>
<ol>
<li>Adolescents can consent to sex with adults (<a href="#minor-adult-sex-rebuttal">rebuttal</a>)</li>
<li>Pressuring someone into sex is an “intermediate” offense between overtly
consensual sex and sexual assault.</li>
<li>Making someone drunk for the purpose of sexual assault is “intermediate”
offense between overtly consensual sex and sexual assault.</li>
</ol>
<p>This is an example of Stallman’s regular insistence that sexual crimes be
discussed in highly specific language for the purpose of establishing the
gravity of the crime so that the public may judge the offender by measures.</p>
<p>Moreover, in this respect Stallman’s defenses of sexual misconduct are based on
a dismissal of the importance of consent. Stallman consistently defends
scenarios where he presumes consent due to a perceived absence of violent
coercion.</p>
<p>This form of defense also appears in Stallman’s frequent defenses of Julian
Assange.</p>
<blockquote>
<p>Personal attacks against Julian Assange are used to distract attention from
the heroic achievements of Wikileaks.</p>
<p>Ironically, this article itself exaggerates criticism of Assange by stating
that the allegations against him consist of “rape” — they do not.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2012-may-aug.html#2_July_2012_(Attacks)">stallman.org, 2 July 2012 “Attacks”</a></p>
<p>The editors acknowledge that the political circumstances surrounding allegations
of sexual misconduct by Julian Assange may represent due cause to doubt the
allegations. However, we draw attention to the fact that Stallman does not argue
from a presumption of Assange’s innocence, but rather from an objection to the
presumed act being classified as rape.</p>
<p>Among other accusations, one of the presumed acts is that a woman woke up to
discover Assange having unprotected sex with her as she slept. The two had had
consensual sex the prior evening on the condition that Assange used a condom. We
can conclude that Stallman dismisses the conditional nature of the victim’s
consent regarding condoms, and argues that the consent agreed upon on the prior
evening “carries over” to sex with a sleeping victim the following morning.
Stallman made this clear on August 12th, 2012:</p>
<blockquote>
<p>If Assange had sex with a sleeping woman, the morning after they had sex and
then slept together, was that rape? MP George Galloway says no.</p>
<p>Waking up your lover with sex is a tradition that has given pleasure to many,
and prohibiting it by designating it as rape is absurd. If that’s what the law
says in some country, that law is absurd.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2012-may-aug.html#21_August_2012_(Assange)">stallman.org, 21 August 2012 “Assange”</a></p>
<p>We also identify one additional theme in Stallman’s defenses of sexual
misconduct, which are based on an outright misrepresentation of the events
concerned. For example, in response to a case where Ohio State athletics teacher
Dr Richard Strauss was revealed to have sexually abused 177 students over the
course of 20 years, Stallman writes the following:</p>
<blockquote>
<p>Should we accept stretching the terms “sexual abuse” and “molestation” to
include looking without touching?</p>
<p>I do not accept it.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2019-may-aug.html#11_June_2019_(Stretching_meaning_of_terms)">stallman.org, 11 June 2019 “Stretching meaning of terms”</a></p>
<p>The article Stallman cites includes the following quote:</p>
<blockquote>
<p>Many of Strauss’s accusers who have spoken publicly said they were masturbated
or otherwise touched inappropriately during physical exams or leered at in the
locker rooms.</p>
</blockquote>
<p>– <a href="https://www.theguardian.com/us-news/2019/may/17/ohio-state-report-reveals-team-doctor-sexually-abused-at-least-177-athletes">The Guardian, 17 May 2019</a></p>
<p>We also cite the following example from December 2017:</p>
<blockquote>
<p>Mormon feminists are challenging sexual abuse in the Mormon church, which
combines with scorn for women that aren’t “chaste” to cause great suffering.</p>
<p>There are fathers that rape their daughters — and there are also “recovered
memory therapists” that implant false memories of childhood sexual abuse that
didn’t happen. A priori, either one could have happened here. The fact that
Carol did not remember the abuse until she worked with a therapist makes me
suspect the latter. It seems that Carol’s sister also need “help” to remember.</p>
<p>I hope there is a way to determine which one really occurred.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2017-sep-dec.html#1_December_2017_(Mormon_sexual_abuse)">stallman.org, 1 December 2017 “Mormon sexual abuse”</a></p>
<p>In this example, Stallman invokes “recovered memory therapists”, which is not
referenced in the cited text, to sow doubt on the stories of Mormon survivors of
rape.</p>

<h3 id="dismissal-of-legal-norms-regarding-sexual-assault">
	Dismissal of legal norms regarding sexual assault
	
	<a href="#dismissal-of-legal-norms-regarding-sexual-assault"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We have catalogued 24 primary sources in which Stallman misrepresents or
downplays sexual assault between 2015 and 2024 as part of a broader political
program that aims to dismiss the experiences of victims and erode social and
legal norms around sexual assault.</p>
<p><a href="https://stallman-report.org/on-sexual-assault">Appendix: Stallman on sexual assault</a></p>
<p>Like “children”, in Stallman’s speech “sexual assault” is an idiolectical term
which Stallman defines in his “anti-glossary”:</p>
<blockquote>
<p><strong>Sexual assault</strong>: The term is applied to a broad range of actions, from rape
on one end, to the least physical contact on the other, as well as everything
in between. It acts as propaganda for treating them all the same. That would
be wrong.</p>
<p>The term is further stretched to include sexual harassment, which does not
refer to a single act, but rather to a series of acts that amounts to a form
of gender bias. Gender bias is rightly prohibited in certain situations for
the sake of equal opportunity, but that is a different issue.</p>
<p>I don’t think that rape should be treated the same as a momentary touch.
People we accuse have a right to those distinctions, so I am careful not to
use the term “sexual assault” to categorize the actions of any person on any
specific occasion.<sup id="fnref1:8"><a href="#fn:8" role="doc-noteref">8</a></sup></p>
</blockquote>
<p>The gross misrepresentation of sexual harassment in this quote is not lost on
the editors, and is covered in the next section in detail.</p>
<p>Sexual assault is more accurately defined as an assault of a sexual nature. It
refers to an act of assault – an unwanted physical interactions – with
sexual motivations. The United States National Center for Victims of Crime
provides the following explanation:</p>
<blockquote>
<p>Sexual assault is an act of forcing another person into sexual activity
against his or her will. Sexual assault takes many forms, including rape or
attempted rape, as well as any unwanted sexual contact. The crime includes
forced sexual intercourse (rape), sodomy (oral or anal sexual acts), child
molestation, incest, fondling, and attempted rape.<sup id="fnref:21"><a href="#fn:21" role="doc-noteref">21</a></sup></p>
</blockquote>
<p>Stallman has misrepresented sexual assault many times. For instance, in October
2023, Stallman writes the following:</p>
<blockquote>
<p>I warned that the stretchable term “sexual assault”, which extends from grave
crimes such as rape through significant crimes such as groping and down to no
clear lower bound, could be stretched to criminalize minor things, perhaps
even stealing a kiss. Now this has happened.</p>
<p>What next? Will a pat on the arm or a hug be criminalized? There is no clear
limit to how far this can go, when a group builds up enough outrage to push
it.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2023-jul-oct.html#15_October_2023_(Sexual_assault_for_stealing_a_kiss)">stallman.org, 15 October 2023 “Sexual assault for stealing a kiss”</a></p>
<p>In this note, Stallman cites the case of Luis Rubiales, who was under
investigation for kissing a female football player on television as part of a
series of incidents called the “Rubiales affair”, for which Rubiales was
indicted on criminal charges of sexual assault and faces a potential prison
sentence.<sup id="fnref:22"><a href="#fn:22" role="doc-noteref">22</a></sup></p>
<p>The idea of “stealing a kiss” is a familiar refrain for Stallman’s program
speaking against social and legal norms around sexual assault. In 2019, he
writes:</p>
<blockquote>
<p>If it is true that he persistently pressured her to kiss him, on stage and
off, if he stuck his tongue into her mouth despite her objections, that could
well be sexual harassment. He should have accepted no for an answer the first
time she said it. However, calling a kiss “sexual assault” is an exaggeration,
an attempt to equate it to much graver acts, that are crimes.</p>
<p>The term “sexual assault” encourages that injustice, and I believe it has been
popularized specifically with that intention. That is why I reject that term.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2019-jul-oct.html#30_July_2019_(Al_Franken)">stallman.org, 30 July 2019 “Al Franken”</a></p>
<p>“Stealing a kiss” is the least “grave” of the acts Stallman questions the
legitimacy of the label of sexual assault, but Stallman has also questioned its
use for incidents such as the rape of minors.<sup id="fnref:23"><a href="#fn:23" role="doc-noteref">23</a></sup> It is this
“gravity” that Stallman fixes on when questioning sexual assault, which he
wishes to understand for the purpose of how he, and the reader, should “judge”
the offender.</p>
<blockquote>
<p>Due to the vagueness of the term “sexual assault” together with the dishonest
law that labels sex with adolescents as “rape” even if they are willing, we
cannot tell from this article what sort of acts Maraj was found to have
committed. So we can’t begin to judge whether those acts were wrong.<sup id="fnref1:20"><a href="#fn:20" role="doc-noteref">20</a></sup></p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2017-nov-feb.html#13_November_2017_(Jelani_Maraj)">stallman.org, 13 November 2017 “Jelani Maraj”</a></p>
<p>This line of questioning is precisely the wrong response to news of sexual
assault. The vagueness of the term as presented to the public is deliberate; it
protects the privacy of both the victim and the accused. It is the concern of
the legal system to determine the severity of the offense, not the general
public.</p>
<p>The Rape, Abuse &amp; Incest National Network (RAINN) provides resources on
appropriate ways to respond to sexual assault, for instance their “TALK”
framework specifically advises against minimizing the victim’s experiences,
pressing them for details, or challenging their experience (“Are you sure that
counts as assault?” is an example given in their resources).<sup id="fnref:24"><a href="#fn:24" role="doc-noteref">24</a></sup> RAINN
also offers the following advice:<sup id="fnref:25"><a href="#fn:25" role="doc-noteref">25</a></sup></p>
<blockquote>
<p>It can be extremely difficult for survivors to come forward and share their
story. They may feel ashamed, concerned that they won’t be believed, or
worried they’ll be blamed. Leave any “why” questions or investigations to the
experts—your job is to support this person. Be careful not to interpret
calmness as a sign that the event did not occur—everyone responds to traumatic
events differently. The best thing you can do is to believe them.</p>
</blockquote>
<p>We re-iterate the earlier position that Stallman’s rhetoric <a href="#rhetorical-harm">causes material
harm</a> to victims of sexual assault, and we have illustrated
that his political program contributes to an environment where the harm
suffered by sexual assault victims is exacerbated by creating an atmosphere of
confusion and doubt in which sexual violence can thrive.</p>
<p>On the subject of sexual assault, Stallman advances a political agenda which
systematically undermines the importance of consent in sexual and intimate
interactions, objectifying women as subjects of men’s desires, enabling men to
force their sexual desires on women, and dismissing womens’ agency in choosing
how to express intimacy and interact with others.</p>

<h3 id="dismissal-of-legal-norms-regarding-sexual-harassment">
	Dismissal of legal norms regarding sexual harassment
	
	<a href="#dismissal-of-legal-norms-regarding-sexual-harassment"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We have catalogued 13 primary sources in which Stallman misrepresents or
downplays sexual harassment between 2014 and 2018 as part of a broader political
program that aims to erode social and legal norms around sexual harassment.</p>
<p><a href="https://stallman-report.org/on-sexual-harassment">Appendix: Stallman on sexual harassment</a></p>
<p>Stallman’s “anti-glossary” indirectly defines sexual harassment in its
definition of sexual assault:<sup id="fnref2:8"><a href="#fn:8" role="doc-noteref">8</a></sup></p>
<blockquote>
<p>The term [sexual assault] is further stretched to include sexual
harassment, which does not refer to a single act, but rather to a series of
acts that amounts to a form of gender bias. Gender bias is rightly prohibited
in certain situations for the sake of equal opportunity, but that is a
different issue.</p>
</blockquote>
<p>We also note the following quote from November 2017:</p>
<blockquote>
<p>The term “sexual assault” is not suitable for a serious discussion, because it
covers crimes of varying severities which call for different responses, plus
sexual harassment which is not a crime.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2017-nov-feb.html#3_November_2017_(Saboteur_of_Energy)">stallman.org, 3 November 2017 “Saboteur of Energy”</a></p>
<p>Both of these examples are gross misrepresentations of sexual harassment. The US
Equal Employment Opportunity Commission provides a more appropriate explanation:</p>
<blockquote>
<p>It is unlawful to harass a person (an applicant or employee) because of that
person’s sex. Harassment can include “sexual harassment” or unwelcome sexual
advances, requests for sexual favors, and other verbal or physical harassment
of a sexual nature.</p>
<p>Harassment does not have to be of a sexual nature, however, and can include
offensive remarks about a person’s sex. For example, it is illegal to harass a
woman by making offensive comments about women in general.</p>
<p>Both victim and the harasser can be either a woman or a man, and the victim
and harasser can be the same sex.<sup id="fnref:26"><a href="#fn:26" role="doc-noteref">26</a></sup></p>
</blockquote>
<p>Stallman’s remarks on sexual harassment are antifactual. Sexual harassment is a
crime, and it is not reducible to a kind of gender bias. It may consist of
several actions forming a pattern of behavior, but isolated events of sufficient
severity may also constitute sexual harassment.</p>
<p>We also note that the definition given in Stallman’s anti-glossary changed
sometime between February and March 2019. Previously it read as follows:</p>
<blockquote>
<p>[Acts that constitute sexual assault] are not merely different in degree. They
are different in kind. Rape is a grave crime. Being groped is unpleasant but
not as grave as robbery. Sexual harassment is a not an action at all, but
rather a pattern of actions that constitutes economic unfairness. How can it
make sense to group these behaviors things together?</p>
</blockquote>
<p>– <a href="https://web.archive.org/web/20190214062917/http://www.stallman.org:80/antiglossary.html">stallman.org, “Anti-glossary”, archived February 2019 by archive.org</a></p>

<p>Our report finds this change noteworthy on the basis that Stallman completed a
mandatory course on sexual harassment in his role at MIT in September 2018, five
months prior to the edit:</p>
<blockquote>
<p>In September MIT demanded that I take an online course about sexual
harassment; although I don’t teach classes or even meet undergraduates, they
treated me like a professor on the “be overcautious at every opportunity”
principle. But I was unable to do so until MIT arranged to let me log in on
an MIT kiosk terminal and bypassed the two-factor requirement for me.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2019-may-aug.html#7_May_2019_(Mobile_phone_numbers_for_Facebook)">stallman.org, 7 May 2019 “Mobile phone numbers for Facebook”</a></p>
<p>The editors of this report sought to ascertain the nature of the training that
Stallman received. The 2018 Annual Report from MIT’s Committee on Sexual
Misconduct Prevention and Response<sup id="fnref:27"><a href="#fn:27" role="doc-noteref">27</a></sup> states that MIT faculty
and staff members were required to complete the <em>Haven for Faculty and Staff</em>
course provided by <a href="https://everfi.com/">Everfi</a>, a US training provider which
is relied upon by many educational institutions for training its faculty
members. The editors reached out to the MIT committe and to Everfi for comment
regarding the cirriculum and did not receive a response. However, MIT’s annual
reports provide some insight into the training material. The committe’s inagural
report in 2016 describes this program as follows:</p>
<blockquote>
<p>Haven’s introduction is completely customizable, including a welcome video
(e.g., from the President, Provost, or Chancellor), a list of campus resources
and contact information, and any other desired materials. The program
continues with videos on supporting survivors, encouraging bystander
intervention, and recognizing the potential for violence on campus or in the
workplace, and then provides details on Title IX and other legislation. Four
additional videos follow: “A Student Disclosure” about how to respond when a
student or employee initiates a discussion about sexual misconduct; “Always
Around” on policies and responses to stalking; “A Concerned Co‐worker” about
intimate partner violence that affects the workplace; and “Unwanted Attention”
about addressing inappropriate behavior from a supervisor. Questions are posed
before and after each video; incorrect answers trigger a gentle steering
toward the most appropriate response.<sup id="fnref:28"><a href="#fn:28" role="doc-noteref">28</a></sup></p>
</blockquote>
<p>The 2018 report also provides insights into the cirruclum:</p>
<blockquote>
<p>All faculty and staff were required to complete Haven for Faculty and Staff,
an online education program that includes examples and scenarios that faculty
and staff may face around sexual assault, domestic violence, stalking, and
sexual harassment.<sup id="fnref1:27"><a href="#fn:27" role="doc-noteref">27</a></sup></p>
</blockquote>
<p>The committee also summarizes the outcomes of the training program in their 2019
report, which provides a few examples of specific goals associated with the
training program.<sup id="fnref:29"><a href="#fn:29" role="doc-noteref">29</a></sup> In particular, this report draws attention
to the survey results enumerated in Appendix C, which indicate that the
program’s goals were in part to obtain favorable responses to the following
questions of note:</p>
<ul>
<li>I have a good understanding of what constitutes sexual assault, relationship
violence, stalking, and sexual harassment.</li>
<li>I am aware of strategies for preventing sexual assault, relationship violence,
stalking, and sexual harassment.</li>
<li>I am confident in my ability to respond to disclosures of sexual assault,
relationship violence, stalking, and sexual harassment.</li>
<li>A person should never be blamed for being the victim of sexual assault, abuse,
or harassment.</li>
<li>I think sexist jokes and langauge contribute to the issues of sexual assault,
relationship violence, stalking, and sexual harassment.</li>
<li>I plan to play an active role in addressing sexual assault, relationship
violence, stalking, and sexual harassment at my institution.</li>
</ul>
<p>From this we conclude that from September 2018 onwards, Stallman should have
posessed a working understanding of sexual assault and sexual harassment, as
well as the appropriate language and tone for discussing the matter,
particularly with respect to the best interests of victims. Our sources note
that Stallman’s website maintains misleading definitions of sexual assault and
sexual harassment to the present day, and we cite 15 examples of Stallman
misrepresenting sexual assault following his 2018 training.</p>
<p><a href="https://stallman-report.org/on-sexual-assault">Appendix: Stallman on sexual assault</a></p>
<p>Our report notes that the sort of sexual harassment that Stallman consistently
defends has long-term effects on the well-being of victims. Young women who are
subjected to a man in a position of power “stealing a kiss” are objectified and
reduced to a sexual object with no agency over consent in their interactions, an
experience which prevents them from accessing education and employment
opportunities on equal terms with respect to their male peers. Experiences of
sexual harassment have long-term consequences for victims, including increased
rates of symptoms of anxiety and depression for months following the incident,
including in relatively “less grave” cases that Stallman defends such as sexual
jokes or remarks and unwelcome advances. (Johansson et al,
2024)<sup id="fnref:30"><a href="#fn:30" role="doc-noteref">30</a></sup></p>
<p>A 2019 study by Pinchevsky et al also characterizes the harmful effects of
sexual harassment by distinguishing “non-contact” and “contact” harassment,
where the former does not involve physical contact between the perpetrator and
the victim.</p>
<blockquote>
<p>McGinley et al. (2016) found that experiences of non-contact SH [Sexual
Harassment] undermined the health of college students. Non-contact SH is
associated with decreased mental health (i.e., depression, anxiety) and
increased health-risk behavior such as substance use as a coping mechanism,
particularly among White females and sexual minorities (McGinley et al. 2016).
Paludi et al. (2006) identified other research that noted the consequences of
non-contact SH including changes in physical and mental health. Additionally,
victims of non-contact SH are more likely to experience future SH (Petersen
and Hyde 2009).<sup id="fnref:31"><a href="#fn:31" role="doc-noteref">31</a></sup></p>
</blockquote>

<h3 id="support-for-the-possession-of-child-sexual-abuse-material">
	Support for the possession of child sexual abuse material
	
	<a href="#support-for-the-possession-of-child-sexual-abuse-material"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We have catalogued 55 primary sources in which Stallman advocates for the
possession and/or distribution of child sexual abuse material (aka “child
pornography”) between 2003 and 2019, none of which are addressed by <a href="https://stallman-report.org/on-children#2019-retraction">Stallman’s
2019 retraction</a> on sexual relationships between
adults and minors under the age of 12 or 13.</p>
<p><a href="https://stallman-report.org/on-child-pornography">Appendix: Stallman on child sexual abuse material</a></p>
<p>The following quote from Stallman in June 2017 is a typical example:</p>
<blockquote>
<p>In the US, people convicted for having copies of child pornography tend to get
longer prison sentences than those convicted of having sex with children.</p>
<p>Mere possession of child pornography should not be a crime at all. To
prosecute people for possessing something published, no matter what it may be,
is a big threat to human rights.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2017-mar-jun.html#5_June_2017_(Possession_of_child_porn)">stallman.org, 5 June 2017 “Possession of child porn”</a></p>
<p>Stallman’s discourse on child sexual abuse material (CSAM) rely on several
recurring key points:</p>
<ol>
<li>A blanket objection to censorship in any form</li>
<li>Objections to legal norms regarding CSAM on the basis that CSAM depicting
minors over the age of 12-13 do not depict an objectionable act, as adults
having sex with minors in this age range is not a form of abuse (<a href="#minor-adult-sex-rebuttal">rebuttal</a>)</li>
<li>The assertion that minors distributing explicit images with similar-aged
peers should not be prosecuted, often paired with a defense of adults who
distribute images produced in this manner</li>
</ol>
<p>We will first address Stallman’s political positions on CSAM which the editors
do not find unreasonable. First, Stallman often expresses concern that law
enforcement tools and technologies developed to curtail the production and
distribution of CSAM will be applied more generally and infringe on legitimate
freedoms; the editors find this concern reasonable but disagree with Stallman’s
conclusion that CSAM possession and/or distribution should be legalized on this
premise.</p>
<p>Stallman also defends the practice of “sexting”, or exchange of sexually
explicit material, between consenting minors of similar age; the editors find
this argumentation reasonable. However, we object to Stallman’s use of this
argument as the basis for a more general argument in favor of the legalization
of CSAM possession and distribution.</p>
<p>Stallman has also defended “sexting” cases which involve the sexual exploitation
of a minor by an adult. In 2016, Stallman writes the following in response to
the case of a 21 year-old man soliciting a 16 year-old girl for explicit images:</p>
<blockquote>
<p>A Pennsylvania man has been imprisoned for receiving nude photos from his
16-year-old girlfriend, and will have to register as a sex offender, but
“only” for 15 years.</p>
<p>The willfully blind law pretends there is no difference between a teenager and
a child.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2016-jul-oct.html#30_August_2016_(Man_imprisoned_for_receiving_nude_photos)">stallman.org, 30 August 2016 “Man imprisoned for receiving nude photos”</a></p>
<p>We now present a general rebuttal of Stallman’s position on CSAM. Our report
indicates four groups of people who experience material harm as a consequence
of CSAM distribution:</p>
<ul>
<li>Victims of child sexual abuse</li>
<li>Consumers of child sexual assault material</li>
<li>Criminal investigators exposed to CSAM in their work</li>
<li>Online content moderators tasked with CSAM removal</li>
</ul>
<p>The possession and distribution of CSAM exacerbates the harm done to victims of
sexual abuse. Experts on sexual violence assert that the distribution of CSAM
causes children to be victimized twice: first by the perpetrator of their abuse,
and again by the person who view it.<sup id="fnref:32"><a href="#fn:32" role="doc-noteref">32</a></sup> Jennifer Martin explains how
CSAM impacts survivors of abuse:</p>
<blockquote>
<p>This persistent shame means that traumatic stress symptoms may continue
indefinitely in cases of [child sexual abuse material]. Children’s ongoing
efforts to make meaning of their abuse experience may be ineffective, and they
may continually fear what parents, caregivers, and others may think if they
discover, or are shown, images of the abuse (Palmer, 2006). Victims may be
further subjected to shame by the knowledge that images of the abuse are
stored in law enforcement databases and may be accessed and shared
indefinitely among and between legal agencies globally (Muir, 2005). They are
powerless over the distribution or accessibility of the images of abuse, and
must contend with the fact that they may be gazed upon by anyone at any time
because their abuse images are “out there” in the public arena of cyberspace
and can be forever shared and downloaded. The child may internalize the shame
and humiliation of the “global gaze” thereby adding to the child’s traumatic
burden. The never-ending threat of this gaze can influence – have power over -
how the child thinks, feels, and behaves. Ainley(1998) referred to the
“constant torture of the random but ever possible gaze”.<sup id="fnref:33"><a href="#fn:33" role="doc-noteref">33</a></sup></p>
</blockquote>
<p>Consumers of CSAM also experience harm. According to Kothari et al,
consumers of CSAM are at a high risk of suicide and experience symptoms
associated with post-traumatic stress disorder and adjustment disorder.
Offenders experience extreme feelings of stress, shame, and self-hatred, which
impacts their ability to seek help (Kothari et al 2021).<sup id="fnref:34"><a href="#fn:34" role="doc-noteref">34</a></sup></p>
<p>We also draw attention to the experiences of the online content moderators and
criminal investigators who are tasked with combating the distribution of CSAM.
Members of law enforcement who are exposed to CSAM in their work experience
experience an elevated risk of sexual post-traumatic stress symptoms (sexual
PTSS) (Gewirtz-Meydan et al, 2023).<sup id="fnref:35"><a href="#fn:35" role="doc-noteref">35</a></sup> Online content
moderators are susceptible to similar risks, and often experience symptoms
associated with post-traumatic and secondary traumatic stress (Spence et al,
2021).<sup id="fnref:36"><a href="#fn:36" role="doc-noteref">36</a></sup></p>
<p>This report also draws attention to a particularly disturbing source from
Richard Stallman’s website, entitled “Suggestion to the target of a witch hunt”,
dated February 2015.<sup id="fnref:37"><a href="#fn:37" role="doc-noteref">37</a></sup> This article is listed on the front page of
Stallman’s website in a section on political articles outside of the scope of
Stallman’s free software political program. In this article, Stallman reveals
that someone had emailed him asking for advice because they were “drawn to look
at images of sex with children”. Stallman published his response as follows:</p>
<blockquote>
<p>I don’t think it is wrong to distribute “child porn” images, even when they
[depict] children rather than adolescents. However, making them is wrong if it
involves real sex with a child. For the sake of opposing sexual abuse of real
children, I suggest that you boycott the images that involve real children.
Imaginary children can’t be hurt by drawing them.</p>
<p>I can’t suggest any way you could talk publicly about your prediliction
without being the object of a witch hunt. Americans go nuts where they imagine
that children are in danger, and in their frenzy they exaggerate tiny risks —
look at how they jail parents for letting children go to the park or stay home
without an escort.</p>
<p>To be sure, a child faces the danger of sexual abuse mainly while at home. But
not while home alone with no members or friends of the family present.</p>
</blockquote>
<p>Stallman updated the page in 2016 with an additional note:</p>
<blockquote>
<p>2016 note: I support prosecution of those that perpetrate real abuse (sexual
or not) of real children. By “real” I mean specifically that I do not follow
states’ definitions of these terms. In fact, some states stretch the terms to
the point of absurd injustice. There is a tendency to define adolescents as
“children” and define all sex involving adolescents as “sexual abuse”.
Infantilizing adolescents is harmful to society in many ways.</p>
<p>Since this is an ethical question, not a legal one, the question of the right
definitions is for us to consider, not for states to dictate.</p>
</blockquote>
<p>The editors were particularly alarmed by this page. The person who reached out
to Stallman for advice is subject to all of the harm faced by consumers of CSAM
as discussed earlier in our report, and faces the risk of arrest and criminal
prosecution if caught. Rather than providing this person with resources to seek
help, Stallman states that there is nothing wrong with his behavior and uses the
opportunity to publicly re-enforce his political program regarding CSAM and the
sexual abuse of minors.</p>
<p>The editors cite this as an example of direct harm caused by Stallman and as
evidence that Stallman’s remarks are taken seriously, that he is viewed as an
authority on sexual matters by some of those who read his work, and that he has
been consulted for his opinion on these matters by his readers.</p>

<h3 id="legal-and-social-normalization-of-sex-between-humans-and-animals">
	Legal and social normalization of sex between humans and animals
	
	<a href="#legal-and-social-normalization-of-sex-between-humans-and-animals"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We have catalogued 12 primary sources from 2003-2018 in which Stallman advocates
for humans having sex with animals (bestiality) or the possession and
distribution of pornography featuring humans having sex with animals. None of
Stallman’s remarks on bestiality have been retracted.</p>
<p><a href="https://stallman-report.org/on-bestiality">Appendix: Stallman on bestiality</a></p>
<p>Stallman remarked most recently on the subject of pornography featuring humans
and animals in 2018:</p>
<blockquote>
<p>Prudish censorship attacks again in the UK, convicting someone for possessing
“extreme pornography”, including images of sex with animals.</p>
<p>I can’t imagine a possible reason to punish people for this. The article does
not report that the animals were harmed, or that they objected to the
experience, or that they thought of it as sexual. The law does not consider
these questions pertinent.</p>
<p>What is, however, clear is that prohibiting the possession of copies of some
image or text — no matter what that image or text may be — threatens human
rights. It creates excuses to search through people’s possessions and files.
It creates ways to make people vulnerable to criminal charges without their
cooperation or even their knowledge. All such laws must be repealed.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2018-sep-dec.html#14_December_2018_('Extreme_pornography'_conviction)">stallman.org, 14 December 2018 “‘Extreme pornography’ conviction”</a></p>
<p>The editors note that Stallman’s remarks here are consistent with <a href="#support-for-the-possession-of-child-sexual-abuse-material">his rhetoric
on child sexual abuse material</a>.
In addition to arguing for the legal normalization of this kind of pornographic
images, Stallman has explicitly supported the act depicted therein, for instance
in 2017:</p>
<blockquote>
<p>European countries are passing laws against having sex with an animal. (We are
talking about sex practices that don’t physically hurt the animal.)</p>
<p>These laws have no rational basis. We know that some animals enjoy sex with
humans. Others don’t. But really, if you smear something on your genitals that
tastes good to dogs, and have a dog lick you off, it harms no one. Why should
this be illegal except mindless religion?</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2017-sep-dec.html#10_October_2017_(Laws_against_having_sex_with_an_animal)">stallman.org, 10 October 2017 “Laws against having sex with an animal”</a></p>
<p>Stallman explained his views in more detail in 2016:</p>
<blockquote>
<p>A national campaign seeks to make all US states prohibit sex between humans
and nonhuman animals.</p>
<p>This campaign seems to be sheer bull-headed prudery, using the perverse
assumption that sex between a human and an animal hurts the animal. That’s
true for some ways of having sex, and false for others.</p>
<p>For instance, I’ve heard that some women get dogs to lick them off. That
doesn’t hurt the dog at all. Why should it be prohibited?</p>
<p>When male dolphins have sex with people, that doesn’t hurt the dolphins. Quite
the contrary, they like it very much. Why should it be prohibited?</p>
<p>I’ve also read that female gorillas sometimes express desire for sex with men.
If they both like it, who is harmed? Why should this be prohibited?</p>
<p>The proponents of this law claim that any kind of sex between humans and other
species implies that the human is a “predator” that we need to lock up. That’s
clearly false, for the cases listed above. Making a prohibition based on
prejudice, writing it in an overbroad way, is what prissy governments tend to
do where sex is concerned. The next step is to interpret it too strongly with
“zero tolerance”.</p>
<p>Will people convicted of having dogs lick them off be required to live at
least 1000 feet from any dogs?</p>
<p>This law should be changed to prohibit only acts in which the animal is
physically forced to have sex, or physically injured.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2016-sep-dec.html#14_December_2016_(Campaign_of_bull-headed_prudery)">stallman.org, 14 December 2016 “Campaign of bull-headed prudery”</a></p>
<p>It is straightforwardly understood that animals cannot express consent; they are
not capable of meaningfully communicating “yes” or “no” and they cannot explain
their subjective experiences or advocate for themselves following sexual abuse.
It is also understood that animal victims of sexual abuse experience symptoms
similar to human victims; animal victims of sexual abuse commonly display signs
of depression, anxiety, and aggression (Kunz 2019).<sup id="fnref:38"><a href="#fn:38" role="doc-noteref">38</a></sup></p>
<p>The actors involved in pornographic films depicting bestiality are often coerced
and humiliated by the act. Linda Lovelace is a famous pornographic actress who
became widely known for her appearance in the 1972 film “Deep Throat”, later
stating that she was coerced and raped on screen in this film. In 1969, she
appeared in a film where she was coerced into performing sexual acts with a dog.
She was forced to perform these acts at gunpoint and later explained the lasting
effect of this experience:</p>
<blockquote>
<p>I am able to handle almost everything that has happened to me in my life… but
I’m still not able to handle that day. A dog. An animal. I’ve been raped by
men who were no better than animals, but this was an actual animal and that
represented a huge dividing line. (…)</p>
<p>There were no greater humiliations left for me. The memory of that day and
that dog does not fade the way other memories do. The overwhelming sadness
that I felt on that day is with me at this moment, stronger than ever. It was
a bad day, such a bad day.<sup id="fnref:39"><a href="#fn:39" role="doc-noteref">39</a></sup></p>
</blockquote>
<p>Stallman’s remarks on bestiality are consistent with his broader dismissal of
the importance of consent with respect to sexual interactions, be they animals,
minors, subordinates, women whose consent is contingent on the use of
contraception, or women who have previously consented to sex; in all of these
cases Stallman absolves the perpetrator of wrongdoing and argues that images of
these acts of sexual violence should not be subject to censorship.</p>

<h3 id="legal-and-social-normalization-of-necrophilia">
	Legal and social normalization of necrophilia
	
	<a href="#legal-and-social-normalization-of-necrophilia"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We have catalogued 3 primary sources from 2003-2013 in which Stallman advocates
for humans having sex with corpses (necrophilia) or the possession and
distribution of pornography featuring humans having sex with corpses. We have
also identified several more contemporary sources where Stallman remarks on the
abuse of corpses generally in a non-sexual context, using the same argumentation
used to advocate for necrophilia, as recently as 2023. None of Stallman’s
remarks on the abuse of corpses, for sexual purposes or otherwise, have been
retracted.</p>
<p><a href="https://stallman-report.org/on-necrophilia">Appendix: Stallman on necrophilia</a></p>
<p>For example, Stallman writes the following in April 2008 in a statement calling
for pornography featuring living individuals having sex with corpses to be
legalized:</p>
<blockquote>
<p>It is true that victims of real violence suffer. (Never mind that in making
movies of violence, typically nobody is actually hurt.) The true oppressive
spirit of this law starts to show in the prohibition of images of sex with
corpses. Are we supposed to believe that corpses can suffer? Or are some cruel
prudes trying to impose their prejudices by force?</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2008-mar-jun.html#30%20April%202008%20(Possession%20of%20extreme%20pornography)">30 April 2008 (Possession of “extreme pornography”)</a></p>
<p>The editors acknowledge that Stallman has not made any explicit statements in
support of necrophilia since 2010. However, Stallman has often used the same
line of argumentation in statements on the abuse of corpses more generally than
for the purpose of sexual gratification since 2010. For example, in December
2023:</p>
<blockquote>
<p>Brittany Watts, of Ohio, had a miscarriage at home and disposed of the
nonviable fetus as people often do. Now she faces possible charges of “abuse
of a corpse”.</p>
<p>The very idea of sentencing someone to prison for “abuse of a corpse” is
absurd, since whatever is done to a corpse can’t injure any person.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2023-sep-dec.html#28_December_2023_(Brittany_Watts)">stallman.org, 28 December 2023 “Brittany Watts”</a></p>
<p>Legal and social norms regarding the treatment of corpses acknowledge the agency
of the deceased individual and their right to self-determination and bodily
autonomy following their death. Moreover, the editors point out that a corpse is
unable to express consent.</p>
<p>The treatment of a corpse is also a matter of respect for friends and family of
the deceased, who should be allowed to grieve in peace without the knowledge
that their loved one’s corpse is being exploited for sexual gratification. The
editors of this report cannot imagine a more traumatic grieving process than one
which contends with the knowledge that images of the desecration of your loved
one’s corpse are being distributed for the sexual gratification of others.</p>
<p>It is noted that the rape of corpses is a war crime which has been reported in
several conflicts and has been employed for the purpose of subjecting the
population to terror, humiliation, and trauma.<sup id="fnref:40"><a href="#fn:40" role="doc-noteref">40</a></sup></p>

<h2 id="credible-allegations-of-sexual-misconduct">
	Credible allegations of sexual misconduct
	
	<a href="#credible-allegations-of-sexual-misconduct"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>There are numerous allegations of misconduct regarding Richard Stallman. Most of
these are hearsay recounts of individual experiences with Stallman. This report
only includes allegations which have been corroborated or are otherwise
considered verifiable.</p>
<h3 id="testimony-of-betsy-s">
	Testimony of Betsy S.
	
	<a href="#testimony-of-betsy-s"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>The testimony of “Betsy S.” recalls the following interaction with Richard
Stallman, which would have taken place in the early 1980’s.</p>
<blockquote>
<p>When I was a teen freshman, I went to a buffet lunch at an Indian restaurant
in Central Square with a graduate student friend and others from the AI lab. I
don’t know if he and I were the last two left, but at a table with only the
two of us, Richard Stallman told me of his misery and that he’d kill himself
if I didn’t go out with him.</p>
<p>I felt bad for him and also uncomfortable and manipulated. I did not like
being put in that position — suddenly responsible for an “important” man. What
had I done to get into this situation? I decided I could not be responsible
for his living or dying, and would have to accept him killing himself. I
declined further contact.</p>
<p>He was not a man of his word or he’d be long dead.</p>
</blockquote>
<p>We consider the report verifiable on the basis that Stallman has corroborated
Betsy’s recollection of events in a July 2020 statement on the subject:</p>
<blockquote>
<p>A note to Betsy S.</p>
<p>Betsy S met me at a lunch around 40 years ago. I am sure her recounting of her
recollections is sincere, but she must have misunderstood the last thing I
said to her. She said she didn’t want an acquaintance with me. That no, on top
of so many noes from others, impelled me to express despair; she seems to have
misconstrued that as a demand.</p>
<p>Betsy S, I regret that this misunderstanding caused you distress. I never
intended to demand anything of you. I only ever wished you well.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2020-jul-oct.html#19_July_2020_(A_note_to_Betsy_S.)">stallman.org, 19 July 2020 “A note to Betsy S.”</a></p>
<p>Betsy’s testimony describes an experience of sexual coercion, wherein Stallman
threatens violence (to himself) if Betsy does not date him.</p>
<p>At the time of this incident, Betsy would have been a freshman at MIT, no older
than 19, and Stallman would have been approximately 27 years old, a graduate
student having been established at the AI lab for about nine years at this time.
Stallman exploited this power differential in an attempt to take advantage of
this young woman, coercing her into dating him.</p>
<p>Stallman’s 2020 response is lacking in several respects. The editors point out
that at the time this response was written, Stallman should have been equipped
with the requisite training to understand the gravity of this incident given his
September 2019 course on sexual harassment and sexual violence at MIT, which is
discussed in detail <a href="#ref-sexual-harassment-training">earlier in this report</a>.</p>
<p>We also draw attention to the phrasing of Stallman’s apology. Stallman blames
Betsy for misunderstanding his intent when he threatened suicide if Betsy did
not agree to date him. Stallman also excuses his behavior by shifting
responsibility to Betsy and to women collectively, citing both that Betsy did
not want an acquaintance with Stallman and that his actions were motivated by a
series of romantic rejections. Stallman does not demonstrate an understanding of
why his behavior was wrong, and does not take responsibility for his behavior;
instead he “apologises” for Betsy’s behavior (i.e. misunderstanding him).</p>
<h3 id="emacs-virgin-incidents">
	“Emacs virgin” incidents
	
	<a href="#emacs-virgin-incidents"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>Richard Stallman has often performed a satirical routine as “St. IGNUcius” of
the “Church of EMACS” at numerous events. The routine includes a ceremony
regarding the “EMACS virgin” (a person who has not used EMACS before) with
sexualized overtones. Prior to a 2009, Stallman emphasized in his routine that
the virgin must be female, after 2009 Stallman referred to the EMACS virgin as a
“person” who has not used EMACS.</p>
<p>We have several uncorroborated testimonies of women, including minors, being
overtly sexualized during this routine, some without consent. In the course of
our research we discovered that one of these routines was recorded, in which
Stallman brings a 13 year-old girl on stage and makes sexually suggestive
remarks about her in front of a crowd at FKFT 2008 in Barcelona.</p>

<p>We highlight the following quote from the transcript of this event:</p>
<blockquote>
<p>I saw her experiment once. She actually typed Ctrl+V to scroll the screen. But
I think that– at that point that’s like having kissed, so she’s still a
virgin for now. [Stallman approaches the girl and places a hand on her
shoulder.] But I hope to do something about that. And, by the way, that
reminds me that one of the other advantages of the Church of EMACS is that
being a saint in this church does not require celibacy.</p>
</blockquote>
<p>Following a particularly controversial performance of this routine at the 2009
Gran Canaria Desktop Summit, Stallman made the following statement:</p>
<blockquote>
<p>Some of the people in the audience in my speech in the Gran Canaria Desktop
Summit thought that my joke about the Virgin of Emacs was intended to make
some kind of statement about women.</p>
<p>I was surprised by that reaction, since I had told the same joke dozens of
times and this is the first report of interpreting it that way.  In any case,
it was a misunderstanding: the only intended meaning of the Cult of the Virgin
of Emacs is to parody another Cult of the Virgin. The whole St IGNUius routine
makes fun of me, the free software movement and religion, through parody.</p>
<p>To be abundantly clear, my views about women in connection with free software
are simply that they deserve freedom in using computers, just as men do.  Some
women already appreciate this freedom and have become free software activists.
We need more people, regardless of sex, to do this, so that someday all women,
and all men, will enjoy the freedom that free software offers.</p>
<p>Misunderstanding is not a good outcome.  To help avoid misunderstandings of
this kind in the future, since August I have changed the joke so that the
Virgin of Emacs can be of either sex.<sup id="fnref:41"><a href="#fn:41" role="doc-noteref">41</a></sup></p>
</blockquote>
<p>Stallman has also <a href="https://stallman.org/articles/virgin-of-emacs.html">issued a statement on stallman.org</a>
about the routine.</p>
<h3 id="pleasure-cards">
	Pleasure cards
	
	<a href="#pleasure-cards"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>In lieu of a traditional business card, it has been reported that Richard
Stallman employs a “pleasure card”, which solicits “tender embraces” from the
recipient.</p>
<p><img src="https://stallman-report.org/old-card.png" alt="A picture of Richard Stallman’s original “pleasure card” with sensitive details retracted. The headline reads “Sharing good books, good food and exotic music and dance; tender embraces; unusual sense of humor”"></p>
<p>It has been suggested that Stallman gives these cards to people regardless of
gender, but that when Stallman hands this card to women he often does so to
supplement a romantic or sexual proposition. The editors reached out to a woman
who received a “pleasure card” from Stallman for an interview, who we will refer
to as Ms. W. The editors have independently corroborated Ms. W’s testimony.</p>
<blockquote>
<p>I was at one of my first events as a speaker, at the speaker’s dinner.
Stallman was there and he approached me to chat. He spent a few minutes
talking about himself, showing no interest in me or what I was working on. He
didn’t ask about my work, or my situation, he just wanted to… pick me up.
I got the impression that he just assumed that he was entitled to my attention
based on his fame and reputation.</p>
<p>He hit on me for a few minutes and handed me his “pleasure card”, then told me
he would be around later that evening – I just thought, “yeah, and I’ll be
hanging out with my spouse and kid”. He moved to touch my arm, and I backed
off and avoided him for the rest of the event.</p>
<p>It didn’t even feel like he was particularly attracted to me – it felt like I
was just a woman under 40 and that was enough.</p>
</blockquote>
<p>The editors note that the event in question had an anti-harassment policy. Ms. W
elaborated on her thoughts after the fact:</p>
<blockquote>
<p>I mean, I’m middle aged, I brought my kid – this wasn’t my first rodeo. And
it just felt so inappropriate in a professional context. If I had met Stallman
in another context and we weren’t “coworkers”, in a way, it would have been…
unwanted, but not inappropriate. There’s a difference between, like, some of
us clicked and went out for drinks and it was flirty, and this taking place at
the speaker’s dinner. And what made it so inappropriate was the power
differential, of handing this to a relatively unknown woman when you’re
Richard Stallman and you’re giving the keynote.</p>
<p>I’ve been hit on at events before – but it often felt like a peer-to-peer
sort of thing, whereas the proposition from Stallman was more… “do you want
to be an acolyte?” There was some kind of power dynamic at play. He was 20, 25
years older than me – it was like your dad was hitting on you. It didn’t feel
like he was my peer, and we hadn’t talked enough to register if I was actually
interested in him at all.</p>
</blockquote>
<p>Ms. W notes that she attended a second event with Richard Stallman a year later,
where she indicates that Stallman participated in writing the code of conduct,
then violated that code of conduct when performing his <a href="#emacs-virgin-incidents">“St. Ignutius”
routine</a>, as well as at other occasions. Ms. W reported
her concerns to the event organizers, her testimony corroborated by Matthew
Garrett (member of the FSF board of directors at the time), and was met with
disbelief. Ms. W explains that the organizers were aware of Stallman’s
reputation at the time, and they stated “He’s one of those neckbeardy guys, but
we think he’ll behave himself. We talked about it.”</p>
<p>The editors have found a photograph which is alleged to be Stallman’s new
business card circa 2023, with the reference to “tender embraces” removed:</p>
<p><img src="https://stallman-report.org/new-card.png" alt="A picture of Richard Stallman’s current business card with sensitive details retracted. The headline now reads “sharing good books, good food, and far-away music &amp; dance; thoughtful and emotional conversation; unusual sense of humor”"></p>
<h3 id="testimonies-of-former-fsf-staff">
	Testimonies of former FSF staff
	
	<a href="#testimonies-of-former-fsf-staff"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>Georgia Young was the Program Manager for the FSF between 2015 and 2018 and
testified to Stallman’s conduct and character on Twitter in March 2021.</p>
<blockquote>
<p>I worked at the FSF from 2015-2018 &amp; was shop steward for a while. I recall
having a months-long conversation with [Executive Director] John Sullivan
about why racist &amp; sexist ‘hacker humor’ from the 90s needed to be removed
from gnu.org. RMS didn’t get why it was harmful.</p>
<p>The abortion joke<sup id="fnref:42"><a href="#fn:42" role="doc-noteref">42</a></sup> (‘contributed’ by RMS) in a technical
manual? He threw a fit when it was removed. (…)</p>
<p>The thing that (people) who have never had to actually work with RMS don’t
understand is that MANY people who deeply respected him tried to help him
learn to not objectify women, shout over others at Libreplanet as if it was
his birthday party, (and) stop shit like ’emacs virgins’.</p>
</blockquote>
<p>– <a href="https://x.com/georgialyle/status/1374504389155508232">@georgialyle on Twitter, 24 March 2021</a></p>
<p>Paul Fisher worked for 3 years on the staff of the Free Software Foundation and
worked as a volunteer for 6 years, ceasing his involvement in 2004. Paul
testified to his experiences on Twitter in March 2021:</p>
<blockquote>
<p>I worked at the FSF for 3 years and volunteered for over 6 years — that ended
in 2004. I witnessed misogyny, sexual objectification, and abuse carried out
by RMS. I banded together with my coworkers, formed a union, negotiated a
contract, and was elected shop steward.</p>
<p>While RMS started the free software movement and the GNU GPL was a
groundbreaking document, the community still has a right to hold him to
account for his abhorrent actions and harmful speech. RMS should not be part
of the FSF.</p>
</blockquote>
<p>– <a href="https://x.com/paulnivin/status/1374499598853545986">@paulnivin on Twitter, 24 March 2021</a></p>
<p>Paul also explained a few days later that the formation of the FSF staff union
was motivated by Stallman’s poor conduct.</p>
<blockquote>
<p>RMS created non-safe spaces at both MIT &amp; the FSF. When I was at the FSF, RMS
had little to no empathy for the staff. The FSF was not a healthy, functional
workplace. We formed a union to help protect ourselves from RMS — he
controlled our pay, benefits, and workplace conditions.</p>
<p>Everything was controlled by RMS — not the executive director, and not the
board. The union helped turn FSF employment into what most people think of as
a “normal” office job. It didn’t fix everything. Some of the issues that we
did fix:</p>
<p>RMS did not believe in providing raises — prior cost of living adjustments
were a battle and not annual. RMS believed that if a precedent was created for
increasing wages, the logical conclusion would be that employees would be paid
infinity dollars and the FSF would go bankrupt.</p>
<p>RMS did not believe in providing bereavement leave. What if all your close
friends and family die one after another? It’s conceivable you would be gone
from the office for days, or weeks, if not months. What if you lie about who
is dying?</p>
<p>RMS would often throw tantrums and threaten to fire employees for perceived
infractions. FSF staff had to show up to work each day, not knowing if RMS had
eliminated their position the night before.</p>
<p>Respectively, the union provided a formula for allocating a portion of any
budget surplus to COLAs and wage increases, bereavement leave, and progressive
discipline for workers, ensuring that union employees could not be fired at
RMS’ whim.</p>
<p>RMS has not apologized for the harm he’s caused. Both MIT &amp; the FSF
successfully separated themselves from RMS in 2019. Why did the secret group
of voting FSF members reelect him to the board? Why.</p>
</blockquote>
<p>– <a href="https://x.com/paulnivin/status/1377079987950395393">@paulnivin on Twitter, 31 March 2021</a></p>
<p>The allegation that the FSF staff union was formed due to Stallman’s conduct is
corroborated by David Turner, founder of the FSF’s GPL Compliance Labs:</p>
<blockquote>
<p>Funny confluence of RMS and tech union tweets today. We unionized FSF, in
large part, because RMS.</p>
</blockquote>
<p>– <a href="https://x.com/NovalisDMT/status/1172573166956437505">@NovalisDMT on Twitter, 13 September 2019</a></p>
<p>Matthew Garret, member of the FSF board of directors between 2014 and 2017 and
winner of the FSF Award for the Advancement of Free Software wrote the
following:</p>
<blockquote>
<p>I know of at least one other case where Stallman has decided to protect an
abuser. (…)</p>
<p>Free software is an amazing thing, and [Richard Stallman] is a liability
towards it.</p>
<p>His refusal to take action and insistence on making excuses for an abuser is
why I quit the FSF board. The FSF’s former general counsel threatened a board
member at an FSF event. [Stallman] threatened to overrule staff if they
attempted to enforce the event code of conduct and refused to tell the
abuser’s employer.</p>
</blockquote>
<p>– <a href="https://x.com/mjg59/status/1172286576082214912">@mjg59 on Twitter, 13 September 2019</a></p>

<h2 id="misconduct-of-the-fsf-board-of-directors">
	Misconduct of the FSF board of directors
	
	<a href="#misconduct-of-the-fsf-board-of-directors"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>This report alleges misconduct on the part of the 2019 FSF board of directors,
and that the present-day board of directors is responsible both for enabling
Richard Stallman and for failing to provide a workplace free of sexual
harassment under federal and Massachusetts law.</p>
<p>The composition of the board in 2019, when the question of Stallman’s continued
role in the FSF was under discussion, was as follows:<sup id="fnref:43"><a href="#fn:43" role="doc-noteref">43</a></sup></p>
<ul>
<li>Alexandre Oliva</li>
<li>Benjamin Mako Hill</li>
<li>Bradley Kuhn</li>
<li>Geoffrey Knauth</li>
<li>Gerald Jay Sussman</li>
<li>Henry Poole</li>
<li>Kat Walsh</li>
<li>Richard Stallman</li>
</ul>
<p>This report has reason to believe that Bradley Kuhn, Kat Walsh, and Benjamin
Mako Hill were not party to the misconduct of the 2019 board of directors. Mr.
Kuhn’s public statements following his ejection from the board of directors have
been a valuable source in the preparation of this report. Ms. Walsh voted
against Stallman’s return to office and resigned from her position on the Board
of Directors a few days following Stallman’s return.<sup id="fnref:44"><a href="#fn:44" role="doc-noteref">44</a></sup> Mr. Hill also
publicly spoke against Stallman’s re-instatement and quit his positions at the
FSF.<sup id="fnref:45"><a href="#fn:45" role="doc-noteref">45</a></sup></p>
<p>Present-day members of the FSF Board of Directors are as
follows:<sup id="fnref:46"><a href="#fn:46" role="doc-noteref">46</a></sup></p>
<ul>
<li>Christina Haralanova</li>
<li>Geoffrey Knauth</li>
<li>Gerald Jay Sussman</li>
<li>Henry Poole</li>
<li>Ian Kelling</li>
<li>John Gilmore</li>
<li>Maria Chiara Pievatolo</li>
<li>Richard M. Stallman</li>
</ul>
<h3 id="lack-of-transparency-in-governance">
	Lack of transparency in governance
	
	<a href="#lack-of-transparency-in-governance"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>The Free Software Foundation, like most non-profits, maintains a Board of
Directors which is responsible for directing its activities. The members of the
Board of Directors are disclosed in the FSF’s annual filings<sup id="fnref:47"><a href="#fn:47" role="doc-noteref">47</a></sup> and are
clearly enumerated on the FSF’s website.<sup id="fnref1:46"><a href="#fn:46" role="doc-noteref">46</a></sup></p>
<p>The governance of the FSF is also subject to a group of “Voting
Members”.<sup id="fnref:48"><a href="#fn:48" role="doc-noteref">48</a></sup> At the time of writing, this group is composed of the
following members:</p>
<ul>
<li>Alexandre Oliva</li>
<li>Christina Haralanoa</li>
<li>Geoffrey Knauth</li>
<li>Gerald Jay Sussman</li>
<li>Henry Poole</li>
<li>Ian Kelling</li>
<li>John Gilmore</li>
<li>Maria Chiara Pievatolo</li>
<li>Odile Bénassy</li>
<li>Richard M. Stallman</li>
</ul>
<p>Information about the Voting Members on the FSF’s website and has only been
available since 2021, following Richard Stallman’s return to the Board of
Directors. According to the FSF, the primary function of the Voting Members is
electing the Board of Directors.</p>
<p>According to public statements from the FSF, Richard Stallman resigned from all
positions of governance on September 17th, 2019.<sup id="fnref:49"><a href="#fn:49" role="doc-noteref">49</a></sup> From this point
until Stallman’s return to the Board of Directors on April 12th, 2021, all
public statements from the FSF supported the conclusion that Stallman was no
longer involved in the governance of the Free Software Foundation.</p>
<p>However, the testimony of Bradley Kuhn alleges that Stallman never resigned as a
Voting Member, and remained a Voting Member throughout the period of his
resignation.<sup id="fnref:50"><a href="#fn:50" role="doc-noteref">50</a></sup></p>
<p>This report alleges that the FSF has maintained, particularly between the events
of 2019 and 2021, a “shadow government” which is subject to a lack of
transparency in their role and operations, and alleges misconduct in misleading
the public on the nature of Stallman’s role in the FSF between 2019 and 2021.</p>
<h3 id="knowledge-of-stallmans-misconduct">
	Knowledge of Stallman’s misconduct
	
	<a href="#knowledge-of-stallmans-misconduct"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>Bradley Kuhn was the Executive Director of the Free Software Foundation between
2001 and 2005, and served on the board of directors from March 2010 to October
2019.<sup id="fnref:51"><a href="#fn:51" role="doc-noteref">51</a></sup> Following <a href="#ejection-of-bradley-kuhn">Mr. Kuhn’s expulsion</a>,
he issued a public statement on the 2019 controversy<sup id="fnref1:50"><a href="#fn:50" role="doc-noteref">50</a></sup> where he
asserts that Stallman’s behavior was well-known to the FSF for at least two
years prior to the public outcry; other sources suggest it was known for longer:</p>
<blockquote>
<p>For the last two years, I had been a loud internal voice in the FSF leadership
regarding RMS’ Free-Software-unrelated public statements; I felt strongly that
it was in the best interest of the FSF to actively seek to limit such
statements, and that it was my duty to FSF to speak out about this within the
organization. (…)</p>
<p>I attempted to argue with him at length to convince him that some of his
positions were harmful to sexual assault survivors and those who are
sex-trafficked, and to the people who devote their lives in service to such
individuals. More importantly to the FSF, I attempted to persuade RMS that
launching a controversial campaign on sexual behavior and morality was counter
to his and FSF’s mission to advance software freedom, and told RMS that my
duty as an FSF Director was to assure the best outcome for the FSF, which
<abbr title="in my opinion">IMO</abbr> didn’t include having a leader who made
such statements.</p>
</blockquote>
<h3 id="ejection-of-bradley-kuhn">
	Ejection of Bradley Kuhn
	
	<a href="#ejection-of-bradley-kuhn"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>In October 2019, Bradley Kuhn was removed from the Free Software Foundation
Board of Directors and Voting Members. Mr. Kuhn’s public remarks on the matter
provide insight into the misconduct of the board during the scandal of
2019.<sup id="fnref2:50"><a href="#fn:50" role="doc-noteref">50</a></sup> Quoting Mr. Kuhn:</p>
<blockquote>
<p>I was narrowly (by exactly one vote) voted out (of all my FSF roles) by FSF’s
Voting Members.</p>
<p>I was voted out for various reasons. The most relevant reason was a
fundamental disagreement about the criteria and requirements for RMS’ return
to the FSF Board of Directors. In particular, during September-October 2019, I
was insisting that one qualification for reinstatement was a complete,
unqualified apology for RMS’ September 2019 statements that (a) “she [Virginia
Giuffre] presented herself to him [Marvin Minksy][sic] as entirely willing”,
and (b) Giuffre (who was sex-trafficed by Jeffrey Epstein) committed “an
injustice” by accusing Minksy[sic] of sexual assault in her deposition.</p>
</blockquote>
<p>The FSF’s “Voting Members” are responsible for electing the FSF Board of
Directors. Mr. Kuhn reports that Richard Stallman was a Voting Member at this
time.</p>
<p>We conclude from this testimony that Mr. Kuhn was ejected from the Free Software
Foundation governance for the apparent purpose of facilitating Richard
Stallman’s eventual return to the Board of Directors, in particular that his
return not be contingent on apologizing for his behavior, disenfranchising Mr.
Kuhn of his legitimate vote on the matter of the membership of the Board of
Directors, and demonstrates that the FSF was preparing for Stallman’s
re-instatement even as they were facilitating his resignation.</p>
<h3 id="failure-to-account-for-sexual-harassment">
	Failure to account for sexual harassment
	
	<a href="#failure-to-account-for-sexual-harassment"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We assert that the Free Software Foundation’s consistent protection for Richard
Stallman despite prior knowledge of allegations of his misconduct signals
incompetence with respect to their legal obligation to maintain a workplace free
of sexual harassment. Allegations of misconduct while Stallman was conducting
official FSF business are enumerated above, but did not appear to instigate an
investigation by FSF leadership. We contend that an appropriate response to the
allegations would have been to perform an investigation similar to one
undertaken by the editors of this report, which would have presented a clear
case for Stallman’s removal.</p>
<p>We also argue that the FSF has created a “hostile work environment” under US
and Massachusetts law. In one respect, we hold that retaining in leadership an
individual who does not understand sexual harassment or sexual assault and
continuously makes public statements to this effect constitutes a hostile work
environment. We also argue that the 2019 expulsion of Bradley Kuhn constitutes
illegal retaliation under Title VII of the United States Civil Rights Act of
1964.</p>
<p>Moreover, regardless of the conclusion of proceedings following the 2019 scandal
involving Richard Stallman, we feel that the FSF leadership failed to implement
prudent steps to address sexual harassment in its workplace at a moment when it
would have been obvious to do so.</p>
<p>It is unknown to the authors of this report if the FSF is in full compliance
with Massachusetts law regarding sexual harassment, in particular if they have
prepared a policy regarding sexual harassment, have established processes for
reporting sexual harassment, or annually provide materials to this effect to all
employees. However, we note that Massachusetts provides optional recommendations
that the FSF does not appear to have implemented in the aftermath of the 2019
scandal:</p>
<blockquote>
<p>Employers and labor organizations are encouraged to conduct an education and
training program for new employees and members, within one year of
commencement of employment or membership, which includes at a minimum the
information set forth in this section. Employers are encouraged to conduct
additional training for new supervisory and managerial employees and members
within one year of commencement of employment or membership, which shall
include at a minimum the information set forth in subsection (b), the specific
responsibilities of supervisory and managerial employees and the methods that
such employees should take to ensure immediate and appropriate corrective
action in addressing sexual harassment complaints. Employers, labor
organizations and appropriate state agencies are encouraged to cooperate in
making such training available.<sup id="fnref:52"><a href="#fn:52" role="doc-noteref">52</a></sup></p>
</blockquote>
<p>We note that Richard Stallman recieved mandatory sexual harassment training at
MIT in September 2018, which we discuss <a href="#ref-sexual-harassment-training">earlier in the
report</a>. Everfi, the company responsible for
the training program Stallman recieved, provides course materials which are
compliant with California AB 1825 requirements on mandatory
training.<sup id="fnref:53"><a href="#fn:53" role="doc-noteref">53</a></sup> These requirements include that training
materials cover, among other things, identifying retalitory behavior under
federal law and an employer’s obligation to complete an investigation upon
receiving a report of sexual harassment. It is reasonable to assume that
Stallman is familiar with these legal norms.</p>
<p>Gerald Sussman, also on the FSF Board of Directors during the 2019 scandal, was
also a member of the MIT faculty during its 2018 mandatory training program and
presumably received similar training.</p>
<h3 id="2020-form-990-irs-filing">
	2020 Form 990 IRS filing
	
	<a href="#2020-form-990-irs-filing"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>This report also draws attention to the Free Software Foundation’s 2020 Form 990
tax filing.<sup id="fnref:54"><a href="#fn:54" role="doc-noteref">54</a></sup></p>
<p>Richard Stallman has been reported on the FSF’s Form 990 filings as an officer
of the organization in every year except for 2020, following Stallman’s
ostensible removal from the governance of the FSF. However, if Bradley Kuhn’s
allegations that Stallman’s was as a Voting Member in this year are true, the
absence of Stallman in this filing may be fraudulent.</p>
<p>The IRS instructions for Form 990 in 2020 provide the following instructions for
supplying the list of Directors and other notable members of the organization:</p>
<blockquote>
<p>A “director or trustee” is a member of the organization’s governing body, but
only if the member has voting rights.<sup id="fnref:55"><a href="#fn:55" role="doc-noteref">55</a></sup></p>
</blockquote>
<p>The report also notes that several present-day FSF Voting Members are not
included on the Free Software Foundation’s Form 990 filing in 2022.</p>
<h3 id="regarding-the-fsf-codes-of-ethics">
	Regarding the FSF codes of ethics
	
	<a href="#regarding-the-fsf-codes-of-ethics"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>The report notes that the FSF has published two codes of ethics, respectively
applying to the Board of Directors<sup id="fnref:56"><a href="#fn:56" role="doc-noteref">56</a></sup> and its Voting
Members,<sup id="fnref:57"><a href="#fn:57" role="doc-noteref">57</a></sup> and respectively appearing in December 2021 and
July 2022. As such, neither were in force at the time the above reported
misconduct took place.</p>
<p>However, this report takes this opportunity to offer a retrospective analysis of
FSF board members’ and voting members’ 2019 conduct with respect to these codes
of ethics, as well as a contemporary analysis of their conduct.</p>
<p>On the subject of erroneous IRS filings, we find that treasurer Geoffrey Knauth
actions have contravened the following provision:</p>
<blockquote>
<p>Members of the board of directors will conduct the business affairs of the
organization in good faith and with honesty, integrity, due diligence, and
reasonable competence.</p>
</blockquote>
<p>If the failure to report the Voting Members on the FSF’s Form 990, previously
and to the present day, is part of a larger program of deliberately obscuring
the governance of the Free Software Foundation, we assert this contravenes the
principles of good faith, honesty, and integrity; if the filings are simply a
mistake and there is no broader objective to obscure the governance of the FSF,
we assert that this contravenes the principles of due diligence and reasonable
competence. To assist the reader in choosing a suitable interpretation, we
acknowledge that Bradley Kuhn’s statement reports that demands for transparency
were among the likely reasons he was expelled from his role.<sup id="fnref3:50"><a href="#fn:50" role="doc-noteref">50</a></sup></p>
<p>On the subject of Mr. Kuhn’s expulsion from the FSF governing bodies, we
consider the following principle of the Voting Member’s code of ethics:</p>
<blockquote>
<p>A Voting Member must act in good faith in accord with the regulations of the
Free Software Foundation, including its articles of incorporation and its
bylaws.</p>
</blockquote>
<p>We assert that the removal of Mr. Kuhn for the purpose of installing an
electorate that would re-instate Richard Stallman on favorable terms is not
acting in good faith and hold the quorum accountable to this.</p>
<p>It is noted by Mr. Kuhn that Richard Stallman was among the Voting Members that
voted for Mr. Kuhn’s expulsion. We find that this contravenes the following
provision of the code of ethics for board members:</p>
<blockquote>
<p>Board members shall all avoid placing–and the appearance of placing–one’s
own self interest or any third-party interest, including the interests of
associate members, above that of the organization as a whole.</p>
</blockquote>
<p>We argue that the continued support of the Free Software Foundation Board of
Directors for Richard Stallman’s platform places the interests of Richard
Stallman above that of the organization as a whole, in particular with respect
to the formal policy of non-cooperation many institutions in the free software
community have adopted with respect to the FSF so long as Stallman remains on
the board.</p>
<p>We also note that Richard Stallman’s prolonged political program in defense of
sexual violence contravenes the following provision:</p>
<blockquote>
<p>Members of the FSF’s board of directors acknowledge that their statements and
actions have greater potential to reflect broadly on the organization because
of their leadership position and will take seriously their position of public
visibility and trust.</p>
</blockquote>
<p>This report notes that the FSF defines no particular recourse for violations of
its codes of ethics.</p>


<p>Following the 2021 re-instatement of Richard Stallman to his position on the
Free Software Foundation board of directors, numerous individuals and
institutions in the free software community spoke out in protest. Our report
focuses on institutions, on the basis that institutional policies of
non-cooperation with the FSF over the role of Richard Stallman is a significant
obstacle to the objectives of Free Software Foundation.</p>
<p>In 2021, 61 institutions and 3,003 individuals signed an open letter calling for
Stallman to be removed from all leadership positions, and calling for the board
of directors of the Free Software Foundation to resign.<sup id="fnref:58"><a href="#fn:58" role="doc-noteref">58</a></sup> An
additional 33 GNU project maintainers and developers collectively called for
Stallman’s removal in 2019.<sup id="fnref:59"><a href="#fn:59" role="doc-noteref">59</a></sup></p>
<p>This report highlights the following institutions that have explicitly withdrawn
financial support and/or adopted a policy of non-cooperation with the Free
Software Foundation over concerns regarding Richard Stallman:</p>
<ul>
<li><a href="https://codema.in/d/Xdi7EPS9/statement-on-richard-stallman-rejoining-the-fsf-board">Free Software Community of India</a></li>
<li><a href="https://fsfe.org/news/2021/news-20210324-01.html">Free Software Foundation Europe</a></li>
<li><a href="https://x.com/bad_packets/status/1374081329340456962">Okta Bad Packets</a></li>
<li><a href="https://www.outreachy.org/blog/2021-03-23/fsf-participation-barred/">Outreachy</a></li>
<li><a href="https://www.redhat.com/en/blog/red-hat-statement-about-richard-stallmans-return-free-software-foundation-board">Red Hat</a></li>
<li><a href="https://fedoramagazine.org/fedora-council-statement-on-richard-stallman-rejoining-fsf-board/">The Fedora Project</a></li>
<li><a href="https://opensource.org/blog/OSI_Response">The Open Source Initiative</a></li>
<li><a href="https://x.com/torproject/status/1374754834050654212">Tor Foundation</a></li>
<li><a href="https://news.opensuse.org/2021/04/12/a-message-from-the-opensuse-board/">openSUSE</a></li>
</ul>

<h2 id="recommendations-for-reconciliation-and-closure">
	Recommendations for reconciliation and closure
	
	<a href="#recommendations-for-reconciliation-and-closure"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>This report provides the following recommendations to parties involved for
seeking reconciliation and closure for the problems enumerated herein.</p>
<h3 id="recommendations-to-richard-stallman">
	Recommendations to Richard Stallman
	
	<a href="#recommendations-to-richard-stallman"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>To Mr. Stallman, we offer the following advice:</p>
<ol>
<li>We urge you to issue a detailed retraction of the positions enumerated in
this publication and cease all future statements of this nature, and to
demonstrate your renewed understanding of the subject matter.</li>
<li>We urge you to meaningfully apologize for the material harm you’ve done in
the course of your political work to defend sexual violence and undermine the
experiences of victims of sexual violence.</li>
<li>We urge you to remove all political notes and articles cited by this report
from your website, or update them with a link to your retraction.</li>
<li>We ask you to step down from all positions at the FSF and the GNU project and
entrust it to a new generation of leaders.</li>
</ol>
<p>We are of the unfortunate opinion that the scope and extent of your misconduct
disqualifies you from formal positions of power within our community
indefinitely. Your influence on the free software community is profound and
immeasurable, as is the harm you have done to victims of sexual violence. If you
wish to cement the positive parts of your legacy, you must contend with the
consequences of your violent political program.</p>
<h3 id="recommendations-to-the-fsf-leadership">
	Recommendations to the FSF leadership
	
	<a href="#recommendations-to-the-fsf-leadership"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We offer the following recommendations to the FSF Board of Directors and Voting
Members.</p>
<ol>
<li>
<p>To Voting Members, if Richard Stallman fails to step down of his own accord,
we urge you to convene a meeting of voting members at the earliest possible
occasion in accordance with section 7 of the FSF by-laws for the purpose of
removing Richard Stallman from both the Voting Members and the Board of
Directors.</p>
</li>
<li>
<p>If Richard Stallman fails to retract his political statements on sexual
violence, we encourage you to release a statement denouncing them.</p>
</li>
<li>
<p>To all members of the present-day Voting Members and Board of Directors who
were contemporaneous with the 2019 scandal and the associated patterns of
misconduct, we urge you to step down from your posts and allow new leaders to
fill your roles. Namely:</p>
<ul>
<li>Alexandre Oliva</li>
<li>Geoffrey Knauth</li>
<li>Gerald Sussman</li>
<li>Henry Poole</li>
</ul>
<p>In particular we call upon Mr. Knauth to uphold his 2021 pledge to resign “as
soon as there is a clear path for new leadership assuring continuity of the
FSF’s mission and compliance with fiduciary
requirements”.<sup id="fnref:60"><a href="#fn:60" role="doc-noteref">60</a></sup></p>
</li>
<li>
<p>The leadership is called upon to improve the FSF codes of ethics to prevent
future errors of this sort, including the institution of reasonable measures
of recourse in the event of violations of the codes of ethics.</p>
</li>
<li>
<p>The leadership is called upon to implement a comprehensive program of sexual
harassment training within the Free Software Foundation, as well as policies
and procedures for handling allegations of sexual harassment.</p>
</li>
<li>
<p>All parties complicit in the platforming of Richard Stallman are encouraged
to consider publishing a written apology for their conduct.</p>
</li>
</ol>
<p>We strongly urge you to take these actions in the best interests of the free
software community and the future of the Free Software Foundation.</p>
<h3 id="recommendations-to-the-gnu-project">
	Recommendations to the GNU project
	
	<a href="#recommendations-to-the-gnu-project"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>To the leadership of the GNU project, we recommend the following steps:</p>
<ol>
<li>The removal of Stallman as the “chief GNUisance” of the GNU project if he
fails to step down of his own accord.</li>
<li>Replacing the transphobic <a href="https://www.gnu.org/philosophy/kind-communication.en.html">GNU Kind Communication Guidelines</a>
authored by Stallman with a Code of Conduct which better addresses the needs
and safety of the community.</li>
</ol>

<p>If the leadership of the Free Software Foundation fails to account for these
problems, we call upon the community to boycott the FSF. Consider cancelling
your membership fees. We also encourage members of the community to voice their
support for these calls to action, disseminate our report as broadly as
possible, and raise our voices in condemnation of sexual violence and those who
protect perpetrators of it.</p>
<h2 id="acknowledgements">
	Acknowledgements
	
	<a href="#acknowledgements"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>This report was prepared in collaboration with a number of researchers,
advisors, and victims of sexual violence. The editors acknowledge that the
process of researching the material for this report or testifying to experiences
of sexual violence and harassment was traumatic for those involved, and we thank
them for their bravery and cooperation.</p>

<p>The editors of this report may be reached by email to
<a href="mailto:editors@stallman-report.org">editors@stallman-report.org</a>.</p>
<p>Our PGP public key is:</p>
<pre tabindex="0"><code>-----BEGIN PGP PUBLIC KEY BLOCK-----

mDMEZwPpMxYJKwYBBAHaRw8BAQdAExd8GOPBecIUkRGuuH7EN/z5bxSmGVexI6MC
1+Ko4XW0NVN0YWxsbWFuIFJlcG9ydCBlZGl0b3JzIDxlZGl0b3JzQHN0YWxsbWFu
LXJlcG9ydC5vcmc+iJkEExYKAEEWIQQUUS1wAhze9vx2OMAoMLLhOrisUAUCZwPp
MwIbAwUJBaOagAULCQgHAgIiAgYVCgkICwIEFgIDAQIeBwIXgAAKCRAoMLLhOris
UPNyAQCQVLGVof5sv+gV77p1GFnw6Uw8sE821378OCC+uUxo9AEA1WgrLe74OMd3
TSnJnK+PFDHwMhtCdvlLwirNXoE8Hwy4OARnA+kzEgorBgEEAZdVAQUBAQdA/cqY
UGagKg2nmyiI0EvItMKZMo9NF3bSa+rtrvoel2MDAQgHiH4EGBYKACYWIQQUUS1w
Ahze9vx2OMAoMLLhOrisUAUCZwPpMwIbDAUJBaOagAAKCRAoMLLhOrisUMu+AP4x
k10EFSC6DndhVSaI7CsuTZaopnxDUn5a+Tw+PSWtfQD+P5v9ydYqFZLKs+YALvb1
D1YiaJ+qmhQ+TEF64FX7xwM=
=DOmk
-----END PGP PUBLIC KEY BLOCK-----
</code></pre>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: X11 tool to share a screen area in any video meeting (254 pts)]]></title>
            <link>https://github.com/splitbrain/clipscreen</link>
            <guid>41837204</guid>
            <pubDate>Mon, 14 Oct 2024 13:11:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/splitbrain/clipscreen">https://github.com/splitbrain/clipscreen</a>, See on <a href="https://news.ycombinator.com/item?id=41837204">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">clipscreen</h2><a id="user-content-clipscreen" aria-label="Permalink: clipscreen" href="#clipscreen"></a></p>
<p dir="auto">clipscreen is a simple application that creates a virtual monitor that mirrors a portion of your screen. A green rectangle highlights the specified area.</p>
<p dir="auto">Why's this useful? You can use any screen sharing tool (Google Meet, Microsoft Teams, Jitsi Meet, etc.) to share the virtual monitor instead of your entire screen. No need to share individual windows and having to switch between them, just move any window you wannt to share into the green border.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compile</h2><a id="user-content-compile" aria-label="Permalink: Compile" href="#compile"></a></p>
<p dir="auto">Ensure you have the following installed on your system:</p>
<ul dir="auto">
<li>X11 development libraries</li>
<li>Cairo graphics library</li>
<li>A C++ compiler (e.g., g++)</li>
</ul>
<p dir="auto">Then simply run the following command to compile the application:</p>

<p dir="auto">Note: The application has only been tested on Linux and xorg. I doubt it will work on any other system.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Run the compiled executable with the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./clipscreen <width> <height> <x> <y>"><pre>./clipscreen <span>&lt;</span>width<span>&gt;</span> <span>&lt;</span>height<span>&gt;</span> <span>&lt;</span>x<span>&gt;</span> <span>&lt;</span>y<span>&gt;</span></pre></div>
<ul dir="auto">
<li><code>&lt;width&gt;</code>: The width of the overlay and virtual monitor.</li>
<li><code>&lt;height&gt;</code>: The height of the overlay and virtual monitor.</li>
<li><code>&lt;x&gt;</code>: The x-coordinate of the top-left corner of the overlay and virtual monitor.</li>
<li><code>&lt;y&gt;</code>: The y-coordinate of the top-left corner of the overlay and virtual monitor.</li>
</ul>
<p dir="auto">For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./clipscreen 800 600 100 100"><pre>./clipscreen 800 600 100 100</pre></div>
<p dir="auto">This command will create an 800x600 overlay window starting at position (100, 100) on your screen.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Termination</h2><a id="user-content-termination" aria-label="Permalink: Termination" href="#termination"></a></p>
<p dir="auto">To terminate the application, press <code>Ctrl+C</code> in the terminal where the application is running.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright 2024 Andreas Gohr <a href="mailto:andi@splitbrain.org">andi@splitbrain.org</a></p>
<p dir="auto">Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p>
<p dir="auto">The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p>
<p dir="auto">THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>