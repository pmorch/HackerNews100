<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 19 Apr 2025 15:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Android phones will soon reboot themselves after sitting unused for three days (104 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/04/android-phones-will-soon-reboot-themselves-after-sitting-unused-for-3-days/</link>
            <guid>43735902</guid>
            <pubDate>Sat, 19 Apr 2025 12:14:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/04/android-phones-will-soon-reboot-themselves-after-sitting-unused-for-3-days/">https://arstechnica.com/gadgets/2025/04/android-phones-will-soon-reboot-themselves-after-sitting-unused-for-3-days/</a>, See on <a href="https://news.ycombinator.com/item?id=43735902">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      
                      
          <p>A silent update rolling out to virtually all Android devices will make your phone more secure, and all you have to do is not touch it for a few days. The new feature implements auto-restart of a locked device, which will make your personal data harder to extract. It's coming as part of a Google Play Services update, though, so there's nothing you can do to speed along the process.</p>
<p>Google is preparing to release a <a href="https://support.google.com/product-documentation/answer/14343500">new update to Play Services</a> (v25.14), which brings a raft of tweaks and improvements. First <a href="https://9to5google.com/2025/04/14/android-auto-restart-security/">spotted</a> by 9to5Google, the update was officially released on April 14, but as with all Play Services updates, it could take a week or more to reach all devices. When 25.14 arrives, Android devices will see a few minor improvements, including prettier settings screens, improved connection with cars and watches, and content previews when using Quick Share.</p>
<p>Most importantly, Play Services 25.14 adds a feature that Google describes thusly: "With this feature, your device automatically restarts if locked for 3 consecutive days."</p>
<p>This is similar to a feature known as Inactivity Reboot that Apple added to the iPhone in <a href="https://arstechnica.com/gadgets/2024/10/apple-releases-ios-18-1-macos-15-1-with-apple-intelligence/">iOS 18.1</a>. This actually caused some annoyance among law enforcement officials who believed they had suspects' phones stored in a readable state, only to find they were rebooting and becoming harder to access due to this feature.</p>

          
                      
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Map of British Dialects (2023) (157 pts)]]></title>
            <link>https://starkeycomics.com/2023/11/07/map-of-british-english-dialects/</link>
            <guid>43734953</guid>
            <pubDate>Sat, 19 Apr 2025 08:02:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://starkeycomics.com/2023/11/07/map-of-british-english-dialects/">https://starkeycomics.com/2023/11/07/map-of-british-english-dialects/</a>, See on <a href="https://news.ycombinator.com/item?id=43734953">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

				
<figure><a href="https://i0.wp.com/starkeycomics.com/wp-content/uploads/2023/11/British-accents-5-JPG-1.jpg?ssl=1"><img data-recalc-dims="1" decoding="async" width="697" height="974" data-attachment-id="1885" data-permalink="https://starkeycomics.com/2023/11/07/map-of-british-english-dialects/british-accents-5-jpg-1/" data-orig-file="https://i0.wp.com/starkeycomics.com/wp-content/uploads/2023/11/British-accents-5-JPG-1.jpg?fit=945%2C1321&amp;ssl=1" data-orig-size="945,1321" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="British-accents-5-JPG-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/starkeycomics.com/wp-content/uploads/2023/11/British-accents-5-JPG-1.jpg?fit=215%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/starkeycomics.com/wp-content/uploads/2023/11/British-accents-5-JPG-1.jpg?fit=697%2C974&amp;ssl=1" src="https://i0.wp.com/starkeycomics.com/wp-content/uploads/2023/11/British-accents-5-JPG-1.jpg?resize=697%2C974&amp;ssl=1" alt="" srcset="https://i0.wp.com/starkeycomics.com/wp-content/uploads/2023/11/British-accents-5-JPG-1.jpg?resize=733%2C1024&amp;ssl=1 733w, https://i0.wp.com/starkeycomics.com/wp-content/uploads/2023/11/British-accents-5-JPG-1.jpg?resize=215%2C300&amp;ssl=1 215w, https://i0.wp.com/starkeycomics.com/wp-content/uploads/2023/11/British-accents-5-JPG-1.jpg?resize=768%2C1074&amp;ssl=1 768w, https://i0.wp.com/starkeycomics.com/wp-content/uploads/2023/11/British-accents-5-JPG-1.jpg?w=945&amp;ssl=1 945w" sizes="(max-width: 697px) 100vw, 697px"></a></figure>



<h3><strong><em>This map took me a long time to make, and is very detailed, but will always be incomplete and inaccurate due to the nature of language. </em></strong></h3>



<h3>Why this map is so detailed</h3>



<p>The diversity of English dialects in the United Kingdom is enormous.</p>



<p>It’s common for people from either side of a river, mountain, or even town to speak noticeably different ways, with particular features that immediately mark someone out as being from a specific area, to those who have an ear for it.</p>



<p>This is pretty normal in any large region that has been speaking a language continually for 1600 years. You will find the same thing in Germany, Norway, France, and countless other countries. Languages evolve over time, and physical distance between regions means that new features often spread slowly, leading to dialectal differences. Sometimes these differences are small, and only easily recognised by people from the relevant region. Other times there are very clear distinctions, with neighbouring dialects sounding almost like different languages to those unaccustomed to them.</p>



<p>Here I have tried to capture as much nuance as possible. I’ve spent the last few years pooling together every study, survey, map, and database I can find, and then subjecting my image to several rounds of peer feedback. The members of my Facebook group, “Ah yes, the British accent”, were also a huge help in trying to make these borders as accurate as possible.  The end result is an image which is, to my knowledge, the most detailed map of British dialects ever made. But it is still very much unfinished, and it always will be.</p>







<h3>Why this map is wrong, and always will be</h3>



<p>Maps are great. They allow us to display complex geographic data in a way that is visually appealing and easily understood. But often reality is just not that simple. </p>



<p><strong>There’s no precise definition of a “dialect”</strong></p>



<p>The definition is easy enough: a dialect is a form of language that has distinct vocabulary, pronunciation (accent), and/or grammar. But how different does a way of speaking need to be to constitute a different dialect? If any noticeable difference between the way two areas speak is a dialect, then my image is actually using very broad categories and missing much detail. My own tiny hometown (on the border between “North Cumbrian” and “West Northumbrian”) has words and pronunciations that don’t fit it into either grouping, but I think showing my town as a distinct bubble here would be a dangerous precedent. During my research for this image I talked with many people who, like me, perceive their village, town, or even street as being distinct from the surrounding area, and they’re probably right. But that kind of precision would make my image far more complicated, far harder to create, and likely far less accurate in other ways. So I’ve drawn lines around larger areas where more obvious distinctions can be found, without any strong criteria for what constitutes a dialect. I’ve also tried to show the similarities between neighbouring dialects by using a colour scheme that changes gradually across the country.</p>



<p><strong>Borders between dialects are rarely hard lines</strong></p>



<p>More often than not, dialects do not suddenly change as you move from one region to another. They flow and merge over time in complex and messy ways, like coloured inks diffusing into water. Yes, there is definitely a difference between the dialect of Barrow in southern Cumbria and Carlisle in the north, but in reality the region between them is a spectrum, and the placement of any dialect border across Cumbria is pretty arbitrary. Dialects on either side of it will have more in common with each other than they do with the rest of their side of the county.</p>



<p>In an attempt to show this, the first few drafts of this map had no white borders, instead just similar neighbouring colours against each other. However, this turned out to be a nightmare for colour-blind people. I then put in a combination of dotted lines for smaller distinctions, and solid, thick lines to show different dialect groups. But that didn’t feel right either, as while it’s easy to group together the East Midlands as separate from the West Midlands, the “Mid” Midlands doesn’t really have any solid distinction like that: it’s a gradient between the two sides. So I’ve left it all as dotted lines, almost as an apology for the map’s inability to show the true messy gradient that exists. I hope they serve to convey the vagueness and permeability of these “borders”.</p>



<p>In reality these colours should blur and fade together with gradients and checkerboard markings and about 35 extra spatial dimensions, but that would very much defeat the point of trying to make an intelligible map. </p>



<p><strong>Some dialects are not geographically specific at all</strong></p>



<p>While most dialects can be described as regional, spoken mainly in one area, that isn’t always the case. London is the prime example of this: my map cops out with “London Dialects” (plural), because in reality London is incredibly diverse, and deserves its own map to show that complexity. Except no, nobody will ever map that map, because more than anywhere geography is not how London dialects are arranged. Cultural and socioeconomic background is a much bigger deciding factor, and dialects like Multicultural London English aren’t found in one area, but all across London.</p>



<p>Other examples of dialects hat aren’t in this image because they aren’t specifically regional include Received Pronunciation, the “standard” prestige dialect spoken across the southern Britain; and Pitmatic, a dialect spoken by scattered coal-mining towns across the northeast of England. </p>











<p>So yes, this map may be unsatisfying, arbitrary, and unfinished, and no amount of work on it will really change that. It exists mainly as a testament to the huge dialectal diversity of the English language within the UK, and as a way for me to express my fascination and love for that diversity.</p>







<h3>Other important notes</h3>



<p><strong>What I mean by “British”</strong></p>



<p>This is a map specifically of British English dialects. That means dialects of the English language of the UK (England, Scotland, Wales, and Northern Ireland), and the Crown Dependencies (The Isle of Man, The Bailiwick of Jersey, and the Bailiwick of Guernsey), which are not part of the UK but are “British”. It does not include Ireland, because Ireland is not British. I considered making this another of my “British and Irish” maps, but honestly this project was already enormous and impossible enough that adding the Republic of Ireland into the mix felt like a step too far. <br>If you’re unclear on the use of terms like “UK” and “British”, this post is for you:<br><a href="https://starkeycomics.com/2020/05/21/britain-vs-gb-vs-uk-vs-british-isles/"><strong>The difference between Britain, Great Britain, and the United Kingdom.</strong></a></p>



<p><strong>Why Northern Ireland is included</strong></p>



<p>I <em>did</em> include Northern Ireland, which could be a little controversial (as so many things involving NI are). Broadly speaking, there are two main groups of people in Northern Ireland: those who see themselves as Irish, and those who see themselves as British. Because there are people in Northern Ireland who see themselves as British, I think it’s fitting to include them on a map of British dialects. This does not mean everyone in Northern Ireland is British, and if I ever make a map of Irish dialects, I will also be including Northern Ireland in that, as it obviously has a major Irish population. </p>



<p>There is also a strong link between Ulster dialects and the dialects of Scotland, as both have a strong Scots influence, which makes Northern Ireland an important part of the picture here. If you want to read more about how the languages of Britain and Ireland have evolved and influenced each other, see this post:<br><a href="https://starkeycomics.com/2019/03/01/a-brief-history-of-british-and-irish-languages/"><strong>A Brief History of British and Irish Languages</strong></a></p>



<p><strong>Why Scots/Doric are not included:</strong></p>



<p>This map is specifically of the English language, and Scots (and its subset, Doric), are not English. Scots is a close sibling to English, but it is distinct enough to be considered its own language. That said, the English dialects of the Scottish Lowlands are heavily influenced by Scots (with many speakers being bilingual in the two languages), and so the dialects are largely the same as the dialects of Scots. The notable exception being that there is no English dialect called “Doric”. Similarly Welsh, Scottish Gaelic, Cornish, Manx, and Irish do not belong on this map, despite being spoken within its borders. For a full list of languages spoken in the UK and Ireland and how they relate, this is the post for you: <br><a href="https://starkeycomics.com/2019/03/01/every-native-british-and-irish-language/"><strong>Every Native British and Irish Language</strong></a></p>















<p>I’ve got a ton of other maps on my site now, but if you enjoyed this one I recommend <strong><a href="https://starkeycomics.com/2024/05/10/eight-british-and-irish-accent-maps/">Eight British and Irish Accent maps</a></strong>, which includes a bunch of maps (eight, actually) showing the differences in pronunciation of specific sounds and works in Britain and Ireland. </p>



<p>If you appreciate content like this and would like to help fund and motivate my work, I’d massively appreciate a donation to my<a href="https://www.patreon.com/starkeycomics"> <strong>Patreon account</strong></a>. As a small creator with a deep hatred of ads, my Patrons are the only income I receive for my Starkey Comics stuff, and without them I’d find it harder to justify just how much of my time and effort I put into these images.</p>



<p>Oh, and make sure to <strong><a href="https://www.facebook.com/starkeycomics/">give my facebook page a follow</a></strong>. <br>I share everything I make on my facebook page, often before I get round to sharing it here on my site.</p>




				
								
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Undercutf1 – F1 Live Timing TUI with Driver Tracker, Variable Delay (162 pts)]]></title>
            <link>https://github.com/JustAman62/undercut-f1</link>
            <guid>43734910</guid>
            <pubDate>Sat, 19 Apr 2025 07:50:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/JustAman62/undercut-f1">https://github.com/JustAman62/undercut-f1</a>, See on <a href="https://news.ycombinator.com/item?id=43734910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><h2 tabindex="-1" dir="auto">undercut-f1</h2><a id="user-content-undercut-f1" aria-label="Permalink: undercut-f1" href="#undercut-f1"></a></p>
<p dir="auto">undercut-f1 is an open source F1 Live Timing client.</p>
<p dir="auto"><code>undercutf1</code> is a TUI application which uses <code>UndercutF1.Data</code> to show a Live Timing screen during sessions, and records the data for future session replays.
F1 live broadcasts are usually delayed by some undeterminable amount (usually 30-60 seconds), so the TUI allows you to delay the data being displayed so that you can match up what you see on your screen to what you see on your TV.</p>
<p dir="auto">The <code>UndercutF1.Data</code> library is provided to facilitate connectivity with the F1 Live Timing data stream, and handle all the processing of the incoming data. It also allows for "simulated" streams, where previously recorded data streams can be played back to allow for easy development/testing.</p>
<p dir="auto">Feature Highlights:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JustAman62/undercut-f1/blob/master/docs/screenshots/race-timing-screen.png"><img src="https://github.com/JustAman62/undercut-f1/raw/master/docs/screenshots/race-timing-screen.png" alt="Timing Tower during a Race"></a></p>
<ul dir="auto">
<li><a href="#timing-tower-during-a-race">Timing Tower</a> showing for each driver:
<ul dir="auto">
<li>Live sector times, with colouring for personal/overall fastest</li>
<li>Last &amp; Best Lap</li>
<li>Current tyre</li>
<li>Age of current tyre</li>
<li>Interval to driver in front</li>
<li>Gap to leader</li>
<li>Gap <a href="#using-a-cursor-to-display-relative-gap-for-a-specific-driver">between a selected driver</a> and all other drivers (useful for monitoring pit windows)</li>
</ul>
</li>
<li><a href="#tyre-stint--strategy">Pit Stop Strategy</a> gives you at-a-glance information about all the drivers strategies</li>
<li><a href="#race-control-page">Race Control</a> messages including investigations, penalties, lap deletions, and weather</li>
<li><a href="#driver-tracker">Driver Tracker</a> shows the position of selected drivers on a live track map</li>
<li>Lap-by-lap <a href="#using-a-cursor-to-view-timing-history-by-lap">Timing History</a> to observe gaps over time</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#inspiration">Inspiration</a></li>
<li><a href="#undercutf1-in-action">UndercutF1 in Action</a>
<ul dir="auto">
<li><a href="#timing-tower-during-a-race">Timing Tower during a Race</a></li>
<li><a href="#using-a-cursor-to-display-relative-gap-for-a-specific-driver">Using a Cursor to Display Relative Gap for a Specific Driver</a></li>
<li><a href="#timing-tower-during-practicequalifying">Timing Tower during Practice/Qualifying</a></li>
<li><a href="#race-control-page">Race Control Page</a></li>
<li><a href="#driver-tracker">Driver Tracker</a></li>
<li><a href="#tyre-stint--strategy">Tyre Stint / Strategy</a></li>
<li><a href="#using-a-cursor-to-view-timing-history-by-lap">Using a Cursor to View Timing History by Lap</a></li>
<li><a href="#listen-to-and-transcribe-team-radio">Listen to and Transcribe Team Radio</a></li>
</ul>
</li>
<li><a href="#getting-started-with-undercutf1">Getting Started with <code>undercutf1</code></a>
<ul dir="auto">
<li><a href="#installation">Installation</a>
<ul dir="auto">
<li><a href="#install-and-run-as-a-dotnet-tool">Install and run as a dotnet tool</a></li>
<li><a href="#install-and-run-the-standalone-executable">Install and run the standalone executable</a></li>
<li><a href="#run-directly-from-source">Run directly from Source</a></li>
</ul>
</li>
<li><a href="#start-timing-for-a-live-session">Start Timing for a Live Session</a></li>
<li><a href="#start-timing-for-a-pre-recorded-session">Start Timing for a Pre-recorded Session</a></li>
<li><a href="#download-a-previous-session-data-for-replay">Download a previous session data for replay</a></li>
<li><a href="#during-the-session">During the Session</a>
<ul dir="auto">
<li><a href="#managing-delay">Managing Delay</a></li>
<li><a href="#using-the-cursor">Using the Cursor</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#logging">Logging</a></li>
<li><a href="#live-timing-data-source">Live Timing Data Source</a></li>
<li><a href="#data-recording-and-replay">Data Recording and Replay</a></li>
<li><a href="#notice">Notice</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspiration</h2><a id="user-content-inspiration" aria-label="Permalink: Inspiration" href="#inspiration"></a></p>
<p dir="auto">This project is heavily inspired by the <a href="https://github.com/theOehrly/Fast-F1">FastF1 project by theOehrly</a>. They did a lot of the work understanding the SignalR stream coming from the F1 Live Timing service. Visit their project if you'd like to do any sort of data analysis on past F1 events, or gather live timing data using their module.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">UndercutF1 in Action</h2><a id="user-content-undercutf1-in-action" aria-label="Permalink: UndercutF1 in Action" href="#undercutf1-in-action"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Timing Tower during a Race</h3><a id="user-content-timing-tower-during-a-race" aria-label="Permalink: Timing Tower during a Race" href="#timing-tower-during-a-race"></a></p>
<p dir="auto">Monitor sector times and gaps, see recent race control messages, capture position changes, observe pit strategies, and more with the standard Timing Tower view.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JustAman62/undercut-f1/blob/master/docs/screenshots/race-timing-screen.png"><img src="https://github.com/JustAman62/undercut-f1/raw/master/docs/screenshots/race-timing-screen.png" alt="Timing Tower during a Race"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using a Cursor to Display Relative Gap for a Specific Driver</h3><a id="user-content-using-a-cursor-to-display-relative-gap-for-a-specific-driver" aria-label="Permalink: Using a Cursor to Display Relative Gap for a Specific Driver" href="#using-a-cursor-to-display-relative-gap-for-a-specific-driver"></a></p>
<p dir="auto">Use the cursor controlled by the <kbd>▼</kbd>/<kbd>▲</kbd> <code>Cursor</code> actions in the <kbd>O</kbd> <code>Timing Tower</code> screen to select a specific driver (in this case Norris) to see the relative interval between that driver and all other. This is useful for determining where a driver will fall to after a pit stop, or looking at pit windows during under cuts.</p>
<p dir="auto">Additionally, the gap between the selected drivers and those around them over the last four laps will be displayed at the bottom of the screen. This allows you to easily see evolving gaps over time and evaluate how soon a driver may catch up or pull away.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JustAman62/undercut-f1/blob/master/docs/screenshots/relative-gap-race.png"><img src="https://github.com/JustAman62/undercut-f1/raw/master/docs/screenshots/relative-gap-race.png" alt="Relative gaps for a specific driver"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Timing Tower during Practice/Qualifying</h3><a id="user-content-timing-tower-during-practicequalifying" aria-label="Permalink: Timing Tower during Practice/Qualifying" href="#timing-tower-during-practicequalifying"></a></p>
<p dir="auto">Monitor live/best sector times, gaps, tyres, and lap deletions easily with the specialized timing tower for non-race sessions.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JustAman62/undercut-f1/blob/master/docs/screenshots/quali-timing-screen.png"><img src="https://github.com/JustAman62/undercut-f1/raw/master/docs/screenshots/quali-timing-screen.png" alt="Timing Tower during Practice/Qualifying"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Race Control Page</h3><a id="user-content-race-control-page" aria-label="Permalink: Race Control Page" href="#race-control-page"></a></p>
<p dir="auto">The <code>Race Control</code> page shows all Race Control Messages for the session, along with other session data such as the Weather.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JustAman62/undercut-f1/blob/master/docs/screenshots/race-control-screen.png"><img src="https://github.com/JustAman62/undercut-f1/raw/master/docs/screenshots/race-control-screen.png" alt="Race Control Page"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Driver Tracker</h3><a id="user-content-driver-tracker" aria-label="Permalink: Driver Tracker" href="#driver-tracker"></a></p>
<p dir="auto">The <code>Driver Tracker</code> page shows a track map overlayed with selected drivers. Use the <kbd>▼</kbd>/<kbd>▲</kbd> <code>Cursor</code> actions to choose drivers, then use the <kbd>⏎</kbd> <code>Toggle Select</code> action to toggle the inclusion of the driver on the track map. The driver under the current cursor position will also be highlighted on the map, and timing gaps will switch to interval between that driver and all other drivers.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JustAman62/undercut-f1/blob/master/docs/screenshots/driver-tracker.png"><img src="https://github.com/JustAman62/undercut-f1/raw/master/docs/screenshots/driver-tracker.png" alt="Driver Tracker Page"></a></p>
<p dir="auto">NOTE: Currently the track map is only supported in the iTerm2 terminal (by implementing the <a href="https://iterm2.com/documentation-images.html" rel="nofollow">iTerm2's Inline Image Protocol</a>), and terminals which implement the <a href="https://sw.kovidgoyal.net/kitty/graphics-protocol/" rel="nofollow">Kitty Graphics Protocol</a>. Other protocols (such as Sixel) may be supported in the future. If the track map doesn't work in your terminal, please raise an issue and I will try and fix/implement support.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tyre Stint / Strategy</h3><a id="user-content-tyre-stint--strategy" aria-label="Permalink: Tyre Stint / Strategy" href="#tyre-stint--strategy"></a></p>
<p dir="auto">The <code>Tyre Stint</code> page shows the tyre strategy for all the drivers. At a glance, see what tyres the drivers have used, how old they are, and if they are on an offset strategy to any other drivers.</p>
<p dir="auto">Use the <kbd>▼</kbd>/<kbd>▲</kbd> <code>Cursor</code> actions to view more information for a particular drivers strategy.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JustAman62/undercut-f1/blob/master/docs/screenshots/tyre-stint-screen.png"><img src="https://github.com/JustAman62/undercut-f1/raw/master/docs/screenshots/tyre-stint-screen.png" alt="Tyre Stint"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using a Cursor to View Timing History by Lap</h3><a id="user-content-using-a-cursor-to-view-timing-history-by-lap" aria-label="Permalink: Using a Cursor to View Timing History by Lap" href="#using-a-cursor-to-view-timing-history-by-lap"></a></p>
<p dir="auto">In the <code>Timing by Lap</code> page, you can use the cursor controlled by the <kbd>▼</kbd>/<kbd>▲</kbd> <code>Cursor</code> actions to view historical snapshots of the timing tower at the end of every lap. This view will show position changes during that lap, and relative changes in Gap and Interval. Scrolling through laps allows you to build a picture of how the race is unfolding.</p>
<p dir="auto">Charts on the right display how Gap to Leader and Lap Time for all selected drivers over the last 15 laps, letting you see trends and catch strategies unfolding.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JustAman62/undercut-f1/blob/master/docs/screenshots/timing-history-screen.png"><img src="https://github.com/JustAman62/undercut-f1/raw/master/docs/screenshots/timing-history-screen.png" alt="Using a Cursor to View Timing History by Lap"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Listen to and Transcribe Team Radio</h3><a id="user-content-listen-to-and-transcribe-team-radio" aria-label="Permalink: Listen to and Transcribe Team Radio" href="#listen-to-and-transcribe-team-radio"></a></p>
<p dir="auto">Listen to team radio clips from anytime in the session, and use a local ML model (Whisper) to transcribe the audio on demand. Transcription accuracy is fairly low, depending on the that days audio quality and driver. Suggestions welcome for improving this!</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JustAman62/undercut-f1/blob/master/docs/screenshots/team-radio.png"><img src="https://github.com/JustAman62/undercut-f1/raw/master/docs/screenshots/team-radio.png" alt="Listen to and Transcribe Team Radio"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started with <code>undercutf1</code></h2><a id="user-content-getting-started-with-undercutf1" aria-label="Permalink: Getting Started with undercutf1" href="#getting-started-with-undercutf1"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Install and run as a dotnet tool</h4><a id="user-content-install-and-run-as-a-dotnet-tool" aria-label="Permalink: Install and run as a dotnet tool" href="#install-and-run-as-a-dotnet-tool"></a></p>
<p dir="auto"><code>undercutf1</code> is available as a <code>dotnet</code> tool from NuGet, which means it can be installed system-wide simply by running:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install globally using the -g flag
dotnet tool install -g undercutf1

# Assuming the dotnet tools directory is on your path, simply execute undercutf1
undercutf1"><pre><span><span>#</span> Install globally using the -g flag</span>
dotnet tool install -g undercutf1

<span><span>#</span> Assuming the dotnet tools directory is on your path, simply execute undercutf1</span>
undercutf1</pre></div>
<p dir="auto">This method is recommended as it is easy to keep the app updated using <code>dotnet tool update -g undercutf1</code>. You'll need the .NET 9 SDK installed to use this installation method. If you'd rather not install the SDK, try the <a href="#install-and-run-the-standalone-executable">standalone installation option below</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Install and run the standalone executable</h4><a id="user-content-install-and-run-the-standalone-executable" aria-label="Permalink: Install and run the standalone executable" href="#install-and-run-the-standalone-executable"></a></p>
<p dir="auto">Standalone executables are attached to each GitHub release. Download the executable for your system OS/architecture and simply run it directly. The list of artifacts are available on the <a href="https://github.com/JustAman62/undercut-f1/releases/latest">release page for the latest release</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Download the latest executable (in this case for osx-arm64)
curl https://github.com/JustAman62/undercut-f1/releases/latest/download/undercutf1-osx-arm64 -o ./undercutf1 -L

# Execute undercutf1 to start the TUI
./undercutf1"><pre><span><span>#</span> Download the latest executable (in this case for osx-arm64)</span>
curl https://github.com/JustAman62/undercut-f1/releases/latest/download/undercutf1-osx-arm64 -o ./undercutf1 -L

<span><span>#</span> Execute undercutf1 to start the TUI</span>
./undercutf1</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Run directly from Source</h4><a id="user-content-run-directly-from-source" aria-label="Permalink: Run directly from Source" href="#run-directly-from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Checkout the git repository
git clone git@github.com:JustAman62/undercut-f1.git
cd undercut-f1

# Run the console project with `dotnet run`
dotnet run --project UndercutF1.Console/UndercutF1.Console.csproj"><pre><span><span>#</span> Checkout the git repository</span>
git clone git@github.com:JustAman62/undercut-f1.git
<span>cd</span> undercut-f1

<span><span>#</span> Run the console project with `dotnet run`</span>
dotnet run --project UndercutF1.Console/UndercutF1.Console.csproj</pre></div>
<p dir="auto">By default, data will be saved and read from the <code>~/undercut-f1</code> directory. See <a href="#configuration">Configuration</a> for information on how to configure this.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Start Timing for a Live Session</h3><a id="user-content-start-timing-for-a-live-session" aria-label="Permalink: Start Timing for a Live Session" href="#start-timing-for-a-live-session"></a></p>
<ol dir="auto">
<li>Start <code>undercutf1</code> as described above</li>
<li>Navigate to the <kbd>S</kbd> <code>Session</code> Screen</li>
<li>Start a Live Session with the <kbd>L</kbd> <code>Start Live Session</code> action.</li>
<li>Switch to the Timing Tower screen with the <kbd>T</kbd> <code>Timing Tower</code> action</li>
</ol>
<p dir="auto">During the session, streamed timing data will be written to <code>~/undercut-f1/data/&lt;session-name&gt;</code>. This will allow for <a href="#start-timing-for-a-pre-recorded-session">future replays</a> of this recorded data.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Start Timing for a Pre-recorded Session</h3><a id="user-content-start-timing-for-a-pre-recorded-session" aria-label="Permalink: Start Timing for a Pre-recorded Session" href="#start-timing-for-a-pre-recorded-session"></a></p>
<p dir="auto">Data for pre-recorded sessions should be stored in the <code>~/undercut-f1/data/&lt;session-name&gt;</code> directory. Sample data can be found in this repos <a href="https://github.com/JustAman62/undercut-f1/blob/master/Sample%20Data">Sample Data</a> folder. To use this sample data, copy one of the folders to <code>~/undercut-f1/data</code> and then it will be visible in step 4 below.</p>
<ol dir="auto">
<li>
<p dir="auto">OPTIONAL: Download sample data to ~/undercut-f1/data. If you already have data, or have checked out the repository, skip to the next step.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create the directory for the data if it doesn't exist
mkdir -p ~/undercut-f1/2024_Silverstone_Race

# Download the live and subscribe data files
curl https://raw.githubusercontent.com/JustAman62/undercut-f1/refs/heads/master/Sample%20Data/2024_Silverstone_Race/live.txt -o ~/undercut-f1/2024_Silverstone_Race/live.txt

curl https://raw.githubusercontent.com/JustAman62/undercut-f1/refs/heads/master/Sample%20Data/2024_Silverstone_Race/subscribe.txt -o ~/undercut-f1/2024_Silverstone_Race/subscribe.txt"><pre><span><span>#</span> Create the directory for the data if it doesn't exist</span>
mkdir -p <span>~</span>/undercut-f1/2024_Silverstone_Race

<span><span>#</span> Download the live and subscribe data files</span>
curl https://raw.githubusercontent.com/JustAman62/undercut-f1/refs/heads/master/Sample%20Data/2024_Silverstone_Race/live.txt -o <span>~</span>/undercut-f1/2024_Silverstone_Race/live.txt

curl https://raw.githubusercontent.com/JustAman62/undercut-f1/refs/heads/master/Sample%20Data/2024_Silverstone_Race/subscribe.txt -o <span>~</span>/undercut-f1/2024_Silverstone_Race/subscribe.txt</pre></div>
</li>
<li>
<p dir="auto">Start <code>undercutf1</code> as described <a href="#installation">above</a></p>
</li>
<li>
<p dir="auto">Navigate to the <kbd>S</kbd> <code>Session</code> Screen</p>
</li>
<li>
<p dir="auto">Start a Simulated Session with the <kbd>F</kbd> <code>Start Simulation</code> action.</p>
</li>
<li>
<p dir="auto">Select the session to start using the Up/Down arrows, then pressing <kbd>Enter</kbd></p>
</li>
<li>
<p dir="auto">Switch to the Timing Tower screen with the <kbd>T</kbd> <code>Timing Tower</code> action</p>
</li>
<li>
<p dir="auto">Optionally skip forward in time a bit by decreasing the delay with <kbd>N</kbd> (or <kbd>⇧ Shift</kbd> + <kbd>N</kbd> to decrease by 30 seconds).</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download a previous session data for replay</h3><a id="user-content-download-a-previous-session-data-for-replay" aria-label="Permalink: Download a previous session data for replay" href="#download-a-previous-session-data-for-replay"></a></p>
<p dir="auto">F1 provides static timing data files for already completed sessions. This data can be downloaded and converted into the same format <code>undercutf1</code> uses to save live recorded data. You can then replay the old session using the steps above.</p>
<ol dir="auto">
<li>List the meetings that have data available to import with <code>undercutf1 import &lt;year&gt;</code></li>
<li>Review the list of meetings returned from the command, and list the available sessions inside the chosen meeting with <code>undercutf1 import &lt;year&gt; --meeting-key &lt;meeting-key&gt;</code></li>
<li>Review the list of sessions, and select one to import: <code>undercutf1 import &lt;year&gt; --meeting-key &lt;meeting-key&gt; --session-key &lt;session-key&gt;</code></li>
<li>Data that is imported will be saved to the configured <code>DATA_DIRECTORY</code>. See <a href="#configuration">Configuration</a> for information on how to change this.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">During the Session</h3><a id="user-content-during-the-session" aria-label="Permalink: During the Session" href="#during-the-session"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Managing Delay</h4><a id="user-content-managing-delay" aria-label="Permalink: Managing Delay" href="#managing-delay"></a></p>
<p dir="auto">All session data, whether live or pre-recorded, is sent to a <code>Channel</code> that acts as a delayed-queue. After a short delay, data points are pulled from the queue and processed, leading to updates on the timing screens. The amount of this delay can be changed with the <kbd>M</kbd>/<kbd>N</kbd> <code>Delay</code> actions whilst on the timing screens. Hold <kbd>⇧ Shift</kbd> to change the delay by 30 seconds instead of 5. When using <code>undercutf1</code> during a live session, you may wish to increase this delay to around ~50 seconds (actual number may vary) to match with the broadcast delay and avoid being spoiled about upcoming action.</p>
<p dir="auto">Simulated sessions start with a calculated delay equal to the amount of time between the start of the actual session and now. This means you can decrease the delay with the <kbd>N</kbd> <code>Delay</code> action to fast-forward through the session.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Using the Cursor</h4><a id="user-content-using-the-cursor" aria-label="Permalink: Using the Cursor" href="#using-the-cursor"></a></p>
<p dir="auto">There is a global cursor that is controlled with the <kbd>▼</kbd>/<kbd>▲</kbd> <code>Cursor</code> actions. What this cursor does depends on the screen, for example is can be used in the Timing Tower screen to scroll through Race Control Messages, or to select a driver on the Tower to see comparative intervals.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">UndercutF1 can be configured using either a simple <code>config.json</code> file, through the command line at startup, or using environment variables. JSON configuration will be loaded from <code>~/undercut-f1/config.json</code>, if it exists.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>JSON Path</th>
<th>Command Line</th>
<th>Environment Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>dataDirectory</code></td>
<td><code>--data-directory</code></td>
<td><code>UNDERCUTF1_DATADIRECTORY</code></td>
<td>The directory in which JSON timing data is read or written from.</td>
</tr>
<tr>
<td><code>verbose</code></td>
<td><code>-v|--verbose</code></td>
<td><code>UNDERCUTF1_VERBOSE</code></td>
<td>Whether verbose logging should be enabled. Default: <code>false</code>. Values: <code>true</code> or <code>false</code>.</td>
</tr>
<tr>
<td><code>apiEnabled</code></td>
<td><code>--with-api</code></td>
<td><code>UNDERCUTF1_APIENABLED</code></td>
<td>Whether the app should expose an API at <a href="http://localhost:61937/" rel="nofollow">http://localhost:61937</a>. Default: <code>false</code>. Values: <code>true</code> or <code>false</code>.</td>
</tr>
<tr>
<td><code>notify</code></td>
<td><code>--notify</code></td>
<td><code>UNDERCUTF1_NOTIFY</code></td>
<td>Whether the app should sent audible BELs to your terminal when new race control messages are received. Default: <code>true</code>. Values: <code>true</code> or <code>false</code>.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Logging</h2><a id="user-content-logging" aria-label="Permalink: Logging" href="#logging"></a></p>
<p dir="auto"><code>UndercutF1.Data</code> writes logs using the standard <code>ILogger</code> implementation. SignalR client logs are also passed to the standard <code>ILoggerProvider</code>.</p>
<p dir="auto">When running <code>undercutf1</code> logs are available in two places:</p>
<ul dir="auto">
<li>Logs are stored in memory and viewable the <kbd>L</kbd> <code>Logs</code> screen. Logs can be scrolled on this screen, and the minimum level of logs shown can be changed with the <kbd>M</kbd> <code>Minimum Log Level</code> action.</li>
<li>Log files are written to <code>~/undercut-f1/logs</code>.</li>
</ul>
<p dir="auto">Default log level is set to <code>Information</code>. More verbose logging can be enabled with the <a href="#configuration"><code>verbose</code> config option</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Live Timing Data Source</h2><a id="user-content-live-timing-data-source" aria-label="Permalink: Live Timing Data Source" href="#live-timing-data-source"></a></p>
<p dir="auto">F1 live timing is streamed using <code>SignalR</code>. The <code>UndercutF1.Data</code> simply connects to this endpoint, subscribes to the data feed, and listens for messages. It subscribes to the following "topics":</p>
<ul dir="auto">
<li><code>Heartbeat</code></li>
<li><code>ExtrapolatedClock</code></li>
<li><code>TopThree</code></li>
<li><code>TimingStats</code></li>
<li><code>TimingAppData</code></li>
<li><code>WeatherData</code></li>
<li><code>TrackStatus</code></li>
<li><code>DriverList</code></li>
<li><code>RaceControlMessages</code></li>
<li><code>SessionInfo</code></li>
<li><code>SessionData</code></li>
<li><code>LapCount</code></li>
<li><code>TimingData</code></li>
<li><code>CarData.z</code></li>
<li><code>Position.z</code></li>
<li><code>ChampionshipPrediction</code></li>
<li><code>TeamRadio</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Data Recording and Replay</h2><a id="user-content-data-recording-and-replay" aria-label="Permalink: Data Recording and Replay" href="#data-recording-and-replay"></a></p>
<p dir="auto">All events received by the live timing client will be written to the configured <code>Data Directory</code>, see <a href="#configuration">see Configuration for details</a>. Files will be written to a subdirectory named using the current sessions name, e.g. <code>~/undercut-f1/data/Jeddah_Race/</code>. In this directory, two files will be written:</p>
<ul dir="auto">
<li><code>subscribe.txt</code> contains the data received at subscription time (i.e. when the live timing client connected to the stream)</li>
<li><code>live.txt</code> contains an append-log of every message received in the stream</li>
</ul>
<p dir="auto">Both of these files are required for future simulations/replays. The <code>IJsonTimingClient</code> supports loading these files and processing them in the same way live data would be. Data points will be replayed in real time, using an adjustable delay.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notice</h2><a id="user-content-notice" aria-label="Permalink: Notice" href="#notice"></a></p>
<p dir="auto">undercut-f1 is unofficial and are not associated in any way with the Formula 1 companies. F1, FORMULA ONE, FORMULA 1, FIA FORMULA ONE WORLD CHAMPIONSHIP, GRAND PRIX and related marks are trade marks of Formula One Licensing B.V.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: New world record – verified Goldbach Conjecture up to 4*10^18+7*10^13 (170 pts)]]></title>
            <link>https://medium.com/@jay_gridbach/grid-computing-shatters-world-record-for-goldbach-conjecture-verification-1ef3dc58a38d</link>
            <guid>43734583</guid>
            <pubDate>Sat, 19 Apr 2025 06:11:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@jay_gridbach/grid-computing-shatters-world-record-for-goldbach-conjecture-verification-1ef3dc58a38d">https://medium.com/@jay_gridbach/grid-computing-shatters-world-record-for-goldbach-conjecture-verification-1ef3dc58a38d</a>, See on <a href="https://news.ycombinator.com/item?id=43734583">Hacker News</a></p>
Couldn't get https://medium.com/@jay_gridbach/grid-computing-shatters-world-record-for-goldbach-conjecture-verification-1ef3dc58a38d: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[JavaScript Views, the Hard Way – A Pattern for Writing UI (133 pts)]]></title>
            <link>https://github.com/matthewp/views-the-hard-way</link>
            <guid>43733636</guid>
            <pubDate>Sat, 19 Apr 2025 02:10:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/matthewp/views-the-hard-way">https://github.com/matthewp/views-the-hard-way</a>, See on <a href="https://news.ycombinator.com/item?id=43733636">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Writing JavaScript Views the Hard Way</h2><a id="user-content-writing-javascript-views-the-hard-way" aria-label="Permalink: Writing JavaScript Views the Hard Way" href="#writing-javascript-views-the-hard-way"></a></p>
<p dir="auto">Learn how to build views in plain JavaScript in a way that is maintainable, performant, and fun. <em>Writing JavaScript Views the Hard Way</em> is inspired by such books as <a href="https://learncodethehardway.org/c/" rel="nofollow">Learn C the Hard Way</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is this?</h2><a id="user-content-what-is-this" aria-label="Permalink: What is this?" href="#what-is-this"></a></p>
<p dir="auto">Writing JavaScript Views the Hard Way is a pattern for writing JavaScript views. It is meant to serve as an alternative to using frameworks and libraries such as <a href="https://reactjs.org/" rel="nofollow">React</a>, <a href="https://vuejs.org/" rel="nofollow">Vue</a> and <a href="https://lit-html.polymer-project.org/" rel="nofollow">lit-html</a>.</p>
<p dir="auto">It is a <strong>pattern</strong>, not a library. This document explains how to write views in such a way as to avoid the <a href="https://en.wikipedia.org/wiki/Spaghetti_code" rel="nofollow">spaghetti code</a> problems that commonly occur when writing low-level imperative code.</p>
<p dir="auto">We call this technique <em>the hard way</em> because it askews abstractions in favor of directness.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Advantages over frameworks</h3><a id="user-content-advantages-over-frameworks" aria-label="Permalink: Advantages over frameworks" href="#advantages-over-frameworks"></a></p>
<p dir="auto">There are several reasons why you might be interested in writing your views the hard way:</p>
<ul dir="auto">
<li><strong>Performance</strong>: <em>Writing JavaScript Views the Hard Way</em> uses direct imperative code, so there are no unnecessary operations performed. Whether a hot or cold path, using this technique ensures nearly the best possible performance you can get in JavaScript.</li>
<li><strong>0 dependencies</strong>: This technique uses no dependencies, so your code will <em>never</em> have to be upgraded. Have you ever used a library that released a breaking change that took you a day to upgrade? You'll never experience that problem again.</li>
<li><strong>Portability</strong>: Code written with simple imperative views is portable to any framework. That makes it perfect for low-level components that you might want to share with several framework communities. But I recommend using it on full apps as well.</li>
<li><strong>Maintainability</strong>: Despite the reputation of imperative code being difficult to maintain, views written with Writing JavaScript Views the Hard Way are <em>extremely</em> maintainable. This is because they follow strict conventions (you'll learn these later). These conventions ensure you always know where to look in a view. Additionally it follows a <em>props down, events up</em> model that makes data sharing straight-forward.</li>
<li><strong>Browser support</strong>: Code written in this manner is supported by all browsers; full-stop. We do use events to make passing data back up the component tree and our examples use a newer, nicer API, to do that, but you can use an older technique (discussed in the compatibility section) to get you back to at least IE9. But if you want to go further back than that even, substitute passing functions as props instead of using events and you can use this technique in IE6 if you want. And it will be by far the most performant solution you'll find for old browsers.</li>
<li><strong>Easier to debug</strong>: Using this approach stack traces become shallow (usually only a few function calls). This is because there are no layers between events and your code. Everything is your code, and as long as you name your functions, you'll get incredible stack traces that make it easy to trace where something goes wrong.</li>
<li><strong>Functional</strong>: This doesn't differentiate the technique vs <em>all</em> frameworks but it's worth pointing out at a benefit. <em>Writing JavaScript Views the Hard Way</em> is not functional in the immutable sense; there are definitely mutations; but it is functional in the sense that you're dealing with plain functions (no classes in sight) and without side-effects outside of the view's local state.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">The structure</h2><a id="user-content-the-structure" aria-label="Permalink: The structure" href="#the-structure"></a></p>
<p dir="auto">Enough with the arguments for now, let's talk about the structure. A view component written with <em>Writing JavaScript Views the Hard Way</em> looks like the following. This is a full <strong>hello world</strong>. From here we'll break down each part and explain it on its own.</p>
<p dir="auto">Once you understand each part you'll know how to build components/views using this pattern; it's everything you need to know.</p>
<div dir="auto" data-snippet-clipboard-copy-content="const template = document.createElement('template');
template.innerHTML = `
  <div>Hello <span id=&quot;name&quot;>world</span>!</div>
`;

function clone() {
  return document.importNode(template.content, true);
}

function init() {
  /* DOM variables */
  let frag = clone();
  let nameNode = frag.querySelector('#name');

  /* State variables */
  let name;

  /* DOM update functions */
  function setNameNode(value) {
    nameNode.textContent = value;
  }

  /* State update functions */
  function setName(value) {
    if(name !== value) {
      name = value;
      setNameNode(value);
    }
  }

  /* State logic */

  /* Event dispatchers */

  /* Event listeners */

  /* Initialization */

  function update(data = {}) {
    if(data.name) setName(data.name);
    return frag;
  }

  return update;
}

export default init;"><pre><span>const</span> <span>template</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'template'</span><span>)</span><span>;</span>
<span>template</span><span>.</span><span>innerHTML</span> <span>=</span> <span>`</span>
<span>  &lt;div&gt;Hello &lt;span id="name"&gt;world&lt;/span&gt;!&lt;/div&gt;</span>
<span>`</span><span>;</span>

<span>function</span> <span>clone</span><span>(</span><span>)</span> <span>{</span>
  <span>return</span> <span>document</span><span>.</span><span>importNode</span><span>(</span><span>template</span><span>.</span><span>content</span><span>,</span> <span>true</span><span>)</span><span>;</span>
<span>}</span>

<span>function</span> <span>init</span><span>(</span><span>)</span> <span>{</span>
  <span>/* DOM variables */</span>
  <span>let</span> <span>frag</span> <span>=</span> <span>clone</span><span>(</span><span>)</span><span>;</span>
  <span>let</span> <span>nameNode</span> <span>=</span> <span>frag</span><span>.</span><span>querySelector</span><span>(</span><span>'#name'</span><span>)</span><span>;</span>

  <span>/* State variables */</span>
  <span>let</span> <span>name</span><span>;</span>

  <span>/* DOM update functions */</span>
  <span>function</span> <span>setNameNode</span><span>(</span><span>value</span><span>)</span> <span>{</span>
    <span>nameNode</span><span>.</span><span>textContent</span> <span>=</span> <span>value</span><span>;</span>
  <span>}</span>

  <span>/* State update functions */</span>
  <span>function</span> <span>setName</span><span>(</span><span>value</span><span>)</span> <span>{</span>
    <span>if</span><span>(</span><span>name</span> <span>!==</span> <span>value</span><span>)</span> <span>{</span>
      <span>name</span> <span>=</span> <span>value</span><span>;</span>
      <span>setNameNode</span><span>(</span><span>value</span><span>)</span><span>;</span>
    <span>}</span>
  <span>}</span>

  <span>/* State logic */</span>

  <span>/* Event dispatchers */</span>

  <span>/* Event listeners */</span>

  <span>/* Initialization */</span>

  <span>function</span> <span>update</span><span>(</span><span>data</span> <span>=</span> <span>{</span><span>}</span><span>)</span> <span>{</span>
    <span>if</span><span>(</span><span>data</span><span>.</span><span>name</span><span>)</span> <span>setName</span><span>(</span><span>data</span><span>.</span><span>name</span><span>)</span><span>;</span>
    <span>return</span> <span>frag</span><span>;</span>
  <span>}</span>

  <span>return</span> <span>update</span><span>;</span>
<span>}</span>

<span>export</span> <span>default</span> <span>init</span><span>;</span></pre></div>
<p dir="auto">This is the basic structure. More details are to follow. First let's concentrate on the module's parts and exports.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">template</h3><a id="user-content-template" aria-label="Permalink: template" href="#template"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="const template = document.createElement('template');
template.innerHTML = `
  <div>Hello <span id=&quot;name&quot;>world</span>!</div>
`;"><pre><span>const</span> <span>template</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'template'</span><span>)</span><span>;</span>
<span>template</span><span>.</span><span>innerHTML</span> <span>=</span> <span>`</span>
<span>  &lt;div&gt;Hello &lt;span id="name"&gt;world&lt;/span&gt;!&lt;/div&gt;</span>
<span>`</span><span>;</span></pre></div>
<p dir="auto">This is the view's template. It's a <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/template" rel="nofollow">&lt;template&gt;</a> element. Setting its <code>innerHTML</code> causes the browser to parse and save this HTML as the template's <code>template.content</code> property. This is a <a href="https://developer.mozilla.org/en-US/docs/Web/API/DocumentFragment" rel="nofollow">DocumentFragment</a> that can be quickly cloned.</p>
<p dir="auto">Notice that there are no interpolations with data. This is because, as of now, the browser doesn't support any such API. In the spirit of <em>The Hard Way</em> we use only what the browser gives us.</p>
<p dir="auto">So instead of interpolating, we add elements at points within the HTML that we will want to update later. In this example we have <code>&lt;span id="name"&gt;world&lt;/span&gt;</code>. This gives us something that we can query and update later (via <code>frag.querySelector('#name')</code> for example).</p>
<blockquote>
<p dir="auto"><em>Note</em>: ids are global to the document. If the component you are working on is likely to be used multiple times in an application (such as a list item) you should probably not use ids, but rather a class name or possibly a <a href="https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_data_attributes" rel="nofollow">data attribute</a>.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">clone()</h3><a id="user-content-clone" aria-label="Permalink: clone()" href="#clone"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="function clone() {
  return document.importNode(template.content, true);
}"><pre><span>function</span> <span>clone</span><span>(</span><span>)</span> <span>{</span>
  <span>return</span> <span>document</span><span>.</span><span>importNode</span><span>(</span><span>template</span><span>.</span><span>content</span><span>,</span> <span>true</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto">This function is mostly for convenience. All it does is clone the template and return the result.</p>
<p dir="auto">However there are cases where you might want to slightly adjust the output. <a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/importNode" rel="nofollow">document.importNode</a> returns a fragment; there are cases where you want the returned node to be an <strong>element</strong> (mostly so that the consumer of your view can set up event listeners). So to return the root element you can change <strong>clone</strong> to:</p>
<div dir="auto" data-snippet-clipboard-copy-content="function clone() {
  return document.importNode(template.content, true).firstElementChild;
}"><pre><span>function</span> <span>clone</span><span>(</span><span>)</span> <span>{</span>
  <span>return</span> <span>document</span><span>.</span><span>importNode</span><span>(</span><span>template</span><span>.</span><span>content</span><span>,</span> <span>true</span><span>)</span><span>.</span><span>firstElementChild</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">init()</h3><a id="user-content-init" aria-label="Permalink: init()" href="#init"></a></p>
<p dir="auto">This is the function that gets called by parent views in order to create a new view instance. Going with our hello world example, a consumer that wants to insert this into the page would do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="<!doctype html>
<html lang=&quot;en&quot;>
<title>Hello world</title>

<main></main>

<script type=&quot;module&quot;>
  import init from './view.js';

  const main = document.querySelector('main');
  const update = init();

  main.appendChild(update({ name: 'world' }));
</script>"><pre><span>&lt;!doctype html<span>&gt;</span></span>
<span>&lt;</span><span>html</span> <span>lang</span>="<span>en</span>"<span>&gt;</span>
<span>&lt;</span><span>title</span><span>&gt;</span>Hello world<span>&lt;/</span><span>title</span><span>&gt;</span>

<span>&lt;</span><span>main</span><span>&gt;</span><span>&lt;/</span><span>main</span><span>&gt;</span>

<span>&lt;</span><span>script</span> <span>type</span>="<span>module</span>"<span>&gt;</span>
  <span>import</span> <span>init</span> <span>from</span> <span>'./view.js'</span><span>;</span>

  <span>const</span> <span>main</span> <span>=</span> <span>document</span><span>.</span><span>querySelector</span><span>(</span><span>'main'</span><span>)</span><span>;</span>
  <span>const</span> <span>update</span> <span>=</span> <span>init</span><span>(</span><span>)</span><span>;</span>

  <span>main</span><span>.</span><span>appendChild</span><span>(</span><span>update</span><span>(</span><span>{</span> <span>name</span>: <span>'world'</span> <span>}</span><span>)</span><span>)</span><span>;</span>
<span>&lt;/</span><span>script</span><span>&gt;</span></pre></div>
<p dir="auto">Notice here that <code>view</code> returns another function, <code>update</code>. This is the way that parent views can pass props down to the view. We'll discuss this concept more in the <a href="https://github.com/matthewp/views-the-hard-way/blob/main/update">#update</a> section.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">React comparison</h4><a id="user-content-react-comparison" aria-label="Permalink: React comparison" href="#react-comparison"></a></p>
<p dir="auto">Since <code>init</code> creates a new view instance, it's similar in that way to a component's constructor. To give an example, <a href="https://reactjs.org/docs/react-component.html#constructor" rel="nofollow">React.Component</a> does setup work in its constructor and updates happen in its <code>render</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="class Welcome extends React.Component {
  constructor(props) {
    super(props);
    this.state = {};
  }

  render() {
    return <div>Hello {this.props.name}!</div>;
  }
}"><pre><span>class</span> <span>Welcome</span> <span>extends</span> <span>React</span><span>.</span><span>Component</span> <span>{</span>
  <span>constructor</span><span>(</span><span>props</span><span>)</span> <span>{</span>
    <span>super</span><span>(</span><span>props</span><span>)</span><span>;</span>
    <span>this</span><span>.</span><span>state</span> <span>=</span> <span>{</span><span>}</span><span>;</span>
  <span>}</span>

  <span>render</span><span>(</span><span>)</span> <span>{</span>
    <span>return</span> <span>&lt;</span><span>div</span><span>&gt;</span>Hello <span>{</span><span>this</span><span>.</span><span>props</span><span>.</span><span>name</span><span>}</span>!<span>&lt;/</span><span>div</span><span>&gt;</span><span>;</span>
  <span>}</span>
<span>}</span></pre></div>
<p dir="auto">Using this component in another class illustrates how <em>Writing JavaScript Views the Hard Way</em> is similar:</p>
<div dir="auto" data-snippet-clipboard-copy-content="class App {
  render() {
    return <Welcome name=&quot;world&quot; />
  }
}"><pre><span>class</span> <span>App</span> <span>{</span>
  <span>render</span><span>(</span><span>)</span> <span>{</span>
    <span>return</span> <span>&lt;</span><span>Welcome</span> <span>name</span><span>=</span><span>"world"</span> <span>/&gt;</span>
  <span>}</span>
<span>}</span></pre></div>
<p dir="auto">To make the comparison, in our parent component/view we do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="function init() {
  /* DOM variables */
  let frag = clone();
  let hostNode = frag.querySelector('#host');

  /* DOM views */
  let updateWelcome = welcomeView();

  /* DOM update functions */
  function setHostNode(welcomeFrag) {
    hostNode.appendChild(welcomeFrag());
  }

  /* Initialization */
  setHostNode(updateWelcome());

  function update(data = {}) {
    // This is equivalent to render() being called.
    if(data.name) updateWelcome(data);

    return frag;
  }

  return update;
}"><pre><span>function</span> <span>init</span><span>(</span><span>)</span> <span>{</span>
  <span>/* DOM variables */</span>
  <span>let</span> <span>frag</span> <span>=</span> <span>clone</span><span>(</span><span>)</span><span>;</span>
  <span>let</span> <span>hostNode</span> <span>=</span> <span>frag</span><span>.</span><span>querySelector</span><span>(</span><span>'#host'</span><span>)</span><span>;</span>

  <span>/* DOM views */</span>
  <span>let</span> <span>updateWelcome</span> <span>=</span> <span>welcomeView</span><span>(</span><span>)</span><span>;</span>

  <span>/* DOM update functions */</span>
  <span>function</span> <span>setHostNode</span><span>(</span><span>welcomeFrag</span><span>)</span> <span>{</span>
    <span>hostNode</span><span>.</span><span>appendChild</span><span>(</span><span>welcomeFrag</span><span>(</span><span>)</span><span>)</span><span>;</span>
  <span>}</span>

  <span>/* Initialization */</span>
  <span>setHostNode</span><span>(</span><span>updateWelcome</span><span>(</span><span>)</span><span>)</span><span>;</span>

  <span>function</span> <span>update</span><span>(</span><span>data</span> <span>=</span> <span>{</span><span>}</span><span>)</span> <span>{</span>
    <span>// This is equivalent to render() being called.</span>
    <span>if</span><span>(</span><span>data</span><span>.</span><span>name</span><span>)</span> <span>updateWelcome</span><span>(</span><span>data</span><span>)</span><span>;</span>

    <span>return</span> <span>frag</span><span>;</span>
  <span>}</span>

  <span>return</span> <span>update</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Sections within init()</h3><a id="user-content-sections-within-init" aria-label="Permalink: Sections within init()" href="#sections-within-init"></a></p>
<p dir="auto">The <code>init</code> function acts as a closure for the view instance. It contains variables and functions that are used to change state, modify the DOM, listen to events, and receive updates from parent views.</p>
<p dir="auto">Conventional wisdom says this sort of imperative code causes chaos. <em>Writing JavaScript Views the Hard Way</em> prevents this by structuring each part of a view into sections. This <strong>convention</strong> gives you a place to put everything your view needs.</p>
<p dir="auto">After all, all code is imperative in the end. Abstractions are just conventions in hiding. A <strong>pattern</strong>, which is all <em>Writing JavaScript Views the Hard Way</em> is, are conventions in plain site.</p>
<p dir="auto">With that being said, here are the sections of <code>init</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">DOM variables</h4><a id="user-content-dom-variables" aria-label="Permalink: DOM variables" href="#dom-variables"></a></p>
<p dir="auto">At the top of the <code>init</code> function is a comment <code>/* DOM variables */</code> and it looks something like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="function init() {
  /* DOM variables */
  let frag = clone();
  let nameNode = frag.querySelector('#name');

  // More stuff later...
}"><pre><span>function</span> <span>init</span><span>(</span><span>)</span> <span>{</span>
  <span>/* DOM variables */</span>
  <span>let</span> <span>frag</span> <span>=</span> <span>clone</span><span>(</span><span>)</span><span>;</span>
  <span>let</span> <span>nameNode</span> <span>=</span> <span>frag</span><span>.</span><span>querySelector</span><span>(</span><span>'#name'</span><span>)</span><span>;</span>

  <span>// More stuff later...</span>
<span>}</span></pre></div>
<p dir="auto">Let's break down what goes here:</p>
<ul dir="auto">
<li>The <strong>fragment</strong> or root element returned by <a href="#clone">clone()</a> is assigned to <code>frag</code>.</li>
<li><code>nameNode</code> is plucked from within the <code>frag</code> and held as a variable.</li>
</ul>
<p dir="auto">This pattern will be used for any nodes which might need to be modified during the course of the view's lifetime.</p>
<p dir="auto">By convention DOM nodes are named like <code>fooNode</code>. Where <code>foo</code> is contextual to the node's usage and <code>Node</code> denotes that it is a DOM node.</p>
<p dir="auto">As you'll see in below sections, there are many types of bindings in a view. Having conventional names makes it easier to tell what is what.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">DOM views</h4><a id="user-content-dom-views" aria-label="Permalink: DOM views" href="#dom-views"></a></p>
<p dir="auto">After <strong>DOM variables</strong> come <strong>DOM views</strong>. These are other views used within the current view. If you are used to component libraries, a view is very much like a component. In the same way that components use other components, so do views use other views.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import conditionalView from './conditiona.js';

function init() {
  /* DOM variables */
  let frag = clone();

  /* DOM views */
  let updateCondition = conditionalView();

  // More stuff later...
}"><pre><span>import</span> <span>conditionalView</span> <span>from</span> <span>'./conditiona.js'</span><span>;</span>

<span>function</span> <span>init</span><span>(</span><span>)</span> <span>{</span>
  <span>/* DOM variables */</span>
  <span>let</span> <span>frag</span> <span>=</span> <span>clone</span><span>(</span><span>)</span><span>;</span>

  <span>/* DOM views */</span>
  <span>let</span> <span>updateCondition</span> <span>=</span> <span>conditionalView</span><span>(</span><span>)</span><span>;</span>

  <span>// More stuff later...</span>
<span>}</span></pre></div>
<p dir="auto">As a convention view instances are named <code>updateFoo</code>. This isn't strictly required, of course, but it helps to distinguish them vs. the other types of variables within a view.</p>
<p dir="auto">The view instances are themselves functions. You pass them an object of properties which the view will use to update itself. Later in this document <code>update</code> is described.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">State variables</h4><a id="user-content-state-variables" aria-label="Permalink: State variables" href="#state-variables"></a></p>
<p dir="auto">After <strong>DOM views</strong> comes <strong>State variables</strong>. State variables are simply variables that are not DOM nodes or DOM views. Anything else like strings or numbers fit here.</p>
<p dir="auto">State variables are useful because they give us a mechanism to prevent mutating the DOM unless the variable has changed. Later when we discuss <strong>State update functions</strong> you'll see how that works, and why it is useful.</p>
<div dir="auto" data-snippet-clipboard-copy-content="function init() {
  /* DOM variables */
  let frag = clone();
  let nameNode = frag.querySelector('[name]');

  /* State variables */
  let name = 'world';

  // More stuff later...
}"><pre><span>function</span> <span>init</span><span>(</span><span>)</span> <span>{</span>
  <span>/* DOM variables */</span>
  <span>let</span> <span>frag</span> <span>=</span> <span>clone</span><span>(</span><span>)</span><span>;</span>
  <span>let</span> <span>nameNode</span> <span>=</span> <span>frag</span><span>.</span><span>querySelector</span><span>(</span><span>'[name]'</span><span>)</span><span>;</span>

  <span>/* State variables */</span>
  <span>let</span> <span>name</span> <span>=</span> <span>'world'</span><span>;</span>

  <span>// More stuff later...</span>
<span>}</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">DOM update functions</h4><a id="user-content-dom-update-functions" aria-label="Permalink: DOM update functions" href="#dom-update-functions"></a></p>
<p dir="auto">There are essentially 2 uses for views: Mutating the DOM and listening to user input from the DOM.</p>
<p dir="auto"><strong>DOM update functions</strong> comes after <strong>State variables</strong> and provide the mechanism for updating DOM nodes.</p>
<div dir="auto" data-snippet-clipboard-copy-content="function init() {
  /* DOM variables */
  let frag = clone();
  let nameNode = frag.querySelector('[name]');

  /* DOM update functions */
  function setNodeName(value) {
    nameNode.value = value;
  }

  // More stuff later...
}"><pre><span>function</span> <span>init</span><span>(</span><span>)</span> <span>{</span>
  <span>/* DOM variables */</span>
  <span>let</span> <span>frag</span> <span>=</span> <span>clone</span><span>(</span><span>)</span><span>;</span>
  <span>let</span> <span>nameNode</span> <span>=</span> <span>frag</span><span>.</span><span>querySelector</span><span>(</span><span>'[name]'</span><span>)</span><span>;</span>

  <span>/* DOM update functions */</span>
  <span>function</span> <span>setNodeName</span><span>(</span><span>value</span><span>)</span> <span>{</span>
    <span>nameNode</span><span>.</span><span>value</span> <span>=</span> <span>value</span><span>;</span>
  <span>}</span>

  <span>// More stuff later...</span>
<span>}</span></pre></div>
<p dir="auto">In the <a href="#dom-variables">DOM variables</a> section we mention that DOM variables are named like <code>nameNode</code>. This helps to distinguish them from other types of variables in a view, like state variables.</p>
<p dir="auto">In the same way, DOM update functions are named by convention as <code>setNodeName</code>. Breaking this down:</p>
<ul dir="auto">
<li><code>set</code> is an action we are taking on the node. It doesn't have to be set, it could be <code>change</code> or <code>delete</code> depending on what you are doing and what language you prefer.</li>
<li><code>name</code> specifies that this node holds information about a <strong>name</strong>.</li>
<li><code>Node</code> specifies that it is a DOM node that is being updated.</li>
</ul>
<p dir="auto"><em><strong>Important</strong></em></p>
<p dir="auto">It is critical that DOM nodes only be modified by DOM update functions. One of the difficulties in writing low-level imperative views is keeping track of where DOM mutations can occur.</p>
<p dir="auto">By restricting mutations to only DOM update functions we can more easily figure out where a mutation occurs. Additionally you can easily stick a breakpoint inside of this function and see the stack trace to figure out how we got here.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/361671/52751232-ca8b6180-2fbc-11e9-96bc-393c68a019ef.png"><img src="https://user-images.githubusercontent.com/361671/52751232-ca8b6180-2fbc-11e9-96bc-393c68a019ef.png" alt="An example of shallow stack traces that you get from using views the hard way"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">State update functions</h4><a id="user-content-state-update-functions" aria-label="Permalink: State update functions" href="#state-update-functions"></a></p>
<p dir="auto">After the DOM update functions comes the <strong>State update functions</strong>. This is a lot like the DOM update functions, but instead it is where <strong>State variables</strong> are changed. An example of a state update function:</p>
<div dir="auto" data-snippet-clipboard-copy-content="function init() {
  /* DOM variables */
  let frag = clone();
  let nameNode = frag.querySelector('.name');

  /* State variables */
  let name;

  /* DOM update functions */
  function setNameNode(value) {
    nameNode.textContent = value;
  }

  /* State update functions */
  function setName(value) {
    if(name !== value) {
      name = value;
      setNameNode(value);
    }
  }

  // More stuff later...
}"><pre><span>function</span> <span>init</span><span>(</span><span>)</span> <span>{</span>
  <span>/* DOM variables */</span>
  <span>let</span> <span>frag</span> <span>=</span> <span>clone</span><span>(</span><span>)</span><span>;</span>
  <span>let</span> <span>nameNode</span> <span>=</span> <span>frag</span><span>.</span><span>querySelector</span><span>(</span><span>'.name'</span><span>)</span><span>;</span>

  <span>/* State variables */</span>
  <span>let</span> <span>name</span><span>;</span>

  <span>/* DOM update functions */</span>
  <span>function</span> <span>setNameNode</span><span>(</span><span>value</span><span>)</span> <span>{</span>
    <span>nameNode</span><span>.</span><span>textContent</span> <span>=</span> <span>value</span><span>;</span>
  <span>}</span>

  <span>/* State update functions */</span>
  <span>function</span> <span>setName</span><span>(</span><span>value</span><span>)</span> <span>{</span>
    <span>if</span><span>(</span><span>name</span> <span>!==</span> <span>value</span><span>)</span> <span>{</span>
      <span>name</span> <span>=</span> <span>value</span><span>;</span>
      <span>setNameNode</span><span>(</span><span>value</span><span>)</span><span>;</span>
    <span>}</span>
  <span>}</span>

  <span>// More stuff later...</span>
<span>}</span></pre></div>
<p dir="auto">The naming convention for State update functions is <code>setName</code>:</p>
<ul dir="auto">
<li><code>set</code> is the action we are taking on the state.</li>
<li><code>name</code> denotes the name of the state variable.</li>
</ul>
<p dir="auto">The other thing to notice about State update functions is the following logic (from the example):</p>
<div dir="auto" data-snippet-clipboard-copy-content="if(name !== value) {
  name = value;
  setNameNode(value);
}"><pre><span>if</span><span>(</span><span>name</span> <span>!==</span> <span>value</span><span>)</span> <span>{</span>
  <span>name</span> <span>=</span> <span>value</span><span>;</span>
  <span>setNameNode</span><span>(</span><span>value</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto">The convention here is to check if the incoming value is different from the current value. If so, reassign the State variable to the new value. Secondary, you also often have a corresponding <strong>DOM update function</strong> that gets called when the state variable changes. This is important because it means that we are only updating DOM when necessary.</p>
<p dir="auto">Usually a State update function is only going to call <em>one</em> DOM update function. This isn't always the case, but if you find yourself modifying multiple DOM nodes in a single State update function, it might be the case that your State variable is responsibility for too much. Instead break up the state into smaller pieces, create more State update functions, and call those in the <strong>State logic</strong> section (discussed below).</p>
<p dir="auto"><em><strong>Important</strong></em></p>
<p dir="auto">Like with DOM update functions, it's critical that State variables are <em>only</em> updated within the State update functions. In particular to state, it's important so that:</p>
<ul dir="auto">
<li>You can easily track and debug where state is modified, as for each variable there's only one place where it can happen.</li>
<li>By only mutating state in the State update functions, you consolidate the logic in one place.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compatibility</h2><a id="user-content-compatibility" aria-label="Permalink: Compatibility" href="#compatibility"></a></p>
<p dir="auto"><strong>TODO</strong></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hands-On Large Language Models (125 pts)]]></title>
            <link>https://github.com/HandsOnLLM/Hands-On-Large-Language-Models</link>
            <guid>43733553</guid>
            <pubDate>Sat, 19 Apr 2025 01:52:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models">https://github.com/HandsOnLLM/Hands-On-Large-Language-Models</a>, See on <a href="https://news.ycombinator.com/item?id=43733553">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Hands-On Large Language Models</h2><a id="user-content-hands-on-large-language-models" aria-label="Permalink: Hands-On Large Language Models" href="#hands-on-large-language-models"></a></p>
<p dir="auto"><a href="https://www.linkedin.com/in/jalammar/" rel="nofollow"><img src="https://camo.githubusercontent.com/6a0c6a9c7dbb027b88582bc4a733240f959c30c38a30ee0a0c44a789925f1e56/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466f6c6c6f772532304a61792d626c75652e7376673f6c6f676f3d6c696e6b6564696e" data-canonical-src="https://img.shields.io/badge/Follow%20Jay-blue.svg?logo=linkedin"></a>
<a href="https://www.linkedin.com/in/mgrootendorst/" rel="nofollow"><img src="https://camo.githubusercontent.com/ebe48ac2cefd3aa7d6c9e6555d45271753ae640a7d5acf91d00e5ba1d90650d7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466f6c6c6f772532304d61617274656e2d626c75652e7376673f6c6f676f3d6c696e6b6564696e" data-canonical-src="https://img.shields.io/badge/Follow%20Maarten-blue.svg?logo=linkedin"></a>
<a href="https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?utm_campaign=handsonllm-launch&amp;utm_medium=partner" rel="nofollow"><img src="https://camo.githubusercontent.com/baaa1e8f1e9ce719f6777328468b164c78c03f48588ed22ebfc7807f6f59a50e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446565704c6561726e696e672e4149253230436f757273652d4e4557212d266c6162656c436f6c6f723d626c61636b26636f6c6f723d7265642e7376673f6c6f676f3d646174613a696d6167652f737667253262786d6c3b6261736536342c50484e325a79423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d636949485a705a58644362336739496a41754d4441774d7a59314d6a6778494330774c6a41774d4445304d4445304d69417a4d7934794f53417a4d7934784e53492b43676b38634746306143426b50534a4e4d5459754e6a517a49444d7a4c6a45304e574d744d7934794f5449674d4330324c6a55784c5334354e7a49744f5334794e4459744d6934334f544e684d5459754e546734494445324c6a55344f434177494441784c5459754d544d744e7934304d7a68424d5459754e544133494445324c6a55774e794177494441784c6a4d794944457a4c6a4d30595445324c6a5531494445324c6a5531494441674d4445304c6a55314e5330344c6a51344e5545784e6934324e6a55674d5459754e6a5931494441674d4445784d79347a4f5459754d7a4534595445324c6a6378494445324c6a6378494441674d4445354c6a59784e6934354e4451674d5459754e6a4934494445324c6a59794f434177494441784e7934304e7941324c6a45774d7941784e6934314d6a49674d5459754e544979494441674d4445794c6a67774e4341354c6a49774e324d77494451754d7a6b324c5445754e7a557a494467754e6a45744e4334344e7a51674d5445754e7a4535595445324c6a5934494445324c6a5934494441674d4445744d5445754e7a5935494451754f445530656d30754d5449314c5459754e6a4934597a59754f544132494441674d5449754e5445334c5455754e6a6b34494445794c6a55784e7930784d6934334d7941774c5463754d444d744e5334324d5330784d6934334d6a55744d5449754e5445334c5445794c6a63794e5330324c6a6b774e6941774c5445794c6a55784e7941314c6a59354f4330784d6934314d5463674d5449754e7a4931494441674e7934774d6a63674e5334324d5445674d5449754e7a4d674d5449754e544533494445794c6a637a656d30744c6a45794e5330794c6a6b784f474d744e6934794f446b674d4330784d53347a4f4459744e4334354d6a55744d5445754d7a67324c5445784c6a41774d6b4d314c6a49314e7941324c6a5579494445774c6a4d32494445754e546b674d5459754e6a517a494445754e546c6a4e6934794f4451674d4341784d53347a4f4459674e4334354d7941784d53347a4f4459674d5445754d444133637930314c6a41354e7941784d5334774d4449744d5445754d7a6732494445784c6a41774d6e70744c5334794e4449744e4334314d44686a4e4334334e794177494467754e6a4d7a4c544d754e6a6335494467754e6a4d7a4c5467754d6a4534494441744e4334314d7a67744d7934344f4455744f4334794d6a45744f4334324d7a4d744f4334794d6a45744e4334334e4463674d4330344c6a597a4d69417a4c6a59334f5330344c6a597a4d6941344c6a49794d534177494451754e54517a49444d754f446731494467754d6a4534494467754e6a4d79494467754d6a4534656949675a6d6c736244306949305a454e4545324d53497650676f384c334e325a7a343d" data-canonical-src="https://img.shields.io/badge/DeepLearning.AI%20Course-NEW!-&amp;labelColor=black&amp;color=red.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAuMDAwMzY1MjgxIC0wLjAwMDE0MDE0MiAzMy4yOSAzMy4xNSI+Cgk8cGF0aCBkPSJNMTYuNjQzIDMzLjE0NWMtMy4yOTIgMC02LjUxLS45NzItOS4yNDYtMi43OTNhMTYuNTg4IDE2LjU4OCAwIDAxLTYuMTMtNy40MzhBMTYuNTA3IDE2LjUwNyAwIDAxLjMyIDEzLjM0YTE2LjU1IDE2LjU1IDAgMDE0LjU1NS04LjQ4NUExNi42NjUgMTYuNjY1IDAgMDExMy4zOTYuMzE4YTE2LjcxIDE2LjcxIDAgMDE5LjYxNi45NDQgMTYuNjI4IDE2LjYyOCAwIDAxNy40NyA2LjEwMyAxNi41MjIgMTYuNTIyIDAgMDEyLjgwNCA5LjIwN2MwIDQuMzk2LTEuNzUzIDguNjEtNC44NzQgMTEuNzE5YTE2LjY4IDE2LjY4IDAgMDEtMTEuNzY5IDQuODU0em0uMTI1LTYuNjI4YzYuOTA2IDAgMTIuNTE3LTUuNjk4IDEyLjUxNy0xMi43MyAwLTcuMDMtNS42MS0xMi43MjUtMTIuNTE3LTEyLjcyNS02LjkwNiAwLTEyLjUxNyA1LjY5OC0xMi41MTcgMTIuNzI1IDAgNy4wMjcgNS42MTEgMTIuNzMgMTIuNTE3IDEyLjczem0tLjEyNS0yLjkxOGMtNi4yODkgMC0xMS4zODYtNC45MjUtMTEuMzg2LTExLjAwMkM1LjI1NyA2LjUyIDEwLjM2IDEuNTkgMTYuNjQzIDEuNTljNi4yODQgMCAxMS4zODYgNC45MyAxMS4zODYgMTEuMDA3cy01LjA5NyAxMS4wMDItMTEuMzg2IDExLjAwMnptLS4yNDItNC41MDhjNC43NyAwIDguNjMzLTMuNjc5IDguNjMzLTguMjE4IDAtNC41MzgtMy44ODUtOC4yMjEtOC42MzMtOC4yMjEtNC43NDcgMC04LjYzMiAzLjY3OS04LjYzMiA4LjIyMSAwIDQuNTQzIDMuODg1IDguMjE4IDguNjMyIDguMjE4eiIgZmlsbD0iI0ZENEE2MSIvPgo8L3N2Zz4="></a></p>
<p dir="auto">Welcome! In this repository you will find the code for all examples throughout the book <a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961" rel="nofollow">Hands-On Large Language Models</a> written by <a href="https://www.linkedin.com/in/jalammar/" rel="nofollow">Jay Alammar</a> and <a href="https://www.linkedin.com/in/mgrootendorst/" rel="nofollow">Maarten Grootendorst</a> which we playfully dubbed: <br></p>
<p dir="auto"><b><i>"The Illustrated LLM Book"</i></b></p>
<p dir="auto">Through the visually educational nature of this book and with <strong>almost 300 custom made figures</strong>, learn the practical tools and concepts you need to use Large Language Models today!</p>
<p dir="auto"><a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961" rel="nofollow"><img src="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/raw/main/images/book_cover.png" width="50%" height="50%"></a></p>

<p dir="auto">The book is available on:</p>
<ul dir="auto">
<li><a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961" rel="nofollow">Amazon</a></li>
<li><a href="https://www.shroffpublishers.com/books/computer-science/large-language-models/9789355425522/" rel="nofollow">Shroff Publishers (India)</a></li>
<li><a href="https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/" rel="nofollow">O'Reilly</a></li>
<li><a href="https://www.amazon.com/Hands-Large-Language-Models-Alammar-ebook/dp/B0DGZ46G88/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=" rel="nofollow">Kindle</a></li>
<li><a href="https://www.barnesandnoble.com/w/hands-on-large-language-models-jay-alammar/1145185960" rel="nofollow">Barnes and Noble</a></li>
<li><a href="https://www.goodreads.com/book/show/210408850-hands-on-large-language-models" rel="nofollow">Goodreads</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<p dir="auto">We advise to run all examples through Google Colab for the easiest setup. Google Colab allows you to use a T4 GPU with 16GB of VRAM for free. All examples were mainly built and tested using Google Colab, so it should be the most stable platform. However, any other cloud provider should work.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Chapter</th>
<th>Notebook</th>
</tr>
</thead>
<tbody>
<tr>
<td>Chapter 1: Introduction to Language Models</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter01/Chapter%201%20-%20Introduction%20to%20Language%20Models.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 2: Tokens and Embeddings</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter02/Chapter%202%20-%20Tokens%20and%20Token%20Embeddings.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 3: Looking Inside Transformer LLMs</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter03/Chapter%203%20-%20Looking%20Inside%20LLMs.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 4: Text Classification</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter04/Chapter%204%20-%20Text%20Classification.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 5: Text Clustering and Topic Modeling</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter05/Chapter%205%20-%20Text%20Clustering%20and%20Topic%20Modeling.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 6: Prompt Engineering</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter06/Chapter%206%20-%20Prompt%20Engineering.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 7: Advanced Text Generation Techniques and Tools</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter07/Chapter%207%20-%20Advanced%20Text%20Generation%20Techniques%20and%20Tools.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 8: Semantic Search and Retrieval-Augmented Generation</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter08/Chapter%208%20-%20Semantic%20Search.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 9: Multimodal Large Language Models</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter09/Chapter%209%20-%20Multimodal%20Large%20Language%20Models.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 10: Creating Text Embedding Models</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter10/Chapter%2010%20-%20Creating%20Text%20Embedding%20Models.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 11: Fine-tuning Representation Models for Classification</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter11/Chapter%2011%20-%20Fine-Tuning%20BERT.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
<tr>
<td>Chapter 12: Fine-tuning Generation Models</td>
<td><a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter12/Chapter%2012%20-%20Fine-tuning%20Generation%20Models.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">You can check the <a href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/.setup">setup</a> folder for a quick-start guide to install all packages locally and you can check the <a href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/.setup/conda">conda</a> folder for a complete guide on how to setup your environment, including conda and PyTorch installation.
Note that the depending on your OS, Python version, and dependencies your results might be slightly differ. However, they
should this be similar to the examples in the book.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reviews</h2><a id="user-content-reviews" aria-label="Permalink: Reviews" href="#reviews"></a></p>
<blockquote>
<p dir="auto">"<em>Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics in their new book. Bolstered with working code, timelines, and references to key papers, their book is a valuable resource for anyone looking to understand the main techniques behind how Large Language Models are built.</em>"</p>
<p dir="auto"><strong>Andrew Ng</strong> - founder of <a href="https://www.deeplearning.ai/" rel="nofollow">DeepLearning.AI</a></p>
</blockquote>
<hr>
<blockquote>
<p dir="auto">"<em>This is an exceptional guide to the world of language models and their practical applications in industry. Its highly-visual coverage of generative, representational, and retrieval applications of language models empowers readers to quickly understand, use, and refine LLMs. Highly recommended!</em>"</p>
<p dir="auto"><strong>Nils Reimers</strong> - Director of Machine Learning at Cohere | creator of <a href="https://github.com/UKPLab/sentence-transformers">sentence-transformers</a></p>
</blockquote>
<hr>
<blockquote>
<p dir="auto">"<em>I can’t think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models.</em>"</p>
<p dir="auto"><strong>Josh Starmer</strong> - <a href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw" rel="nofollow">StatQuest</a></p>
</blockquote>
<hr>
<blockquote>
<p dir="auto">"<em>If you’re looking to get up to speed in everything regarding LLMs, look no further! In this wonderful book, Jay and Maarten will take you from zero to expert in the history and latest advances in large language models. With very intuitive explanations, great real-life examples, clear illustrations, and comprehensive code labs, this book lifts the curtain on the complexities of transformer models, tokenizers, semantic search, RAG, and many other cutting-edge technologies. A must read for anyone interested in the latest AI technology!</em>"</p>
<p dir="auto"><strong>Luis Serrano, PhD</strong> - Founder and CEO of <a href="https://www.youtube.com/@SerranoAcademy" rel="nofollow">Serrano Academy</a></p>
</blockquote>
<hr>
<blockquote>
<p dir="auto">"<em>Hands-On Large Language Models brings clarity and practical examples to cut through the hype of AI. It provides a wealth of great diagrams and visual aids to supplement the clear explanations. The worked examples and code make concrete what other books leave abstract. The book starts with simple introductory beginnings, and steadily builds in scope. By the final chapters, you will be fine-tuning and building your own large language models with confidence.</em>"</p>
<p dir="auto"><strong>Leland McInnes</strong> - Researcher at the Tutte Institute for Mathematics and Computing | creator of <a href="https://github.com/lmcinnes/umap">UMAP</a> and <a href="https://github.com/scikit-learn-contrib/hdbscan">HDBSCAN</a></p>
</blockquote>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional Resources</h2><a id="user-content-additional-resources" aria-label="Permalink: Additional Resources" href="#additional-resources"></a></p>
<p dir="auto">We attempted to put as much information into the book without it being overwhelming. However, even with a 400-page book there is still much to discover!</p>
<p dir="auto">We continue to create more guides that compliment the book and go more in-depth into new and exciting topics:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state" rel="nofollow">A Visual Guide to Mamba</a></th>
<th><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization" rel="nofollow">A Visual Guide to Quantization</a></th>
<th><a href="https://jalammar.github.io/illustrated-stable-diffusion/" rel="nofollow">The Illustrated Stable Diffusion</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/images/mamba.png"><img src="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/raw/main/images/mamba.png" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/images/quant.png"><img src="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/raw/main/images/quant.png" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/images/diffusion.png"><img src="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/raw/main/images/diffusion.png" alt=""></a></td>
</tr>
<tr>
<td><strong><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts" rel="nofollow">A Visual Guide to Mixture of Experts</a></strong></td>
<td><strong><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms" rel="nofollow">A Visual Guide to Reasoning LLMs</a></strong></td>
<td><strong><a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1" rel="nofollow">The Illustrated DeepSeek-R1</a></strong></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/images/moe.png"><img src="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/raw/main/images/moe.png" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/images/reasoning.png"><img src="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/raw/main/images/reasoning.png" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/images/deepseek.png"><img src="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/raw/main/images/deepseek.png" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">For more information on these visual/illustrated guides, check out the <a href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/bonus">bonus</a> folder.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">Please consider citing the book if you consider it useful for your research:</p>
<div data-snippet-clipboard-copy-content="@book{hands-on-llms-book,
  author       = {Jay Alammar and Maarten Grootendorst},
  title        = {Hands-On Large Language Models},
  publisher    = {O'Reilly},
  year         = {2024},
  isbn         = {978-1098150969},
  url          = {https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/},
  github       = {https://github.com/HandsOnLLM/Hands-On-Large-Language-Models}
}"><pre><code>@book{hands-on-llms-book,
  author       = {Jay Alammar and Maarten Grootendorst},
  title        = {Hands-On Large Language Models},
  publisher    = {O'Reilly},
  year         = {2024},
  isbn         = {978-1098150969},
  url          = {https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/},
  github       = {https://github.com/HandsOnLLM/Hands-On-Large-Language-Models}
}
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cozy video games can quell stress and anxiety (487 pts)]]></title>
            <link>https://www.reuters.com/business/retail-consumer/cozy-video-games-can-quell-stress-anxiety-2025-01-27/</link>
            <guid>43733097</guid>
            <pubDate>Sat, 19 Apr 2025 00:21:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/retail-consumer/cozy-video-games-can-quell-stress-anxiety-2025-01-27/">https://www.reuters.com/business/retail-consumer/cozy-video-games-can-quell-stress-anxiety-2025-01-27/</a>, See on <a href="https://news.ycombinator.com/item?id=43733097">Hacker News</a></p>
Couldn't get https://www.reuters.com/business/retail-consumer/cozy-video-games-can-quell-stress-anxiety-2025-01-27/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Hypertext TV (198 pts)]]></title>
            <link>https://hypertext.tv/</link>
            <guid>43732805</guid>
            <pubDate>Fri, 18 Apr 2025 23:29:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hypertext.tv/">https://hypertext.tv/</a>, See on <a href="https://news.ycombinator.com/item?id=43732805">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="menu" aria-label="Menu Options" data-astro-cid-2j44jlrm="" id="menu" data-open="false">  <hr data-astro-cid-2j44jlrm=""> <p> <label role="menuitem" tabindex="-1" data-astro-cid-2j44jlrm=""> <span data-astro-cid-2j44jlrm="">Brightness</span>  </label> <label role="menuitem" tabindex="-1" data-astro-cid-2j44jlrm=""> <span data-astro-cid-2j44jlrm="">Color</span>  </label> <label role="menuitem" tabindex="-1" data-astro-cid-2j44jlrm=""> <span data-astro-cid-2j44jlrm="">Tint</span>  </label> <label role="menuitem" tabindex="-1" data-astro-cid-2j44jlrm=""> <span data-astro-cid-2j44jlrm="">Horizontal</span>  </label> <label role="menuitem" tabindex="-1" data-astro-cid-2j44jlrm=""> <span data-astro-cid-2j44jlrm="">Vertical</span>  </label></p><hr data-astro-cid-2j44jlrm=""> <p><a href="https://hypertext.tv/" role="menuitem" tabindex="-1" onclick="fathom.trackEvent('menu: click tv guide')" data-astro-cid-2j44jlrm="">TV Guide</a> <a href="https://hypertext.tv/test" role="menuitem" tabindex="-1" onclick="fathom.trackEvent('menu: click testing')" data-astro-cid-2j44jlrm="">Testing</a> <a href="https://hypertext.tv/credits" role="menuitem" tabindex="-1" onclick="fathom.trackEvent('menu: click credits')" data-astro-cid-2j44jlrm="">Credits</a></p><hr data-astro-cid-2j44jlrm=""> <p><a href="https://github.com/evadecker/hypertext.tv" role="menuitem" tabindex="-1" onclick="fathom.trackEvent('menu: click add website')" data-astro-cid-2j44jlrm="">Add a website</a> <a href="https://github.com/evadecker/hypertext.tv/issues" role="menuitem" tabindex="-1" onclick="fathom.trackEvent('menu: click report issue')" data-astro-cid-2j44jlrm="">Report an issue</a></p><hr data-astro-cid-2j44jlrm=""> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI's new reasoning AI models hallucinate more (114 pts)]]></title>
            <link>https://techcrunch.com/2025/04/18/openais-new-reasoning-ai-models-hallucinate-more/</link>
            <guid>43732506</guid>
            <pubDate>Fri, 18 Apr 2025 22:43:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/04/18/openais-new-reasoning-ai-models-hallucinate-more/">https://techcrunch.com/2025/04/18/openais-new-reasoning-ai-models-hallucinate-more/</a>, See on <a href="https://news.ycombinator.com/item?id=43732506">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">OpenAI’s <a href="https://techcrunch.com/2025/04/16/openai-launches-a-pair-of-ai-reasoning-models-o3-and-o4-mini/">recently launched o3 and o4-mini AI models</a> are state-of-the-art in many respects. However, the new models still hallucinate, or make things up — in fact, they hallucinate <em>more</em> than several of OpenAI’s older models.</p>

<p>Hallucinations have proven to be one of the biggest and most difficult problems to solve in AI, impacting <a href="https://techcrunch.com/2024/08/14/study-suggests-that-even-the-best-ai-models-hallucinate-a-bunch/">even today’s best-performing systems</a>. Historically, each new model has improved slightly in the hallucination department, hallucinating less than its predecessor. But that doesn’t seem to be the case for o3 and o4-mini.</p>







<p>According to OpenAI’s internal tests, o3 and o4-mini, which are so-called reasoning models, hallucinate <em>more often</em> than the company’s previous reasoning models — o1, o1-mini, and o3-mini — as well as OpenAI’s traditional, “non-reasoning” models, such as GPT-4o.</p>

<p>Perhaps more concerning, the ChatGPT maker doesn’t really know why it’s happening. </p>

<p>In its technical report for <a href="https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf" target="_blank" rel="noreferrer noopener nofollow">o3 and o4-mini</a>, OpenAI writes that “more research is needed” to understand why hallucinations are getting worse as it scales up reasoning models. O3 and o4-mini perform better in some areas, including tasks related to coding and math. But because they “make more claims overall,” they’re often led to make “more accurate claims as well as more inaccurate/hallucinated claims,” per the report.</p>

<p>OpenAI found that o3 hallucinated in response to 33% of questions on PersonQA, the company’s in-house benchmark for measuring the accuracy of a model’s knowledge about people. That’s roughly double the hallucination rate of OpenAI’s previous reasoning models, o1 and o3-mini, which scored 16% and 14.8%, respectively. O4-mini did even worse on PersonQA — hallucinating 48% of the time.</p>

<p>Third-party <a href="https://transluce.org/investigating-o3-truthfulness" target="_blank" rel="noreferrer noopener nofollow">testing</a> by Transluce, a nonprofit AI research lab, also found evidence that o3 has a tendency to make up actions it took in the process of arriving at answers. In one example, Transluce observed o3 claiming that it ran code on a 2021 MacBook Pro “outside of ChatGPT,” then copied the numbers into its answer. While o3 has access to some tools, it can’t do that.</p>


<p>“Our hypothesis is that the kind of reinforcement learning used for o-series models may amplify issues that are usually mitigated (but not fully erased) by standard post-training pipelines,” said Neil Chowdhury, a Transluce researcher and former OpenAI employee, in an email to TechCrunch.</p>

<p>Sarah Schwettmann, co-founder of Transluce, added that o3’s hallucination rate may make it less useful than it otherwise would be.</p>

<p>Kian Katanforoosh, a Stanford adjunct professor and CEO of the upskilling startup Workera, told TechCrunch that his team is already testing o3 in their coding workflows, and that they’ve found it to be a step above the competition. However, Katanforoosh says that o3 tends to hallucinate broken website links. The model will supply a link that, when clicked, doesn’t work.</p>







<p>Hallucinations may help models arrive at interesting ideas and be creative in their “thinking,” but they also make some models a tough sell for businesses in markets where accuracy is paramount. For example, a law firm likely wouldn’t be pleased with a model that inserts lots of factual errors into client contracts.</p>

<p>One promising approach to boosting the accuracy of models is giving them web search capabilities. OpenAI’s GPT-4o with web search achieves&nbsp;<a href="https://openai.com/index/new-tools-for-building-agents/" target="_blank" rel="noreferrer noopener nofollow">90% accuracy</a>&nbsp;on SimpleQA, another one of OpenAI’s accuracy benchmarks. Potentially, search could improve reasoning models’&nbsp;hallucination rates, as well — at least in cases where users are willing to expose prompts to a third-party search provider.</p>

<p>If scaling up reasoning models indeed continues to worsen hallucinations, it’ll make the hunt for a solution all the more urgent.</p>

<p>“Addressing hallucinations across all our models is an ongoing area of research, and we’re continually working to improve their accuracy and reliability,” said OpenAI spokesperson Niko Felix in an email to TechCrunch.</p>

<p>In the last year, the broader AI industry has pivoted to focus on reasoning models after <a href="https://techcrunch.com/2024/11/20/ai-scaling-laws-are-showing-diminishing-returns-forcing-ai-labs-to-change-course/">techniques to improve traditional AI models started showing diminishing returns</a>. Reasoning improves model performance on a variety of tasks without requiring massive amounts of computing and data during training. Yet it seems reasoning also may lead to more hallucinating — presenting a challenge.</p>
</div><div>
	
	
	
	

	
<div>
	<p>
		Maxwell Zeff is a senior reporter at TechCrunch specializing in AI and emerging technologies. Previously with Gizmodo, Bloomberg, and MSNBC, Zeff has covered the rise of AI and the Silicon Valley Bank crisis. He is based in San Francisco. When not reporting, he can be found hiking, biking, and exploring the Bay Area’s food scene.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/maxwell-zeff/" data-event="button" href="https://techcrunch.com/author/maxwell-zeff/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I passionately hate hype, especially the AI hype (177 pts)]]></title>
            <link>https://unixdigest.com/articles/i-passionately-hate-hype-especially-the-ai-hype.html</link>
            <guid>43732047</guid>
            <pubDate>Fri, 18 Apr 2025 21:32:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unixdigest.com/articles/i-passionately-hate-hype-especially-the-ai-hype.html">https://unixdigest.com/articles/i-passionately-hate-hype-especially-the-ai-hype.html</a>, See on <a href="https://news.ycombinator.com/item?id=43732047">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>


<p>Published on <span id="pubdate">2024-08-21</span>. Modified on <span id="moddate">2024-08-23</span>.</p>
<p><span id="description">I truly and passionately hate hype. From the fakeness of it to the sheer stupidity it represents, but perhaps most of all, because of the devastating consequence it often results in.</span></p>

<p>Hype is basically to promote or publicize something extravagantly and the modern hype promoted by "Big Tech" is not much different from medieval <a href="https://en.wikipedia.org/wiki/Charlatan">charlatans</a>.</p>

<p>Hype begins by some company or industry wanting to earn a profit by some new product or service. Massive propaganda is launched which is then propagated by the media, who's objectivity and investigation procedure is totally sidestepped by yet another desperate craving for profit. Then all of this is followed-up by the majority of the masses, which unfortunately mostly consists of mindless sheeple that easily become dumpfounded by all of this, and as a result, start parroting the hype like lunatics.</p>

<p>Hype is always bad.</p>

<p>Hype hurts investors who end up loosing a lot of money because all the "golden" promises where exaggerated or simply fake.</p>

<p>Hype often hurts the medium to smaller businesses because rather than doing what actually works, procedures are changed and complication is introduced, very often resulting in vendor lock-in, higher financial expenses, less security and poorer end results.</p>

<p>Hype also hurts the individual. People who invest time and money in studying and learning the hyped up technology only to realize, often many years later, that better ways actually exist. This is when the old becomes new again and people discover things for the first time that has been obvious to the previous generations all along.</p>

<p>Often, the price that people end up paying is not only in the loss of time and money, but also in some of the most important valuables, namely privacy and personal freedom.</p>

<p>When hype is driven by big corporations or by governments, it is always bad. The reason is simple. The interest at hand is never the benefit of the people, it's the benefit of the company or ruling elite. It is always an exaggeration or a blatant lie. That is one of the reasons why we must always strive to think independently and avoid being swooped by the media and general opinions of the crowd.</p>

<blockquote>
  <p>In the world of tech, hype is rife. When a new technology emerges, people get overly enthusiastic and want to apply it everywhere—we've seen it with big data, AI, data science, blockchain, ChatGPT, and so on. I've seen companies hire as many as 70 people to build a product with the goal of following a trend without defining what the product would do or whether clients would want it.</p>
  <p>Just a few weeks ago, for example, a company reached out to me looking for advice on how they could use ChatGPT in their accounting software. They said this was because they were trying to raise funding, and investors wouldn't like it if they weren't using ChatGPT for something. I've documented many examples of this phenomenon in the context of AI in my book.</p>
  <p>— <a href="https://emaggiori.com/">Dr. Emmanuel Maggiori</a> (computer scientist)</p>
</blockquote>

<h2>The current AI hype</h2>

<p>In technology, AI is currently the new big hype. Before AI, it was "The Cloud", which unfortunately has still not settled, but are now also being interwoven with AI.</p>

<p>The term "Artificial Intelligence" (AI) is grossly misleading as no form or sort of intelligence exists at all. The only reason why this terminology is being used is because it is easier to sell. When most people think of AI they tend to think of something from science fiction.</p>

<p>In my humble opinion, about perhaps 10% of the AI hype is based upon useful facts, which is the relevant tools the technology provides, the rest is exaggerated rubbish.</p>

<p>Still, many companies (mainly in the US, but also elsewhere in the world) are firing people because they have been lead to believe that they can save money by utilizing AI rather than real people. This is a big mistake. While "AI" is certainly useful for a lot of minor tasks when used as tools, this is nothing but careless acts of greedy mismanagement.</p>

<p>AI functions greatly as a "search engine" replacement, but replacing customer and service people with AI is going to hurt the business in the long run. Nobody wants to talk to an AI when they need support. We all HATE that! It is bad enough that when you need service and support you end up talking to someone on the other side of the planet who's using some kind of answer sheet with absolutely no clue on how to really help you.</p>

<p>The least skilled, the least experienced, the least productive people will be the ones recommending AI the most. The people who actually think will be treated as either being stupid or totally backwards.</p>

<p>Last, but not least, a lot of the current AI products represents the biggest and most appalling examples of plagiarizing ever witnessed.</p>

<blockquote>
  <p>The marketing team in the company I work for is now labeling everything a computer does as "AI powered". Technology that existed decades ago is now suddenly "AI powered" - just because they are jumping on the hype wagon.</p>
</blockquote>

<p>It is a real shame that some of the most beneficial tools ever invented, such as computers, modern databases, data centers, etc. exist in an industry that has become so obsessed with hype and trends that it resembles the fashion industry.</p>

<p>Another major problem with this new round of hype is that it is taking place in the middle of a major inflation crisis and it is taking place in a time when we need to conserve resources, energy and water. Yet, if anything, AI is slowly becoming a major drain on both resources, energy and water (for cooling in data centers).</p>

<p>Take a step back, pause and breathe before you fall prey to all the hype. A lot of people is going to lose a lot of money. I can say that with complete conviction because most investments into AI have been made on false promises, promises that AI simply cannot and never will be able to fulfill.</p>

<h2>Recommended material</h2>

<ul>
  <li>
    <p><a href="https://www.youtube.com/watch?v=Nd7wrC62LEk&amp;t=602s">AI HYPE - Explained by Dr. Emmanuel Maggiori</a> (1 hour 20 min. YouTube video)</p>
    <p>Jesse Wright is joined by Dr. Emmanuel Maggiori, computer scientist and author of the book <a href="https://emaggiori.com/smart-until-dumb/">Smart Until It's Dumb: Why artificial intelligence keeps making epic mistakes (and why the AI bubble will burst)</a>, in a thought-provoking conversation about AI and the future of AI.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=VPSZFUiElls">The "Modern Day Slaves" Of The AI Tech World</a> (52 min. YouTube video).</p>
    <p>Meet the invisible workforce behind tech giants like Google, Facebook, Amazon, and Uber. These underpaid and disposable workers label images, moderate content, and train AI systems, often earning less than minimum wage. Their work is essential yet remains in the shadows, unacknowledged by the companies that depend on them.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=gSPjoNczOsI">Decoding AI: A Go Programmer's Perspective</a> (30 min. YouTube video).</p><p>This is a really good talk by Beth Anderson from BBC. It was presented at the newly held UK GopherCon.</p>
    <p>Beth debunks common myths and provides a candid look under the hood to reveal what the technologies are and how they work when we attempt to use them in production systems.</p>
    <p>Beth began her journey in the field of AI in the 90s when she studied Computer Science and Artificial Intelligence, culminating in a Master's thesis focused on machine learning—utilising convolutional neural networks to classify audio waveforms. Beth later joined the BBC's pioneering AI team, Datalab, developing recommendation engines and other tooling within the BBC. Beth is also involved with the BBC's AI&amp;ML community which is focused on the responsible use of AI within the organisation.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=Pv0cfsastFs">Beyond the Hype: A Realistic Look at Large Language Models</a> (40 min. YouTube video).</p>
    <p>This presentation by Jodie Burchell was recorded at GOTO Amsterdam 2024.</p>
    <p>Jodie Burchell is a data scientist and developer at JetBrains.</p>
    <p><i>If you've been remotely tuned in to the latest developments in large language models (LLMs), you've likely been inundated with news, ranging from claims that these models will replace numerous white-collar jobs to declarations of sentience and an impending AI apocalypse. At this stage, the hype surrounding these models has far surpassed the actual useful information available.</i></p>
    <p><i>In this talk, we'll cut through the noise and delve deep into the current applications, risks, and limitations of LLMs. We'll start with early research endeavours aimed at creating an "artificial brain" and trace the path that has led us to today's sophisticated text models. Along the way, we'll address how these models have been mistaken for intelligent systems.</i></p>
    <p><i>We'll shed light on the actual requirements for developing true artificial general intelligence, and see how far LLMs are from this goal. We'll end with a practical demonstration of how you can use LLMs in a way that plays to their strengths, by showing you how to build a system which leans into these models powerful natural language capabilities.</i></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=dDUC-LqVrPU">Has Generative AI Already Peaked? - Computerphile</a> (13 min. YouTube video).</p>
    <p>The paper mentioned in the video is this one: <a href="https://arxiv.org/abs/2404.04125">No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</a> (PDF 47 MB).</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=20TAkcy3aBY&amp;t=223s">Jon Stewart On The False Promises of AI | The Daily Show</a> (11 min. YouTube video).</p>
  </li>
  <li>
    <p><a href="https://emaggiori.com/employed-in-tech-for-years-but-almost-never-worked/">I've been employed in tech for years, but I've almost never worked</a> by Dr. Emmanuel Maggiori.</p>
  </li>
</ul>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Full Text Search of US Court records (351 pts)]]></title>
            <link>https://www.judyrecords.com/</link>
            <guid>43731552</guid>
            <pubDate>Fri, 18 Apr 2025 20:24:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.judyrecords.com/">https://www.judyrecords.com/</a>, See on <a href="https://news.ycombinator.com/item?id=43731552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                        <p><a href="https://www.judyrecords.com/">judyrecords</a></p><form action="/" method="get">

<div>

        <p>
    

    <a href="https://www.judyrecords.com/info#searchTipsHeader">search tips</a>
</p></div>

</form>



            <p><span>740 million<span>+</span></span>
                <br><span>United States Court Cases</span>
            </p>

            <ul>

                <li><a href="https://www.judyrecords.com/">home</a></li>
                <li><a href="https://www.judyrecords.com/terms">terms</a></li>
                <li><a href="https://www.judyrecords.com/info">info</a></li>
                
                <li><a href="https://www.judyrecords.com/api">API</a></li>            </ul>

                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UML diagram for the DDD example in Evans' book (111 pts)]]></title>
            <link>https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book</link>
            <guid>43731250</guid>
            <pubDate>Fri, 18 Apr 2025 19:40:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book">https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book</a>, See on <a href="https://news.ycombinator.com/item?id=43731250">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/blob/main/top_image.png"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/top_image.png" title="Top image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">UML Diagram for the DDD Example in Evans' Book</h2><a id="user-content-uml-diagram-for-the-ddd-example-in-evans-book" aria-label="Permalink: UML Diagram for the DDD Example in Evans' Book" href="#uml-diagram-for-the-ddd-example-in-evans-book"></a></p>
<p dir="auto">This project uses UML diagrams to illustrate the structure and behavior of the DDD example—a cargo shipping system—from Eric Evans' book (<em>Domain-Driven Design: Tackling Complexity in the Heart of Software</em>). These diagrams are created based on the source code of the <a href="https://github.com/citerus/dddsample-core">dddsample-core</a> project on GitHub. The diagrams aim to help us understand how the example implements the strategic and tactical designs of DDD and the mechanisms by which it operates.</p>
<p dir="auto">If you want to get into DDD theory, check out Evans' book. If you want to get into the implementation details, check out the source code of the dddsample-core project. Also, if you want to understand the UML model behind these diagrams, open the model file (ddd-example-in-evans-book.asta) using the modeling tool <a href="https://astah.net/download" rel="nofollow">Astah Professional/UML/Viewer</a>.</p>
<p dir="auto">Clicking on the UML diagram image below opens it in Diagram Map <a href="#footnote1">*1</a>.</p>
<p dir="auto"><sub><a id="user-content-footnote1">*1</a>: Diagram Map allows you to zoom and pan, like Google Maps, when viewing a UML diagram. You can view it with any browser. To create one on your own, <a href="https://astah.net/download" rel="nofollow">Astah Professional/UML</a> and the <a href="https://sites.google.com/view/m-plus-plugin/download" rel="nofollow">m+ plug-in</a> are required.</sub></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">User-system interaction</h2><a id="user-content-user-system-interaction" aria-label="Permalink: User-system interaction" href="#user-system-interaction"></a></p>
<p dir="auto">This sequence diagram illustrates the key interactions between the user and the system, with references to other diagrams. The interactions show two use cases: (1) tracking the handling of specified cargo, and (2) booking new cargo followed by assigning it a delivery route.</p>
<p dir="auto"><a href="https://takaakit.github.io/uml-diagram-for-ddd-example-in-evans-book/uml_diagram/User-system%20interaction/diagram_map.html?highlight=0" title="User-system interaction" rel="nofollow"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/uml_diagram/User-system%20interaction/diagram_map.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Domain model overview</h2><a id="user-content-domain-model-overview" aria-label="Permalink: Domain model overview" href="#domain-model-overview"></a></p>
<p dir="auto">This class diagram illustrates the basic elements of the domain model and their relationships. The layout of the domain model elements is based on the diagram presented in the dddsample-core project. Attributes and operations are hidden to focus on understanding the elements and their relationships. If you want to see them, open the model file (ddd-example-in-evans-book.asta) using the modeling tool Astah.</p>
<p dir="auto"><a href="https://takaakit.github.io/uml-diagram-for-ddd-example-in-evans-book/uml_diagram/Domain%20model%20overview/diagram_map.html?highlight=0" title="Domain model overview" rel="nofollow"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/uml_diagram/Domain%20model%20overview/diagram_map.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Domain objects for "ABC123" cargo</h2><a id="user-content-domain-objects-for-abc123-cargo" aria-label="Permalink: Domain objects for &quot;ABC123&quot; cargo" href="#domain-objects-for-abc123-cargo"></a></p>
<p dir="auto">This object diagram shows the basic domain objects and thier connections. In particular, it shows the object snapshot associated with the cargo object "ABC123," which is preset as sample data. The cargo "ABC123," which is en route from HongKong to Helsinki, is currently at the port of New York, where unloading has been completed.</p>
<p dir="auto"><a href="https://takaakit.github.io/uml-diagram-for-ddd-example-in-evans-book/uml_diagram/Domain%20objects%20for%20ABC123%20cargo/diagram_map.html?highlight=0" title="Domain objects for ABC123 cargo" rel="nofollow"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/uml_diagram/Domain%20objects%20for%20ABC123%20cargo/diagram_map.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overall structure</h2><a id="user-content-overall-structure" aria-label="Permalink: Overall structure" href="#overall-structure"></a></p>
<p dir="auto">This class diagram illustrates the overall structure of the cargo shipping system: it is composed of two contexts, the Booking context and the Transport network context. The system is implemented using Spring projects such as Spring Boot / MVC / Data JPA. Note that this diagram focuses on elements and relationships that may be considered and does not reflect the entire implementation. Attributes and operations are also hidden to focus on understanding the elements and their relationships. If you want to see them, open the model file (ddd-example-in-evans-book.asta) using the modeling tool Astah.</p>
<p dir="auto"><a href="https://takaakit.github.io/uml-diagram-for-ddd-example-in-evans-book/uml_diagram/Overall%20structure/diagram_map.html?highlight=0" title="Overall structure" rel="nofollow"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/uml_diagram/Overall%20structure/diagram_map.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Behavior 1: Initialization</h2><a id="user-content-behavior-1-initialization" aria-label="Permalink: Behavior 1: Initialization" href="#behavior-1-initialization"></a></p>
<p dir="auto">This communication diagram illustrates a scenario in which the user launches the cargo shipping system, with a focus on messages that may be worth considering.</p>
<p dir="auto"><a href="https://takaakit.github.io/uml-diagram-for-ddd-example-in-evans-book/uml_diagram/Behavior%201%20Initialization/diagram_map.html?highlight=0" title="Behavior 1: Initialization" rel="nofollow"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/uml_diagram/Behavior%201%20Initialization/diagram_map.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Behavior 2: Cargo tracking</h2><a id="user-content-behavior-2-cargo-tracking" aria-label="Permalink: Behavior 2: Cargo tracking" href="#behavior-2-cargo-tracking"></a></p>
<p dir="auto">This communication diagram shows a scenario in which the user requests tracking for cargo "ABC123" on the cargo tracking page. The cargo "ABC123," which is preset by the system and en route from HongKong to Helsinki, is currently at the port of New York, where unloading has been completed.</p>
<p dir="auto"><a href="https://takaakit.github.io/uml-diagram-for-ddd-example-in-evans-book/uml_diagram/Behavior%202%20Cargo%20tracking/diagram_map.html?highlight=0" title="Behavior 2: Cargo tracking" rel="nofollow"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/uml_diagram/Behavior%202%20Cargo%20tracking/diagram_map.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Behavior 3-1: Cargo booking</h2><a id="user-content-behavior-3-1-cargo-booking" aria-label="Permalink: Behavior 3-1: Cargo booking" href="#behavior-3-1-cargo-booking"></a></p>
<p dir="auto">This communication diagram shows a scenario in which the user registers new cargo on the cargo booking page. The new cargo is set to be shipped from New York to Helsinki.</p>
<p dir="auto"><a href="https://takaakit.github.io/uml-diagram-for-ddd-example-in-evans-book/uml_diagram/Behavior%203-1%20Cargo%20booking/diagram_map.html?highlight=0" title="Behavior 3-1: Cargo booking" rel="nofollow"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/uml_diagram/Behavior%203-1%20Cargo%20booking/diagram_map.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Behavior 3-2: Route suggestion</h2><a id="user-content-behavior-3-2-route-suggestion" aria-label="Permalink: Behavior 3-2: Route suggestion" href="#behavior-3-2-route-suggestion"></a></p>
<p dir="auto">This communication diagram illustrates a scenario in which the user requests cargo routing on the cargo booking result page, followed by the presentation of route candidates.</p>
<p dir="auto"><a href="https://takaakit.github.io/uml-diagram-for-ddd-example-in-evans-book/uml_diagram/Behavior%203-2%20Route%20suggestion/diagram_map.html?highlight=0" title="Behavior 3-2: Route suggestion" rel="nofollow"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/uml_diagram/Behavior%203-2%20Route%20suggestion/diagram_map.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Behavior 3-3: Route assignment</h2><a id="user-content-behavior-3-3-route-assignment" aria-label="Permalink: Behavior 3-3: Route assignment" href="#behavior-3-3-route-assignment"></a></p>
<p dir="auto">This communication diagram illustrates a scenario in which the user requests to assign new cargo to a delivery route on the route selection page.</p>
<p dir="auto"><a href="https://takaakit.github.io/uml-diagram-for-ddd-example-in-evans-book/uml_diagram/Behavior%203-3%20Route%20assignment/diagram_map.html?highlight=0" title="Behavior 3-3: Route assignment" rel="nofollow"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/uml_diagram/Behavior%203-3%20Route%20assignment/diagram_map.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extra: Directed graph</h2><a id="user-content-extra-directed-graph" aria-label="Permalink: Extra: Directed graph" href="#extra-directed-graph"></a></p>
<p dir="auto">This is a directed graph <a href="#footnote2">*2</a> that represents the structural elements and their relationships, associated with the Booking context and the Transport network context. This graph allows you to view the element relationships in various layouts. Note that the node colors in this graph are automatically assigned on a per‑folder basis and do not correspond to the element colors in the UML diagram shown above.</p>
<p dir="auto"><a href="https://takaakit.github.io/uml-diagram-for-ddd-example-in-evans-book/directed_graph/directed_graph.html" title="Directed graph of structural elements and their relationships" rel="nofollow"><img src="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/raw/main/directed_graph/directed_graph.jpg"></a></p>
<p dir="auto"><sub><a id="user-content-footnote2">*2</a>: To create this directed graph on your own, <a href="https://astah.net/download" rel="nofollow">Astah Professional/UML</a> and the <a href="https://sites.google.com/view/m-plus-plugin/download" rel="nofollow">m+ plug-in</a> are required.</sub></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">References</h2><a id="user-content-references" aria-label="Permalink: References" href="#references"></a></p>
<ul dir="auto">
<li>Evans, Eric. Domain-Driven Design: Tackling Complexity in the Heart of Software, Addison-Wesley, 2004.</li>
<li>Evans, Eric. Domain-Driven design Reference: Definitions and pattern summaries. Dog Ear Publishing, 2014.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Links</h2><a id="user-content-links" aria-label="Permalink: Links" href="#links"></a></p>
<ul dir="auto">
<li><a href="https://github.com/citerus/dddsample-core">citerus/dddsample-core</a> @GitHub</li>
<li><a href="https://astah.net/download" rel="nofollow">Astah Professional/UML/Viewer</a> download page</li>
<li><a href="https://sites.google.com/view/m-plus-plugin/download" rel="nofollow">m+ plug-in</a> download page</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Diagram map (diagram_map.html) and Directed graph (directed_graph.html) include the following libraries:</p>
<ul dir="auto">
<li><a href="https://d3js.org/" rel="nofollow">D3.js</a> is copyrighted by Mike Bostock and is released under the <a href="https://opensource.org/licenses/BSD-3-Clause" rel="nofollow">BSD license</a>.</li>
<li><a href="https://popper.js.org/" rel="nofollow">Popper.js</a> is copyrighted by Federico Zivolo and is released under the <a href="https://opensource.org/licenses/MIT" rel="nofollow">MIT license</a>.</li>
<li><a href="https://atomiks.github.io/tippyjs" rel="nofollow">Tippy.js</a> is copyrighted by atomiks and is released under the <a href="https://opensource.org/licenses/MIT" rel="nofollow">MIT license</a>.</li>
<li><a href="https://gka.github.io/chroma.js" rel="nofollow">Chroma.js</a> is copyrighted by Gregor Aisch and is released under the <a href="https://opensource.org/licenses/BSD-3-Clause" rel="nofollow">BSD license</a>.</li>
<li><a href="https://threejs.org/" rel="nofollow">Three.js</a> is copyrighted by three.js authors and is released under the <a href="https://opensource.org/licenses/MIT" rel="nofollow">MIT license</a>.</li>
<li><a href="https://www.vantajs.com/" rel="nofollow">Vanta.js</a> is copyrighted by Teng Bao and is released under the <a href="https://opensource.org/licenses/MIT" rel="nofollow">MIT license</a>.</li>
</ul>
<p dir="auto">Files and data in this project other than the above libraries are under the <a href="https://creativecommons.org/publicdomain/zero/1.0/" rel="nofollow">Creative Commons Zero (CC0) license</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Found a mistake?</h2><a id="user-content-found-a-mistake" aria-label="Permalink: Found a mistake?" href="#found-a-mistake"></a></p>
<p dir="auto">If you spot any mistakes in the diagrams or model, please open an <a href="https://github.com/takaakit/uml-diagram-for-ddd-example-in-evans-book/issues">issue</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[15,000 lines of verified cryptography now in Python (405 pts)]]></title>
            <link>https://jonathan.protzenko.fr/2025/04/18/python.html</link>
            <guid>43731165</guid>
            <pubDate>Fri, 18 Apr 2025 19:28:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jonathan.protzenko.fr/2025/04/18/python.html">https://jonathan.protzenko.fr/2025/04/18/python.html</a>, See on <a href="https://news.ycombinator.com/item?id=43731165">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>In November 2022, I opened <a href="https://github.com/python/cpython/issues/99108">issue 99108</a> on
Python’s GitHub repository, arguing that after a recent CVE in its implementation of
SHA3, Python should embrace verified code for all of its hash-related infrastructure.</p>

<p>As of last week, this issue is now closed, and every single hash and HMAC algorithm exposed by
default in Python is now provided by <a href="https://github.com/project-everest/hacl-star/">HACL*</a>, the
verified cryptographic library. There was no loss of functionality, and the transition was entirely
transparent for Python users. Python now vendors (includes in its repository) 15,000 lines of
verified C code from HACL*. Pulling newer versions from the upstream HACL* repository
is entirely automated and is done by invoking a script. HACL* was able to successfully implement
new features to meet all of the requirements of Python, such as: additional modes for the Blake2
family of algorithms, a new API for SHA3 that covers all Keccak variants, strict abstraction
patterns to deal with build system difficulties, proper error management (notably, allocation
failures), and instantiating HACL’s generic “streaming” functionality with the HMAC algorithm,
including an optimization that requires keeping two hash states at once.</p>

<p>This is the culmination of 2.5 years of work, and could not have happened without the invaluable
help of <a href="https://aymericfromherz.github.io/">Aymeric Fromherz</a>, who shouldered a lot of the
implementation burden. <a href="https://www.sonho.fr/">Son Ho</a> had a key contribution early on, generalizing
HACL’s “streaming” functionality to be able to handle block algorithms that require a “pre-input”.
This slightly obscure generalization was actually essential to implement a suitable, optimized HMAC
that keeps two hash states under the hood. On the Python side, Gregory P. Smith, Bénédikt Tran, and
later Chris Eibl were big champions and provided a lot of help. Finally, the HACS series of
workshops created connections (hello, Paul Kehrer!) and provided sufficient momentum to make this
happen. A warm thank you to both the Python and verified cryptographic communities!</p>

<p>As I oftentimes like to do, I’ll provide a little bit of a look behind the scenes, and comment on
some of the more interesting technical aspects that are too low-level for a research paper.</p>

<h2 id="a-primer-on-streaming-apis">A Primer on Streaming APIs</h2>

<p>Many cryptographic algorithms are <em>block</em> algorithms, meaning that they assume their input is
provided block by block, with special treatment for the first and last blocks. Well-known block
algorithms include hash algorithms, MAC algorithms (e.g. Poly1305, HMAC), and more. In practice, the
block API is not user-friendly: rarely does one have the data already chunked in blocks;
furthermore, computing a result (e.g., a hash) invalidates the state of the block algorithm, which
makes e.g. computing the intermediary hashes of the TLS transcript difficult.</p>

<p>For these reasons, cryptographic libraries typically expose <em>streaming</em> APIs, meaning clients can
provide inputs of any length; the library then takes care of buffering the input, flushing the
buffer as soon as a full block is obtained. A streaming API typically also allows extracting
intermediary hashes without invalidating the state.</p>

<p>Streaming APIs are hard, because they manipulate long-lived state with complex invariants: the
(unverified) reference implementation of SHA3 was hit with a <a href="https://nvd.nist.gov/vuln/detail/cve-2022-37454">bad
CVE</a> in 2022, which Python “inherited”, because it
vendored that very same SHA3 implementation.</p>

<p>Streaming APIs are hard also because the
underlying block algorithms differ in a myriad of different ways: all hash algorithms accept an
empty final block, <em>except</em> for Blake2; some need to retain the key at run-time (Poly1305), some can discard it
after initialization (HMAC, optimized); some need some initial input before processing the key
(Blake2), some don’t; and so on.</p>

<p>Given this inherent complexity, streaming algorithms are a good candidate for verification: we wrote
a <a href="https://arxiv.org/abs/2102.01644">research paper</a> in 2021 about this very problem.</p>

<h2 id="fully-generic-verification">Fully generic verification</h2>

<p>The main idea from the paper is that one can capture, using dependent types, what a block algorithm
<em>is</em>. Once that’s done, it suffices to author and verify a generic streaming construction once and
for all over an abstract definition of block algorithms. Then, just like you instantiate a template
in C++, you apply the generic streaming construction to a concrete block API and voilà – a
streaming API for that one block algorithm, “for free”.</p>

<p>The fist hitch is that there’s quite a big delta between what we presented in the paper (Listing
12), and <a href="https://github.com/hacl-star/hacl-star/blob/897e23d315c08f7a375408d60d1a4918477fcaa0/code/streaming/Hacl.Streaming.Interface.fsti#L254">what actually lives in the
repository</a>.
Specifically, capturing <em>any</em> block algorithm requires a lot of genericity.</p>
<ul>
  <li>The user may or may not specify the length of the final digest – for instance,
each SHA3 hash has a fixed output length, but the Shake variants produce a result whose length is
user-provided.</li>
  <li>The block algorithm may expect to receive a pre-input before the blocks of data. For e.g. SHA2,
the pre-input is empty, but for Blake2 in keyed hash mode, the pre-input is the key block.</li>
  <li>Blocks cannot be processed eagerly, because some algorithms (Blake2) do not allow for the final
block to be empty – this vastly complicates buffer management, and interacts with the pre-input.</li>
  <li>Some algorithms need to retain extra pieces of state: for instance, the key length for Blake2 can
be tweaked at initialization-time, but needs to be retained in the long-lived state.</li>
  <li>To avoid invalidating the block state upon intermediary digest extraction, our streaming API
copies the state under the hood – in some cases, it’s easier to stack-allocate this copy, but in
other cases, a heap-allocated copy makes more sense.</li>
  <li>In some cases, we wanted one API per algorithm (we have four APIs for
SHA2-{224,256,384,512}); in other cases, we wanted one API per algorithm family (we have one API
that covers all 6 Keccak algorithms: 4 variants of SHA3, and 2 variants of Shake).</li>
</ul>

<p>Getting to that level of genericity took multiple rounds, driven by the successive requirements of
Python. Ultimately, for HMAC, which was the final algorithm we landed in Python, we realized that
our proofs and definitions were generic enough that we did not need any further tweaks to
“instantiate” our generic streaming API with HMAC.</p>

<h2 id="a-bulletproof-build">A bulletproof build</h2>

<p>One highlight of submitting a PR to Python is that their infrastructure has more CI coverage than we
could possibly dream of: a complete build of Python runs over 50+ toolchains and architectures. The
flipside? We discovered some pretty annoying corner cases.</p>

<p>One particularly tricky build issue surfaced when dealing with HMAC. As a reminder,
<a href="https://en.wikipedia.org/wiki/HMAC">HMAC</a> is a generic construction that, given a hash algorithm,
provides a keyed message authentication code – in short, there is a high-level HMAC piece of code
that defers most of the work to individual hash algorithms. Each hash algorithm may itself come in a
variety of <em>implementations</em>: for instance, HMAC-Blake2b is implemented both by HMAC-Blake2b-32
(regular implementation) and HMAC-Blake2b-256 (AVX2 wide-vector implementation).</p>

<p>This already causes problems: <code>HMAC.c</code> may call functions from <code>Blake2b_256.c</code>, if Python is running
on a machine with AVX2. However, only <code>Blake2b_256.c</code> may be compiled with <code>-mavx2</code>: code from
<code>HMAC.c</code> will execute on all machines, even those without AVX2, meaning it must <em>not</em> be compiled
with <code>-mavx2</code>. So far, so good, and this is something we had done before.</p>

<p>The problem came with <code>HMAC.c</code> creating the initial state for <code>Blake2b_256.c</code>:</p>

<div><pre><code><span>#include</span> <span>&lt;immintrin.h&gt;</span><span>
</span>
<span>// ...</span>
  <span>__m256i</span> <span>*</span><span>blake2b_256_state</span> <span>=</span> <span>aligned_malloc</span><span>(</span><span>sizeof</span><span>(</span><span>__m256i</span><span>)</span><span>*</span><span>4</span><span>);</span>
<span>// ...</span>

</code></pre></div>

<p>Most toolchains were happy with this code – <code>immintrin.h</code> defines the type <code>__m256i</code>, and even
though <code>HMAC.c</code> cannot assume AVX2 instructions are available, it’s not to hard for a
compiler to zero-initialize <code>blake2b_256_state</code> without resorting to AVX2 instructions… except,
some older compilers refused to process the <code>immintrin.h</code> header unless <code>-mavx2</code> was used, which
defeated the whole purpose.</p>

<p>This required a considerable amount of refactoring to use the well-known “C abstract struct”
pattern, which essentially defines an abstract type in C.</p>

<div><pre><code><span>// Blake2b_256.h</span>
<span>typedef</span> <span>struct</span> <span>blake2b_256_st_s</span> <span>blake2b_256_st</span><span>;</span>
<span>blake2b_256_st</span> <span>*</span><span>blake2b_256_malloc</span><span>();</span>

<span>// Blake2b_256.c</span>
<span>#include</span> <span>&lt;immintrin.h&gt;</span><span>
</span><span>typedef</span> <span>struct</span> <span>blake2b_256_st_s</span> <span>{</span>
    <span>__m256i</span> <span>contents</span><span>[</span><span>4</span><span>];</span>
<span>}</span> <span>blake2b_256_st</span><span>;</span>

<span>// HMAC.c</span>
<span>blake2b_256_st</span> <span>*</span><span>blake2b_256_state</span> <span>=</span> <span>blake2b_256_malloc</span><span>();</span>
</code></pre></div>

<p>What made this extra difficult is that the C code is auto-generated from F*, which has a <em>very</em>
different notion of abstraction. The compiler that goes from F* to C,
<a href="https://github.com/FStarLang/karamel/">krml</a>, had to be overhauled to perform a much more
fine-grained analysis that handles various levels of visibility (public functions, library-internal
functions, translation-unit internal functions) even in the presence of such “abstract structs”.</p>

<h2 id="handling-memory-allocation-failures">Handling memory allocation failures</h2>

<p>While our original modeling of C in F* allowed reasoning about memory allocation failures, no one
had ever bothered to do so in practice. For Python, it was desirable to be able to propagate memory
allocation failures. This meant we had to refine our definition of a generic, mutable piece of state
(such as the block state); our definition of a block algorithm (such as SHA2-256); and our generic
streaming construction to all be able to propagate memory allocation failures all the way up to the
caller. Thankfully, this didn’t turn out to be a huge deal: we inserted <code>option</code> types all along the
way, and because we had one single generic streaming construction, the implementation and proofs had
to be updated only once for the 15+ concrete instances of the streaming API.</p>

<p>The presence of <code>option</code> types in the source compiles to tagged unions in the generated C; this is a
little verbose, and we may change our definition of a piece of state to feature a <code>has_failed</code>
run-time function that can assess whether a memory allocation failed, at the expense of more
complexity and verification effort.</p>

<h2 id="propagating-changes-from-upstream-hacl-to-python">Propagating changes from upstream HACL* to Python</h2>

<p>My initial Python PR contained a shell script that would fetch the required files from the upstream
HACL* repository; ditch a bunch of superfluous definitions in headers via well-crafted sed
invocations; and tweak a few include paths in-place, also using my favorite refactoring tool (yes,
sed). The benefit was the the initial PR was lean and clean.</p>

<p>Later on, once it became clear that the upstream code was maintainable and pretty stable, that pile
of seds was eliminated, on the basis that it’s not the end of the world if a header contains a few
extra definitions, and it all makes maintenance easier.</p>

<p>Now, anyone who wishes to refresh HACL* can run the shell script in their checkout of Python, and
provided they tweak the expected hash in Python’s SBOM (software bill of materials), they are good
to go and can integrate the latest improvements.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I’m delighted to see such a large-scale integration of verified cryptographic code in a flagship
project like Python. This demonstrates that verified cryptographic is not only ready from an
academic perspective, but also mature enough to be integrated in real-world software while meeting
all engineering expectations. Thanks to everyone who helped along this journey!</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[There's Life Inside Earth's Crust (117 pts)]]></title>
            <link>https://www.noemamag.com/theres-life-inside-earths-crust/</link>
            <guid>43730701</guid>
            <pubDate>Fri, 18 Apr 2025 18:35:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.noemamag.com/theres-life-inside-earths-crust/">https://www.noemamag.com/theres-life-inside-earths-crust/</a>, See on <a href="https://news.ycombinator.com/item?id=43730701">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="article">

        <div role="group">
    <p>Credits</p>
    <p><span>Karen G. Lloyd is the Wrigley Chair in Environmental Studies and professor of Earth science at the University of Southern California. This essay is adapted from her forthcoming </span><a href="https://urldefense.com/v3/__https:/press.princeton.edu/books/hardcover/9780691236117/intraterrestrials__;!!LIr3w8kk_Xxm!vTg8t3-G7wboTS4DBsyBiTHmbbYYhj-LP6Xrl2oDkeVNQWut-i3mfJCBZ-Fgpx6HlrudjeMg3DkdOg$"><span>book</span></a><span> “Intraterrestrials: Discovering the Strangest Life on Earth” (Princeton University Press, 2025).</span></p>
</div>
<p>The way to spot a cold methane seep on the ocean floor is to look for the life that gathers around it, like antelopes at a savanna watering hole: clams, mussels, crabs, shrimp, fish, sea anemones and creepy, otherworldly worms. These seeps, exposed by movements of tectonic plates or other geological processes, allow ancient, deeply buried methane to burble through the Earth’s crust and into the water column, where it becomes a kind of manna from heaven — a highly energetic food in what is otherwise a barren desert. Single-celled microbes eat the gas; the crustaceans, worms and other creatures in turn eat the microbes. For microbiologists like myself, this motley crew of creatures is a precious sight, but not because I’m interested in studying them — it’s the tiny microbes I make these half-mile descents for.</p><p>Ever since I became a microbiologist, a series of questions has gnawed at me: Are there life-­forms hiding inside the Earth? And if there are, how do they survive? Would their nature be so strange that they change our conception of life itself?&nbsp;</p><p>The major categories of visible life on Earth have been pretty much settled for centuries. But it wasn’t until the 1980s that scientists found “intraterrestrials” — microscopic organisms living in what the biogeochemist David Valentine calls a “microbial purgatory deep below the Earth’s surface.” Soon followed by other revelations of life inside Earth’s crust, these discoveries revealed that we had been missing major branches on the tree of life. Indeed, these microbes proved that our assumptions about the boundaries of life were wrong — and wildly so.</p><p>As it turns out, much of Earth’s habitable space lies deep under thousands of feet of sediments and rock. You might think that any sad little trickle of life in that deep, dark underworld might like to clamber back up to the surface, desperate for life­giving sunlight. But anyone who’s traveled to a methane seep at the bottom of the sea knows the opposite is true: life flourishes in the dark, and it does so on terms antithetical to those that we who inhabit the thin green layer at the Earth’s surface know.</p><p>Intraterrestrials survive without sun and oxygen; instead, via thermodynamics (the art of moving energy around), they are able to respire most elements of the periodic table. Radioactive uranium doesn’t really have a “life support” ring to it, yet that’s just what it is for some intraterrestrials. Arsenic is poisonous to humans and many other creatures, but intraterrestrials respire it too, essentially cleaning up our toxic pollution. Gold is not very chemically reactive — that’s why humans use it in currency and jewelry. Yet there are microbes that consume it.&nbsp;</p><p>We know they are alive because we can see the effects of their respiration on the chemicals around them, and we can see that they are intact and not degraded. We know they must be refreshing their cellular biomass. But they are doing it immensely slowly — gradually replacing their parts, lipid by lipid, nucleotide by nucleotide. It takes roughly half a century for them to replace all their molecules.</p><!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      “Are there life-­forms hiding inside the Earth? And if there are, how do they survive? Would their nature be so strange that they change our conception of life itself?”    </p>

    
    
  </blockquote>
</figure><p>These single-celled organisms can live for hundreds of thousands, perhaps millions, of years. (By contrast, the oldest multicellular organisms are trees, some of which can live to be thousands of years old, although none of the cells that compose them live for that long.) Theoretically, therefore, when I scoop deep sediment samples, I could be touching microbes that have been living and breathing continuously since well before humans were even a species.&nbsp;</p><p>By clinging to the bitter edge of what scientists ever would have guessed is the necessary amount of energy to support life, the intraterrestrials have earned a different relationship with time than we have. Indeed, many endure in environments where they don’t get enough energy to produce new cells, instead surviving on 0.00001­% of the power that supports all other known types of cell growth on Earth.&nbsp;</p><p>Here’s where the script of life really gets flipped, and where Darwinian evolution needs to be looked at anew. How does a species that doesn’t produce progeny for thousands of years or longer even evolve, given the importance of progeny in the process of evolution?</p><p>To answer this question, first we have to think about what these slow organisms would experience in their lifetimes. They aren’t concerned about the length of a day. They’re buried so deep that they can’t detect the sun anyway. They probably don’t even notice a change in seasons. However, they might care about other, longer geological rhythms: the opening and closing of oceanic basins through plate tectonics, the formation and subsidence of new island chains, new fluid flows brought on by the slow formation of cracks in Earth’s crust. The biology I was taught in school considered these events to be evolutionary drivers for a species, not individuals. For instance, Darwin’s finches evolved new beak shapes because they had become isolated on an island with a particular shape of seed to eat. This evolution happened over the geological timescale of island formation, but it occurred in a species lineage, not in an individual bird.</p><p>We know, however, that individuals are also capable of changing along with the rhythms of their environment. An arctic fox’s fur changes from white to brown when the snow melts every spring. Many people (not me, sadly) wake up at the same time each morning without the aid of an alarm. Daily and yearly rhythms seem like reasonable things for a person or an animal to keep track of. Ice ages, less so.</p><p>Anticipating environmental changes over timescales of eons seems ridiculous. A finch would not evolve the ability to swim because it anticipated that its island would subside into the sea in 100,000 years. But these scenarios may be reasonable for the interstellar travelers. They might count on island subsidence the way humans expect the sun to rise tomorrow.</p><hr><p>Buried in the deepest darkness underground, eating strange food, playing with the laws of thermodynamics and living on unrelatably long timescales, intraterrestrials have remained largely remote and aloof from humans. However, the microbes that dwell in the deep subsurface biosphere affect our lives in innumerable ways.</p><p>On a planetary scale, they play a key role in regulating Earth’s level of oxygenation. In addition, without the nutrients recycled by intraterrestrials in the seafloor, such as iron and nitrogen, phytoplankton would be severely limited in their ability to make oxygen for us. Intraterrestrials are also uniquely suited to detoxify our worst waste by breathing radioactive uranium, arsenic, organic carcinogens and other nasty stuff. So in effect, they have helped us develop as a species without poisoning ourselves.</p><p>Given how intrinsically entwined intraterrestrials are in Earth systems, they might also play an outsized role in how Earth responds to human-made climate change. Each time I climb inside a submersible and sink to the ocean floor or stick a probe into volcanic rock or drill into solid Arctic permafrost, I’m looking both backward into deep geologic history and forward into the future in search of evidence as to how great an impact intraterrestrials could have on our warming planet.</p><p>Some of the oldest permafrost in the world is in Siberia, where soils have been frozen solid for at least 1.1 million years. All signs point to the existence of living microbes there — their DNA is intact, not broken to bits like decayed body parts, and they have genes adapted to survive in the thin brine veins that run through permafrost. Much of the carbon on Earth’s surface is bound up in permafrost, so when it thaws, subsurface microbes might increase their metabolic activity, which would convert soil carbon to methane and carbon dioxide, both of which can exacerbate climate change. Almost every summer, new patches of previously frozen ground thaw, waking up new layers of microbes.&nbsp;</p><!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      “It wasn’t until the 1980s that scientists found ‘intraterrestrials’ — microscopic organisms living in what the biogeochemist David Valentine calls a ‘microbial purgatory deep below the Earth’s surface.’”    </p>

    
    
  </blockquote>
</figure><p>Currently, about 1,600 petagrams of carbon are sequestered in permanently frozen soils in Siberia, Greenland, Canada, Alaska, Antarctica, high-altitude locations at lower latitudes and even underwater. This number is about twice the amount of carbon currently in the atmosphere. Turning it all into gas would be catastrophic.</p><p>Most global climate change scenario models do not consider the influence of permafrost intraterrestrials. It’s not that the modelers haven’t heard of permafrost microbes or that they don’t think they’re important. It’s that global climate models are not yet able to handle that level of nuance. That’s why I go out in the Arctic to struggle with flimsy drill rods in gusty winds and temperatures around -60 degrees Fahrenheit: to figure out not just how intraterrestrials withstand such harsh conditions, but also whether they are likely to contribute to our doom or perhaps take some of the sting out of climate change.</p><p>We do know that some types of microbes hold promise, a possible source of salvation. Microbes in deep aquifers and tiny rock fractures could play a role in a process called carbon capture and storage (CCS), which would remove some of the greenhouse gases from our atmosphere. Most of the excess green­house gases currently in our atmosphere came from burning fossil fuels that originated underground. CCS would return those gases underground, but for it to be successful, it has to be really deep — safely out of reach of the atmosphere for thousands of years.&nbsp;</p><p>If the subsurface were like an inert container, we could just pump that gas down there, sit back and watch the global temperature stabilize and the glaciers creep back to where they’re supposed to be. But Earth’s crust is not an empty vessel. In addition to all the chemicals in the rocks that might interact with the injected gases, mercurial intraterrestrials are also hiding in every crack and crevice.</p><p>Those microbes might suck up this extra carbon from CCS and help turn it into solid rock, making sure it stays well away from our atmosphere for a very long time. Or they might use hydrogen generated from rocks and combine it with carbon dioxide to make methane, which could escape and accelerate global warming. The science of intraterrestrials is far too young to make any clear predictions just yet.</p><hr><p>Of course, CCS becomes less critical if we are able to quickly shift away from fossil fuel to energy captured from the sun, wind, tides, temperature gradients, dams, plants and algae. And if those renewables stand a chance of one day replacing fossil fuels, their transport and storage must be flexible; flexibility requires good batteries, and good batteries need metals.</p><p>Intraterrestrials have an intimate relationship with a wide variety of metals, many of which are commercially relevant. When intraterrestrials breathe these metals, their respiration often changes the metals’ mineral forms and determines whether they stay sequestered in soil and rock or whether they leach out of it.</p><p>Currently, all the metals in the batteries that power our personal electronics (as well as the large lithium-ion batteries used for transportation and high-powered magnets for windmills, for example) are mined. However, a lot of Earth’s metals can be found in grapefruit-sized nodules at the bottom of the ocean and in metal-rich deposits near hydrothermal vents. The process by which these metals accumulate in this way is extremely slow: It takes about a million years to create a layer of metals on a nodule that is as thick as a layer of nail polish. In other words, these polymetallic nodules are not a renewable resource: If someone removed them, it would take longer than the human species has been in existence to detect a noticeable replenishment.</p><p>For decades, researchers and entrepreneurs have probed these deep-sea nodules to figure out if they could be profitably extracted. So far, it remains too expensive and difficult. In the past few years, however, precious metals have become increasingly important, drastically changing the economic calculus around deep-sea mining.</p><p>Polymetallic nodule mining at a large scale would involve ships lowering dredges the size of a couple trucks onto the seafloor, which then move along it, scooping up everything in the upper few inches. A lot of important biological and chemical processes happen in those upper layers, and nutrients from there travel to the surface ocean, stimulating photosynthesis, fueling fish populations and regulating the gases in our atmosphere.</p><p>Deep-sea mining would destroy these layers and much more. After the material dredged from the ocean floor is funneled back up to surface, polymetallic nodules will be separated from sediments that, if dumped overboard, would darken the upper ocean, harming marine animals and killing off phytoplankton, which sequesters greenhouse gases and makes the oxygen and food that support the abundant life in our oceans. Piping it back down to the seafloor would mitigate some of the harms, but there is no consensus on how deep the mining companies should go to release sediment.&nbsp;</p><!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      “As it turns out, much of Earth’s habitable space lies deep under thousands of feet of sediments and rock.”    </p>

    
    
  </blockquote>
</figure><p>The amount of seafloor currently available for nodule exploration is more than four times larger than all the seafloor impacted by offshore oil and gas, and at least twice as large as what is currently dredged for bottom fisheries. The Clarion Clipperton Zone, where companies are currently conducting “small-scale” mining operations, is bigger than India and Pakistan combined. The entire footprint of current mining on land represents only about 3% of the area that may be covered by deep-sea mining if operations are scaled. As a result, hazards are going to pile up, with significant negative environmental consequences for the deep sea.</p><p>Polymetallic nodules are not the only targets for commercial mining in the deep sea. Polymetallic sulfides, otherwise known as hydrothermal vents, and cobalt-rich ferromanganese crusts are in the crosshairs too. Hydrothermal vents form when hot, metal-laden subsurface fluids shoot from the deep like a firehose into cold, oxygenated seawater, which forces metals in the fluids to precipitate.&nbsp;</p><p>Cobalt-rich ferromanganese crusts, by contrast, form in the areas of the seafloor where ocean currents have swept the area clean of any sediments. These currents expose basalt produced at mid-ocean spreading ridges, which can precipitate metals like cobalt, nickel, platinum, manganese, thallium and tellurium out of seawater. Both of these types of structures make good physical substrates for deep-sea mussels and clams, as well as provide metals that fuel microbial life in the ocean. Like polymetallic nodules, these ecosystems are quietly humming along at the seafloor, providing services that keep the whole ocean in proper balance.</p><p>Developing a deep-sea mining industry could wipe away towering spires of minerals built by microbial chemical reactions of deep, ancient fluids. It might destroy unknown intraterrestrials before we have a chance to discover them. Many of the geological, chemical, physical and biological pro­cesses happening right now in the deep sea — processes that have been occurring for millions or even billions of years —­ have yet to be documented. These could also be destroyed, and along with them, the life-giving functions they provide.</p><p>The deep sea is one of the least understood places on the planet. I love it because it sits at the sweet spot between mysteriousness and accessibility. But it is imperiled by growing demands for its bounty. Intraterrestrials — almost certainly the oldest life on Earth — can tell us much about how life and Earth co-evolved and the ecosystems that might carry genes or pro­cesses that are helpful for medicine or ameliorating climate change.&nbsp;</p><p>We only discovered these beings a few decades ago. So far, what we’ve learned about them is mind-blowing. And we learn more every day. Continuing to study them opens up pathways to understanding ways of living that seem completely alien to the life that exists on the surface of the Earth, as well as possibilities for existing in broader harmony with our planet.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Judge Rules Blanket Search of Cell Tower Data Unconstitutional (444 pts)]]></title>
            <link>https://www.404media.co/judge-rules-blanket-search-of-cell-tower-data-unconstitutional/</link>
            <guid>43730545</guid>
            <pubDate>Fri, 18 Apr 2025 18:16:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/judge-rules-blanket-search-of-cell-tower-data-unconstitutional/">https://www.404media.co/judge-rules-blanket-search-of-cell-tower-data-unconstitutional/</a>, See on <a href="https://news.ycombinator.com/item?id=43730545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p><em>This article was produced in collaboration&nbsp;</em><a href="https://www.courtwatch.news/?ref=404media.co" rel="noopener noreferrer"><em>with Court Watch</em></a><em>, an independent outlet that unearths overlooked court records.&nbsp;</em><a href="https://www.courtwatch.news/?ref=404media.co" rel="noopener noreferrer"><em>Subscribe to them here</em></a><em>.</em></p><p>A judge in Nevada has ruled that “tower dumps”—the law enforcement practice of grabbing vast troves of private personal data from cell towers—is unconstitutional. The judge also ruled that the cops could, this one time, still use the evidence they obtained through this unconstitutional search.&nbsp;</p><p>Cell towers record the location of phones near them about every seven seconds. When the cops request a tower dump, they ask a telecom for the numbers and personal information of every single phone connected to a tower during a set time period. Depending on the area, these tower dumps can return tens of thousands of numbers.</p>
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>Cops have been able to sift through this data to solve crimes. But tower dumps are also a massive privacy violation that flies in the face of the Fourth Amendment, which protects people from unlawful search and seizure. When the cops get a tower dump they’re not just searching and seizing the data of a suspected criminal, they’re sifting through the information of everyone who was in the location.</p><p>A Nevada man, Cory Spurlock, is facing charges related to dealing marijuana and a murder-for-hire scheme. Cops used a tower dump to connect his cellphone with the location of some of the crimes he is accused of. Spurlock’s lawyers argued that the tower dump was an unconstitutional search and that the evidence obtained during it should not be. The cops got a warrant to conduct the tower dump but argued it wasn’t technically a “search” and therefore wasn’t subject to the Fourth Amendment.</p><p>U.S. District Juste Miranda M. Du rejected this argument, but wouldn’t suppress the evidence. “The Court finds that a tower dump is a search and the warrant law enforcement used to get it is a general warrant forbidden under the Fourth Amendment,” she said in a ruling filed on April 11. “That said, because the Court appears to be the first court within the Ninth Circuit to reach this conclusion and the good faith exception otherwise applies, the Court will not order any evidence suppressed.”</p><p>Du argued that the officers acted in good faith when they filed the warrant and that they didn’t know the search was unconstitutional when they conducted it. According to Du, the warrant wasn’t unconstitutional when a judge issued it.</p><p>Du’s ruling is the first time the United States Court of Appeals for the Ninth Circuit has ruled on the constitutionality of tower dumps, but this isn’t the first time a federal judge has weighed in. One in Mississippi <a href="https://www.courtwatch.news/p/exclusive-judge-rules-tower-dumps-unconstitutional?ref=404media.co"><u>came to the same conclusion</u></a> in February. A few weeks later, the Department of Justice <a href="https://www.courtwatch.news/p/exclusive-doj-to-appeal-tower-dump-ruling?ref=404media.co"><u>appealed the ruling</u></a>.</p><p>There’s a decent chance that one of these cases will wind its way up to the Supreme Court and that SCOTUS will have to make a ruling about tower dumps. The last time the issue was in front of them, they kicked the can back to the lower courts.</p><p>In 2018, the Supreme Court considered <a href="https://www.oyez.org/cases/2017/16-402?ref=404media.co"><em><u>Carpenter v. United States</u></em></a>, a case where the FBI used cell phone location data to investigate a series of robberies. The Court decided that law enforcement agencies violate the Fourth Amendment when they ask for cell phone location data without a warrant. But the ruling was narrow and the Court declined to rule on the issue of tower dumps.</p><p>According to the court records for Spurlock’s case, the tower dump that caught him captured the private data of 1,686 users. An expert who testified before the court about the dump noted that “the wireless company users whose phones showed up in the tower dump data did not opt in to sharing their location with their wireless provider, and indeed, could not opt out from appearing in the type of records received in response to [the] warrant.”</p>
<!--kg-card-begin: html-->

<!--kg-card-end: html-->

                    <div>
    <div>
      <p>About the author</p>
      <p>Matthew Gault is a writer covering weird tech, nuclear war, and video games. He’s worked for Reuters, Motherboard, and the New York Times.</p>
      
    </div>
      <p><img data-src="https://www.gravatar.com/avatar/87e07bd5bb3d003b0b135303a3e7f8b9?s=250&amp;r=x&amp;d=mp" alt="Matthew Gault" src="https://www.gravatar.com/avatar/87e07bd5bb3d003b0b135303a3e7f8b9?s=250&amp;r=x&amp;d=mp">  
      </p>
  </div>
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a Doom-like game fit inside a QR code (474 pts)]]></title>
            <link>https://github.com/Kuberwastaken/backdooms</link>
            <guid>43729683</guid>
            <pubDate>Fri, 18 Apr 2025 16:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Kuberwastaken/backdooms">https://github.com/Kuberwastaken/backdooms</a>, See on <a href="https://news.ycombinator.com/item?id=43729683">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">The Backdooms: Running DOOM on a QR Code</h2><a id="user-content-the-backdooms-running-doom-on-a-qr-code" aria-label="Permalink: The Backdooms: Running DOOM on a QR Code" href="#the-backdooms-running-doom-on-a-qr-code"></a></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Kuberwastaken/backdooms/blob/main/qrcode.png"><img src="https://github.com/Kuberwastaken/backdooms/raw/main/qrcode.png" alt="QR Code for The Backdooms"></a>
</p>
<p dir="auto">
    Yes, this is literally the entire game. You can <a href="https://scanqr.org/" rel="nofollow">scan it</a> to play.
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1fa4cef0c40c141ac0ed954ecdb8d2119b7b110a222d5feef9b9ff01e3363977/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d4b7562657277617374616b656e266d6573736167653d4261636b646f6f6d7326636f6c6f723d626c61636b266c6f676f3d676974687562"><img src="https://camo.githubusercontent.com/1fa4cef0c40c141ac0ed954ecdb8d2119b7b110a222d5feef9b9ff01e3363977/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d4b7562657277617374616b656e266d6573736167653d4261636b646f6f6d7326636f6c6f723d626c61636b266c6f676f3d676974687562" alt="Kuberwastaken - Backdooms" data-canonical-src="https://img.shields.io/static/v1?label=Kuberwastaken&amp;message=Backdooms&amp;color=black&amp;logo=github"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/90d2eb9460dae650d20e525e61ffe4973f067ee0dde23e1e5b706b5630d039e9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d31332e302d626c61636b"><img src="https://camo.githubusercontent.com/90d2eb9460dae650d20e525e61ffe4973f067ee0dde23e1e5b706b5630d039e9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d31332e302d626c61636b" alt="Version 13.0" data-canonical-src="https://img.shields.io/badge/version-13.0-black"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/56265e049c38cb332bf7419805d7c1d985832a34e869aef4801771fe9b416133/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c61636b"><img src="https://camo.githubusercontent.com/56265e049c38cb332bf7419805d7c1d985832a34e869aef4801771fe9b416133/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c61636b" alt="License MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-black"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Project Overview</h2><a id="user-content--project-overview" aria-label="Permalink: 🚀 Project Overview" href="#-project-overview"></a></p>
<p dir="auto"><strong>The Backdooms</strong> is a compressed, self-extracting and infinitely generating HTML game inspired by DOOM 1993 and The Backrooms that can be launched and played in a web browser directly from a QR code.</p>
<p dir="auto">This project was a week-long study I performed (now slightly longer) designed to push the limits of QR code storage and compression, to demonstrate an innovative method of hosting lightweight web applications entirely within a QR code.</p>
<hr>
<p dir="auto">
    <a href="https://kuberwastaken.github.io/backdooms" rel="nofollow">Play a slightly less compressed version of this game here</a>
    <br>
    <span>Yes, it's 8bit Undertale Music There</span>
</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📜 Features</h2><a id="user-content--features" aria-label="Permalink: 📜 Features" href="#-features"></a></p>
<p dir="auto">✅ <strong>Fully Offline:</strong> No internet connection is required to play the game after scanning the QR code. The URL basically has the ENTIRE code to the game</p>
<p dir="auto">✅ <strong>Extreme Compression:</strong> Utilizes a combination of Zlib compression with Gzip Decompression stream along with base64 encoding to make the final result extremely compressed.</p>
<p dir="auto">✅ <strong>Self-Extracting Webpage:</strong> Uses the <code>DecompressionStream</code> API to dynamically decompress and execute the game within the browser.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📦 Installation &amp; Dependencies</h2><a id="user-content--installation--dependencies" aria-label="Permalink: 📦 Installation &amp; Dependencies" href="#-installation--dependencies"></a></p>
<p dir="auto">Well, technically speaking, just a modern web browser.</p>
<p dir="auto">But if we do get into the nitty gritty of generating a QR code of a ≈ 2.5kb game:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li><strong>Python 3.7+</strong></li>
<li><code>qrcode</code> library (for generating QR codes)</li>
<li><code>pillow</code> (for QR code image handling)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install Dependencies</h3><a id="user-content-install-dependencies" aria-label="Permalink: Install Dependencies" href="#install-dependencies"></a></p>

<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛠️ Usage</h2><a id="user-content-️-usage" aria-label="Permalink: 🛠️ Usage" href="#️-usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">1️⃣ Convert Your Game to a QR Code</h3><a id="user-content-1️⃣-convert-your-game-to-a-qr-code" aria-label="Permalink: 1️⃣ Convert Your Game to a QR Code" href="#1️⃣-convert-your-game-to-a-qr-code"></a></p>
<p dir="auto">Run the script with the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 QRGEN.py <your-game.html> <output-qrcode.png>"><pre>python3 QRGEN.py <span>&lt;</span>your-game.html<span>&gt;</span> <span>&lt;</span>output-qrcode.png<span>&gt;</span></pre></div>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 QRGEN.py EVEN-SMALLER-SLAMMER-BACKROOMS.html qrcode.png"><pre>python3 QRGEN.py EVEN-SMALLER-SLAMMER-BACKROOMS.html qrcode.png</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">2️⃣ Scan the QR Code</h3><a id="user-content-2️⃣-scan-the-qr-code" aria-label="Permalink: 2️⃣ Scan the QR Code" href="#2️⃣-scan-the-qr-code"></a></p>
<p dir="auto">Use a smartphone or QR scanner to open the game directly in a web browser.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">3️⃣ Play Instantly 🎮</h3><a id="user-content-3️⃣-play-instantly-" aria-label="Permalink: 3️⃣ Play Instantly 🎮" href="#3️⃣-play-instantly-"></a></p>
<p dir="auto">Enjoy <em>The Backdooms</em> without needing to download or install anything!</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔍 Technical Breakdown</h2><a id="user-content--technical-breakdown" aria-label="Permalink: 🔍 Technical Breakdown" href="#-technical-breakdown"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compression Workflow</h3><a id="user-content-compression-workflow" aria-label="Permalink: Compression Workflow" href="#compression-workflow"></a></p>
<section data-identity="4a077bf4-deaa-438f-a48b-a67a51b6075d" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;flowchart TD\n    A[Read Input HTML] --&amp;gt; B[Compress with Zlib]\n    B --&amp;gt;|wbits=15| C[Base64 Encode]\n    C --&amp;gt; D[Embed in HTML Wrapper]\n    \n    subgraph browser[Browser Processing]\n        D --&amp;gt; E[DecompressionStream 'gzip']\n        E --&amp;gt; F{Format Mismatch}\n    end\n    \n    F --&amp;gt; G[Convert to Data URI]\n    G --&amp;gt; H{Fits QR Code?}\n    H --&amp;gt;|Yes| I[Generate QR]\n    H --&amp;gt;|No| J[Reduce HTML Size]\n    J --&amp;gt; A\n&quot;}" data-plain="flowchart TD
    A[Read Input HTML] --> B[Compress with Zlib]
    B -->|wbits=15| C[Base64 Encode]
    C --> D[Embed in HTML Wrapper]
    
    subgraph browser[Browser Processing]
        D --> E[DecompressionStream 'gzip']
        E --> F{Format Mismatch}
    end
    
    F --> G[Convert to Data URI]
    G --> H{Fits QR Code?}
    H -->|Yes| I[Generate QR]
    H -->|No| J[Reduce HTML Size]
    J --> A
">
      <pre lang="mermaid" aria-label="Raw mermaid code">flowchart TD
    A[Read Input HTML] --&gt; B[Compress with Zlib]
    B --&gt;|wbits=15| C[Base64 Encode]
    C --&gt; D[Embed in HTML Wrapper]
    
    subgraph browser[Browser Processing]
        D --&gt; E[DecompressionStream 'gzip']
        E --&gt; F{Format Mismatch}
    end
    
    F --&gt; G[Convert to Data URI]
    G --&gt; H{Fits QR Code?}
    H --&gt;|Yes| I[Generate QR]
    H --&gt;|No| J[Reduce HTML Size]
    J --&gt; A
</pre>
    </div>
  <span role="presentation">
    <span data-view-component="true">
      <span>Loading</span>
</span>
  </span>
</section>

<p dir="auto"><strong>Read Input HTML:</strong> The process starts by reading the given HTML content from a file or input source.</p>
<p dir="auto"><strong>Zlib Compression + GZip decompression:</strong> The HTML is compressed using Zlib and uses Decompressionstream from GZip for best compression</p>
<p dir="auto"><strong>Base64 Encoding:</strong> The compressed data is encoded in Base64, ensuring that it remains text-based and can be embedded in an HTML file safely.</p>
<p dir="auto"><strong>Embedding in HTML Wrapper:</strong> A JavaScript-based self-extracting HTML wrapper is created. This wrapper includes a DecompressionStream API function that automatically decompresses the content when opened in a browser.</p>
<p dir="auto"><strong>Data URI Conversion:</strong> The entire HTML is converted into a data:text/html;base64,... format, allowing it to be stored and shared easily without a physical file.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">QR Code Generation Logic</h3><a id="user-content-qr-code-generation-logic" aria-label="Permalink: QR Code Generation Logic" href="#qr-code-generation-logic"></a></p>
<p dir="auto">The system first tries to generate the smallest possible QR version using qr.make(fit=True), which dynamically adjusts the QR size based on content length.</p>
<p dir="auto">If the required version exceeds 40 (the QR code standard limit), it forces version 40 with fit=False.</p>
<p dir="auto">The lowest error correction level L (which allows the maximum data capacity) is used to fit as much data as possible.</p>
<p dir="auto">If the data is still too large for QR v40 with level L, the process fails, and an error is returned.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Outcome</h3><a id="user-content-outcome" aria-label="Permalink: Outcome" href="#outcome"></a></p>
<p dir="auto">If successful, a QR code is generated and displayed.</p>
<p dir="auto">If not, the process terminates with an error message indicating that the data is too large to be encoded in a QR code.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Understand the Frustration and Trial and Error</h2><a id="user-content-understand-the-frustration-and-trial-and-error" aria-label="Permalink: Understand the Frustration and Trial and Error" href="#understand-the-frustration-and-trial-and-error"></a></p>
<p dir="auto">Read about the development journey on my blog <a href="https://kuberwastaken.github.io/blog/Projects/How-I-Managed-To-Get-Doom-In-A-QR-Code" rel="nofollow">MindDump</a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📜 License</h2><a id="user-content--license" aria-label="Permalink: 📜 License" href="#-license"></a></p>
<p dir="auto">This project is released under the <strong>MIT License</strong>—free to use, modify, and share.
It would make me EXTREMELY happy to see other QR games or even seeing better versions of DOOM in a QR code, given there's so little resources related to this</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙌 Credits</h2><a id="user-content--credits" aria-label="Permalink: 🙌 Credits" href="#-credits"></a></p>
<ul dir="auto">
<li>id Software for Developing DOOM</li>
<li>matttkc for putting this idea in my head 5 years ago</li>
<li>Toby Fox for the amazing music in Undertale, this game's GitHub hosted version uses an 8 bit version of Bonetrousle</li>
</ul>
<p dir="auto">Developed by <strong>Kuber Mehta</strong> :)</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>