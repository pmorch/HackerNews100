<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 05 Oct 2024 13:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Linux from Scratch (117 pts)]]></title>
            <link>https://www.linuxfromscratch.org/index.html</link>
            <guid>41747966</guid>
            <pubDate>Sat, 05 Oct 2024 05:43:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.linuxfromscratch.org/index.html">https://www.linuxfromscratch.org/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41747966">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

     <p>Linux From Scratch (LFS) is a project that provides you with
        step-by-step instructions for building your own custom Linux system,
        entirely from source code.
     </p>

     <p>Currently, the Linux From Scratch organization consists of the following
        subprojects:
     </p>

     <ul id="subs">

       <li><a href="https://www.linuxfromscratch.org/lfs/">LFS</a> :: Linux From Scratch is the main book, the
       base from which all other projects are derived.</li>
       
       <li><a href="https://www.linuxfromscratch.org/blfs/">BLFS</a> :: Beyond Linux From Scratch helps you
       extend your finished LFS installation into a more customized and usable
       system.</li>
       
       <li><a href="https://www.linuxfromscratch.org/alfs/">ALFS</a> :: Automated Linux From Scratch provides
       tools for automating and managing LFS and BLFS builds.</li>

       <li><a href="https://www.linuxfromscratch.org/hints/">Hints</a> :: The Hints project is a collection of
       documents that explain how to enhance your LFS system in ways that are
       not included in the LFS or BLFS books.</li>

       <li><a href="https://www.linuxfromscratch.org/patches/">Patches</a> :: The Patches project serves as a
       central repository for all patches useful to an LFS user.</li> 

       <li><a href="https://www.linuxfromscratch.org/lfs/LFS-EDITORS-GUIDE.html">LFS Editor's Guide</a> :: A document that 
       describes the LFS development process.</li> 

       <li><a href="https://www.linuxfromscratch.org/museum/">Museum</a> :: Copies of ancient LFS and BLFS versions.</li> 
     </ul>

       <br>
      
     </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple releases Depth Pro, an AI model that rewrites the rules of 3D vision (107 pts)]]></title>
            <link>https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/</link>
            <guid>41747863</guid>
            <pubDate>Sat, 05 Oct 2024 05:10:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/">https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=41747863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<section>
			
			<p><time title="2024-10-04T18:52:31+00:00" datetime="2024-10-04T18:52:31+00:00">October 4, 2024 11:52 AM</time>
			</p>
			
		</section>
		<div>
					<p><img width="750" height="421" src="https://venturebeat.com/wp-content/uploads/2024/10/nuneybits_Flat_vector_design_of_the_Apple_logo_integrated_into__6d245237-c107-4fb8-b31c-e089bfd3f5f2-1.webp?w=750" alt="Credit: VentureBeat made with Midjourney"></p><p><span>Credit: VentureBeat made with Midjourney</span></p>		</div><!-- .article-media-header -->
	</div><div id="primary">

		<article id="content">
			<div>
				<div id="boilerplate_2682874"><!-- wp:paragraph -->
<p><em>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. <a href="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav" data-type="link" data-id="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav">Learn More</a></em></p>
<!-- /wp:paragraph -->

<!-- wp:separator {"opacity":"css","className":"is-style-wide"} -->
<hr>
<!-- /wp:separator --></div><p><a href="https://machinelearning.apple.com/">Apple’s AI research team</a> has developed a new model that could significantly advance how machines perceive depth, potentially transforming industries ranging from augmented reality to autonomous vehicles.</p>



<p>The system, called&nbsp;<a href="https://arxiv.org/pdf/2410.02073">Depth Pro</a>, is able to generate detailed 3D depth maps from single 2D images in a fraction of a second—without relying on the camera data traditionally needed to make such predictions.</p>



<p>The technology, detailed in a research paper titled&nbsp;<em>“<a href="https://arxiv.org/pdf/2410.02073">Depth Pro: Sharp Monocular Metric Depth in Less Than a Second</a>,”</em>&nbsp;is a major leap forward in the field of monocular depth estimation, a process that uses just one image to infer depth.</p>



<p>This could have far-reaching applications across sectors where real-time spatial awareness is key. The model’s creators, led by Aleksei Bochkovskii and Vladlen Koltun, describe&nbsp;Depth Pro&nbsp;as one of the fastest and most accurate systems of its kind.</p>



<figure><img fetchpriority="high" decoding="async" width="830" height="853" src="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12%E2%80%AFAM.png?w=584" alt="" srcset="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png 830w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=300,308 300w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=768,789 768w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=584,600 584w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=52,52 52w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=400,411 400w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=750,771 750w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=578,594 578w" sizes="(max-width: 830px) 100vw, 830px"><figcaption>A comparison of depth maps from Apple’s Depth Pro, Marigold, Depth Anything v2, and Metric3D v2. Depth Pro excels in capturing fine details like fur and birdcage wires, producing sharp, high-resolution depth maps in just 0.3 seconds, outperforming other models in accuracy and detail. (credit: arxiv.org)</figcaption></figure>







<p>Monocular depth estimation has long been a challenging task, requiring either multiple images or metadata like focal lengths to accurately gauge depth.</p>



<p>But&nbsp;Depth Pro&nbsp;bypasses these requirements, producing high-resolution depth maps in just 0.3 seconds on a standard GPU. The model can create 2.25-megapixel maps with exceptional sharpness, capturing even minute details like hair and vegetation that are often overlooked by other methods.</p>



<p>“These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction,” the researchers explain in their paper. This architecture allows the model to process both the overall context of an image and its finer details simultaneously—an enormous leap from slower, less precise models that came before it.</p>



<figure><img decoding="async" width="846" height="953" src="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18%E2%80%AFAM.png?w=533" alt="" srcset="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png 846w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=300,338 300w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=768,865 768w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=533,600 533w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=400,451 400w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=750,845 750w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=578,651 578w" sizes="(max-width: 846px) 100vw, 846px"><figcaption>A comparison of depth maps from Apple’s Depth Pro, Depth Anything v2, Marigold, and Metric3D v2. Depth Pro excels in capturing fine details like the deer’s fur, windmill blades, and zebra’s stripes, delivering sharp, high-resolution depth maps in 0.3 seconds. (credit: arxiv.org)</figcaption></figure>



<h2 id="h-metric-depth-zero-shot-learning">Metric depth, zero-shot learning</h2>



<p>What truly sets&nbsp;Depth Pro&nbsp;apart is its ability to estimate both relative and absolute depth, a capability called “metric depth.”</p>



<p>This means that the model can provide real-world measurements, which is essential for applications like augmented reality (AR), where virtual objects need to be placed in precise locations within physical spaces.</p>



<p>And&nbsp;Depth Pro&nbsp;doesn’t require extensive training on domain-specific datasets to make accurate predictions—a feature known as “zero-shot learning.” This makes the model highly versatile. It can be applied to a wide range of images, without the need for the camera-specific data usually required in depth estimation models.</p>



<p>“Depth Pro produces metric depth maps with absolute scale on arbitrary images ‘in the wild’ without requiring metadata such as camera intrinsics,” the authors explain. This flexibility opens up a world of possibilities, from enhancing AR experiences to improving autonomous vehicles’ ability to detect and navigate obstacles.</p>



<p>For those curious to experience Depth Pro firsthand, a <a href="https://huggingface.co/spaces/akhaliq/depth-pro">live demo</a> is available on the Hugging Face platform.</p>



<figure><img decoding="async" width="1387" height="375" src="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50%E2%80%AFAM.png?w=800" alt="" srcset="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png 1387w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=300,81 300w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=768,208 768w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=800,216 800w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=400,108 400w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=750,203 750w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=578,156 578w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=930,251 930w" sizes="(max-width: 1387px) 100vw, 1387px"><figcaption>A comparison of depth estimation models across multiple datasets. Apple’s Depth Pro ranks highest overall with an average rank of 2.5, outperforming models like Depth Anything v2 and Metric3D in accuracy across diverse scenarios. (credit: arxiv.org)</figcaption></figure>



<h2 id="h-real-world-applications-from-e-commerce-to-autonomous-vehicles">Real-world applications: From e-commerce to autonomous vehicles</h2>



<p>This versatility has significant implications for various industries. In e-commerce, for example,&nbsp;Depth Pro&nbsp;could allow consumers to see how furniture fits in their home by simply pointing their phone’s camera at the room. In the automotive industry, the ability to generate real-time, high-resolution depth maps from a single camera could improve how self-driving cars perceive their environment, boosting navigation and safety.</p>



<p>“The method should ideally produce metric depth maps in this zero-shot regime to accurately reproduce object shapes, scene layouts, and absolute scales,” the researchers write, emphasizing the model’s potential to reduce the time and cost associated with training more conventional AI models.</p>



<h2 id="h-tackling-the-challenges-of-depth-estimation">Tackling the challenges of depth estimation</h2>



<p>One of the toughest challenges in depth estimation is handling what are known as “flying pixels”—pixels that appear to float in mid-air due to errors in depth mapping.&nbsp;Depth Pro&nbsp;tackles this issue head-on, making it particularly effective for applications like 3D reconstruction and virtual environments, where accuracy is paramount.</p>



<p>Additionally,&nbsp;Depth Pro&nbsp;excels in boundary tracing, outperforming previous models in sharply delineating objects and their edges. The researchers claim it surpasses other systems “by a multiplicative factor in boundary accuracy,” which is key for applications that require precise object segmentation, such as image matting and medical imaging.</p>



<h2 id="h-open-source-and-ready-to-scale">Open-source and ready to scale</h2>



<p>In a move that could accelerate its adoption, Apple has made&nbsp;Depth Pro&nbsp;open-source. The code, along with pre-trained model weights, is <a href="https://github.com/apple/ml-depth-pro" target="_blank" rel="noreferrer noopener">available on GitHub</a>, allowing developers and researchers to experiment with and further refine the technology. The repository includes everything from the model’s architecture to pretrained checkpoints, making it easy for others to build on Apple’s work.</p>



<p>The research team is also encouraging further exploration of&nbsp;Depth Pro’s potential in fields like robotics, manufacturing, and healthcare. “We release code and weights at&nbsp;<a href="https://github.com/apple/ml-depth-pro" target="_blank" rel="noreferrer noopener">https://github.com/apple/ml-depth-pro</a>,”&nbsp;the authors write, signaling this as just the beginning for the model.</p>



<h2 id="h-what-s-next-for-ai-depth-perception">What’s next for AI depth perception</h2>



<p>As artificial intelligence continues to push the boundaries of what’s possible,&nbsp;<em>Depth Pro</em>&nbsp;sets a new standard in speed and accuracy for monocular depth estimation. Its ability to generate high-quality, real-time depth maps from a single image could have wide-ranging effects across industries that rely on spatial awareness.</p>



<p>In a world where AI is increasingly central to decision-making and product development,&nbsp;<em>Depth Pro</em>&nbsp;exemplifies how cutting-edge research can translate into practical, real-world solutions. Whether it’s improving how machines perceive their surroundings or enhancing consumer experiences, the potential uses for&nbsp;<em>Depth Pro</em>&nbsp;are broad and varied.</p>



<p>As the researchers conclude, “Depth Pro dramatically outperforms all prior work in sharp delineation of object boundaries, including fine structures such as hair, fur, and vegetation.” With its open-source release,&nbsp;<em>Depth Pro</em>&nbsp;could soon become integral to industries ranging from autonomous driving to augmented reality—transforming how machines and people interact with 3D environments.</p>
<div id="boilerplate_2660155">
				<p><strong>VB Daily</strong></p>
				<p>Stay in the know! Get the latest news in your inbox daily</p>
				
				<p>By subscribing, you agree to VentureBeat's <a href="https://venturebeat.com/terms-of-service/">Terms of Service.</a></p>
				<p id="boilerplateNewsletterConfirmation">
					Thanks for subscribing. Check out more <a href="https://venturebeat.com/newsletters/">VB newsletters here</a>.
				</p>
				<p>An error occured.</p>
			</div>			</div><!-- .article-content -->

							
			
		</article><!-- #content .article-wrapper -->

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't squander public trust on bullshit (229 pts)]]></title>
            <link>https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit</link>
            <guid>41746180</guid>
            <pubDate>Fri, 04 Oct 2024 22:42:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit">https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit</a>, See on <a href="https://news.ycombinator.com/item?id=41746180">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>At 4.50am local time today, this statewide emergency alert was sent out to every cellphone in Texas:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg" width="1201" height="410" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:410,&quot;width&quot;:1201,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123540,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>I don’t know who Seth Altman is, nor do I care. Why? Because Seth Altman’s offense took place in Lubbock, Texas. I live in Austin, Texas. Four hundred miles away. What I do care about however is the misuse of emergency alert systems and public trust.</p><p><span>Sending out a screeching alert to 30million+ people over 250 million square miles in the middle of the night should only be used in the absolute DIREST OF CIRCUMSTANCES… circumstances like “Texas is under threat from hurricane/chemical leak/nuclear weapons, seek shelter now!” It should </span><em>never</em><span> be used for something that’s utterly irrelevant to 99.99% of people. </span></p><p>Why? Because the public’s trust in government emergency protocols is already hanging by a thread, and in order for those protocols to work when we really need them, they will need to be received and listened to. Instead, Texans by the hundreds of thousands are now turning off their phone’s emergency alerts, possibly forever. Why would anyone with a life to lead leave them on and risk getting their sleep disrupted over personally inconsequential events hundreds of miles away? </p><p>Such an outcome could be truly dangerous for Texas in the long run. If and when a real major emergency strikes, we will no longer have this important tool of public awareness or coordination. And that’s just the second order effects! There are likely going to be some excess deaths today as a direct result—there are 30m+ people in Texas, many of whom are in weak cardiovascular health. I would not be surprised at all if hospitals report an spike in cardiovascular events today. Not to mention an increase in road accidents; Texas is a notoriously driving-heavy state, and few things are worse for safe driving than sleep impairment.</p><p>So I hope the local government takes a long hard look at its alert-pressing finger. We all know the lesson of the Boy Who Cried Wolf, exhausting his village with his over-zealous cries. Well this time the village is thirty million people. Heads need to roll.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs, Theory of Mind, and Cheryl's Birthday (168 pts)]]></title>
            <link>https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb</link>
            <guid>41745788</guid>
            <pubDate>Fri, 04 Oct 2024 21:34:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb">https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb</a>, See on <a href="https://news.ycombinator.com/item?id=41745788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_product_navbar&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>

                  <li>
      
      
</li>

                  <li>
      
      <div>
                <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;white_papers_ebooks_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;white_papers_ebooks_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      White papers, Ebooks, Webinars

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:norvig/pytudes" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="PE5AWI3p_iSziKdZC8ErZLcqeumyHJV5L9EaTYKLjdtd6HPcmwaEpev76mcRxZhzajMzDiv0gUxEzL3nTiTiKQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="norvig/pytudes" data-current-org="" data-current-owner="norvig" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=norvig%2Fpytudes" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="21d4ffb04cbc87fb443820ed00ec47c8a2db97dff8dd678de0053e74fc7c83f8" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>
          
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Max Schrems wins privacy case against Meta over data on sexual orientation (125 pts)]]></title>
            <link>https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86</link>
            <guid>41745181</guid>
            <pubDate>Fri, 04 Oct 2024 20:17:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86">https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86</a>, See on <a href="https://news.ycombinator.com/item?id=41745181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>LONDON (AP) — The European Union’s top court said Friday that social media company Meta can’t use public information about a user’s sexual orientation obtained outside its platforms for personalized advertising under the bloc’s strict data privacy rules. </p><p>The decision from the Court of Justice of the European Union in Luxembourg is a <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/meta-facebook-instagram-whatsapp-ai-artificial-intelligence-8d6cb3424ee410c641d0acdb154601f5">victory for Austrian privacy activist Max Schrems</a></span>, who has been a thorn in the side of Big Tech companies over their compliance with 27-nation bloc’s data privacy rules. </p><p>The EU court issued its ruling after Austria’s supreme court asked for guidance in Schrems’ case on how to apply the privacy rules, known as the General Data Protection Regulation, or GDPR. </p><p>Schrems had complained that Facebook had processed personal data including information about his sexual orientation to target him with online advertising, even though he had never disclosed on his account that he was gay. The only time he had publicly revealed this fact was during a panel discussion. </p>
    

<p>“An online social network such as Facebook cannot use all of the personal data obtained for the purposes of targeted advertising, without restriction as to time and without distinction as to type of data,” the court said in a press release summarizing its decision. </p>



<p>Even though Schrems revealed he was gay in the panel discussion, that “does not authorise the operator of an online social network platform to process other data relating to his sexual orientation, obtained, as the case may be, outside that platform, with a view to aggregating and analysing those data, in order to offer him personalised advertising.” </p>
    
<p>Meta said it was awaiting publication of the court’s full judgment and that it “takes privacy very seriously.”</p><p>“Everyone using Facebook has access to a wide range of settings and tools that allow people to manage how we use their information,” the company said in a statement. </p>
    

<p>Schrems’ lawyer, Katharina Raabe-Stuppnig, lawyer representing Mr Schrems, welcomed the court’s decision. </p><p>“Meta has basically been building a huge data pool on users for 20 years now, and it is growing every day. However, EU law requires ‘data minimisation’,” she said in a statement. “Following this ruling only a small part of Meta’s data pool will be allowed to be used for advertising — even when users consent to ads.” </p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How were 70s versions of games like Pong built without a programmable computer? (183 pts)]]></title>
            <link>https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra</link>
            <guid>41745032</guid>
            <pubDate>Fri, 04 Oct 2024 19:57:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra">https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra</a>, See on <a href="https://news.ycombinator.com/item?id=41745032">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>They were made by mostly avoiding 'computing' concepts altogether, and treating it more like a mechanical thing.</p>
<p>For example with Pong a major component is usually timers - every xth of a second the timer will emit a signal. You have timers calibrated to match the horizontal refresh of the screen, so they'll 'ring' at the same point on each scanline. Then you have timers calibrated to the vertical refresh, so they'll ring on the same scanline each frame.</p>
<p>The ball is then just two discrete timers for vertical and horizontal position, and their rings are sent through an AND gate that will raise the voltage going to the display when both are ringing causing a white dot to appear. The paddles build on this concept with a medium length timer that can be started and stopped to define the length.</p>
<p>To move the ball or paddles the timers can be advanced or delayed by a control signal. Or more accurately the timers are always paused at a certain point, like during half of the horizontal blanking period. This pausing is then shortened once to advance the timer (move left/up) or increased once to delay it (move right/down).</p>
<p>Since both the paddle and the ball timers are emitting a '1' when they are to be displayed you can impliment collision detection by performing another AND operation. So if both a paddle and the ball are being drawn at the same moment you know they've collided and can adjust a latch controlling the ball direction accordingly.</p>
<p>If you get into the Atari 2600 you'll find that it's really weird compared to other consoles (sprites with no clearly defined X coordinate, instead only the ability to place it at the actual current location of the CRT beam or nudge it a small amount either way), but that it starts to make a lot of sense when you realize they were implementing their Pong logic for a programmable chip.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mitmproxy 11: Full HTTP/3 Support (251 pts)]]></title>
            <link>https://mitmproxy.org/posts/releases/mitmproxy-11/</link>
            <guid>41744434</guid>
            <pubDate>Fri, 04 Oct 2024 18:52:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitmproxy.org/posts/releases/mitmproxy-11/">https://mitmproxy.org/posts/releases/mitmproxy-11/</a>, See on <a href="https://news.ycombinator.com/item?id=41744434">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>We are excited to announce the release of mitmproxy 11, which introduces full support for HTTP/3 in both transparent
and reverse proxy modes. We’re also bringing in a ton of DNS improvements that we’ll cover in this blog post.</p>
<h5 id="editorial-note"><em>Editorial Note:</em></h5>
<p><em>Hi! I’m <a href="https://mitmproxy.org/authors/gaurav-jain/">Gaurav Jain</a>, one of the students selected for this year’s Google Summer of Code program to work on mitmproxy.
During this summer, I’ve worked on improving various low-level networking parts of mitmproxy some of which include
HTTP/3 and DNS. You can find my project report <a href="https://gist.github.com/errorxyz/af6f26549e9122f3ff3b93fd9d257df1">here</a>.</em></p>
<h2 id="http3">HTTP/3</h2>
<p>HTTP/3 now “just works” for reverse proxies. Your mitmproxy instance will listen for
both TCP and UDP packets and handle all HTTP versions thrown at it:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmproxy --mode reverse:https://http3.is
</span></span></code></pre></div><p>Our transparent proxy modes now all support HTTP/3 as well:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmproxy --mode wireguard
</span></span><span><span>$ mitmproxy --mode local
</span></span><span><span>$ mitmproxy --mode transparent
</span></span></code></pre></div><p>We have successfully tested HTTP/3 support with Firefox, Chrome, various cURL builds, and other clients to iron out
compatibility issues.
The only major limitation we are aware of at this time is that Chrome <a href="https://issues.chromium.org/issues/40138351#comment15">does not trust user-added Certificate Authorities for QUIC</a>.
This means you will either need to provide a publicly trusted certificate (e.g. from Let’s Encrypt), start Chrome with
a <a href="https://www.chromium.org/quic/playing-with-quic/#generate-certificates">command line switch</a>, or accept that it falls back to HTTP/2. Alternatively, Firefox doesn’t do such shenanigans.
For more HTTP/3 troubleshooting tips, you can check out <a href="https://github.com/mitmproxy/mitmproxy/issues/7025#issuecomment-2351138170">#7025</a>.</p>
<p>Bringing HTTP/3 support to mitmproxy is a major effort that was started in 2022 by <a href="https://mitmproxy.org/authors/manuel-meitinger/">Manuel Meitinger</a> and <a href="https://mitmproxy.org/authors/maximilian-hils/">Maximilian Hils</a>.
QUIC and HTTP/3 make up an increasing share of network traffic in the wild, and we’re super excited to have this ready
and enabled by default now!</p>
<h2 id="improved-dns-support">Improved DNS Support</h2>
<p>With the advent of DNS <a href="https://blog.cloudflare.com/speeding-up-https-and-http-3-negotiation-with-dns/">HTTPS records</a> and new privacy enhancements such as <a href="https://en.wikipedia.org/wiki/Server_Name_Indication#Encrypted_Client_Hello">Encrypted Client Hello (ECH)</a>, mitmproxy’s DNS
functionality is becoming increasingly important. We’re happy to share multiple advancements on this front:</p>
<h4 id="support-for-query-types-beyond-aaaaa">Support for Query Types Beyond A/AAAA</h4>
<p>mitmproxy’s old DNS implementation used <code>getaddrinfo</code> to resolve queries. This is convenient because everything is taken
care of by libc, but the <code>getaddrinfo</code> API only supports A/AAAA queries for IPv4 and IPv6 addresses. It doesn’t allow us
to answer queries for e.g. <a href="https://blog.cloudflare.com/speeding-up-https-and-http-3-negotiation-with-dns/">HTTPS records</a>, which are used to signal HTTP/3 support.</p>
<p>To overcome this limitation, we’ve reimplemented our DNS support on top of <a href="https://github.com/hickory-dns/hickory-dns">Hickory&nbsp;DNS</a>, a Rust-based DNS library.
Using Hickory, we now obtain the operating system’s default nameservers on Windows, Linux, and macOS and forward
non-A/AAAA queries there. This behavior can also be customized with the new <a href="https://docs.mitmproxy.org/stable/concepts-options/#dns_name_servers"><code>dns_name_servers</code> option</a>:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmdump --mode dns --set dns_name_servers<span>=</span>8.8.8.8
</span></span></code></pre></div><p><img src="https://mitmproxy.org/posts/releases/mitmproxy-11/dns.png" alt="dns"></p>
<h4 id="skipping-etchosts">Skipping /etc/hosts</h4>
<p>By switching to Hickory, we now also have the option to ignore the system’s hosts
file (<code>/etc/hosts</code> on Linux) with the new <a href="https://docs.mitmproxy.org/stable/concepts-options/#dns_use_hosts_file"><code>dns_use_hosts_file</code> option</a>. We plan to move mitmproxy’s internal
DNS resolution to Hickory as well, at which point this feature will become incredibly useful in allowing transparent
redirection on the same machine for specific domains. At the moment, such a setup would cause mitmproxy to recursively
connect to itself because we always take the hosts file into account.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ echo <span>"192.0.2.1 mitmproxy.org"</span> &gt;&gt; /etc/hosts
</span></span><span><span>
</span></span><span><span>$ mitmdump --mode dns
</span></span><span><span>$ dig @127.0.0.1 +short mitmproxy.org
</span></span><span><span>192.0.2.1
</span></span><span><span>
</span></span><span><span>$ mitmdump --mode dns --set dns_use_hosts_file<span>=</span>false
</span></span><span><span>$ dig @127.0.0.1 +short mitmproxy.org
</span></span><span><span>3.161.82.13
</span></span></code></pre></div><h4 id="support-for-dns-over-tcp">Support for DNS-over-TCP</h4>
<p>DNS uses UDP by default, but may also use TCP to support records that do not fit into a single UDP packet. mitmproxy has
previously gotten away with only supporting UDP, but now that we support arbitrary query types, message size and thus
TCP support is more important. Long story short, DNS-over-TCP works with mitmproxy 11!</p>
<h4 id="stripping-encrypted-client-hello-ech-keys">Stripping Encrypted Client Hello (ECH) Keys</h4>
<p>Unless a custom certificate is configured, mitmproxy uses the Server Name Indication (SNI) transmitted in the TLS
ClientHello to construct a valid certificate. Conversely, if no SNI is present, we may not be able
to generate a certificate that is trusted by the client.</p>
<p><a href="https://en.wikipedia.org/wiki/Server_Name_Indication#Encrypted_Client_Hello">Encrypted Client Hello (ECH)</a> is an exciting new technology to increase privacy on the web. In short, the client uses
the new DNS HTTPS records to obtain an ECH key before establishing a connection, and then already encrypts the initial
ClientHello handshake message with that key. If both DNS queries and handshake are encrypted, passive intermediaries
cannot learn the target domain, only the target IP address (which is not conclusive for shared hosting and Content Delivery
Networks). This is a great advancement for privacy, but also breaks mitmproxy’s way of generating certificates.
To fix this, mitmproxy now strips ECH keys from HTTPS records. This way the client has no keys to encrypt the initial
handshake message with, and mitmproxy still learns the target domain and can construct a matching certificate.</p>
<p>Of course, ECH adds complexity for us and sometimes makes mitmproxy harder to use for our users. Nonetheless, we are
excited to see these privacy advancements being made for the rest of the web!</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>This work supported by <a href="https://summerofcode.withgoogle.com/">Google Summer of Code</a> under the umbrella of the <a href="https://www.honeynet.org/">Honeynet&nbsp;Project</a>, and the
<a href="https://nlnet.nl/entrust/">NGI0 Entrust fund</a> established by <a href="https://nlnet.nl/">NLnet</a>. Thank you to my mentor <a href="https://mitmproxy.org/authors/maximilian-hils/">Maximilian Hils</a> for the
invaluable guidance and support.</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: FFmpeg-over-IP – Connect to remote FFmpeg servers (124 pts)]]></title>
            <link>https://github.com/steelbrain/ffmpeg-over-ip</link>
            <guid>41743780</guid>
            <pubDate>Fri, 04 Oct 2024 17:49:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/steelbrain/ffmpeg-over-ip">https://github.com/steelbrain/ffmpeg-over-ip</a>, See on <a href="https://news.ycombinator.com/item?id=41743780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">ffmpeg over IP</h2><a id="user-content-ffmpeg-over-ip" aria-label="Permalink: ffmpeg over IP" href="#ffmpeg-over-ip"></a></p>
<p dir="auto">Connect to remote ffmpeg servers. Are you tired of unsuccessfully trying to pass your GPU through to a docker
container running in a VM? So was I! <code>ffmpeg-over-ip</code> allows you to run an ffmpeg server on a machine with access
to a GPU (Linux, Windows, or Mac) and connect to it from a remote machine. The only thing you need is Node.js
installed and a shared filesystem (could be NFS, SMB, etc.) between the two machines.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><code>ffmpeg-over-ip</code> consists of two main parts, the server and the client. Both are packed neatly into single JS
files. You can download these from the <a href="https://www.npmjs.com/package/ffmpeg-over-ip?activeTab=code" rel="nofollow">npm interface</a> or by <code>npm install ffmpeg-over-ip</code> and then copying
them to the relevant places. You don't need any <code>node_modules</code> to run the server or the client.</p>
<p dir="auto">The javascript files require Node.js runtime to work. If you want standalone files that you can mount in a docker
container, you can find these in the <a href="https://github.com/steelbrain/ffmpeg-over-ip/releases">Github Releases</a>. On the releases page, you may have to click <strong>"Show all
assets"</strong> to see the files.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">The server and the client are both configured using JSONC (JSON with comments) configuration files. The paths
of these files can be flexible. To identify which paths are being used, you can invoke either with <code>--debug-print-search-paths</code>.</p>
<p dir="auto">Template/example configuration files are provided in this repository for your convinience. Unless the server and the client
share the same filesystem, you may have to specify <code>rewrites</code> in the server configuration file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Both the server and the client files are executable, so long as there is a Node.js installation available. If you intend
to use this in a docker container, you can directly mount the client file to where the container would expect a regular
ffmpeg executable to be, ie <code>docker run -v ./path/to/client-bin:/usr/lib/jellyfin-ffmpeg/ffmpeg ...</code>.</p>
<p dir="auto">The server and the client communicate commands over HTTP, so make sure that whatever port you specify on the server is
allowed through the firewall.</p>
<p dir="auto">Assuming you <strong>download one of the release files</strong>, here's what the usage would look like</p>
<p dir="auto">On the client side</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./ffmpeg-over-ip-client --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.client.jsonc # Change the stuff you want
$ ./ffmpeg-over-ip-client <use like ffmpeg, add ffmpeg args here>"><pre>$ ./ffmpeg-over-ip-client --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.client.jsonc <span><span>#</span> Change the stuff you want</span>
$ ./ffmpeg-over-ip-client <span>&lt;</span>use like ffmpeg, add ffmpeg args here<span>&gt;</span></pre></div>
<p dir="auto">On the server side</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./ffmpeg-over-ip-server --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.server.jsonc # Change the stuff you want, especially the rewrites
$ ./ffmpeg-over-ip-server"><pre>$ ./ffmpeg-over-ip-server --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.server.jsonc <span><span>#</span> Change the stuff you want, especially the rewrites</span>
$ ./ffmpeg-over-ip-server</pre></div>
<p dir="auto">Assuming you want to <strong>download these from npm</strong>, here's how you would do it</p>
<p dir="auto">On the client side:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-client --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.client.jsonc # Change the stuff you want
$ ./node_modules/.bin/ffmpeg-over-ip-client <use like ffmpeg, add ffmpeg args here>"><pre>$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-client --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.client.jsonc <span><span>#</span> Change the stuff you want</span>
$ ./node_modules/.bin/ffmpeg-over-ip-client <span>&lt;</span>use like ffmpeg, add ffmpeg args here<span>&gt;</span></pre></div>
<p dir="auto">On the server side:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-server --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.server.jsonc # Change the stuff you want, especially the rewrites
$ ./node_modules/.bin/ffmpeg-over-ip-server"><pre>$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-server --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.server.jsonc <span><span>#</span> Change the stuff you want, especially the rewrites</span>
$ ./node_modules/.bin/ffmpeg-over-ip-server</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The contents of this project are licensed under the terms of the MIT License.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Open source framework OpenAI uses for Advanced Voice (184 pts)]]></title>
            <link>https://github.com/livekit/agents</link>
            <guid>41743327</guid>
            <pubDate>Fri, 04 Oct 2024 17:01:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/livekit/agents">https://github.com/livekit/agents</a>, See on <a href="https://news.ycombinator.com/item?id=41743327">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://github.com/livekit/agents/raw/main/.github/banner_dark.png">
  <source media="(prefers-color-scheme: light)" srcset="https://github.com/livekit/agents/raw/main/.github/banner_light.png">
  <img alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png">
</picture></themed-picture>


<p>
Looking for the JS/TS library? Check out <a href="https://github.com/livekit/agents-js">AgentsJS</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ [NEW] OpenAI Realtime API support</h2><a id="user-content--new-openai-realtime-api-support" aria-label="Permalink: ✨ [NEW] OpenAI Realtime API support" href="#-new-openai-realtime-api-support"></a></p>
<p dir="auto">We're partnering with OpenAI on a new <code>MultimodalAgent</code> API in the Agents framework. This class completely wraps OpenAI’s Realtime API, abstract away the raw wire protocol, and provide an ultra-low latency WebRTC transport between GPT-4o and your users’ devices. This same stack powers Advanced Voice in the ChatGPT app.</p>
<ul dir="auto">
<li>Try the Realtime API in our <a href="https://playground.livekit.io/" rel="nofollow">playground</a> [<a href="https://github.com/livekit-examples/realtime-playground">code</a>]</li>
<li>Check out our <a href="https://docs.livekit.io/agents/openai" rel="nofollow">guide</a> to building your first app with this new API</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is Agents?</h2><a id="user-content-what-is-agents" aria-label="Permalink: What is Agents?" href="#what-is-agents"></a></p>
<p dir="auto">The Agents framework allows you to build AI-driven server programs that can see, hear, and speak in realtime. Your agent connects with end user devices through a LiveKit session. During that session, your agent can process text, audio, images, or video streaming from a user's device, and have an AI model generate any combination of those same modalities as output, and stream them back to the user.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Plugins for popular LLMs, transcription and text-to-speech services, and RAG databases</li>
<li>High-level abstractions for building voice agents or assistants with automatic turn detection, interruption handling, function calling, and transcriptions</li>
<li>Compatible with LiveKit's <a href="https://github.com/livekit/sip">telephony stack</a>, allowing your agent to make calls to or receive calls from phones</li>
<li>Integrated load balancing system that manages pools of agents with edge-based dispatch, monitoring, and transparent failover</li>
<li>Running your agents is identical across localhost, <a href="https://github.com/livekit/livekit">self-hosted</a>, and <a href="https://cloud.livekit.io/" rel="nofollow">LiveKit Cloud</a> environments</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To install the core Agents library:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install livekit-agents"><pre>pip install livekit-agents</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Plugins</h2><a id="user-content-plugins" aria-label="Permalink: Plugins" href="#plugins"></a></p>
<p dir="auto">The framework includes a variety of plugins that make it easy to process streaming input or generate output. For example, there are plugins for converting text-to-speech or running inference with popular LLMs. Here's how you can install a plugin:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install livekit-plugins-openai"><pre>pip install livekit-plugins-openai</pre></div>
<p dir="auto">The following plugins are available today:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Plugin</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-anthropic/" rel="nofollow">livekit-plugins-anthropic</a></td>
<td>LLM</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-azure/" rel="nofollow">livekit-plugins-azure</a></td>
<td>STT, TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-deepgram/" rel="nofollow">livekit-plugins-deepgram</a></td>
<td>STT</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-cartesia/" rel="nofollow">livekit-plugins-cartesia</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-elevenlabs/" rel="nofollow">livekit-plugins-elevenlabs</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-playht/" rel="nofollow">livekit-plugins-playht</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-google/" rel="nofollow">livekit-plugins-google</a></td>
<td>STT, TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-nltk/" rel="nofollow">livekit-plugins-nltk</a></td>
<td>Utilities for working with text</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-rag/" rel="nofollow">livekit-plugins-rag</a></td>
<td>Utilities for performing RAG</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-openai/" rel="nofollow">livekit-plugins-openai</a></td>
<td>LLM, STT, TTS, Assistants API, Realtime API</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-silero/" rel="nofollow">livekit-plugins-silero</a></td>
<td>VAD</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation and guides</h2><a id="user-content-documentation-and-guides" aria-label="Permalink: Documentation and guides" href="#documentation-and-guides"></a></p>
<p dir="auto">Documentation on the framework and how to use it can be found <a href="https://docs.livekit.io/agents" rel="nofollow">here</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example agents</h2><a id="user-content-example-agents" aria-label="Permalink: Example agents" href="#example-agents"></a></p>
<ul dir="auto">
<li>A basic voice agent using a pipeline of STT, LLM, and TTS [<a href="https://kitt.livekit.io/" rel="nofollow">demo</a> | <a href="https://github.com/livekit/agents/blob/main/examples/voice-pipeline-agent/minimal_assistant.py">code</a>]</li>
<li>Voice agent using the new OpenAI Realtime API [<a href="https://playground.livekit.io/" rel="nofollow">demo</a> | <a href="https://github.com/livekit-examples/realtime-playground">code</a>]</li>
<li>Super fast voice agent using Cerebras hosted Llama 3.1 [<a href="https://cerebras.vercel.app/" rel="nofollow">demo</a> | <a href="https://github.com/dsa/fast-voice-assistant/">code</a>]</li>
<li>Voice agent using Cartesia's Sonic model [<a href="https://cartesia-assistant.vercel.app/" rel="nofollow">demo</a>]</li>
<li>Agent that looks up the current weather via function call [<a href="https://github.com/livekit/agents/blob/main/examples/voice-pipeline-agent/function_calling_weather.py">code</a>]</li>
<li>Voice agent that performs a RAG-based lookup [<a href="https://github.com/livekit/agents/tree/main/examples/voice-pipeline-agent/simple-rag">code</a>]</li>
<li>Video agent that publishes a stream of RGB frames [<a href="https://github.com/livekit/agents/tree/main/examples/simple-color">code</a>]</li>
<li>Transcription agent that generates text captions from a user's speech [<a href="https://github.com/livekit/agents/tree/main/examples/speech-to-text">code</a>]</li>
<li>A chat agent you can text who will respond back with genereated speech [<a href="https://github.com/livekit/agents/tree/main/examples/text-to-speech">code</a>]</li>
<li>Localhost multi-agent conference call [<a href="https://github.com/dsa/multi-agent-meeting">code</a>]</li>
<li>Moderation agent that uses Hive to detect spam/abusive video [<a href="https://github.com/dsa/livekit-agents/tree/main/hive-moderation-agent">code</a>]</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's <a href="https://livekit.io/join-slack" rel="nofollow">Slack community</a>.</p>

<markdown-accessiblity-table><table>
<thead><tr><th colspan="2">LiveKit Ecosystem</th></tr></thead>
<tbody>
<tr><td>Realtime SDKs</td><td><a href="https://github.com/livekit/client-sdk-js">Browser</a> · <a href="https://github.com/livekit/client-sdk-swift">iOS/macOS/visionOS</a> · <a href="https://github.com/livekit/client-sdk-android">Android</a> · <a href="https://github.com/livekit/client-sdk-flutter">Flutter</a> · <a href="https://github.com/livekit/client-sdk-react-native">React Native</a> · <a href="https://github.com/livekit/rust-sdks">Rust</a> · <a href="https://github.com/livekit/node-sdks">Node.js</a> · <a href="https://github.com/livekit/python-sdks">Python</a> · <a href="https://github.com/livekit/client-sdk-unity">Unity</a> · <a href="https://github.com/livekit/client-sdk-unity-web">Unity (WebGL)</a></td></tr><tr></tr>
<tr><td>Server APIs</td><td><a href="https://github.com/livekit/node-sdks">Node.js</a> · <a href="https://github.com/livekit/server-sdk-go">Golang</a> · <a href="https://github.com/livekit/server-sdk-ruby">Ruby</a> · <a href="https://github.com/livekit/server-sdk-kotlin">Java/Kotlin</a> · <a href="https://github.com/livekit/python-sdks">Python</a> · <a href="https://github.com/livekit/rust-sdks">Rust</a> · <a href="https://github.com/agence104/livekit-server-sdk-php">PHP (community)</a></td></tr><tr></tr>
<tr><td>UI Components</td><td><a href="https://github.com/livekit/components-js">React</a> · <a href="https://github.com/livekit/components-android">Android Compose</a> · <a href="https://github.com/livekit/components-swift">SwiftUI</a></td></tr><tr></tr>
<tr><td>Agents Frameworks</td><td><b>Python</b> · <a href="https://github.com/livekit/agents-js">Node.js</a> · <a href="https://github.com/livekit/agent-playground">Playground</a></td></tr><tr></tr>
<tr><td>Services</td><td><a href="https://github.com/livekit/livekit">LiveKit server</a> · <a href="https://github.com/livekit/egress">Egress</a> · <a href="https://github.com/livekit/ingress">Ingress</a> · <a href="https://github.com/livekit/sip">SIP</a></td></tr><tr></tr>
<tr><td>Resources</td><td><a href="https://docs.livekit.io/" rel="nofollow">Docs</a> · <a href="https://github.com/livekit-examples">Example apps</a> · <a href="https://livekit.io/cloud" rel="nofollow">Cloud</a> · <a href="https://docs.livekit.io/home/self-hosting/deployment" rel="nofollow">Self-hosting</a> · <a href="https://github.com/livekit/livekit-cli">CLI</a></td></tr>
</tbody>
</table></markdown-accessiblity-table>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[12 Months of Mandarin (416 pts)]]></title>
            <link>https://isaak.net/mandarin/</link>
            <guid>41742432</guid>
            <pubDate>Fri, 04 Oct 2024 15:28:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://isaak.net/mandarin/">https://isaak.net/mandarin/</a>, See on <a href="https://news.ycombinator.com/item?id=41742432">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <p>Estimates for achieving intermediate fluency in Mandarin Chinese range up to spending years and around 4000 total hours (2,200h classroom hours, 1,800 outside). I did it in 1500 hours total and less than a year.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>There is a lot of disagreement on language proficiency estimates. They are unreliable and inaccurate. My rough best estimates:<br>
Mean: <strong>2,200h classroom + ~1800h outside -&gt; 50% pass ILR 3 -&gt; true average level is ~ILR 2+</strong><br>
My journey: <strong>150h classroom + 1350h outside -&gt; 50/50 shot at passing ILR 3 exam -&gt; true average level ILR 2+</strong><br>
Detailed walkthrough: One of our best sources, the US Department of State, sadly has a lot of content under NDAs, including exams. That said, they <a href="https://www.state.gov/foreign-language-training/?ref=isaak.net">estimate</a> that reaching "General Professional Fluency" (<a href="https://www.govtilr.org/Skills/ILRscale2.htm?ref=isaak.net">ILR 3</a>) takes 2,200 <em>classroom</em> hours over 88 weeks. This is 2,200/88 = 25 hours a week. Diplomats describe studying a language at the DOS as "the hardest thing you will do in your life". <em>Classroom</em> hours do not include the heavy homework and content consumption. For them, this is not a 25h part-time breeze. It is life.<br>
What the program sounds like, it takes more like 45h a week for 88 weeks (3096h). Add some private language practice, travelling, and content consumption, and studying for the actual exam (which seems brutal). Now, getting to ILR3 fluency seems like taking <strong>well above 4000 hours</strong>, all included.<br>
This is being good on paper. These kinds of language proficiency levels tend to fall short in real-life. In fact, they fall short even on paper: Only some <a href="https://www.reddit.com/r/languagelearning/comments/rd19bj/success_rates_in_2011_and_2012_of_the_fsi_at/?ref=isaak.net">50% pass</a> the ILR3 test after the 88 weeks. In additio, tests are a mediocre proxy for actual communication ability. My sense from reading online is that the diplomats at ILR3 on paper have only truly mastered something like ILR2.<br>
Similarly, I spent <strong>some 1,500 hours total</strong>, including everything like classroom hours, tutoring hours, content consumption, and conversations. On paper, I would guess I am at ILR3, but that is probably an overestimate. So call it ILR2+.<br>
To be fair, also take generously take some time off that 4000-hour estimate: Call it usually takes <strong>3000 hours to get to a similarly strong</strong> ILR2+ level / on-paper ILR3. Hence the 3000h vs 1500h comparison.<br>
This is my rough outline of the core estimate. I do not really care enough to get into more detail or studying for passing tests. I just enjoy learning the language and using it. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>Over the last 365 days, I studied Mandarin for fun. With anki, tutors, and traveling accelerating my learning, I ended up getting to the level of comfortable conversational fluency. My Mandarin isn't perfect nor perfectly fluent, but I can now handle everything up to technical conversations in the area of my PhD.&nbsp;</p><p>For serious language learners, I also jotted down a longer list of methodds: <a href="https://isaak.net/mandarinmethods/">isaak.net/mandarinmethods</a></p><h2 id="humble-beginnings-%E2%80%94-%E7%AD%9A%E8%B7%AF%E8%93%9D%E7%BC%95">Humble Beginnings — 筚路蓝缕</h2><p><strong>Month 1:</strong> Last September, I was deep into my math undergrad. It was pretty dry. I was looking for some fun non-math side project.<sup><a href="#fn1" id="fnref1">[1]</a></sup> I flirted with French, Russian, archery, parkour, and Japanese. But those didn’t ignite my passion. I happened to watch a snippet of the anime Demon Slayer in an obscure <a href="https://www.youtube.com/watch?v=-VoT0TY0emM&amp;ref=isaak.net">Chinese fan dub</a>. Ironically, this caught my attention. I also had lots of Chinese friends, so why not learn a little Mandarin? Oh my, I had no idea how obsessed I'd end up with this "little" side project.</p>
<p>Berkeley had a breakneck-speed Mandarin beginner class. I loved it. Within a week, we learned pinyin. We learned the tones. We learned to read. We learned to write. Then started talking immediately, every single day. Talking in horribly horribly broken Chinese, but nevertheless having conversations.<sup><a href="#fn2" id="fnref2">[2]</a></sup> I learned the very most important survival vocabulary, like: <em>I am Isaak</em> and <em>Yes, I live in America</em> and <em>Sorry, no, I’m not a basketball player for the Golden State Warriors</em>.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>I had virtually no math background, so I spent a lot of my time studying math from absolute scratch. It was brutal. It was rewarding. It also was very dry. Many months of many proofs, many \[ and some forgotten \], many months of many ∴ and many more □. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>Between running from Chinese class to discrete math classes, I’d be staring into the orange California sunset sky, and mumbling random Demon Slayer anime phrases to myself in Chinese, like "Lightning Breath - First Slash and Lightning" (雷之呼吸，一之型，霹雳一闪!). Oops... <a href="#fnref2">↩︎</a></p>
</li>
</ol>
</section>
<figure><img src="https://isaak.net/content/images/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg" alt="" loading="lazy" width="2000" height="1394" srcset="https://isaak.net/content/images/size/w600/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 600w, https://isaak.net/content/images/size/w1000/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 1000w, https://isaak.net/content/images/size/w1600/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 1600w, https://isaak.net/content/images/size/w2400/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption><i><em>Sorry no, I’m not Steph Curry. But come chat anyway! (Qingyuan, China)</em></i></figcaption></figure><p>The beginning was by far the hardest time, and many tuned out or dropped out. But I had lots of fun. I played a lot. I wrote a horrible poem about humanity colonizing Mars. My Chinese was absolute crap, but I was improving <em>fast</em>. Chinese is my fifth language, and I had a few tricks up my sleeve:</p><h2 id="intense-self-study-%E2%80%94-%E8%87%AA%E5%BC%BA%E4%B8%8D%E6%81%AF">Intense Self-Study — 自强不息</h2><p><strong>Month 3: </strong>Spaced repetition is a superweapon. The spaced repetition app Anki is the core reason why I was able to study Chinese efficiently. Alongside Anki, I adopted other methods to learn <em>faster</em>. </p><p>Frequency-based learning. Comprehensible input. Reading lots as soon as I could, especially graded readers. Buying a calligraphy pen-brush and learned how to write the 600 Chinese characters. FSRS. Creating a 100,000-card Anki megadeck. For all the nitty-gritty language learning tips, check out <a href="https://isaak.net/mandarinmethods/" rel="noreferrer">my methods post</a>. </p><p>Early on, I started watching anime dubs like Boruto or Scissor Seven. I really enjoyed myself despite barely understanding anything. Every few minutes I collected a new word to study. The content I watched in those early days felt like colorful images with funny sounds which occasionally made sense.&nbsp;</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdkzR7VAlJdrPr9X0XuxXJFcdOB9gnxfeRHyrtUpagDZ2krk5NsaA9653_HhAGhzu1L4LmF-vgN9e_rkNWXRd6CRNZex7DUk9SFfjqcITATrrGw8Nx_BcWKjrvXo06bFKYRAtk6bLBYaLLsZ_bK-LIu2ezL?key=va-1OSyREuqkqYOZib3JNw" alt="" loading="lazy" width="379" height="213"><figcaption><i><em>Colorful images with funny sounds which occasionally make sense</em></i></figcaption></figure><p>On day 70 I reached a vocabulary of 1050, of which 460 characters.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>I don’t study for tests and never took one. Yet, note that in the major Chinese system to track language progress, the fourth tier out of six (old HSK4) requires a vocabulary of roughly 1200. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>The other superweapon I implemented was personalized tutoring. My first month studying Chinese was mostly in a 20-people class. But then, I took Bloom's Two-Sigma effect to heart and got myself lots of 1-1 tutoring. The more time I spent on tutoring, the more it accelerated my studies.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Bloom%27s_2_sigma_problem?ref=isaak.net">Bloom’s Two-Sigma</a> effect is the phenomenon of tutored students vastly outperforming students in normal classes. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>There’s legends like <a href="https://www.chinese-forums.com/forums/topic/43939-independent-chinese-study-review/?ref=isaak.net"><u>Tamu</u></a> spending dozens of hours with tutors, but I’d mostly spend up to six hours a week. More would start to detract from my main focus, which were still my math studies. My default for working with tutors was to lead a "normal" conversation. I had two strict rules for conversations with tutors: 1. Only Chinese, no English. 2. Correct every single mistake I make. </p><figure><img src="https://isaak.net/content/images/2024/10/IMG_3800.jpeg" alt="" loading="lazy" width="2000" height="2042" srcset="https://isaak.net/content/images/size/w600/2024/10/IMG_3800.jpeg 600w, https://isaak.net/content/images/size/w1000/2024/10/IMG_3800.jpeg 1000w, https://isaak.net/content/images/size/w1600/2024/10/IMG_3800.jpeg 1600w, https://isaak.net/content/images/size/w2400/2024/10/IMG_3800.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption><i><em>I definitely lost all my social anxiety after 3 days of walking around tourist-packed Hawaii beaches loudly talking broken Chinese</em></i></figcaption></figure><p>At the start, this tutoring was excruciatingly slow. But it was very worth it. After the chat, I’d ask them to send me a summary of my key mistakes and newly learned vocabulary. It’d add that to my Anki.&nbsp;</p><p>I made lots of mistakes. I still do. Tutoring gives me a tight and fast feedback loop on fixing my mistakes.</p><h2 id="traveling-%E2%80%94-%E5%AD%A6%E4%BB%A5%E8%87%B4%E7%94%A8">Traveling — 学以致用</h2><p><strong>Month 4:</strong> Winter break was approaching. I knew all the Chinese I needed to know to travel. (<em>No, sorry, I’m not LeBron James.</em>) I figured out tourist visas, and just went for it. After Christmas in chafing-lips-freezing-cold Austria, I found myself wandering around fogged-up-glasses-humid Taipei.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>Winter in Taiwan was actually comfortable. Summer in Shanghai was crazy. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<figure><div><p><img src="https://isaak.net/content/images/2024/10/IMG_3882.jpeg" width="2000" height="2053" loading="lazy" alt="" srcset="https://isaak.net/content/images/size/w600/2024/10/IMG_3882.jpeg 600w, https://isaak.net/content/images/size/w1000/2024/10/IMG_3882.jpeg 1000w, https://isaak.net/content/images/size/w1600/2024/10/IMG_3882.jpeg 1600w, https://isaak.net/content/images/2024/10/IMG_3882.jpeg 2316w" sizes="(min-width: 720px) 720px"></p><p><img src="https://isaak.net/content/images/2024/10/conan-glasses.png" width="883" height="768" loading="lazy" alt="" srcset="https://isaak.net/content/images/size/w600/2024/10/conan-glasses.png 600w, https://isaak.net/content/images/2024/10/conan-glasses.png 883w" sizes="(min-width: 720px) 720px"></p></div><figcaption><p><i><em>Too humid to see, too humid to sweat, too cool to break a sweat </em></i><span>😎</span></p></figcaption></figure><p>I usually like to travel alone, and then figure out things on the spot. I’d walk around with airpods in, sugar-shocked from eating too many sugar gourds, explaining to my tutor in great detail what tasty novel things I ate at the night market. When not on a tutoring call, I’d sit in cafes and study, wander around markets, or talk to locals. </p><p>Being a foreigner with passable Mandarin is... amusing. When meditating in a small Buddhist temple town in middle-of-nowhere rural Sichuan, I was a local celebrity. 300 primary schooler filed past me, who definitely hadn't seen an Austrian-African foreigner speaking Mandarin. They totally lost it. It was fun, but also tiring. Eventually I preferred to be in the cities, where being a foreigner wasn’t a miracle.&nbsp;</p><p>At least, I got a good taste of why being famous must be great for exactly three minutes, and then quite frankly horrible forever after. Not again. I’m okay, thanks.</p><p>In total, I was in Mainland China three times this year, for a total of two months.<sup><a href="#fn1" id="fnref1">[1]</a></sup> It goes without saying that every journey gave me an enormous boost in my learning pace. The first time travelling got me from <em>broken</em> to <em>comfortable in all day-to-day situations</em>. Every time I travelled I learned roughly 1000 new words/characters. Every time immensely boosted my fluent expression and listening ability.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>I ended up going to China again fairly soon, in Spring break. This time I went with an adventure-hungry Austrian friend who was also learning Chinese, and we pushed each other to get better and better. In total, I’m lucky to have spent almost 2 months of this year traveling and working remote, most of that time in Shanghai. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfU23htFShMc4vmV-YwC2T4CE9qS_IPMTfnOep7B9g2MA-pXbecj4NJ3BB7BfuBLmAmSQ7NXGGUTjgwjzY6ejE6umiSeMRmT-O08hyAhaBLeXgdAdBm_7TUWinNUPsVUtDOWNYN9AjYLKqsUMb9cj2C3JAh?key=va-1OSyREuqkqYOZib3JNw" alt="" loading="lazy" width="194" height="145"><figcaption><i><em>Hot single qi sources near you (Emeishan, Sichuan)</em></i></figcaption></figure><h2 id="the-marathon-%E2%80%94-%E6%8C%81%E4%B9%8B%E4%BB%A5%E6%81%92">The Marathon — 持之以恒</h2><p><strong>Month 6:</strong> My Chinese still had far to go. Apart from the study sprints while traveling, I tried to keep up a consistently high pace back at home. Chinese wasn’t my focus then — math and neuro were. Chinese was consistently the largest side project, clocking some 15 hours a week.</p><p>Consistency was the most important part to keep a high pace of progress. Here’s what a typical focused day might’ve looked like:</p><ul><li>Wake up, 1 hour of Anki</li><li>Do my main thing for 8-9 hours (math undergrad, neuro grad school, …)</li><li>1 hour tutoring call before dinner some days</li><li>1 hour of Chinese content before sleep, e.g. anime dubs or books</li></ul><p>It was quite literally the marathon. Here’s my habitually doing Anki on a treadmill:</p><figure data-kg-thumbnail="https://isaak.net/content/media/2024/10/running-short-squarish-compressed-2_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://isaak.net/content/media/2024/10/running-short-squarish-compressed-2.mp4" poster="https://img.spacergif.org/v1/1080x1350/0a/spacer.png" width="1080" height="1350" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:11</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://isaak.net/content/media/2024/10/running-short-squarish-compressed-2_thumb.jpg"></figure><p>Some 7 months from start, I reached 5,000 known words/characters. The old highest level (HSK6) also required a vocabulary of 5,000 (different) words. So this was an epic goal to hit.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>Again, I wasn't taking studying for tests — I was studying for myself. But to compare, the (old) HSK6 requiring a vocabulary of 5,000 words and characters. Most of HSK6 is business vocabulary that's not useful to me. My goal was to pass my “personal HSK6”: 5,000 words which showed up commonly in the content I watched and loved. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>Immersion is best done while traveling. Still, I started to immerse myself as well as I could at home. My devices would be in Chinese. I started taking some notes primarily in Mandarin. I had lots of social support throughout too: I was lucky to be able to build new relationships entirely in Chinese. For example, my grad school supervisor, a Tsinghua graduate now at MIT, was more than happy to teach me neuroscience in Chinese. I had tutors. I turned older relationships fully Chinese. This had me constantly speaking Mandarin day-to-day.</p><p><strong>Month 12: </strong>Exactly 365 days after I started, I reached a vocabulary of 8000 words and characters.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdSe8Wt-7sEziRiPjp_mO0JZ6rHEK_kdZkBDlxJZ0e-7HBiN__o1wAXc8Fsdb8_Nu20BQTQWKO841-ADHl7o7VO1RB9XEtcyuQiQXyCRD5N4jYXjDtM76t_rpyLBg7dro0U04raEzKooGKAiPfYzuND38_U?key=va-1OSyREuqkqYOZib3JNw" alt="" loading="lazy" width="624" height="63"></figure><p>8000 words and characters makes most content I encounter relatively understandable. My vocabulary is a weird personal mix: Basics including everything up to HSK5, anime vocabulary, biology, mathematics, and random everyday stuff from travelling.</p><p>Vocabulary is only one part of fluency. It's important to keep eyes on the goal: The goal of any language is to communicate effectively. Prompted for feedback on my progress, my usually reserved tutor recently commented: “This was the fastest learning pace from zero to advanced conversational fluency I have ever seen." </p><p>That's kind, but being<em> fluent </em>feels like it’ll always be an overstatement. Especially for Chinese. I’m definitely not <em>Fluent™</em>. I sometimes still get my tones wrong. Full-speed native speech is sometimes still tough. Local dialects remain a complete mystery to me.</p><p>I’d say I’m <em>comfortable </em>with Chinese. I can <em>comfortably</em> travel in any Mandarin-speaking place. I can <em>comfortably </em>hold long conversations. I can <em>comfortably </em>watch most content. I can <em>comfortably </em>build relationships entirely in Mandarin.&nbsp;</p><p>The goal? I want to reach a level where the legendary Three-Body Problem will be <em>comfortably </em>readable.</p><hr><p>Curious about more? Check out my methods post: <a href="https://isaak.net/mandarinmethods/">isaak.net/mandarinmethods</a></p>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: One – A new React framework unifying web, native and local-first (450 pts)]]></title>
            <link>https://onestack.dev</link>
            <guid>41742278</guid>
            <pubDate>Fri, 04 Oct 2024 15:15:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onestack.dev">https://onestack.dev</a>, See on <a href="https://news.ycombinator.com/item?id=41742278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Creating websites and&nbsp;apps is simply too complex.</p><p>One is a new React framework for web and<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="blue" aria-describedby="tooltip-content" role="button" tabindex="0"><span>native</span></span>, built on Vite. It&nbsp;simplifies things with<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="green" aria-describedby="tooltip-content" role="button" tabindex="0"><span>universal</span></span>, <a href="https://onestack.dev/docs/routing" role="link">typed routing</a> seamlessly across<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="purple" aria-describedby="tooltip-content" role="button" tabindex="0"><span>static</span></span>,<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="red" aria-describedby="tooltip-content" role="button" tabindex="0"><span>server</span></span>, and<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="pink" aria-describedby="tooltip-content" role="button" tabindex="0"><span>client</span></span> <!-- -->pages. Plus, an amazing new solution&nbsp;to&nbsp;data.</p><div id="zero"><p>Local, First</p><p>Simpler code, better results, cross-platform — that's&nbsp;the goal.</p><p>With One and <a href="https://tamagui.dev/">Tamagui</a>, we're close… but there's still <em>one</em> big pain point.<!-- --> <b>Let's&nbsp;talk&nbsp;about&nbsp;data</b>.</p><p>Native apps feel better and are easier to write thanks to client-side databases. Say&nbsp;bye&nbsp;to server boundaries, lose&nbsp;the glue code, mutate instantly, and have things Just&nbsp;Work™&nbsp;offline…</p><p>So, <b>why don't we use them on&nbsp;the&nbsp;web?</b></p><p>Well, web needs small bundles, and has limited storage. Add in sync, caching, composition… there's 0 great options.</p><p>It's why we're excited to partner with<!-- --> <b><a target="_blank" href="https://zerosync.dev/" role="link">Zero</a></b> <!-- -->to include it as the default, ejectable solution to data. Zero solves for all the above <a href="https://onestack.dev/docs/data" role="link">and&nbsp;more</a>. It even works with&nbsp;Postgres.</p><p>One<!-- --> <span><svg viewBox="0 0 590 590" width="20.65" height="20.65" style="border-radius:1000px;overflow:hidden;width:20.65px;height:20.65px"><svg width="590px" height="590px" viewBox="0 0 590 590"><defs><filter x="-93.3%" y="-81.2%" width="286.7%" height="262.4%" filterUnits="objectBoundingBox" id="filter-1"><feGaussianBlur stdDeviation="22" in="SourceGraphic"></feGaussianBlur></filter><filter x="-13.5%" y="-46.9%" width="126.9%" height="193.8%" filterUnits="objectBoundingBox" id="filter-2"><feGaussianBlur stdDeviation="20" in="SourceGraphic"></feGaussianBlur></filter><filter x="-23.9%" y="-22.5%" width="147.8%" height="145.1%" filterUnits="objectBoundingBox" id="filter-3"><feGaussianBlur stdDeviation="41" in="SourceGraphic"></feGaussianBlur></filter></defs><g id="favicon" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g transform="translate(-0, 0)" fill-rule="nonzero"><circle id="Oval" fill="#F5CA05" cx="295" cy="295" r="295"></circle><circle id="Oval" fill="#FFFFFF" cx="311" cy="230" r="110"></circle><path d="M299.32294,286 L339.7951,281.25 C342.598367,279.138889 344,276.324074 344,272.805556 C344,269.287037 342.247958,267 338.743875,265.944444 L329.282851,265.944444 L321.398664,178.333333 C320.347439,173.055556 318.244989,172 312.988864,172 C307.732739,172 305.63029,173.583333 304.053452,175.166667 C302.476615,176.75 302.476615,182.555556 301.951002,183.611111 C301.42539,184.666667 291.438753,188.361111 287.759465,189.944444 C284.080178,191.527778 284.080178,201.555556 287.759465,203.666667 C291.438753,205.777778 298.271715,202.083333 301.42539,204.722222 L307.207127,267.527778 C299.339925,268.871363 294.784617,270.102844 293.541203,271.222222 C291.676081,272.901289 290.91314,274.388889 291.438753,279.666667 C291.789161,283.185185 294.417223,285.296296 299.32294,286 Z" id="Path-7" fill="#000000"></path><ellipse id="Oval" fill="#FFFFFF" opacity="0.453031994" filter="url(#filter-1)" transform="translate(200.0089, 137.737) rotate(46) translate(-200.0089, -137.737)" cx="200.008945" cy="137.73703" rx="35.3577818" ry="40.6350626"></ellipse><path d="M521,138 C482.503431,98.2196247 448.723549,71.1799277 419.660355,56.880909 C376.065564,35.432381 347.543959,28.4841486 295.097041,26.8563104 C242.650124,25.2284722 225.598176,37.942459 183.728994,56.880909 C155.816206,69.5065424 119.573208,92.6834255 75,126.411558 C102.798028,89.5392443 133.411045,63.0262947 166.839053,46.8727095 C216.981065,22.6423316 259.733728,10 295.097041,10 C330.460355,10 373.740828,20.0085949 428.633136,46.8727095 C465.228008,64.7821192 496.016963,95.1578827 521,138 Z" id="Path-8" fill="#FFFFFF" opacity="0.773065476" filter="url(#filter-2)"></path><path d="M361.057245,44 C431.694309,123.939704 467.935264,174.984191 469.780109,197.133462 C472.547375,230.357369 482.654123,254.819372 459.752272,321.224371 C436.85042,387.629371 415.418677,407.823985 383.224042,440.562863 C361.760952,462.388781 259.019605,478.230174 75,488.087041 C207.883501,556.029014 286.171,590 309.862498,590 C333.553996,590 368.739389,581.727273 415.418677,565.181818 C481.196175,535.021945 525.881624,499.994866 549.475024,460.10058 C584.865123,400.259152 591.955643,340.867492 589.586372,292.181818 C587.2171,243.496144 582.366118,196.314838 555.280613,172.31528 C537.223611,156.315576 472.482488,113.543815 361.057245,44 Z" id="Path-9" fill="#000000" opacity="0.0963076637" filter="url(#filter-3)"></path></g></g></svg></svg></span> <!-- -->is working to make Zero great on server and client. Our proof of concept has no flickers, waterfalls, or config.</p><p>We love it, and think you will too.</p><p><span></span><a href="https://onestack.dev/docs/data" role="link"><span>Read More</span></a></p></div><a target="_blank" href="https://testflight.apple.com/join/aNcDUHZY" role="link"><div><p><img width="80" height="80" src="https://onestack.dev/testflight.webp" alt="Testflight Icon"></p><p>Try the demo app on Testflight</p></div></a><div><p>Team</p><p>Hello. We're the creators of <a href="https://tamagui.dev/" role="link">Tamagui</a>. We built One out of our experience at<!-- --> <a href="https://app.uniswap.org/" role="link">Uniswap</a> and creating<!-- --> <a href="https://tamagui.dev/takeout" role="link">Takeout</a>.</p></div><div><p>Copyright 2024 Tamagui, LLC</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting my daily news from a dot matrix printer (803 pts)]]></title>
            <link>https://aschmelyun.com/blog/getting-my-daily-news-from-a-dot-matrix-printer/</link>
            <guid>41742210</guid>
            <pubDate>Fri, 04 Oct 2024 15:08:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aschmelyun.com/blog/getting-my-daily-news-from-a-dot-matrix-printer/">https://aschmelyun.com/blog/getting-my-daily-news-from-a-dot-matrix-printer/</a>, See on <a href="https://news.ycombinator.com/item?id=41742210">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>For a while now I've started my day by unlocking my phone and scrolling through different news and social media sites to see what's going on. It's not exactly <em>great</em> for my mental health and I've been trying to cut down on screen time for a while. I still want to stay up-to-date though, especially after I get up in the morning.</p>
<p>I recently purchased a dot matrix printer from eBay, and thought it would be a great excuse to have a custom "front page" printed out and ready for me each day. So, that's what I built!</p>
<p>Printer ASMR noises in the video below 👇</p>
<video width="270" height="360" controls="" title="Dotmatrix Printing Example">
  <source src="https://aschmelyun.com/storage/images/blog/dotmatrix_example_2_bt709.mp4" type="video/mp4">
</video>
<p>I'll take this article to dive in and show you what I used, how I set it up, and the <strong>PHP script</strong> that powers it all.</p>
<blockquote>
<p>Interested in that full source code? Check it out on the <a href="https://github.com/aschmelyun/dotmatrix-daily-news">GitHub repo</a>!</p>
</blockquote>
<h2>Purchasing the hardware</h2>
<p>The supply list for this project was pretty small, and with the exception of the printer, most of this can be found on Amazon or other online retailers.</p>
<ul>
<li>Dot matrix printer</li>
<li>Raspberry Pi Zero W [<a href="https://vilros.com/products/raspberry-pi-zero-w-basic-starter-kit-1">link</a>]</li>
<li>Serial to USB adapter [<a href="https://www.amazon.com/dp/B00IDU0T1Y?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1">link</a>]</li>
<li>Power supply</li>
</ul>
<p>The printer I purchased was a <a href="https://www.computerhistory.org/collections/catalog/102666267">Star NP-10</a> from what looks like the mid-80's. I can't be 100% sure, but any dot matrix printer with a serial port should do the trick. The prices range from about $80-120 USD, but I was able to get this one for about half that price because it was marked as "unsure if working".</p>
<p>It did need a little cleaning up and some tuning of the ink ribbon cartridge (isn't that cool, it's like a typewriter!), but after that it fired right up and ran through the test page print.</p>
<p>After that, I hooked everything up. The Raspberry Pi is connected to my WiFi, and then via USB to the serial port of the printer. After turning on the printer and <code>ssh</code>ing into the Pi, I can verify that the printer is available at <code>/dev/usb/lp0</code>.</p>
<p>Now, <strong>how do I get this thing to print?</strong></p>
<h2>Figuring out the printer's code</h2>
<p>Because the printer is available at <code>lp0</code> I wanted to see if I could just echo raw text to it and have it come through the printer. So I ran the following:</p>
<pre><code data-theme="material-theme-palenight" data-lang="bash"><!-- Syntax highlighted by torchlight.dev --><p><span>echo</span><span> </span><span>"</span><span>Hello, world!</span><span>"</span><span> </span><span>&gt;</span><span> </span><span>/dev/usb/lp0</span></p></code></pre>
<p>Which resulted in an error that the file couldn't be accessed. Bummer, a permissions issue. Easily fixed though with some <code>chmod</code>'ing:</p>
<pre><code data-theme="material-theme-palenight" data-lang="bash"><!-- Syntax highlighted by torchlight.dev --><p><span>sudo</span><span> </span><span>chmod</span><span> </span><span>666</span><span> </span><span>/dev/usb/lp0</span></p></code></pre>
<p>There might be a better way to handle that, but it allowed my echo to go through, and I saw the text available on the printer! Alright, I can send raw data to the printer via this file, so let's find a way to scale this up.</p>
<p>I use PHP as my language of choice in a day-to-day basis, and this is no exception. I write a basic script that accesses the file through <code>fopen()</code> and starts writing text to it. I try a few sentences, some spacing, and then some unicode art, but quickly find out that there's not as much character support on the printer as I was sending.</p>
<p><img src="https://aschmelyun.com/storage/images/blog/dotmatrix-encoding-errors.jpg" alt="Picture of a printed sheet showing a bunch of wrongly-encoded characters"></p>
<p>So I thought it was about time that I start digging into how this thing <em>actually works</em>. Thanks to the hard work and dedication of internet hoarders, I found a <a href="https://www.minuszerodegrees.net/manuals/Star%20Micronics/dot_matrix/Star%20Micronics%20-%20NP-10%20-%20Users%20Manual.pdf">full manual for the printer</a> scanned and uploaded as a PDF.</p>
<p>Come to find out, either because of the age or just the manufacturing decision, this printer has a <strong>very specific character set</strong> that it accepts. Loosely based off of the IBM PC's <a href="https://en.wikipedia.org/wiki/Code_page_437">Code Page 437</a> it consists mostly of your standard alpha-numeric characters, but with a small set of special symbols, lines, and boxes. Neat!</p>
<p>Sending these characters to the printer is pretty straightforward, I can just echo out the hex values with PHP like so:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>$</span><span>horizontalDouble </span><span>=</span><span> </span><span>"</span><span>\xCD</span><span>"</span><span>;</span></p><p><span>$</span><span>deg </span><span>=</span><span> </span><span>"</span><span>\xF8</span><span>"</span><span>;</span></p><p><span>echo</span><span> </span><span>str_repeat</span><span>($</span><span>horizontalDouble</span><span>,</span><span> </span><span>24</span><span>);</span></p><p><span>echo</span><span> </span><span>'</span><span>78</span><span>'</span><span> </span><span>.</span><span> </span><span>$</span><span>deg </span><span>.</span><span> </span><span>'</span><span>F</span><span>'</span><span> </span><span>.</span><span> PHP_EOL</span><span>;</span></p></code></pre>
<p>Alright, so I'm able to write text to the printer just fine, and include some special characters and design symbols. Now I need to figure out <em>what</em> I want to see every morning.</p>
<h2>Gathering the data</h2>
<p>I knew I wanted four distinct sections for my personal front page: <strong>weather, stocks, major news headlines, and a few top reddit posts</strong>. After all, that's usually what I end up look at on my phone in the morning.</p>
<p>Additionally, since this is an experimental project, I wanted to remain super cheap for this data, free if at all possible. Thankfully there's an amazing <a href="https://github.com/public-apis/public-apis">GitHub repo</a> for free and public APIs, so I just went through there and found the ones I needed.</p>
<ul>
<li>The weather pulls from <a href="https://open-meteo.com/en/docs">Open-Meteo</a> and no API key is needed</li>
<li>Stocks data pulls from <a href="https://twelvedata.com/docs">twelvedata</a> that offers a generous free tier</li>
<li>News headlines pull from <a href="https://developer.nytimes.com/get-started">NYTimes</a> which has a decent free tier, good enough for this project</li>
<li>Reddit posts pull from <a href="https://www.reddit.com/r/redditdev/comments/rvqirc/how_to_get_reddit_api_data_using_json/">Reddit JSON</a> which is free (but I had to spoof my User-Agent)</li>
</ul>
<p>For each of the sections, I wrote some basic PHP code to pull in the payload from the API endpoint and compile the data I wanted into a larger overall array. I only wanted specific stocks, types of headlines, and subreddit posts, and if any of the sections couldn't have data to present I just simply crash the script early so I can start it again at a later time.</p>
<p>This can be seen in this snippet which I use for pulling news headlines:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>// Get news headlines data</span></p><p><span>echo</span><span> </span><span>"</span><span>Fetching news headlines data...</span><span>"</span><span> </span><span>.</span><span> PHP_EOL</span><span>;</span></p><p><span>$</span><span>newsUrl </span><span>=</span><span> NEWS </span><span>.</span><span> </span><span>"</span><span>?api-key=</span><span>"</span><span> </span><span>.</span><span> NEWSKEY</span><span>;</span></p><p><span>$</span><span>newsData </span><span>=</span><span> </span><span>[];</span></p><p><span>$</span><span>newsAmount </span><span>=</span><span> </span><span>0</span><span>;</span></p><p><span>$</span><span>data </span><span>=</span><span> </span><span>json_decode</span><span>(</span><span>file_get_contents</span><span>($</span><span>newsUrl</span><span>),</span><span> </span><span>true);</span></p><p><span>if</span><span> </span><span>(!</span><span>isset</span><span>($</span><span>data</span><span>[</span><span>'</span><span>results</span><span>'</span><span>]))</span><span> </span><span>{</span></p><p><span>    </span><span>die</span><span>(</span><span>"</span><span>Unable to retrieve news data</span><span>"</span><span>);</span></p><p><span>}</span></p><p><span>foreach</span><span> </span><span>($</span><span>data</span><span>[</span><span>'</span><span>results</span><span>'</span><span>]</span><span> </span><span>as</span><span> </span><span>$</span><span>article</span><span>)</span><span> </span><span>{</span></p><p><span>    </span><span>if</span><span> </span><span>(</span></p><p><span>        </span><span>($</span><span>article</span><span>[</span><span>'</span><span>type</span><span>'</span><span>]</span><span> </span><span>===</span><span> </span><span>'</span><span>Article</span><span>'</span><span>)</span><span> </span><span>&amp;&amp;</span></p><p><span>        </span><span>(</span><span>in_array</span><span>($</span><span>article</span><span>[</span><span>'</span><span>section</span><span>'</span><span>],</span><span> </span><span>[</span><span>'</span><span>U.S.</span><span>'</span><span>,</span><span> </span><span>'</span><span>World</span><span>'</span><span>,</span><span> </span><span>'</span><span>Weather</span><span>'</span><span>,</span><span> </span><span>'</span><span>Arts</span><span>'</span><span>]))</span><span> </span><span>&amp;&amp;</span></p><p><span>        </span><span>($</span><span>newsAmount </span><span>&lt;</span><span> MAXNEWS</span><span>)</span></p><p><span>    </span><span>)</span><span> </span><span>{</span></p><p><span>        </span><span>$</span><span>newsData</span><span>[]</span><span> </span><span>=</span><span> </span><span>$</span><span>article</span><span>;</span></p><p><span>        </span><span>$</span><span>newsAmount</span><span>++;</span></p><p><span>    </span><span>}</span></p><p><span>}</span></p></code></pre>
<p>The <code>NEWS</code>, <code>NEWSKEY</code>, and <code>MAXNEWS</code> variables are all constants instantiated at the top of the script for easy editing.</p>
<p>Running this compiles everything I want to see displayed on the paper, but now I need to take on the actual task of formatting everything for the printer, and sending it the raw data.</p>
<h2>Printing out my front page</h2>
<p>I could just print out a heading for each section, but that's a little boring. I wanted a bit of <strong><em>flair</em></strong> to the project, so I decided to have a box at the top displaying the current date, day of the week, and my front page name all nicely bordered.</p>
<p>It took a little math, but I got everything working by using a combination of the hex values I talked about above, <code>str_repeat</code> and the knowledge that the page width for this printer is <strong>80 characters</strong>.</p>
<p>Now, just simply go through each section, print a little heading:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>str_repeat</span><span>($</span><span>horizontalSingle</span><span>,</span><span> </span><span>3</span><span>)</span><span> </span><span>.</span><span> </span><span>"</span><span> WEATHER </span><span>"</span><span> </span><span>.</span><span> </span><span>str_repeat</span><span>($</span><span>horizontalSingle</span><span>,</span><span> </span><span>(</span><span>PAGEWIDTH </span><span>-</span><span> </span><span>9</span><span>))</span><span> </span><span>.</span><span> </span><span>"</span><span>\n</span><span>"</span><span>;</span></p></code></pre>
<p>And then print out the data that I want displayed for that section:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>"</span><span>   </span><span>"</span><span> </span><span>.</span><span> </span><span>round</span><span>(($</span><span>weatherData</span><span>[</span><span>'</span><span>daily</span><span>'</span><span>][</span><span>'</span><span>daylight_duration</span><span>'</span><span>][</span><span>0</span><span>]</span><span> </span><span>/</span><span> </span><span>3600</span><span>),</span><span> </span><span>2</span><span>)</span><span> </span><span>.</span><span> </span><span>"</span><span>h of Sunlight  -  Sunrise: </span><span>"</span><span> </span><span>.</span><span> </span><span>date</span><span>(</span><span>'</span><span>g:ia</span><span>'</span><span>,</span><span> </span><span>strtotime</span><span>($</span><span>weatherData</span><span>[</span><span>'</span><span>daily</span><span>'</span><span>][</span><span>'</span><span>sunrise</span><span>'</span><span>][</span><span>0</span><span>]))</span><span> </span><span>.</span><span> </span><span>"</span><span>  -  Sunset: </span><span>"</span><span> </span><span>.</span><span> </span><span>date</span><span>(</span><span>'</span><span>g:ia</span><span>'</span><span>,</span><span> </span><span>strtotime</span><span>($</span><span>weatherData</span><span>[</span><span>'</span><span>daily</span><span>'</span><span>][</span><span>'</span><span>sunset</span><span>'</span><span>][</span><span>0</span><span>]))</span><span> </span><span>.</span><span> </span><span>"</span><span>\n</span><span>"</span><span>;</span></p></code></pre>
<p>For the weather and stocks, I knew I wouldn't hit the edge of the paper so I just wrote everything in single long lines. But that's different for the news headlines and Reddit posts.</p>
<p>If I just feed the printer one long line of text, it's smart enough to cut it and start printing on another line. But, I didn't want a word getting cut off in the middle and starting on the next line. So I implemented a small function to handle line length and instead return back an array of lines with a max length corresponding to the page width.</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>function</span><span> </span><span>splitString</span><span>($</span><span>string</span><span>,</span><span> </span><span>$</span><span>maxLength </span><span>=</span><span> PAGEWIDTH</span><span>)</span><span> </span><span>{</span></p><p><span>    </span><span>$</span><span>result </span><span>=</span><span> </span><span>[];</span></p><p><span>    </span><span>$</span><span>words </span><span>=</span><span> </span><span>explode</span><span>(</span><span>'</span><span> </span><span>'</span><span>,</span><span> </span><span>$</span><span>string</span><span>);</span></p><p><span>    </span><span>$</span><span>currentLine </span><span>=</span><span> </span><span>''</span><span>;</span></p><p><span>    </span><span>foreach</span><span> </span><span>($</span><span>words </span><span>as</span><span> </span><span>$</span><span>word</span><span>)</span><span> </span><span>{</span></p><p><span>        </span><span>if</span><span> </span><span>(</span><span>strlen</span><span>($</span><span>currentLine </span><span>.</span><span> </span><span>$</span><span>word</span><span>)</span><span> </span><span>&lt;=</span><span> </span><span>$</span><span>maxLength</span><span>)</span><span> </span><span>{</span></p><p><span>            </span><span>$</span><span>currentLine </span><span>.=</span><span> </span><span>($</span><span>currentLine </span><span>?</span><span> </span><span>'</span><span> </span><span>'</span><span> </span><span>:</span><span> </span><span>''</span><span>)</span><span> </span><span>.</span><span> </span><span>$</span><span>word</span><span>;</span></p><p><span>        </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span></p><p><span>            </span><span>if</span><span> </span><span>($</span><span>currentLine</span><span>)</span><span> </span><span>{</span></p><p><span>                </span><span>$</span><span>result</span><span>[]</span><span> </span><span>=</span><span> </span><span>$</span><span>currentLine</span><span>;</span></p><p><span>                </span><span>$</span><span>currentLine </span><span>=</span><span> </span><span>$</span><span>word</span><span>;</span></p><p><span>            </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span></p><p><span>                </span><span>// If a single word is longer than maxLength, split it</span></p><p><span>                </span><span>$</span><span>result</span><span>[]</span><span> </span><span>=</span><span> </span><span>substr</span><span>($</span><span>word</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>$</span><span>maxLength</span><span>);</span></p><p><span>                </span><span>$</span><span>currentLine </span><span>=</span><span> </span><span>substr</span><span>($</span><span>word</span><span>,</span><span> </span><span>$</span><span>maxLength</span><span>);</span></p><p><span>            </span><span>}</span></p><p><span>        </span><span>}</span></p><p><span>    </span><span>}</span></p><p><span>    </span><span>if</span><span> </span><span>($</span><span>currentLine</span><span>)</span><span> </span><span>{</span></p><p><span>        </span><span>$</span><span>result</span><span>[]</span><span> </span><span>=</span><span> </span><span>$</span><span>currentLine</span><span>;</span></p><p><span>    </span><span>}</span></p><p><span>    </span><span>return</span><span> </span><span>$</span><span>result</span><span>;</span></p><p><span>}</span></p></code></pre>
<p>Then I can just use it like so:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>foreach</span><span> </span><span>(</span><span>splitString</span><span>($</span><span>redditPost</span><span>)</span><span> </span><span>as</span><span> </span><span>$</span><span>line</span><span>)</span><span> </span><span>{</span></p><p><span>    </span><span>fwrite</span><span>($</span><span>printer</span><span>,</span><span> </span><span>$</span><span>line</span><span>)</span><span> </span><span>.</span><span> </span><span>"</span><span>\n</span><span>"</span><span>;</span></p><p><span>}</span></p></code></pre>
<p>Now all that's left to do is run the script!</p>
<h2>Usage and wrapping up</h2>
<p>I can fire off the printer manually by just running <code>php print.php</code> but I've instead set up a cron job to handle it for me.</p>
<p>Every morning at around 8am it starts printing out my personalized front page. I rip it off and check it out in the morning while drinking my coffee.</p>
<p><img src="https://aschmelyun.com/storage/images/blog/dotmatrix-example-print.jpg" alt="Example page printed"></p>
<p>As silly as it might sound, it just feels better having that finite amount of news on a single sheet of paper. Of being able to stop there instead of endlessly scrolling through websites and social media apps.</p>
<p>Also, this was a super fun project and I'm hoping I can find more uses for this dot matrix printer. Working with physical hardware (especially older specimens like this) is always a blast for me, and being able to integrate them with new technology or use them in interesting ways just ignites pure passion and reinforces why I became a programmer in the first place.</p>
<p>So, what do you think? If you have any ideas for projects like this, or just have a question or comment, I'd love to hear it! Catch me on <a href="https://twitter.com/aschmelyun">Twitter</a> if you'd like to chat more.</p>


            <div>
                
                <p>Subscribe using the form below and about 1-2 times a month you'll receive an email containing helpful hints, new packages, and interesting articles I've found on PHP, JavaScript, Docker and more.</p>
                
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ESP32: leaving love notes and entering demoscene territory (2022) (142 pts)]]></title>
            <link>https://theor.xyz/esp32-love-notes-demoscene/</link>
            <guid>41741614</guid>
            <pubDate>Fri, 04 Oct 2024 14:10:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theor.xyz/esp32-love-notes-demoscene/">https://theor.xyz/esp32-love-notes-demoscene/</a>, See on <a href="https://news.ycombinator.com/item?id=41741614">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-bvzihdzo=""><article data-astro-cid-bvzihdzo=""><h3 data-astro-cid-bvzihdzo="">A treatise on scope-creep and rabbit holes</h3><!-- {JSON.stringify(Astro.props)} --><!-- {pubDate && <FormattedDate date={pubDate} />}
        {
          updatedDate && (
            <div class="last-updated-on">
              Last updated on <FormattedDate date={updatedDate} />
            </div>
          )
        } --><hr data-astro-cid-bvzihdzo=""><nav><h2 id="table-of-content">Table of content</h2><ol><li><a href="#a-very-sensible-plan">A very sensible plan</a></li><li><a href="#iteration-speed-flashing-times-and-ota">Iteration speed: flashing times and OTA</a><ol><li><a href="#a-dead-end-wasm3-runtime">A dead-end: wasm3 runtime</a></li><li><a href="#a-simulator-with-sdl2">A simulator with SDL2</a></li><li><a href="#gnu-rocket-demoscene-tracker">GNU rocket: demoscene tracker</a></li><li><a href="#rocket-editor">Rocket editor</a></li></ol></li><li><a href="#upgrading-to-a-color-display">Upgrading to a color display</a><ol><li><a href="#tooling-for-palette-indexed-bitmaps">Tooling for palette-indexed bitmaps</a></li><li><a href="#rle-compression-and-delta-encoding-algorithm">RLE compression and delta encoding algorithm</a></li></ol></li><li><a href="#adding-sound-">Adding sound ?</a></li><li><a href="#laser-cutting-a-front-panel">Laser-Cutting a Front Panel</a></li><li><a href="#conclusion--on-scope-creep-and-rabbit-holes">Conclusion : On Scope-Creep and Rabbit Holes</a></li></ol></nav>





<p>I’ve been tinkering with arduino-like platforms recently (more on the midi synth soon) and got the idea to make a small message board to leave notes to my girlfriend. This is how I ended up with a simulator, an integration with a demoscene tracker, custom tools to make palette cycling effects, having to learn CAD to make a laser-cut front panel and more.</p>
<p><img src="https://theor.xyz/_astro/banner.1a01109f_ZulFlx.webp" alt="Result" width="1920" height="1080" loading="lazy" decoding="async"></p>
<h2 id="a-very-sensible-plan"><a data-link="" href="#a-very-sensible-plan"><span></span></a>A very sensible plan</h2>
<p>At first, I had a simple goal: Use an ESP32 (240mhz, 512k RAM, luxury !) and a SSD1306 display (128x64, monochrome). Use the ESP WiFi to change the message remotely. That was easy enough : the SSD1306 is supported by the <a href="https://learn.adafruit.com/adafruit-gfx-graphics-library">Adafruit-GFX</a> library, which makes simple rendering trivial.</p>
<p>I initially pulled the messages from a github-hosted txt file, which required a constant WiFi connection. I then reversed the whole thing and got the ESP32 to store the message in its EEPROM. Holding a ‘secret’ capacitive button starts a small web server with a primitive form to change the message. Not having to wait for a connection skips a 3-15 seconds loading time at start-up. This is the web server package: <a href="https://github.com/me-no-dev/ESPAsyncWebServer">https://github.com/me-no-dev/ESPAsyncWebServer</a></p>
<p>To avoid adding a power button, the board goes to sleep : when hibernating, it consumes less than 5 uA. The capacitive button on the front wakes it up.</p>
<p>I then started experimenting with fancier scenes : at first, particles showering the message, then I got into animations and…</p>
<h2 id="iteration-speed-flashing-times-and-ota"><a data-link="" href="#iteration-speed-flashing-times-and-ota"><span></span></a>Iteration speed: flashing times and OTA</h2>
<p>At this point, the flashing time got into the 40s range. To iterate on animation timing and sprite positioning, it’s less than ideal ; working at Unity, it’s like having to recompile the engine when moving an object.</p>
<p>I discovered the ESP32 it flashable over-the-air (OTA) using WiFi or bluetooth. There’s a neat package adding OTA upload to the web server: <a href="https://github.com/ayushsharma82/AsyncElegantOTA">AsyncElegantOTA</a> . You can then ’curl’ the firmware after the build to it.</p>
<h2 id="a-dead-end-wasm3-runtime"><a data-link="" href="#a-dead-end-wasm3-runtime"><span></span></a>A dead-end: wasm3 runtime</h2>
<p>The ESP32 has a nice little file system using the flash as storage, <a href="https://docs.espressif.com/projects/esp-idf/en/latest/esp32/api-reference/storage/spiffs.html">SPIFFS</a>. An interpreter loading code from there would completely circumvent the flashing step, leaving only a (quicker) file upload. Lua, MicroPython etc… seemed heavier than needed given the task at hand. WebAssembly is a great candidate, and lets the user choose which language to use, as long as it compiles to WASM.</p>
<p>I have a prototype using the fantastic <a href="https://github.com/wasm3/wasm3">Wasm3 interpreter</a>, which can run on embedded mcus : see the <a href="https://github.com/wasm3/embedded-wasm-apps">Embedded Wasm Apps</a> repo. I used <a href="https://www.assemblyscript.org/">AssemblyScript</a>, which is basically typescript-to-wasm, to output a web assembly binary, that would get ’curl’ed to the web-server, giving me hot-reloading on save. Quite a nice workflow ! The one painful part is having to write bindings manually. I didn’t find a tool to do so ; I’m sure it will happen, if it hasn’t already. A project close to my goals is <a href="https://wasm4.org/">WASM-4</a>, which I definitely need to investigate later.</p>
<p>We’re getting into <a href="https://en.wikipedia.org/wiki/Fantasy_video_game_console">Fantasy console</a> territory : if you’re interested, here is a short list of interesting platforms:</p>
<ul>
<li><a href="https://www.lexaloffle.com/pico-8.php">Pico8</a> is the most impressive one - it doesn’t run on embedded, but has an integrated sprite and tile editor, a music tracker, etc. Paid, closed source</li>
<li><a href="https://pixelvision8.itch.io/pv8">PixelVision8</a> is an open-source C# platform, quite close to Pico8 in terms of features. Supports Lua too, no embedded either</li>
<li><a href="https://wasm4.org/">Wasm4</a> apparently runs on MCUs: see <a href="https://twitter.com/alvaroviebrantz/status/1518343016011943939">Twitter</a></li>
</ul>
<h2 id="a-simulator-with-sdl2"><a data-link="" href="#a-simulator-with-sdl2"><span></span></a>A simulator with SDL2</h2>
<p>In parallel, I started working on a desktop simulator - a SDL2 window displaying the same scenes. I copy-pasted files and hacked them around until it worked. It gives me an even faster iteration loop AND debugging, at the cost of accuracy (at least in terms of performance).</p>
I still had to restart the app every time
I changed a timing/position/…, but it’s a 2-second long loop. Better !
<h2 id="gnu-rocket-demoscene-tracker"><a data-link="" href="#gnu-rocket-demoscene-tracker"><span></span></a>GNU rocket: demoscene tracker</h2>
<p>I then got a flashback : years ago, I stumbled upon <a href="https://github.com/rocket/rocket">GNU Rocket</a>, which is a “sync-tracker”, a kind of primitive video editing tool. Your program/demo declares a bunch of tracks : a track outputs a float value that varies according to time, eg. interpolating the x axis of a sprite position from 0 to 10 between the frames 12 and 36.</p>
<p>At authoring time, the Rocket client will connect to an editor, in which you can scrub time and edit values :</p>

<p>From the editor, you can then send an ’export’ command. It will save the data, in my case on the SPIFFS file system. The rocket client, if not connected to an editor, will open these files and play them, which is what I do in my “production” build.</p>
<p>In the simplest case, I have the client step linearly through time - it just moves forward. In some cases, the time change is itself an artefact of user interactions, allowing to rewind the timeline ; in another case, I have a rocket track which is itself interpreted as playing/pausing/looping the timeline.</p>
<h2 id="rocket-editor"><a data-link="" href="#rocket-editor"><span></span></a>Rocket editor</h2>
<p>The bundled editor uses Qt. It’s a great framework ; I have scars from building it.</p>
<p>I found [Emoon’s editor], which is a SDL2 app. Great, not DPI-aware, very small font. I eventually hacked <a href="https://github.com/edoreshef/ground-control">Ground Control</a> (fork <a href="https://github.com/theor/ground-control">here on my github</a>) to add font scaling, a more readable font and color palette (IMO) and a bunch of small fixes:</p>

<h2 id="upgrading-to-a-color-display"><a data-link="" href="#upgrading-to-a-color-display"><span></span></a>Upgrading to a color display</h2>
<p>Monochrome, 128x64 is limiting - I aimed for a grayscale display initially, but eventually settled on a <a href="https://www.adafruit.com/product/1431">SSD1351</a> OLED display, 128*128px by 16-bit colors.</p>
<p>My first realization was that, if Adafruit-GFX’s SSD1306 implementation was buffered, the 1351 one wasn’t. I switched to <a href="https://github.com/Bodmer/TFT_eSPI">TFT-eSPI</a>, a more complex but more optimized library. It has a sprite class, that I use as a back buffer. The scene writes everything there, then it’s sent once to the actual display.</p>
<p>I settled on 4-bit indexed bitmaps, with a 16 colors palette <em>per bitmap</em>, meaning I can have more than 16 colors on the screen in the end. The raw power of the ESP is enough to compensate for the lack of dedicated graphics hardware and gives me &gt;40fps pre-optimization.
Then I hit the usual wall: making art is hard and not what I’m good at. So as usual, I threw more code at it, on top of using the right tools for the job.</p>

<p><a href="https://www.aseprite.org/">aseprite</a> is a fantastic pixel art editor and supports both indexed palettes and animation. It allowed me to reuse bitmaps I found and constrain them to 16 colors. I then wrote a small WPF tool (old habits…) to visualize palette swaps, eg. interpolating the palette to fake a day-night cycle:</p>

<p>It also exports the palette and the bitmap as C header files, ready to be included in the project:</p>
<pre tabindex="0"><code><span><span>static</span><span> </span><span>const</span><span> </span><span>uint16_t</span><span> </span><span>river1_palette</span><span>[</span><span>16</span><span>]</span><span> </span><span>=</span><span> </span><span>{</span></span>
<span><span>    </span><span>0x</span><span>0841</span><span>,</span><span> </span><span>0x</span><span>FFFF</span><span>,</span><span> </span><span>0x</span><span>5901</span><span>,</span><span> </span><span>0x</span><span>7182</span><span>,</span></span>
<span><span>    </span><span>0x</span><span>CE17</span><span>,</span><span> </span><span>0x</span><span>8C0F</span><span>,</span><span> </span><span>0x</span><span>62EB</span><span>,</span><span> </span><span>0x</span><span>3145</span><span>,</span></span>
<span><span>    </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span><span>,</span></span>
<span><span>    </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span></span>
<span><span>}</span><span>;</span></span></code></pre>
<pre tabindex="0"><code><span><span>// 222 x 39 px</span></span>
<span><span>static</span><span> </span><span>const</span><span> </span><span>uint8_t</span><span> </span><span>bmp_river1</span><span>[</span><span>1545</span><span>]</span><span> PROGMEM </span><span>=</span><span> </span><span>{</span></span>
<span><span>    </span><span>0x</span><span>06</span><span>,</span><span> </span><span>0x</span><span>00</span><span>,</span><span> </span><span>0x</span><span>19</span><span>,</span><span> </span><span>0x</span><span>01</span><span>,</span><span> </span><span>0x</span><span>B1</span><span>,</span><span> </span><span>0x</span><span>DD</span><span>,</span><span> </span><span>0x</span><span>01</span><span>,</span><span> </span><span>0x</span><span>01</span><span>,</span></span>
<span><span>    // ...</span></span>
<span><span>}</span></span>
<span></span>
<span><span>static</span><span> </span><span>const</span><span> SpriteInfo spr_river1 </span><span>=</span><span> </span><span>{</span><span> bmp_river1</span><span>,</span><span> </span><span>222</span><span>,</span><span>39</span><span>,</span><span> </span><span>1545</span><span> </span><span>}</span><span>;</span></span></code></pre>
<p>Palette swapping and cycling are decades-old tricks - see the <a href="https://rasterscroll.com/mdgraphics/graphical-effects/palette-swapping/">RasterScroll Sega MegaDrive/Genesis doc here</a>.</p>
<p>I highly recommend that <a href="https://www.youtube.com/watch?v=aMcJ1Jvtef0">GDC talk from Mark Ferrari</a>, artist on The Secret of Monkey Island and, more recently, <a href="https://store.steampowered.com/app/569860/Thimbleweed_Park/">Thimbleweed Park</a>, going into details about those kinds of tricks and the tooling to make them. If you just want a quick demo, see his <a href="http://www.effectgames.com/demos/canvascycle/">palette cycling demo here</a>. A quick sample:</p>

<h2 id="rle-compression-and-delta-encoding-algorithm"><a data-link="" href="#rle-compression-and-delta-encoding-algorithm"><span></span></a>RLE compression and delta encoding algorithm</h2>
<p>Let’s consider another background image: ’222*89 px / 2 (4 bits per pixel) = 9879b’. Which is alright given the space available on the ESP32 but it does slow down the flashing (assets as SPIFFS files/hot reloading is something I forbade myself to get into) and… I just could not let it go.</p>
<p>I went with a simple RLE encoding. That frame goes from 9879 bytes to 2279 bytes, 23% of the original size.</p>
<p>I also have animations :</p>
<p><img src="https://theor.xyz/_astro/frames.e0d08955_ZesJH8.webp" alt="Frames" width="1626" height="1404" loading="lazy" decoding="async"></p>
<p>Instead of storing the whole frame every time, I decided to just store the delta to the first frame. Matching tooling:</p>
<p><img src="https://theor.xyz/_astro/frames-delta.8cb9ccd6_aDPJG.webp" alt="Delta to the first frame" width="1626" height="1404" loading="lazy" decoding="async"></p>
<p>Each subsequent frame went down to ~100 bytes on average. Quite a gain.</p>
<p>It’s not my first foray into compression : see the <a href="https://theor.xyz/unicode-graphs/">URL encoding of my unicode graph editor</a></p>
<h2 id="adding-sound-"><a data-link="" href="#adding-sound-"><span></span></a>Adding sound ?</h2>
<p>I’m now experimenting with the <a href="https://github.com/earlephilhower/ESP8266Audio">ESP8266Audio</a> lib (which also works with the ESP32). Very promising - it even supports .mod files. I’ m running it through an <a href="https://www.adafruit.com/product/3885">Adafruit STEMMA Speaker</a>, which is way enough. More to come later about that</p>
<h2 id="laser-cutting-a-front-panel"><a data-link="" href="#laser-cutting-a-front-panel"><span></span></a>Laser-Cutting a Front Panel</h2>
<p>I recently found that the fantastic <a href="https://www.lespacemaker.com/">lespacemaker</a> maker space is really close to my place - and they have a laser cutter. First draft using acrylic : <img src="https://theor.xyz/_astro/panel.8b5bc6aa_Zz77Hb.webp" alt="Acrylic front panel" width="1698" height="1150" loading="lazy" decoding="async"> Maybe I’ll make it a full case eventually… This was made using <a href="https://cadquery.readthedocs.io/en/latest/">CadQuery</a>, a python lib to CAD using code. I did try Fusion, Sketch up and more (too button-clicky, found myself dreaming about writing a plugin), openscad (too slow) and a few others, but in the end…</p>
<h2 id="conclusion--on-scope-creep-and-rabbit-holes"><a data-link="" href="#conclusion--on-scope-creep-and-rabbit-holes"><span></span></a>Conclusion : On Scope-Creep and Rabbit Holes</h2>
<p>No one knowing me will be surprised that this whole thing went down a deep and twisted rabbit hole. That’s alright ; unlike a few years ago, I eventually “shipped” something that ended up our very own kitchen counter. I also learned a lot in a various fascinating subjects (I2C vs SPI, capacitive buttons debouncing, palette cycling, etc.).</p>
<p>All of that definitely happened because it wasn’t a work project.</p>
<p>I think scope creep is alright when learning ; it gives you a wider rather than deeper knowledge, which can be valuable too. <em>Controlled</em> scope creep can lead to better products, at a greater risk. The risk evaluation is the critical part here.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Computer scientists combine two 'beautiful' proof methods (102 pts)]]></title>
            <link>https://www.quantamagazine.org/computer-scientists-combine-two-beautiful-proof-methods-20241004/</link>
            <guid>41741358</guid>
            <pubDate>Fri, 04 Oct 2024 13:48:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/computer-scientists-combine-two-beautiful-proof-methods-20241004/">https://www.quantamagazine.org/computer-scientists-combine-two-beautiful-proof-methods-20241004/</a>, See on <a href="https://news.ycombinator.com/item?id=41741358">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-role="selectable">
    <p>How do you prove something is true? For mathematicians, the answer is simple: Start with some basic assumptions and proceed, step by step, to the conclusion. QED, proof complete. If there’s a mistake anywhere, an expert who reads the proof carefully should be able to spot it. Otherwise, the proof must be valid. Mathematicians have been following this basic approach for well over 2,000 years.</p>
<p>Then, in the 1980s and 1990s, computer scientists reimagined what a proof could be. They developed a dizzying variety of new approaches, and when the dust settled, two inventions loomed especially large: <a href="https://www.quantamagazine.org/how-to-prove-you-know-a-secret-without-giving-it-away-20221011/">zero-knowledge proofs</a>, which can convince a skeptic that a statement is true without revealing why it is true, and <a href="https://www.quantamagazine.org/how-computer-scientists-learned-to-reinvent-the-proof-20220523/">probabilistically checkable proofs</a>, which can persuade a reader of the truth of a proof even if they only see a few tiny snippets of it.</p>
<p>“These are, to me, two of the most beautiful notions in all of theoretical computer science,” said <a href="https://www.cst.cam.ac.uk/people/tg508">Tom Gur</a>, a computer scientist at the University of Cambridge.</p>
<p>It didn’t take long for researchers to try combining these two types of proof. They won a partial victory in the late 1990s, using lesser versions of each condition. For decades, no one could merge the ideal version of zero knowledge with the ideal version of probabilistic checkability.</p>
<p>Until now. In a <a href="http://arxiv.org/abs/2403.11941">paper</a> that marks the culmination of seven years of work, Gur and two other computer scientists have finally combined the ideal versions of the two kinds of proof for an important class of problems.</p>
<p>“It’s a very important result,” said <a href="https://starkware.co/about-us/">Eli Ben-Sasson</a>, a theoretical computer scientist and founder of the company StarkWare, which develops cryptographic applications of zero-knowledge proofs. “It solves a very old and well-known open problem that has baffled researchers, including myself, for a very long time.”</p>
<h2><strong>Check, Please </strong></h2>
<p>The story begins in the early 1970s, when computer scientists began to formally study the difficulty of the problems they were asking computers to solve. Many of these problems share an important property: If someone finds a valid solution, they can easily convince a skeptical “verifier” that it really is valid. The verifier, in turn, will always be able to spot if there’s a mistake. Problems with this property belong to a class that researchers call NP.</p>

<p>To understand how such verification can work, consider this classic NP problem: Given a map divided into different regions, is it possible to fill it in using just three colors without giving adjacent regions the same color? Depending on the map, this problem can be notoriously difficult. But if you manage to find a valid coloring, you can prove it by showing a verifier a properly colored map. The verifier just needs to glance at every border.</p>
<p>A decade later, two graduate students pioneered a different way to think about mathematical proof. Shafi Goldwasser and Silvio Micali, both at the University of California, Berkeley, had been wondering whether it was possible to prevent cheating in an online poker game. That would require somehow proving that the cards in each player’s hand were drawn randomly, without also revealing what those cards were.</p>
<p>Goldwasser and Micali answered their own question with a resounding yes by inventing zero-knowledge proofs in a seminal <a href="https://dl.acm.org/doi/10.1145/22145.22178">1985 paper</a>, co-authored with the University of Toronto computer scientist Charles Rackoff. The following year, Micali and two other researchers followed up with <a href="https://dl.acm.org/doi/10.1145/116825.116852">a paper</a> showing that the solution to any problem in NP can be verified using a specific kind of zero-knowledge proof.</p>
<p>To get a sense of these proofs, suppose that you once again want to convince a verifier that a particular map is three-colorable — but this time, you don’t want the verifier to learn how to color it themself. Instead of drawing an example, you can prove it through an interactive process. Start by coloring in the map, and then carefully cover every region with black tape, leaving only the borders visible. The verifier then picks a border at random, and you’ll uncover the regions on either side, revealing two different colors.</p>
<p>Now repeat this process many times, randomly switching up the color scheme before each round so that the verifier can’t piece together any consistent information about your solution. (For example, swap red and blue regions and leave green regions unchanged.) If you’re bluffing, the verifier will eventually find a spot where the map isn’t properly colored. If you’re telling the truth, you’ll be able to convince them beyond a reasonable doubt in about as much time as it would take to verify a proof using the standard approach.</p>
<figure>
    <p><img width="1120" height="2388" src="https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Desktop-v1-1.jpg" alt="A graphic of zero knowledge proofs" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Desktop-v1-1.jpg 1120w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Desktop-v1-1-807x1720.jpg 807w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Desktop-v1-1-244x520.jpg 244w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Desktop-v1-1-768x1637.jpg 768w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Desktop-v1-1-720x1536.jpg 720w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Desktop-v1-1-961x2048.jpg 961w" sizes="(max-width: 1120px) 100vw, 1120px"><img width="923" height="2560" src="https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Mobile-v1-scaled.jpg" alt="a graphic of zero knowledge proofs" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Mobile-v1-scaled.jpg 923w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Mobile-v1-620x1720.jpg 620w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Mobile-v1-188x520.jpg 188w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Mobile-v1-768x2130.jpg 768w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Mobile-v1-554x1536.jpg 554w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Zero_Knowledge_Proofs-crMarkBelan-Mobile-v1-738x2048.jpg 738w" sizes="(max-width: 923px) 100vw, 923px">    </p>
            <figcaption>
            <p>Mark Belan/<i data-stringify-type="italic">Quanta Magazine</i></p>
        </figcaption>
    </figure>

<p>This zero-knowledge proof is strikingly different from the standard approach in two ways: It’s an interactive process rather than a document, and each participant relies on randomness to ensure that the other can’t predict their decisions. But because of that randomness, there’s now always a chance that a flawed proof will be deemed valid. Still, it’s easy to make that probability extremely small, and computer scientists quickly got over their discomfort at this looser definition of proof.</p>
<p>As <a href="https://web.cs.ucla.edu/~sahai/">Amit Sahai</a>, a computer scientist at the University of California, Los Angeles, put it, “If the chance that something is not correct is less than one out of the number of particles in the universe, it seems reasonable for us to call that a proof.”</p>
<h2><strong>Pretty Cool Proofs</strong></h2>
<p>Researchers soon realized that randomized interactive proofs could do more than just hide secret information. They also enabled easy verification for problems much harder than those in NP. <a href="https://ieeexplore.ieee.org/document/89520">One type</a> of interactive proof even worked for all problems in a class called NEXP. With ordinary proofs, the solutions to these problems can take as long just to verify as the hardest NP problems take to solve.</p>
<p>The proof revolution culminated in one final surprising discovery: You can still get the full power of interactive proofs without any interactions.</p>
<p>In principle, removing interactivity is straightforward. “The prover lists out all the possible challenges he could ever get from the verifier, and then just writes out all of his answers ahead of time,” Sahai said. The catch is that for complicated problems like the hardest ones in NEXP, the resulting document would be enormous, far too long to read from start to finish.</p>
<p>Then in 1992, the computer scientists Sanjeev Arora and Shmuel Safra <a href="https://dl.acm.org/doi/10.1145/273865.273901">defined</a> a new class of noninteractive proofs: probabilistically checkable proofs, or PCPs. Along with <a href="https://dl.acm.org/doi/10.1145/226643.226652">other researchers</a>, they showed that any solution to a NEXP problem could be rewritten in this special form. While PCPs are even longer than ordinary proofs, they can be rigorously vetted by a verifier who only reads small snippets. That’s because a PCP effectively multiplies and distributes any error in an ordinary proof. Trying to find an error in a normal proof is like hunting for a tiny dollop of jam by nibbling on a slice of toast. A PCP “spreads the jam uniformly over the piece of bread,” Gur said. “Wherever you take a bite, it doesn’t matter, you will always find it.”</p>
<figure>
    <p><img width="1120" height="1374" src="https://www.quantamagazine.org/wp-content/uploads/2024/10/Probabilistically_Checkable_Proofs-crMarkBelan-Desktop-v1-1.jpg" alt="A graphic of Probabilistically Checkable Proofs" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/10/Probabilistically_Checkable_Proofs-crMarkBelan-Desktop-v1-1.jpg 1120w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Probabilistically_Checkable_Proofs-crMarkBelan-Desktop-v1-1-424x520.jpg 424w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Probabilistically_Checkable_Proofs-crMarkBelan-Desktop-v1-1-768x942.jpg 768w" sizes="(max-width: 1120px) 100vw, 1120px"><img width="1120" height="1716" src="https://www.quantamagazine.org/wp-content/uploads/2024/10/Probabilistically_Checkable_Proofs-crMarkBelan-Mobile-v1.jpg" alt="A graphic of Probabilistically Checkable Proofs" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/10/Probabilistically_Checkable_Proofs-crMarkBelan-Mobile-v1.jpg 1120w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Probabilistically_Checkable_Proofs-crMarkBelan-Mobile-v1-339x520.jpg 339w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Probabilistically_Checkable_Proofs-crMarkBelan-Mobile-v1-768x1177.jpg 768w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Probabilistically_Checkable_Proofs-crMarkBelan-Mobile-v1-1003x1536.jpg 1003w" sizes="(max-width: 1120px) 100vw, 1120px">    </p>
            <figcaption>
            <p>Mark Belan/<i data-stringify-type="italic">Quanta Magazine</i></p>
        </figcaption>
    </figure>

<p>The crucial element was, again, randomness — the verifier’s choice of snippets would have to be unpredictable, to ensure that a dishonest prover couldn’t hide inconsistencies anywhere.</p>
<p>Arora, Safra and others showed that PCPs could also dramatically speed up verification for more common NP problems. Soon after, Arora and four other researchers improved PCPs further, pushing the speed of NP proof verification to a theoretical limit — a <a href="https://dl.acm.org/doi/10.1145/278298.278306">celebrated result</a> known as the PCP theorem.</p>
<p>“This is considered one of the big achievements of theoretical computer science,” said <a href="https://yuvali.cswp.cs.technion.ac.il/">Yuval Ishai</a>, a cryptographer at the Technion in Haifa, Israel.</p>
<p>The road to the PCP theorem had been anything but straightforward. Researchers started with zero-knowledge proofs for NP problems that used both interactivity and randomness. Then they realized that similar proofs could be used to verify the solutions to far harder problems. Finally, they showed that by transforming those proofs into noninteractive PCPs, they could verify a solution in less time than it would take to just read the proof. Computer scientists were feeling triumphant.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Is Light So Fast? (133 pts)]]></title>
            <link>https://profmattstrassler.com/2024/10/01/why-is-the-speed-of-light-so-fast-part-1/</link>
            <guid>41741333</guid>
            <pubDate>Fri, 04 Oct 2024 13:46:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://profmattstrassler.com/2024/10/01/why-is-the-speed-of-light-so-fast-part-1/">https://profmattstrassler.com/2024/10/01/why-is-the-speed-of-light-so-fast-part-1/</a>, See on <a href="https://news.ycombinator.com/item?id=41741333">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="841aeed" data-element_type="widget" data-widget_type="post-comments.theme_comments">
				<!-- .comment-content -->

							<!-- .comment-body -->
		<!-- #comment-## -->
<!-- .children -->
<!-- #comment-## -->
<!-- .children -->
<!-- #comment-## -->
<!-- .children -->
<!-- #comment-## -->
		<li id="comment-458118">
			<article id="div-comment-458118">
				<!-- .comment-meta -->

				<div>
					<p>The law of conservation of energy states that the total energy of an isolated system remains constant; it is said to be conserved over time, however it further states that energy can neither be created nor destroyed – only converted from one form of energy to another. In other words Energy and mass have always existed, so yo cannot create something out of nothing but from the available mass and energy that has always existed. To my thinking and agreeable to Principles of Relativity, that even the so called vacuum of space or the so called ether is a form or energy or mass (so far unexplained as dark matter) as it has always existed, even if in another form (call it dark matter). As a theologian, I see the Wavicles as (organized power) from an intelligent being, that passes through all energy and mass under two laws. 1) To GOVERN His creatures, according to immutable laws (elements that are given shape, image and mass and which eventually will be converted to another (energy/mass) within a higher sphere/dominion/principality/dimension or realm. 2) To CONTROL the lower (non intelligent elements that make up mass as dust, plants, gas, planets, stars quasars, Pulsars etc, according to laws.<br>
There is no place where there is no law, neither is there no place where there is no mass or energy.<br>
There is a place currently unknown to most intelligent beings which is indicated as the ‘Nucleus of all Nuclei” of which I will defer to legitimise until a later time. In every discourse you make either in your book , podcast, Youtube or any medium, I see all you work as easily converted into theology. I certainly lack the intelligence as far as the mathematical equations that you have attained, never theless, your presentations make perfect sense to me in another bandwidth. Thanking you for your energy in helping many see beyond their limited horizons. Kind regards  Joseph</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		<ol>
		<li id="comment-458121">
			<article id="div-comment-458121">
				<!-- .comment-meta -->

				<div>
					<p>On this: “even the so called vacuum of space or the so called ether is a form or energy or mass (so far unexplained as dark matter) as it has always existed, even if in another form (call it dark matter).” I think you might want to read the book’s Chapters 1-8 carefully.  Things — objects — are not forms of energy.  Energy is carried <strong>by</strong> things.  The same is true of mass.  If you imagine making things out of energy or out of mass, you are misunderstanding what energy and mass are.  They are properties of substances, not the ingredients for substances.  Do not let loose language lead you astray on this.</p>
<p>Also, I’d like to ask you again to please hold your religious theorizing from this blog, particularly when it veers into scientific speculation with no experimental basis (as in “within a higher sphere/dominion…” or “Nucleus of all nuclei”.)  I’d encourage you to write about these ideas on a blog devoted to the larger questions of truth, philosophy, religion, metaphysics, and the like. This is a site devoted to the very limited methods and lessons of science.  </p>
<p>In science, we assume there are laws of nature and do experiments to understand what follows from those assumptions.  Remarkably, what we learn proves to be very powerful in the practical, material world. But we cannot use these methods to definitively address larger questions of truth, religion, fundamental origins, etc.  </p>
<p>You can try to obtain answers for such questions as you see fit, but these are not scientific answers, as they require assumptions beyond those that are necessary to do science.  You’re welcome to make those assumptions, of course. But the consequences of those assumptions are not scientific, and belong on a different blog than this one.  Here we walk a narrow, straight path, knowing that sticking to that path means that we can answer a few questions with great clarity, while leaving other important questions, including ones essential to being human, completely unaddressed.</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
		<li id="comment-458124">
			<article id="div-comment-458124">
				<!-- .comment-meta -->

				<div>
					<p>In GR, conservation laws are a little bit more complex.</p>
<p>At first, it was not clear how conservation of energy applied to GR, and that gave way to the discussions and debate of Hilbert, Klein, Einstein and Noether about this topic.</p>
<p>It was Noether who made this topic crystal-clear.</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
	<!-- .comment-list -->

		


		<p id="respond">
			<h3 id="reply-title">Leave a Reply<small></small></h3>			
		</p>

		
		<!-- .comments-area -->
		</div></div>]]></description>
        </item>
    </channel>
</rss>