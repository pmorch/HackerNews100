<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 05 Mar 2024 17:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Facebook Is Not Working (168 pts)]]></title>
            <link>https://metastatus.com/</link>
            <guid>39604746</guid>
            <pubDate>Tue, 05 Mar 2024 15:36:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://metastatus.com/">https://metastatus.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39604746">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Open-source project ZLUDA lets CUDA apps run on AMD GPUs (120 pts)]]></title>
            <link>https://www.cgchannel.com/2024/02/open-source-project-zluda-lets-cuda-apps-run-on-amd-gpus/</link>
            <guid>39604745</guid>
            <pubDate>Tue, 05 Mar 2024 15:36:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cgchannel.com/2024/02/open-source-project-zluda-lets-cuda-apps-run-on-amd-gpus/">https://www.cgchannel.com/2024/02/open-source-project-zluda-lets-cuda-apps-run-on-amd-gpus/</a>, See on <a href="https://news.ycombinator.com/item?id=39604745">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
<p><img src="https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_f-960x480.jpg" alt="" width="960" height="480" srcset="https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_f-960x480.jpg 960w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_f-500x250.jpg 500w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_f-768x384.jpg 768w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_f-610x305.jpg 610w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_f-640x320.jpg 640w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_f-300x150.jpg 300w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_f.jpg 1200w" sizes="(max-width: 960px) 100vw, 960px"></p>
<p><br>
Andrzej Janik has released <a href="https://github.com/vosen/ZLUDA/releases/tag/v3" target="_blank">ZLUDA 3</a>, a new version of his open-source project that enables GPU-based applications designed for NVIDIA GPUs <a href="https://github.com/vosen/ZLUDA" target="_blank">to run on other manufacturers’ hardware</a>.</p>
<p>The wrapper technology is designed to enable existing applications to run on new hardware unmodified, without the need for any work on their developers’ part.</p>
<p>While previous versions of ZLUDA enabled CUDA applications to run on Intel GPUs, with version 3, that has switched to AMD GPUs.</p>
<p>ZLUDA has been confirmed to work, to varying degrees of success, with software including Blender, photogrammetry apps 3DF Zephyr and RealityCapture, and the Arnold renderer.</p>
<p>Below, we’ve compiled our own FAQs on the project, summarising how it might affect CG artists, and how well other CUDA-based CG applications are likely to run on AMD GPUs under ZLUDA.</p>
<p><strong>ZLUDA? Isn’t that for Intel GPUs?</strong><br>
ZLUDA was first released in 2020, initially as a drop-in replacement for CUDA on Intel GPUs.</p>
<p>It attracted a fair amount of attention in the open-source community, but in 2021, shortly after the release of version 2, Janik <a href="https://thenewstack.io/zluda-a-cuda-for-intel-gpus-needs-a-new-maintainer/" target="_blank">announced</a> that he was unable to keep developing the project.</p>
<p>No reason was given, and that seemed to be it for ZLUDA.</p>
<p>However, this week, Janik updated the <a href="https://github.com/vosen/ZLUDA?tab=readme-ov-file#faq" target="_blank">FAQs section</a> of the ZLUDA GitHub repository to explain the reasons for the radio silence.</p>
<p>In 2021, while he was still working at Intel – he was a Software Engineering Manager on the Visual Technologies team – the firm began to evaluate ZLUDA as a potential official technology.</p>
<p>Intel eventually decided there was “no business case for running CUDA applications on Intel GPUs”, and in 2022, Janik left the company, and, now a freelance contractor, approached AMD.</p>
<p>AMD evaluated ZLUDA for two years, but also decided not to go further with the project – at which point, Janik open-sourced the updated code.</p>
<p>It’s a fascinating story, and one told in more detail in <a href="https://www.phoronix.com/review/radeon-cuda-zluda" target="_blank">this article on Phoronix</a>, which first reported the news earlier this week.</p>
<p><strong>Why might this matter to CG artists?</strong><br>
Version 3 of ZLUDA is intended to enable GPU-based applications developed using NVIDIA’s <a href="https://developer.nvidia.com/about-cuda" target="_blank">CUDA</a> API to run on AMD GPUs.</p>
<p>That’s significant in industries like VFX, motion graphics and visualization, because a number of key CG applications, particularly renderers, are CUDA-based, and effectively NVIDIA-only.</p>
<p>Although AMD has its own technology, <a href="https://github.com/ROCm/HIP" target="_blank">HIP</a>, for porting CUDA apps to run on its hardware, it requires work on the part of the software developer.</p>
<p>HIP has been used to create AMD-compatible versions of <a href="https://www.cgchannel.com/2023/05/maxon-releases-redshift-3-5-15/" target="_blank">Redshift</a> and Blender’s <a href="https://www.cgchannel.com/2021/11/cycles-x-now-runs-on-amd-gpus/" target="_blank">Cycles</a> renderer, but so far, few other CG tools developers have followed suit.</p>
<p>In contrast, ZLUDA – version 3 of which is actually built on HIP – is designed to enable CUDA applications run on AMD GPUs unmodified.</p>
<p>That means that artists can, at least in theory, take existing version of NVIDIA-only applications and run them on AMD hardware.</p>
<p>Previously NVIDIA-only software that Janik has tested with ZLUDA includes photogrammetry apps <a href="https://www.3dflow.net/3df-zephyr-photogrammetry-software/" target="_blank">3DF Zephyr</a> and <a href="https://www.capturingreality.com/realitycapture" target="_blank">RealityCapture</a>, and Autodesk’s <a href="https://www.autodesk.com/products/arnold/overview?term=1-YEAR&amp;tab=subscription&amp;plc=ARNOL" target="_blank">Arnold renderer</a>.</p>
<p><br><img src="https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_Arnold-960x480.jpg" alt="" width="960" height="480" srcset="https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_Arnold.jpg 960w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_Arnold-500x250.jpg 500w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_Arnold-768x384.jpg 768w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_Arnold-610x305.jpg 610w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_Arnold-640x320.jpg 640w, https://www.cgchannel.com/wp-content/uploads/2024/02/240216_ZLUDA3_RunCUDASoftwareOnAMDGPUs_Arnold-300x150.jpg 300w" sizes="(max-width: 960px) 100vw, 960px"></p>
<p><em>Gleb Alexandrov’s <a href="https://cloud.blender.org/p/gallery/564a0d34c379cf089a7ad48f" target="_blank">Attic scene</a> rendered in Arnold under ZLUDA. Support for Arnold is a “proof of concept”: only one other scene has rendered successfully using ZLUDA’s OptiX implementation.</em></p>
<p><strong>How fast is running a CUDA application under ZLUDA?</strong><br>
Janik describes CUDA apps as running with “near-native performance” on AMD GPUs.</p>
<p>Benchmark scores shown in Phoronix’s article and <a href="https://blenderartists.org/t/zluda-v2-opensource-cuda-driver-for-intel-products-amd-what-about-you-hehe/1287308/8" target="_blank">this thread</a> on the Blender Artists forum suggest that for Blender, performance under ZLUDA is similar to the native HIP backend.</p>
<p>However, the ZLUDA GitHub repository notes that both 3DF Zephyr and Reality Capture are “much slower” under ZLUDA.</p>
<p>In addition, many developers of GPU renderers also use a second NVIDA API, <a href="https://developer.nvidia.com/rtx/ray-tracing/optix" target="_blank">OptiX</a>, to accelerate ray tracing, which also contributes to performance.</p>
<p>ZLUDA has “minimum” support for OptiX, but only on Linux, not Windows, and the implementation is described as “buggy, unoptimized and incomplete”.</p>
<p>In fact, ZLUDA-Optix – used primarily for “proof of concept” support for Arnold – is not included in the redistributable version of ZLUDA: to use it, you have to build it yourself.</p>
<p><strong>Will other CG applications run on AMD GPUs under ZLUDA?</strong><br>
Without user testing, it’s difficult to say how well other CUDA-based CG applications will run under ZLUDA.</p>
<p>However, it seems unlikely to be a magic solution: there are a number of <a href="https://github.com/vosen/ZLUDA?tab=readme-ov-file#known-issues" target="_blank">known issues</a>, and Janik has had limited success with other GPU renderers.</p>
<p>The <a href="https://www.chaos.com/vray/benchmark" target="_blank">V-Ray benchmark</a> runs on “certain ‘lucky’ older combinations” of ZLUDA and HIP, but <a href="https://render.otoy.com/octanebench/" target="_blank">OctaneBench</a>, the OctaneRender benchmark, doesn’t run at all. </p>
<p><strong>Will more CUDA-based CG applications run under ZLUDA in future?</strong><br>
Janik says that without the backing of Intel or AMD, “realistically [ZLUDA] is now abandoned”.</p>
<p>Although he says that he is “open to any offers that could move the project forward”, without it, he is only likely to add support for NVIDIA technologies that interest him personally, like <a href="https://www.nvidia.com/en-gb/geforce/technologies/dlss/" target="_blank">DLSS</a>.</p>
<p>However, the source code is publicly available, and Janik suggests that even its current state, ZLUDA could be used by software devs as part of a “more gradual porting from CUDA to HIP”.</p>
<p><strong>License and system requirements</strong><br>
Compiled versions of ZLUDA 3 are available for Windows and Linux. The source code is available under either an <a href="https://github.com/vosen/ZLUDA?tab=Apache-2.0-1-ov-file" target="_blank">Apache 2.0</a> or <a href="https://github.com/vosen/ZLUDA?tab=MIT-2-ov-file" target="_blank">MIT license</a>.</p>
<p><a href="https://github.com/vosen/ZLUDA" target="_blank">Download ZLUDA 3 from the project’s GitHub repository</a></p>
<p><br><em>Have your say on this story by following CG Channel on <a href="https://www.facebook.com/cgchannel" target="_blank">Facebook</a>, <a href="https://www.instagram.com/thecgchannel/" target="_blank">Instagram</a> and <a href="https://twitter.com/theCGchannel" target="_blank">X (formerly Twitter)</a>. As well as being able to comment on stories, followers of our social media accounts can see videos we don’t post on the site itself, including making-ofs for the latest VFX movies, animations, games cinematics and motion graphics projects.</em></p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook, Instagram, WhatsApp outage (428 pts)]]></title>
            <link>https://downdetector.com/status/facebook/</link>
            <guid>39604590</guid>
            <pubDate>Tue, 05 Mar 2024 15:27:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://downdetector.com/status/facebook/">https://downdetector.com/status/facebook/</a>, See on <a href="https://news.ycombinator.com/item?id=39604590">Hacker News</a></p>
Couldn't get https://downdetector.com/status/facebook/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Shen Programming Language (107 pts)]]></title>
            <link>https://shenlanguage.org/</link>
            <guid>39602472</guid>
            <pubDate>Tue, 05 Mar 2024 12:30:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shenlanguage.org/">https://shenlanguage.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39602472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                              <td colspan="2">
                              <div>
                                <table>
                                  <tbody><tr>
                                    <td>
                                     
   						
						<p>

						Our mission is to bring the power of Shen technology to 
						every major programming platform used by industry and 
						deliver to programmers the great power of Shen.</p>
						
						<p>

						The word 'Shen' means 'highest spirit' in Chinese and indicates 
						our goal is to transcend the divisions between computer 
						languages.</p>
						
						<p>Since 2021 Shen has been based on the <a href="https://shenlanguage.org/s-series.html"> <span color="#B6D4D1"> S series kernels</span></a>.</p>
						
						<h2>Features</h2>
						
						<ul><li>pattern matching, </li>
<li>						          lambda calculus consistency,</li>
<li>                      macros for defining domain specific languages,</li>
<li>                      optional lazy evaluation,</li>
<li>                      static type checking based on sequent calculus, </li>
<li>                      one of the most powerful systems for typing in functional programming,</li>
<li>                      an integrated fully functional Prolog,</li>
<li>                      an inbuilt compiler-compiler,</li>
<li>                      a BSD kernel under 15 languages (Lisp, Python, Javascript, C ...)</li>
<li>                      and operating systems (Windows, Linux, OS/X),</li>
<li>                      is extensively documented in a book </li>
<li>                      has nearly a decade of use.</li>
</ul>                      
							
							<a href="https://shenlanguage.org/reviews.html"> <span color="#B6D4D1"> Read some reviews of Shen.</span></a>
<h2>Publications</h2>

<p>Find the support pages and purchase links for materials on Shen below.</p>

							 
							  </td>
                                    <td>
                                    
     
<h2>LATEST NEWS</h2>
                  <p>The <a href="https://shenlanguage.org/logiclab.html"> <span color="#B6D4D1"> support page</span></a> for the text <b> Programming the Logic Lab </b> now includes readable online access to the book.</p>
									<p>______________________________</p>
                  <p>The <a href="https://shenlanguage.org/lpc.html"> <span color="#B6D4D1"> support page</span></a> for the text <b> Logic, Proof and Computation </b> is established.</p>
									<p>______________________________</p>
									<p><a href="https://shenlanguage.org/THORN.pdf"> <span color="#B6D4D1"> THORN</span></a> <b> T</b>heorem prover derived from <b> HORN </b> clause logic is available.</p> 
									<p>______________________________</p>
									<p>Shen Education Channel starts on <a href="https://www.youtube.com/channel/UCZrMxRSqe0LITKewwhK2nvA/videos"> <span color="#B6D4D1"> Youtube</span></a>.</p>
									<p>______________________________</p>
									<p> <a href="https://shenlanguage.org/yggdrasil.html"> <span color="#B6D4D1"> Yggdrasil project</span></a> launched - the grand unification of programming languages.</p>
									<p>______________________________</p>
									<p>Want to advertise on this site? Go to the contacts page.</p> </td>
                                  </tr>
                                </tbody></table>
                              </div>
                              </td>
                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facing reality about the EU is a core requirement for good management (315 pts)]]></title>
            <link>https://www.baldurbjarnason.com/2024/facing-reality-in-the-eu-and-tech/</link>
            <guid>39602417</guid>
            <pubDate>Tue, 05 Mar 2024 12:22:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.baldurbjarnason.com/2024/facing-reality-in-the-eu-and-tech/">https://www.baldurbjarnason.com/2024/facing-reality-in-the-eu-and-tech/</a>, See on <a href="https://news.ycombinator.com/item?id=39602417">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
 <p>You can’t be a good manager or executive, in any industry, if you operate in constant denial of the facts on the ground. Arguing from ideology or beliefs that aren’t grounded in observation, measurement, or study, is the hallmark of a politician or media personality, not a manager responsible for other people’s jobs.</p>
<p>This should be obvious, but it isn’t.</p>
<p>Basing your opinion on factional loyalty and vibes is fine for a blogger or a pundit, but it does not make for a sound management strategy.</p>
<p>Unfortunately, it’s the default for modern executives, best seen in the popularity of mass lay-offs – a strategy that has been resoundly proven to be counter-productive, costly, even disastrous, along multiple dimensions, over multiple decades of study.</p>
<p>There are a few examples of this trend towards denying reality. The starkest one being Elon Musk behaving as if European labour unions don’t exist and that labour is entirely powerless, leading his companies to lose money on strikes and other collective actions.</p>
<p>But Elon Musk isn’t alone. It’s very common for US punditry to completely misunderstand the EU and analyse it as if it were a US political entity – imagining that its actions are driven by the same political and social dynamics as a protectionist industry within the US. They treat the EU’s actions as analogous to a coalition of traditional media companies, such as The New York Times and Washington Post, trying to bolster their industry against tech. They cover EU statements as if they were the comments of the taxi industry trying to stave off Uber and Lyft.</p>
<p>But that’s just not the reality of the situation and to understand what’s going on – to be able to make sound management decisions and form executive strategy – you need to understand what the EU is. More specifically, you need to understand the European Single Market.</p>
<p>It is trivially obvious that the management at Apple, Google, and Microsoft have not done this work. Apple especially seems to be in denial about the nature of the EU.</p>
<p>This should worry you, because understanding it isn’t hard – the equivalent of a single high school civics lesson – but they seem to be basing billion-dollar executive decisions on wishful thinking, and that’s a cause for concern.</p>
<h2 id="the-european-single-market">The European Single Market</h2>
<blockquote>
  <p>The single market: bringing Europe together</p>
  <p>One of the cornerstones of EU integration, the single market makes it possible for goods, services, capital and people to move across the bloc as freely as within a single country.</p>
  <p>It includes both EU and non-EU countries: Iceland, Liechtenstein and Norway take part through the European Economic Area they have established with the EU, while Switzerland has concluded a series of bilateral agreements with the EU that give the country partial access to the single market.</p>
<p><a href="https://www.europarl.europa.eu/topics/en/article/20230112STO66302/30-years-of-eu-single-market-benefits-and-challenges-infographics">30 years of EU single market: benefits and challenges</a></p>
</blockquote>
<p>The single market is, from the perspective of the EU itself, its single most important project. Greater trade unity and competitiveness with other markets is the very reason why its predecessors were formed. The EU is for lowering internal barriers to trade and services while protecting that internal market with whatever tools it deems necessary.</p>
<p>Most of the time that has involved some sort of <em>external</em> barrier.</p>
<p>This is the reason why I get a bit frustrated whenever I see somebody in tech dismisses the EU as just trying to protect European companies from competition with their glorious and wonderful US companies.</p>
<p>It is, to put it bluntly, an ignorant thing to say.</p>
<p>The EU absolutely <em>is</em> for protecting and strengthening the European single market.</p>
<p><em>This is not a gotcha!</em></p>
<p>There’s a lot to dislike about the EU. They operate internally on some of the worst ideas to come out of post-war economics theory. They are all too willing to let individual member countries slide into fascism. And, as an institution, they are largely all too convinced that an all-seeing universal surveillance state would be a good thing, actually.</p>
<p>These are all good reasons to criticise the EU.</p>
<p><em>But the single market is what it’s for.</em> Without it, the EU would cease to exist. To understand what motivates EU, as an organisation, you need to understand the single market.</p>
<p>Whenever I point out on social media that the single market is the purpose of the EU, I get bombarded by replies saying: “No you’re wrong. The EU was founded to preserve peace in Europe. Gotcha!”</p>
<p>What mechanism do you think they used to preserve that peace? Cozy feelings about elections for the EU parliament? Happy thoughts about student exchange programs?</p>
<p>The single market <em>is</em> the EU peace project, which makes protecting it an existential issue to many involved in the EU.</p>
<blockquote>
<p>The process of European integration has many elements in common with the liberal theory of peace articulated by the US President Woodrow Wilson (1918) in his ‘Fourteen Points’ address to the Congress. In particular, there is and has been an emphasis on trade liberalization, democracy and open agreements as means of ensuring peace. Although the Second World War had shaken belief in the liberal approach to peace, the EU’s founding fathers—including Jean Monnet, Robert Schuman, Paul-Henri Spaak, Alcide de Gaspari and Konrad Adenauer—sought to build upon it.</p>
<p>Their approach to promoting peace through integration stressed the importance of trade liberalization and (at least implicitly) democracy. To these ingredients, they added an emphasis on functional integration and the creation of supranational institutions.</p>
<p><a href="https://journals.sagepub.com/doi/full/10.1177/1369148116685274">“European integration as a peace project”</a></p>
</blockquote>
<p>Many people today either don’t know this or have forgotten, even people heavily involved in EU industry, but the EU was formed on the basis of the liberal idea that integrated economies will not go to war.</p>
<p>Liberalism in general has always operated on the theory that free trade promotes peace. It’s what drove much of the US’s efforts towards globalisation in the post-war era.</p>
<p>But the EU peace project is explicitly not global.</p>
<blockquote>
<p>Their European project also diverged from the Wilsonian vision in another, significant way. The European project was explicitly regional, rather than global, plurilateral rather than multilateral.</p>
<p><a href="https://journals.sagepub.com/doi/full/10.1177/1369148116685274">“European integration as a peace project”</a></p>
</blockquote>
<p>The purpose of is the EU single market, and EU big industries generally believe it has worked.</p>
<p>In the words of <em>BDI</em>, <a href="https://english.bdi.eu/bdi/about-us">the Federation of German Industries</a>:</p>
<blockquote>
<p>Alongside the preservation of lasting peace in Europe, the single market is the greatest achievement of the European integration project. At the same time, it is the central basis for the future of the European Union. Only a well-integrated market is competitive, creates prosperity and jobs, guarantees stability and secures Europe’s political influence in the world. The continuous deepening of the single market in all areas is therefore imperative so we can continue to defend and claim our European sovereignty, our standards and values globally.</p>
<p><a href="https://urbis.secure.europarl.europa.eu/urbis/system/files/generated/document/en/20220101_Publications_BDI_Making%2520the%2520Single%2520Market%2520the%2520EU%2527s%2520Growth%2520Engine.pdf">“Making the Single Market the EU’s Growth Engine”</a></p>
</blockquote>
<h2 id="why-should-us-tech-companies-care-about-eu-history">Why should US tech companies care about EU history?</h2>
<p>When US tech is fighting the EU on core policy issues, they are not fighting a few individual bureaucrats with an agenda. They are fighting a multinational trade organisation with an agenda.</p>
<p>To be effective, it pays to understand the agenda.</p>
<p>Integrating a single market has been a decades-long task along multiple dimensions, much of it taking the form of standardisation.</p>
<ul>
<li>Voltage harmonisation ensured that a device bought in one EU country won’t be destroyed by simply plugging it into an outlet in a different country. The EU standardised on 230v electricity.</li>
<li>The many <a href="https://commission.europa.eu/business-economy-euro/product-safety-and-requirements/eu-product-requirements_en">product standards</a> aren’t just there for the consumers but also for <em>trade</em> and <em>industry</em>. Businesses can better trust the work products they’re buying and industries need to worry less about parts and materials. The US has product standards as well, for similar reasons, but the EU arguably takes it further.</li>
<li>Phone charger harmonisation. We went from having dozens of different connectors to only two and, soon, hopefully only one.</li>
</ul>
<p>This is only a tiny part of the massive project that is the European Single Market, but it highlights just how important <em>standardisation</em> is to the EU theory of what makes a market free and open. The core mechanism that helps unify the single market is standardisation and the ability to move freely within the market.</p>
<p><a href="https://en.wikipedia.org/wiki/European_Union_roaming_regulations">The EU ban on roaming charges</a> is a good example. European mobile phone companies generally aren’t allow to inflict surcharges for using their service across the EU.</p>
<p>Or, another way to put it would be that the EU does not let private parties turn the single market into multiple subdivided markets under private control. It doesn’t matter that most of these companies were European, their practices threatened the very concept of a single market, so the practice had to be eased out.</p>
<p>Private parties are not allowed to divide or fragment the single market, allowing that in the long term is an existential threat to the EU, because the single market is <em>what the EU is for.</em></p>
<p>The roaming regulations are also widely considered to be a success, even beyond their obvious popularity among EU consumers.</p>
<blockquote>
<p>Assuming that our estimates are representative across the EU, the total consumer surplus gain of RLAH would be around €2 billion in 2017.</p>
<p><a href="https://academic.oup.com/ej/advance-article/doi/10.1093/ej/uead101/7444993?login=false#437472001">“The Welfare Effects of Mobile Internet Access: Evidence from Roam-Like-at-Home”</a></p>
</blockquote>
<p>And:</p>
<blockquote>
<p>In total, however, the suggestive evidence presented here suggests that consumer surplus increased more than network operator surplus decreased, implying that RLAH had overall positive welfare effects.</p>
<p><a href="https://academic.oup.com/ej/advance-article/doi/10.1093/ej/uead101/7444993?login=false#437472001">“The Welfare Effects of Mobile Internet Access: Evidence from Roam-Like-at-Home”</a></p>
</blockquote>
<p>Data use increased across the board, meaning that digital service providers benefited from the change, increasing economic activity there as well.</p>
<p>From the EU’s perspective, taking action to prevent private parties from fragmenting and taking private control over the single market simultaneously grew the economy and increased consumer surplus.</p>
<p>This is the operating theory behind much of the actions the EU takes regarding market regulation and product standardisation: <em>a single market built on standards is more profitable for both businesses and consumers.</em></p>
<p>It’s obviously a deeply capitalist perspective, but if you want to understand the actions and motivations of the EU as an organisation you need to understand their operating theories.</p>
<p>They are also quite pragmatic about it. For example, the EU has standardised on USB-C as the standard connector for phones. Phones are not infrastructure and people upgrade them regularly. USB-C is a flexible connector that’s even today capable of much more than most phones need, both in terms of data transfer and current. Mandating that future phones all use the same connector doesn’t require you to swap out existing phones or chargers and would increase customer surplus in the future at relatively little cost to the industry, which seems to be migrating to USB-C anyway. <em>Even Apple</em>, before the latest USB-C enabled iPhone was released, has slowly been migrating many of its products over to USB-C. That’s why standardising on USB-C looked to the EU to be the market equivalent of a free lunch and Apple’s protests to the contrary sounded weak.</p>
<p>In contrast, EU electrical plugs aren’t standardised. <em>The REFIT Platform</em>, at the EU Commission’s behest, explained it this way:</p>
<blockquote>
<p>The REFIT Platform does not recommend harmonising the plugs and socket-outlet systems in Europe, due to (i) the strong social and economic impact on the citizens without evident benefits in terms of safety and (ii) the fact that the EU and Member States may currently have other legislative and investment priorities.</p>
<p><a href="https://commission.europa.eu/system/files/2017-09/xii24a_plugs_and_sockets.pdf">“REFIT Platform Opinion on the submission by a citizen on Plugs and Socket”</a></p>
</blockquote>
<p>Plugs and outlets are infrastructure and can be in place for decades. The costs of standardisation would almost certainly be much higher than any potential consumer or business benefit, so they aren’t standardised.</p>
<p>The EU, as an organisation, has a specific economic theory that guides most of its actions. Once you understand the theory, they become very predictable.</p>
<p>This is why Apple’s battle against the EU on both app stores and web browsers could become very costly.</p>
<h2 id="the-single-market-versus-apples-privately-fragmented-app-store-market">The single market versus Apple’s privately fragmented App Store market</h2>
<p>Much like roaming, App Stores let private companies subdivide and control the single market to their own financial gain. When much of the digital economy is taking place on phones, tablets, and various other devices that are largely limited to App Stores, this is effectively ceding the single market to a fragmented market that’s entirely under corporate control.</p>
<p>This is against the core operating theory behind the EU. They would be institutionally against this even if the companies in question were European. Many, if not most, of the mobile phone operators affected by the roaming regulations were European. That didn’t earn them a pass on compliance.</p>
<p>The web is also the closest thing to an international common standard we have for delivering digital services and software.</p>
<p>It shouldn’t be a surprise to anybody that the EU is very concerned about preserving the single market in digital services and software. That means they <em>have to</em> do something about Apple’s control over the iOS App Store and exclusion of competing web browsers. From their perspective, they don’t really have a choice.</p>
<p>What should surprise you is how accommodating and outright <em>gentle</em> the EU has been with Apple’s shenanigans over the years, whether it was about exploiting loopholes to avoid phone plug standardisation, or their <a href="https://ec.europa.eu/commission/presscorner/detail/en/ip_24_1161">violations of EU antitrust regulation</a> that <em>pre-date</em> the new <em>Digital Markets Act</em>.</p>
<p>That’s because the EU is manifestly pro-business, but it feels forced to act because <em>the single market is the EU and the EU is the single market.</em></p>
<p>To Apple, the App Store is a side line. To the EU, the single market is the foundation of its existence.</p>
<p>Any time you see two entities of similar size fight, bet on the one that thinks it’s fighting for its life.</p>
<p>Preserving the single market is <em>explicitly</em> the goal of the new <em>Digital Single Market</em> regulation. In a resolution adopted by the EU Parliament on the 30th anniversary of the single market the MEP emphasised that:</p>
<blockquote>
<p>… the recent entry into force of the Digital Markets Act and the Digital Services Act represents an essential contribution to the creation of a harmonised, fair, competitive and trustworthy digital single market; considers it essential to ensure the effective implementation and enforcement of these two legislative acts including by making available sufficient financial and human resources; calls on the Commission to monitor implementation of these acts continuously and closely and to report to the relevant parliamentary committee accordingly;</p>
<p><a href="https://www.europarl.europa.eu/doceo/document/TA-9-2023-0007_EN.html">“European Parliament resolution of 18 January 2023 on the 30th anniversary of the single market: celebrating achievements and looking towards future developments”</a></p>
</blockquote>
<p>The EU’s desire to prevent the fragmentation of the digital single market has the backing of the EU political sector.</p>
<p>And, remember, the EU is a capitalist entity that serves European industrial and capital needs. But they, too, are <em>strongly</em> in favour of the EU taking action to prevent the fragmentation of the digital single market, what the BDI usually calls <em>digital sovereignty</em>:</p>
<blockquote>
<p>Studies show: Completion of the digital single market would unleash a growth potential of around EUR 110 billion per year. Therefore, the European Commission, the European Parliament and the Member States should work towards a uniform, innovation-promoting, technology-open and industry-friendly regulatory framework for digital policy.</p>
<p><a href="https://urbis.secure.europarl.europa.eu/urbis/system/files/generated/document/en/20220101_Publications_BDI_Making%2520the%2520Single%2520Market%2520the%2520EU%2527s%2520Growth%2520Engine.pdf">“Making the Single Market the EU’s Growth Engine”</a></p>
</blockquote>
<p>I’m not asking you to agree with either of these organisations, but if you are in a management or executive position at a non-EU tech company, you absolutely do need to <em>understand</em> them, otherwise you are going to get blindsided, just like Apple.</p>
<h2 id="apple-got-blindsided-by-the-eu-because-they-refused-to-understand-the-principle-theories-that-motivate-the-eu">Apple got blindsided by the EU because they refused to understand the principle theories that motivate the EU</h2>
<p><a href="https://www.theverge.com/2024/3/4/24005938/european-commission-antitrust-apple-investigation-anti-steering-rules-app-developers">This is costing them money</a>, and it’s going to cost them more money in the future.</p>
<p>Normally when the EU regulates a given sector, it does so with ample lead time and works with industry to make sure that they understand their obligations.</p>
<p>Apple instead thought that the regulatory contact from the EU during the lead time to the DMA was an opportunity for it to lecture the EU on its right to exist. Then its executives made up some fiction in their own minds as to what the regulation meant, announced their changes, <a href="https://techcrunch.com/2024/03/01/apple-reverses-decision-about-blocking-web-apps-on-iphones-in-the-eu/">only to discover later that they were full of bullshit.</a></p>
<p>This was entirely Apple’s own fault. For months, we’ve been hearing leaks about Apple’s talks with the EU about the Digital Market Act. Those talks <em>were not negotiations</em> even though Apple seems to have thought they were. Talks like those are to help companies implement incoming regulations, with some leeway for interpretation on the EU’s side to accommodate business interests.</p>
<p>Remember what I wrote about electrical plugs? The EU is pro-business – often criticised for being essentially a pro-business entity – and not in favour of regulation for regulation’s sake.</p>
<p>If Apple had faced reality and tried to understand the facts <em>as they are</em>, they would have used the talks to clarify all of these issues and more well in advance of the <em>DMA</em> taking effect.</p>
<p>But they didn’t because they have caught the tech industry management disease of demanding that reality bend to their ideas and wishes.</p>
<p>They were behaving like Elon Musk.</p>
<p>And we certainly don’t want more Elon Musks in the world.</p>

  
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[France enshrines 'freedom' to abortion in Constitution, in world first (121 pts)]]></title>
            <link>https://www.lemonde.fr/en/politics/article/2024/03/04/france-enshrines-freedom-to-abortion-in-constitution-in-world-first_6584252_5.html</link>
            <guid>39602035</guid>
            <pubDate>Tue, 05 Mar 2024 11:28:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lemonde.fr/en/politics/article/2024/03/04/france-enshrines-freedom-to-abortion-in-constitution-in-world-first_6584252_5.html">https://www.lemonde.fr/en/politics/article/2024/03/04/france-enshrines-freedom-to-abortion-in-constitution-in-world-first_6584252_5.html</a>, See on <a href="https://news.ycombinator.com/item?id=39602035">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="habillagepub">             <header> <div>    <ul>       <li>  <a href="https://www.lemonde.fr/en/politics/">  French Politics </a>  </li>  <li> <a href="https://www.lemonde.fr/en/abortion/"> Abortion </a> </li>    </ul>    <div>   <p>    A joint session of Parliament voted to adopt a constitutional reform to protect the 'freedom of women to voluntarily terminate a pregnancy, which is guaranteed.' </p>  </div>          </div> </header>       <section> <article>                       <figure> <picture> <source srcset=" https://img.lemde.fr/2024/03/04/0/0/2500/1667/556/0/75/0/af765b9_1709565633119-ef63c4b-1709564858902-unnamed-7.jpg 556w, https://img.lemde.fr/2024/03/04/0/0/2500/1667/600/0/75/0/af765b9_1709565633119-ef63c4b-1709564858902-unnamed-7.jpg 600w, https://img.lemde.fr/2024/03/04/0/0/2500/1667/664/0/75/0/af765b9_1709565633119-ef63c4b-1709564858902-unnamed-7.jpg 664w, https://img.lemde.fr/2024/03/04/0/0/2500/1667/700/0/75/0/af765b9_1709565633119-ef63c4b-1709564858902-unnamed-7.jpg 700w, https://img.lemde.fr/2024/03/04/0/0/2500/1667/800/0/75/0/af765b9_1709565633119-ef63c4b-1709564858902-unnamed-7.jpg 800w" sizes="(min-width: 1024px) 556px, 100vw" alt="Members of French Parliament applaud at a joint session of Parliament to vote on a reform of the Constitution to include the " guaranteed="" freedom"="" to="" abortion,="" in="" versailles,="" on="" march="" 4,="" 2024."="" width="664" height="443"> <img src="https://img.lemde.fr/2024/03/04/0/0/2500/1667/664/0/75/0/af765b9_1709565633119-ef63c4b-1709564858902-unnamed-7.jpg" alt="Members of French Parliament applaud at a joint session of Parliament to vote on a reform of the Constitution to include the " guaranteed="" freedom"="" to="" abortion,="" in="" versailles,="" on="" march="" 4,="" 2024."="" sizes="(min-width: 1024px) 556px, 100vw" width="664" height="443"> </picture>     </figure>             <p>France's Parliament approved a bill to enshrine a woman's right to an abortion in the Constitution in a historic vote on Monday, March 4, as lawmakers gathered for a joint session of Parliament at the Palace of Versailles. The bill was approved in an overwhelming 780-72 vote, and nearly the entire joint session stood in a long standing ovation.</p>            <p>The constitutional reform amends Article 34 of the French Constitution to specify that "the law determines the conditions by which is exercised the freedom of women to voluntarily terminate a pregnancy, which is guaranteed."</p>                                              <p>The measure was promised by President Emmanuel Macron following a rollback of abortion rights in court rulings in the United States. After the bill's adoption, Macron described the move as "French pride" that had sent a "universal message." A special public ceremony is planned to celebrate the move on International Women's Rights Day on March 8.</p>                 <p>In the lead up to the historic vote, Prime Minister Gabriel Attal addressed the 925 lawmakers gathered for the joint session in Versailles, and called on them to make France a leader in women's rights and set an example in defense of women's rights for countries around the world.</p>            <p>"We have a moral debt to women," Attal said. He paid tribute to Simone Veil, a prominent legislator, former health minister and key feminist who in 1975 championed the bill that decriminalized abortion in France. "We have a chance to change history," Attal said in a moving and determined speech. "Make Simone Veil proud," he said to a standing ovation.</p>                                                       <figure> <picture> <source srcset=" https://img.lemde.fr/2024/03/04/0/0/7178/4788/556/0/75/0/245a529_0c4f6a2921d1492995162eb8c8fed44a-0-24faf5d003014df3872c6cbc62a9cf77.jpg 556w, https://img.lemde.fr/2024/03/04/0/0/7178/4788/600/0/75/0/245a529_0c4f6a2921d1492995162eb8c8fed44a-0-24faf5d003014df3872c6cbc62a9cf77.jpg 600w, https://img.lemde.fr/2024/03/04/0/0/7178/4788/664/0/75/0/245a529_0c4f6a2921d1492995162eb8c8fed44a-0-24faf5d003014df3872c6cbc62a9cf77.jpg 664w, https://img.lemde.fr/2024/03/04/0/0/7178/4788/700/0/75/0/245a529_0c4f6a2921d1492995162eb8c8fed44a-0-24faf5d003014df3872c6cbc62a9cf77.jpg 700w, https://img.lemde.fr/2024/03/04/0/0/7178/4788/800/0/75/0/245a529_0c4f6a2921d1492995162eb8c8fed44a-0-24faf5d003014df3872c6cbc62a9cf77.jpg 800w" sizes="(min-width: 1024px) 556px, 100vw" alt="French Prime Minister Gabriel Attal speaks during the joint session of both houses of Parliament at the Palace of Versailles in Versailles, west of Paris, Monday, March 4, 2024." width="664" height="443"> <img src="https://img.lemde.fr/2024/03/04/0/0/7178/4788/664/0/75/0/245a529_0c4f6a2921d1492995162eb8c8fed44a-0-24faf5d003014df3872c6cbc62a9cf77.jpg" alt="French Prime Minister Gabriel Attal speaks during the joint session of both houses of Parliament at the Palace of Versailles in Versailles, west of Paris, Monday, March 4, 2024." sizes="(min-width: 1024px) 556px, 100vw" width="664" height="443"> </picture>     </figure>             <p>The <a href="https://www.lemonde.fr/en/france/article/2024/01/30/france-s-assemblee-nationale-approves-bill-securing-right-to-abortion_6479552_7.html">Assemblée Nationale overwhelmingly approved</a> the proposal in January. The <a href="https://www.lemonde.fr/en/france/article/2024/02/29/france-one-step-closer-to-enshrining-abortion-freedom-in-constitution-after-senat-vote_6571897_7.html">Sénat adopted the bill last Wednesday</a>, clearing a key hurdle for legislation intended to make "a woman's right to have an abortion irreversible." The measure needed to be approved by a three-fifths majority in the joint session for the Constitution to be amended.</p>                <p>None of France's major political parties represented in Parliament have questioned the right to abortion, including Marine Le Pen's far-right Rassemblement National (RN) party and the conservative Les Républicains party. However, some lawmakers have voted against inscribing abortion rights into the Constitution in previous votes in both houses.</p>            <p>Le Pen said Monday her party would vote in favor of the bill but added that "there is no need to make this a historic day." Out of the 91 RN members of Parliament, 49 voted for the bill, 11 against, and 20 abstained.</p>            <p>The right to an abortion has broad support among the French public. A recent poll showed support at over 80%, consistent with previous surveys. The same poll also showed that a solid majority of people are in favor of enshrining it in the Constitution.</p>            <h2>Citing the US</h2>            <p>There were scenes of celebrations around France ahead of the historical vote. The Eiffel Tower was lit up in celebration after the change was passed with the slogan "My Body My Choice."</p>                     <figure> <picture> <source srcset=" https://img.lemde.fr/2024/03/04/0/0/4112/2741/556/0/75/0/249a80e_2024-03-04t181107z-1128684611-rc25f6a565ty-rtrmadp-3-france-politics-abortion.JPG 556w, https://img.lemde.fr/2024/03/04/0/0/4112/2741/600/0/75/0/249a80e_2024-03-04t181107z-1128684611-rc25f6a565ty-rtrmadp-3-france-politics-abortion.JPG 600w, https://img.lemde.fr/2024/03/04/0/0/4112/2741/664/0/75/0/249a80e_2024-03-04t181107z-1128684611-rc25f6a565ty-rtrmadp-3-france-politics-abortion.JPG 664w, https://img.lemde.fr/2024/03/04/0/0/4112/2741/700/0/75/0/249a80e_2024-03-04t181107z-1128684611-rc25f6a565ty-rtrmadp-3-france-politics-abortion.JPG 700w, https://img.lemde.fr/2024/03/04/0/0/4112/2741/800/0/75/0/249a80e_2024-03-04t181107z-1128684611-rc25f6a565ty-rtrmadp-3-france-politics-abortion.JPG 800w" sizes="(min-width: 1024px) 556px, 100vw" alt="The Eiffel Tower lights up with the message " my="" body="" choice"="" after="" french="" lawmakers="" enshrined="" the="" right="" to="" abortion="" in="" its="" constitution="" during="" a="" special="" congress="" versailles,="" paris,="" france,="" march="" 4,="" 2024."="" width="664" height="443"> <img src="https://img.lemde.fr/2024/03/04/0/0/4112/2741/664/0/75/0/249a80e_2024-03-04t181107z-1128684611-rc25f6a565ty-rtrmadp-3-france-politics-abortion.JPG" alt="The Eiffel Tower lights up with the message " my="" body="" choice"="" after="" french="" lawmakers="" enshrined="" the="" right="" to="" abortion="" in="" its="" constitution="" during="" a="" special="" congress="" versailles,="" paris,="" france,="" march="" 4,="" 2024."="" sizes="(min-width: 1024px) 556px, 100vw" width="664" height="443"> </picture>     </figure>             <p>With the right to an abortion added to the constitution, it will be much harder to prevent women from voluntarily terminating a pregnancy in France, women's rights and equality activists said. Sarah Durocher, a leader in the Family Planning movement, said Monday's vote is "a victory for feminists and a defeat for the anti-choice activists." "We increased the level of protection to this fundamental right," said Anne-Cécile Mailfert of the Women's Foundation. "It's a guarantee for women today and in the future to have the right to abort in France."</p>                     <figure> <picture> <source srcset=" https://img.lemde.fr/2024/03/04/0/0/5237/3492/556/0/75/0/1d84c49_5813277-01-06.jpg 556w, https://img.lemde.fr/2024/03/04/0/0/5237/3492/600/0/75/0/1d84c49_5813277-01-06.jpg 600w, https://img.lemde.fr/2024/03/04/0/0/5237/3492/664/0/75/0/1d84c49_5813277-01-06.jpg 664w, https://img.lemde.fr/2024/03/04/0/0/5237/3492/700/0/75/0/1d84c49_5813277-01-06.jpg 700w, https://img.lemde.fr/2024/03/04/0/0/5237/3492/800/0/75/0/1d84c49_5813277-01-06.jpg 800w" sizes="(min-width: 1024px) 556px, 100vw" alt="A woman clenches her fist and others hold flags of the Women's Foundation rights group as they gather at the Place du Trocadéro in Paris, on March 4, 2024." width="664" height="443"> <img src="https://img.lemde.fr/2024/03/04/0/0/5237/3492/664/0/75/0/1d84c49_5813277-01-06.jpg" alt="A woman clenches her fist and others hold flags of the Women's Foundation rights group as they gather at the Place du Trocadéro in Paris, on March 4, 2024." sizes="(min-width: 1024px) 556px, 100vw" width="664" height="443"> </picture>     </figure>             <p>The government argued in its introduction to the bill that the right to abortion is threatened in the United States. "Unfortunately, this event is not isolated: in many countries, even in Europe, there are currents of opinion that seek to hinder at any cost the freedom of women to terminate their pregnancy if they wish," the introduction to the French legislation says.</p>                                              <p>The <a href="https://www.lemonde.fr/en/international/article/2022/06/25/in-the-united-states-abortion-is-no-longer-a-federal-right_5987955_4.html">decision by the US Supreme Court to strip women of the right to abortion</a> has reverberated across Europe's political landscape, forcing the issue back into public debate in France at a time of political upheaval.</p>       <section> <p> Partner service </p> <a href="https://learn-french.lemonde.fr/?rfextension=LME" target="_blank"> <p>Learn French with Gymglish</p> <p>Thanks to a daily lesson, an original story and a personalized correction, in 15 minutes per day.</p> <p>Try for free</p> </a> </section>        <p>"It only takes a moment for everything we thought that we have achieved to fade away," said Yaël Braun-Pivet, the first female president of a chamber of French Parliament, in her address to the joint session.</p>                     <figure> <picture> <source srcset=" https://img.lemde.fr/2024/03/04/0/0/4093/2774/556/0/75/0/02c4065_2024-03-04t180031z-1504764799-rc25f6ayv3i4-rtrmadp-3-france-politics-abortion.JPG 556w, https://img.lemde.fr/2024/03/04/0/0/4093/2774/600/0/75/0/02c4065_2024-03-04t180031z-1504764799-rc25f6ayv3i4-rtrmadp-3-france-politics-abortion.JPG 600w, https://img.lemde.fr/2024/03/04/0/0/4093/2774/664/0/75/0/02c4065_2024-03-04t180031z-1504764799-rc25f6ayv3i4-rtrmadp-3-france-politics-abortion.JPG 664w, https://img.lemde.fr/2024/03/04/0/0/4093/2774/700/0/75/0/02c4065_2024-03-04t180031z-1504764799-rc25f6ayv3i4-rtrmadp-3-france-politics-abortion.JPG 700w, https://img.lemde.fr/2024/03/04/0/0/4093/2774/800/0/75/0/02c4065_2024-03-04t180031z-1504764799-rc25f6ayv3i4-rtrmadp-3-france-politics-abortion.JPG 800w" sizes="(min-width: 1024px) 556px, 100vw" alt="Yaël Braun-Pivet, president of the Assemblée Nationale, applauds after French lawmakers enshrined the right to abortion in its constitution, at the Versailles Palace near Paris, France, March 4, 2024." width="664" height="443"> <img src="https://img.lemde.fr/2024/03/04/0/0/4093/2774/664/0/75/0/02c4065_2024-03-04t180031z-1504764799-rc25f6ayv3i4-rtrmadp-3-france-politics-abortion.JPG" alt="Yaël Braun-Pivet, president of the Assemblée Nationale, applauds after French lawmakers enshrined the right to abortion in its constitution, at the Versailles Palace near Paris, France, March 4, 2024." sizes="(min-width: 1024px) 556px, 100vw" width="664" height="443"> </picture>     </figure>             <p>Amending the constitution is a laborious process and a rare event in France, requiring either a referendum or a three-fifths majority vote in Parliament after both houses agree on common language. Since it was enacted in 1958, the French Constitution has been amended 17 times. The last time was in 2008, when Parliament was awarded more powers and French citizens were granted the right to bring their grievances to the Constitutional Court.</p>                         <section>  <p>  <span>Le Monde with AP and AFP</span>  </p>   </section>       </article>   </section>                   </section><section id="js-capping-old-article" data-full="0" data-mini="0"> <section id="js-capping-old-article-header"> <span></span> <p>Lecture restreinte</p> </section> <section id="js-capping-old-article-content"> <p>Votre abonnement n’autorise pas la lecture de cet article</p>  <p>Pour plus d’informations, merci de contacter notre service commercial.</p> </section> </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cloudflare Announces Firewall for AI (192 pts)]]></title>
            <link>https://blog.cloudflare.com/firewall-for-ai</link>
            <guid>39602023</guid>
            <pubDate>Tue, 05 Mar 2024 11:26:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/firewall-for-ai">https://blog.cloudflare.com/firewall-for-ai</a>, See on <a href="https://news.ycombinator.com/item?id=39602023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post"><article><p>03/04/2024</p><section><p>9 min read</p><div><figure><img src="https://blog.cloudflare.com/content/images/2024/03/WAF-for-AI.png" alt="Cloudflare announces Firewall for AI" loading="lazy" width="1600" height="905"></figure><p>Today, Cloudflare is announcing the development of Firewall for AI, a protection layer that can be deployed in front of <a href="https://www.cloudflare.com/en-gb/learning/ai/what-is-large-language-model/">Large Language Models (LLMs)</a> to identify abuses before they reach the models. </p><p>While AI models, and specifically LLMs, are surging, customers tell us that they are concerned about the best strategies to secure their own LLMs. Using LLMs as part of Internet-connected applications introduces new vulnerabilities that can be exploited by bad actors. </p><p>Some of the vulnerabilities affecting traditional web and API applications apply to the LLM world as well, including injections or data exfiltration. However, there is a new set of threats that are now relevant because of the way LLMs work. For example, researchers have <a href="https://thehackernews.com/2024/02/new-hugging-face-vulnerability-exposes.html">recently discovered</a> a vulnerability in an AI collaboration platform that allows them to hijack models and perform unauthorized actions.</p><p>Firewall for AI is an advanced <a href="https://www.cloudflare.com/learning/ddos/glossary/web-application-firewall-waf/">Web Application Firewall (WAF)</a> specifically tailored for applications using LLMs. It will comprise a set of tools that can be deployed in front of applications to detect vulnerabilities and provide visibility to model owners. The tool kit will include products that are already part of WAF, such as Rate Limiting and Sensitive Data Detection, and a new protection layer which is currently under development. This new validation analyzes the prompt submitted by the end user to identify attempts to exploit the model to extract data and other abuse attempts. Leveraging the size of Cloudflare network, Firewall for AI runs as close to the user as possible, allowing us to identify attacks early and protect both end user and models from abuses and attacks. </p><p>Before we dig into how Firewall for AI works and its full feature set, let’s first examine what makes LLMs unique, and the attack surfaces they introduce. We’ll use the <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">OWASP Top 10 for LLMs</a> as a reference.</p><h2 id="why-are-llms-different-from-traditional-applications">Why are LLMs different from traditional applications?</h2><p>When considering LLMs as Internet-connected applications, there are two main differences compared with more traditional web apps. </p><p>First, the way users interact with the product. Traditional apps are deterministic in nature. Think about a bank application — it’s defined by a set of operations (check my balance, make a transfer, etc.). The security of the business operation (and data) can be obtained by controlling the fine set of operations accepted by these endpoints: “GET /balance” or “POST /transfer”. </p><p>LLM operations are non-deterministic by design. To start with, LLM interactions are based on natural language, which makes identifying problematic requests harder than matching attack signatures. Additionally, unless a response is cached, LLMs typically provide a different response every time — even if the same input prompt is repeated. This makes limiting the way a user interacts with the application much more difficult. This poses a threat to the user as well, in terms of being exposed to misinformation that weakens the trust in the model.</p><p>Second, a big difference is how the application control plane interacts with the data. In traditional applications, the control plane (code) is well separated from the data plane (database). The defined operations are the only way to interact with the underlying data (e.g. show me the history of my payment transactions). This allows security practitioners to focus on adding checks and guardrails to the control plane and thus protecting the database indirectly. </p><p>LLMs are different in that the training data becomes part of the model itself through the training process, making it extremely difficult to control how that data is shared as a result of a user prompt. Some architectural solutions are being explored, such as separating LLMs into different levels and segregating data. However, no silver bullet has yet been found.</p><p>From a security perspective, these differences allow attackers to craft new attack vectors that can target LLMs and fly under the radar of existing security tools designed for traditional web applications.</p><h3 id="owasp-llm-vulnerabilities">OWASP LLM Vulnerabilities</h3><p>The <a href="https://owasp.org/">OWASP</a> foundation <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">released a list of</a> the top 10 classes of vulnerabilities for LLMs, providing a useful framework for thinking about how to secure language models. Some of the threats are reminiscent of the <a href="https://owasp.org/www-project-top-ten/">OWASP top 10 for web applications</a>, while others are specific to language models. </p><p>Similar to web applications, some of these vulnerabilities can be best addressed when the LLM application is designed, developed, and trained. For example, <em>Training Data Poisoning</em> can be carried out by introducing vulnerabilities in the training data set used to train new models. Poisoned information is then presented to the user when the model is live. <em>Supply Chain Vulnerabilities </em>and<em> Insecure Plugin Design </em>are vulnerabilities introduced in components added to the model, like third-party software packages.<em> </em>Finally, managing authorization and permissions is crucial when dealing with<em> Excessive Agency</em>,<em> </em>where unconstrained models can perform unauthorized actions within the broader application or infrastructure. </p><p>Conversely, <em>Prompt Injection</em>, <em>Model Denial of Service</em>, and <em>Sensitive Information Disclosure</em> can be mitigated by adopting a proxy security solution like Cloudflare Firewall for AI. In the following sections, we will give more details about these vulnerabilities and discuss how Cloudflare is optimally positioned to mitigate them.</p><h3 id="llm-deployments">LLM deployments</h3><p>Language model risks also depend on the deployment model. Currently, we see three main deployment approaches: internal, public, and product LLMs. In all three scenarios, you need to protect models from abuses, protect any proprietary data stored in the model, and protect the end user from misinformation or from exposure to inappropriate content.</p><ul><li><strong>Internal LLMs:</strong> Companies develop LLMs to support the workforce in their daily tasks. These are considered corporate assets and shouldn’t be accessed by non-employees. Examples include an AI co-pilot trained on sales data and customer interactions used to generate tailored proposals, or an LLM trained on an internal knowledge base that can be queried by engineers.</li><li><strong>Public LLMs:</strong> These are LLMs that can be accessed outside the boundaries of a corporation. Often these solutions have free versions that anyone can use and they are often trained on general or public knowledge. Examples include <a href="https://openai.com/gpt-4">GPT</a> from OpenAI or <a href="https://www.anthropic.com/product">Claude</a> from Anthropic.</li><li><strong>Product LLM:</strong> From a corporate perspective, LLMs can be part of a product or service offered to their customers. These are usually self-hosted, tailored solutions that can be made available as a tool to interact with the company resources. Examples include customer support chatbots or <a href="https://blog.cloudflare.com/security-analytics-ai-assistant/">Cloudflare AI Assistant</a>. </li></ul><p>From a risk perspective, the difference between Product and Public LLMs is about who carries the impact of successful attacks. Public LLMs are considered a threat to data because data that ends up in the model can be accessed by virtually anyone. This is one of the reasons many corporations advise their employees not to use confidential information in prompts for publicly available services. Product LLMs can be considered a threat to companies and their intellectual property if models had access to proprietary information during training (by design or by accident).</p><h2 id="firewall-for-ai">Firewall for AI</h2><p>Cloudflare Firewall for AI will be deployed like a traditional WAF, where every API request with an LLM prompt is scanned for patterns and signatures of possible attacks. </p><p>Firewall for AI can be deployed in front of models hosted on the Cloudflare <a href="https://blog.cloudflare.com/workers-ai">Workers AI</a> platform or models hosted on any other third party infrastructure. It can also be used alongside Cloudflare <a href="https://developers.cloudflare.com/ai-gateway/">AI Gateway</a>, and customers will be able to control and set up Firewall for AI using the WAF control plane.</p><figure><img src="https://blog.cloudflare.com/content/images/2024/03/image1-1.png" alt="" loading="lazy" width="1690" height="573"><figcaption><em>Firewall for AI works like a traditional web application firewall. It is deployed in front of an LLM application and scans every request to identify attack signatures</em></figcaption></figure><h3 id="prevent-volumetric-attacks">Prevent volumetric attacks</h3><p>One of the threats listed by OWASP is Model Denial of Service. Similar to traditional applications, a <a href="https://www.cloudflare.com/learning/ddos/glossary/denial-of-service/">DoS attack</a> is carried out by consuming an exceptionally high amount of resources, resulting in reduced service quality or potentially increasing the costs of running the model. Given the amount of resources LLMs require to run, and the unpredictability of user input, this type of attack can be detrimental. </p><p>This risk can be mitigated by adopting rate limiting policies that control the rate of requests from individual sessions, therefore limiting the context window. By proxying your model through Cloudflare today, you get <a href="https://www.cloudflare.com/ddos/">DDoS protection</a> out of the box. You can also use Rate Limiting and <a href="https://blog.cloudflare.com/advanced-rate-limiting/">Advanced Rate Limiting</a> to manage the rate of requests allowed to reach your model by setting a maximum rate of request performed by an individual IP address or API key during a session.</p><h3 id="identify-sensitive-information-with-sensitive-data-detection">Identify sensitive information with Sensitive Data Detection</h3><p>There are two use cases for sensitive data, depending on whether you own the model and data, or you want to prevent users from sending data into public LLMs. </p><p>As defined by <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">OWASP</a>, <em>Sensitive Information Disclosure</em> happens when LLMs inadvertently reveal confidential data in the responses, leading to unauthorized data access, privacy violations, and security breaches. One way to prevent this is to add strict prompt validations. Another approach is to identify when personally identifiable information (PII) leaves the model. This is relevant, for example, when a model was trained with a company knowledge base that may include sensitive information, such asPII (like social security number), proprietary code, or algorithms.</p><p>Customers using LLM models behind Cloudflare WAF can employ the Sensitive Data Detection (SDD) WAF managed ruleset to identify certain PII being returned by the model in the response. Customers can review the SDD matches on WAF Security Events. Today, SDD is offered as a set of managed rules designed to scan for financial information (such as credit card numbers) as well as secrets (API keys). As part of the roadmap, we plan to allow customers to create their own custom fingerprints. </p><p>The other use case is intended to prevent users from sharing PII or other sensitive information with external LLM providers, such as OpenAI or Anthropic. To protect from this scenario, we plan to expand SDD to scan the request prompt and integrate its output with AI Gateway where, alongside the prompt's history, we detect if certain sensitive data has been included in the request. We will start by using the existing SDD rules, and we plan to allow customers to write their own custom signatures. Relatedly, obfuscation is another feature we hear a lot of customers talk about. Once available, the expanded SDD will allow customers to obfuscate certain sensitive data in a prompt before it reaches the model. SDD on the request phase is being developed.</p><h2 id="preventing-model-abuses">Preventing model abuses</h2><p>Model abuse is a broader category of abuse. It includes approaches like “prompt injection” or submitting requests that generate hallucinations or lead to responses that are inaccurate, offensive, inappropriate, or simply off-topic. </p><p>Prompt Injection is an attempt to manipulate a language model through specially crafted inputs, causing unintended responses by the LLM. The results of an injection can vary, from extracting sensitive information to influencing decision-making by mimicking normal interactions with the model. A classic example of prompt injection is manipulating a CV to affect the output of <a href="https://kai-greshake.de/posts/inject-my-pdf/">resume screening tools</a>.</p><p>A common use case we hear from customers of our AI Gateway is that they want to avoid their application generating toxic, offensive, or problematic language. The risks of not controlling the outcome of the model include reputational damage and harming the end user by providing an unreliable response. </p><p>These types of abuse can be managed by adding an additional layer of protection that sits in front of the model. This layer can be trained to block injection attempts or block prompts that fall into categories that are inappropriate.</p><h3 id="prompt-and-response-validation">Prompt and response validation</h3><p>Firewall for AI will run a series of detections designed to identify prompt injection attempts and other abuses, such as making sure the topic stays within the boundaries defined by the model owner. Like other existing WAF features, Firewall for AI will automatically look for prompts embedded in HTTP requests or allow customers to create rules based on where in the JSON body of the request the prompt can be found.</p><p>Once enabled, the Firewall will analyze every prompt and provide a score based on the likelihood that it’s malicious. It will also tag the prompt based on predefined categories. The score ranges from 1 to 99 which indicates the likelihood of a prompt injection, with 1 being the most likely. </p><p>Customers will be able to create WAF rules to block or handle requests with a particular score in one or both of these dimensions. You’ll be able to combine this score with other existing signals (like bot score or attack score) to determine whether the request should reach the model or should be blocked. For example, it could be combined with a bot score to identify if the request was malicious and generated by an automated source.</p><figure><img src="https://blog.cloudflare.com/content/images/2024/03/Slice-1.png" alt="" loading="lazy" width="1600" height="1018"><figcaption><em>Detecting prompt injections and prompt abuse is part of the scope of Firewall for AI. Early iteration of the product design</em></figcaption></figure><p>Besides the score, we will assign tags to each prompt that can be used when creating rules to prevent prompts belonging to any of these categories from reaching their model. For example, customers will be able to create rules to block specific topics. This includes prompts using words categorized as offensive, or linked to religion, sexual content, or politics, for example.</p><h2 id="how-can-i-use-firewall-for-ai-who-gets-this">How can I use Firewall for AI? Who gets this?</h2><p>Enterprise customers on the Application Security Advanced offering can immediately start using Advanced Rate Limiting and Sensitive Data Detection (on the response phase). Both products can be found in the WAF section of the Cloudflare dashboard. Firewall for AI’s prompt validation feature is currently under development and a beta version will be released in the coming months to all Workers AI users. Sign up to <a href="https://cloudflare.com/lp/firewall-for-ai/">join the waiting list</a> and get notified when the feature becomes available.</p><h2 id="conclusion">Conclusion</h2><p>Cloudflare is one of the first security providers launching a set of tools to secure AI applications. Using Firewall for AI, customers can control what prompts and requests reach their language models, reducing the risk of abuses and data exfiltration. Stay tuned to learn more about how AI application security is evolving.</p></div></section><div><p>We protect <a target="_blank" href="https://www.cloudflare.com/network-services/" rel="noreferrer">entire corporate networks</a>, help customers build <a target="_blank" href="https://workers.cloudflare.com/" rel="noreferrer">Internet-scale applications efficiently</a>, accelerate any <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/" rel="noreferrer">website or Internet application</a>, <a target="_blank" href="https://www.cloudflare.com/ddos/" rel="noreferrer">ward off DDoS attacks</a>, keep <a target="_blank" href="https://www.cloudflare.com/application-security/" rel="noreferrer">hackers at bay</a>, and can help you on <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/" rel="noreferrer">your journey to Zero Trust</a>.</p><p>Visit <a target="_blank" href="https://one.one.one.one/" rel="noreferrer">1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.</p><p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/" rel="noreferrer">start here</a>. If you're looking for a new career direction, check out <a target="_blank" href="https://www.cloudflare.com/careers" rel="noreferrer">our open positions</a>.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why do tree-based models still outperform deep learning on tabular data? (2022) (103 pts)]]></title>
            <link>https://arxiv.org/abs/2207.08815</link>
            <guid>39601710</guid>
            <pubDate>Tue, 05 Mar 2024 10:44:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2207.08815">https://arxiv.org/abs/2207.08815</a>, See on <a href="https://news.ycombinator.com/item?id=39601710">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2207.08815.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data ($\sim$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Leo Grinsztajn [<a href="https://arxiv.org/show-email/6e9e462c/2207.08815">view email</a>]&nbsp;[via CCSD proxy]      <br>    <strong>[v1]</strong>
        Mon, 18 Jul 2022 08:36:08 UTC (20,916 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I spend £8,500 a year to live on a train (264 pts)]]></title>
            <link>https://metro.co.uk/2024/03/03/spend-8-500-a-year-live-a-train-20388001/</link>
            <guid>39601538</guid>
            <pubDate>Tue, 05 Mar 2024 10:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://metro.co.uk/2024/03/03/spend-8-500-a-year-live-a-train-20388001/">https://metro.co.uk/2024/03/03/spend-8-500-a-year-live-a-train-20388001/</a>, See on <a href="https://news.ycombinator.com/item?id=39601538">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<figure><div>

<p><img loading="lazy" decoding="sync" width="644" height="338" data-rsz="shrink" src="https://metro.co.uk/wp-content/uploads/2024/03/SEI_194337327-bc53.jpg?quality=90&amp;strip=all&amp;zoom=1&amp;resize=644%2C338" alt="Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard." fetchpriority="high">
</p></div>
<figcaption>‘I have a lot of freedom and can decide every day where I want to go’ (Picture: Getty Images / Facebook)</figcaption></figure><p><em>Uch</em>. TRAINS. They’re a necessary evil in many of our lives. Horrible big tin cans full of smelly people that never turn up on time and make you late for everything. The less time spent on them the better. At least for most of us in the UK, anyway.</p>
<p>Not so for digital nomad Lasse Stolley. This <a href="https://metro.co.uk/tag/germany/?ico=auto_link_lifestyle_P2_LNK1" data-track="inline-tag-auto-link_article">German</a> teenager can’t get enough of them. He’s not a trainspotter, though. He’s more of a train<em>squatter</em>.</p>
<p>Okay, ‘squatter’ isn’t really accurate. While the 17-year-old does indeed live on trains, he does so entirely legally. And with a surprising amount of comfort.</p>
<p>Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard.</p>
<p>The self-employed coder technically has no fixed abode and appears to really enjoy his unusual way of life, something which he chronicles regularly on his blog, <a href="https://leben-im-zug.de/">Life on the Train</a>.</p>
<figure><div>

<p><img loading="lazy" decoding="async" width="540" height="360" data-rsz="shrink" src="https://metro.co.uk/wp-content/themes/metro-parent/img/fallback.png" data-src="https://metro.co.uk/wp-content/uploads/2024/03/GettyImages-1948505416-3b2c.jpg?quality=90&amp;strip=all&amp;zoom=1&amp;resize=540%2C360" alt="Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard.">
</p></div>
<figcaption>Lasse travels a whopping 600 miles each and every day (Picture: Getty Images)</figcaption></figure><h2>Embarking on an unusual journey</h2>
<p>‘I’ve been living on the train as a digital nomad for a year and a half now,’ Lasse told <a href="https://www.businessinsider.de/leben/bahncard100-17-jaehriger-lebt-seit-2022-in-zuegen-der-bahn/">Business Insider</a> recently. ‘At night I sleep on the moving Intercity Express (ICE) train and during the day I sit in a seat, at a table and work as a programmer, surrounded by many other commuters and passengers. I travel from one end of the country to the other. I’m exploring the whole of Germany.’</p>
<p>‘I decided to live on a train when I was 16 years old. My school days were behind me and the whole world was open to me. So in the summer of 2022, I decided to give in to my wanderlust, leave my parents’ house in Schleswig-Holstein behind and embark on a huge adventure.’</p>
<p>‘If I feel like travelling to the sea, I take the train north in the morning. If I long for the hustle and bustle of the big city, then I look for a connection to Berlin or Munich. Or I take the express train to the Alps for a hiking trip.’</p>
<p>‘I use the app to organise the next connection in the evening and sleep while I race along the tracks towards my destination. I don’t have a place to retreat to. My home <em>is</em> the train.’</p>
<p>‘The early months were tough and I had to learn a lot about how it all worked. Everything was different than how I’d imagined.’</p>
<figure><div>

<p><img loading="lazy" decoding="async" width="540" height="675" data-rsz="shrink" src="https://metro.co.uk/wp-content/themes/metro-parent/img/fallback.png" data-src="https://metro.co.uk/wp-content/uploads/2024/03/362276901_142677792192526_8628661540473484597_n-32dd.jpg?quality=90&amp;strip=all&amp;zoom=1&amp;resize=540%2C675" alt="Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard.">
</p></div>
<figcaption>The 17 year-old calls himself a ‘digital nomad’ and really takes it quite literally (Picture: Facebook)</figcaption></figure><h2>Costs, overnights and The Parent Question</h2>
<p>Lasse says that, all things considered, it costs him around €10,000 (£8,500) a year to live the way he does.</p>
<p>‘I have a lot of freedom and can decide every day where I want to go, whether it’s to the Alps, to a big city or to the sea. I’m completely flexible.’</p>
<p>He’s forced to keep on the ball, though. You know how it is with trains. Even the unsurprisingly much more efficient German rail system. ‘Every night I have to make sure that I catch the night train and sometimes I have to reschedule very quickly because it suddenly doesn’t arrive.’</p>
<p>What do Lasse’s mum and dad think of his decision? ‘I had to do a lot of convincing,’ he says. Once he’d done that convincing, his parents checked out the legal side of it and agreed. They helped him sell off the majority of his possessions and now fully back their son’s decision.</p>
<figure><div>

<p><img loading="lazy" decoding="async" width="540" height="360" data-rsz="shrink" src="https://metro.co.uk/wp-content/themes/metro-parent/img/fallback.png" data-src="https://metro.co.uk/wp-content/uploads/2024/03/GettyImages-1833760864-c460.jpg?quality=90&amp;strip=all&amp;zoom=1&amp;resize=540%2C360" alt="Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard.">
</p></div>
<figcaption>The unlimited pass means that Lasse has now seen every inch of his homeland (Picture: Getty Images)</figcaption></figure><h2>Keeping luggage to a minimum</h2>
<p>Luggage is, obviously, something of an issue. Lasse has to travel light.</p>
<p>‘The most important thing is my laptop and my noise-cancelling headphones, which at least give me a little privacy on the train.’</p>
<p>‘An important aspect of minimalism on the train is the reduction of material possessions,’ Lasse says. ‘Since the available space is very limited, you have to choose carefully what you really need. It means getting rid of unnecessary items and limiting yourself to the bare essentials.’</p>
<p>‘The challenge of not accumulating more and more things is a central component of minimalist living. Especially with a backpack, you quickly reach a space limit.’</p>
<figure><div>

<p><img loading="lazy" decoding="async" width="540" height="525" data-rsz="shrink" src="https://metro.co.uk/wp-content/themes/metro-parent/img/fallback.png" data-src="https://metro.co.uk/wp-content/uploads/2024/03/354036290_111526388641000_2911877444056073821_n-f1ad-e1709471979145.jpg?quality=90&amp;strip=all&amp;zoom=1&amp;resize=540%2C525" alt="Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard.">
</p></div>
<figcaption>Lasse’s parents took some convincing (Picture: Lasse Stolley’s Facebook)</figcaption></figure><h2>Reflecting on an hectic 18 months</h2>
<p>‘This life means a pretty restless existence. To switch off, I just look out the window and watching the scenery. That calms me down a lot. Then I just let my thoughts wander.’</p>
<p>‘My favourite route leads through the Middle Rhine Valley between Mainz and Bonn. Here the trains always travel very slowly along the river. It’s a beautifully picturesque route that stretches at the foot of the vineyards. The view outside is wonderful.’</p>
<p>‘I’ve travelled a total of over 500,000 kilometres (310,000 miles) since I started living on the train. I don’t know how much longer I want to travel through Germany and wake up somewhere different every day, though.’</p>
<p>‘My Bahncard 100 is still valid for six months. I haven’t seen enough yet.’</p>
<p>
	<span data-track-module="mor-link_article">
	MORE : <a href="https://metro.co.uk/2024/03/02/benidorm-travel-warning-uk-tourists-face-1-000-fines-new-ban-20384776/?ico=more_text_links">Benidorm travel warning as UK tourists face £1,000 fines over new beach rules</a>
	</span>
	</p>
<p>
	<span data-track-module="mor-link_article">
	MORE : <a href="https://metro.co.uk/2024/03/01/mum-of-two-dies-suddenly-falling-sick-flight-20382396/?ico=more_text_links">Mum-of-two dies after suddenly falling sick on flight</a>
	</span>
	</p>
<p>
	<span data-track-module="mor-link_article">
	MORE : <a href="https://metro.co.uk/2024/03/01/tube-journeys-will-cheaper-certain-times-next-week-20377984/?ico=more_text_links">Tube journeys to be cheaper at certain times from next week</a>
	</span>
	</p>						
					
					<!-- Article below content widget area -->
					

					<div data-track-module="email-signup-shortcode_travel"><p><h3>Get need-to-know travel news, inspiration and advice from Metro every week.</h3></p><h4>Sign up here...</h4></div><p><span>This site is protected by reCAPTCHA and the Google <a href="https://policies.google.com/privacy">Privacy Policy</a> and <a href="https://policies.google.com/terms">Terms of Service</a> apply.</span>
				</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Radicle: Open-Source, Peer-to-Peer, GitHub Alternative (493 pts)]]></title>
            <link>https://app.radicle.xyz/nodes/seed.radicle.garden/rad:z3gqcJUoA1n9HaHKufZs5FCSGazv5</link>
            <guid>39600810</guid>
            <pubDate>Tue, 05 Mar 2024 08:39:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://app.radicle.xyz/nodes/seed.radicle.garden/rad:z3gqcJUoA1n9HaHKufZs5FCSGazv5">https://app.radicle.xyz/nodes/seed.radicle.garden/rad:z3gqcJUoA1n9HaHKufZs5FCSGazv5</a>, See on <a href="https://news.ycombinator.com/item?id=39600810">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Linux Crosses 4% Market Share Worldwide (164 pts)]]></title>
            <link>https://linuxiac.com/linux-crosses-four-percent-market-share-worldwide/</link>
            <guid>39600172</guid>
            <pubDate>Tue, 05 Mar 2024 06:49:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linuxiac.com/linux-crosses-four-percent-market-share-worldwide/">https://linuxiac.com/linux-crosses-four-percent-market-share-worldwide/</a>, See on <a href="https://news.ycombinator.com/item?id=39600172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>Linux has surpassed a 4% share in the desktop operating system market as of the end of February 2024. According to the <a href="https://gs.statcounter.com/os-market-share/desktop/worldwide" target="_blank" rel="noreferrer noopener">latest data from StatCounter</a>, a leading web traffic analysis tool, Linux’s market share has reached 4.03%.</p>



<p>At first glance, the number might seem modest, but it represents a significant leap. Let’s break it down. <a href="https://linuxiac.com/linux-hits-3-percent-market-share/">It took Linux 30 years to secure a 3% share of desktop operating systems</a>, a milestone reached last June.</p>



<p>Impressively, the open-source operating system has surged by an additional 1% in the last eight months.</p>



<figure><a href="https://linuxiac.b-cdn.net/wp-content/uploads/2024/03/linux-marketshare-february-2024.jpg"><img decoding="async" width="1024" height="802" src="https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/linuxiac.com/wp-content/uploads/2024/03/linux-marketshare-february-2024-1024x802.jpg" data-spai-egr="1" alt="Linux desktop market share, February 2024" srcset="https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/linuxiac.com/wp-content/uploads/2024/03/linux-marketshare-february-2024-1024x802.jpg 1024w, https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/linuxiac.com/wp-content/uploads/2024/03/linux-marketshare-february-2024-380x297.jpg 380w, https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/linuxiac.com/wp-content/uploads/2024/03/linux-marketshare-february-2024-768x601.jpg 768w, https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/linuxiac.com/wp-content/uploads/2024/03/linux-marketshare-february-2024.jpg 1183w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Linux desktop market share, February 2024</figcaption></figure>



<p>Now, we’re all curious about the journey Linux is on and where it’ll end up by the year’s end. Will we be celebrating a milestone of surpassing 5% market share? It’s a goal many of us who champion open source are eagerly hoping to achieve.</p>



<p>The rise in Linux’s popularity can be attributed to several factors. Firstly, the open-source nature of Linux has made it a favored choice among developers, IT professionals, and tech enthusiasts who appreciate the flexibility and control it offers.</p>



<p>Additionally, the security and stability of Linux have been key selling points, making it an attractive option for both personal and professional use.</p>



<p>However, while having great features is important, an attractive presentation often captures attention first, something both Windows and macOS understand well. This is precisely where the top Linux desktop distros have made remarkable strides, significantly enhancing their appearance and user-friendliness in recent years.</p>



<p>With the continuous improvement and user-friendly designs of distributions such as <a href="https://linuxiac.com/ubuntu/">Ubuntu</a>, Fedora, <a href="https://linuxiac.com/linux-mint/">Mint</a>, and many others, Linux has become more accessible to a broader audience, including those who may not be as technically inclined.</p>



<p>Is the much-anticipated “Linux on the Desktop” year upon us? Well, not exactly. The truth is, seeing Linux dominate desktops any time soon is quite unlikely, but then again, achieving widespread desktop dominance was never the primary aim of Linux. It’s more of an ongoing, lighthearted debate among enthusiasts than a serious expectation.</p>



<p>However, it’s worth noting and celebrating that Linux’s desktop usage has surpassed 4% and even saw a growth of 1% in just the last eight months – a feat that was beyond the expectations of many. So, let’s take a moment to appreciate this achievement. It may seem small to some, but it’s a significant stride forward for those who hold Linux dear.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stable Diffusion 3: Research Paper (309 pts)]]></title>
            <link>https://stability.ai/news/stable-diffusion-3-research-paper</link>
            <guid>39599958</guid>
            <pubDate>Tue, 05 Mar 2024 06:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stability.ai/news/stable-diffusion-3-research-paper">https://stability.ai/news/stable-diffusion-3-research-paper</a>, See on <a href="https://news.ycombinator.com/item?id=39599958">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">
        
          
<article id="sections" data-page-sections="62f2452bc121595f4d87c71c">
  
  
    
    


  


<div data-content-field="main-content" data-item-id="" data-test="page-section" data-section-theme="bright-inverse" data-section-id="62f2452bc121595f4d87c71e" data-controller="SectionWrapperController" data-current-styles="{
&quot;imageOverlayOpacity&quot;: 0.15,
&quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
&quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
&quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
&quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
&quot;contentWidth&quot;: &quot;content-width--wide&quot;,
&quot;sectionTheme&quot;: &quot;bright-inverse&quot;,
&quot;sectionAnimation&quot;: &quot;none&quot;,
&quot;backgroundMode&quot;: &quot;image&quot;
}" data-current-context="{
&quot;video&quot;: {
&quot;playbackSpeed&quot;: 0.5,
&quot;filter&quot;: 1,
&quot;filterStrength&quot;: 0,
&quot;zoom&quot;: 0,
&quot;videoSourceProvider&quot;: &quot;none&quot;
},
&quot;backgroundImageId&quot;: null,
&quot;backgroundMediaEffect&quot;: null,
&quot;divider&quot;: null,
&quot;typeName&quot;: &quot;blog-side-by-side&quot;
}" data-animation="none">
  <article id="article-">
  
    
    
    
    <div data-layout-label="Post Body" data-type="item" id="item-65e63d2437be6e755f2ef692"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-4eb8968a0fbd2de4a226">
  <h2>Key Takeaways:</h2><ul data-rte-list="default"><li><p>Today, we’re publishing our <span data-text-attribute-id="aea29532-822b-4f7e-a716-d7f164d6abb5"><a href="https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf">research paper</a></span> that dives into the underlying technology powering Stable Diffusion 3.</p></li><li><p>Stable Diffusion 3 outperforms state-of-the-art text-to-image generation systems such as DALL·E 3, Midjourney v6, and Ideogram v1 in typography and prompt adherence, based on human preference evaluations.&nbsp;</p></li><li><p>Our new Multimodal Diffusion Transformer (MMDiT) architecture uses separate sets of weights for image and language representations, which improves text understanding and spelling capabilities&nbsp;compared to previous versions of SD3.</p></li></ul>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_3258">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png" data-image-dimensions="1920x1235" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png" width="1920" height="1235" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_2549">
  <p>Following our announcement of the <a href="https://stability.ai/news/stable-diffusion-3" target=""><span>early preview of Stable Diffusion 3</span></a>, today we are publishing the <a href="https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf" target=""><span>research paper</span></a> which outlines the technical details of our upcoming model release. The paper will be accessible on arXiv soon, and we invite you to sign up for <a href="https://stability.ai/stablediffusion3"><span>the waitlist</span> </a>to participate in the early preview.&nbsp;</p><h2>Performance</h2>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_15593">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg" data-image-dimensions="640x480" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg" width="640" height="480" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>With SD3 as a baseline, this chart outlines the areas it wins against competing models based on human evaluations of Visual Aesthetics, Prompt Following, and Typography. </em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_15946">
  <p>We have compared output images from Stable Diffusion 3 with various other open models including <a href="https://stability.ai/news/stable-diffusion-sdxl-1-announcement"><span>SDXL</span></a>, <a href="https://stability.ai/news/stability-ai-sdxl-turbo"><span>SDXL Turbo</span></a>, <a href="https://stability.ai/news/introducing-stable-cascade"><span>Stable Cascade</span></a>, Playground v2.5 and Pixart-α as well as closed-source systems such as DALL·E 3, Midjourney v6 and Ideogram v1 to evaluate performance based on human feedback. During these tests, human evaluators were provided with example outputs from each model and asked to select the best results based on how closely the model outputs follow the context of the prompt it was given (“prompt following”), how well text was rendered based on the prompt (“typography”) and, which image is of higher aesthetic quality (“visual aesthetics”).&nbsp;</p><p>From the results of our testing, we have found that Stable Diffusion 3 is equal to or outperforms current state-of-the-art text-to-image generation systems in all of the above areas.&nbsp;</p><p>In early, unoptimized inference tests on consumer hardware our largest SD3 model with 8B parameters fits into the 24GB VRAM of a RTX 4090 and takes 34 seconds to generate an image of resolution 1024x1024 when using 50 sampling steps. Additionally, there will be multiple variations of Stable Diffusion 3 during the initial release, ranging from 800m to 8B parameter models to further eliminate hardware barriers.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_4412">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png" data-image-dimensions="1920x1310" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png" width="1920" height="1310" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_5284">
  <h2>Architecture Details&nbsp;</h2><p>For text-to-image generation, our model has to take both modalities, text and images, into account. This is why we call this new architecture MMDiT, a reference to its ability to process multiple modalities. As in previous versions of Stable Diffusion, we use pretrained models to derive suitable text and image representations. Specifically, we use three different text embedders - two CLIP models and T5 - to encode text representations, and an improved autoencoding model to encode image tokens.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_5773">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png" data-image-dimensions="596x520" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png" width="596" height="520" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Conceptual visualization of a block of our modified multimodal diffusion transformer: MMDiT.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_6570">

<p>The SD3 architecture builds upon the <a href="https://arxiv.org/abs/2212.09748"><span>Diffusion Transformer (“DiT”, Peebles &amp; Xie, 2023)</span></a>. Since text and image embeddings are conceptually quite different, we use two separate sets of weights for the two modalities. As shown in the above figure, this is equivalent to having two independent transformers for each modality, but joining the sequences of the two modalities for the attention operation, such that both representations can work in their own space yet take the other one into account.&nbsp;</p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_7981">

<p>By using this approach, information is allowed to flow between image and text tokens to improve overall comprehension and typography within the outputs generated. This architecture is also easily extendable to multiple modalities such as video, as we discuss in our <a href="https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf" target=""><span>paper</span>.</a></p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_8435">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png" data-image-dimensions="1920x659" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png" width="1920" height="659" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_9295">

<p>Thanks to Stable Diffusion 3’s improved prompt following, our model has the ability to create images that focus on various different subjects and qualities while also remaining highly flexible with the style of the image itself.</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_9722">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png" data-image-dimensions="1920x1592" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png" width="1920" height="1592" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_10076">
  <h2>Improving Rectified Flows by Reweighting</h2><p>Stable Diffusion 3 employs a Rectified Flow (RF) formulation (<a href="https://arxiv.org/abs/2209.03003"><span>Liu et al., 2022</span></a>; <a href="https://arxiv.org/abs/2209.15571"><span>Albergo &amp; Vanden-Eijnden,2022</span></a>; <a href="https://arxiv.org/abs/2210.02747"><span>Lipman et al., 2023</span></a>), where data and noise are connected on a linear trajectory during training. This results in straighter inference paths, which then allow sampling with fewer steps. Furthermore, we introduce a novel trajectory sampling schedule into the training process. This schedule gives more weight to the middle parts of the trajectory, as we hypothesize that these parts result in more challenging prediction tasks. We test our approach against 60 other diffusion trajectories such as <a href="https://arxiv.org/abs/2112.10752"><span>LDM</span></a>, <a href="https://arxiv.org/abs/2206.00364"><span>EDM</span></a> and <a href="https://arxiv.org/abs/2105.05233"><span>ADM</span></a>, using multiple datasets, metrics, and sampler settings for comparison. The results indicate that while previous RF formulations show improved performance in few step sampling regimes, their relative performance declines with more steps. In contrast, our re-weighted RF variant consistently improves performance.</p><h2>Scaling Rectified Flow Transformer Models</h2>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_11203">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png" data-image-dimensions="1600x1139" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png" width="1600" height="1139" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_12066">
  <p>We conduct a scaling study for text-to-image synthesis with our reweighted Rectified Flow formulation and MMDiT backbone. We train models ranging from 15 blocks with 450M parameters to 38 blocks with 8B parameters and observe a smooth decrease in the validation loss as a function of both model size and training steps (top row). To test whether this translates into meaningful improvements of the model outputs, we also evaluate automatic image-alignment metrics (<a href="https://arxiv.org/abs/2310.11513"><span>GenEval</span></a>) as well as human preference scores (ELO) (bottom row). Our results demonstrate a strong correlation between these metrics and the validation loss, indicating that the latter is a strong predictor of overall model performance. Furthermore, the scaling trend shows no signs of saturation, which makes us optimistic that we can continue to improve the performance of our models in the future.</p><h2>Flexible Text Encoders</h2><p>By removing the memory-intensive 4.7B parameter T5 text encoder for inference, SD3’s memory requirements can be significantly decreased with only small performance loss. Removing this text encoder does not affect visual aesthetics (win rate w/o T5: 50%) and results only in slightly reduced text adherence (win rate 46%) as seen in the above image under the “Performance” section. However, we recommend including T5 for using SD3’s full power in generating written text, since we observe larger performance drops in typography generation without it (win rate 38%) as seen in the examples below:</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_13449">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png" data-image-dimensions="1596x1268" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png" width="1596" height="1268" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Removing T5 for inference only results in significant performance drops when rendering very complex prompts involving many details or large amounts of written text. The above figure shows three random samples per example.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_13802">
  <p>To learn more about MMDiT, Rectified Flows, and the research behind Stable Diffusion 3, read our full research paper <a href="https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf" target=""><span>here</span></a>.</p><p>To stay updated on our progress follow us on<a href="https://twitter.com/stabilityai"> <span>Twitter</span></a>,<a href="https://www.instagram.com/stability.ai/"> <span>Instagram</span></a>,<a href="https://www.linkedin.com/company/stability-ai"> <span>LinkedIn</span></a>, and join our<a href="https://discord.gg/stablediffusion"> <span>Discord Community</span></a>.</p>
</div></div>
  
</article>

</div>

  
</article>


          

          
            
              

            
          
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sell for half a billion and get nothing (2021) (320 pts)]]></title>
            <link>https://www.fundablestartups.com/blog/half-a-billion</link>
            <guid>39598903</guid>
            <pubDate>Tue, 05 Mar 2024 02:47:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fundablestartups.com/blog/half-a-billion">https://www.fundablestartups.com/blog/half-a-billion</a>, See on <a href="https://news.ycombinator.com/item?id=39598903">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              <p>In July 2018, Paddy Power Betfair (now known as Flutter) acquired FanDuel for $465M in cash. On the surface, this looks like a great win for the FanDuel founders and employees. However, because the two lead investors held strong liquidation preference rights, <strong>the FanDuel founders and most employees received nothing</strong> in this massive deal.</p>

<h3>What is Liquidation Preference?</h3>
<p>Liquidation preference is one of the most important terms in a term sheet. Liquidation preference determines who gets paid first and how much they get paid when there’s an acquisition.</p>
<p>Because they take on significant risks, investors expect to get “VIP” head-of-line privileges to be paid upon a liquidation event such as an acquisition. Employees wait in line and collect proceeds only after all of the preferred investors take their share.</p>
<p><img src="https://kajabi-storefronts-production.global.ssl.fastly.net/kajabi-storefronts-production/blogs/25302/images/3gJxOCS6J821JWbd67gE_liquidation_preference_investor_priority_tiny.png"></p>

<h3>What Are Liquidation Preference Terms?</h3>
<p>There are two components to liquidation preference. The first is the preference multiple, which basically says the investor gets a certain multiple of their investment amount. If the investor invested $5M and got a 2x preference, they would get paid $10M before the common shareholders got paid anything. For founders, obviously a 1x multiple is better than a 3x multiple.</p>
<p>The second component of liquidation preference is participation, which determines whether the investor can take additional proceeds after the preference multiple is paid out. Capped or full participation rights would allow the investor to “double dip” in their payout.</p>
<p>The simplified 3x3 matrix below provides a simplified view of how liquidation preference works. Healthy startups can get terms more in the lower left corner. Marginal startups may find funding, but liquidation preference terms will likely trend towards the upper right corner.</p>
<p><img src="https://kajabi-storefronts-production.global.ssl.fastly.net/kajabi-storefronts-production/blogs/25302/images/1FyTFXR2Sj6aBk9f8Q5v_liquidation_preference_3x3_matrix_tiny.png"></p>
<h3><br>How Liquidation Preference Hurt FanDuel Founders</h3>
<p>When the FanDuel founders raised funds, two key investors received a liquidation preference that entitled them to the first $559M in an acquisition. Founders and employees would be paid only if the acquisition exceeded $559M. Because the Paddy Power Betfair was for just $465M, the founders received nothing.</p>
<p>To help founders visualize how liquidation preference could affect their startup, we have a liquidation preference scenario calculator available in our <a title="Free Training and Tools for Startup Founders" href="https://www.fundablestartups.com/resources">free members area</a>.</p>
<p><img src="https://kajabi-storefronts-production.global.ssl.fastly.net/kajabi-storefronts-production/blogs/25302/images/ZFIz7jxRnKpQ0Eijz1tG_liquidation_preference_model_tiny.jpg">&nbsp;</p>
<h3>Why Founders Couldn’t Stop the Deal</h3>
<p>But wait. If this was such a horrible deal for the founders, why did they do the deal? The reality was the founders couldn’t stop the deal because they also granted the same two lead investors drag along rights. This drag along right forced the other shareholders to accept the decisions made by these two investors. Imagine how the founders felt when they received the notice below that the drag along right was being exercised and <strong>they could do nothing to stop getting short-changed</strong>.</p>
<blockquote>
<p><img src="https://kajabi-storefronts-production.global.ssl.fastly.net/kajabi-storefronts-production/blogs/25302/images/M6v7cIqTbGD5rkronJ6C_FanDuel_drag_along_notice_tiny.png"></p>
</blockquote>

<h3>Lessons Learned: Build a Very Fundable Startup</h3>
<p>Every founder should learn from this disastrous scenario the importance of building a very healthy, fundable startup. A healthy, vibrant startup draws more investors during fundraising. The competition gives founders the leverage to negotiate for more founder-friendly terms. Healthy startups get better valuations, better terms, and raise funds with much less effort.</p>
<p>Building a healthy startup requires great execution. Great execution involves doing dozens of tasks and processes the right way. Learn how to execute well with our <a title="Premium Training and Tools for Startup Founders" href="https://www.fundablestartups.com/training">premium startup training</a>. If you are new to our startup training, we recommend starting by watching the 7 Keys to Triple Your Startup Payout, located in our <a href="https://www.fundablestartups.com/resource_redirect/offers/yFLUCuzb">Basic Tier</a>.</p>
<p>For more detail on liquidation preferences, refer to chapter 11 in our <a title="21 Secrets of Successful Startups" href="https://www.fundablestartups.com/resources#section-1703194520014">book</a>. Good luck building your own healthy, fundable startup!</p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Screen time robs average toddler of hearing 1,000 words spoken by adult a day (128 pts)]]></title>
            <link>https://www.theguardian.com/australia-news/2024/mar/04/does-children-toddlers-kids-watching-tv-impact-development-learning</link>
            <guid>39598776</guid>
            <pubDate>Tue, 05 Mar 2024 02:30:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/australia-news/2024/mar/04/does-children-toddlers-kids-watching-tv-impact-development-learning">https://www.theguardian.com/australia-news/2024/mar/04/does-children-toddlers-kids-watching-tv-impact-development-learning</a>, See on <a href="https://news.ycombinator.com/item?id=39598776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The average toddler is missing out on hearing more than 1,000 words spoken by an adult each day due to screen time, setting back their language skills, a first-of-its kind study has found.</p><p>The research, <a href="https://jamanetwork.com/journals/jamapediatrics/fullarticle/10.1001/jamapediatrics.2023.6790?guestAccessKey=af1b82f5-2ff4-4cc9-a88c-2720ef541470&amp;utm_source=For_The_Media&amp;utm_medium=referral&amp;utm_campaign=ftm_links&amp;utm_content=tfl&amp;utm_term=030424" data-link-name="in body link">published on Tuesday</a> in the Journal of the American Medical Association (Jama) Pediatrics, tracked 220 Australian families over two years to measure the relationship between family screen use and children’s language environment.</p><p>Families recorded all the audio around their child using advanced speech recognition technology over a 16-hour period on an average day at home. They repeated this process every six months between the ages of 12 and 36 months.</p><figure id="6e513fda-4428-4ad3-be44-6e21418fd35e" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:3,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;My kid had too much screen time – so I tested out these alternatives&quot;,&quot;elementId&quot;:&quot;6e513fda-4428-4ad3-be44-6e21418fd35e&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/lifeandstyle/2024/jan/19/smartphone-alternatives-kids-toys&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>The lead researcher, Dr Mary Brushe from the Telethon Kids Institute, said: “The technology we use is essentially like a Fitbit, but instead of counting the number of steps, this device counts the number of words spoken by, to and around the child.”</p><p>The device also picked up electronic noise, which the researchers analysed to calculate screen time.</p><p>The researchers found young children’s exposure to screens including TVs and phones was interfering with their language opportunities, with the association most pronounced at three years of age.</p><p>For every extra minute of screen time, the three-year-olds in the study were hearing seven fewer words, speaking five fewer words themselves and engaging in one less conversation.</p><ul>
 <li><p><strong><a href="https://www.theguardian.com/email-newsletters?CMP=copyembed" data-link-name="in body link">Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup</a></strong></p></li>
</ul><p>The study found the average three-year-old in the study was exposed to two hours and 52 minutes of screen time a day. Researchers estimated this led to those children being exposed to 1,139 fewer adult words, 843 fewer child words and 194 fewer conversations.</p><p>Because the study couldn’t capture parents’ silent phone use, including reading emails, texting or quietly scrolling through websites or social media, Brushe said they might have underestimated how much screen usage is affecting children.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-11">skip past newsletter promotion</a><p id="EmailSignup-skip-link-11" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><figure id="9c4e1ade-b6d4-4517-9383-bf5abbca3c3f" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:12,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Kids are on their phones more than ever. We asked parents what they’re doing about it&quot;,&quot;elementId&quot;:&quot;9c4e1ade-b6d4-4517-9383-bf5abbca3c3f&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/lifeandstyle/2024/feb/01/screen-time-phones-kids-limit&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>A language-rich home environment was critical in supporting infants and toddlers’ language development, Brushe said. While some educational children’s shows were designed to help children’s language skills, very young kids in the age group of the study could struggle to translate television shows into their own life, she said.</p><p>This study did not differentiate between whether children were watching high- or low-quality screen content.</p><p>Previous research in the area had relied on parents self-reporting their own and their child’s screen time, and only studied short periods of time.</p><p>“To our knowledge, no studies conducted since the rapid uptake of mobile phones and tablets have actually tracked children’s screen time and their early language experiences over an extended period of time,” Brushe said.</p><p>Prof Angela Morgan, the leader of the speech and language group at the Murdoch Children’s Research Institute, which was not involved in the study, said: “To my knowledge it’s the most robust examination of looking at screen time and interactions between parents and children that we’ve had available.</p><p>“For all children, the biggest opportunities for language learning are of course in those first few years of life … we know that early predictors do predict your later language outcomes, so it is really important that they’ve been looking at this question in the early years.”</p><figure id="826fe14b-8c71-4076-a366-80553c782dad" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:19,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;As parents, can we all agree that a bit of screen time for children is actually a good thing? | Rhiannon Lucy Cosslett&quot;,&quot;elementId&quot;:&quot;826fe14b-8c71-4076-a366-80553c782dad&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/commentisfree/2023/mar/15/parents-screen-time-children-tv-cbeebies&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>Amber Flohm, the vice-president of the NSW Teachers Federation, said members who taught in early education and primary school had said how children were affected significantly by the increased amount of time spent on screen.</p><p>Flohm said teachers had noted language skills going backwards, both in conversation between children themselves and teachers and in reading and writing skills. The pandemic exacerbated the situation, but teachers had noted the trends around the increased used of screen time “at least the last five or six years pre-Covid”, she said.</p><p>The research in the study was carried out between 2018 and 2021, with some families undertaking their 30- or 36-month recording day early in the pandemic. However, researchers said participants’ average screen times did not appear to have increased substantially compared with those who completed their recordings prior to the pandemic.</p><p>Due to the advanced speech recognition technology only being able to code for English, only English-speaking households were part of the study.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[European crash tester says carmakers must bring back physical controls (913 pts)]]></title>
            <link>https://arstechnica.com/cars/2024/03/carmakers-must-bring-back-buttons-to-get-good-safety-scores-in-europe/</link>
            <guid>39598189</guid>
            <pubDate>Tue, 05 Mar 2024 01:02:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/cars/2024/03/carmakers-must-bring-back-buttons-to-get-good-safety-scores-in-europe/">https://arstechnica.com/cars/2024/03/carmakers-must-bring-back-buttons-to-get-good-safety-scores-in-europe/</a>, See on <a href="https://news.ycombinator.com/item?id=39598189">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      do that here, too    —
</h4>
            
            <h2 itemprop="description">In 2026, Euro NCAP points will be deducted if some controls aren't physical.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/03/GettyImages-1491147785-800x533.jpg" alt="man pushing red triangle warning car button">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/03/GettyImages-1491147785-scaled.jpg" data-height="1707" data-width="2560">Enlarge</a> <span>/</span> A car's hazard warning lights will need a physical control to get a five-star EuroNCAP score in 2026.</p></figcaption>  </figure>

  




<!-- cache hit 104:single/related:09847d295e1e25abd2a94d9739516e83 --><!-- empty -->
<p>Some progress in the automotive industry is laudable. Cars are safer than ever and more efficient, too. But there are other changes we'd happily leave by the side of the road. That glossy "piano black" trim that's been overused the last few years, for starters. And the industry's overreliance on touchscreens for functions that used to be discrete controls. Well, the automotive safety organization European New Car Assessment Programme (Euro NCAP) feels the same way about that last one, and it says the controls ought to change in 2026.</p>
<p>"The overuse of touchscreens is an industry-wide problem, with almost every vehicle-maker moving key controls onto central touchscreens, obliging drivers to take their eyes off the road and raising the risk of distraction crashes," said Matthew Avery, Euro NCAP's director of strategic development.</p>                                            
                                                        
<p>"New Euro NCAP tests due in 2026 will encourage manufacturers to use separate, physical controls for basic functions in an intuitive manner, limiting eyes-off-road time and therefore promoting safer driving," he said.</p>
<p>Now, Euro NCAP is not insisting on everything being its own button or switch. But the organization wants to see physical controls for turn signals, hazard lights, windshield wipers, the horn, and any SOS features like the European Union's <a href="https://europa.eu/youreurope/citizens/travel/security-and-emergencies/emergency-assistance-vehicles-ecall/index_en.htm">eCall feature</a>.</p>
<p>Tesla is probably at greatest risk here, having recently ditched physical stalks that instead move the turn signal functions to haptic buttons on the steering wheel. (Ferrari also has its <a href="https://arstechnica.com/cars/2022/12/the-2023-ferrari-296-gts-we-drive-ferraris-plug-in-hybrid-convertible/">turn signals on the steering wheel</a>, but Ferrari does not appear in Euro NCAP's database so probably doesn't care.)</p>
<p>Euro NCAP is not a government regulator, so it has no power to mandate carmakers use physical controls for those functions. But a five-star safety score from Euro NCAP is a strong selling point, similar to the Insurance Institute for Highway Safety's coveted Top Safety Pick program here in the US, and it's likely this pressure will be effective. Perhaps someone should start bugging IIHS to do the same.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Based: Simple linear attention language models (146 pts)]]></title>
            <link>https://www.together.ai/blog/based</link>
            <guid>39597847</guid>
            <pubDate>Tue, 05 Mar 2024 00:15:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.together.ai/blog/based">https://www.together.ai/blog/based</a>, See on <a href="https://news.ycombinator.com/item?id=39597847">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Introducing Based, a simple efficient architecture that combines two familiar primitives – sliding window attention and linear attention – to offer high-quality language modeling with strong associative recall capabilities! At inference time, Based decodes without a KV-cache, enabling a 24x throughput improvement over Transformers with Flash-Attention 2!</p><h2><strong>Overview</strong></h2><p>In an <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis">ICLR paper</a> (and <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis">blogpost</a>) we posted towards the end of last year, we share the finding that many efficient architectures (e.g. <a href="https://arxiv.org/abs/2312.00752">Mamba</a>, <a href="https://github.com/BlinkDL/RWKV-LM">RWKV</a>, <a href="https://arxiv.org/abs/2302.10866">Hyena</a>, <a href="https://arxiv.org/abs/2307.08621">RetNet</a>) underperform Transformers on recall, the ability to ground generations on information seen in-context, which is critical for in-context learning and copying. We used this analysis to design a new Based architecture (previewed in this <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology2-based">blogpost</a>). We’re excited to share the latest progress in this line of work.&nbsp;</p><p>Our recent work digs deeper into the recall challenge. We begin by illustrating a fundamental tradeoff between a model’s recall abilities and the size of its recurrent state during generation. This analysis informs the design of Based, a simple recurrent architecture that outperforms prior sub-quadratic models on real-world recall-intensive tasks (information extraction, reading comprehension) and in-context learning (few-shot natural language understanding on SuperGLUE). At the same time, Based offers fast&nbsp; generation speeds: Based is 56% and 44% faster at processing prompts than FlashAttention-2 and Mamba respectively (4k sequence length, 1.3Bn parameters). Based also offers 24x higher throughput than FlashAttention-2 in next token prediction (generating 1024 tokens, 128 batch size, 1.3Bn parameters).&nbsp;</p><p>We’re particularly excited about the <em>simplicity</em> of Based. Using just two well-known, familiar, attention-like building blocks, sliding window attention (with <em>tiny</em> window sizes) and linear attention (with Taylor series approximation of exp(QK^T)), we can outperform the strongest sub-quadratic architectures on language modeling and achieve massive speedups over optimized Transformers!&nbsp;</p><p>This blogpost provides an overview of our 1) analysis on recall in sub-quadratic architectures that leads to the Based architecture’s design and 2) how we make Based go brrrr!&nbsp;</p><h2><strong>Motivating analysis: the recall-memory tradeoff</strong></h2><p><strong>‍</strong>The main question driving our exploration is: <em>can we drastically improve the real-world speed and memory consumption of language models without compromising on recall and in-context learning capability?</em>&nbsp;</p><p>To begin answering this question, we had to first think about what slows architectures down. Efficient architectures (<em>e.g.</em> Mamba) are much faster than Transformers at inference time (<em>e.g. </em>5x higher throughput) in large part because they have a reduced memory footprint. Smaller memory footprint means larger batch sizes and less I/O. However, it also makes intuitive sense that reducing memory footprint too much could hurt a model’s capacity to recall information seen earlier in the sequence. This looked to us like a classic “<em>no free lunch”</em> situation, so we took a number of popular architectures, varied the hyper-parameters that affected the memory footprint, and evaluated performance on a challenging synthetic associative recall task.</p><p><em>The recall-memory tradeoff. </em><strong>&nbsp;</strong>We found that all architectures obeyed a fundamental tradeoff: the less memory the model consumed during inference, the worse it did on associative recall. We focused on the <em>recurrent state size, </em>the number of bytes used to represent previously seen tokens when generating tokens one-by-one (<em>i.e.</em> recurrently).&nbsp;</p><p>In attention, the <em>state </em>is commonly referred to as the KV-cache, and it grows with the length of the sequence. In the top right of Figure 1, we can see that attention performs recall perfectly, albeit at the cost of a huge recurrent state. Sliding window attention provides a way to cap the size of the KV-cache, but we found (unsurprisingly) that recall performance drops off rapidly as we reduce the size of the recurrent state (<em>e.g.</em> from 100% with 1MB recurrent state to 50% with a 65 KB recurrent state) (Figure 1, light blue).&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e557e1f75f13323664294a_blogpost-01.png" loading="lazy" alt=""></p></figure><p>Excitingly, we found that Mamba expands the pareto frontier of the recall-memory tradeoff curve beyond sliding window attention. This means it is making <strong>better use of limited recurrent state size</strong> than approaches like sliding window attention.&nbsp;</p><p>The natural question is: <em>are there other, perhaps simpler, models that can also expand the pareto frontier?</em></p><h2>‍<strong>Based<em>: </em>a simple model at the pareto frontier</strong></h2><p><strong>‍</strong>To answer this question, we started studying why the simplest alternatives to softmax attention fail to strike a favorable tradeoff. As a further design principle, we searched for primitives that could scale well on current and future hardware. For instance, it would be nice if our primitives could leverage GPU Tensor Cores, specialized hardware on modern GPUs that can perform matrix multiplications (GEMMs) 16x faster for 16x16 matrices than the default (CUDA cores)!</p><p>In our <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis">ICLR paper</a>, we did a deep dive on why any model with a convolutional view (<em>e.g. </em>H3 or Hyena) will struggle on recall. Next, we considered two of the simplest efficient attention techniques out there: (1) <a href="https://arxiv.org/abs/2004.05150">sliding</a> <a href="https://arxiv.org/abs/2007.14062">window</a> <a href="https://mistral.ai/news/announcing-mistral-7b/">attention</a> and (2) <a href="https://arxiv.org/abs/2006.16236">linear attention</a> (<em>i.e.</em> attention without softmax).</p><p>Our experiments on real-world language modeling (up to 1.4bn parameters) and synthetic associative recall suggested to us that neither primitive alone would suffice to navigate the pareto frontier.</p><ol role="list"><li>We found that pure linear attention models struggled to perform precise local token shifts and token comparisons, skills important in recall (Fu et al., 2023; Arora et al., 2023a), as well as dense attention. Expanding on our findings, we do find that our pure linear attention model improves over earlier sub-quadratic architectures. Focusing on the recall-intensive slice of the Pile test set (i.e. next token predictions that force the model to use the prior context vs. memorized knowledge), the 355M pure linear attention model outperforms RWKV-v5 by 0.1 ppl and H3 by 2.6 ppl (Table 1, paper). Pure linear attention is even comparable to the Mamba architecture on this recall-slice – 2.21 ppl for Mamba vs. 2.29 for pure linear attention! However, we observe a sizeable gap to Transformers, which achieve 1.87 ppl on the recall slice.&nbsp;</li><li>In sliding window attention, models can only recall tokens within the sliding window (Figure 2, center). As we increase the window size, the recurrent state grows linearly and has a non-linear effect on speed during parallel training and inference (Figure 2, left).</li></ol><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e556927f9f5016be3204c8_figure-01.png" loading="lazy" alt=""></p></figure><p>However, we find the two primitives are complementary – linear attention for modeling long-range token interactions and sliding window for local token interactions in the sequence. We combined them into a single architecture, called Based (Figure 2, right).&nbsp;</p><ol role="list"><li>Sliding window attention can perform the precise <em>local </em>shifts needed for associative recall. We use <em>tiny </em>window sizes (e.g. 64 in experiments) contrasting the larger window sizes in architectures like <a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral-7B</a> and the recently proposed <a href="https://arxiv.org/abs/2402.19427">Griffin</a>. Intuitively more attention (larger window sizes) is nice from a quality perspective, but we’d like to balance quality and wall-clock speed. To balance these objectives, let’s take a look at the left plot in the above figure. Observe that the latency of matrix multiplication for 16x16 vs. 64x64 matrices are roughly equal, and beyond 64, latency grows non-linearly with the window size.&nbsp; Note that the rough similarity between 16x16 and 64x64 is because the latter keeps the GPU tensor core occupancy high enough to saturate!</li><li>Linear attention enables <em>global </em>token interactions, while maintaining a fixed size recurrent state. Unlike softmax attention, the size of linear attention’s recurrent state is a function of hyperparameters (<em>e.g.</em> choice of feature map) and not sequence length. This allows us to traverse the tradeoff space smoothly. We use a <strong>Taylor approximation of the exponential function as the feature map</strong>, that was first used in <a href="https://arxiv.org/abs/2402.04347">our prior work</a> on linear attentions!</li></ol><p>Critically, the recurrent state size in Based does not grow with the sequence length, as it does in attention. Instead, it is determined by the linear attention feature dimension and the window size. <strong>By dialing these hyperparameters, we can tradeoff recall for throughput and navigate the pareto frontier in Figure 1.&nbsp;&nbsp;</strong></p><p>‍</p><p>Despite its simplicity, on real language modeling experiments (up to at least 1.3 billion parameters), Based is competitive with Mamba in terms of overall Pile perplexity and standard zero-shot benchmarks from the <a href="https://github.com/EleutherAI/lm-evaluation-harness">LM eval harness</a> (shown under Question Answering - Common).&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e5522e1c637fb50e3155b9_oIYg-xYDMbP9eAfBzRAVAfObwm4UrEQo_3fvq2ToPW8kGV722Si8b_h6oYgY6oPB-52hlPHe_gF5od4BQMrG440RNL7Dfwk_V1iSNOgafKsPQQaPRFaOnV4r5ix0zhLMh3hnI5lL4yi8wIJgRiDAN2A.png" alt=""></p></figure><p>These commonly-used zero-shot benchmarks are limited to extremely short text, so they don’t stress test models’ recall capabilities. To address this shortcoming, we <a href="https://arxiv.org/abs/2304.09433">curated a small suite of <em>real world recall-intensive</em> benchmarks</a> that require recalling information from long documents (<em>e.g. </em>information extraction from <a href="https://pubmed.ncbi.nlm.nih.gov/21321283/https://pubmed.ncbi.nlm.nih.gov/21321283/">FDA documents</a> and <a href="https://paperswithcode.com/dataset/swde">raw HTML</a>, and reading comprehension).<em> </em>Based is the strongest sub-quadratic architecture on these tasks, outperforming Mamba by 6.22 accuracy points on average. However, both Based and Mamba still underperform the strongest Transformer baseline, sometimes by large margins. This is consistent with our “no free lunch” observation above.&nbsp;</p><p>It’s important to note that we don’t believe Based is the only architecture that can operate at this point on the tradeoff curve. For example, we show in our paper that we can replace the sliding window attention with short-convolutions (filter size 3) and achieve similar performance within 0.1 perplexity points. We suspect that there are lots of other architectures that can also match this pareto frontier and we’re hopeful there are even others that can even expand beyond it!&nbsp;</p><h2>‍<strong>How we use our fixed-size recurrent state matters too!&nbsp;</strong></h2><p>There are many recurrent architectures that might have the same hidden state size, but our work highlights how the featurization (e.g. linear attention feature map, state update mechanism) matters as well. Our choice for the map in Based is surprisingly simple (<em>high-school calculus is all you need)</em>: approximating the exponential with a Taylor series. We compute $\phi$ such that $\phi(q) \phi(k)^T \approx \exp (q k^T)$. We use just the second-order Taylor series as in our prior work, where $\hat{\exp}(x) = 1 + x + x^2 / 2$! Note that if $x$ has dimension $d’$ then the&nbsp; $x^2$ term will have dimension $d’^2$! The result of the key-value outer product (step 1 above) grows quickly in $d’$, expanding the state size for Based.&nbsp;</p><p><em>How much does our choice of featurization vs. the expanded state size matter when leading to the quality of Based?</em><strong><em> </em></strong>The model’s ability to <em>use the state effectively</em> is key. Shown in the accuracy vs. recurrent state size tradeoff curves, several alternatives to the Taylor map fall <em>below</em> the pareto frontier. Below we compare to models that expand the state size using learned projections and then apply popular feature maps (<a href="https://arxiv.org/abs/2009.14794">Performer</a>, <a href="https://arxiv.org/abs/2202.08791">CosFormer</a>, <a href="https://proceedings.mlr.press/v119/katharopoulos20a.html">PosELU</a>) from the literature. We train these models on the <a href="https://github.com/HazyResearch/zoology">MQAR synthetic</a> test for associative recall and sweep hyperparameters (learning rate) for all points shown in the plot below, finding that the Taylor map is most effective. This trend carries to real world experiments on the Pile language modeling corpus (see our paper for more).</p><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e5fa9f5c6f4e90a0584448_Horizontal%20Figure.png" loading="lazy" alt=""></p></figure><h2>IO and dataflow-aware implementation</h2><p>The next key question is how to make Based competitive in wall clock efficiency. Linear attention is theoretically more efficient than standard attention as a function of sequence length. However, existing implementations of linear attention methods are often <em>slower</em> than well-optimized attention implementations like <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a>.&nbsp;</p><p>In Based, we use the 2nd degree Taylor approximation, which expands the dimension of the keys, leading to large state sizes and large memory consumption O(Nd’<sup>2</sup>d), in sequence length N, key dimension d’, and value dimension d (discussed above). The large resulting key-value state makes naïve implementations of Taylor linear attention quite slow.&nbsp;</p><p>First let’s revisit a bit of context on how the hardware works. GPUs have small amounts of fast-to-access memory (thread-specific registers, shared memory at the warp/32-threads level using SRAM) and large amounts of slow-to-access memory (HBM). It is important to reduce the number of reads-and-writes between slow HBM and SRAM as well as SRAM and registers to unlock efficiency. We present new IO-aware algorithms for the Taylor linear attention forward pass and inference that reduce the HBM to SRAM data movement by $O(Nd'^2)$ bytes and the SRAM to register data movement by $O(Nd^{2}d')$ bytes. Our algorithm allows holding the KV state <em>in-thread-register</em> at feature dimension d’ = 16, which we use in experiments.&nbsp;</p><p>Below we include a comparison between the naive Taylor attention forward pass, an implementation that leverages the popular linear attention kernels from <a href="https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention.py">Fast Transformers</a>, and our custom kernels are shown below across batch size (sequence length 1024).&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e5ffb985fe463472153665_Screenshot%202024-03-04%20at%209.06.47%E2%80%AFAM.png" loading="lazy" alt=""></p></figure><p>We then compare the end-to-end generation speeds of FlashAttention-2, Mamba, and Based 360M and 1.3Bn parameter models using our IO-aware algorithms. We hold the batch size to 2 for prefill, and generate 1024 tokens for next token prediction. Strikingly, Based achieves up to 24x higher throughput than FlashAttention-2!&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e557d27b21a0953533a862_Figure%20from%20DM.png" loading="lazy" alt=""></p></figure><p><em>Stay tuned! </em>These algorithms are implemented in an exciting new CUDA DSL called ThunderKittens, that’s being developed by our lab. Stay tuned for more on this soon – we hope the DSL improves the accessibility of CUDA development! In contrast to frameworks like Triton, which makes opinionated decisions about the supported scope of operations the user can perform, our DSL is <em>embedded</em> in C++. We’re really excited to share it and get your feedback! We’re cooking up more model artifacts alongside in the coming weeks, motivated by the question: <em>What models does the hardware want?&nbsp;</em></p><p>‍</p><p>You can play with our checkpoints and evaluations on <a href="https://huggingface.co/collections/hazyresearch/based-65d77fb7a6f9c813c8b94339c">Hugging Face</a> and in this code repository: <strong><em>&nbsp;</em></strong><a href="https://github.com/HazyResearch/based">https://github.com/HazyResearch/based</a>! Huge thank you to<a href="https://www.together.ai/"> Together AI</a>, <a href="https://hai.stanford.edu/">Stanford HAI</a>, and <a href="https://crfm.stanford.edu/">Stanford CRFM</a> for supporting this work! Please send your feedback and questions to: Simran Arora (<a href="mailto:simarora@stanford.edu">simarora@stanford.edu</a>), Sabri Eyuboglu (<a href="mailto:eyuboglu@stanford.edu">eyuboglu@stanford.edu</a>), Michael Zhang (<a href="mailto:mzhang@stanford.edu">mzhang@stanford.edu</a>).&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Miles Davis and the recording of Kind of Blue (236 pts)]]></title>
            <link>https://www.esquire.com/entertainment/music/a46871755/james-kaplan-miles-davis-3-shades-of-blue-excerpt/</link>
            <guid>39597525</guid>
            <pubDate>Mon, 04 Mar 2024 23:22:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.esquire.com/entertainment/music/a46871755/james-kaplan-miles-davis-3-shades-of-blue-excerpt/">https://www.esquire.com/entertainment/music/a46871755/james-kaplan-miles-davis-3-shades-of-blue-excerpt/</a>, See on <a href="https://news.ycombinator.com/item?id=39597525">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>Every product was carefully curated by an Esquire editor. We may earn a commission from these links.</span></p><div data-journey-body="standard-article"><p data-journey-content="true" data-node-id="0"><em>Jazz was at the apex of its artistic power and commercial popularity when, in 1959, some of the music's greatest innovators gathered to record in New York City. In this excerpt from the new book </em><em></em>3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool<em> (Penguin Press, March 5, 2024), author James Kaplan puts us in the room as Davis and his collaborators record "So What," the track that leads off what is often hailed as the greatest jazz album ever</em><em>. </em></p><hr data-node-id="1"><p data-journey-content="true" data-node-id="2">March 2, 1959—a late-winter Monday in the second-to-last year of the Eisenhower administration. Fair and mild in Manhattan. Among the top stories in <em>The New York</em> <em>Times</em> that morning: a fogbound collision between the American Export liner <em>Constitution </em>and an oil tanker; the “commuter crisis” caused by ever-rising automobile use in the metropolitan area; tensions between white colonials and Black natives in East Africa. This last article quotes a British banker alleging “the vast unreadiness of the great majority of Africans for self-government.”</p><p data-journey-content="true" data-node-id="3">An older, staider world. On the first page of the second section, a story by the young reporter Gay Talese about “Crazy Couple Clubs”—groups of jaded suburbanites seeking unusual amusements in the city: visits to yoga clubs, night court, Bowery restaurants. And deeper into the section, on what was still called the Theatres page, a review (glowing) by the paper’s jazz critic, John S. Wilson, of a Thelonious Monk concert at Town Hall. “He has carried apparent uncertainty to a high and refined art,” Wilson wrote. “He makes each performance a fresh and provocative experience.”</p><p data-journey-content="true" data-node-id="4">If the Crazy Couple Club of Manhasset—by the evidence of the photograph in the <em>Times</em> piece a prosperous and cheerfully self-satisfied group—had dared to extend their Bowery slumming beyond ethnic restaurants, they might have wandered into the cozy, smoky, messy confines of the Five Spot Café, at 5 Cooper Square, whose owners, two Italian American brothers and ex-GIs named Joe and Iggy Termini, had for the past three years been booking some of the greatest jazz musicians of the day, including Cecil Taylor, Cannonball Adderley, and, most notably, Thelonious Monk, whom the Terminis had helped regain his New York City cabaret card—a conditional ID issued by the police department as a (legally and practically questionable) method of discouraging narcotics use—six years after Monk had lost his card, and with it the right to play in clubs that served alcohol, in a mistaken 1951 drug bust.</p><div size="medium" data-embed="body-image" data-lazy-id="P0-7" data-node-id="5"><p><img alt="new york circa 1959 jazz trumpeter miles davis records at 30th street studios in circa 1959 in new york city, new york photo by vernon l smith photo by michael ochs archivesgetty images" title="Recording At 30th Street Studios" loading="lazy" width="2400" height="3479" decoding="async" data-nimg="1" sizes="100vw" srcset="https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=640:* 640w, https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=768:* 980w, https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=980:* 1120w, https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=980:* 1200w, https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=980:* 1920w" src="https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=980:*"></p><div><figcaption><span>Michael Ochs Archives</span><span>//</span><span>Getty Images</span></figcaption><p>Miles Davis at 30th Street Studio in New York City circa 1959. </p></div></div> <p data-journey-content="true" data-node-id="7">Amid the tobacco and reefer fumes and beer reek of that tiny, dark saloon (a glass of gin cost fifty cents; a pitcher of beer, a dollar), the members of the Crazy Couple Club of Manhasset might have found themselves sitting shoulder to shoulder with (though they almost certainly would have failed to recognize) such Five Spot habitués as the painters Willem de Kooning, Joan Mitchell, and Mark Rothko; the writers Jack Kerouac, Allen Ginsberg, and Frank O’Hara; and the young jazz titans Miles Davis, John Coltrane, and Bill Evans.</p><p data-journey-content="true" data-node-id="8">The Five Spot was closed on Mondays, but on that March Monday Davis, Coltrane, and Evans had other business anyway: in Columbia Records’ 30th Street Studio, they were joining the alto saxophonist Cannonball Adderley, bassist Paul Chambers, and drummer Jimmy Cobb to begin making, under Miles’s leadership, what would become the bestselling, and arguably most beloved, jazz album of all time, Miles’s <em>Kind of Blue</em>. March 2 and April 22: three tunes recorded on the first date (“So What,” “Freddie Freeloader,” and “Blue in Green”), two on the second (“All Blues” and “Flamenco Sketches”). Every complete take but one (“Flamenco Sketches”) was a first take, the process similar, as Evans later wrote in the LP’s liner notes, to a genre of Japanese visual art in which black watercolor is applied spontaneously to a thin stretched parchment, with no unnatural or interrupted strokes possible, Miles’s cherished ideal of spontaneity achieved.</p><p data-journey-content="true" data-node-id="9">The quiet and enigmatic majesty of the resulting record both epitomizes jazz and transcends the genre. The album’s powerful and enduring mystique has made it widely beloved among musicians and music lovers of every category: jazz, rock, classical, rap. This is the story of the three geniuses who joined forces to create one of the great classics in Western music—how they rose up in the world, came together like a chance collision of particles in deep space, produced a brilliant flash of light, and then went on their separate ways to jazz immortality.</p><hr data-node-id="10"><div size="medium" data-embed="body-image" data-lazy-id="P0-8" data-node-id="11"><p><img alt="american jazz pianist bill evans 1929 1980 at the piano during a performance filmed for the bbc television music series 'jazz 625' at bbc television theatre in shepherd's bush, london on 19th march 1965 photo by david redfernredferns" title="Bill Evans Trio On BBC Jazz 625 TV Series" loading="lazy" width="2400" height="2396" decoding="async" data-nimg="1" sizes="100vw" srcset="https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=640:* 640w, https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=768:* 980w, https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=980:* 1120w, https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=980:* 1200w, https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=980:* 1920w" src="https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=980:*"></p><div><figcaption><span>David Redfern</span><span>//</span><span>Getty Images</span></figcaption><p>Bill Evans at the piano during a performance filmed for the BBC in London in March 1965.</p></div></div><p data-journey-content="true" data-node-id="12">No musician ever goes into a record date expecting to make history; every man in Miles’s band had recorded dozens of times before. “Professionals,” Bill Evans said, “have to go in at 10 o’clock on a Wednesday and make a record and hope to catch a really good day.” On the face of it, there was nothing remarkable about Project B 43079. </p><p data-journey-content="true" data-node-id="13">The control booth at 30th Street was up a flight of stairs from the studio floor, in what had once been the balcony of the old church: producer Irving Townsend, recording engineer Fred Plaut, and Plaut’s assistant Bob Waller looked down from above as Miles talked to the musicians, who were placed around the open floor much as they’d stand onstage in a concert. On some recording sessions, Columbia producers used rolling baffles to isolate musicians or singers and eliminate sound leakage; at Davis’s direction, this session would proceed baffle-free, all musicians constantly aware of, and inspired by, each other’s playing. Sound leakage from one player’s mike to another’s was not only expected but essential. Each man had his own Telefunken U-49 microphone, except for Cobb, who had two, one pointed at the snare and one overhead to pick up the cymbals. </p><p data-journey-content="true" data-node-id="14">The state of studio recording in 1959 was such that the musicians rather than the engineer were responsible for regulating the loudness or softness of their instruments, by dynamics or distance from the mike. As Davis picked up his horn, Waller started the tapes rolling—one master and one safety—on the Ampex reel-to-reel recorders, and Townsend pushed the intercom button. </p><p data-journey-content="true" data-node-id="15"> “Miles, where are you gonna work now?” he asked. The producer was referring to Davis’s position in relation to the microphone, from which he had apparently stepped back momentarily.</p><p data-journey-content="true" data-node-id="16">“Right here,” Miles said.</p><p data-journey-content="true" data-node-id="17">“When I play it I’m gonna raise my horn a little bit,” Miles said. His customary playing stance, onstage or in the recording studio, was to point his trumpet straight at the floor as he played, a position that communicated contemplation and moodiness, though it was primarily a way of regulating his tone. “Can I move this down a little bit?” He indicated the mike.</p><p data-journey-content="true" data-node-id="18">“It’s against policy to move a microphone,” Townsend said, deadpan. The old church echoed with laughter.</p><hr data-node-id="19"><p data-journey-content="true" data-node-id="20">Outside the 30th Street Studio, Manhattan was Manhattaning: rounded buses and big yellow cabs grinding up and down the avenues; car horns and scraps of radio music and pedestrians’ voices echoing in the deep-shadowed side streets. Outside, the everyday clamor and clash of a city afternoon in late-winter 1959; inside, the densest quiet as a passage outside of time proceeded: the recording of CO 62291, the number that would come to be titled “So What,” leading off the album soon to be known as <em>Kind of Blue</em>.</p><p data-journey-content="true" data-node-id="21">The first take began. There was a false start of four seconds, followed by an incomplete take of forty-nine seconds. Townsend interrupted from the booth: something was interfering with the song’s profound hush. “Hold it,” the producer said. “Sorry—listen, we gotta watch it because, ah, there’s noises all the way through this. This is so quiet to begin with, and every click—watch the snare too, we’re picking up some of the vibrations on it—”</p><p data-journey-content="true" data-node-id="22">Miles, ever on the lookout for meaningful accidentals, demurred. “Well, that goes with it,” he said. “<em>All </em>that goes with it.”</p><p data-journey-content="true" data-node-id="23">“All right,” Townsend allowed. “Not all the other noises, though . . .”</p><p data-journey-content="true" data-node-id="24">Another false start, seventeen seconds. An incomplete take, a minute eleven. A telephone rang in the control booth. Once quiet was restored, three more false starts, of sixteen, seven, and fifteen seconds.</p><p data-journey-content="true" data-node-id="25">Then, history.</p><section data-embed="pullquote" data-lazy-id="P0-9" data-node-id="26"><blockquote><blockquote>The full Take 3 was nine minutes and thirty-five seconds of musical transcendence.</blockquote></blockquote></section><p data-journey-content="true" data-node-id="27">Someone—some say it was Gil Evans; Bill Evans’s biographer Peter Pettinger and the trumpeter Wallace Roney asserted it was Bill Evans—had sketched out a single-line introduction to the piece, a hushed dialogue between piano and bass, proceeding at its own dreamy pace and built on meditative, European art song–esque chords (the fourth, with two white piano keys between the lower and upper notes, being an interval which, enigmatically, presents as neither major nor minor), then on skipping single notes played in unison by the two instruments. Paul Chambers then set the rhythm, plucking the eight-note figure that was to become immortal, the call that began the rhythmic call-and-response of “So What.” Evans then answered, followed by the rest of the sextet.</p><p data-journey-content="true" data-node-id="28">“The piano’s (and then the band’s) answering ‘amen’ (or ‘so what’) riffs,” Pettinger writes, “were built up largely in fourths, as opposed to the thirds that are basic to the tonal system, as exemplified by Bobby Timmons’s comparable composition ‘Moanin’,’ recorded by him some four months earlier.”</p><p data-journey-content="true" data-node-id="29">Timmons, the pianist for Art Blakey’s Jazz Messengers, was all of twenty-two when he wrote “Moanin’,” a call-and-response number which, unlike Miles’s, had a strong, foursquare gospel feeling. Call-and-response was an ancient form, with roots in African ritual, civics, and music; it traveled to America and underlay African American work songs and religious rituals from 1619 on. Timmons’s song, like Blakey’s quintet and Horace Silver’s compositions and bands, was hugely influential in pointing jazz in a more soulful direction. Miles would have known the tune well—would he have enjoyed its old-fashioned wholeheartedness? been impatient with it? It didn’t matter: He was proceeding on his own musical path, channeling strong emotions through the prisms and filters of his biting intelligence and contrary spirit. Some people called this Cool; under the surface it was anything but.</p><div size="medium" data-ad-exclude="(min-width: 90rem)" data-embed="embed-product" data-node-id="30"><p><h2>3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool</h2></p><a target="_blank" rel="nofollow noopener" href="https://www.amazon.com/dp/0525561005" aria-label="$35 at Amazon for 3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool" data-href="https://www.amazon.com/dp/0525561005" data-product-url="https://www.amazon.com/dp/0525561005" data-affiliate="true" data-affiliate-url="https://www.amazon.com/dp/0525561005" data-affiliate-network="{&quot;id&quot;:&quot;6d6f68a9-010c-4169-97b0-14dd8a217bdb&quot;,&quot;site_id&quot;:&quot;8b6b0f67-bcde-4d9d-86c6-dedff1abae12&quot;,&quot;is_active&quot;:true,&quot;details&quot;:null,&quot;metadata&quot;:{},&quot;network&quot;:{&quot;id&quot;:&quot;469ce69f-4798-416d-9432-eaa9954b4053&quot;,&quot;name&quot;:&quot;Amazon&quot;,&quot;is_active&quot;:true,&quot;business_unit_id&quot;:&quot;ad046b46-538b-42cb-aa54-c3d158875ed6&quot;,&quot;details&quot;:&quot;&quot;,&quot;metadata&quot;:{},&quot;created_at&quot;:&quot;2021-07-28T16:03:03.241365+00:00&quot;,&quot;last_updated_at&quot;:&quot;2021-07-28T16:03:03.241381+00:00&quot;}}" data-vars-ga-call-to-action="$35 at Amazon" data-vars-ga-media-role="" data-vars-ga-media-type="Single Product Embed" data-vars-ga-outbound-link="https://www.amazon.com/dp/0525561005" data-vars-ga-product-id="e3aca9e6-8276-4b48-b1ed-ea713fd7e0e8" data-vars-ga-product-price="$35.00" data-vars-ga-product-retailer-id="ea986cbc-1ff3-48eb-b7a9-52cb72928f2c" data-vars-ga-link-treatment="(not set) | (not set)" data-vars-ga-sku="0525561005" data-vars-ga-magento-tracking="1"><div><p><img srcset="https://hips.hearstapps.com/vader-prod.s3.amazonaws.com/1706194460-713WQ70oeyL.jpg?crop=1xw:1xh;center,top&amp;resize=640:* 640w, https://hips.hearstapps.com/vader-prod.s3.amazonaws.com/1706194460-713WQ70oeyL.jpg?crop=1xw:1xh;center,top&amp;resize=980:* 980w" alt="3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool" title="3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool" src="https://hips.hearstapps.com/vader-prod.s3.amazonaws.com/1706194460-713WQ70oeyL.jpg?crop=1xw:1xh;center,top&amp;resize=980:*" width="1684" height="2560" decoding="async" loading="lazy"></p></div></a><div size="medium"><p><h2>3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool</h2></p></div></div><p data-journey-content="true" data-node-id="31">The full Take 3 was nine minutes and thirty-five seconds of musical transcendence. Miles’s solo, an impromptu composition in itself, would gain its own immortality: generations of musicians would memorize it note for note. Miles is talking to you in that solo, playing in the middle sonic range of the human voice, and he’s got all kinds of things to say, in brief and at length. He starts and stops; he starts again and goes on. And we’re freshly astonished at how very much he can express, in so few notes, <em>in the moment</em>. </p><p data-journey-content="true" data-node-id="32">The richness each of the soloists was able to create improvising over just two chords, D and E♭ Dorian, vindicates Miles’s modal concept. Coltrane was in exploratory rather than loud and fast form, traveling up and down each scale to find astringent delights. Cannonball was no less seeking, but lush toned as always, and unable not to find melodies and tuneful fillips, even in this minimalist frame. And Evans’s solo was perhaps most in sync with the tune’s hushed simplicity: playing quiet arpeggios and complex chords a little shyly at first, but then growing more assertive—and surprising: “I’m thinking of the end of Bill’s solo on ‘So What,’” Herbie Hancock told the writer Ashley Kahn. “He plays these phrases, a second apart. He plays seconds.” Still filled with wonderment forty years after the fact, Hancock was talking about an interval on the piano that’s barely an interval—two adjacent keys played simultaneously. By itself, the sound is dissonant; in this context it’s startlingly expressive. “I had never heard anybody do that before,” Hancock said. “He’s following the modal concept maybe more than anybody else. That just opened up a whole vista for me.”</p><p data-journey-content="true" data-node-id="33">CO 62291 wasn’t yet officially named on the day it was recorded, but in the years after <em>Kind of Blue</em>’s release, more than one person would take credit for its title: John Szwed writes that it “may have been suggested to [Davis] by Beverly Bentley [a girlfriend of Miles’s, later Norman Mailer’s fourth wife], who said it sounded like his favorite dismissive remark, but folks in East St. Louis were more likely to believe that it came from Miles’s brother-in-law’s retort when Miles told him in 1944 that he was leaving for New York: ‘So what?’” And the actor Dennis Hopper, who said he was a close friend of Davis’s, recalled that the phrase was a comeback he, Hopper, used to deploy, jab-like, when Miles ran his mouth while the two of them sparred together: “Oh come on, Miles, so what?”</p><p data-journey-content="true" data-node-id="34">“So one time I came into the [jazz] club,” Hopper remembered, “and he said, ‘I wrote a little song for you’—and he played ‘So What.’”</p><hr data-node-id="35"><div size="medium" data-embed="body-image" data-lazy-id="P0-10" data-node-id="36"><p><img alt="jazz great john coltrane 1926 1967 playing his tenor saxophone during an impulse records recording session in new york, early 1960s photo by robert abbott sengstackegetty images" title="John Coltrane" loading="lazy" width="2400" height="3439" decoding="async" data-nimg="1" sizes="100vw" srcset="https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=640:* 640w, https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=768:* 980w, https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=980:* 1120w, https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=980:* 1200w, https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=980:* 1920w" src="https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=980:*"></p><div><figcaption><span>Robert Abbott Sengstacke</span><span>//</span><span>Getty Images</span></figcaption><p>John Coltrane playing tenor saxophone during a recording session in New York in the early 1960s.</p></div></div><p data-journey-content="true" data-node-id="37">The word “timeless” has become a cliché, a selling tool for luxury goods. And yet <em>Kind of Blue</em> is a timeless album, and “So What” arguably its signature number. What is this about? For sixty years and more, jazz and popular music had consisted of songs that told stories, either explicitly—in lyrics—or in their construction. The most common song framework in both genres was known as AABA: two choruses followed by a bridge (aka channel, release, or middle eight), followed by an out-chorus. (Popular songs of the first half of the twentieth century also typically began with a verse: a brief, explanatory introduction that might or might not be included in performance or on recordings.) The sound of tunes made this way was a satisfying blend of exposition and resolution. </p><p data-journey-content="true" data-node-id="38">Popular songs, which often became the explicit or implicit basis for jazz tunes, were written in a given key, and while they might wander chordally—see Oscar Hammerstein and Jerome Kern’s “All the Things You Are” or the bridge of Richard Rodgers and Lorenz Hart’s “Have You Met Miss Jones?”—they tended, satisfyingly, to come back home to that first chord. This was even truer of the blues, with its intrinsic I-IV-V format, a structure that was restrictive but deeply pleasing. A story was told, and you learned the outcome, even if it was sad. (See: “Moanin’.”) You knew how it turned out—maybe you knew before the song started—but hearing about it could make you forget your troubles for a while or identify with the singer’s or the musician’s troubles.</p><p data-journey-content="true" data-node-id="39">But with Miles, in life and in art, it was always the thing withheld. And the essence of modal music—the essence of “So What”—was that you had no idea how it turned out, or if it turned out. Which was pretty much the way the world was looking at that moment, and maybe the way (you had to think) it was going to look from then on.</p><p data-journey-content="true" data-node-id="40">It was 1959; the world jostled and rocked. American automobiles sprouted double headlights and fins. Batista fled Havana; Castro entered. Khrushchev met Mao, visited Disneyland, debated Nixon in a model kitchen at the American National Exhibition in Moscow. Alaska and Hawaii joined the Union; the flag gained two stars. The crashes of commercial airliners were a depressingly regular event. The music died in Clear Lake, Iowa; the Clutters died in Holcomb, Kansas. Johnny and the Moondogs, a British guitar trio led by the eighteen-year-old art student John Lennon (his bandmates were the seventeen-year-old Paul McCartney and sixteen-year-old George Harrison), played gigs around Liverpool whenever they could find a drummer. <em>Rocky and Bullwinkle</em> and <em>Bonanza</em>—in color—debuted. As did the Xerox machine. As did the Barbie doll. NASA named the seven original astronauts, and the Space Age began. <em>Gypsy</em> and <em>The Sound of Music</em> and <em>A Raisin in the Sun</em> premiered on Broadway. The Boeing 707 and the ICBM were introduced: travelers could now fly to far-off destinations at unprecedented speeds, as could nuclear bombs.</p><p data-journey-content="true" data-node-id="41">So what.</p><hr data-node-id="42"><p data-journey-content="true" data-node-id="43"><em>From </em>3 SHADES OF BLUE: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool<em> by James Kaplan, to be published on March 5, 2024, by Penguin Press, an imprint of Penguin Publishing Group, a division of Penguin Random House, LLC. Copyright © 2024 by James Kaplan. </em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS acquires Talen's 960MW nuclear data center campus in Pennsylvania (185 pts)]]></title>
            <link>https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/</link>
            <guid>39597184</guid>
            <pubDate>Mon, 04 Mar 2024 22:47:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/">https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/</a>, See on <a href="https://news.ycombinator.com/item?id=39597184">Hacker News</a></p>
Couldn't get https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[How I keep myself alive using Golang (602 pts)]]></title>
            <link>https://www.bytesizego.com/blog/keeping-alive-with-go</link>
            <guid>39597131</guid>
            <pubDate>Mon, 04 Mar 2024 22:42:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bytesizego.com/blog/keeping-alive-with-go">https://www.bytesizego.com/blog/keeping-alive-with-go</a>, See on <a href="https://news.ycombinator.com/item?id=39597131">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: 3 years and 1M users later, I just open-sourced my "Internet OS" (1028 pts)]]></title>
            <link>https://github.com/HeyPuter/puter</link>
            <guid>39597030</guid>
            <pubDate>Mon, 04 Mar 2024 22:31:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/HeyPuter/puter">https://github.com/HeyPuter/puter</a>, See on <a href="https://news.ycombinator.com/item?id=39597030">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h3 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/45728aadda95f178ec0e4f26999b5f7cc4cd74882d80872ecb46142258177f66/68747470733a2f2f6173736574732e70757465722e736974652f70757465722d6c6f676f2e706e67"><img width="80" alt="Puter.com, The Personal Cloud Computer: All your files, apps, and games in one place accessible from anywhere at any time." src="https://camo.githubusercontent.com/45728aadda95f178ec0e4f26999b5f7cc4cd74882d80872ecb46142258177f66/68747470733a2f2f6173736574732e70757465722e736974652f70757465722d6c6f676f2e706e67" data-canonical-src="https://assets.puter.site/puter-logo.png"></a></h3><a id="" aria-label="Permalink: " href="#"></a></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Desktop Environment in the Browser!</h3><a id="user-content-desktop-environment-in-the-browser" aria-label="Permalink: Desktop Environment in the Browser!" href="#desktop-environment-in-the-browser"></a></p>

<div dir="auto"><h3 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3069986234ddeb463c411dc613d2d62d64cf7f485d2b76c15946ab02607c174c/68747470733a2f2f6173736574732e70757465722e736974652f70757465722e636f6d2d73637265656e73686f742d322e77656270"><img width="700" alt="screenshot" src="https://camo.githubusercontent.com/3069986234ddeb463c411dc613d2d62d64cf7f485d2b76c15946ab02607c174c/68747470733a2f2f6173736574732e70757465722e736974652f70757465722e636f6d2d73637265656e73686f742d322e77656270" data-canonical-src="https://assets.puter.site/puter.com-screenshot-2.webp"></a></h3><a id="user-content--1" aria-label="Permalink: " href="#-1"></a></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Puter</h2><a id="user-content-puter" aria-label="Permalink: Puter" href="#puter"></a></p>
<p dir="auto">Puter is an advanced open-source desktop environment in the browser, designed to be feature-rich, exceptionally fast, and highly extensible. It can be used to build remote desktop environments or serve as an interface for cloud storage services, remote servers, web hosting platforms, and more.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/HeyPuter/puter
cd puter
npm install
npm start"><pre>git clone https://github.com/HeyPuter/puter
<span>cd</span> puter
npm install
npm start</pre></div>
<p dir="auto">This will launch Puter at <a href="http://localhost:4000/" rel="nofollow">http://localhost:4000</a> (or the next available port).</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Deploy to Production</h2><a id="user-content-deploy-to-production" aria-label="Permalink: Deploy to Production" href="#deploy-to-production"></a></p>
<p dir="auto">Detailed guide on how to deploy Puter in production: <a href="https://github.com/HeyPuter/puter/blob/master/docs/prod.md">docs/prod.md</a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">❓ What's the use case for Puter?</h3><a id="user-content--whats-the-use-case-for-puter" aria-label="Permalink: ❓ What's the use case for Puter?" href="#-whats-the-use-case-for-puter"></a></p>
<p dir="auto">Puter can be used as:</p>
<ul dir="auto">
<li>An alternative to Dropbox, Google Drive, OneDrive, etc. with a fresh interface and powerful features.</li>
<li>Remote desktop environment for servers and workstations.</li>
<li>A platform for building and hosting websites, web apps, and games.</li>
<li>A friendly, open-source project and community to learn about web development, cloud computing, distributed systems, and much more!</li>
</ul>

<p dir="auto"><h3 tabindex="-1" dir="auto">❓ Why isn't Puter built with React, Angular, Vue, etc.?</h3><a id="user-content--why-isnt-puter-built-with-react-angular-vue-etc" aria-label="Permalink: ❓ Why isn't Puter built with React, Angular, Vue, etc.?" href="#-why-isnt-puter-built-with-react-angular-vue-etc"></a></p>
<p dir="auto">For performance reasons, Puter is built with vanilla JavaScript and jQuery. Additionally, we'd like to avoid complex abstractions and to remain in control of the entire stack, as much as possible.</p>
<p dir="auto">Also partly inspired by some of our favorite projects that are not built with frameworks: <a href="https://github.com/microsoft/vscode">VSCode</a>, <a href="https://www.photopea.com/" rel="nofollow">Photopea</a>, and <a href="https://www.onlyoffice.com/" rel="nofollow">OnlyOffice</a>.</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">❓ Why jQuery?</h3><a id="user-content--why-jquery" aria-label="Permalink: ❓ Why jQuery?" href="#-why-jquery"></a></p>
<p dir="auto">Puter interacts directly with the DOM and jQuery provides an elegant yet powerful API to manipulate the DOM, handle events, and much more. It's also fast, mature, and battle-tested.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">The default wallpaper is created by <a href="https://unsplash.com/photos/blue-orange-and-yellow-wallpaper-E8Ufcyxz514" rel="nofollow">Milad Fakurian</a> and published on <a href="https://unsplash.com/" rel="nofollow">Unsplash</a>.</p>
<p dir="auto">Icons by <a href="https://github.com/PapirusDevelopmentTeam/papirus-icon-theme">Papirus</a> under GPL-3.0 license.</p>
<p dir="auto">Icons by <a href="https://iconoir.com/" rel="nofollow">Iconoir</a> under MIT license.</p>
<p dir="auto">Icons by <a href="https://github.com/elementary/icons">Elementary Icons</a> under GPL-3.0 license.</p>
<p dir="auto">Icons by <a href="https://tabler.io/" rel="nofollow">Tabler Icons</a> under MIT license.</p>
<p dir="auto">Icons by <a href="https://icons.getbootstrap.com/" rel="nofollow">bootstrap-icons</a> under MIT license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Borrow checking without lifetimes (105 pts)]]></title>
            <link>https://smallcultfollowing.com/babysteps/blog/2024/03/04/borrow-checking-without-lifetimes/</link>
            <guid>39594362</guid>
            <pubDate>Mon, 04 Mar 2024 18:52:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://smallcultfollowing.com/babysteps/blog/2024/03/04/borrow-checking-without-lifetimes/">https://smallcultfollowing.com/babysteps/blog/2024/03/04/borrow-checking-without-lifetimes/</a>, See on <a href="https://news.ycombinator.com/item?id=39594362">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>This blog post explores an alternative formulation of Rust’s type system that eschews <em>lifetimes</em> in favor of <em>places</em>. The TL;DR is that instead of having <code>'a</code> represent a <em>lifetime</em> in the code, it can represent a set of <em>loans</em>, like <code>shared(a.b.c)</code> or <code>mut(x)</code>. If this sounds familiar, it should, it’s the basis for <a href="https://smallcultfollowing.com/babysteps/blog/2023/09/22/polonius-part-1/">polonius</a>, but reformulated as a type system instead of a static analysis. This blog post is just going to give the high-level ideas. In follow-up posts I’ll dig into how we can use this to support interior references and other advanced borrowing patterns. In terms of implementation, I’ve mocked this up a bit, but I intend to start extending <a href="https://github.com/rust-lang/a-mir-formality">a-mir-formality</a> to include this analysis.</p><h2 id="why-would-you-want-to-replace-lifetimes">Why would you want to replace lifetimes?</h2><p>Lifetimes are the best and worst part of Rust. The best in that they let you express very cool patterns, like returning a pointer into some data in the middle of your data structure. But they’ve got some serious issues. For one, the idea of what a lifetime is rather abstract, and hard for people to grasp (“what does <code>'a</code> actually represent?”). But also Rust is not able to express some important patterns, most notably interior references, where one field of a struct refers to data owned by another field.</p><h2 id="so-what-is-a-lifetime-exactly">So what <em>is</em> a lifetime exactly?</h2><p>Here is the definition of a lifetime from the RFC on non-lexical lifetimes:</p><blockquote><p>Whenever you create a borrow, the compiler assigns the resulting reference a lifetime. This lifetime corresponds to the span of the code where the reference may be used. The compiler will infer this lifetime to be the smallest lifetime that it can have that still encompasses all the uses of the reference.</p></blockquote><p><a href="https://rust-lang.github.io/rfcs/2094-nll.html#what-is-a-lifetime">Read the RFC for more details.</a></p><h2 id="replacing-a-lifetime-with-an-origin">Replacing a <em>lifetime</em> with an <em>origin</em></h2><p>Under this formulation, <code>'a</code> no longer represents a <em>lifetime</em> but rather an <strong>origin</strong> – i.e., it explains where the reference may have come from. We define an origin as a <strong>set of loans</strong>. Each loan captures some <strong>place expression</strong> (e.g. <code>a</code> or <code>a.b.c</code>), that has been borrowed along with the mode in which it was borrowed (<code>shared</code> or <code>mut</code>).</p><pre tabindex="0"><code>Origin = { Loan }

Loan = shared(Place)
     | mut(Place)

Place = variable(.field)*  // e.g., a.b.c
</code></pre><h2 id="defining-types">Defining types</h2><p>Using origins, we can define Rust types roughly like this (obviously I’m ignoring a bunch of complexity here…):</p><pre tabindex="0"><code>Type = TypeName &lt; Generic* &gt;
     | &amp; Origin Type
     | &amp; Origin mut Type
     
TypeName = u32 (for now I'll ignore the rest of the scalars)
         | ()  (unit type, don't worry about tuples)
         | StructName
         | EnumName
         | UnionName

Generic = Type | Origin
</code></pre><p>Here is the first interesting thing to note: there is no <code>'a</code> notation here! This is because I’ve not introduced generics yet. Unlike Rust proper, this formulation of the type system has a concrete syntax (<code>Origin</code>) for what <code>'a</code> represents.</p><h2 id="explicit-types-for-a-simple-program">Explicit types for a simple program</h2><p>Having a fully explicit type system also means we can easily write out example programs where all types are fully specified. This used to be rather challenging because we had no notation for lifetimes. Let’s look at a simple example, a program that ought to get an error:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>let</span><span> </span><span>mut</span><span> </span><span>counter</span>: <span>u32</span> <span>=</span><span> </span><span>22_</span><span>u32</span><span>;</span><span>
</span></span></span><span><span><span></span><span>let</span><span> </span><span>p</span>: <span>&amp;</span> <span>/*{shared(counter)}*/</span><span> </span><span>u32</span><span> </span><span>=</span><span> </span><span>&amp;</span><span>counter</span><span>;</span><span>
</span></span></span><span><span><span></span><span>//       ---------------------
</span></span></span><span><span><span>//       no syntax for this today!
</span></span></span><span><span><span></span><span>counter</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span> </span><span>// Error: cannot mutate `counter` while `p` is live
</span></span></span><span><span><span></span><span>println!</span><span>(</span><span>"</span><span>{p}</span><span>"</span><span>);</span><span>
</span></span></span></code></pre></div><p>Apart from the type of <code>p</code>, this is valid Rust. Of course, it won’t compile, because we can’t modify <code>counter</code> while there is a live shared reference <code>p</code> (<a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2021&amp;gist=1a05f0a4aad12c33345ca4adc1cd9bb2">playground</a>). As we continue, you will see how the new type system formulation arrives at the same conclusion.</p><h2 id="basic-typing-judgments">Basic typing judgments</h2><p>Typing judgments are the standard way to describe a type system. We’re going to phase in the typing judgments for our system iteratively. We’ll start with a simple, fairly standard formulation that doesn’t include borrow checking, and then show how we introduce borrow checking. For this first version, the typing judgment we are defining has the form</p><pre tabindex="0"><code>Env |- Expr : Type
</code></pre><p>This says, “in the environment <code>Env</code>, the expression <code>Expr</code> is legal and has the type <code>Type</code>”. The <em>environment</em> <code>Env</code> here defines the local variables in scope. The Rust expressions we are looking at for our <a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2021&amp;gist=1a05f0a4aad12c33345ca4adc1cd9bb2">sample program</a> are pretty simple:</p><pre tabindex="0"><code>Expr = integer literal (e.g., 22_u32)
     | &amp; Place
     | Expr + Expr
     | Place (read the value of a place)
     | Place = Expr (overwrite the value of a place)
     | ...
</code></pre><p>Since we only support one scalar type (<code>u32</code>), the typing judgment for <code>Expr + Expr</code> is as simple as:</p><pre tabindex="0"><code>Env |- Expr1 : u32
Env |- Expr2 : u32
----------------------------------------- addition
Env |- Expr1 + Expr2 : u32
</code></pre><p>The rule for <code>Place = Expr</code> assignments is based on subtyping:</p><pre tabindex="0"><code>Env |- Expr : Type1
Env |- Place : Type2
Env |- Type1 &lt;: Type2
----------------------------------------- assignment
Env |- Place = Expr : ()
</code></pre><p>The rule for <code>&amp;Place</code> is somewhat more interesting:</p><pre tabindex="0"><code>Env |- Place : Type
----------------------------------------- shared references
Env |- &amp; Place : &amp; {shared(Place)} Type
</code></pre><p>The rule just says that we figure out the type of the place <code>Place</code> being borrowed (here, the place is <code>counter</code> and its type will be <code>u32</code>) and then we have a resulting reference to that type. The origin of that reference will be <code>{shared(Place)}</code>, indicating that the reference came from <code>Place</code>:</p><pre tabindex="0"><code>&amp;{shared(Place)} Type
</code></pre><h2 id="computing-liveness">Computing liveness</h2><p>To introduce borrow checking, we need to phase in the idea of <strong>liveness</strong>.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> If you’re not familiar with the concept, the NLL RFC has a <a href="https://rust-lang.github.io/rfcs/2094-nll.html#liveness">nice introduction</a>:</p><blockquote><p>The term “liveness” derives from compiler analysis, but it’s fairly intuitive. We say that a variable is live if the current value that it holds may be used later.</p></blockquote><p>Unlike with NLL, where we just computed live <strong>variables</strong>, we’re going to compute <strong>live places</strong>:</p><pre tabindex="0"><code>LivePlaces = { Place }
</code></pre><p>To compute the set of live places, we’ll introduce a helper function <code>LiveBefore(Env, LivePlaces, Expr): LivePlaces</code>. <code>LiveBefore()</code> returns the set of places that are live before <code>Expr</code> is evaluated, given the environment <code>Env</code> and the set of places live after expression. I won’t define this function in detail, but it looks roughly like this:</p><pre tabindex="0"><code>// `&amp;Place` reads `Place`, so add it to `LivePlaces`
LiveBefore(Env, LivePlaces, &amp;Place) =
    LivePlaces ∪ {Place}

// `Place = Expr` overwrites `Place`, so remove it from `LivePlaces`
LiveBefore(Env, LivePlaces, Place = Expr) =
    LiveBefore(Env, (LivePlaces - {Place}), Expr)

// `Expr1` is evaluated first, then `Expr2`, so the set of places
// live after expr1 is the set that are live *before* expr2
LiveBefore(Env, LivePlaces, Expr1 + Expr2) =
    LiveBefore(Env, LiveBefore(Env, LivePlaces, Expr2), Expr1)
    
... etc ...
</code></pre><h2 id="integrating-liveness-into-our-typing-judgments">Integrating liveness into our typing judgments</h2><p>To detect borrow check errors, we need to adjust our typing judgment to include liveness. The result will be as follows:</p><pre tabindex="0"><code>(Env, LivePlaces) |- Expr : Type
</code></pre><p>This judgment says, “in the environment <code>Env</code>, and given that the function will access <code>LivePlaces</code> in the future, <code>Expr</code> is valid and has type <code>Type</code>”. Integrating liveness in this way gives us some idea of what accesses will happen in the future.</p><p>For compound expressions, like <code>Expr1 + Expr2</code>, we have to adjust the set of live places to reflect control flow:</p><pre tabindex="0"><code>LiveAfter1 = LiveBefore(Env, LiveAfter2, Expr2)
(Env, LiveAfter1) |- Expr1 : u32
(Env, LiveAfter2) |- Expr2 : u32
----------------------------------------- addition
(Env, LiveAfter2) |- Expr1 + Expr2 : u32
</code></pre><p>We start out with <code>LiveAfter2</code>, i.e., the places that are live after the entire expression. These are also the same as the places live after expression 2 is evaluated, since this expression doesn’t itself reference or overwrite any places. We then compute <code>LiveAfter1</code> – i.e., the places live after <code>Expr1</code> is evaluated – by looking at the places that are live <em>before</em> <code>Expr2</code>. This is a bit mind-bending and took me a bit of time to see. The tricky bit here is that liveness is computed <em>backwards</em>, but most of our typing rules (and intution) tends to flow <em>forwards</em>. If it helps, think of the “fully desugared” version of <code>+</code>:</p><pre tabindex="0"><code>let tmp0 = &lt;Expr1&gt;
    // &lt;-- the set LiveAfter1 is live here (ignoring tmp0, tmp1)
let tmp1 = &lt;Expr2&gt;
    // &lt;-- the set LiveAfter2 is live here (ignoring tmp0, tmp1)
tmp0 + tmp1
    // &lt;-- the set LiveAfter2 is live here
</code></pre><h2 id="borrow-checking-with-liveness">Borrow checking with liveness</h2><p>Now that we know liveness information, we can use it to do borrow checking. We’ll introduce a “permits” judgment:</p><pre tabindex="0"><code>(Env, LiveAfter) permits Loan
</code></pre><p>that indicates that “taking the loan Loan would be allowed given the environment and the live places”. Here is the rule for assignments, modified to include liveness and the new “permits” judgment:</p><pre tabindex="0"><code>(Env, LiveAfter - {Place}) |- Expr : Type1
(Env, LiveAfter) |- Place : Type2
(Env, LiveAfter) |- Type1 &lt;: Type2
(Env, LiveAfter) permits mut(Place)
----------------------------------------- assignment
(Env, LiveAfter) |- Place = Expr : ()
</code></pre><p>Before I dive into how we define “permits”, let’s go back to our example and get an intution for what is going on here. We want to declare an error on this assigment:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>let</span><span> </span><span>mut</span><span> </span><span>counter</span>: <span>u32</span> <span>=</span><span> </span><span>22_</span><span>u32</span><span>;</span><span>
</span></span></span><span><span><span></span><span>let</span><span> </span><span>p</span>: <span>&amp;</span><span>{</span><span>shared</span><span>(</span><span>counter</span><span>)}</span><span> </span><span>u32</span><span> </span><span>=</span><span> </span><span>&amp;</span><span>counter</span><span>;</span><span>
</span></span></span><span><span><span></span><span>counter</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span> </span><span>// &lt;-- Error
</span></span></span><span><span><span></span><span>println!</span><span>(</span><span>"</span><span>{p}</span><span>"</span><span>);</span><span> </span><span>// &lt;-- p is live
</span></span></span></code></pre></div><p>Note that, because of the <code>println!</code> on the next line, <code>p</code> will be in our <code>LiveAfter</code> set. Looking at the type of <code>p</code>, we see that it includes the loan <code>shared(counter)</code>. The idea then is that mutating counter is illegal because there is a live loan <code>shared(counter)</code>, which implies that <code>counter</code> must be immutable.</p><p>Restating that intution:</p><blockquote><p>A set <code>Live</code> of live places <em>permits</em> a loan <code>Loan1</code> if, for every live place <code>Place</code> in <code>Live</code>, the loans in the type of <code>Place</code> are compatible with <code>Loan1</code>.</p></blockquote><p>Written more formally:</p><pre tabindex="0"><code>∀ Place ∈ Live {
    (Env, Live) |- Place : Type
    ∀ Loan2 ∈ Loans(Type) { Compatible(Loan1, Loan2) }
}
-----------------------------------------
(Env, Live) permits Loan1
</code></pre><p>This definition makes use of two helper functions:</p><ul><li><code>Loans(Type)</code> – the set of loans that appear in the type</li><li><code>Compatible(Loan1, Loan2)</code> – defines if two loans are compatible. Two shared loans are always compatible. A mutable loan is only compatible with another loan if the places are disjoint.</li></ul><h2 id="conclusion">Conclusion</h2><p>The goal of this post was to give a high-level intution. I wrote it from memory, so I’ve probably overlooked a thing or two. In follow-up posts though I want to go deeper into how the system I’ve been playing with works and what new things it can support. Some high-level examples:</p><ul><li>How to define subtyping, and in particular the role of liveness in subtyping</li><li>Important borrow patterns that we use today and how they work in the new system</li><li>Interior references that point at data owned by other struct fields and how it can be supported</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[There are only 12 binaries in Talos Linux (143 pts)]]></title>
            <link>https://www.siderolabs.com/blog/there-are-only-12-binaries-in-talos-linux/</link>
            <guid>39594355</guid>
            <pubDate>Mon, 04 Mar 2024 18:52:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.siderolabs.com/blog/there-are-only-12-binaries-in-talos-linux/">https://www.siderolabs.com/blog/there-are-only-12-binaries-in-talos-linux/</a>, See on <a href="https://news.ycombinator.com/item?id=39594355">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="60b29bb" data-element_type="widget" data-widget_type="theme-post-content.default">
			
<figure><img fetchpriority="high" decoding="async" width="1024" height="538" src="https://mlxz2ksifyzh.i.optimole.com/w:1024/h:538/q:mauto/ig:avif/f:best/https://www.siderolabs.com/wp-content/uploads/2024/03/12-binaries-banner-1.png" alt=""></figure>



<p>Linux is a core component of your Kubernetes cluster. The distribution you choose will have a big impact on how quickly you can create a cluster, the stability of your workloads, and how much maintenance you’ll need to perform.</p>



<p>When creating a version of Linux for containers or Kubernetes, for many companies and distributions the common practice is to start with a general-purpose Linux and strip away things you don’t need. This results in a smaller footprint variation of the main distribution—e.g. <a href="https://wiki.ubuntu.com/Minimal">Ubuntu minimal</a>—but it always starts from a big, general purpose Linux and tries to make it smaller.</p>



<p>Talos Linux takes the opposite approach. What if the distribution only had to run Kubernetes? What is the minimal set of tooling and executables needed?</p>



<p>There are <strong>12 unique binaries</strong> in /bin and /sbin in Talos 1.7.0. This greatly reduces the size of installation, the maintenance needed for the operating system, and the possible security vulnerabilities of the system.</p>



<p>For reference here are some other popular distributions and how many binaries they include by default. This also counts executables that are symlinked or hard linked to another file (e.g. <code>lvm</code> is often symlinked multiple times for <code>lvs</code> and <code>vgs</code>).</p>



<figure><table><tbody><tr><td>Talos Linux</td><td>29</td></tr><tr><td>Ubuntu Server 22.04</td><td>2780</td></tr><tr><td>Amazon Linux 2</td><td>1382</td></tr><tr><td>Flatcar Container Linux</td><td>2391</td></tr></tbody></table></figure>



<p>All distros were set up with default installation options. No additional packages were installed and binaries were counted with:</p>



<pre><code>ls -1 $(echo $PATH | tr ':' '\n') | wc -l</code></pre>



<p>Talos doesn’t provide a shell, <code>ls</code>, <code>tr</code>, or <code>wc</code>. Files were counted via the API and we didn’t count directories:</p>



<pre><code>talosctl list -n $NODE_IP /sbin | wc -l
talosctl list -n $NODE_IP /bin | wc -l</code></pre>



<p>With only 12 unique files on the system we can tell you what each one does.</p>



<h2>/sbin/init</h2>



<p>The init binary is one of the biggest strengths of Talos Linux. Talos doesn’t ship with a general purpose init system like systemd. Talos’ init system is purpose built for running the Kubelet and a container runtime. The init system exposes a declarative API which is how the system is configured and maintained.</p>



<p>The init binary is <a href="https://github.com/siderolabs/talos/blob/main/internal/app/machined/main.go">called machined</a> and is written in go. It’s less than 400 lines of code and can be understood by a go developer in less than a day. As <a href="https://github.com/systemd/systemd/blob/main/src/core/main.c">opposed to systemd</a> which is over 3000 lines of C code I’ll never comprehend.</p>



<p>The <code>/sbin/init</code> binary is hard linked to <code>/sbin/dashboard</code>, <code>/sbin/poweroff</code>, <code>/sbin/shutdown</code>, and <code>/sbin/wrapperd</code>. While this technically is 5 files, it’s a single file hard linked 4 times to provide convenience commands.</p>



<p>The dashboard is used for providing local and remote information about the node. You can see an overview of how it works on YouTube.</p>



<figure><p>
<iframe title="Talos dashboard overview" width="800" height="450" src="https://www.youtube.com/embed/52Wmkb0H-98?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>The binaries <code>poweroff</code> and <code>shutdown</code> are commands to cleanly shut down the node. These are used by the kernel and external tools, but Talos uses the system API to shutdown.</p>



<p>The <a href="https://github.com/siderolabs/talos/blob/main/internal/app/wrapperd/main.go">wrapperd</a> binary is used during init to fork processes with reduced privledges. Because a child process will inherit a lot from the parent process wrapperd is used to remove kernel capabilities like CAP_SYSADMIN.</p>



<p>All of the other binaries on the system are included from other packages we build from source. You can see how they get built <a href="https://github.com/siderolabs/pkgs">from GitHub</a> and we will review each binary below.</p>



<h2>/bin/containerd*</h2>



<p>This is the container runtime that ships with Talos. It is commonly used with Kubernetes clusters and is the default container runtime option for the majority of providers.</p>



<p>This also includes <code>/bin/containerd-shim-runc-v2</code> and <code>/bin/containerd-shim</code>. Both of these shims provide the same function (executing a container under <code>runc</code>) but <code>containerd-shim</code> was originally used with docker and <code>containerd-shim-runc-v2</code> is used from containerd.</p>



<h2>/bin/runc</h2>



<p>This is the true <a href="https://github.com/opencontainers/runc">parent process of your containers</a>. It is daemonless so the containerd service can restart if needed without stopping all your containers.</p>



<h2>/sbin/modprobe</h2>



<p>The command <a contenteditable="false" href="https://linux.die.net/man/8/modprobe">modprobe</a> is for managing kernel modules to add or remove functionality from your kernel. This is often for adding support for special hardware (e.g. GPUs) but is also used for additional kernel tooling.</p>



<p>Talos doesn’t use modprob directly but some modules require the binary to load other modules. You can add kernel modules to Talos via <a href="https://www.talos.dev/latest/talos-guides/configuration/system-extensions/">system extensions</a> and use pre-built extensions from the <a href="https://www.talos.dev/latest/learn-more/image-factory/">Image Factory</a>.</p>



<h2>/sbin/lvm</h2>



<p>The <code>lvm</code> binary is used for managing logical volumes in Linux. This is provided for services that run in Kubernetes that may need or expect a logical volume to be present on the host (e.g. <a href="https://rook.io/docs/rook/latest-release/Getting-Started/Prerequisites/prerequisites/#lvm-package">rook</a>).</p>



<h2>/sbin/dmsetup</h2>



<p>This is used for managing logical volumes that use the device-mapper driver. It’s similar to <code>lvs</code> commands but a separate binary for more complex disk configuration.</p>



<h2>/sbin/udevd</h2>



<p>The udevd daemon takes kernel messages and passes the messages to other systems to read the messages. It can be configured as part of the <a href="https://www.talos.dev/latest/reference/configuration/v1alpha1/config/#Config.machine.udev">Talos machine config</a>.</p>



<h2>/sbin/mkfs.xfs</h2>



<p>This will create an <a href="https://en.wikipedia.org/wiki/XFS">XFS file system</a> on a disk or logical volume.</p>



<h2>/sbin/xfs_repair</h2>



<p>This is used to repair an XFS file system if it becomes corrupted.</p>



<h2>/sbin/xtables-legacy-multi</h2>



<p>This binary is symlinked by <code>iptables*</code> and <code>ip6tables*</code> to configure IP tables on the host. Container network interface (CNI) providers often mount directories from the host and expect these commands to exist because they cannot easily be run from within a container.</p>



<p>These symlinks account for 12 total files in the system but they all perform common iptables commands.</p>



<h2>Conclusion</h2>



<p>It may seem impossible but that’s the entire system. Every binary is required to bootstrap a Kubernetes cluster or run a node as part of the cluster. This is why we call Talos Linux the <a href="https://www.siderolabs.com/platform/talos-os-for-kubernetes/">Kubernetes Operating System</a>.</p>



<p>There are more executable files in /lib and /usr but those are Shared Object (.so) files and Kernel modules (.ko). These are necessary to run the system for drivers and various programs but are not called directly.</p>



<p>If you would like to download and install Talos on your system of choice you can get started at <a href="https://talos.dev/" target="_blank" rel="noreferrer noopener">https://talos.dev</a>.</p>



<p>To get an even easier interface to managing Kubernetes clusters on-prem or in a cloud provider check out Omni at <a href="https://www.siderolabs.com/platform/saas-for-kubernetes/" target="_blank" rel="noreferrer noopener">https://www.siderolabs.com/platform/saas-for-kubernetes/</a>.</p>



<p>If you have questions or want to get started come join the <a href="https://slack.dev.talos-systems.io/">Talos community Slack</a>.</p>



<figure><p>
<iframe title="Welcome to Omni" width="800" height="450" src="https://www.youtube.com/embed/0gPF0_fLins?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yuzu emulator developers settle Nintendo lawsuit, pay $2.4M in damages (502 pts)]]></title>
            <link>https://twitter.com/oatmealdome/status/1764704580724576465</link>
            <guid>39593647</guid>
            <pubDate>Mon, 04 Mar 2024 18:00:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/oatmealdome/status/1764704580724576465">https://twitter.com/oatmealdome/status/1764704580724576465</a>, See on <a href="https://news.ycombinator.com/item?id=39593647">Hacker News</a></p>
Couldn't get https://twitter.com/oatmealdome/status/1764704580724576465: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Opus 1.5 released: Opus gets a machine learning upgrade (362 pts)]]></title>
            <link>https://opus-codec.org/demo/opus-1.5/</link>
            <guid>39593256</guid>
            <pubDate>Mon, 04 Mar 2024 17:36:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opus-codec.org/demo/opus-1.5/">https://opus-codec.org/demo/opus-1.5/</a>, See on <a href="https://news.ycombinator.com/item?id=39593256">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div id="xiphlogo">
      <p><a href="https://www.xiph.org/"><img src="https://www.xiph.org/images/logos/fish_xiph_org.png" alt="Fish Logo and Xiph.org"></a></p>
      
    </div>


    
    <a href="https://opus-codec.org/">
    <img src="https://opus-codec.org/demo/opus-1.5/opus-1.5_logo.png" alt="Banner"></a>
    <p>
    Opus gets another major update with the release of version 1.5. This release brings quality improvements, including
    ML-based ones, while remaining fully compatible with RFC&nbsp;6716. Here are some of the most noteworthy upgrades.
    </p>

    <h2>Opus Gets a Serious Machine Learning Upgrade</h2>

    <p>This <a href="https://opus-codec.org/downloads/">1.5 release</a> is unlike any of the previous ones. It brings many new features
    that can improve quality and the general audio experience.
    That is achieved through machine learning. Although Opus has
    included machine learning — and even deep learning — before
    (e.g. for <a href="https://jmvalin.ca/opus/opus-1.3/">speech/music detection</a>),
    this is the first time it has used deep learning techniques to process or generate the signals
    themselves.</p>

    <p> Instead of designing a new ML-based codec
    from scratch, we prefer to improve Opus in a fully-compatible way.
    That is an important design goal for ML in Opus.
    Not only does that ensure Opus
    keeps working on older/slower devices, but it also provides an easy upgrade path. Deploying
    a new codec can be a long, painful process. Compatibility means that older and newer
    versions of Opus can coexist, while still providing the benefits of the new version
    when available.</p>

    <p>Deep learning also often gets associated with powerful GPUs, but
    in Opus, we have optimized everything such that it easily runs on most
    CPUs, including phones. We have been careful to avoid huge models (unlike LLMs with
    their hundreds of billions of parameters!). In the end, most users should not notice the extra cost,
    but people using older (5+ years) phones or microcontrollers might. For that reason, all new
    ML-based features are disabled by default in Opus 1.5. They require both a compile-time
    switch (for size reasons) and then a run-time switch (for CPU reasons).</p>

    <p>The following sections describe the new features enabled by ML.</p>

    <h2>Dealing with Packet Loss</h2>

    <p>Packet loss is one of the main annoyances one can encounter during a call. It does not
    matter how good the codec is if the packets do not get through.
    That's why most codecs have <i>packet loss concealment</i> (PLC) that can fill in for missing
    packets with plausible audio that just extrapolates what was being said and avoids leaving
    a hole in the audio (a common thing to hear with Bluetooth headsets). PLC is a place
    where ML can help a lot. Instead of using carefully hand-tuned concealment heuristics, we can just
    let a Deep Neural Network (DNN) do it. The technical details are in our
    <a href="https://arxiv.org/pdf/2205.05785.pdf">Interspeech 2022 paper</a>, for which we got the
    second place in the <a href="https://www.microsoft.com/en-us/research/academic-program/audio-deep-packet-loss-concealment-challenge-interspeech-2022/results/">Audio Deep Packet Loss Concealment Challenge</a>.</p>

    <p>When building Opus, using --enable-deep-plc will compile in the deep PLC code at a cost of
    about 1 MB in binary size.
    To actually enable it at run time, you will need to set the decoder complexity to 5 or more.
    Previously, only the encoder had a complexity knob, but the decoder is now getting one too.
    It can be set with the -dec_complexity option to opus_demo, or OPUS_SET_COMPLEXITY() in the
    API (like for the encoder).
    The extra complexity from running PLC at a high loss rate is about 1% of a laptop CPU core.
    Because deep PLC only affects the decoder, turning it on does not have any compatibility
    implications.
    </p>

    <h3>Deep REDundancy (DRED)</h3>

    <p>PLC is great for filling up occasional missing packets, but unfortunately
    packets often go missing in bursts. When that happens, entire phonemes or words are lost. Of course,
    new generative models could easily be used to seamlessly fill any gap with very plausible words, but
    we believe it is good to have the listener hear the <i>same</i> words that were spoken.
    The way to achieve that is through redundancy. Opus already includes
    a low-bitrate redundancy (LBRR) mechanism to transmit every speech frame twice, but only twice.
    While this helps reduce the impact
    of loss, there's only so much it can do for long bursts.
    </p>

    <p>That is where ML can help. We were certainly not the first to think about using
    ML to make a very low bitrate speech codec. However (we think) we are the first to
    design one that is optimized solely for transmitting redundancy. A regular codec needs to
    have short packets (typically 20 ms) to keep the latency low
    and it has to limit its use of prediction specifically to avoid making the packet
    loss problem even worse. For redundancy, we don't have these problems.
    Each packet will contain a large (up to 1 second) chunk of redundant audio
    that will be transmitted all at once.
    Taking advantage of that, the Opus Deep REDundancy (DRED) uses a rate-distortion-optimized
    variational autoencoder (RDO-VAE) to efficiently compress acoustic parameters in such a way that it can
    transmit one second of redundancy with about 12-32 kb/s overhead.
    Every 20-ms packet is effectively transmitted <i>50 times</i> at a cost similar
    to the existing LBRR.
    See this <a href="https://www.amazon.science/blog/neural-encoding-enables-more-efficient-recovery-of-lost-audio-packets">
    demo</a> for a high-level overview of the science behind DRED, or read the
    <a href="https://arxiv.org/pdf/2212.04453.pdf">ICASSP 2023 paper</a> for all the details and math
    behind it.
    </p>

    <a href="https://opus-codec.org/demo/opus-1.5/dred_results.png"><img src="https://opus-codec.org/demo/opus-1.5/dred_results.png" alt="recurrent units"></a>
    <p>
    Subjective testing (MOS) results measuring the improvement provided by DRED with one second
    redundancy for a range of
    realistic packet loss conditions.
    The results show that DRED achieves much higher quality than what either neural PLC alone,
    or LBRR with neural PLC can achieve.
    When DRED is combined with LBRR, the quality approaches that of the no-loss case.
    In these tests, we used 24&nbsp;kb/s for the <em>base</em> Opus layer, 16&nbsp;kb/s extra for LBRR,
    and 32&nbsp;kb/s extra for DRED.
    </p>

    <p>Use the --enable-dred configure option (which automatically turns on --enable-deep-plc) to
    enable DRED.
    Doing so increases the binary size by about 2&nbsp;MB, with a run-time cost around 1% like for deep PLC.
    Beware that DRED is not yet standardized and the version included in Opus&nbsp;1.5 will
    not be compatible with the final version.
    That being said, it is still safe to experiment with it in applications since the bitstream
    carries an experiment version number and any version incompatibility will be detected and simply cause
    the DRED payload to be ignored (no erroneous decoding or loud noises).</p>


    <h3>Neural Vocoder</h3>

    <p>The very low complexity of deep PLC and DRED is made possible by new neural vocoder technology
    we created specifically for this project. The original papers linked above used a
    <a href="https://arxiv.org/pdf/2202.11169.pdf">highly-optimized</a> version of the original
    <a href="https://jmvalin.ca/demo/lpcnet/">LPCNet vocoder</a>, but even that was not quite
    fast enough. So we came up with a new framewise autoregressive generative
    adversarial network (FARGAN) vocoder that uses pitch prediction to achieve
    a complexity of 600 MFLOPS: 1/5 of LPCNet. That
    allows it to run with less than 1% of a CPU core on laptops or even recent phones.
    We don't yet have a paper or writeup on FARGAN, but we are working on fixing that.</p>

    <h2>Low-Bitrate Speech Quality Enhancement</h2>

    <p>Given enough bits, most speech codecs — including Opus — are able to reach a quality
      level close to transparency.
      Unfortunately, the real world sometimes doesn't give us "enough bits". Suddenly, the coding
      artifacts can become audible, or even annoying.
      The classical approach to mitigate this problem is to apply simple, handcrafted
      postfilters that reshape the coding noise to make it less noticeable.
      While those postfilters usually provide a noticeable improvement, their effectiveness is limited. They
      can't work wonders.
    </p>

    <p>
      The rise of ML and DNNs has produced a number of new and much more powerful enhancement methods,
      but these are typically large, high in complexity, and cause additional decoder delay.
      Instead, we went for a different approach: start with the tried-and-true postfilter idea
      and sprinkle just enough DNN magic on top of it.
      Opus 1.5 includes two enhancement methods: the Linear Adaptive Coding Enhancer (LACE) and a
      Non-Linear variation (NoLACE).
      From the signal point of view, LACE is very similar to a classical postfilter.
      The difference comes from a DNN that
      optimizes the postfilter coefficients on-the-fly based on all the data available to the decoder.
      The audio itself never goes through the DNN.
      The result is a small and very-low-complexity model (by DNN standards) that can run even
      on older phones. An explanation of the internals of LACE is given in this short
      <a href="https://www.youtube.com/watch?v=W47qh9Wp9E0">video presentation</a> and more technical
      details can be found in the corresponding <a href="https://arxiv.org/abs/2307.06610">WASPAA 2023 paper</a>.
      NoLACE is an extension of LACE that requires more computation but is
      also much more powerful due to extra non-linear signal processing.
      It still runs without significant overhead on
      recent laptop and smartphone CPUs. Technical details about NoLACE are given in the corresponding
      <a href="https://arxiv.org/abs/2309.14521">ICASSP 2024 paper</a>.
    </p>

    <a href="https://opus-codec.org/demo/opus-1.5/nolace_results.png"><img src="https://opus-codec.org/demo/opus-1.5/nolace_results.png" alt="recurrent units"></a>
    <p>
    Subjective testing (MOS) results comparing the speech decoded from the default
    decoder to the enhanced speech produced by LACE and NoLACE from that same decoder.
    The uncompressed speech has a MOS of 4.06.
    The results show that using NoLACE, Opus is now perfectly usable down to 6 kb/s.
    At 9 kb/s, NoLACE-enhanced speech is already close to transparency, and better than
    the non-enhanced 12 kb/s.
    </p>

    <p>
      To try LACE and NoLACE, just add the --enable-osce configure flag when building Opus.
      Then, to enable LACE at run-time, set the decoder complexity to 6.
      Set it to 7 or higher to enable NoLACE instead of LACE. Building with --enable-osce increases
      the binary size by about 1.6 MB, roughly 0.5 MB for LACE and 1.1 MB for NoLACE. The LACE model has a
      complexity of 100 MFLOPS which leads to a run-time cost of ~0.15% CPU usage. The NoLACE model has a complexity
      of 400 MFLOPS which corresponds to a run-time cost of ~0.75% CPU usage.
      LACE and NoLACE are currently only applied when the frame size is 20 ms (the default) and the bandwidth
      is at least wideband.
      Although LACE and NoLACE have not yet been standardized, turning them on does not have
      compatibility implications since the enhancements are independent of the encoder.
    </p>

    <h3>Samples</h3>

    <p>OK, nice graphs, but how does it actually sound? The following samples demonstrate
    the effect of LACE or NoLACE on Opus wideband speech quality at different bitrates. We recommend listening with good headphones,
    especially for higher bitrates. </p>

    <div>
      <p><audio controls="" id="speech_player" src="https://opus-codec.org/demo/opus-1.5/samples/female_nolace_12k.wav">
        Your browser does not support the audio tag.
      </audio></p><div>
      <p>Select sample</p>
      <ul>
        <li onclick="setSpeechSample(0, this);" id="speech_default_sample">Female</li>
        <li onclick="setSpeechSample(1, this);">Male</li>
      </ul>
      </div>
      <div>
      <p>Select enhancement</p>
      <ul>
        <li onclick="setSpeechCodec(1, this);">None</li>
        <li onclick="setSpeechCodec(2, this);">LACE</li>
        <li onclick="setSpeechCodec(3, this);" id="speech_default_codec">NoLACE</li>
        <li onclick="setSpeechCodec(4, this);">Uncompressed</li>
      </ul>
      </div>
      <div>
      <p>Select bitrate</p>
      <ul id="speech_bitrate_selector">
        <li onclick="setSpeechRate(0, this);">6 kb/s</li>
        <li onclick="setSpeechRate(1, this);" id="speech_default_rate">9 kb/s</li>
        <li onclick="setSpeechRate(2, this);">12 kb/s</li>
      </ul>
      </div>
      <p>Select where to start playing when selecting a new sample</p>
      <p id="speech_restart_string">Player will <b>continue</b> when changing sample.</p>
    </div>
    <p>Demonstrating the effect of LACE and NoLACE on speech quality at 6, 9, and 12 kb/s.
    </p>

    <h2>WebRTC Integration</h2>

    <p>Using the deep PLC or the quality enhancements should typically require only minor
    code changes. DRED is an entirely different story. It requires closer integration with
    the jitter buffer to ensure that redundancy gets used.</p>
    <p>In a real-time communications system, the size of the jitter buffer determines the
    maximum amount of packet arrival lateness that can be tolerated without producing
    an audible gap in audio playout.
    In the case of packet loss, we can treat the DRED data similarly to
    late arriving audio packets. We take care to only insert this data into the jitter
    buffer if we have observed prior loss. In ideal circumstances, an adaptive jitter
    buffer (like NetEq used in WebRTC) will try to minimize its size in order to preserve
    interactive latency. If data arrives too late for playback, there will be an audible
    gap, but the buffer will then grow to accommodate the new nominal lateness. If network
    conditions improve the buffer can shrink back down, using time scaling to play the
    audio at a slightly faster rate. In the case of DRED, there will always be a loss vs.
    latency tradeoff. In order to make use of the DRED data and cover prior lost packets,
    we will need to tolerate a larger jitter buffer.
    But because we treat DRED similarly to late packet arrival, we can take advantage
    of the existing adaptation in NetEq to provide a reasonable compromise in loss vs. latency.
    </p>

    <p>You can try out DRED using the patches in our <a href="https://github.com/xiph/webrtc-opus-ng/tree/opus-ng">
    webrtc-opus-ng</a> fork of the Google WebRTC repository.
    Using these patches, we were able to evaluate how DRED compares to other approaches.
    And yes, it still works well even with 90% loss.
    See the results below.
    </p>

    <a href="https://opus-codec.org/demo/opus-1.5/dred_plcmos.png"><img src="https://opus-codec.org/demo/opus-1.5/dred_plcmos.png" alt="DRED PLCMOSv2 results"></a>
    <p>
    Objective evaluation of different redundancy schemes under simulated realistic
    packet loss (see <a href="#lossgen">Realistic Loss Simulator</a> below) using Microsoft's
    <a href="https://github.com/microsoft/PLC-Challenge/tree/main/PLCMOS">PLCMOS v2</a> (higher is better).
    All conditions use 48 kb/s, except for DRED+LBRR which uses 64 kb/s to fully take advantage
    of both forms of redundancy.
    Results show that even under extremely lossy conditions, DRED is able to maintain
    acceptable quality.
    It may look strange that the DRED quality increases past 60% loss, but that can be explained by
    the reduced amount of switching between regular packets and DRED redundancy.
    </p>

    <h3>Samples</h3>

    <p>Of course, hearing is believing, so here are some samples produced with the WebRTC patches.
    These should be close to what one might experience during a meeting when packets start to drop.
    Notice some gaps at the beginning as the jitter buffer adapts and is then able to take full advantage
    of DRED. </p>

    <div>
      <p><audio controls="" id="dred_player" src="https://opus-codec.org/demo/opus-1.5/samples/dred/spe48_loss_50_dred_48kbps.wav">
        Your browser does not support the audio tag.
      </audio></p><div>
      <p>Select loss rate</p>
      <ul>
        <li onclick="setDREDPercent(0, this);">0%</li>
        <li onclick="setDREDPercent(1, this);">10%</li>
        <li onclick="setDREDPercent(2, this);">20%</li>
        <li onclick="setDREDPercent(3, this);">30%</li>
        <li onclick="setDREDPercent(4, this);">40%</li>
        <li onclick="setDREDPercent(5, this);" id="dred_default_percent">50%</li>
        <li onclick="setDREDPercent(6, this);">60%</li>
        <li onclick="setDREDPercent(7, this);">70%</li>
        <li onclick="setDREDPercent(8, this);">80%</li>
        <li onclick="setDREDPercent(9, this);">90%</li>
      </ul>
      </div>
      <div>
      <p>Select redundancy</p>
      <ul id="dred_bitrate_selector">
        <li onclick="setDREDSample(0, this);">None (48 kb/s)</li>
        <li onclick="setDREDSample(1, this);">LBRR (48 kb/s)</li>
        <li onclick="setDREDSample(2, this);" id="dred_default_sample">DRED (48 kb/s)</li>
        <li onclick="setDREDSample(3, this);">DRED+LBRR (64 kb/s)</li>
      </ul>
      </div>
      <p>Select where to start playing when selecting a new sample</p>
      <p id="dred_restart_string">Player will <b>continue</b> when changing sample.</p>
    </div>
    <p>Evaluating the effectiveness of the different redundancy options. These audio samples are generated
      using real packet loss traces with the entire WebRTC stack.
    </p>

    <h2>IETF and Standardization</h2>

    <p>To ensure compatibility with the existing standard and future extensions of Opus,
    this work is being conducted within the newly-created IETF
    <a href="https://datatracker.ietf.org/wg/mlcodec/">mlcodec</a> working group.
    This effort is currently focused on three topics:
    a generic extension mechanism for Opus, deep redundancy, and speech coding enhancement.
    </p>

    <h3>Extension Format</h3>

    <p>The new DRED mechanism requires adding extra information to Opus packets while
    allowing an older decoder that does not know about DRED to still decode the regular Opus data.
    We found that the best way to achieve that was through the Opus padding mechanism.
    In the original specification, padding was added to make it possible to make a packet
    bigger if needed (e.g., to meet a constant bitrate even when the encoder produced fewer
    bits than the target).
    Thanks to padding, we can transmit extra information in a packet in a way that an
    older decoder will just not see (so it won't get confused).
    Of course, if we're going to all that trouble, we might as well make sure we're also
    able to handle any future extensions.
    Our <a href="https://datatracker.ietf.org/doc/draft-ietf-mlcodec-opus-extension/">
    Opus extension Internet-Draft</a> defines a format <i>within</i> the Opus padding
    that can be used to transmit both deep redundancy, but also any future extension
    that may become useful. See our
    <a href="https://datatracker.ietf.org/meeting/118/materials/slides-118-mlcodec-opus-extension-mechanism-00.pdf">presentation at IETF&nbsp;118</a>
    for diagrams of how the extensions fit within an Opus packet.
    </p>

    <h3>DRED Bitstream</h3>

    <p>We are also working on standardizing DRED. Standardizing an ML algorithm is
    challenging because of the tradeoff between compatibility and extensibility.
    That's why our <a href="https://datatracker.ietf.org/doc/draft-valin-opus-dred/">
    DRED Internet-Draft</a> describes how to decode extension bits into acoustic features,
    but leaves implementers free to make both better encoders and also better
    vocoders that may further improve on the quality and/or complexity.</p>

    <h3>Enhancement</h3>
    <p>
      For enhancement, we also follow the general strategy to standardize as little as
      possible, since we also expect future research to produce better methods than we
      currently have. That's why we will specify requirements an enhancement method like
      LACE or NoLACE should satisfy in order to be allowed in an opus decoder rather than
      specifying the methods themselves.
      A corresponding <a href="https://datatracker.ietf.org/doc/draft-buethe-opus-speech-coding-enhancement/">
      enhancement Internet-Draft</a>
      has already been created for that purpose.
    </p>

    <h2>Other Improvements</h2>

    <p>Here are briefly some other changes in this release.</p>

    <h3>AVX2 Support</h3>

    <p>Opus now has support and run-time detection for AVX2.
    On machines that support AVX2/FMA (from around 2015 or newer), both the new DNN
    code and the SILK encoder will be significantly faster thanks to the use of
    256-bit SIMD.</p>

    <h3>More NEON Optimizations</h3>

    <p>Existing ARMv7 Neon optimization were re-enabled for AArch64, resulting
    in more efficient encoding.
    The new DNN code can now take advantage of the Arm <i>dot product</i> extensions
    that significantly speed up 8-bit integer dot products on a Cortex-A75 or newer
    (~5 year old phones). Support is detected at run-time, so
    these optimizations are safe on all Arm CPUs.</p>

    <a name="lossgen"></a><h3>Realistic Loss Simulator</h3>
    <p>As a side effect of trying to tune the DRED encoder to maximize quality, we realized
    we needed a better way of simulating packet loss.
    For some purposes, testing with random loss patterns (like tossing a coin repeatedly)
    can be good enough, but since DRED is specifically designed to handle bust loss (which
    is rare with independent random losses) we needed something better.
    As part of the Audio Deep Packet Loss Concealment Challenge, Microsoft
    <a href="https://github.com/microsoft/PLC-Challenge">made available</a>
    some more realistic recorded packet loss traces.
    A drawback of such real data is that one cannot control the percentage of loss
    or generate sequences longer than those in the dataset.
    So we trained a generative packet loss model that can simulate realistic losses with
    a certain target overall percentage of loss.
    Packet loss traces are quite simple and our generative
    model fits in fewer than 10,000 parameters.
    To simulate loss with opus_demo, you need to build with --enable-lossgen.
    Then add -sim-loss &lt;percentage&gt; to the opus_demo command line.
    Note that the loss generator is just an initial design, so feedback is welcome.
    </p>

    <p>
    Because we believe that this loss generator can be useful to other applications,
    we have made it easy to extract it from Opus and use it in other applications.
    The main source file for the generator is <a href="https://gitlab.xiph.org/xiph/opus/-/blob/main/dnn/lossgen.c">
    dnn/lossgen.c</a>. Comments in the file contain information about the other
    dependencies needed for the loss generator.</p>

    <h2> Conclusion </h2>
    <p>
      We hope we demonstrated how our new ML-based tools substantially
      improve error robustness and speech quality with a very modest
      performance impact and without sacrificing compatibility.
      And we're only getting started. There's still more to come.
      We encourage everyone to try out these new features for themselves.
      Please <a href="https://www.opus-codec.org/contact/">let us know</a> about your experience (good or bad)
      so we can continue to improve them.
      Enjoy!
    </p>

    <address>—The Opus development team
      <br>March 4th, 2024
    </address>


    <h2>Additional Resources</h2>
    <ol>
      <li>First and foremost: <a href="https://www.opus-codec.org/">The Opus Project Homepage</a></li>
      <li>The basic Opus techniques for music coding are described in the AES paper:
      <a href="https://jmvalin.ca/papers/aes135_opus_celt.pdf">High-Quality, Low-Delay Music Coding in
      the Opus Codec</a></li>
      <li>The basic Opus techniques for speech coding are described in this other AES paper:
      <a href="https://jmvalin.ca/papers/aes135_opus_silk.pdf">Voice Coding with Opus</a></li>

      <li>Join our development discussion in <a href="irc://irc.libera.chat/opus">#opus at irc.libera.chat</a> (→<a href="https://web.libera.chat/#opus" onclick="document.getElementById('chatbox').innerHTML='<iframe src=\'https://web.libera.chat/#opus\' width=800 height=600/>';return false;">web interface</a>)</li>
    </ol>
      
    <hr>

    <div>
        <p>
          (C) Copyright 2024 Xiph.Org Foundation
        </p>
      </div>

  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The women who coined the expression 'surfing the internet' (2019) (112 pts)]]></title>
            <link>https://www.surfertoday.com/surfing/the-woman-who-coined-the-expression-surfing-the-internet</link>
            <guid>39592993</guid>
            <pubDate>Mon, 04 Mar 2024 17:20:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.surfertoday.com/surfing/the-woman-who-coined-the-expression-surfing-the-internet">https://www.surfertoday.com/surfing/the-woman-who-coined-the-expression-surfing-the-internet</a>, See on <a href="https://news.ycombinator.com/item?id=39592993">Hacker News</a></p>
Couldn't get https://www.surfertoday.com/surfing/the-woman-who-coined-the-expression-surfing-the-internet: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Improving Network Performance with Linux Flowtables (150 pts)]]></title>
            <link>https://www.ubicloud.com/blog/improving-network-performance-with-linux-flowtables</link>
            <guid>39592771</guid>
            <pubDate>Mon, 04 Mar 2024 17:05:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ubicloud.com/blog/improving-network-performance-with-linux-flowtables">https://www.ubicloud.com/blog/improving-network-performance-with-linux-flowtables</a>, See on <a href="https://news.ycombinator.com/item?id=39592771">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" data-doc-height="1" role="banner"><p><a href="https://www.ubicloud.com/"><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/64fe48116c52fe1a51e17279_ubicolud%20logo.png" loading="lazy" alt=""></a></p></div><div id="w-node-decdb48f-56e8-4c35-c577-932285e9b439-32a26126"><p>March 4, 2024 · 5 min read</p><div><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e39641837eb115bbff4328_Furkan%20Sahin%20Picture.jpeg" loading="lazy" sizes="40px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e39641837eb115bbff4328_Furkan%20Sahin%20Picture-p-500.jpeg 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e39641837eb115bbff4328_Furkan%20Sahin%20Picture.jpeg 792w" alt=""></p><div><p>Furkan Sahin</p><p>Senior Software Engineer</p></div></div><div><p>We’re building an opensource alternative to AWS. Among other things, that means running a ton of VMs,which we do on Linux. We rely on Linux KVM for virtualization, and keep each VM in a separate namespace for isolation.</p><p>In a setup like this, the networking stack has to provide encryption in transit, dynamically assign public IPv4 addresses to VMs, and allow flexible firewall rules. For encryption, we can offload logic to the underlying network card, if the card supports it. This saves CPU cycles on the machine and improves VM performance. For IPv4 assignment and firewall rules, we use Linux’s <a href="https://netfilter.org/">Netfilter / Nftables</a>. This subsystem provides a powerful way to handle packets addressed to the host.</p><p>We started to wonder if we could offload some of the packet processing logic to the network card, similarly to what we can do with encryption, in order to save even more CPU cycles. While investigating, we came across <a href="https://docs.kernel.org/networking/nf_flowtable.html">flowtables</a>—a network acceleration feature in the kernel that works like a routing cache. That is, the kernel remembers the routing decisions for packets that belong to a particular connection.</p><p>When we introduced flowtables into our stack, it reduced network latencies by 7.5%, and all it took was a 7-line change. We thought that was remarkable and worth sharing! So in this post, we describe how we use Netfilter and flowtables for packet processing, and include a simple benchmark that shows flowtables’ benefits.</p></div><div id="sign-up-and-sign-in"><h3>Background</h3><div><p>Ubicloud uses an established pattern in building public cloud services. A control plane manages a data plane, where the data plane usually uses open source software. The control plane holds the data model, responds to web requests, and coordinates changes to the data plane (nodes).</p><p>For example, when the user wants to update firewall rules on a VM, we register this change with the control plane. The control plane then finds the corresponding bare metal instance running the VM and pushes changes to the data plane. Ubicloud’s data plane then reprograms the Linux hosts’ networking rules for the firewall changes to take effect.</p></div><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design.jpg" loading="lazy" sizes="(max-width: 479px) 65vw, (max-width: 767px) 64vw, (max-width: 991px) 65vw, (max-width: 1439px) 50vw, 604.796875px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design-p-500.jpg 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design-p-800.jpg 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design-p-1080.jpg 1080w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design-p-1600.jpg 1600w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design.jpg 2286w" alt=""></p><p>To reprogram the host OS, Ubicloud uses the Netfilter project. Netfilter is a framework inside the Linux kernel that provides hooks for features like packet filtering, connection tracking, and network address translation (NAT). For firewall rules, we use Netfilter’s packet filtering feature. (For assigning IPv4 addresses to VMs, we use the NAT feature.) Let’s look at how firewall rules work in a bit more detail.<br></p></div><div id="sign-up-and-sign-in"><h3>An Example: Implementing Firewall Rules</h3><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e384bbde896d9f2c34c235_Linux%20kernel%20packet%20forwarding%20path.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 92vw, (max-width: 991px) 94vw, (max-width: 1439px) 72vw, 864px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e384bbde896d9f2c34c235_Linux%20kernel%20packet%20forwarding%20path-p-500.png 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e384bbde896d9f2c34c235_Linux%20kernel%20packet%20forwarding%20path-p-800.png 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e384bbde896d9f2c34c235_Linux%20kernel%20packet%20forwarding%20path.png 964w" alt=""></p><p>Classic forwarding path for a packet. Please see Acknowledgements, CC&nbsp;BY-SA&nbsp;4.0 license<br>‍</p><div><p>The above diagram displays the classic packet forwarding path within the Linux kernel, also identifying the Netfilter hooks. To make things more concrete, let’s consider an example where we implement firewall rules in Ubicloud. In this example, on a VM, we want to allow incoming TCP connections on port 5432 for all IP addresses, and reject other traffic.</p><p>The VM has an IPv4 address of 12.12.12.12. A packet then comes to the host with the following packet contents:</p></div><ol role="list"><li>Source address: 11.11.11.11</li><li>Destination address: 12.12.12.12</li><li>Destination port: 5432</li><li>Protocol: TCP</li></ol></div><p>The routing lookup on the host detects &nbsp;that the packet isn’t for itself and needs to be forwarded to the VM. As a result, the packet traverses the Netfilter forward hook. The forward stage then has the following filter (which we configured) that needs to be applied:</p><div><pre><code>
  ip saddr 0.0.0.0/0 tcp dport 5432 ip daddr 12.12.12.12 accept
</code>
</pre></div><div><p>This rule says if the packet is coming from any address, using the TCP protocol, with destination port 5432 and destination IPv4 address 12.12.12.12, accept it. So, the forward hook simply passes the packet to the post-routing hook to be sent to the destination address. If the destination port or any of these fields doesn’t match this rule, Nftables checks for any other rules in the chain. If no other rules exist, Nftables follows the table policy to take the appropriate action.</p><p>What’s interesting here is that a lot of the work is done in the pre-routing, routing, forward, and post-routing stages.</p></div><div id="enter-billing"><h3><strong>Flowtables: Optimizing Network Traffic Handling</strong></h3><p>Flowtables is an optimization to improve network packet throughput. By remembering and reusing the connection based packet processing decisions, flowtables reduces the number of repetitive processing steps for each packet. This further reduces CPU utilization and improves network latency and throughput.</p><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38bc2b0108aa9f3d8ea2c_Linux%20kernel%20forwarding%20with%20flowtables.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 92vw, (max-width: 991px) 94vw, (max-width: 1439px) 72vw, 864px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38bc2b0108aa9f3d8ea2c_Linux%20kernel%20forwarding%20with%20flowtables-p-500.png 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38bc2b0108aa9f3d8ea2c_Linux%20kernel%20forwarding%20with%20flowtables-p-800.png 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38bc2b0108aa9f3d8ea2c_Linux%20kernel%20forwarding%20with%20flowtables.png 964w" alt=""></p><p>Adding flowtables into a packet's forwarding path. Please see Acknowledgements, CC&nbsp;BY-SA&nbsp;4.0 license<br>‍<br></p></div><p>The previous diagram shows how Netfilter / Nftables work when flowtables are applied. Further, since flowtables integrate nicely with Netfilter, enabling them is straightforward. In Ubicloud’s case, enabling flowtables just took <a href="https://github.com/ubicloud/ubicloud/pull/1009">seven lines of code!</a></p><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 92vw, (max-width: 991px) 94vw, (max-width: 1439px) 72vw, 864px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code-p-500.png 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code-p-800.png 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code-p-1080.png 1080w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code-p-1600.png 1600w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code-p-2000.png 2000w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code.png 2200w" alt=""></p><div id="enter-billing"><h3><strong>Simple Latency Benchmarks with PostgreSQL</strong></h3><div><p>We designed a simple benchmark to measure our networking stack’s latency at the host level. For this, we created a <a href="https://www.ubicloud.com/docs/managed-postgresql/quickstart">Ubicloud PostgreSQL</a> instance and installed pgbench on the host machine. pgbench is a simple benchmarking tool provided by PostgreSQL; it’s a nice fit for simple benchmarking because we can tweak pgbench’s parameters to focus on the networking overhead.</p><p>We first initialized pgbench using its initialization option (-i) and then ran pgbench for benchmarking:</p></div><div><pre><code>
  pgbench -i -s 100 demo-pg.postgresql.ubicloud.com

  pgbench -c 1 -j 1 -T 60 -P 1 -S demo-pg.postgresql.ubicloud.com
</code>
</pre></div><div><p>By keeping the client and thread counts at one, we could better isolate flowtable optimization’s impact. Additionally, we ran pgbench directly from the host against Ubicloud’s managed PostgreSQL. This way, we could remove any variance associated with taking an actual network hope; and only measure the end-to-end latency for one pgbench SELECT query.</p><p>Our observations were clear and consistent:</p></div><ul role="list"><li>Without flowtables (our original Netfilter / Nftables implementation), the average latency was 0.127ms.<br></li><li>With flowtables, the average latency decreased to 0.118ms. This showed a latency improvement of 7.5%.</li><li>There are two ways to think about these improvements. First, we didn’t take network latency into account in this benchmark. So, we’d expect real life latency benefits to be lower.</li><li>Second, and on the flip side, most of the end-to-end latency was associated with the pgbench client sending a query to PostgreSQL and receiving the reply. We didn’t work to measure this latency. Intuitively, we’d expect flowtable’s throughput benefits (shaved off CPU cycles) to be more important than its latency benefits.</li></ul></div><div id="create-vm"><h3>Conclusion</h3><div><p>We use Netfilter / Nftables on our data plane bare metal instances to provide cloud networking services. These Linux kernel features are reliable and portable. Recently, we introduced flowtables into our networking setup at Ubicloud. The change took seven lines of code and improved latency by 7.5% in a simple application benchmark.</p><p>As we work on Ubicloud, we’re actively learning more about and making improvements to our networking layer. If you have any questions or feedback for us, we’d love to hear from you. Please feel free to drop us a line at <a href="#">info@ubicloud.com</a>.</p></div></div><div id="create-vm"><h3>Acknowledgements</h3><p>As we worked to introduce flowtables into Ubicloud’s networking stack, a significant portion of our understanding came from the blog post, <a href="https://thermalcircle.de/doku.php?id=blog:linux:flowtables_1_a_netfilter_nftables_fastpath">"Flowtables: A Netfilter nftables Fastpath"</a>. Andrej's post provides an in-depth look into flowtables and their benefits. We’d like to thank its author Andrej Stender for that comprehensive work!</p></div></div></div>]]></description>
        </item>
    </channel>
</rss>