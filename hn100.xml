<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 06 May 2025 03:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Replacing Kubernetes with systemd (2024) (172 pts)]]></title>
            <link>https://blog.yaakov.online/replacing-kubernetes-with-systemd/</link>
            <guid>43899236</guid>
            <pubDate>Mon, 05 May 2025 20:40:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.yaakov.online/replacing-kubernetes-with-systemd/">https://blog.yaakov.online/replacing-kubernetes-with-systemd/</a>, See on <a href="https://news.ycombinator.com/item?id=43899236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                            <p>Yes, I'm fully aware those are two separate things, but hear me out here for a moment.</p><p>Back in 2018 I was hearing a lot of stuff from all angles and all sorts of friends and influences about Kubernetes, and from what I heard it seemed like a pretty promising piece of kit to use. At the time, I actually went out and bought a NUC to act as a little hypervisor so that I could play around with a small cluster at home.</p><p>Funnily enough, my blog post on this was <a href="https://blog.yaakov.online/learning-kubernetes-at-home/" rel="noreferrer">six years ago to the very day</a>.</p><p>The main lesson that I learned is that although Kubernetes is made up of all sorts of pieces and web services and sidecars and webhooks, basically acts as a giant <code>while</code> loop as follows:</p><pre><code>while (true)
{
    var current_state = GetCurrentState();
    var desired_state = GetDesiredState();
    var diff = CalculateDiff(current_state, desired_state);
    ApplyDiff(diff);
}</code></pre><p>If I said there should be a Pod here, and there wasn't, Kubernetes would create it. If I said there should be 3 replicas and there were 4, Kubernetes would get rid of one.</p><p>This actually extended out in really cool ways, such as with <a href="https://cert-manager.io/?ref=blog.yaakov.online" rel="noreferrer">cert-manager</a>. If I said there should be a valid TLS certificate for some domain, and told Kubernetes how it could request one, then if the certificate was missing or expiring, Kubernetes would go out and get a new certificate and install it in the web server automagically.</p><p>But as the memes go, what I was using Kubernetes for was fun to experiment with, but total overkill.</p><figure><img src="https://blog.yaakov.online/content/images/2024/02/C-NknkeUwAAxSQs.jpeg" alt="A few small pieces of wood tied down to a large flatbed truck." loading="lazy" width="2000" height="1984" srcset="https://blog.yaakov.online/content/images/size/w600/2024/02/C-NknkeUwAAxSQs.jpeg 600w, https://blog.yaakov.online/content/images/size/w1000/2024/02/C-NknkeUwAAxSQs.jpeg 1000w, https://blog.yaakov.online/content/images/size/w1600/2024/02/C-NknkeUwAAxSQs.jpeg 1600w, https://blog.yaakov.online/content/images/2024/02/C-NknkeUwAAxSQs.jpeg 2048w" sizes="(min-width: 720px) 720px"><figcaption><span>"Deployed my blog on Kubernetes." - @dexhorthy, </span><a href="https://twitter.com/dexhorthy/status/856639005462417409?lang=en&amp;ref=blog.yaakov.online"><span>https://twitter.com/dexhorthy/status/856639005462417409?lang=en</span></a></figcaption></figure><p>Whilst most problems I encountered did provide a legitimate learning experience, it turns out that Kubernetes, particularly on a NUC, is not bedroom-friendly. Kubernetes chews through a lot of resources, and <code>while (true)</code> loops tend to chew through a lot of CPU. This made my computers run constantly, run hot, keep the fan running, and made it hotter and noisier and harder to sleep.</p><p>Even in the cloud, this effect gets felt in different ways. My personal experience on Azure Kubernetes Service was that I immediately lose a massive chunk of RAM to their Kubernetes implementation, and it uses about 7-10% idle CPU on worker nodes. Even with single-instance <a href="https://microk8s.io/?ref=blog.yaakov.online" rel="noreferrer">Microk8s</a> on a small VPS I had an idle CPU load hovering around 12% on a 2x vCPU x86_64 box, and <a href="http://k3s.io/?ref=blog.yaakov.online" rel="noreferrer">K3S</a> which is supposed to be leaner is at about 6% constant CPU consumption on a 2x vCPU Ampere A1 machine.</p><p>(No guesses as to which cloud provider that latter one is running on.)</p><p>I even tried running Kubernetes on a Raspberry Pi but I couldn't actually find an implementation that would happily run without kicking up heat/fans and that would actually leave enough CPU behind for my workloads.</p><p>Yet the thing that kept bringing me back was the automation. Particularly with GitOps and <a href="https://www.weave.works/oss/flux/?ref=blog.yaakov.online" rel="noreferrer">Flux</a>, making changes was a breeze. With the container image automation and recent addition of <a href="https://fluxcd.io/flux/guides/webhook-receivers/?ref=blog.yaakov.online" rel="noreferrer">webhooks in Flux v2</a>, all I had to do was push a new container image and within seconds my servers had pulled the new container image and were running the new version of the application in production.</p><p>Every so often I would stick my head back out of Kubernetes and look at the rest of the world and see if there is something else that can do the same container automation and just like Noah's dove, I would come back empty-handed.</p><p>The only solutions I could find were "just recreate the whole container with all of the original command line arguments" as though I have the patience to manage that or remember each flag, or "here is some magic goop that works if you give it full control of <code>docker.sock</code>" which I never really liked the idea of.</p><p>I have even been sorely tempted to build my own thing but surely, <em>surely</em>, there is something out there already that does this, right?</p><p>Well, recently, I came across <a href="https://docs.podman.io/en/latest/markdown/podman-auto-update.1.html?ref=blog.yaakov.online" rel="noreferrer">Podman auto-updating</a>. The simplest way to explain <a href="https://podman.io/?ref=blog.yaakov.online" rel="noreferrer">Podman</a> is an alternative Docker CLI (yes I know that is oversimplified), but it has one particular feature that caught my eye.</p><p>Once you create a container, Podman can automatically <a href="https://docs.podman.io/en/latest/markdown/podman-generate-systemd.1.html?ref=blog.yaakov.online" rel="noreferrer">generate a systemd service</a> file to start and stop that container. Starting the service creates (or replaces) the container, and stopping the service removes the container. So that already takes care of my "manage each original flag" problem.</p><p>But the cherry on top if that if you tag your containers with <code>io.containers.autoupdate</code>, then once a day on a timer or on-demand when you <code>podman auto-update</code>, it will check for a new image and if there is one it will recreate the container for you with the new image.</p><p><a href="https://fedoramagazine.org/auto-updating-podman-containers-with-systemd/?ref=blog.yaakov.online" rel="noreferrer">This article from Fedora Magazine</a> basically gave me 99% of the magic sauce. There were only two more components I needed to make this work:</p><ol><li>Run <code>systemctl --user enable mycontainer.service</code> to make the container start up automatically, whenever I log in.</li><li>Run <code>loginctl enable-linger</code> so that I "log in" when the server starts up.</li></ol><p>These three components - Podman, systemd, and user lingering, now give me 99% of the benefit I was getting from Kubernetes with vastly reduced complexity and none of the CPU/memory hits associated with it.</p><p>I've migrated a full set of services from one VPS to a new one with half the vCPUs and RAM. It's only been running for a handful of hours so far, but I can see that it is running significantly lighter, snappier, and with a lower compute cost to boot, which gives me higher service density and more bang for my buck.</p><p>Of course, as my luck would have it, Podman integration with systemd appears to be deprecated already and they're now talking about defining containers in "Quadlet" files, whatever those are. I guess that will be something to learn some other time.</p>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Real-time AI Voice Chat at ~500ms Latency (247 pts)]]></title>
            <link>https://github.com/KoljaB/RealtimeVoiceChat</link>
            <guid>43899028</guid>
            <pubDate>Mon, 05 May 2025 20:17:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/KoljaB/RealtimeVoiceChat">https://github.com/KoljaB/RealtimeVoiceChat</a>, See on <a href="https://news.ycombinator.com/item?id=43899028">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Real-Time AI Voice Chat 🎤💬🧠🔊</h2><a id="user-content-real-time-ai-voice-chat-" aria-label="Permalink: Real-Time AI Voice Chat 🎤💬🧠🔊" href="#real-time-ai-voice-chat-"></a></p>
<p dir="auto"><strong>Have a natural, spoken conversation with an AI!</strong></p>
<p dir="auto">This project lets you chat with a Large Language Model (LLM) using just your voice, receiving spoken responses in near real-time. Think of it as your own digital conversation partner.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description FastVoiceTalk_compressed_step3_h264.mp4">FastVoiceTalk_compressed_step3_h264.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/7604638/440153612-16cc29a7-bec2-4dd0-a056-d213db798d8f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY0ODQ1MDIsIm5iZiI6MTc0NjQ4NDIwMiwicGF0aCI6Ii83NjA0NjM4LzQ0MDE1MzYxMi0xNmNjMjlhNy1iZWMyLTRkZDAtYTA1Ni1kMjEzZGI3OThkOGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDVUMjIzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTY0YWVlN2NiNDc3ZjZkMDMzZGJlY2QyNGNlZTQzNDM0M2I1ZjJmM2EwOTgxYzVmMzZlYjU2MjdhNzIzNTRlNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.dbPAgkPJn2NKy48qo_JdokryT9DGts4KHQ767eBEWGs" data-canonical-src="https://private-user-images.githubusercontent.com/7604638/440153612-16cc29a7-bec2-4dd0-a056-d213db798d8f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY0ODQ1MDIsIm5iZiI6MTc0NjQ4NDIwMiwicGF0aCI6Ii83NjA0NjM4LzQ0MDE1MzYxMi0xNmNjMjlhNy1iZWMyLTRkZDAtYTA1Ni1kMjEzZGI3OThkOGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDVUMjIzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTY0YWVlN2NiNDc3ZjZkMDMzZGJlY2QyNGNlZTQzNDM0M2I1ZjJmM2EwOTgxYzVmMzZlYjU2MjdhNzIzNTRlNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.dbPAgkPJn2NKy48qo_JdokryT9DGts4KHQ767eBEWGs" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><em>(early preview - first reasonably stable version)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's Under the Hood?</h2><a id="user-content-whats-under-the-hood" aria-label="Permalink: What's Under the Hood?" href="#whats-under-the-hood"></a></p>
<p dir="auto">A sophisticated client-server system built for low-latency interaction:</p>
<ol dir="auto">
<li>🎙️ <strong>Capture:</strong> Your voice is captured by your browser.</li>
<li>➡️ <strong>Stream:</strong> Audio chunks are whisked away via WebSockets to a Python backend.</li>
<li>✍️ <strong>Transcribe:</strong> <code>RealtimeSTT</code> rapidly converts your speech to text.</li>
<li>🤔 <strong>Think:</strong> The text is sent to an LLM (like Ollama or OpenAI) for processing.</li>
<li>🗣️ <strong>Synthesize:</strong> The AI's text response is turned back into speech using <code>RealtimeTTS</code>.</li>
<li>⬅️ <strong>Return:</strong> The generated audio is streamed back to your browser for playback.</li>
<li>🔄 <strong>Interrupt:</strong> Jump in anytime! The system handles interruptions gracefully.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Features ✨</h2><a id="user-content-key-features-" aria-label="Permalink: Key Features ✨" href="#key-features-"></a></p>
<ul dir="auto">
<li><strong>Fluid Conversation:</strong> Speak and listen, just like a real chat.</li>
<li><strong>Real-Time Feedback:</strong> See partial transcriptions and AI responses as they happen.</li>
<li><strong>Low Latency Focus:</strong> Optimized architecture using audio chunk streaming.</li>
<li><strong>Smart Turn-Taking:</strong> Dynamic silence detection (<code>turndetect.py</code>) adapts to the conversation pace.</li>
<li><strong>Flexible AI Brains:</strong> Pluggable LLM backends (Ollama default, OpenAI support via <code>llm_module.py</code>).</li>
<li><strong>Customizable Voices:</strong> Choose from different Text-to-Speech engines (Kokoro, Coqui, Orpheus via <code>audio_module.py</code>).</li>
<li><strong>Web Interface:</strong> Clean and simple UI using Vanilla JS and the Web Audio API.</li>
<li><strong>Dockerized Deployment:</strong> Recommended setup using Docker Compose for easier dependency management.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technology Stack 🛠️</h2><a id="user-content-technology-stack-️" aria-label="Permalink: Technology Stack 🛠️" href="#technology-stack-️"></a></p>
<ul dir="auto">
<li><strong>Backend:</strong> Python 3.x, FastAPI</li>
<li><strong>Frontend:</strong> HTML, CSS, JavaScript (Vanilla JS, Web Audio API, AudioWorklets)</li>
<li><strong>Communication:</strong> WebSockets</li>
<li><strong>Containerization:</strong> Docker, Docker Compose</li>
<li><strong>Core AI/ML Libraries:</strong>
<ul dir="auto">
<li><code>RealtimeSTT</code> (Speech-to-Text)</li>
<li><code>RealtimeTTS</code> (Text-to-Speech)</li>
<li><code>transformers</code> (Turn detection, Tokenization)</li>
<li><code>torch</code> / <code>torchaudio</code> (ML Framework)</li>
<li><code>ollama</code> / <code>openai</code> (LLM Clients)</li>
</ul>
</li>
<li><strong>Audio Processing:</strong> <code>numpy</code>, <code>scipy</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Before You Dive In: Prerequisites 🏊‍♀️</h2><a id="user-content-before-you-dive-in-prerequisites-️" aria-label="Permalink: Before You Dive In: Prerequisites 🏊‍♀️" href="#before-you-dive-in-prerequisites-️"></a></p>
<p dir="auto">This project leverages powerful AI models, which have some requirements:</p>
<ul dir="auto">
<li><strong>Operating System:</strong>
<ul dir="auto">
<li><strong>Docker:</strong> Linux is recommended for the best GPU integration with Docker.</li>
<li><strong>Manual:</strong> The provided script (<code>install.bat</code>) is for Windows. Manual steps are possible on Linux/macOS but may require more troubleshooting (especially for DeepSpeed).</li>
</ul>
</li>
<li><strong>🐍 Python:</strong> 3.9 or higher (if setting up manually).</li>
<li><strong>🚀 GPU:</strong> <strong>A powerful CUDA-enabled NVIDIA GPU is <em>highly recommended</em></strong>, especially for faster STT (Whisper) and TTS (Coqui). Performance on CPU-only or weaker GPUs will be significantly slower.
<ul dir="auto">
<li>The setup assumes <strong>CUDA 12.1</strong>. Adjust PyTorch installation if you have a different CUDA version.</li>
<li><strong>Docker (Linux):</strong> Requires <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" rel="nofollow">NVIDIA Container Toolkit</a>.</li>
</ul>
</li>
<li><strong>🐳 Docker (Optional but Recommended):</strong> Docker Engine and Docker Compose v2+ for the containerized setup.</li>
<li><strong>🧠 Ollama (Optional):</strong> If using the Ollama backend <em>without</em> Docker, install it separately and pull your desired models. The Docker setup includes an Ollama service.</li>
<li><strong>🔑 OpenAI API Key (Optional):</strong> If using the OpenAI backend, set the <code>OPENAI_API_KEY</code> environment variable (e.g., in a <code>.env</code> file or passed to Docker).</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started: Installation &amp; Setup ⚙️</h2><a id="user-content-getting-started-installation--setup-️" aria-label="Permalink: Getting Started: Installation &amp; Setup ⚙️" href="#getting-started-installation--setup-️"></a></p>
<p dir="auto"><strong>Clone the repository first:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/KoljaB/RealtimeVoiceChat.git
cd RealtimeVoiceChat"><pre>git clone https://github.com/KoljaB/RealtimeVoiceChat.git
<span>cd</span> RealtimeVoiceChat</pre></div>
<p dir="auto">Now, choose your adventure:</p>
<details>
<summary><strong>🚀 Option A: Docker Installation (Recommended for Linux/GPU)</strong></summary>
<p dir="auto">This is the most straightforward method, bundling the application, dependencies, and even Ollama into manageable containers.</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Build the Docker images:</strong>
<em>(This takes time! It downloads base images, installs Python/ML dependencies, and pre-downloads the default STT model.)</em></p>

<p dir="auto"><em>(If you want to customize models/settings in <code>code/*.py</code>, do it <strong>before</strong> this step!)</em></p>
</li>
<li>
<p dir="auto"><strong>Start the services (App &amp; Ollama):</strong>
<em>(Runs containers in the background. GPU access is configured in <code>docker-compose.yml</code>.)</em></p>

<p dir="auto">Give them a minute to initialize.</p>
</li>
<li>
<p dir="auto"><strong>(Crucial!) Pull your desired Ollama Model:</strong>
<em>(This is done <em>after</em> startup to keep the main app image smaller and allow model changes without rebuilding. Execute this command to pull the default model into the running Ollama container.)</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Pull the default model (adjust if you configured a different one in server.py)
docker compose exec ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

# (Optional) Verify the model is available
docker compose exec ollama ollama list"><pre><span><span>#</span> Pull the default model (adjust if you configured a different one in server.py)</span>
docker compose <span>exec</span> ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

<span><span>#</span> (Optional) Verify the model is available</span>
docker compose <span>exec</span> ollama ollama list</pre></div>
</li>
<li>
<p dir="auto"><strong>Stopping the Services:</strong></p>

</li>
<li>
<p dir="auto"><strong>Restarting:</strong></p>

</li>
<li>
<p dir="auto"><strong>Viewing Logs / Debugging:</strong></p>
<ul dir="auto">
<li>Follow app logs: <code>docker compose logs -f app</code></li>
<li>Follow Ollama logs: <code>docker compose logs -f ollama</code></li>
<li>Save logs to file: <code>docker compose logs app &gt; app_logs.txt</code></li>
</ul>
</li>
</ol>
</details>
<details>
<summary><strong>🛠️ Option B: Manual Installation (Windows Script / venv)</strong></summary>
<p dir="auto">This method requires managing the Python environment yourself. It offers more direct control but can be trickier, especially regarding ML dependencies.</p>
<p dir="auto"><strong>B1) Using the Windows Install Script:</strong></p>
<ol dir="auto">
<li>Ensure you meet the prerequisites (Python, potentially CUDA drivers).</li>
<li>Run the script. It attempts to create a venv, install PyTorch for CUDA 12.1, a compatible DeepSpeed wheel, and other requirements.

<em>(This opens a new command prompt within the activated virtual environment.)</em>
Proceed to the <strong>"Running the Application"</strong> section.</li>
</ol>
<p dir="auto"><strong>B2) Manual Steps (Linux/macOS/Windows):</strong></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Create &amp; Activate Virtual Environment:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m venv venv
# Linux/macOS:
source venv/bin/activate
# Windows:
.\venv\Scripts\activate"><pre>python -m venv venv
<span><span>#</span> Linux/macOS:</span>
<span>source</span> venv/bin/activate
<span><span>#</span> Windows:</span>
.<span>\v</span>env<span>\S</span>cripts<span>\a</span>ctivate</pre></div>
</li>
<li>
<p dir="auto"><strong>Upgrade Pip:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m pip install --upgrade pip"><pre>python -m pip install --upgrade pip</pre></div>
</li>
<li>
<p dir="auto"><strong>Navigate to Code Directory:</strong></p>

</li>
<li>
<p dir="auto"><strong>Install PyTorch (Crucial Step - Match Your Hardware!):</strong></p>
<ul dir="auto">
<li><strong>With NVIDIA GPU (CUDA 12.1 Example):</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# Verify your CUDA version! Adjust 'cu121' and the URL if needed.
pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121"><pre><span><span>#</span> Verify your CUDA version! Adjust 'cu121' and the URL if needed.</span>
pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121</pre></div>
</li>
<li><strong>CPU Only (Expect Slow Performance):</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# pip install torch torchaudio torchvision"><pre><span><span>#</span> pip install torch torchaudio torchvision</span></pre></div>
</li>
<li><em>Find other PyTorch versions:</em> <a href="https://pytorch.org/get-started/previous-versions/" rel="nofollow">https://pytorch.org/get-started/previous-versions/</a></li>
</ul>
</li>
<li>
<p dir="auto"><strong>Install Other Requirements:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<ul dir="auto">
<li><strong>Note on DeepSpeed:</strong> The <code>requirements.txt</code> may include DeepSpeed. Installation can be complex, especially on Windows. The <code>install.bat</code> tries a precompiled wheel. If manual installation fails, you might need to build it from source or consult resources like <a href="https://github.com/erew123/deepspeedpatcher">deepspeedpatcher</a> (use at your own risk). Coqui TTS performance benefits most from DeepSpeed.</li>
</ul>
</li>
</ol>
</details>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running the Application <g-emoji alias="arrow_forward">▶️</g-emoji></h2><a id="user-content-running-the-application-️" aria-label="Permalink: Running the Application ▶️" href="#running-the-application-️"></a></p>
<p dir="auto"><strong>If using Docker:</strong>
Your application is already running via <code>docker compose up -d</code>! Check logs using <code>docker compose logs -f app</code>.</p>
<p dir="auto"><strong>If using Manual/Script Installation:</strong></p>
<ol dir="auto">
<li><strong>Activate your virtual environment</strong> (if not already active):
<div dir="auto" data-snippet-clipboard-copy-content="# Linux/macOS: source ../venv/bin/activate
# Windows: ..\venv\Scripts\activate"><pre><span><span>#</span> Linux/macOS: source ../venv/bin/activate</span>
<span><span>#</span> Windows: ..\venv\Scripts\activate</span></pre></div>
</li>
<li><strong>Navigate to the <code>code</code> directory</strong> (if not already there):

</li>
<li><strong>Start the FastAPI server:</strong>

</li>
</ol>
<p dir="auto"><strong>Accessing the Client (Both Methods):</strong></p>
<ol dir="auto">
<li>Open your web browser to <code>http://localhost:8000</code> (or your server's IP if running remotely/in Docker on another machine).</li>
<li><strong>Grant microphone permissions</strong> when prompted.</li>
<li>Click <strong>"Start"</strong> to begin chatting! Use "Stop" to end and "Reset" to clear the conversation.</li>
</ol>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration Deep Dive 🔧</h2><a id="user-content-configuration-deep-dive-" aria-label="Permalink: Configuration Deep Dive 🔧" href="#configuration-deep-dive-"></a></p>
<p dir="auto">Want to tweak the AI's voice, brain, or how it listens? Modify the Python files in the <code>code/</code> directory.</p>
<p dir="auto"><strong><g-emoji alias="warning">⚠️</g-emoji> Important Docker Note:</strong> If using Docker, make any configuration changes <em>before</em> running <code>docker compose build</code> to ensure they are included in the image.</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>TTS Engine &amp; Voice (<code>server.py</code>, <code>audio_module.py</code>):</strong></p>
<ul dir="auto">
<li>Change <code>START_ENGINE</code> in <code>server.py</code> to <code>"coqui"</code>, <code>"kokoro"</code>, or <code>"orpheus"</code>.</li>
<li>Adjust engine-specific settings (e.g., voice model path for Coqui, speaker ID for Orpheus, speed) within <code>AudioProcessor.__init__</code> in <code>audio_module.py</code>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>LLM Backend &amp; Model (<code>server.py</code>, <code>llm_module.py</code>):</strong></p>
<ul dir="auto">
<li>Set <code>LLM_START_PROVIDER</code> (<code>"ollama"</code> or <code>"openai"</code>) and <code>LLM_START_MODEL</code> (e.g., <code>"hf.co/..."</code> for Ollama, model name for OpenAI) in <code>server.py</code>. Remember to pull the Ollama model if using Docker (see Installation Step A3).</li>
<li>Customize the AI's personality by editing <code>system_prompt.txt</code>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>STT Settings (<code>transcribe.py</code>):</strong></p>
<ul dir="auto">
<li>Modify <code>DEFAULT_RECORDER_CONFIG</code> to change the Whisper model (<code>model</code>), language (<code>language</code>), silence thresholds (<code>silence_limit_seconds</code>), etc. The default <code>base.en</code> model is pre-downloaded during the Docker build.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Turn Detection Sensitivity (<code>turndetect.py</code>):</strong></p>
<ul dir="auto">
<li>Adjust pause duration constants within the <code>TurnDetector.update_settings</code> method.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>SSL/HTTPS (<code>server.py</code>):</strong></p>
<ul dir="auto">
<li>Set <code>USE_SSL = True</code> and provide paths to your certificate (<code>SSL_CERT_PATH</code>) and key (<code>SSL_KEY_PATH</code>) files.</li>
<li><strong>Docker Users:</strong> You'll need to adjust <code>docker-compose.yml</code> to map the SSL port (e.g., 443) and potentially mount your certificate files as volumes.</li>
</ul>
<details>
<summary><strong>Generating Local SSL Certificates (Windows Example w/ mkcert)</strong></summary>
<ol dir="auto">
<li>Install Chocolatey package manager if you haven't already.</li>
<li>Install mkcert: <code>choco install mkcert</code></li>
<li>Run Command Prompt <em>as Administrator</em>.</li>
<li>Install a local Certificate Authority: <code>mkcert -install</code></li>
<li>Generate certs (replace <code>your.local.ip</code>): <code>mkcert localhost 127.0.0.1 ::1 your.local.ip</code>
<ul dir="auto">
<li>This creates <code>.pem</code> files (e.g., <code>localhost+3.pem</code> and <code>localhost+3-key.pem</code>) in the current directory. Update <code>SSL_CERT_PATH</code> and <code>SSL_KEY_PATH</code> in <code>server.py</code> accordingly. Remember to potentially mount these into your Docker container.</li>
</ul>
</li>
</ol>
</details>
</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing 🤝</h2><a id="user-content-contributing-" aria-label="Permalink: Contributing 🤝" href="#contributing-"></a></p>
<p dir="auto">Got ideas or found a bug? Contributions are welcome! Feel free to open issues or submit pull requests.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License 📜</h2><a id="user-content-license-" aria-label="Permalink: License 📜" href="#license-"></a></p>
<p dir="auto">The core codebase of this project is released under the <strong>MIT License</strong> (see the <a href="https://github.com/KoljaB/RealtimeVoiceChat/blob/main/LICENSE">LICENSE</a> file for details).</p>
<p dir="auto">This project relies on external specific TTS engines (like <code>Coqui XTTSv2</code>) and LLM providers which have their <strong>own licensing terms</strong>. Please ensure you comply with the licenses of all components you use.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Databricks in talks to acquire startup Neon for about $1B (115 pts)]]></title>
            <link>https://www.upstartsmedia.com/p/scoop-databricks-talks-to-acquire-neon</link>
            <guid>43899016</guid>
            <pubDate>Mon, 05 May 2025 20:16:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.upstartsmedia.com/p/scoop-databricks-talks-to-acquire-neon">https://www.upstartsmedia.com/p/scoop-databricks-talks-to-acquire-neon</a>, See on <a href="https://news.ycombinator.com/item?id=43899016">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg" width="1456" height="972" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:972,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:5707007,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.upstartsmedia.com/i/162915455?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Databricks is busy buying up a bunch of startups to join its San Francisco office. CREDIT: Smith Collection/Gado/Getty Images</figcaption></figure></div><p>Data and AI unicorn Databricks is in talks to make a splash with another startup acquisition, Upstarts has learned.</p><p>Databricks is in advanced talks to acquire startup Neon, a maker of an open-source database engine, four sources tell Upstarts exclusively. Databricks is expected to pay in the ballpark of $1 billion for Neon, two of the sources say.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.upstartsmedia.com/subscribe?coupon=e8236a37&amp;utm_content=162915455&quot;,&quot;text&quot;:&quot;Get 10% off for 1 year&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.upstartsmedia.com/subscribe?coupon=e8236a37&amp;utm_content=162915455" rel=""><span>Get 10% off for 1 year</span></a></p><p>But despite some industry insiders describing the deal as if it’s done, the talks remain ongoing and could still fall through, several sources add. The total amount could also still exceed $1 billion when employee retention packages are factored in.</p><p>Neon and CEO Nikita Shamgunov did not respond to a comment request. Databricks declined to comment through a spokesperson.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Possibly a Serious Possibility (179 pts)]]></title>
            <link>https://kucharski.substack.com/p/possibly-a-serious-possibility</link>
            <guid>43898380</guid>
            <pubDate>Mon, 05 May 2025 19:11:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kucharski.substack.com/p/possibly-a-serious-possibility">https://kucharski.substack.com/p/possibly-a-serious-possibility</a>, See on <a href="https://news.ycombinator.com/item?id=43898380">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Sherman Kent was rattled. It was March 1951 and Kent, a CIA analyst, had found himself in a troubling conversation about some recent intelligence. A few days earlier, Kent's team had </span><a href="https://www.cia.gov/resources/csi/studies-in-intelligence/archives/vol-8-no-4/words-of-estimative-probability/" rel="">released a report</a><span> titled ‘Probability of an Invasion of Yugoslavia in 1951’, which concluded that Soviet aggression against Yugoslavia ‘should be considered a serious possibility’.</span></p><p>Kent thought the phrase was clear. But when he ran into the chairman of the Policy Planning Staff, he realised his message hadn’t landed as intended.</p><p>‘By the way, what did you mean by “serious possibility”?’ the chairman asked.</p><p>‘I told him that my personal estimate was on the dark side, namely that the odds were around 65 to 35 in favor of an attack’, Kent would later recall.</p><p>The chairman was taken aback. He—and others—had read the phrase as meaning something much less likely.</p><p>Things got worse when Kent talked to his colleagues on the Board of National Estimates. Some interpreted the phrase as meaning a probability as high as 80%, others as low as 20%. This was the 29th such report that they’d produced. ‘Had Board members been seeming to agree on five months' worth of estimative judgments with no real agreement at all?’ Kent wondered.</p><p>The latest report, intended to sound an alarm, had instead created confusion.</p><p>This wasn’t just a one-off problem. It was a structural flaw in how intelligence was being communicated at the time. Reflecting on the experience, Kent categorised intelligence assessments into three categories:</p><ol><li><p><strong>Near-indisputable facts</strong><span>, like the length of a runway visible in a satellite image.</span></p></li><li><p><strong>Judgement or estimate of something knowable</strong><span>, like whether the runway belongs to a military airfield.</span></p></li><li><p><strong>Judgement or estimate of something unknowable</strong><span>, like whether the airfield will be expanded into a strategic base. Even the adversary may not yet know what they’re going to do with it.</span></p></li></ol><p>Kent noted that most intelligence work tends to sit in the second and third categories, where uncertainty dominates. But Kent realised that even among professionals, the language of such uncertainty was wildly inconsistent. Photo interpreters would use ‘possible’ where he would use ‘probable’. And they used ‘probable’ where Kent would say ‘almost certain’. </p><p><span>Decades later, a </span><a href="https://digitalcommons.usf.edu/jss/vol10/iss1/11/" rel="">NATO study</a><span> would find something similar: ask 23 officers what ‘likely’ means in terms of probability, and you’ll get a dozen different numbers. Modern online surveys show people haven’t improved much, as shown in the below data:</span></p><p>Kent suggested that the toughest intelligence challenge is the ‘difficult but not impossible estimate’. Unfortunately, this was also the kind of judgment people tried hardest to avoid. As Kent put it: ‘What we consciously or subconsciously seek is an expression which conveys a definite meaning but at the same time either absolves us completely of the responsibility or makes the estimate at enough removes from ourselves as not to implicate us’.</p><p><span>The problem reaches beyond the intelligence world. Legal scholars </span><a href="https://scholarship.richmond.edu/law-faculty-publications/193/" rel="">have found</a><span> that courts also rely on ambiguous phrases for ambiguous evidence, with everything-to-everyone phrases like ‘non-whimsical suspicion’ or ‘clear indication’. To Kent, these kinds of ‘lurking weasel’ phrases were simply a way to avoid decisions: language that sounds authoritative but dodges responsibility. ‘Let the judgment be unmistakable and let it be unmistakably ours,’ as Kent put it.</span></p><p><span>Since then, some governments have tried to clean up the language of probability. After the Iraq War—which was influenced by misinterpreted intelligence—the </span><a href="https://osf.io/preprints/psyarxiv/kuyhb_v1" rel="">UK introduced</a><span> a ‘probability yardstick’ for intelligence assessments, standardizing terms like ‘unlikely’, ‘highly likely’, and ‘probable’ with numerical definitions:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png" width="1456" height="325" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d118724-fddf-4308-9998-d603b5989a96_1480x330.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:325,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Such standarisation matters, because in a world where uncertainty is unavoidable, the risk isn’t just that we’re wrong—it’s that we’re misunderstood.</p><p><em><strong><span>If you’re interested in reading more about analysing and communicating uncertainty, you’ll probably like my new book </span><a href="https://proof.kucharski.io/" rel="">Proof: The Uncertain Science of Certainty</a><span>, which is available now. It was </span><a href="https://www.nytimes.com/2025/04/30/books/review/proof-adam-kucharski.html" rel="">reviewed in The New York Times</a><span> this week if you’d like to know more.</span></strong></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Evolving OpenAI's Structure (430 pts)]]></title>
            <link>https://openai.com/index/evolving-our-structure/</link>
            <guid>43897772</guid>
            <pubDate>Mon, 05 May 2025 18:08:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/evolving-our-structure/">https://openai.com/index/evolving-our-structure/</a>, See on <a href="https://news.ycombinator.com/item?id=43897772">Hacker News</a></p>
Couldn't get https://openai.com/index/evolving-our-structure/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[As an experienced LLM user, I don't use generative LLMs often (279 pts)]]></title>
            <link>https://minimaxir.com/2025/05/llm-use/</link>
            <guid>43897320</guid>
            <pubDate>Mon, 05 May 2025 17:22:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minimaxir.com/2025/05/llm-use/">https://minimaxir.com/2025/05/llm-use/</a>, See on <a href="https://news.ycombinator.com/item?id=43897320">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Lately, I’ve been working on codifying a personal ethics statement about my stances on generative AI as I have been very critical about <a href="https://minimaxir.com/2023/10/ai-sturgeons-law/">several</a> <a href="https://minimaxir.com/2024/08/ai-seinfeld/">aspects</a> of modern GenAI, and yet <a href="https://thenib.com/mister-gotcha/">I participate in it</a>. While working on that statement, I’ve been introspecting on how I myself have been utilizing large language models for both my professional work as a Senior Data Scientist at <a href="https://www.buzzfeed.com/">BuzzFeed</a> and for my personal work blogging and <a href="https://github.com/minimaxir">writing open-source software</a>. For about a decade, I’ve been researching and developing tooling around <a href="https://minimaxir.com/2017/04/char-embeddings/">text generation from char-rnns</a>, to the <a href="https://minimaxir.com/2019/09/howto-gpt2/">ability to fine-tune GPT-2</a>, to <a href="https://minimaxir.com/2020/07/gpt3-expectations/">experiments with GPT-3</a>, and <a href="https://minimaxir.com/2023/03/new-chatgpt-overlord/">even more experiments with ChatGPT</a> and other LLM APIs. Although I don’t claim to the best user of modern LLMs out there, I’ve had plenty of experience working against the cons of next-token predictor models and have become very good at finding the pros.</p><p>It turns out, to my surprise, that I don’t use them nearly as often as people think engineers do, but that doesn’t mean LLMs are useless for me. It’s a discussion that requires case-by-case nuance.</p><h2 id="how-i-interface-with-llms">How I Interface With LLMs</h2><p>Over the years I’ve utilized all the tricks to get the best results out of LLMs. The most famous trick is <a href="https://en.wikipedia.org/wiki/Prompt_engineering">prompt engineering</a>, or the art of phrasing the prompt in a specific manner to coach the model to generate a specific constrained output. Additions to prompts such as <a href="https://minimaxir.com/2024/02/chatgpt-tips-analysis/">offering financial incentives to the LLM</a> or simply <a href="https://minimaxir.com/2025/01/write-better-code/">telling the LLM to make their output better</a> do indeed have a quantifiable positive impact on both improving adherence to the original prompt and the output text quality. Whenever my coworkers ask me why their LLM output is not what they expected, I suggest that they apply more prompt engineering and it almost always fixes their issues.</p><p><strong>No one in the AI field is happy about prompt engineering</strong>, especially myself. Attempts to remove the need for prompt engineering with more robust <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">RLHF</a> paradigms have only made it <em>even more rewarding</em> by allowing LLM developers to make use of better prompt adherence. True, “Prompt Engineer” as a job title <a href="https://www.wsj.com/articles/the-hottest-ai-job-of-2023-is-already-obsolete-1961b054?st=DMVDgm&amp;reflink=desktopwebshare_permalink">turned out to be a meme</a> but that’s mostly because prompt engineering is now an expected skill for anyone seriously using LLMs. Prompt engineering works, and part of being a professional is using what works even if it’s silly.</p><p>To that end, <strong>I never use ChatGPT.com</strong> or other normal-person frontends for accessing LLMs because they are harder to control. Instead, I typically access the backend UIs provided by each LLM service, which serve as a light wrapper over the API functionality which also makes it easy to port to code if necessary. Accessing LLM APIs like the ChatGPT API directly allow you to set <a href="https://promptengineering.org/system-prompts-in-large-language-models/">system prompts</a> which control the “rules” for the generation that can be very nuanced. Specifying specific constraints for the generated text such as “keep it to no more than 30 words” or “never use the word ‘delve’” tends to be more effective in the system prompt than putting them in the user prompt as you would with ChatGPT.com. Any modern LLM interface that does not let you explicitly set a system prompt is most likely <a href="https://docs.anthropic.com/en/release-notes/system-prompts">using their own system prompt</a> which you can’t control: for example, when ChatGPT.com had an issue where it was <a href="https://openai.com/index/sycophancy-in-gpt-4o/">too sycophantic</a> to its users, OpenAI <a href="https://simonwillison.net/2025/Apr/29/chatgpt-sycophancy-prompt/">changed the system prompt</a> to command ChatGPT to “avoid ungrounded or sycophantic flattery.” I tend to use <a href="https://www.anthropic.com/">Anthropic</a> Claude’s API — Claude Sonnet in particular — more than any ChatGPT variant because Claude anecdotally is less “robotic” and also handles coding questions much more accurately.</p><p>Additionally with the APIs, you can control the “<a href="https://www.hopsworks.ai/dictionary/llm-temperature">temperature</a>” of the generation, which at a high level controls the creativity of the generation. LLMs by default do not select the next token with the highest probability in order to allow it to give different outputs for each generation, so I prefer to set the temperature to <code>0.0</code> so that the output is mostly deterministic, or <code>0.2 - 0.3</code> if some light variance is required. Modern LLMs now use a default temperature of <code>1.0</code>, and I theorize that higher value is accentuating <a href="https://en.wikipedia.org/wiki/Hallucination_%28artificial_intelligence%29">LLM hallucination</a> issues where the text outputs are internally consistent but factually wrong.</p><h2 id="llms-for-professional-problem-solving">LLMs for Professional Problem Solving!</h2><p>With that pretext, I can now talk about how I have used generative LLMs over the past couple years at BuzzFeed. Here are outlines of some (out of many) projects I’ve worked on using LLMs to successfully solve problems quickly:</p><ul><li>BuzzFeed site curators developed a new <a href="https://www.siteguru.co/seo-academy/website-taxonomy">hierarchal taxonomy</a> to organize thousands of articles into a specified category and subcategory. Since we had no existing labeled articles to train a traditional <a href="https://scikit-learn.org/stable/modules/multiclass.html">multiclass classification</a> model to predict these new labels, I wrote a script to hit the Claude Sonnet API with a system prompt saying <code>The following is a taxonomy: return the category and subcategory that best matches the article the user provides.</code> plus the JSON-formatted hierarchical taxonomy, then I provided the article metadata as the user prompt, all with a temperature of <code>0.0</code> for the most precise results. Running this in a loop for all the articles resulted in appropriate labels.</li><li>After identifying hundreds of distinct semantic clusters of BuzzFeed articles using data science shenanigans, it became clear that there wasn’t an easy way to give each one unique labels. I wrote another script to hit the Claude Sonnet API with a system prompt saying <code>Return a JSON-formatted title and description that applies to all the articles the user provides.</code> with the user prompt containing five articles from that cluster: again, running the script in a loop for all clusters provided excellent results.</li><li>One BuzzFeed writer asked if there was a way to use a LLM to sanity-check grammar questions such as “should I use an <a href="https://www.merriam-webster.com/grammar/em-dash-en-dash-how-to-use">em dash</a> here?” against the <a href="https://www.buzzfeed.com/buzzfeednews/buzzfeed-style-guide">BuzzFeed style guide</a>. Once again I hit the Claude Sonnet API, this time copy/pasting the <em>full</em> style guide in the system prompt plus a command to <code>Reference the provided style guide to answer the user's question, and cite the exact rules used to answer the question.</code> In testing, the citations were accurate and present in the source input, and the reasonings were consistent.</li></ul><p>Each of these projects were off-hand ideas pitched in a morning standup or a Slack DM, and yet each project only took an hour or two to complete a proof of concept (including testing) and hand off to the relevant stakeholders for evaluation. For projects such as the hierarchal labeling, without LLMs I would have needed to do more sophisticated R&amp;D and likely would have taken days including building training datasets through manual labeling, which is not intellectually gratifying. Here, LLMs did indeed follow the <a href="https://en.wikipedia.org/wiki/Pareto_principle">Pareto principle</a> and got me 80% of the way to a working solution, but the remaining 20% of the work iterating, testing and gathering feedback took longer. Even after the model outputs became more reliable, LLM hallucination was still a concern which is why I also advocate to my coworkers to use caution and double-check with a human if the LLM output is peculiar.</p><p>There’s also one use case of LLMs that doesn’t involve text generation that’s as useful in my professional work: <a href="https://platform.openai.com/docs/guides/embeddings">text embeddings</a>. Modern text embedding models technically are LLMs, except instead of having a head which outputs the logits for the next token, it outputs a vector of numbers that uniquely identify the input text in a higher-dimensional space. All improvements to LLMs that the ChatGPT revolution inspired, such as longer context windows and better quality training regimens, also apply to these text embedding models and caused them to improve drastically over time with models such as <a href="https://www.nomic.ai/blog/posts/nomic-embed-text-v1">nomic-embed-text</a> and <a href="https://huggingface.co/Alibaba-NLP/gte-modernbert-base">gte-modernbert-base</a>. Text embeddings have done a lot at BuzzFeed from identifying similar articles to building recommendation models, but this blog post is about generative LLMs so I’ll save those use cases for another time.</p><h2 id="llms-for-writing">LLMs for Writing?</h2><p>No, I don’t use LLMs for writing the text on this very blog, which I suspect has now become a default assumption for people reading an article written by an experienced LLM user. My blog is far too weird for an LLM to properly emulate. My writing style is blunt, irreverent, and occasionally cringe: even with prompt engineering plus <a href="https://www.promptingguide.ai/techniques/fewshot">few-shot prompting</a> by giving it examples of my existing blog posts and telling the model to follow the same literary style precisely, LLMs output something closer to Marvel movie dialogue. But even if LLMs <em>could</em> write articles in my voice I still wouldn’t use them due of the ethics of misrepresenting authorship by having the majority of the work not be my own words. Additionally, I tend to write about very recent events in the tech/coding world that would not be strongly represented in the training data of a LLM if at all, which increases the likelihood of hallucination.</p><p>There is one silly technique I discovered to allow a LLM to improve my writing without having it do <em>my writing</em>: feed it the text of my mostly-complete blog post, and ask the LLM to pretend to be a cynical <a href="https://news.ycombinator.com/news">Hacker News</a> commenter and write five distinct comments based on the blog post. This not only identifies weaker arguments for potential criticism, but it also doesn’t tell me what I <em>should</em> write in the post to preemptively address that negative feedback so I have to solve it organically. When running a rough draft of this very blog post and the Hacker News system prompt through the Claude API (<a href="https://github.com/minimaxir/llm-use/blob/main/criticism_hn.md">chat log</a>), it noted that my examples of LLM use at BuzzFeed are too simple and not anything more innovative than traditional <a href="https://aws.amazon.com/what-is/nlp/">natural language processing</a> techniques, so I made edits elaborating how NLP would not be as efficient or effective.</p><h2 id="llms-for-companionship">LLMs for Companionship?</h2><p>No, I don’t use LLMs as friendly chatbots either. The runaway success of LLM personal companion startups such as <a href="https://character.ai/">character.ai</a> and <a href="https://replika.com/">Replika</a> are alone enough evidence that LLMs have a use, even if the use is just entertainment/therapy and not more utilitarian.</p><p>I admit that I am an outlier since treating LLMs as a friend is the most common use case. Myself being an introvert aside, it’s hard to be friends with an entity who is trained to be as friendly as possible but also habitually lies due to hallucination. I <em>could</em> prompt engineer an LLM to call me out on my bullshit instead of just giving me positive affirmations, but there’s no fix for the lying.</p><h2 id="llms-for-coding">LLMs for Coding???</h2><p>Yes, I use LLMs for coding, but only when I am reasonably confident that they’ll increase my productivity. Ever since the dawn of the original ChatGPT, I’ve asked LLMs to help me write <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expressions</a> since that alone saves me hours, embarrassing to admit. However, the role of LLMs in coding has expanded far beyond that nowadays, and coding is even more nuanced and more controversial on how you can best utilize LLM assistance.</p><p>Like most coders, I Googled coding questions and clicked on the first <a href="https://stackoverflow.com/">Stack Overflow</a> result that seemed relevant, until I decided to start asking Claude Sonnet the same coding questions and getting much more detailed and bespoke results. This was more pronounced for questions which required specific functional constraints and software frameworks, the combinations of which would likely not be present in a Stack Overflow answer. One paraphrased example I recently asked Claude Sonnet while writing <a href="https://minimaxir.com/2025/02/embeddings-parquet/">another blog post</a> is <code>Write Python code using the Pillow library to composite five images into a single image: the left half consists of one image, the right half consists of the remaining four images.</code> (<a href="https://github.com/minimaxir/llm-use/blob/main/pil_composition.md">chat log</a>). Compositing multiple images with <a href="https://pypi.org/project/pillow/">Pillow</a> isn’t too difficult and there’s enough <a href="https://stackoverflow.com/questions/3374878/with-the-python-imaging-library-pil-how-does-one-compose-an-image-with-an-alp">questions/solutions about it on Stack Overflow</a>, but the specific way it’s composited is unique and requires some positioning shenanigans that I would likely mess up on the first try. But Claude Sonnet’s code <a href="https://github.com/minimaxir/mtg-embeddings/blob/main/mtg_related_card_img.ipynb">got it mostly correct</a> and it was easy to test, which saved me time doing unfun debugging.</p><p>However, for more complex code questions particularly around less popular libraries which have fewer code examples scraped from Stack Overflow and <a href="https://github.com/">GitHub</a>, I am more cautious of the LLM’s outputs. One real-world issue I’ve had is that I need a way to log detailed metrics to a database while training models — for which I use the <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer">Trainer class</a> in <a href="https://huggingface.co/docs/transformers/en/index">Hugging Face transformers</a> — so that I can visualize and analyze it later. I asked Claude Sonnet to <code>Write a Callback class in Python for the Trainer class in the Hugging Face transformers Python library such that it logs model training metadata for each step to a local SQLite database, such as current epoch, time for step, step loss, etc.</code> (<a href="https://github.com/minimaxir/llm-use/blob/main/hf_trainer_logger_sqlite.md">chat log</a>). This one I was less optimistic about since there isn’t much code about creating custom callbacks, however the Claude-generated code implemented some helpful ideas that weren’t on the top-of-my-mind when I asked, such a buffer to limit blocking I/O, SQLite config speedups, batch inserts, and connection handling. Asking Claude to “make the code better” twice (why not?) results in a few more unexpected ideas such as SQLite connection caching and using a single column with the JSON column type to store an arbitrary number of metrics, in addition to making the code much more Pythonic. It is still a lot of code such that it’s unlikely to work out-of-the-box without testing in the full context of an actual training loop. However, even if the code has flaws, the ideas themselves are extremely useful and in this case it would be much faster and likely higher quality code overall to hack on this generated code instead of writing my own SQLite logger from scratch.</p><p>For actual data science in my day-to-day work that I spend most of my time, I’ve found that code generation from LLMs is less useful. LLMs cannot output the text result of mathematical operations reliably, with some APIs working around that by <a href="https://platform.openai.com/docs/assistants/tools/code-interpreter">allowing for a code interpreter</a> to perform data ETL and analysis, but given the scale of data I typically work with it’s not cost-feasible to do that type of workflow. Although <a href="https://pandas.pydata.org/">pandas</a> is the standard for manipulating tabular data in Python and has been around since 2008, I’ve been using the relatively new <a href="https://pola.rs/">polars</a> library exclusively, and I’ve noticed that LLMs tend to hallucinate polars functions as if they were pandas functions which requires documentation deep dives to confirm which became annoying. For data visualization, which I don’t use Python at all and instead use <a href="https://www.r-project.org/">R</a> and <a href="https://ggplot2.tidyverse.org/">ggplot2</a>, I really haven’t had a temptation to consult a LLM, in addition to my skepticism that LLMs would know both those frameworks as well. The techniques I use for data visualization have been <a href="https://minimaxir.com/2017/08/ggplot2-web/">unchanged since 2017</a>, and the most time-consuming issue I have when making a chart is determining whether the data points are too big or too small for humans to read easily, which is not something a LLM can help with.</p><p>Asking LLMs coding questions is only one aspect of coding assistance. One of the other major ones is using a coding assistant with in-line code suggestions such as <a href="https://github.com/features/copilot">GitHub Copilot</a>. Despite my success in using LLMs for one-off coding questions, I actually dislike using coding assistants for an unexpected reason: it’s distracting. Whenever I see a code suggestion from Copilot pop up, I have to mentally context switch from writing code to reviewing code and then back again, which destroys my focus. Overall, it was a net neutral productivity gain but a net negative cost as Copilots are much more expensive than just asking a LLM ad hoc questions through a web UI.</p><p>Now we can talk about the elephants in the room — agents, <a href="https://www.anthropic.com/news/model-context-protocol">MCP</a>, and vibe coding — and my takes are spicy. Agents and MCP, at a high-level, are a rebranding of the Tools paradigm popularized by the <a href="https://arxiv.org/abs/2210.03629">ReAct paper</a> in 2022 where LLMs can decide whether a tool is necessary to answer the user input, extract relevant metadata to pass to the tool to run, then return the results. The rapid LLM advancements in context window size and prompt adherence since then have made Agent workflows more reliable, and the standardization of MCP is an objective improvement over normal Tools that I encourage. However, <strong>they don’t open any new use cases</strong> that weren’t already available when <a href="https://www.langchain.com/">LangChain</a> first hit the scene a couple years ago, and now <a href="https://www.polarsparc.com/xhtml/MCP.html">simple implementations of MCP</a> workflows are even more complicated and confusing <a href="https://minimaxir.com/2023/07/langchain-problem/">than it was back then</a>. I personally have not been able to find any novel use case for Agents, not then and not now.</p><p>Vibe coding with coding agents like <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a> or <a href="https://www.cursor.com/en">Cursor</a> is something I have little desire to even experiment with. On paper, coding agents should be able to address my complaints with LLM-generated code reliability since it inherently double-checks itself and it’s able to incorporate the context of an entire code project. However, I have also heard the horror stories of people spending hundreds of dollars by accident and not get anything that solves their coding problems. There’s a fine line between experimenting with code generation and <em>gambling</em> with code generation. Vibe coding can get me 80% of the way there, and I agree there’s value in that for building quick personal apps that either aren’t ever released publicly, or are released with disclaimers about its “this is released as-is” nature. But it’s unprofessional to use vibe coding as a defense to ship knowingly substandard code for serious projects, and the only code I can stand by is the code I am fully confident in its implementation.</p><p>Of course, the coding landscape is always changing, and everything I’ve said above is how I use LLMs for now. It’s entirely possible I see a post on Hacker News that completely changes my views on vibe coding or other AI coding workflows, but I’m happy with my coding productivity as it is currently and I am able to complete all my coding tasks quickly and correctly.</p><h2 id="whats-next-for-llm-users">What’s Next for LLM Users?</h2><p>Discourse about LLMs and their role in society has become bifuricated enough such that making the extremely neutral statement that <a href="https://bsky.app/profile/hankgreen.bsky.social/post/3lnjohdrwf22j">LLMs have some uses</a> is enough to justify a barrage of harrassment. I strongly disagree with AI critic Ed Zitron <a href="https://www.wheresyoured.at/reality-check/">about his assertions</a> that the reason the LLM industry is doomed because OpenAI and other LLM providers can’t earn enough revenue to offset their massive costs as LLMs have no real-world use. Two things can be true simultaneously: (a) LLM provider cost economics are too negative to return positive ROI to investors, and (b) LLMs are useful for solving problems that are meaningful and high impact, albeit not to the AGI hype that would justify point (a). This particular combination creates a frustrating gray area that requires a nuance that an ideologically split social media can no longer support gracefully. Hypothetically, If OpenAI and every other LLM provider suddenly collapsed and no better LLM models would ever be trained and released, open-source and permissively licensed models such as <a href="https://huggingface.co/Qwen/Qwen3-235B-A22B">Qwen3</a> and <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1">DeepSeek R1</a> that perform comparable to ChatGPT are valid <a href="https://en.wikipedia.org/wiki/Substitute_good">substitute goods</a> and they can be hosted on dedicated LLM hosting providers like <a href="https://www.cerebras.ai/">Cerebras</a> and <a href="https://groq.com/">Groq</a> who can actually make money on each user inference query. OpenAI collapsing would not cause the end of LLMs, because LLMs are useful <em>today</em> and there will always be a nonzero market demand for them: it’s a bell that can’t be unrung.</p><p>As a software engineer — and especially as a data scientist — one thing I’ve learnt over the years is that it’s always best to use the right tool when appropriate, and LLMs are just another tool in that toolbox. LLMs can be both productive and counterproductive depending on where and when you use them, but they are most definitely not useless. LLMs are more akin to forcing a square peg into a round hole (at the risk of damaging either the peg or hole in the process) while doing things without LLM assistance is the equivalent of carefully defining a round peg to pass through the round hole without incident. But for some round holes, sometimes shoving the square peg through and asking questions later makes sense when you need to iterate quickly, while sometimes you have to be more precise with both the peg and the hole to ensure neither becomes damaged, because then you have to spend extra time and money fixing the peg and/or hole.</p><p>…maybe it’s okay if I ask an LLM to help me write my metaphors going forward.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: TextQuery – Query CSV, JSON, XLSX Files with SQL (123 pts)]]></title>
            <link>https://textquery.app/</link>
            <guid>43897129</guid>
            <pubDate>Mon, 05 May 2025 16:59:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://textquery.app/">https://textquery.app/</a>, See on <a href="https://news.ycombinator.com/item?id=43897129">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <main data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6="">  <p data-astro-cid-j7pv25f6="">
TextQuery is an <span data-astro-cid-j7pv25f6="">
all-in-one desktop app
</span> to import, query, modify, and visualize your raw data with SQL.
</p> <a href="https://textquery.app/downloads" role="button" data-astro-cid-j7pv25f6="">
Download TextQuery For Free <svg data-src="https://s2.svgbox.net/hero-outline.svg?ic=download" width="24" height="24" data-astro-cid-j7pv25f6=""></svg> </a> <p>
Available for macOS and Windows
</p> </div> <p><img src="https://i.magecdn.com/d2b508/29b284_hero?sz=100p" data-astro-cid-j7pv25f6=""> </p> </div> <div data-astro-cid-j7pv25f6=""> <p>
Trusted by 1500+ individuals from organizations like these
</p>  </div> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">
Import Data in a Few Clicks
</h2> <p data-astro-cid-j7pv25f6="">
TextQuery lets you import multiple data files without needing to
            define schema, writing code, or commands.
</p> <p>
Supported Formats
</p> <div data-astro-cid-j7pv25f6=""> <p><span data-astro-cid-j7pv25f6="">
Data Files
</span><span data-astro-cid-j7pv25f6="">.xlsx, .xls, .csv, .tsv</span> </p> <p><span data-astro-cid-j7pv25f6="">
Structured Data
</span><span data-astro-cid-j7pv25f6="">.json, .jsonld</span> </p> <p><span data-astro-cid-j7pv25f6="">
Compressed Archives
</span><span data-astro-cid-j7pv25f6="">.zip, .gz, .tgz, .tar</span> </p> </div> </div> <p><img src="https://i.magecdn.com/d2b508/2e9a66_import?sz=100p" alt="Product screenshot" data-astro-cid-j7pv25f6=""> </p> </div> <div data-astro-cid-j7pv25f6=""> <p><img src="https://i.magecdn.com/d2b508/6c7d49_code_editor_4?sz=100p" alt="Product screenshot" data-astro-cid-j7pv25f6=""> </p> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">
Powerful SQL Editor
</h2> <p data-astro-cid-j7pv25f6="">
Our best-in-class SQL editor offers multiple features to help you
            write queries faster.
</p> <div data-astro-cid-j7pv25f6=""> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=check_circle" width="20" height="20" data-astro-cid-j7pv25f6=""></svg><p><span data-astro-cid-j7pv25f6="">Query History and Favorites</span> </p></div> </div> </div> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">
Create Beautiful Charts
</h2> <p data-astro-cid-j7pv25f6="">
Create clean, beautiful charts right within the app. Simple to use
            and easy to customize.
</p> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=check_circle" width="20" height="20" data-astro-cid-j7pv25f6=""></svg><p><span data-astro-cid-j7pv25f6="">
Supports Line, Bar, Area, Scatter, Pie Charts
</span> </p></div> <div data-astro-cid-j7pv25f6=""> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=check_circle" width="20" height="20" data-astro-cid-j7pv25f6=""></svg><p><span data-astro-cid-j7pv25f6="">
Customize Title, Description, and Colors
</span> </p></div> <div data-astro-cid-j7pv25f6=""> <!-- Please refer: https://github.com/shubhamjain/svg-loader --> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=check_circle" width="20" height="20" data-astro-cid-j7pv25f6=""></svg><p><span data-astro-cid-j7pv25f6="">
Export to a file or share directly from clipboard
</span> </p></div> </div> </div> <p><img src="https://i.magecdn.com/d2b508/ac4a84_chart_creator?sz=100p" alt="Product screenshot" data-astro-cid-j7pv25f6=""> </p> </div> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">
Thoughtful Features That Simplify Work
</h2> <ul role="list" data-astro-cid-j7pv25f6=""> <li data-astro-cid-j7pv25f6=""> <img src="https://i.magecdn.com/d2b508/566360_inline_editor_2?sz=100p" alt="" data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">
Inline Editor
</h3> <p data-astro-cid-j7pv25f6="">
Quickly edit values, collect your changes, and commit them in one
              go.
</p> </div> </li> <li data-astro-cid-j7pv25f6=""> <img src="https://i.magecdn.com/d2b508/242263_filters_2?sz=100p" alt="" data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">Filters</h3> <p data-astro-cid-j7pv25f6="">
Quickly narrow down table's rows using filters. No need to write
              queries manually.
</p> </div> </li> <li data-astro-cid-j7pv25f6=""> <img src="https://i.magecdn.com/d2b508/dcea52_tabs?sz=100p" alt="" data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">Tabs</h3> <p data-astro-cid-j7pv25f6="">
Work on multiple queries and tables simultaneously using tabs.
</p> </div> </li> <li data-astro-cid-j7pv25f6=""> <img src="https://i.magecdn.com/d2b508/32b263_data_export?sz=100p" alt="" data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">
Data Export
</h3> <p data-astro-cid-j7pv25f6="">
Export data in multiple formats (CSV, JSON, Excel, SQL) or create
              another table with the results.
</p> </div> </li> </ul> </div> <section data-astro-cid-j7pv25f6=""> <figure data-astro-cid-j7pv25f6=""> <p data-astro-cid-j7pv25f6="">5 out of 5 stars</p>  <blockquote data-astro-cid-j7pv25f6=""> <p data-astro-cid-j7pv25f6="">
This is a wonderful app! There's essentially no learning curve if
            you already know SQL, and the chart creation feature is fantastic.
</p> </blockquote> <figcaption data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <p>Greg Williams</p> </div> <p>Gxwilso</p> </div> </figcaption> </figure> </section> <div data-astro-cid-j7pv25f6=""> <p data-astro-cid-j7pv25f6="">
First-class Desktop Experience
</p> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <svg data-src="https://s2.svgbox.net/hero-outline.svg?ic=currency-dollar" width="32" height="32" data-astro-cid-j7pv25f6=""></svg> <h3 data-astro-cid-j7pv25f6="">
Pay Once, Use Forever
</h3> <p data-astro-cid-j7pv25f6="">
We hate needless subscriptions as much as you do. TextQuery comes
              with a <mark data-astro-cid-j7pv25f6="">perpetual license with free updates</mark>.
</p> </div> <div data-astro-cid-j7pv25f6=""> <!-- Please refer: https://github.com/shubhamjain/svg-loader --> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=security" width="28" height="32" data-astro-cid-j7pv25f6=""></svg> <h3 data-astro-cid-j7pv25f6="">
Built with Privacy &amp; Security in Mind
</h3> <p data-astro-cid-j7pv25f6="">
TextQuery <mark data-astro-cid-j7pv25f6="">
neither records nor sends anything related to your usage
</mark>. You can query sensitive data with complete peace of mind.
</p> </div> <div data-astro-cid-j7pv25f6=""> <!-- Please refer: https://github.com/shubhamjain/svg-loader --> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=keyboard" width="32" height="32" data-astro-cid-j7pv25f6=""></svg> <h3 data-astro-cid-j7pv25f6="">
Keyboard Shortcuts
</h3> <p data-astro-cid-j7pv25f6="">
Every frequently used function is <mark data-astro-cid-j7pv25f6="">
accessible via a keyboard shortcut,
</mark> making you fast and efficient.
</p> </div> <div data-astro-cid-j7pv25f6=""> <!-- Please refer: https://github.com/shubhamjain/svg-loader --> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=update" width="32" height="32" data-astro-cid-j7pv25f6=""></svg> <h3 data-astro-cid-j7pv25f6="">
Always improving
</h3> <p data-astro-cid-j7pv25f6="">
We are constantly improving the app from your feedback, and from
              own usage. In past few months, <mark data-astro-cid-j7pv25f6="">
we have shipped hundreds of improvements
</mark>.
</p> </div> </div> </div> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">
Try TextQuery For Free
</h2> <p data-astro-cid-j7pv25f6="">
Evaluate our free version with <a href="https://textquery.app/pricing" data-astro-cid-j7pv25f6="">
certain limitations
</a> and upgrade to Pro when you feel like it.
</p>  </div> </main>    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No Instagram, no privacy (118 pts)]]></title>
            <link>https://blog.wouterjanleys.com/blog/no-instagram-no-privacy/</link>
            <guid>43896228</guid>
            <pubDate>Mon, 05 May 2025 15:37:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.wouterjanleys.com/blog/no-instagram-no-privacy/">https://blog.wouterjanleys.com/blog/no-instagram-no-privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=43896228">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">
        

        

        <div itemprop="articleBody">
            <p>I somehow escaped having an Instagram account.</p>
<p>This means that I am oblivious to all of my friends’ fun broadcasted on Instagram. I don’t feel pressured either to update the abstract audience of everyone I ever connected with online, on where I am, what I am doing or who I am hanging out with.</p>
<p>I feel very blessed.</p>
<p>Still, Instagram hasn’t completely escaped me. And it bugs me.</p>
<p>My wife and I live a pretty social life. My wife’s job required us to temporarily move to a place far away from home. Luckily the culture of the place we moved to is very friendly to foreigners and there is a big bunch of people like us who are also new to this place and eager to make new friends for the time that they are there.</p>
<p>This makes the lines of our friend circles blurred and our social life very dynamic. To illustrate: old friends leave and new friends arrive with such regularity, that the guest list of a birthday party this year will look vastly different from the one we threw last year.</p>
<p>Over the past few months, it has struck me multiple times how people know more about my life than I tell them or likely hear from others. Like: where we travelled last weekend and with whom. How can they know? Instagram. A post from someone else on that trip about that trip. Of course. You don’t have to be on Instagram, to <em>be</em> on instagram.</p>
<p>It’s not the worst problem to have, I know. It’s great that I participate in happenings that are fun enough for other people to post about. And without social media, people also talk about common friends or friends of friends.</p>
<p>Yet sometimes I am not sure about how I feel about other people hearing about my weekend adventures through social media of other people. Not that there is anything to hide, I just hope the message came across well.</p>
<p>Real life interactions usually offer the possibility to be mindful of sensitivities around a certain subject, as can be the case if the topic of discussion is about who is hanging out with whom. The goal of social interactions (offline), I believe, is often less about what all the talking is about than it is to have the interaction. So surely you will try to avoid hurting the feelings of whom you are talking with.</p>
<p>In the loud bursts of social media self-promotion and personal branding, any such nuance of tone is impossible. The audience is way too diverse to cater to all possible sensitivities. Just think of the unease to post something about a night out with the boys that strikes the right tone to all of: partner, boss and said boys. It becomes even more complicated if you have to count in the social fabric between people featuring in your post and your followers. Yet those relationships are equally real, and have just as much context.</p>
<p>Imagine a friend you were on a weekend trip with. This friend talks with another common friend. This common friend could have equally well been on that weekend trip because you like him or her but, due to circumstances, as is life, you did not invite him. You probably would feel uncomfortable with that first friend talking about that trip as if it was the most awesome trip ever, that everyone had non-stop fun and now everyone who was on that trip are best friends for life.</p>
<p>Yet this is the kind of impression an Instagram post or story typically evokes. It’s probably the content most of the first friends’ followers love to see. Except for maybe the few people who wonder why you didn’t ask them to join the trip.</p>
<p>I am happy not to be on Instagram. I would probably freak out by the pressure to post while worrying about how the same post is interpreted by different people. Yet other people still post about me. And now I worry how common friends, who may have seen those posts, interpret such posts, potentially not entirely positively.</p>
<p>Without an Instagram account, I luckily stay blissfully oblivious to whatever content is going around from which my whereabouts or activities can be deduced. It’s probably a lot less about me than I think anyway. At the same time, not really knowing what other people know, can also be a nagging sense of worry. </p>
<p>Every time I talk with someone who gives me a hint that he or she <em>knows</em> what I was doing last Friday night I get a real eerie feeling. Did this person get to see that picture we took right before dinner? Or did she see a story at the end of the night, one too many drinks later, when I was doing that silly dance that surely someone had filmed?</p>
<p>From when I was in law school, I remember it being challenging to find a simple and encompassing definition of the right to privacy. A definition that has worked for me is “being in control of what other people know about you”.</p>
<p>Given this definition, there are many flagrant privacy violations I am subject to and aware of (yes, internet companies that do large scale gathering of personal data, I am thinking of you). I know that I am not in control of what the online store knows about me when it’s nudging me to buy this or that pair of headphones. But somehow this is easy to shrug off. Not being in control of what your friends know about you, due to social media stories spread by other people, feels more unsettling.</p>
<p>How does one fix or regulate this? Maybe we need some sort of social etiquette where it’s frowned upon to post about social gatherings to any audience beyond who already was at that gathering. Although this arguably defies the purpose of social media altogether.</p>
<p>Meanwhile, I’ll continue feeling blessed, not knowing that my new cool friend threw a birthday party last week to which I was not invited. There’s no social disappointment that will discourage me to further invest in that relationship, improving the chances that I will be celebrating his or her birthday next year.</p>
<p>And what about that new friend who wasn’t on the trip, but may or may not have just scrolled through some flashy posts about it after a tough day at work? Hopefully he forgives me and still wants to join the next trip.</p>
        </div>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dimension 126 Contains Twisted Shapes, Mathematicians Prove (101 pts)]]></title>
            <link>https://www.quantamagazine.org/dimension-126-contains-strangely-twisted-shapes-mathematicians-prove-20250505/</link>
            <guid>43896199</guid>
            <pubDate>Mon, 05 May 2025 15:34:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/dimension-126-contains-strangely-twisted-shapes-mathematicians-prove-20250505/">https://www.quantamagazine.org/dimension-126-contains-strangely-twisted-shapes-mathematicians-prove-20250505/</a>, See on <a href="https://news.ycombinator.com/item?id=43896199">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-role="selectable">
    <p>It can be tempting to assume that your intuitions about three-dimensional space carry over to higher-dimensional realms. After all, adding another dimension simply creates a new direction to move around in. It doesn’t change the defining features of space: its endlessness and its uniformity.</p>
<p>But different dimensions have decidedly different personalities. In dimensions 8 and 24, it’s possible to <a href="https://www.quantamagazine.org/sphere-packing-solved-in-higher-dimensions-20160330/">pack balls together</a> especially tightly. In other dimensions, there are “exotic” spheres that look irremediably crumpled. And dimension 3 is the only one that can contain knots — in any higher dimension, you can untangle a knot even while holding its ends fast.</p>
<p>Now, mathematicians have put the finishing touches on a story of dimensional weirdness that has been 65 years in the making. For many decades, researchers have wanted to know which dimensions can host particularly strange shapes — ones so twisted that they cannot be converted into a sphere through a simple procedure called surgery. The existence of these shapes, mathematicians have shown, is intimately intertwined with fundamental questions in topology about the relationships between spheres of different dimensions.</p>
<p>Over the years, mathematicians found that the twisted shapes exist in dimensions 2, 6, 14, 30 and 62. They also showed that such shapes could not possibly exist in any other dimension — save one. No one could determine the status of dimension 126.</p>
<p>Three mathematicians have now settled this final problem. In a <a href="https://arxiv.org/abs/2412.10879">paper posted online</a> last December, <a href="https://waynelin92.github.io/">Weinan Lin</a> and <a href="https://pouiyter.github.io/">Guozhen Wang</a> of Fudan University in Shanghai, along with <a href="https://sites.google.com/view/xuzhouli">Zhouli Xu</a> of the University of California, Los Angeles, proved that 126 is indeed one of the rare dimensions that can host these strangely twisted shapes.</p>
<p>It’s “a very long program, finally finished,” said <a href="https://people.maths.ox.ac.uk/tillmann/">Ulrike Tillmann</a> of the University of Oxford.</p>
<p>The proof, which uses a combination of computer calculations and theoretical insights, is “like a monumental engineering project,” said <a href="https://people.math.harvard.edu/~mjh/">Michael Hopkins</a> of Harvard University. “It’s just jaw-dropping how they did it.”</p>
<h2><strong>The Doomsday Hypothesis</strong></h2>
<p>In the 1950s, the mathematician <a href="https://www.math.stonybrook.edu/~jack/">John Milnor</a> astonished the mathematical world by showing that dimension 7 is <a href="https://mathscinet.ams.org/mathscinet/relay-station?mr=0082103">home to “exotic” spheres</a>. An exotic sphere looks exactly like an ordinary sphere from the perspective of topology, which only considers the features of a shape that don’t change when it is stretched or deformed. But the two spheres have incompatible definitions of smoothness — a curve that’s smooth on an ordinary sphere might not be considered smooth on an exotic sphere. Milnor was eager to explore and classify these exotic spheres, which in some dimensions turned out to be rare and in others numbered in the thousands.</p>
<p>To do this, he <a href="https://mathscinet.ams.org/mathscinet/relay-station?mr=0130696">introduced a technique</a> called surgery, a controlled way to simplify a mathematical shape, or manifold, and potentially convert it into an exotic sphere. The method would become essential to the study of manifolds more generally.</p>
<p>As its name suggests, surgery involves slicing out a piece of a manifold and then sewing in one or more new pieces along the boundary of the cut. You must sew in these new pieces smoothly, without creating sharp corners or edges. (When it comes to questions about twisted shapes, mathematicians also require the surgery to respect the manifold’s “framing,” a technical attribute of how the manifold sits in space.)</p>
<p>To see this process in action, let’s surgically transform a torus (the two-dimensional surface of a doughnut) into a sphere (the two-dimensional surface of a ball):</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Mathematical-surgery-Desktop.V2.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Mathematical-surgery-Mobile.V2.svg" alt="" decoding="async">    </p>
            <figcaption>
            <p>Samuel Velasco/<em>Quanta Magazine</em></p>
        </figcaption>
    </figure>

<p>The result is an ordinary sphere — in fact, there are no 2D exotic spheres. But in certain dimensions, surgery converts some manifolds into ordinary spheres and others into exotic spheres. And sometimes, there’s yet another possibility: manifolds that can’t be converted into a sphere at all.</p>
<p>To visualize this last scenario, we can again look at a torus, only this time we’ll give it some special twists to obstruct surgeries:</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Twists-and-Turns-Desktop.V3.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Twists-and-Turns-Mobile.V3.svg" alt="" decoding="async">    </p>
            <figcaption>
            <p>Samuel Velasco/<em>Quanta Magazine</em></p>
        </figcaption>
    </figure>

<p>Mathematicians have proved that there is no surgery that can transform this twisted torus into a sphere, whether regular or exotic. It’s an entirely different class of manifold.</p>
<p>In 1960, the French mathematician Michel Kervaire came up with an <a href="https://link.springer.com/article/10.1007/BF02565940">invariant</a> — a number you can calculate for a given smooth manifold — that equals zero when the manifold can be surgically converted into a sphere, and 1 when it cannot. So the ordinary torus has a Kervaire invariant of zero, while the twisted torus has a Kervaire invariant of 1.</p>
<p>Kervaire used his invariant to explore the menagerie of possible manifolds in different dimensions. He even used it to come up with a 10-dimensional manifold that has no Kervaire invariant, either zero or 1 — meaning that this manifold must be so crooked that it can have no sensible notion of smoothness at all.</p>
<p>No one had imagined that such a manifold could exist. Faced with the power of the new invariant, mathematicians rushed to determine the Kervaire invariants of manifolds in different dimensions.</p>
<p>Within a few years, they’d proved that twisted manifolds of Kervaire invariant 1 exist in dimensions 2, 6, 14 and 30. These dimensions fit a pattern: Each number is 2 less than a power of 2 (for example, 30 is 2<sup>5</sup> − 2). In 1969, the mathematician William Browder <a href="https://www.jstor.org/stable/1970686?origin=crossref">proved that dimensions of this form</a> are the only ones that might host shapes with a Kervaire invariant of 1.</p>
<p>It was natural to assume that twisted manifolds would exist in all dimensions of this form: 62, 126, 254 and so forth. Based on this assumption, one mathematician even built an entire edifice of conjectures about exotic spheres and other shapes. But the possibility that the original assumption might be false still loomed. It came to be known as the doomsday hypothesis, since it would falsify all these other conjectures.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How are cyber criminals rolling in 2025? (191 pts)]]></title>
            <link>https://vin01.github.io/piptagole/cybcecrime/security/cybersecurity/2025/05/05/state-cyber-security.html</link>
            <guid>43896188</guid>
            <pubDate>Mon, 05 May 2025 15:33:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vin01.github.io/piptagole/cybcecrime/security/cybersecurity/2025/05/05/state-cyber-security.html">https://vin01.github.io/piptagole/cybcecrime/security/cybersecurity/2025/05/05/state-cyber-security.html</a>, See on <a href="https://news.ycombinator.com/item?id=43896188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Speaking of earning a living, would you expect them to pay for web hosting/ cloud providers?</p>

<h2 id="free-content-hosting">Free content hosting</h2>

<h3 id="governments">Governments</h3>

<p>Government departments are not known for their amazing cyber security posture. Couldn’t they be utilized as free hosting providers?</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/gov.png" alt="free-gov-hosting"></p>

<p>and it is truly international</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/international.png" alt="internaitonal"></p>

<h3 id="universities--schools">Universities &amp; schools</h3>

<p>How about universities? they have a lot of web facing services and while they do teach cyber security sometimes, they can’t be that secure?</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/uni.png" alt="yale and oregon state university"></p>

<p>For the zeitgeist-ignorant:</p>

<p><strong>What is Robux?</strong>: A virtual in-game currency used by gaming platform <a href="https://en.wikipedia.org/wiki/Roblox">Roblox</a></p>

<p>Content mostly revolves around: Onlyfans accounts/account generators, Robux, Amazon gift cards and Free movies. I suppose these are the most popular things on the internet these days.</p>



<p>Who doesn’t use an antivirus or a VPN to keep themselves, their family and their employees safe? These obviously unsafe links would get blocked and flagged by these advanced tools. You wish!</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/safe-web.png" alt="safe-web"></p>

<p>and more ..</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/safe-web2.png" alt="safe-web-2"></p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/safe-web3.png" alt="safe-web-3"></p>

<p>Turns out, they know which domains get a good reputation from these link m-analyzers.</p>

<p>Norton, Kaspersky, Zscaler, F-secure, NordVPN, Virustotal, Palo Alto: all of them marked these links as safe. The same for SOAR tools like URLscan.</p>

<h2 id="but-if-we-only-allow-access-to-a-pre-approved-list-of-domains-that-should-help-right">But if we only allow access to a pre-approved list of domains, that should help, right?</h2>

<p>No! Turns out, we are again one step behind. They know that there are some domains which everyone trusts. e.g., everything hosted by google, including the shiny “Looker studio” used to visualize data as pretty graphs.</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/looker.png" alt="Looker studio"></p>

<h2 id="does-this-actually-work">Does this actually work?</h2>

<p>Apparently so! Here are some benign search results performed from a true Robux seeker and movie freeloader’s perspective.</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/seo.png" alt="good seo"></p>

<p>They have to be good at SEO and know their targets. Business as usual. ¯\_(ツ)_/¯</p>

<h2 id="how-are-they-exploiting-these-sites">How are they exploiting these sites?</h2>

<p>I have been advised not to disclose specific vulnerabilities since the parties involved are not most friendly and transparent in handling security reports. While most of these got reported and some even got fixed, I can only disclose high-level details of the compromise path. Some just ghosted me after conveniently fixing the flaws, and one even gave me a phone call, which was somewhat scary and perhaps not worth the adrenaline.</p>

<ul>
  <li>Outdated Wordpress plugins and CMS systems</li>
  <li>Cache poisoning via features like “search my site”</li>
  <li>Credential stuffing</li>
  <li>Dangling DNS records/ Subdomain takeovers</li>
</ul>

<h2 id="why-are-they-doing-it">Why are they doing it?</h2>

<p>Most of these files are not <em>malware</em> per se, since that might be a bit easy to detect out in the wild. But these PDFs and web pages link to a chain of websites. Each link would take the brave person who clicks them to another link and then to another, going through an affiliate network and making small amounts of money on the way.</p>

<p>Some of these links are also just plain phishing, mostly targeting kids looking for free/ cheap gaming gold “Robux”.</p>

<h2 id="this-is-not-new">This is not new</h2>

<p>I came across this post from July 2020 which discusses something similar: <a href="https://medium.com/@thezedwards/july-2020-compromised-paf-subdomains-mostly-via-microsoft-azure-5834ae22733a">https://medium.com/@thezedwards/july-2020-compromised-paf-subdomains-mostly-via-microsoft-azure-5834ae22733a</a></p>


  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: VectorVFS, your filesystem as a vector database (217 pts)]]></title>
            <link>https://vectorvfs.readthedocs.io/en/latest/</link>
            <guid>43896011</guid>
            <pubDate>Mon, 05 May 2025 15:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vectorvfs.readthedocs.io/en/latest/">https://vectorvfs.readthedocs.io/en/latest/</a>, See on <a href="https://news.ycombinator.com/item?id=43896011">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" v-pre="">
            
  <p><a href="https://vectorvfs.readthedocs.io/en/latest/_images/logo_vectorvfs.png"><img alt="Banner" src="https://vectorvfs.readthedocs.io/en/latest/_images/logo_vectorvfs.png">
</a></p><section id="vectorvfs-your-filesystem-as-a-vector-database">

<p>VectorVFS is a lightweight Python package that transforms your Linux filesystem into a vector database by
leveraging the native VFS (Virtual File System) extended attributes. Rather than maintaining a separate
index or external database, VectorVFS stores vector embeddings directly alongside each file—turning your
existing directory structure into an efficient and semantically searchable embedding store.</p>
<p>VectorVFS supports Meta’s Perception Encoders (PE) <a href="https://arxiv.org/abs/2504.13181">[arxiv]</a> which
includes image/video encoders for vision language understanding, it outperforms InternVL3, Qwen2.5VL
and SigLIP2 for zero-shot image tasks. We support both CPU and GPU but if you have a large
collection of images it might take a while in the first time to embed all items if you are
not using a GPU.</p>
<div>
<p>Note</p>
<p>This is the first release of VectorVFS and we are expanding models and data types.
Currently we support only Perception Encoders (PE) and images.</p>
</div>
<section id="key-features">
<h2>Key Features<a href="#key-features" title="Link to this heading">¶</a></h2>
<ul>
<li><p><strong>Zero-overhead indexing</strong>
Embeddings are stored as extended attributes (xattrs) on each file, eliminating the need for external
index files or services.</p></li>
<li><p><strong>Seamless retrieval</strong>
Perform searches across your filesystem, retrieving files by embedding similarity.</p></li>
<li><p><strong>Flexible embedding support</strong>
Plug in any embedding model—from pre-trained transformers to custom feature extractors—and let
VectorVFS handle storage and lookup.</p></li>
<li><p><strong>Lightweight and portable</strong>
Built on native Linux VFS functionality, VectorVFS requires no additional daemons, background
processes or databases.</p></li>
</ul>
<div>
<p role="heading"><span>Contents</span></p>
<ul>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/installation.html">Installation</a><ul>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/installation.html#installing-with-pip">Installing with pip</a></li>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/installation.html#using-the-vfs-script">Using the vfs Script</a></li>
</ul>
</li>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/architecture.html">Design and architecture</a><ul>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/architecture.html#what-is-an-inode">What is an inode?</a></li>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/architecture.html#how-ext4-stores-extended-attributes">How Ext4 stores extended attributes</a></li>
</ul>
</li>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/usage.html">Using the <cite>vfs</cite> command</a><ul>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/usage.html#vfs-search-command"><cite>vfs search</cite> command</a></li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="indices-and-tables">
<h2>Indices and tables<a href="#indices-and-tables" title="Link to this heading">¶</a></h2>
<ul>
<li><p><a href="https://vectorvfs.readthedocs.io/en/latest/genindex.html"><span>Index</span></a></p></li>
<li><p><a href="https://vectorvfs.readthedocs.io/en/latest/py-modindex.html"><span>Module Index</span></a></p></li>
<li><p><a href="https://vectorvfs.readthedocs.io/en/latest/search.html"><span>Search Page</span></a></p></li>
</ul>
</section>


          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Geometrically understanding calculus of inverse functions (2023) (109 pts)]]></title>
            <link>https://tobylam.xyz/2023/11/27/inverse-functions-legendre-part-1</link>
            <guid>43895852</guid>
            <pubDate>Mon, 05 May 2025 15:01:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tobylam.xyz/2023/11/27/inverse-functions-legendre-part-1">https://tobylam.xyz/2023/11/27/inverse-functions-legendre-part-1</a>, See on <a href="https://news.ycombinator.com/item?id=43895852">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">

    <!-- Makes it possible to have an excerpt that has HTML code in it (instead of only plaintext in a string) -->
    
      <p>Given a function such as \(\tan x\), could you write \(\frac{d}{dx} \arctan x\) and \(\int \arctan x \; dx\), just from \(\tan x\), \(\frac{d}{dx} \tan x\) and \(\int \tan x \; dx\)? With some caveats, the inverse function theorem answers the former while the Legendre transformation answers the later. We’ll approach this with as much geometric intuition as possible, avoiding the dry application of formulas.</p>



<p>Instead of approaching the inverse function theorem through formulas, we’ll explore it geometrically—it’s much more intuitive and enjoyable!</p>

<p>But first, to refresh our memory, let’s revisit the formal statement of the inverse function theorem, which relates the derivative of \(f(x)\) and its inverse \(f^{-1}(x)\).</p>

<blockquote>
  <p>Given a continuously differentiable function \(f: \mathbb{R} \to \mathbb{R}\) with \(f'(a) \neq 0\) at some point, the inverse function theorem states that there is some interval \(I\) with \(a \in I\) such that there exists a continuously differentiable inverse \(f^{-1}\) defined on \(f(I)\) such that for all \(x \in I\)</p>
</blockquote><p>

\[\frac{df^{-1}}{dx}(x) = \frac{1}{f'(f^{-1}(x))}.\]

</p><p>A simple example of the theorem in action is finding the derivative of \(\ln x\), which evaluates to \(1/e^{\ln x} = 1/x\). The standard high school approach to deduce the above in high school is to differentiate both sides of \(f^{-1}(f(x)) = x\).</p>

<p>This formal approach is quite dry and things get a lot more interesting when we think geometrically.</p>

<p>Geometrically, given some function \(f(x) = y\) the inverse is just taking the plot of the function and reflecting it about the diagonal line \(y=x\). As such all tangent lines are also reflected along the diagonal line and hence the slope is inversed. Below we have the graph of \(e^x\) (the blue line) and the graph of \(\ln x\) (the red line). And you can see how the tangent line of \(e^x\) at \((2,e^2)\) is reflected along \(y=x\) to give the tangent line of \(\ln x\) at \((e^2, 2)\).</p>

<p><img src="https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-600-8400210a9.webp" srcset="https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-480-8400210a9.webp 480w, https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-600-8400210a9.webp 600w, https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-720-8400210a9.webp 720w, https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-900-8400210a9.webp 900w, https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-1080-8400210a9.webp 1080w, https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-1250-8400210a9.webp 1250w" sizes="(max-width: 600px) 100vw, (max-width: 1000px) 600px, (max-width: 1400px) 600px, (min-width: 1401px) 600px" width="1250" height="628"></p>

<p>Another way of looking at it would be to subjugate \(S=\{(x, f(x)) \, \vert \, x \in I\}\), the plot of \(f(x)\), with a graph transformation \(\phi:(x,y) \to (\hat{x}, \hat{y}) = (y,x)\) and express \(\phi(S)\) as a plot \((\hat{x}, \hat{y}(\hat{x}))\). This way \(\hat{y}\) would be in effect the inverse of \(y\). We then wish to understand how \(\phi\) acts on \(\frac{dy}{dx}\) to give \(\frac{d\hat{y}}{d\hat{x}}\).</p>

<p>You can express this with the following graph</p>

<p><img alt="https://q.uiver.app/#q=WzAsNixbMywwLCIoeSh4KSx4KSJdLFswLDIsIih4LCB5Jyh4KSkiXSxbMywyLCIoXFxoYXR7eH0sXFxmcmFje2RcXGhhdHt5fX17ZFxcaGF0e3h9fSkgIl0sWzAsMCwiKHgseSh4KSkiXSxbNCwwLCIoXFxoYXR7eH0sXFxoYXR7eX0oXFxoYXR7eH0pKSAiXSxbNCwxLCJcXGhhdHt4fSh4KT0geSh4KSBcXFxcIFxcaGF0e3l9KFxcaGF0e3h9KHgpKT15XnstMX0oXFxoYXR7eH0pIl0sWzAsMiwiXFxmcmFje2R9e2RcXGhhdHt4fX0iXSxbMSwyXSxbMywwLCJcXHBoaSgoeCx5KSk9KHkseCkiXSxbMywxLCJcXGZyYWN7ZH17ZHh9Il0sWzAsNCwiPSIsMSx7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6Im5vbmUifSwiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFs0LDUsIiIsMSx7InN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XV0=" src="https://tobylam.xyz/generated/assets/images/legend1-600-fb2c4463e.png" srcset="https://tobylam.xyz/generated/assets/images/legend1-480-fb2c4463e.webp 480w, https://tobylam.xyz/generated/assets/images/legend1-600-fb2c4463e.webp 600w, https://tobylam.xyz/generated/assets/images/legend1-720-fb2c4463e.webp 720w, https://tobylam.xyz/generated/assets/images/legend1-900-fb2c4463e.webp 900w, https://tobylam.xyz/generated/assets/images/legend1-1080-fb2c4463e.webp 1080w, https://tobylam.xyz/generated/assets/images/legend1-1260-fb2c4463e.webp 1260w, https://tobylam.xyz/generated/assets/images/legend1-1304-fb2c4463e.webp 1304w" sizes="(max-width: 600px) 100vw, (max-width: 1000px) 600px, (max-width: 1400px) 600px, (min-width: 1401px) 600px" width="1304" height="782"></p>

<p>and the equalities</p><p>

\[\begin{align*}
    \hat x(x) &amp;= y(x), \\
    \hat y(\hat x(x)) &amp;= x \\
    &amp;= y^{-1}(\hat x(x)).
\end{align*}\]

</p><p>We could then directly calculate</p><p>

\[\begin{align*}
    \frac{d\hat{y}}{d\hat{x}}(\hat x(x)) &amp;= \frac{d\hat{y}}{dx}(x) / \frac{d\hat{x}}{dx}(x) &amp; \text{(Chain rule)} \\
    &amp;= 1 / \frac{dy}{dx}(x).
\end{align*}\]

</p><p>This recovers the inverse function theorem.</p>

<p>This method generalises to other maps \(\phi: \mathbb{R}^2 \to \mathbb{R}^2\) (e.g. \(\phi:(r, \theta) \to (r \cos \theta, r \sin \theta)\)) . These ideas are explored much more deeply in <a href="https://www.cambridge.org/core/books/symmetry-methods-for-differential-equations/805DEEB7D5D0D2040A3A8444B1C8A33E">Hydon’s book on Symmetry Methods for Differential Equations</a>.</p>

<p>By calculating \(\frac{d}{dx} \tan x = \sec^2 x\), we now know that \(\frac{d}{dx} \arctan x = \frac{1}{\text{sec}^2(\arctan x)} = \frac{1}{1+x^2}\). So what about \(\int \arctan x \, dx\)?</p>

<h2 id="integrals-of-inverse-functions-and-the-legendre-transformation">Integrals of inverse functions and the Legendre transformation</h2>

<p>Geometrically, we could show that with \(f\) strictly monotone, we have</p><p>

\[\int_{f(a)}^{f(b)} f^{-1}(y) \, dy + \int^{b}_a f(x) \, dx  = bf(b)-af(a)\]

</p><p>This is <a href="https://en.wikipedia.org/wiki/Integral_of_inverse_functions">a result by Laisant in 1905</a>. Here’s a proof-without-words.</p>

<p><img src="https://tobylam.xyz/generated/assets/images/legend3-800-1a031bab5.png" srcset="https://tobylam.xyz/generated/assets/images/legend3-400-1a031bab5.png 400w, https://tobylam.xyz/generated/assets/images/legend3-600-1a031bab5.png 600w, https://tobylam.xyz/generated/assets/images/legend3-800-1a031bab5.png 800w, https://tobylam.xyz/generated/assets/images/legend3-1000-1a031bab5.png 1000w"></p>

<!-- <small> By <a href="//commons.wikimedia.org/w/index.php?title=User:Jonathan.Steinbuch&amp;amp;action=edit&amp;amp;redlink=1" title="Jonathan Steinbuch - Own work"> Jonathan Steinbuch - Own work, </a>
 <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=16592836">Link</a> </small> -->

<p>You can use this to recover the explicit formula of the integral of \(f^{-1}\), which is left as an exercise! Instead we will use the graph reflection approach used earlier for the inverse function theroem.</p>

<p>Suppose we’re interested in finding \(\int \arctan x \, dx\). Notationally, it is the easiest to let \(y = \int \tan x \, dx\) up to some constant such that \(\frac{dy}{dx} = \tan x\) is the function we want to invert. Let \(S\) be the plot of \(\frac{dy}{dx}\), i.e. \(\{(x, \frac{dy}{dx}) \vert \, x \in I\}\) for some appropiate interval \(I\). Applying the map \(\phi: (x,y) \to (\hat{x}, \hat{y}) = (y,x)\), we assume the existence of some \(\hat{y}\) such that its derivative is the plot of \(\phi(S)\). Now \(\hat{y} = \int \arctan \hat{x} \, d\hat{x}\) which is what we seek.</p>

<p>Consider</p>

<p><img alt="https://q.uiver.app/#q=WzAsNSxbMywwLCIoXFxoYXR7eH0sXFxoYXR7eX0oXFxoYXQgeCkiXSxbMCwyLCIoeCwgXFxmcmFje2R5fXtkeH0oeCkpIl0sWzMsMiwiKFxcaGF0IHgsIFxcZnJhY3tkXFxoYXQgeX17ZFxcaGF0IHh9KFxcaGF0IHgpKSAiXSxbMCwwLCIoeCx5KHgpKSJdLFs1LDNdLFswLDIsIlxcZnJhY3tkfXtkXFxoYXR7eH19Il0sWzEsMiwiXFxwaGkoKHgseSkpPSh5LHgpIl0sWzMsMSwiXFxmcmFje2R9e2R4fSJdXQ==" src="https://tobylam.xyz/generated/assets/images/legend2-600-9a72c68ab.png" srcset="https://tobylam.xyz/generated/assets/images/legend2-480-9a72c68ab.webp 480w, https://tobylam.xyz/generated/assets/images/legend2-600-9a72c68ab.webp 600w, https://tobylam.xyz/generated/assets/images/legend2-720-9a72c68ab.webp 720w, https://tobylam.xyz/generated/assets/images/legend2-900-9a72c68ab.webp 900w, https://tobylam.xyz/generated/assets/images/legend2-1080-9a72c68ab.webp 1080w, https://tobylam.xyz/generated/assets/images/legend2-1232-9a72c68ab.webp 1232w" sizes="(max-width: 600px) 100vw, (max-width: 1000px) 600px, (max-width: 1400px) 600px, (min-width: 1401px) 600px" width="1232" height="712"></p>

<p>with the equalities</p><p>

\[\begin{align*}
    \hat x(x) &amp;= \frac{dy}{dx}(x), \\
    \frac{d\hat y}{d\hat x}(\hat x(x)) &amp;= x. \\
\end{align*}\]

</p><p>Now consider</p><p>

\[\begin{align*}
    &amp;\;\hat{y}(\hat{x}) \\
    &amp;= \int\frac{d\hat y}{d\hat x}(\hat{x}) d\hat{x}  \\
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - \int \hat{x}\frac{d^2\hat y}{d\hat x^2}(\hat{x}) d\hat{x}  &amp; (\text{by parts}) \\
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - \int \hat{x}(x)\frac{d^2\hat y}{d\hat x^2}(\hat{x}(x))  \frac{d\hat x}{dx}(x) dx &amp;\\
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - \int \hat{x}(x)\bigg[\frac{d}{dx}\bigg(\frac{d\hat y}{d\hat x}(\hat{x}(x))\bigg)(x)\bigg] dx &amp; (\text{chain rule})\\ 
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - \int \hat{x}(x) dx &amp; \bigg(\frac{d\hat y}{d\hat x}(\hat{x}(x)) = x\bigg) \\ 
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - \int \frac{dy}{dx}(x) dx &amp; (\text{by def of } \hat{x})\\ 
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - y(x) + C \\
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - y\bigg(\frac{d\hat{y}}{d\hat x}(\hat{x})\bigg) + C.  &amp; (\text{by def of } \hat{x})
\end{align*}\]

</p><p>Let’s try to compute a simple example, the integral of \(\ln x\). Consider</p>

<p><img src="https://tobylam.xyz/generated/assets/images/legend4-600-36dc952bd.png" srcset="https://tobylam.xyz/generated/assets/images/legend4-480-36dc952bd.webp 480w, https://tobylam.xyz/generated/assets/images/legend4-600-36dc952bd.webp 600w, https://tobylam.xyz/generated/assets/images/legend4-720-36dc952bd.webp 720w, https://tobylam.xyz/generated/assets/images/legend4-900-36dc952bd.webp 900w, https://tobylam.xyz/generated/assets/images/legend4-1080-36dc952bd.webp 1080w, https://tobylam.xyz/generated/assets/images/legend4-1260-36dc952bd.webp 1260w, https://tobylam.xyz/generated/assets/images/legend4-1440-36dc952bd.webp 1440w, https://tobylam.xyz/generated/assets/images/legend4-1736-36dc952bd.webp 1736w" sizes="(max-width: 600px) 100vw, (max-width: 1000px) 600px, (max-width: 1400px) 600px, (min-width: 1401px) 600px" width="1736" height="718"></p>

<p>Repeating what we’ve done above, we have</p><p>

\[\begin{align*}
    \hat{y} &amp;= \int \ln \hat{x} \, d\hat{x} \\
    &amp;= \hat{x} \times \ln(\hat{x})  - \int \hat{x} \times \frac{d}{d\hat x}(\ln \hat{x}) d\hat{x}\\
    &amp;= \hat{x} \times \ln(\hat{x}) - \int e^x \times \frac{d}{d\hat x}(\ln \hat x) \times \frac{d}{dx} (\hat x) dx \\
    &amp;= \hat{x} \times \ln(\hat{x}) - \int e^x \times \frac{d}{dx} \big(\ln(e^x)\big) dx \\
    &amp;= \hat{x} \times \ln(\hat{x}) - \int e^x dx \\
    &amp;= \hat{x} \times \ln(\hat{x}) - e^x + C \\
    &amp;= \hat{x} \times \ln(\hat{x}) - e^{\ln (\hat{x})} + C \\
    &amp;= \hat{x} \times \ln(\hat{x}) - \hat{x} + C. 
\end{align*}\]

</p><p>Finally, just by using the formula \(\hat{y} = \hat{x} \times\frac{d\hat y}{d\hat x}(\hat{x}) - y(\hat{y}'(\hat{x})) + C\), if we let \(y = - \ln \vert \cos(x) \vert\), then we have \(y' = \tan(x)\) and \(\hat{y}' = \arctan(x)\) so</p><p>

\[\begin{align*}
    \int \arctan \hat{x} &amp;= \hat{x} \arctan \hat{x} - \ln \vert \cos(\arctan(\hat{x}))\vert + C\\
    &amp;=  \hat{x} \arctan \hat{x} - \frac{1}{2} \ln \big( \frac{1}{1+\hat{x}^2}\big) + C
\end{align*}\]

</p><p>as expected.</p>

<p>The map \(y(x) \to \hat{y}(\hat{x}) := \hat{x}\frac{d\hat y}{d\hat x}(\hat{x}) - y(\frac{d\hat y}{d\hat x}(\hat{x}))\) is called the Legendre transformation, which has wide applications in areas of physics. But mathematically, one could think of it as an analogue to the inverse function theorem: It tells you how the inverse map acts on integrals.</p>

<h2 id="summary">Summary</h2>

<p><img alt="https://q.uiver.app/#q=WzAsNixbMCwxLCJmKHgpIl0sWzAsMiwiZicoeCkiXSxbMCwwLCJGKHgpIl0sWzMsMSwiZyh5KTo9Zl57LTF9KHkpIl0sWzMsMCwiRyh5KSJdLFszLDIsImcnKHkpIl0sWzAsMSwiXFxmcmFje2R9e2R4fSJdLFsyLDAsIlxcZnJhY3tkfXtkeH0iXSxbMCwzXSxbNCwzLCJcXGZyYWN7ZH17ZHl9Il0sWzMsNSwiXFxmcmFje2R9e2R5fSJdLFsxLDUsIlxcdGV4dHtJbnZlcnNlIEZ1bmMuIFRobS59Il0sWzIsNCwiXFx0ZXh0e0xlZ2VuZHJlIFRyYW5zZm9ybX0iXV0=" src="https://tobylam.xyz/generated/assets/images/2023-11-29-summary-600-47e06eab1.png" srcset="https://tobylam.xyz/generated/assets/images/2023-11-29-summary-480-47e06eab1.webp 480w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-600-47e06eab1.webp 600w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-720-47e06eab1.webp 720w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-900-47e06eab1.webp 900w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-1080-47e06eab1.webp 1080w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-1260-47e06eab1.webp 1260w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-1440-47e06eab1.webp 1440w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-1800-47e06eab1.webp 1800w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-1992-47e06eab1.webp 1992w" sizes="(max-width: 600px) 100vw, (max-width: 1000px) 600px, (max-width: 1400px) 600px, (min-width: 1401px) 600px" width="1992" height="824"></p>

<p>with the following relations</p><p>

\[\begin{align*}
    \text{IFT:} \quad &amp; g'(y) = \frac{1}{f'(g(y))} \\
    \text{Legendre:} \quad &amp; G(y) = y\times g(y) - F(g(y)) + C
\end{align*}\]

</p><p>For more ways to understand the ideas covered in this post you can look at the <a href="https://tobylam.xyz/assets/documents/2023-11-27-inverse-functions-legendre.pdf">article</a> I’ve written for the Oxford Invariants journal, as well as the <a href="https://en.wikipedia.org/wiki/Integral_of_inverse_functions">wiki page</a> on the integral of inverse functions.</p>

<!-- 
$$\begin{align*}
    &\int g(y) dy \\
    &= y \times g(y) - \int y dg(y) \\
    &= y \times g(y) - \int y g'(y) dy \\
    &= y \times g(y) - \int f(x) g'(f(x)) f'(x) dx & (\text{Let }y = f(x))\\
    &= y \times g(y) - \int f(x) (g \circ f)'(x) dx \\
    &= y \times g(y) - \int f(x) dx & (g \circ f = \text{id}) \\
    &= y \times g(y) - F(x) + C \\
    &= y \times g(y) - F(g(y)) + C
\end{align*}$$
-->

    
    

    <!-- Only change in this layout is here. Next posts and previous posts. Refer to _plugins/WithinCategoryPostNavigation.rb-->
    <!-- I have no idea why the next and previous posts are swapped. Oh well. The link to all this is here https://ajclarkson.co.uk/blog/jekyll-category-post-navigation/ -->

    <!-- <a href="/2023/12/11/visualization-fundamental-theorem-calculus" class="previous-link">Next post</a> 
    
     <a href="/2023/11/04/graph-sketching-techniques" class="next-link">Previous post</a> 

    <p>All posts in <a href="/archive/categories#Maths"> "maths" </a></p> -->

  </div>
</article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Bracket – selfhosted tournament system (133 pts)]]></title>
            <link>https://github.com/evroon/bracket</link>
            <guid>43895456</guid>
            <pubDate>Mon, 05 May 2025 14:20:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/evroon/bracket">https://github.com/evroon/bracket</a>, See on <a href="https://news.ycombinator.com/item?id=43895456">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/frontend/public/favicon-wide.svg"><img width="500" src="https://github.com/evroon/bracket/raw/master/frontend/public/favicon-wide.svg" alt="Bracket - Tournament System"></a>
</p>
<p dir="auto">
  <a href="https://github.com/evroon/bracket/actions"><img src="https://camo.githubusercontent.com/ccfa14f52676b0092543f3b72359e8a3595d4e5a9f111b58b308d96ef9c1fdad/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f6576726f6f6e2f627261636b65742f6261636b656e642e796d6c" alt="build status" data-canonical-src="https://img.shields.io/github/actions/workflow/status/evroon/bracket/backend.yml"></a>
  <a href="https://crowdin.com/project/bracket" rel="nofollow"><img src="https://camo.githubusercontent.com/1f64c284e1fa22e1e72374f0e4a8f589c7313400d62c337a565936e2672e6c60/68747470733a2f2f6261646765732e63726f7764696e2e6e65742f627261636b65742f6c6f63616c697a65642e737667" alt="translations" data-canonical-src="https://badges.crowdin.net/bracket/localized.svg"></a>
  <a href="https://github.com/evroon/bracket/commits/"><img src="https://camo.githubusercontent.com/38a5ac206dbef151607627d0ac681e963d4d1b595f604c03bb680d3285519072/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f6576726f6f6e2f627261636b6574" alt="last commit" data-canonical-src="https://img.shields.io/github/last-commit/evroon/bracket"></a>
  <a href="https://github.com/evroon/bracket/releases"><img src="https://camo.githubusercontent.com/467f10ca6354ece9832b9f02af28b3a525b3ee24d56b168c68264ebff36e32df/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f6576726f6f6e2f627261636b6574" alt="release" data-canonical-src="https://img.shields.io/github/v/release/evroon/bracket"></a>
  <a href="https://codecov.io/gh/evroon/bracket" rel="nofollow"><img src="https://camo.githubusercontent.com/a0c86e929956b818f5646c51cc8a509eca10d82e7b98c12e42e57363de24efff/68747470733a2f2f636f6465636f762e696f2f67682f6576726f6f6e2f627261636b65742f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d594a4c30445650464647" alt="codecov" data-canonical-src="https://codecov.io/gh/evroon/bracket/branch/master/graph/badge.svg?token=YJL0DVPFFG"></a>
</p>
<p dir="auto">
  <a href="https://www.bracketapp.nl/demo" rel="nofollow">Demo</a>
  ·
  <a href="https://docs.bracketapp.nl/" rel="nofollow">Documentation</a>
  ·
  <a href="https://docs.bracketapp.nl/docs/running-bracket/quickstart" rel="nofollow">Quickstart</a>
  ·
  <a href="https://github.com/evroon/bracket">GitHub</a>
  ·
  <a href="https://github.com/evroon/bracket/releases">Releases</a>
</p>

<p dir="auto">Tournament system meant to be easy to use. Bracket is written in async Python (with
<a href="https://fastapi.tiangolo.com/" rel="nofollow">FastAPI</a>) and <a href="https://nextjs.org/" rel="nofollow">Next.js</a> as frontend using the
<a href="https://mantine.dev/" rel="nofollow">Mantine</a> library.</p>
<p dir="auto">It has the following features:</p>
<ul dir="auto">
<li>Supports <strong>single elimination, round-robin and swiss</strong> formats.</li>
<li><strong>Build your tournament structure</strong> with multiple stages that can have multiple groups/brackets in
them.</li>
<li><strong>Drag-and-drop matches</strong> to different courts or reschedule them to another start time.</li>
<li>Various <strong>dashboard pages</strong> are available that can be presented to the public, customized with a
logo.</li>
<li>Create/update <strong>teams</strong>, and add players to <strong>teams</strong>.</li>
<li>Create <strong>multiple clubs</strong>, with <strong>multiple tournaments</strong> per club.</li>
<li><strong>Swiss tournaments</strong> can be handled dynamically, with automatic scheduling of matches.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/docs/content/img/bracket-screenshot-design.png"><img alt="" src="https://github.com/evroon/bracket/raw/master/docs/content/img/bracket-screenshot-design.png" width="100%"></a></p>
<p dir="auto">
<a href="https://docs.bracketapp.nl/" rel="nofollow"><strong>Explore the Bracket docs&nbsp;&nbsp;▶</strong></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Live Demo</h2><a id="user-content-live-demo" aria-label="Permalink: Live Demo" href="#live-demo"></a></p>
<p dir="auto">A demo is available for free at <a href="https://www.bracketapp.nl/demo" rel="nofollow">https://www.bracketapp.nl/demo</a>. The demo lasts for 30 minutes, after which
your data will de deleted.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quickstart</h2><a id="user-content-quickstart" aria-label="Permalink: Quickstart" href="#quickstart"></a></p>
<p dir="auto">To quickly run bracket to see how it works, clone it and run <code>docker compose up</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:evroon/bracket.git
cd bracket
sudo docker compose up -d"><pre>git clone git@github.com:evroon/bracket.git
<span>cd</span> bracket
sudo docker compose up -d</pre></div>
<p dir="auto">This will start the backend and frontend of Bracket, as well as a postgres instance. You should now
be able to view bracket at <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a>. You can log in with the following credentials:</p>
<ul dir="auto">
<li>Username: <code>test@example.org</code></li>
<li>Password: <code>aeGhoe1ahng2Aezai0Dei6Aih6dieHoo</code>.</li>
</ul>
<p dir="auto">To insert dummy rows into the database, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo docker exec bracket-backend pipenv run ./cli.py create-dev-db"><pre>sudo docker <span>exec</span> bracket-backend pipenv run ./cli.py create-dev-db</pre></div>
<p dir="auto">See also the <a href="https://docs.bracketapp.nl/docs/running-bracket/quickstart" rel="nofollow">quickstart docs</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Read the <a href="https://docs.bracketapp.nl/docs/usage/guide" rel="nofollow">usage guide</a> for how to organize a tournament in Bracket from start to finish.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Read the <a href="https://docs.bracketapp.nl/docs/running-bracket/configuration" rel="nofollow">configuration docs</a> for how to configure Bracket.</p>
<p dir="auto">Bracket's backend is configured using <code>.env</code> files (<code>prod.env</code> for production, <code>dev.env</code> for development etc.).
But you can also configure Bracket using environment variables directly, for example by specifying them in the <code>docker-compose.yml</code>.</p>
<p dir="auto">The frontend doesn't can be configured by environment variables as well, as well as <code>.env</code> files using Next.js' way of loading environment variables.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running Bracket in production</h2><a id="user-content-running-bracket-in-production" aria-label="Permalink: Running Bracket in production" href="#running-bracket-in-production"></a></p>
<p dir="auto">Read the <a href="https://docs.bracketapp.nl/docs/deployment" rel="nofollow">deployment docs</a> for how to deploy Bracket and run it in production.</p>
<p dir="auto">Bracket can be run in Docker or by itself (using <code>pipenv</code> and <code>yarn</code>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development setup</h2><a id="user-content-development-setup" aria-label="Permalink: Development setup" href="#development-setup"></a></p>
<p dir="auto">Read the <a href="https://docs.bracketapp.nl/docs/community/development" rel="nofollow">development docs</a> for how to run Bracket for development.</p>
<p dir="auto">Prerequisites are <code>yarn</code>, <code>postgresql</code> and <code>pipenv</code> to run the frontend, database and backend.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Translations</h2><a id="user-content-translations" aria-label="Permalink: Translations" href="#translations"></a></p>
<p dir="auto">Based on your browser settings, your language should be automatically detected and loaded. For now,
there's no manual way of choosing a different language.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Languages</h2><a id="user-content-supported-languages" aria-label="Permalink: Supported Languages" href="#supported-languages"></a></p>
<p dir="auto">To add/refine translations, <a href="https://crowdin.com/project/bracket" rel="nofollow">Crowdin</a> is used.
See the <a href="https://docs.bracketapp.nl/docs/community/contributing/#translating" rel="nofollow">docs</a> for more information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">More screenshots</h2><a id="user-content-more-screenshots" aria-label="Permalink: More screenshots" href="#more-screenshots"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/docs/content/img/schedule_preview.png"><img alt="" src="https://github.com/evroon/bracket/raw/master/docs/content/img/schedule_preview.png" width="50%"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/docs/content/img/planning_preview.png"><img alt="" src="https://github.com/evroon/bracket/raw/master/docs/content/img/planning_preview.png" width="50%"></a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/docs/content/img/builder_preview.png"><img alt="" src="https://github.com/evroon/bracket/raw/master/docs/content/img/builder_preview.png" width="50%"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/docs/content/img/standings_preview.png"><img alt="" src="https://github.com/evroon/bracket/raw/master/docs/content/img/standings_preview.png" width="50%"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Help</h2><a id="user-content-help" aria-label="Permalink: Help" href="#help"></a></p>
<p dir="auto">If you're having trouble getting Bracket up and running, or have a question about usage or configuration, feel free to ask.
The best place to do this is by creating a <a href="https://github.com/evroon/bracket/discussions">Discussion</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supporting Bracket</h2><a id="user-content-supporting-bracket" aria-label="Permalink: Supporting Bracket" href="#supporting-bracket"></a></p>
<p dir="auto">If you're using Bracket and would like to help support its development, that would be greatly appreciated!</p>
<p dir="auto">Several areas that we need a bit of help with at the moment are:</p>
<ul dir="auto">
<li>⭐ <strong>Star Bracket</strong> on GitHub</li>
<li>🌐 <strong>Translating</strong>: Help make Bracket available to non-native English speakers by adding your language (via <a href="https://crowdin.com/project/bracket" rel="nofollow">crowdin</a>)</li>
<li>📣 <strong>Spread the word</strong> by sharing Bracket to help new users discover it</li>
<li>🖥️ <strong>Submit a PR</strong> to add a new feature, fix a bug, extend/update the docs or something else</li>
</ul>
<p dir="auto">See the <a href="https://docs.bracketapp.nl/docs/community/contributing" rel="nofollow">contribution docs</a> for more information on how to contribute</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>

<markdown-accessiblity-table><table>
<tbody><tr>
    <td>
        <a href="https://github.com/evroon">
            <img src="https://avatars.githubusercontent.com/u/11857441?v=4" width="100;" alt="evroon">
            <br>
            <sub><b>Erik Vroon</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/robigan">
            <img src="https://avatars.githubusercontent.com/u/35210888?v=4" width="100;" alt="robigan">
            <br>
            <sub><b>Null</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/BachErik">
            <img src="https://avatars.githubusercontent.com/u/75324423?v=4" width="100;" alt="BachErik">
            <br>
            <sub><b>BachErik</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/djpiper28">
            <img src="https://avatars.githubusercontent.com/u/13609136?v=4" width="100;" alt="djpiper28">
            <br>
            <sub><b>Danny Piper</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/Sevichecc">
            <img src="https://avatars.githubusercontent.com/u/91365763?v=4" width="100;" alt="Sevichecc">
            <br>
            <sub><b>SevicheCC</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/nvanheuverzwijn">
            <img src="https://avatars.githubusercontent.com/u/943226?v=4" width="100;" alt="nvanheuverzwijn">
            <br>
            <sub><b>Nicolas Vanheuverzwijn</b></sub>
        </a>
    </td></tr>
<tr>
    <td>
        <a href="https://github.com/IzStriker">
            <img src="https://avatars.githubusercontent.com/u/44909896?v=4" width="100;" alt="IzStriker">
            <br>
            <sub><b>IzStriker</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/babeuh">
            <img src="https://avatars.githubusercontent.com/u/60193302?v=4" width="100;" alt="babeuh">
            <br>
            <sub><b>Raphael Le Goaller</b></sub>
        </a>
    </td></tr>
</tbody></table></markdown-accessiblity-table>

<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Bracket is licensed under <a href="https://choosealicense.com/licenses/agpl-3.0/" rel="nofollow">AGPL-v3.0</a>.</p>
<p dir="auto">Please note that any contributions also fall under this license.</p>
<p dir="auto">See <a href="https://github.com/evroon/bracket/blob/master/LICENSE">LICENSE</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Death of Daydreaming: What we lose when phones take away boredom (557 pts)]]></title>
            <link>https://www.afterbabel.com/p/on-the-death-of-daydreaming</link>
            <guid>43894305</guid>
            <pubDate>Mon, 05 May 2025 12:22:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.afterbabel.com/p/on-the-death-of-daydreaming">https://www.afterbabel.com/p/on-the-death-of-daydreaming</a>, See on <a href="https://news.ycombinator.com/item?id=43894305">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><strong>Intro from Jon Haidt:</strong></p><p><span>When I was nearly finished writing </span><em>The Anxious Generation</em><span> in the summer of 2023, I realized that I had left a gap. The book focused on the collapse of mental health, attention, and socialization of Gen Z—but again and again, older readers and friends would tell me: “this is happening to me too.” I realized that the global transition to a phone-based life is transforming </span><em>everyone’s</em><span> consciousness. But how? What exactly is happening to us?</span></p><p><span>I thought back to my first book, </span><em><a href="https://www.happinesshypothesis.com/" rel="">The Happiness Hypothesis</a></em><span>, which explored ten ancient ideas about how to live a good life—ideas found across continents and millennia. I began to see that the phone-based life, and social media in particular, push us to live in ways that are directly contrary to those recommended by nearly every ancient religious and philosophical tradition. These traditions tell us to be </span><em>slow to judge</em><span> and </span><em>quick to forgive</em><span>. They offer practices like meditation to quiet the mind and open the heart to deeper truths and greater communion.</span></p><p><span>In </span><em>The Anxious Generation</em><span>, I explored six of these contradictions in Chapter 8. A simple summary of that chapter is this: the phone-based life diminishes our humanity. Compared to the life we once lived, it degrades us morally, spiritually, socially, and cognitively.</span></p><p><span>Since writing that chapter, I’vebeen on the lookout for others who can express this loss of humanity better than I can. Here’s my short list: L.M. Sacasas for his Substack </span><em><a href="https://theconvivialsociety.substack.com/" rel="">The Convivial Society</a></em><span>; Nicholas Carr, for </span><em><a href="https://www.nicholascarr.com/?page_id=16" rel="">The Shallows</a></em><span> and his more recent book </span><em><a href="https://www.afterbabel.com/p/from-anxiety-to-animosity-how-social" rel="">Superbloom</a></em><span>; and Christine Rosen, for her 2024 book </span><em><a href="https://wwnorton.com/books/9780393241716" rel="">The Extinction of Experience</a><span>.</span></em><span> (For a Gen Z perspective, see </span><a href="https://www.freyaindia.co.uk/" rel="">Freya India</a><span>—especially the post we co-wrote </span><a href="https://www.afterbabel.com/p/degrading-effects-of-life-online" rel="">“On The Degrading Effects of Life Online.”)</a></p><p><span>Today’s post is by Christine Rosen. Christine holds a Ph.D. in history with a focus on American intellectual history. She is </span><a href="https://www.aei.org/profile/christine-rosen/" rel="">a senior fellow at AEI</a><span>, and she was </span><a href="https://www.aei.org/events/the-american-dream-lecture-series-social-media-and-the-fragmentation-of-american-life/" rel="">my discussion partner</a><span> when I gave a talk on </span><em>The Anxious Generation</em><span> at AEI soon after the book launched. That was when I learned about her book project, </span><em><a href="https://wwnorton.com/books/9780393241716" rel="">The Extinction of Experience: Being Human in a Disembodied World.</a></em></p><p>So when her publisher invited me to consider reading an early draft and offering an endorsement, I was glad to say yes. I loved the book, and I hope the blurb I wrote captures its urgency and depth:</p><blockquote><p>Rosen shows us that we are embodied creatures who are rapidly losing the analog world in which our bodies and minds evolved. She shows us that many of the technologically aided advances in convenience and efficiency exact a cost in our humanity, our civility, and ultimately our ability to live together in a democratic society. This is an extremely important book; its message is all the more urgent as AI threatens to make everything effortless and frictionless.</p></blockquote><p><span>I invited Christine to present a section or idea from her book for </span><em>After Babel</em><span>. She chose one of my favorites: the loss of “interstitial time.” Interstices are the gaps between things, as with the cells in your body or the spaces between architectural columns. When applied to time, it means the many bits of time scattered throughout the day such as the five minutes that students have in between classes, or the unknown number of seconds that pass while you are waiting for an elevator. These moments used to be given over to silent reflection or conversation with whoever is around. Now, for most of us, nearly all of them are grabbed by our phones.</span></p><p>Christine traces out the profound consequences of losing those interstitial moments, for our creativity and for our humanity. She shows why it is so important to guard those moments for daydreaming.</p><p><strong>— Jon </strong></p><p><strong>By Christine Rosen</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2140702,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.afterbabel.com/i/162789733?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc045dcff-c182-4cc1-b063-23afb0578a5a_3840x2160.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Source: Shutterstock</figcaption></figure></div><p>Can you remember the last time you daydreamed? Or coped with boredom without reaching for your phone? Before the era of mobile technology, most of us had no choice but to wait without stimulation, and often, that meant being bored.</p><p>But today we need never be bored. We have an indefatigable boredom-killing machine: the smartphone. No matter how brief our wait, the smartphone promises an alleviation for our suffering.</p><p><span>Yet the smartphone’s triumph over boredom might prove a Pyrrhic victory. As Jonathan Haidt showed in </span><em>The Anxious Generation</em><span>, the rapid adoption of smartphones and social media, particularly by the young, led to many negative unintended consequences such as increased rates of depression, anxiety, loneliness, and self-harm. So, too, our efforts to vanquish boredom have had deleterious impacts such as on our ability to let our minds wander, to cultivate patience, and to experience anticipation.</span></p><p>As a member of Generation X, I took boredom for granted. Without access to any kind of mobile technology more sophisticated than a Speak and Spell game, my generation was expected to fill our empty hours in other ways, which usually meant going outside and doing things with our friends. Some kids stayed inside and watched television, of course, but the options for programming were limited. Boredom was part of life, and we accepted and adjusted to this reality. Several decades later, raising my own sons in the age of mobile technology, I saw how quickly expectations had changed for how to spend one’s free time. With access to an iPad or a smartphone, children in the twenty-first century never had to be bored; in fact, everything about the platforms and apps that targeted children habituated them to the idea that they ought never to be bored. I worried about how this might change their expectations and ability to deal with delay, frustration, and empty time as adults.</p><p>Boredom has a purpose. To understand and harness it, we need to give our minds more opportunities to experience it. In the rest of this post, I will explore the many ways our efforts to conquer boredom through technology have produced unintended consequences, including the near-total capture of our attention, the death of daydreaming, and the end of a healthy sense of anticipation in our daily lives.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.afterbabel.com/p/on-the-death-of-daydreaming?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.afterbabel.com/p/on-the-death-of-daydreaming?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><span>“Shepherds do it, cops do it, stevedores and merchants in their shops do it,” technology critic Marshall McLuhan </span><a href="https://mitpress.mit.edu/9780262631594/understanding-media/" rel="">observed</a><span> in </span><em>Understanding Media </em><span>when discussing Greek men’s use of komboloi, or worry beads. The beads, which look like amber-­colored rosaries, were used throughout the day to pass the time, a secular version of praying the rosary. Their use of worry beads also reflects the deeply felt human need to fill interstitial time. We all engage in these weird little rituals: Some people doodle or fidget, others knit, a lot of people used to smoke. The late Psychologist Mihaly Csikszentmihalyi </span><a href="https://www.amazon.com/Flow-Classic-Achieve-Happiness-Introduction/dp/0712657592" rel="">called</a><span> these “the ‘microflow’ activities that help us negotiate the doldrums of the day.” These “small automatic games woven into the fabric of everyday life help reduce boredom … but add little to the positive quality of experience.”</span></p><p>Though the experience of boredom is deeply human, what we reach for when we experience it is socially structured, unique to our moment in time. The worry beads and cigarettes of previous eras have given way to smartphones. Ours is a less carcinogenic but more commodified distraction, with long-term impacts which we’re only beginning to fathom.</p><p><a href="https://www.pewresearch.org/internet/2024/03/11/how-teens-and-parents-approach-screen-time/" rel="">According to</a><span> Pew Research, nine out of ten Americans own a smartphone, and 95 percent of teenagers have access to one. A 2024 Pew survey of teens ages 13-17 found that half said they were online “almost constantly.” The average person spends the vast majority of his or her free moments looking at a screen. For decades, Americans have spent a considerable amount of their leisure time watching television; what has declined significantly is the amount of free time they spend with other people. One recent </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9811250/" rel="">study</a><span> found an increase in social isolation and significant declines in social engagement with family and friends as well as shared leisure time. We spend more of our free time alone, staring at screens, which habituates us to reach for our phones whenever we have a moment alone to ourselves. Screens have become the dominant means for us to alleviate boredom, whether during long stretches of time alone or in fleeting moments throughout the day.</span></p><p><span>This is not just a challenge for the young. Pew Research </span><a href="https://www.pewresearch.org/short-reads/2019/06/18/americans-60-and-older-are-spending-more-time-in-front-of-their-screens-than-a-decade-ago/" rel="">found </a><span>that Americans over the age of sixty “now spend more than half of their daily leisure time. . . in front of screens.”</span></p><p>Lately I’ve seen more people in their cars thwarting stoplight boredom—­that is, unable to sit unmediated for even the few moments that it takes a red light to turn green, they reach for their smartphones. Kids post on social media about boredom throughout the school day (#bored). The space between the time when they experience boredom and when they broadcast it has disappeared.</p><p><span>What happens when we replace boredom with constant distraction and stimulation? Warnings about the harmful effects of too much stimulation are nothing new. “For a living organism, protection against stimuli is an almost more important function than the reception of stimuli,” Sigmund Freud </span><a href="https://www.sas.upenn.edu/~cavitch/pdf-library/Freud_Beyond_P_P.pdf" rel="">observed</a><span>. But given the range and speed of stimuli at our disposal, we might need a new way of thinking about their effects. Stimulation seems too quaint a word.</span></p><p>It is a reasonable human impulse to seek distraction from the uncomfortable experience of boredom. What is new about our present moment is that the method we have chosen to alleviate boredom in the short term has negative long-term impacts on our attention spans and our ability to practice patience. We have created a machine for stimulation far beyond anything imaginable in Freud’s time. We might believe that our attempts to fill our interstitial time with mediated distractions qualify as an effort to optimize our experiences under less-than-optimal conditions. But in fact, we have become more like gambling addicts, habituated to the temporary escape our digital technologies provide.</p><p><span>A fascinating </span><a href="https://press.princeton.edu/books/paperback/9780691160887/addiction-by-design" rel="">study</a><span> of machine gambling in Las Vegas notes that “flow,” that state of being in which someone is so involved in an activity “that nothing else seems to matter,” as Csikszentmihalyi describes it, is precisely the state gamblers seek and attain at the machines, and precisely what machine designers seek to exploit when people initiate play. Yet, while gamblers are experiencing flow, they are not having the kind of optimal long-­term experiences psychologists had in mind when they advocated pursuing activities that put you “in the zone.”</span></p><p>In a less intense way, we all enter this less-than-optimal state when we turn to our devices to alleviate the experience of boredom. The distractions we seek don’t only consume our time, however. They also degrade many habits of mind that require time and patience to form, such as empathy, awareness, and emotional regulation.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.afterbabel.com/p/on-the-death-of-daydreaming/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.afterbabel.com/p/on-the-death-of-daydreaming/comments" rel=""><span>Leave a comment</span></a></p><p><span>In a </span><a href="https://lettersofnote.com/2012/03/06/1984-v-brave-new-world/" rel="">letter</a><span> Aldous Huxley wrote to George Orwell in 1949 he argued, “I feel that the nightmare of 1984 is destined to modulate into the nightmare of a world having more resemblance to that which I imagined in Brave New World.” What did Huxley believe would bring about this dystopia? Not a global world order or a charismatic despot: “The change will be brought about as a result of a felt need for increased efficiency.”</span></p><p>Huxley’s warning has merit. We enjoy the efficiencies and distractions technology brings, but they leave us less skilled at patience. They teach us to value efficiency above all and to be suspicious of idle time, when we should see idle moments as opportunities for reflection and renewal.</p><p>Today you rarely see the word “idle” except when used as a pejorative; to be idle is to be wasteful, and several of the most popular Internet startup companies have targeted underutilized resources such as idle cars (Turo, ZipCar), household equipment (SnapGoods), or empty bedrooms (Airbnb), allowing people to make use of them by renting them out when they aren’t in use.</p><p><span>Some technologists have set their sights higher. Max Levchin, a cofounder of PayPal and investor in many Silicon Valley technology companies, gave a </span><a href="https://max.levch.in/post/41116802381/dld13-keynote" rel="">speech</a><span> at a conference in Munich in which he lamented, “The world of real things is very inefficient.” Harnessing the network effects of big data, he foresees a future where we can more efficiently do many things: “We will definitely see dynamically priced queues for confession-­taking priests and therapists,” he said.</span></p><p><span>Moments of idleness and daydreaming used to be prized for the unexpected pleasure they brought. As Wordsworth </span><a href="https://www.penguinrandomhouse.com/books/295450/selected-poems-by-william-wordsworth-edited-with-an-introduction-and-notes-by-stephen-gill/" rel="">wrote</a><span>, “For this one day we’ll give to idleness . . . One moment now may give us more than fifty hours of reason.” He advocated straying about “voluptuously” through rural fields, asking “no record of the hours given up to vacant musing.” We might not spend our free time lolling about rural glens, but idleness of this variety is the opposite of the instrumental, practical use that our culture encourages us to make of our time. Technologists like Levchin would have us hire out our voluptuous spare time on TaskRabbit. To borrow an image from Wordsworth’s rural fields, we should embrace this fallow time. To be fallow is not the same thing as to be useless; it is to let rest so that cultivation can occur in the future. When mediated experiences co-opt our idle time, we are left with fewer and fewer of these fallow moments, moments that are central to the experience of being human.</span></p><p>With rates of anxiety rising in the U.S., particularly among teens, it is also worth considering how the frenetic pace of the online world, where so many of us spend so much of our time, contributes to our sense of feeling overwhelmed and out of control. Reclaiming our idle time and reorienting ourselves away from screens is one of many small yet radical acts that have the potential to improve the quality of our daily experiences.</p><p><span>A culture without boredom, focused on efficiency, also undermines the act of daydreaming, another thing interstitial time used to be given over to. Daydreaming seems a fusty term in an age when productivity and usefulness are prized. But as psychologists and neurologists have found, a wandering mind––often the first signal of impending boredom––is also a creative mind. In the 1960s, psychologist Jerome Singer, the grandfather of daydreaming studies, </span><a href="https://archive.org/details/innerworldofdayd00samu" rel="">identified</a><span> three kinds of mind-­wandering: the productive, creative “positive constructive daydreaming,” obsessive “guilty–­dysphoric daydreaming,” and “poor attentional control.” Singer believed daydreaming was a positive adaptive behavior—­a bold departure from the conventional wisdom at the time, which linked daydreaming to other psychopathologies such as excessive fantasizing. As one student of Singer’s </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3779797/" rel="">noted</a><span>, Singer’s work found strong associations between daydreaming and the personality trait “openness to experience,” which demonstrates sensitivity, curiosity, and willingness to explore new ideas and feelings.</span></p><p><span>Since then, researchers have found numerous positive effects of a wandering mind. Psychologist Scott Barry Kaufman </span><a href="https://www.scientificamerican.com/blog/beautiful-minds/mind-wandering-a-new-personal-intelligence-perspective/" rel="">summarized</a><span> them:</span></p><blockquote><p>“self-awareness, creative incubation, improvisation and evaluation, memory consolidation, autobiographical planning, goal driven thought, future planning, retrieval of deeply personal memories, reflective consideration of the meaning of events and experiences, simulating the perspective of another person, evaluating the implications of self and others’ emotional reactions, moral reasoning, and reflective compassion.”</p></blockquote><p><span>Daydreaming is also a prompt to memory. As Stefan Van der Stigchel </span><a href="https://mitpress.mit.edu/9780262538565/concentration/" rel="">argues</a><span> in </span><em>Concentration: Staying Focused in Times of Distraction</em><span>, “When you are daydreaming (or </span><em>mind-wandering</em><span>, as it is referred to within scientific circles), memories that you thought were lost forever can come to the surface again.” He adds, “The neural activity that can be observed when you are daydreaming is very similar to that found in the ‘default network,’ a network of regions in the brain that are active during periods of rest.”</span></p><p><span>It can be a challenge to find those periods of rest throughout our day, and when we do, if we are habituated to the stimulation technology provides, it is difficult to quiet our minds. As Moshe Bar </span><a href="https://www.amazon.com/Mindwandering-Constant-Mental-Improve-Creativity/dp/0306925303" rel="">argues</a><span> in </span><em>Mindwandering</em><span>, “the greater challenge is freeing ourselves from the distractions within, which disrupt our attention and intrude on the quality of our experience even when we are in a perfectly quiet place." In other words: we must cultivate habits that allow for mindwandering and daydreaming. We must, every day, try to reclaim the time that technology has colonized.</span></p><p>Why? Anecdotally, history provides many examples of scientific breakthroughs—­“aha!” moments—­that arose during moments of daydreaming or downtime: René Descartes in bed staring at a fly on the ceiling and coming up with coordinate geometry; Albert Einstein’s glimpse of the Bern tower on a streetcar ride prompting the theory of special relativity; the walk in the woods that prompted Nikola Tesla to devise alternating electrical current.</p><p><span>Unstructured, unmediated time is especially important for the development of creativity in children. “In the space between anxiety and boredom was where creativity flourished,” </span><a href="https://www.amazon.com/Mindwandering-Constant-Mental-Improve-Creativity/dp/0306925303" rel="">wrote</a><span> Po Bronson and Ashley Merryman in their examination of declining scores on the Torrance Test for creativity among American children. They hypothesize that one of the reasons creativity scores might be declining is children’s increased use of screen-­based technologies during downtime. Rather than being left to their own imaginative devices, their wandering minds are often captured by devices—­smartphones and other screens that grasp their attention and, in the process, prevent all other possible uses of those moments of idle time.</span></p><p>Now that we have so many ways to fill even the smallest fragments of time, a subtle shift in our psychology of expectation also follows. We are more likely to experience waiting as an unpleasant delay rather than as anticipation. Waiting has become a problem to be solved, rather than a normal human experience. When we are accustomed to easily filling time, opportunities for anticipation, like opportunities for daydreaming, disappear.</p><p><span>Anticipation is a kind of preparation for the future. Actively embracing anticipation is also important for one’s emotional health. Neuroscientist Antonio Damasio </span><a href="https://www.amazon.com/Descartes-Error-Emotion-Reason-Human/dp/014303622X" rel="">calls</a><span> this practice the “imagination response,” and in many ways it resembles daydreaming in its power to prepare the mind for new experiences. Damasio describes an unusual patient, Elliot, who could rationally think through positive and negative likely outcomes for his behavior and could experience happiness or disappointment once something happened to him. What Elliot couldn’t do was imagine or preview those future feelings. Without a functioning imagination response, he could think about the future rationally, but he couldn’t feel it emotionally. As a result, he was usually indecisive and impulsive, which caused him unhappiness.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.afterbabel.com/p/on-the-death-of-daydreaming?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.afterbabel.com/p/on-the-death-of-daydreaming?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>Does it matter if we no longer tolerate boredom, let our minds wander, cultivate a sense of anticipation, and practice patience? Our demand for immediate answers is voracious, and not entirely a bad thing. It drives innovation and commerce and has allowed for communication on a scale barely imaginable a century ago. But living a full meaningful human life means coping with the liminal, those in-­between moments of life when we must endure uneasy or uncomfortable experiences, from boredom during a meeting to bearing witness to another’s illness, to simply being stuck on a bus. In everyday life, we can all try, however modestly, to shift our individual perceptions and behavior by embracing a more generous sense of anticipation and a healthier attitude about delay, by reframing waiting as an opportunity for daydreaming and idle time rather than an excuse for distraction, and by trying to be more patient with one another. Such advice does at least have a long pedigree. Aristotle is said to have warned, “Patience is bitter, but its fruit is sweet.”</p><p>Parents have a crucial role to play in teaching children how to deal with boredom, and it can be as easy and as old-school as simply telling them: “Go outside and play.” Instead of handing a child a slot machine of distraction, encourage them to come up with their own game or activity. Rather than structuring and organizing an activity for your children, let them figure that out for themselves, or with their peers. Children are extraordinarily creative when given the space and time to indulge their wandering minds, but this often requires first overcoming the immediate challenge of handling their frustration and boredom. Placing the burden of alleviating one’s boredom back on a child isn’t a punishment; it’s an opportunity for them to find creative solutions to their discomfort and, as they mature into adults, to identify and cope with feelings of frustration.</p><p>As well, parents should model better behavior by resisting the temptation to pick up our phones whenever we are bored. Try this experiment: For one day, do not pick up your smartphone during small breaks in your routine, such as waiting for the train, or sitting in your car at a stoplight. If you find yourself in a doctor’s waiting room, or waiting for a friend at a restaurant, don’t pick up your phone to fill those few minutes. Pay attention to what is around you, or let your mind wander. This sounds like a simple experiment, but as someone who repeatedly tries and often fails to do this, it is revealing of our own bad habits and a useful prompt for thinking more critically about how we spend our time. Reaching for the phone every time is the easy fix, but it is one that has damaging long-term consequences for individuals and for society.</p><p>In other words: a bit of boredom is good for us, so the next time you have a minute to spare, instead of reaching for your phone, be rebellious: Daydream.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Beauty of Having a Pi-Hole (190 pts)]]></title>
            <link>https://den.dev/blog/pihole/</link>
            <guid>43894175</guid>
            <pubDate>Mon, 05 May 2025 12:06:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://den.dev/blog/pihole/">https://den.dev/blog/pihole/</a>, See on <a href="https://news.ycombinator.com/item?id=43894175">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <header>
      
        <ol>
  
  
    
  
    
  
  <li>
    <a href="https://den.dev/">Hi, I'm Den 👋</a><span>/</span>
  </li>

  
  <li>
    <a href="https://den.dev/blog/">Writing</a><span>/</span>
  </li>

  
  <li>
    <a href="https://den.dev/blog/pihole/">The Beauty Of Having A Pi-hole</a><span>/</span>
  </li>

</ol>


      
      
      <div>
        





  
  



  

  
  
    
  

  

  
    
  

  

  

  <p><time datetime="2024-08-02 00:00:00 +0000 UTC">August 2, 2024</time><span>·</span><span>1592 words</span>
    

    
    
  </p>

  
  
    
  


      </div>
      
    </header>
    <div>
        <p>So, check this little idea that I have - I want to browse the internet without all sorts of unscrupulous actors collecting every little bit of metadata on me and my family they can possibly get their hands on. Radical, I know. Who would’ve thought that I don’t wont every single website I visit syphon metadata about my computer, my browser, and other signals that can help them build a profile without my consent.</p>
<p>Unfortunately, the experience of browsing the web or using software in 2024 boils down to fielding a barrage of ads, malicious scripts, analytics widgets, chatbot widgets, massive “take over your page” banners asking you to consent to cookies, software that won’t shut up and just phones home with every click, and other things that you probably didn’t want to have to deal with. Not that any of these things didn’t exist in some capacity in 2004 (or even 1994), but the sheer amount of privacy-abusing techniques was much smaller.</p>
<p>The whole concept of “ad tech” became “figure out how to abuse the living hell out of every visitor” which is, let’s say, suboptimal. I even alluded to it <a href="https://den.dev/blog/user-hostile-software/" target="_blank" rel="noreferrer noopener">in some of my previous writing</a>.</p>
<p>Which brings me to my main point - <strong>you should absolutely use a <a href="https://pi-hole.net/" target="_blank" rel="noreferrer noopener">Pi-hole</a> in your home network</strong>. It’s not exactly news that this project exists - folks like <a href="https://blog.codinghorror.com/an-exercise-program-for-the-fat-web/" target="_blank" rel="noreferrer noopener">Jeff Atwood</a>, <a href="https://www.troyhunt.com/mmm-pi-hole/" target="_blank" rel="noreferrer noopener">Troy Hunt</a>, <a href="https://www.hanselman.com/blog/blocking-ads-before-they-enter-your-house-at-the-dns-level-with-pihole-and-a-cheap-raspberry-pi" target="_blank" rel="noreferrer noopener">Scott Hanselman</a>, and <a href="https://scotthelme.co.uk/tag/pi-hole/" target="_blank" rel="noreferrer noopener">Scott Helme</a> talked about it <em>for years</em>. I am just adding another datapoint to the camp of “Get this on your network ASAP.”</p>
<p>If you’ve never heard about it, Pi-hole is software that runs on a Raspberry Pi (can technically run even outside a Pi) that acts as the Domain Name Service (DNS) proxy <em>within your own network</em>. That is, if you go to <code>https://example.com</code>, it first hits the Pi-hole before going out and asking authoritative DNS servers for domain information. Its main purpose is blocking requests to domains that you don’t want to hit from your network. Those can be trackers, ad-serving content delivery networks (CDNs), or just any domain that you deem not worthy of receiving data from within your house or place of business.</p>
<p>To give you an idea of how much <em>completely unnecessary</em> network traffic there is, on my own network a whopping <strong>66.6% of all traffic is blocked</strong> with no functional impact on anything that I do. This tells you just how much various devices, sites, and apps like to either phone home or load things that I do not want.</p>





<figure>
  
  <img alt="Screenshot of the Pi-hole dashboard in a web browser." src="https://assets.den.dev/images/postmedia/pihole/pi-hole-stats.png">
  
  <figcaption>Screenshot of the Pi-hole dashboard in a web browser.</figcaption>
</figure>
<h2 id="the-kit">The kit <span><a href="#the-kit" aria-label="Anchor">#</a></span></h2>
<p>Setting Pi-hole up doesn’t take a significant investment. This is a list of recommendations, but of course you can fly solo and come up with your own configuration that suits your needs better. On the surface, you will need:</p>
<ul>
<li>A <a href="https://www.raspberrypi.com/" target="_blank" rel="noreferrer noopener">Raspberry Pi</a>. If you’re in the US, you can get a <a href="https://www.canakit.com/raspberry-pi-5-8gb.html?cid=USD" target="_blank" rel="noreferrer noopener">CanaKit</a> starter kit for roughly $155, which also includes a microSD card you will need for the setup.</li>
<li>A monitor, mouse, and keyboard that you can plug into the Raspberry Pi as you set it up.</li>
<li>Minimal time to <a href="https://docs.pi-hole.net/main/basic-install/" target="_blank" rel="noreferrer noopener">follow instructions on Pi-hole setup</a>.</li>
<li>Some time to make sure that you set up your network to route DNS requests through the Pi-hole.</li>
</ul>
<p>And that’s about it. Frankly, the biggest investment here is time - getting everything set up and validated. Worry not, though - the Pi-hole team have done a marvelous job in making sure that the installation process is as simple as possible.</p>
<h2 id="domain-lists">Domain lists <span><a href="#domain-lists" aria-label="Anchor">#</a></span></h2>
<p>Once you set up the Pi-hole hardware and software, as well as configure your router to point to the Pi-hole device as the target for DNS requests, you will also need to configure <em>which domains to block</em>. You can do that yourself, for example by seeing what flows through your network and then deciding whether you want it or not, or you can use <em>community blocklists</em>. I highly recommend looking at <a href="https://firebog.net/" target="_blank" rel="noreferrer noopener">Firebog</a> as the starting point for your explorations as the community has done extensive research and collected a myriad of domains that you likely haven’t heard about.</p>
<div>
  <p><img src="https://assets.den.dev/images/shared/sweaty.gif" alt="Mole out of the ground, sweating.">
  </p>
  <p>Oooofff... There are quite a few lists there. Do I need to use every single one when I configure my own Pi-hole instance?</p>
</div>
<p>Absolutely not, but it’s a great reference point. What I found is that as you start setting things up you will notice that certain things will break or not work. Conveniently, you can look at the <em>live query log</em> that I mentioned earlier, that will tell you what clients are trying to access specific domains. You can use that view to dynamically block or unblock domains as you see fit.</p>





<figure>
  
  <img alt="Screenshot of the Pi-hole query log in a web browser." src="https://assets.den.dev/images/postmedia/pihole/query-log.png">
  
  <figcaption>Screenshot of the Pi-hole query log in a web browser.</figcaption>
</figure>
<p>Not only that, but you can also set up rules to block domains that fit a certain criteria via regular expressions. For example, and because I see a lot of malicious traffic sourced from Russia, China, and Hong Kong, I can easily block those TLDs:</p>
<p>Now, anything that tries to talk to any server with those TLDs will not escape your network if the requests are routed through the Pi-hole. Now, of course, a country TLD is not necessarily the only attack vector for malware, but blocking them is another small step in terms of improving the overall network security posture.</p>
<h2 id="preventing-devices-bypassing-dns-settings">Preventing devices bypassing DNS settings <span><a href="#preventing-devices-bypassing-dns-settings" aria-label="Anchor">#</a></span></h2>
<p>As you set things up and configure the network-wide DNS provider to be your Pi-hole, you might also not realize that quite a few devices <em>bypass your DNS settings</em> to make sure that they can still serve you ads or collect analytics. To solve this you need to use a trick that is dependent on <em>what hardware you’re using for your router</em>. In my case, because I am within the UniFi ecosystem and using a UDM Pro, I can run the following set of commands when I SSH into the UDM itself:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>iptables -t nat -A PREROUTING ! -s YOUR_PI_HOLE_IP -p tcp --dport <span>53</span> -j DNAT --to YOUR_PI_HOLE_IP
</span></span><span><span>iptables -t nat -A PREROUTING ! -s YOUR_PI_HOLE_IP -p udp --dport <span>53</span> -j DNAT --to YOUR_PI_HOLE_IP
</span></span><span><span>
</span></span><span><span><span># Make sure that we skip 192.168.1.1 since that seems to break UniFi Protect</span>
</span></span><span><span>iptables -t nat -A POSTROUTING -m iprange --src-range 192.168.1.2-192.168.254.254 -j MASQUERADE
</span></span></code></pre></div><p>The Bash script above configures the <code>iptables</code> rules to redirect <em>all</em> DNS traffic (port 53) to my Pi-hole and applies Network Address Translation (NAT) rules to allow for network address masquerading, which is a technique that allows to abstract away the internal network from the public internet by replacing the source IP address with the address of the gateway (your router).</p>
<p>Let’s dissect these commands a bit more in-depth:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>iptables -t nat -A PREROUTING ! -s YOUR_PI_HOLE_IP -p tcp --dport <span>53</span> -j DNAT --to YOUR_PI_HOLE_IP
</span></span></code></pre></div><p>Here is what each argument does:</p>
<table>
<thead>
<tr>
<th>Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-t nat</code></td>
<td>Specifies that the rule is for the NAT table.</td>
</tr>
<tr>
<td><code>-A PREROUTING</code></td>
<td>Adds a rule to the <code>PREROUTING</code> chain, which processes inbound packets <em>before</em> routing.</td>
</tr>
<tr>
<td><code>! -s YOUR_PIHOLE_IP</code></td>
<td>Excludes packets that are originating from the Pi-hole itself.</td>
</tr>
<tr>
<td><code>-p tcp</code></td>
<td>Specifies that the rule applies to TCP packets.</td>
</tr>
<tr>
<td><code>--dport 53</code></td>
<td>Matches packets destined for port 53 (DNS).</td>
</tr>
<tr>
<td><code>-j DNAT</code></td>
<td>Jumps to the destination NAT (DNAT) target, modifying the destination IP address.</td>
</tr>
<tr>
<td><code>--to YOUR_PIHOLE_IP</code></td>
<td>Finally, redirects the packets to the Pi-hole IP address.</td>
</tr>
</tbody>
</table>
<p>The second line does the <em>exact same thing</em> but for UDP packets:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>iptables -t nat -A PREROUTING ! -s YOUR_PI_HOLE_IP -p udp --dport <span>53</span> -j DNAT --to YOUR_PI_HOLE_IP
</span></span></code></pre></div><p>For the third command, we do the following:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>iptables -t nat -A POSTROUTING -m iprange --src-range 192.168.1.2-192.168.254.254 -j MASQUERADE
</span></span></code></pre></div><p>The arguments are somewhat similar to what we saw above, but let’s break them down too:</p>
<table>
<thead>
<tr>
<th>Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-t nat</code></td>
<td>Specifies that the rule is for the NAT table.</td>
</tr>
<tr>
<td><code>-A POSTROUTING</code></td>
<td>Adds a rule to the <code>POSTROUTING</code> chain, which processes inbound packets <em>after</em> routing.</td>
</tr>
<tr>
<td><code>-m iprange --src-range 192.168.1.2-192.168.254.254</code></td>
<td>Matches packets with source IP addresses in the range 192.168.1.2 to 192.168.254.254. You can adjust the ranges fro your own network.</td>
</tr>
<tr>
<td><code>-j MASQUERADE</code></td>
<td>Jumps to the <code>MASQUERADE</code> target, which replaces the source IP address with the router’s IP address for outgoing packets.</td>
</tr>
</tbody>
</table>
<p>Long story longer - the script ensures that all DNS requests (both TCP and UDP) from devices on the network are redirected to the Pi-hole, except for requests originating from the Pi-hole itself.</p>
<p>The <code>MASQUERADE</code> rule ensures that the source IP addresses in the specified range are masked with the router’s IP address, ensuring proper routing and response handling. In my special case, the IP range excludes 192.168.1.1 to avoid breaking UniFi Protect.</p>

<p>Despite having Pi-hole sitting between all the devices on my network and the rest of the internet, there is still a lot of value in having a <em>trusted</em> ad-blocker like <a href="https://ublockorigin.com/" target="_blank" rel="noreferrer noopener">uBlock Origin</a>. That’s mainly because you will still run into domains that you can’t block (e.g., for YouTube) but still want to use the service ad-free (e.g., again - YouTube). I treat the Pi-hole as <em>another layer</em> in blocking unwanted content and requests in addition to an ad-blocker in the browser, which can also helpfully block specific UI elements, such as passive ads or sponsored content that loads from the primary website domain.</p>
<h2 id="conclusion">Conclusion <span><a href="#conclusion" aria-label="Anchor">#</a></span></h2>
<p>Once I set up the Pi-hole on my network, there is no going back. I did the same thing for my parents and in-laws, and will continue advocating it to everyone that wants to hear about it. It makes <em>that big</em> of a difference in the online quality of life.</p>

      </div>
    
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS Built a Security Tool. It Introduced a Security Risk (189 pts)]]></title>
            <link>https://www.token.security/blog/aws-built-a-security-tool-it-introduced-a-security-risk</link>
            <guid>43893906</guid>
            <pubDate>Mon, 05 May 2025 11:37:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.token.security/blog/aws-built-a-security-tool-it-introduced-a-security-risk">https://www.token.security/blog/aws-built-a-security-tool-it-introduced-a-security-risk</a>, See on <a href="https://news.ycombinator.com/item?id=43893906">Hacker News</a></p>
<div id="readability-page-1" class="page"><div fs-richtext-element="rich-text"><p><em>(If you missed the previous parts of this trust policy blog series, we recommend reading parts </em><a href="https://www.token.security/blog/iam-role-trust-policies-misconfigurations-hiding-in-plain-sight" target="_blank"><em>one</em></a><em> and </em><a href="https://www.token.security/blog/secure-cross-account-access-is-tricky-four-common-dangerous-misconceptions" target="_blank"><em>two</em></a><em> first)</em></p><p>In the previous post of this series, we explored four dangerous misconceptions regarding how to securely set up cross-account access in AWS environments.</p><p>In this final post of the series, we’ll walk through a real-world case where even AWS got it wrong. Their <em>Account Assessment for AWS Organizations</em> tool, designed to audit resource-based policies for risky cross-account access, ironically introduced cross-account privilege escalation risks due to flawed deployment instructions. Specifically, customers were effectively encouraged to deploy the tool in lower-sensitivity accounts, creating risky trust paths from insecure environments into highly sensitive ones.</p><p>We’ll share how we discovered the issue, the risks it introduced, how AWS fixed it, and what affected organizations should do to detect and remediate it.</p><h3>How it started</h3><p>While investigating a critical privilege escalation risk involving an IAM role in a customer’s AWS environment, we discovered a role present in both their production and management accounts, each of which trusted two roles in their development account:</p><figure><p><img src="https://cdn.prod.website-files.com/661a822ae40a7d51ecf449bc/680f4186cc6df42ec59bed95_blog3_image1.png" loading="lazy" alt=""></p><figcaption>The risky IAM role we investigated</figcaption></figure><p>These were the details of the privilege escalation risk (sensitive info redacted):</p><figure><p><img src="https://cdn.prod.website-files.com/661a822ae40a7d51ecf449bc/680f41b07a9e8cd78c0a74ed_blog3_image2.png" loading="lazy" alt=""></p></figure><p>Examining the permissions of that role in the production and management accounts, we found it had access to several sensitive IAM and data-related API calls, including:</p><ul role="list"><li><code>iam:ListRoles</code> (lists all IAM roles, helping an attacker identify privileged and vulnerable roles)</li><li><code>iam:ListPolicies</code> (reveals all IAM policies, exposing potential misconfigurations)</li><li><code>secretsmanager:ListSecrets</code> (lists all stored secret names, identifying potential targets)</li><li><code>s3:ListAllMyBuckets</code> (enumerates all S3 buckets, exposing potential sensitive data locations)</li><li><code>kms:ListKeys</code> (lists all encryption keys, indicating what is being encrypted)</li><li><code>kms:GetKeyPolicy</code> (retrieves key policies, which could reveal weak or misconfigured access controls)</li><li>… and more (full policy <a href="https://github.com/aws-solutions/account-assessment-for-aws-organizations/blob/eb194bd76d87481c2bc682b7f03321342c5b3cd3/source/infra/lib/account-assessment-spoke-stack.ts#L57" target="_blank">here</a>)</li></ul><p>These permissions were granted on all resources (<code>resource: "*"</code>), making them particularly dangerous if compromised.</p><p>The organization's dev account had weaker security, making this role assumption path a major risk. An attacker compromising a trusted role in dev could immediately gain these permissions in production and management. Combined with other misconfigurations - such as exposed IAM roles, secret names, KMS keys, or public S3 buckets - this could help lead to compromise of the organization's most sensitive accounts.</p><p>This was already a serious risk, but what we found next revealed that this misconfiguration was not unique to this organization - it was actually inadvertently encouraged by AWS itself.</p><h3>Account Assessment for AWS Organizations</h3><p>The names of these roles were somewhat quirky, and hinted they were part of some automated system. A quick search led us to <a href="https://aws.amazon.com/solutions/implementations/account-assessment-for-aws-organizations/" target="_blank">Account Assessment for AWS Organizations</a>, a tool developed by AWS and published in the <a href="https://aws.amazon.com/solutions/" target="_blank">AWS Solutions Library</a>.</p><p>According to its official documentation, the tool is designed to “centrally evaluate and manage AWS accounts within your AWS Organizations”, helping users better understand account dependencies. AWS lists its <a href="https://docs.aws.amazon.com/solutions/latest/account-assessment-for-aws-organizations/use-cases.html" target="_blank">primary use cases</a> as mergers and acquisitions, security audits, centralized policy explorer and management account transitions.</p><p><strong>So, why was it deployed insecurely?</strong></p><p>Given that this tool was built by AWS to audit resource-based cross-account access, we initially assumed the misconfiguration was a deployment error by the customer. However, after reviewing the documentation, we realized that AWS’s deployment instructions unintentionally encouraged insecure setups, making it highly likely that users would deploy the tool in a way that introduced privilege escalation risks.</p><p>The tool follows a hub-and-spoke architecture:</p><ul role="list"><li>A hub role is deployed in a designated hub account.</li><li>Spoke roles are deployed in all other accounts to be assessed, trusting the hub role for access.</li></ul><p>This design allows the hub role to assume the spoke roles across all accounts, aggregating security data across the organization.</p><figure><p><img src="https://cdn.prod.website-files.com/661a822ae40a7d51ecf449bc/680f41d9677b48b093129595_blog3_diag1.drawio%20(7).png" loading="lazy" alt=""></p></figure><p>When checking AWS's official deployment guidance, we were surprised to see the following instruction:</p><figure><p><img src="https://cdn.prod.website-files.com/661a822ae40a7d51ecf449bc/680f41f4791d880da8a99880_image%20(12).png" loading="lazy" alt=""></p></figure><p>"Hub stack - Deploy to any member account in your AWS Organization except the Organizations management account."</p><p>This is the root cause of the issue - AWS explicitly recommended not deploying the hub in the management account, without clarifying the security implications of the other possible choices.</p><h3><strong>Why this is a major security risk</strong></h3><p>Let’s back up a bit and explain why this instruction, without any clarification, was so problematic.</p><p>As we covered in the <a href="https://www.token.security/blog/secure-cross-account-access-is-tricky-four-common-dangerous-misconceptions" target="_blank">previous blog</a>, allowing a role in a less secure account to assume roles in more sensitive accounts creates a privilege escalation risk.</p><p>By design, the hub role must access all spoke accounts. Since AWS prohibited using the management account as the hub, customers were forced to deploy the hub in a less secure account - often a development, sandbox, or similarly low-sensitivity account. In deployed organizations, this led to the creation of a direct trust path from a lower-security account to higher-security accounts like production, PCI-DSS environments, and even the management account itself.</p><p>The following diagram visualizes the problem:</p><figure><p><img src="https://cdn.prod.website-files.com/661a822ae40a7d51ecf449bc/680f420f57a0fb21afc19cd2_blog3_diag2.drawio%20(3).png" loading="lazy" alt=""></p></figure><p>In the case we investigated, the customer deployed the hub role in their development account, which:</p><ol role="list"><li>(Reasonably) had weaker security controls than production or management.</li><li>Had full access to assume spoke roles across all accounts, including sensitive ones such as the management and production accounts</li></ol><p>This meant that if an attacker compromised the development account, they could pivot into the management account - make it much easier to gain control over the entire AWS organization.</p><p>AWS strongly advises against running workloads in the management account, so simply deploying the hub there is not an ideal solution either. Instead, the only relatively safe option is to choose an account <strong>which is as secure</strong> as the management account, such as a centralized DevOps or Infrastructure account with strict access controls.</p><p>However, some organizations don’t necessarily have such an account, meaning any deployment inherently introduces risk. The problem isn't just where to deploy the hub - it's that AWS's default recommendation pushed organizations into an insecure setup without any reference to the security implications of this choice.</p><h3>Implications for affected organizations</h3><p>Any organization that deployed this tool following AWS’s instructions (before they were fixed on 2025-01-28) with the hub role deployed in an account less secure than the management account - and hasn’t removed it - is at risk.</p><p>For affected orgs, the hub account (where the hub role is deployed) becomes a high-value target. If an attacker compromises this role, whether through direct access or privilege escalation, they can assume spoke roles across all linked accounts - including highly sensitive environments such as management and production.</p><p>Once inside, they can:</p><ul role="list"><li>Enumerate IAM roles, trust policies, and permissions, identifying weak points.</li><li>List all S3 buckets and secret names, mapping out valuable data targets.</li><li>Access KMS key policies, which may reveal misconfigured encryption controls.</li></ul><p>Additionally, the tool’s <a href="https://github.com/aws-solutions/account-assessment-for-aws-organizations/blob/b429bbf2f31dc2f813e903751b8acc5aa3d240ad/source/lib/components/resource-based-policy-component.ts#L10" target="_blank">predefined role names</a> make it even easier for attackers to exploit. If an attacker gains access to any AWS account within an affected organization, they can easily identify whether the tool is deployed and know the exact role names that can be assumed from the hub account. This reduces the effort needed to escalate privileges and pivot into high-value environments.</p><h3>Detecting and remediating</h3><p>Here are two methods to determine if your organization is affected:</p><ul role="list"><li>Use the AWS Console to manually search the IAM role page in any suspected accounts for the hub and spoke roles. These roles contain the strings <code>ScanSpokeResource</code> and <code>AccountAssessment-Spoke-ExecutionRole</code> in their names accordingly.</li><li>Run the following AWS CLI command to check if the tool is deployed in a given account: <code>aws iam list-roles --query "Roles[?contains(RoleName, 'ScanSpokeResource') || contains(RoleName, 'AccountAssessment-Spoke-ExecutionRole')]"</code></li></ul><p><strong>If the tool is deployed -</strong> Check to see when the tool was deployed by examining the roles’ <code>CreateDate</code> property. If deployed before 2025-01-28, we strongly recommend removing the current deployment unless the hub role is already in a high-security account (such as a dedicated Infrastructure or DevOps account).</p><ol role="list"><li>Uninstall the tool by deleting the CloudFormation stacks for the Hub, Spoke, and Org-Management components. See AWS’s <a href="https://docs.aws.amazon.com/solutions/latest/account-assessment-for-aws-organizations/uninstall-the-solution.html" target="_blank">official uninstallation guide</a> for more instructions.</li><li>Redeploy carefully - if the tool is still needed, deploy the hub role in an account with security equivalent to the management account to prevent privilege escalation risks.</li></ol><h3>Reporting and resolution</h3><p>After identifying this issue, we reported it to AWS Security, highlighting the serious risks created by the tool’s deployment guidance. AWS promptly reviewed our findings, acknowledged the security concern, and engaged with us to determine how best to update the documentation to prevent future misconfigurations.</p><p>Based on this engagement, AWS revised the <a href="https://docs.aws.amazon.com/solutions/latest/account-assessment-for-aws-organizations/aws-accounts.html" target="_blank">documentation</a> to explicitly advise customers to deploy the hub role in a high-security account to match the sensitivity of the accounts being assessed:</p><figure><p><img src="https://cdn.prod.website-files.com/661a822ae40a7d51ecf449bc/680f4228ed41b221c8281a94_image%20(13).png" loading="lazy" alt=""></p><figcaption>Updated documentation page, explicitly instructing to deploy the hub in an account as secure as all scanned accounts</figcaption></figure><p>‍</p><p>We thank the AWS Security team, who were highly responsive, open to feedback, and committed to improving customer security. They took the issue seriously from the start, worked with us to determine the best way to update the documentation, and delivered a clear and effective fix - ensuring organizations deploying this tool can avoid unintended privilege escalation risks in the future.</p><h3><strong>Reporting timeline</strong></h3><ul role="list"><li><strong>2024-12-11</strong> - Discovery of the problematic documentation.</li><li><strong>2024-12-12</strong> - Initial report to AWS Security.</li><li><strong>2025-01-02</strong> - AWS acknowledges the issue and begins considering documentation changes.</li><li><strong>2025-01-05</strong> - We emphasize the importance of correcting the instructions.</li><li><strong>2025-01-07</strong> - AWS confirms documentation updates are needed.</li><li><strong>2025-01-13 - 2025-01-17 -</strong> Discussion on how to revise the documentation.</li><li><strong>2025-01-28</strong> - AWS publishes updated documentation with security fixes.</li><li><strong>2025-04-28</strong> - Findings and resolution publicly shared in this blog post.</li></ul><h3>Wrapping up</h3><p>Throughout this blog series, we explored how trust policy risks can slip into even well-managed AWS environments - sometimes through overlooked technical details, subtle misconceptions, or even official tooling. Securing trust relationships is less about following a checklist and more about developing a deeper understanding of how trust mechanisms actually behave in real-world environments.</p><p>We hope this series helped shed light on some of the hidden challenges around trust policies. Thanks for reading!</p><p>‍</p><p>‍</p><p><em>Token Security's machine-first identity security platform detects trust policy risks, including risky cross-account trust policies, whether caused by AWS tooling, human error, or overlooked configurations.</em></p><p><em>Want to see how it works? </em><a href="https://www.token.security/book-a-demo"><strong><em>Book a demo</em></strong></a><em> and let us show you how we help organizations stay ahead of these risks.</em></p><p>‍</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Judge said Meta illegally used books to build its AI (347 pts)]]></title>
            <link>https://www.wired.com/story/meta-lawsuit-copyright-hearing-artificial-intelligence/</link>
            <guid>43893762</guid>
            <pubDate>Mon, 05 May 2025 11:16:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/meta-lawsuit-copyright-hearing-artificial-intelligence/">https://www.wired.com/story/meta-lawsuit-copyright-hearing-artificial-intelligence/</a>, See on <a href="https://news.ycombinator.com/item?id=43893762">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><figure></figure><p><span>Meta’s </span><a href="https://www.wired.com/story/matthew-butterick-ai-copyright-lawsuits-openai-meta/">copyright battle</a> with a group of authors, including Sarah Silverman and Ta-Nehisi Coates, will turn on the question of whether the company’s AI tools produce works that can cannibalize the authors’ book sales.</p><p>US District Court Judge Vince Chhabria spent several hours grilling lawyers from both sides after they each filed motions for partial summary judgment, meaning they want Chhabria to rule on specific issues of the case rather than leaving each one to be decided at trial. The authors allege that Meta illegally used their work to build its generative AI tools, emphasizing that the company pirated their books through “shadow libraries” like LibGen. The social media giant is not denying that it used the work or that <a href="https://www.wired.com/story/new-documents-unredacted-meta-copyright-ai-lawsuit/">it downloaded books from shadow libraries</a> en masse, but insists that <a href="https://www.wired.com/story/battle-over-books3/">its behavior is shielded</a> by the “fair use” doctrine, an exception in US copyright law that allows for permissionless use of copyrighted work in certain cases, including parody, teaching, and news reporting.</p><p>If Chhabria grants either motion, he’ll issue a ruling before the case goes to trial—and likely set an important precedent shaping how courts deal with generative AI copyright cases moving forward. <em>Kadrey v. Meta</em> is one of the <a href="https://www.wired.com/story/ai-copyright-case-tracker/">dozens of lawsuits</a> filed against AI companies that are winding through the US legal system.</p><p>While the authors were heavily focused on the piracy element of the case, Chhabria spoke emphatically about his belief that the big question is whether Meta’s AI tools will hurt book sales and otherwise cause the authors to lose money. “If you are dramatically changing, you might even say obliterating, the market for that person's work, and you're saying that you don't even have to pay a license to that person to use their work to create the product that's destroying the market for their work—I just don't understand how that can be fair use,” he told Meta lawyer Kannon Shanmugam. (Shanmugam responded that the suggested effect was “just speculation.”)</p><p>Chhabria and Shanmugam went on to debate whether Taylor Swift would be harmed if her music was fed into an AI tool that then created billions of robotic knockoffs. Chhabria questioned how this would impact less-established songwriters. “What about the next Taylor Swift?” he asked, arguing that a “relatively unknown artist” whose work was ingested by Meta would likely have their career hampered if the model produced “a billion pop songs” in their style.</p><p>At times, it sounded like the case was the authors’ to lose, with Chhabria noting that Meta was “destined to fail” if the plaintiffs could prove that Meta’s tools created similar works that cratered how much money they could make from their work. But Chhabria also stressed that he was unconvinced the authors would be able to show the necessary evidence. When he turned to the authors’ legal team, led by high-profile attorney David Boies, Chhabria repeatedly asked whether the plaintiffs could actually substantiate accusations that Meta’s AI tools were likely to hurt their commercial prospects. “It seems like you’re asking me to speculate that the market for Sarah Silverman’s memoir will be affected,” he told Boies. “It’s not obvious to me that is the case.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>When defendants invoke the fair use doctrine, the burden of proof shifts to them to demonstrate that their use of copyrighted works is legal. Boies stressed this point during the hearing, but Chhabria remained skeptical that the authors’ legal team would be able to successfully argue that Meta could plausibly crater their sales. He also appeared lukewarm about whether Meta’s decision to download books from places like LibGen was as central to the fair use issue as the plaintiffs argued it was. “It seems kind of messed up,” he said. “The question, as the courts tell us over and over again, is not whether something is messed up but whether it’s copyright infringement.”</p><p>A ruling in the <em>Kadrey</em> case could play a pivotal role in the outcomes of the ongoing legal battles over generative AI and copyright. Earlier this spring, a judge <a href="https://www.wired.com/story/thomson-reuters-ai-copyright-lawsuit/">issued a partial summary judgment</a> in the first AI copyright case, <em>Thomson Reuters v. Ross</em>, siding with the publishing conglomerate Thomson Reuters in its fight against AI startup Ross Intelligence. While the ruling was important, that case was an outlier in several ways—including the fact that it didn’t involve generative AI tools like large language models.</p><p>The outcome of the Kadrey case is being closely watched—in part because it could shake up Silicon Valley. It will certainly have a major impact on Meta, whether it helps entrench the company’s generative AI strategy or forces a significant shift. CEO Mark Zuckerberg emphasized how central AI is to Meta’s present and future in an earnings call on Wednesday. “Everything that I've talked about today is built on top of our AI models and infrastructure,” he <a data-offer-url="https://www.facebook.com/zuck/posts/pfbid02XLbq8Sr6S1Y3QfvLZ1U161qf98Kn1en6Q5TwzwKTzHZ4PtoHU5p83xCYzRjHfcwfl" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.facebook.com/zuck/posts/pfbid02XLbq8Sr6S1Y3QfvLZ1U161qf98Kn1en6Q5TwzwKTzHZ4PtoHU5p83xCYzRjHfcwfl&quot;}" href="https://www.facebook.com/zuck/posts/pfbid02XLbq8Sr6S1Y3QfvLZ1U161qf98Kn1en6Q5TwzwKTzHZ4PtoHU5p83xCYzRjHfcwfl" rel="nofollow noopener" target="_blank">said</a>.</p><p>Chhabria has acknowledged how consequential the case is and how his decisions from the bench could upend whole sectors of tech and culture. “I will issue a ruling later today,” Chhabria said at the hearing’s end. “Just kidding! I will take a lot longer to think about it.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The vocal effects of Daft Punk (375 pts)]]></title>
            <link>https://bjango.com/articles/daftpunkvocaleffects/</link>
            <guid>43893601</guid>
            <pubDate>Mon, 05 May 2025 10:48:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bjango.com/articles/daftpunkvocaleffects/">https://bjango.com/articles/daftpunkvocaleffects/</a>, See on <a href="https://news.ycombinator.com/item?id=43893601">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="articlecontainer">
	<article>
		
		<p>Daft Punk have used a wide variety of vocal effects in their songs. A May 2001 interview in Remix magazine provided a rare insight from Daft Punk themselves on the topic.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/daft-punk-3.jpg" alt="A photo of Daft Punk in a field on a sunny day."></p>
<blockquote>
<p><a href="https://web.archive.org/web/20060103075925/http://remixmag.com/mag/remix_robopop/">“People always ask us what vocoder we use, but every one of our vocal tracks uses a different vocoder effect. We have the old Roland one [an SVC-350], Auto-Tune, and a DigiTech Vocalist.”</a></p></blockquote>
<p>The quote delivers some vital clues, but it’s incomplete, covering only their first two albums. There’s no mention of using a talk box, despite Around The World almost certainly using one. The quote makes it sound like the DigiTech Vocalist is a vocoder, but it’s not. And for that matter, which DigiTech Vocalist model? There’s around 30 pieces of hardware in DigiTech’s Vocalist series, and quite a few of them were around before Discovery’s release in 2001.</p>
<p>I’ve read comments suggesting the DigiTech Vocalist models with the “EX” suffix are special, but nobody seems to know why, and nobody has published a direct comparison to prove or disprove the theory. I decided to take on the challenge, and run the tests myself. Here’s a fraction of the Vocalist units I ended up buying.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/digitech-vocalist-family.jpg" alt="A photo of lots of DigiTech Vocalist models."></p>
<p>Why are there duplicates of the same model? Why is there a Korg ih in that photo? Before this article gets sidetracked with tests and some honestly quite interesting corporate partnerships, mergers, and lawsuits, here is a list of every Daft Punk album song containing robot-like vocal effects, and my guess on which piece of kit was used for the vocals.</p>
<table>
  <thead>
      <tr>
          <th>Album</th>
          <th>Song</th>
          <th>Effects</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Homework</td>
          <td>WDPK 83.7 FM</td>
          <td>Roland SVC-350</td>
      </tr>
      <tr>
          <td>Homework</td>
          <td>Around The World</td>
          <td>Talk box</td>
      </tr>
      <tr>
          <td>Homework</td>
          <td>Teachers</td>
          <td>Ensoniq DP/4+</td>
      </tr>
      <tr>
          <td>Homework</td>
          <td>Oh Yeah</td>
          <td>Ensoniq DP/4+</td>
      </tr>
      <tr>
          <td>Discovery</td>
          <td>One More Time</td>
          <td>Auto-Tune</td>
      </tr>
      <tr>
          <td>Discovery</td>
          <td>Digital Love</td>
          <td>DigiTech Vocalist</td>
      </tr>
      <tr>
          <td>Discovery</td>
          <td>Harder, Better, Faster, Stronger</td>
          <td>DigiTech Talker</td>
      </tr>
      <tr>
          <td>Discovery</td>
          <td>Something About Us</td>
          <td>DigiTech Vocalist</td>
      </tr>
      <tr>
          <td>Human After All</td>
          <td>Human After All</td>
          <td>DigiTech Talker</td>
      </tr>
      <tr>
          <td>Human After All</td>
          <td>The Prime Time Of Your Life</td>
          <td>DigiTech Talker</td>
      </tr>
      <tr>
          <td>Human After All</td>
          <td>Robot Rock</td>
          <td>DigiTech Talker</td>
      </tr>
      <tr>
          <td>Human After All</td>
          <td>The Brainwasher</td>
          <td>Tremolo</td>
      </tr>
      <tr>
          <td>Human After All</td>
          <td>Television Rules The Nation</td>
          <td>DigiTech Talker</td>
      </tr>
      <tr>
          <td>Human After All</td>
          <td>Technologic</td>
          <td>Ensoniq DP/4+</td>
      </tr>
      <tr>
          <td>Human After All</td>
          <td>Emotion</td>
          <td>Roland SVC-350</td>
      </tr>
      <tr>
          <td>Random Access Memories</td>
          <td>Give Life Back To Music</td>
          <td>Sennheiser VSM201</td>
      </tr>
      <tr>
          <td>Random Access Memories</td>
          <td>The Game Of Love</td>
          <td>Sennheiser VSM201</td>
      </tr>
      <tr>
          <td>Random Access Memories</td>
          <td>Within</td>
          <td>Sennheiser VSM201</td>
      </tr>
      <tr>
          <td>Random Access Memories</td>
          <td>Instant Crush</td>
          <td>Auto-Tune and VSM201</td>
      </tr>
      <tr>
          <td>Random Access Memories</td>
          <td>Lose Yourself To Dance</td>
          <td>Talker and VSM201</td>
      </tr>
      <tr>
          <td>Random Access Memories</td>
          <td>Touch</td>
          <td>Sennheiser VSM201</td>
      </tr>
      <tr>
          <td>Random Access Memories</td>
          <td>Get Lucky</td>
          <td>Sennheiser VSM201</td>
      </tr>
      <tr>
          <td>Random Access Memories</td>
          <td>Beyond</td>
          <td>Sennheiser VSM201</td>
      </tr>
      <tr>
          <td>Random Access Memories</td>
          <td>Fragments Of Time</td>
          <td>Talk box (synth solo)</td>
      </tr>
      <tr>
          <td>Random Access Memories</td>
          <td>Doin’ It Right</td>
          <td>Sennheiser VSM201</td>
      </tr>
  </tbody>
</table>
<h3 id="homework-notes-20-january-1997">Homework notes (20 January 1997)&nbsp;<a title="Link to heading" href="#homework-notes-20-january-1997">#</a></h3>
<p>There aren’t many robot vocal effects on Homework, but there is a lot of pitch shifting, likely provided by Daft Punk’s Ensoniq DP/4+, a digital multi-effects units that can do a variety of things. I don’t believe Daft Punk used the vocoder on the Ensoniq DP/4+ for Homework or any of their other albums. The Remix magazine quote says Ensoniq DP/4, but a <a href="https://i.redd.it/te31d8kbt2e11.jpg">gear list in another interview</a> says DP/4+. It doesn’t matter which model was used, as the pitch shifting and vocoder sound the same on both units.</p>
<h3 id="discovery-notes-12-march-2001">Discovery notes (12 March 2001)&nbsp;<a title="Link to heading" href="#discovery-notes-12-march-2001">#</a></h3>
<p>One More Time sounds like Auto-Tune in combination with a Mu-Tron Phasor or Moogerfooger. Harder, Better, Faster, Stronger uses a DigiTech Talker vocoder. Given the DigiTech Talker was used extensively for Human After All, maybe it was one of the last songs recorded for Discovery? The DigiTech Talker wasn’t mentioned in the May 2001 interview, despite its use on Discovery.</p>
<h3 id="human-after-all-notes-14-march-2005">Human After All notes (14 March 2005)&nbsp;<a title="Link to heading" href="#human-after-all-notes-14-march-2005">#</a></h3>
<p>DigiTech Talker and DigiTech Synth Wah are all over the entire album. But, did they use a DigiTech Synth Wah, or DigiTech Bass Synth Wah? They’re very similar pedals. The tremolo effect on The Brainwasher could have been done many ways. Maybe it was just an LFO modulating the amplitude on their Roland S-760 sampler? Maybe it was a guitar pedal? It’s an easy effect that can be achieved many different ways.</p>
<h3 id="random-access-memories-notes-17-may-2013">Random Access Memories notes (17 May 2013)&nbsp;<a title="Link to heading" href="#random-access-memories-notes-17-may-2013">#</a></h3>
<p>In Lose Yourself To Dance, the “everybody’s dancing on the floor” vocals sound very crunchy and DigiTech Talker-like. The vocodeded vocals in Touch sound like a Sennheiser VSM201 switched to unvoiced, or using white noise as the vocoder’s carrier. Instant Crush could be Auto-Tune or some other kind of harmoniser. It sounds like Instant Crush constains some Sennheiser VSM201 chord layers in places.</p>
<hr>
<h3 id="talk-boxes">Talk boxes&nbsp;<a title="Link to heading" href="#talk-boxes">#</a></h3>
<p>Daft Punk’s vocal effects can be broadly split into three categories: Talk boxes, vocoders, and harmonisers. They all sound vaguely similar and robot-like, and you could be forgiven for confusing them, but they’re extremely different techniques and technologies.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/heil-talk-box.jpg" alt="A photo of a Heil Talk Box."></p>
<p>Talk boxes are relatively simple devices — they’re a speaker in a sealed box with a small opening. One end of a hose is fitted to the opening, and the other end is placed into the performer’s mouth, blasting noise towards their throat. The performer can pretend to speak, shaping and filtering the sound coming out of the tube with their vocal tract. A microphone is then needed to record the resulting sound. A keyboard or guitar is typically connected to the talk box unit as the sound source for the speaker. This lets the keyboard or guitar sound like it’s singing. If you’ve heard <a href="https://www.youtube.com/watch?v=UlAa0dc-E4w">Chromeo</a>, 2Pac’s <a href="https://www.youtube.com/watch?v=J7_bMdYfSws">California Love</a>, Peter Frampton’s <a href="https://youtu.be/y7rFYbMhcG8?feature=shared&amp;t=349">Do You Feel Like We Do</a>, or Bon Jovi’s <a href="https://www.youtube.com/watch?v=lDK9QqIzhwk">Livin’ On A Prayer</a> before, you’ve heard a talk box.</p>
<p>I can confirm firing loud sounds into your mouth while holding a tube with your teeth is a bit uncomfortable. In terms of vocal effects used by Daft Punk, I think talk box might be the least used and least interesting, in terms of hunting down the exact hardware used. Talk boxes are simple devices and typically all sound similar. The sound source and performance play a bigger role in the result than the hardware itself.</p>
<p>Also, there aren’t many talk boxes on the market. Daft Punk may have used a Heil Talk Box, a Rocktron Banshee, a home made talk box, or something else. The MXR M222 Talk Box is probably the best option if you’re looking to buy a talk box today, because it has a built in amplifier. The MXR wasn’t around when <a href="https://www.youtube.com/watch?v=K0HSD_i2DvA">Around The World</a> was created though, so that’s not the unit they used.</p>
<p>Daft Punk’s early albums extensively used a Roland Juno-106, so it’s likely that was the sound source for the talk box used on Around The World. It sounds like a sawtooth wave with the filters open.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/daft-punk-juno-106.jpg" alt="A photo of Thomas Bangalter playing a Roland Juno-106 and Roland TR-909."></p>
<p>Even though they’ve been around in a commercial form since the mid 70s, talk boxes aren’t the first device to use human vocal tracts to create robotic sounds — the <a href="https://www.youtube.com/watch?v=kH-krlgo2e8">Sonovox</a> from 1939 takes that prize.</p>
<hr>
<h3 id="vocoders">Vocoders&nbsp;<a title="Link to heading" href="#vocoders">#</a></h3>
<p>Vocoders are a bit like an electronic version of a talk box. Vocoders take two audio inputs — often a voice and a synth — and combine them by filtering the synth with the voice’s frequency response. The filtering is usually done by splitting the signal into frequency bands. The volume of each voice band sets the volume of the repective synth band. More bands usually means a higher quality and more intelligible result. I’ve been calling the inputs “voice” and “synth”, but they’re often referred to as the modulator and carrier. The modulator filters the carrier.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/vocoders.jpg" alt="A photo of vocoders, including a DigiTech Talker and Ultimate VoIS."></p>
<p>Vocoders can be analogue or digital. Good analogue vocoders are physically big and very expensive, due to their complexity, especially if they have lots of frequency bands. They’re also a specialty effect, and therefore usually not mass produced.</p>
<p>The peak for high-end analogue vocoders was the 1970s — the EMS Vocoder 5000 was released in 1976, the Bode/Moog Vocoder in 1977, and the Sennheiser VSM201 in 1977. It’s hard to know exactly how many Sennheiser VSM201s were built, but the highest serial number I’ve seen is 40. The photo below is of the unit with serial number 21. Why should we care about that serial number? It’s the <a href="https://reverb.com/au/item/72543188-1977-sennheiser-vsm201-vocoder-20-band-analog-voice-synthesizer-rare-vocal-microphone-mic-effect-synth-used-by-daft-punk">exact unit Daft Punk used</a> on Random Access Memories, rented from <a href="https://audiorents.com/?s=VSM-201">Audio Rents in Los Angeles</a> for the sessions. Serial number 21 was sold, but Audio Rents have another VSM201, if you’re keen on renting one.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/sennheiser-vsm201.jpg" alt="A photo of a Sennheiser VSM201 with the serial number 21."></p>
<p>The best modern analogue vocoder I’ve heard, by far, is the <a href="https://vocoder.hoerold.com/Ultimate-VoIS.html">Dromedary Modular Ultimate VoIS</a>. It shares a lot of similarities with the Sennheiser VSM201, including accurate voiced/unvoiced detection, silence bridging, and other features. It’s a fraction of the price of a vintage vocoder, and still in production. If you’re looking for a high-end vocoder, it is the one to get.</p>
<p>What about vocoders used by Daft Punk prior to Random Access Memories? It’s incredibly likely Daft Punk used a DigiTech Talker on <a href="https://www.youtube.com/watch?v=gAjR4_CbPpQ">Harder, Better, Faster, Stronger</a> and many of the songs on Human After All. The DigiTech Talker is a digital vocoder, sold as a guitar pedal.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/digitech-talker.jpg" alt="A photo of a DigiTech Talker."></p>
<p>I said “incredibly likely”, because there’s another vocoder that sounds eerily similar. That’s no coincidence — even though the Talker has DigiTech’s name on the front of the pedal, it was designed and manufactured by IVL Technologies in Canada. IVL also teamed up with Electrix to build the Warp Factory vocoder, which sounds very similar, but not identical. It’s pretty clear they’re running the same algorithm.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/warp-factory.jpg" alt="A photo of an Electrix Warp Factory."></p>
<p>IVL specialises in vocal effects and voice processing. They’re still around, but known under a different name — in 2000, TC Helicon was formed as a joint venture between TC Electronic and IVL Technologies. TC Group, the parent company of TC Electronic, took full ownership of TC Helicon in 2005. And finally, Music Tribe (Behringer) purchased TC Group in 2015.</p>
<p>TC Helicon still make a vocoder guitar pedal called the <a href="https://www.tc-helicon.com/tchelicon/product?modelCode=0726-AAP">Talkbox Synth</a>. Sadly, it sounds nothing like the DigiTech Talker. It’s pretty good, but it doesn’t have the crunchy Daft Punk sound. Also, it has a terrible name. It’s not a talk box at all. It’s a vocoder. The DigiTech Talker has a similar naming issue — one of the settings is called “talk box”, and it’s also a vocoder. The recurring theme when researching this article was finding out about IVL/TC Helicon’s incorrectly named products and features.</p>
<p>Here’s a list of some vocoders, including the models being discussed. Many of these vocoders were used on well known songs. A MAM VF-11 was used on Intergalactic by Beastie Boys. Roland VP-330s were used on In The Air Tonight by Phil Collins, Radio Ga Ga by Queen, and I Just Called to Say I Love You by Stevie Wonder. A Korg DVP-1 was used by Air for lots of their songs, including Kelly Watch The Stars. As well as being used by Daft Punk, Sennheiser VSM201s have been used by Herbie Hancock, Giorgio Moroder, and Aphex Twin.</p>
<table>
  <thead>
      <tr>
          <th>Year</th>
          <th>Model</th>
          <th>Bands</th>
          <th>Notes</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>1976</td>
          <td>EMS Vocoder 5000</td>
          <td>22</td>
          <td>Analogue</td>
      </tr>
      <tr>
          <td>1977</td>
          <td>EMS Vocoder 2000</td>
          <td>16</td>
          <td>Analogue</td>
      </tr>
      <tr>
          <td>1977</td>
          <td>EMS Vocoder 3000</td>
          <td>16</td>
          <td>Analogue</td>
      </tr>
      <tr>
          <td>1977</td>
          <td>Bøde/Moog Vocoder</td>
          <td>16</td>
          <td>Analogue</td>
      </tr>
      <tr>
          <td>1977</td>
          <td>Sennheiser VSM201</td>
          <td>20</td>
          <td>Analogue</td>
      </tr>
      <tr>
          <td>1978</td>
          <td>Korg VC-10</td>
          <td>20</td>
          <td>Analogue</td>
      </tr>
      <tr>
          <td>1979</td>
          <td>Roland SVC-350</td>
          <td>11</td>
          <td>Analogue</td>
      </tr>
      <tr>
          <td>1979</td>
          <td>Roland VP-330</td>
          <td>10</td>
          <td>Analogue</td>
      </tr>
      <tr>
          <td>1986</td>
          <td>Korg DVP-1</td>
          <td>16</td>
          <td>Digital</td>
      </tr>
      <tr>
          <td>1997</td>
          <td>DigiTech Talker</td>
          <td>24 (order)</td>
          <td>Digital, linear predictive coding</td>
      </tr>
      <tr>
          <td>1999</td>
          <td>Eletrix Warp Factory</td>
          <td>1 to 24 (order)</td>
          <td>Digital, similar to DigiTech Talker</td>
      </tr>
      <tr>
          <td>2000</td>
          <td>MAM VF-11</td>
          <td>11</td>
          <td>Analogue</td>
      </tr>
      <tr>
          <td>2000</td>
          <td>Next! Vox 11</td>
          <td>11</td>
          <td>Identical to MAM VF-11</td>
      </tr>
      <tr>
          <td>2000</td>
          <td>FAT Procoder PCP330</td>
          <td>11</td>
          <td>Identical to MAM VF-11</td>
      </tr>
      <tr>
          <td>2002</td>
          <td>Alesis ModFx Metavox</td>
          <td>38</td>
          <td>Digital</td>
      </tr>
      <tr>
          <td>2017</td>
          <td>TC Helicon Talkbox Synth</td>
          <td>?</td>
          <td>Digital</td>
      </tr>
      <tr>
          <td>2019</td>
          <td>Behringer VC340</td>
          <td>10</td>
          <td>Analogue, Roland VP-330 clone</td>
      </tr>
      <tr>
          <td>2018</td>
          <td>Electro-Harmonix V256</td>
          <td>8 to 256</td>
          <td>Digital</td>
      </tr>
      <tr>
          <td>2024</td>
          <td>Ultimate VoIS</td>
          <td>18</td>
          <td>Analogue</td>
      </tr>
  </tbody>
</table>
<p>Having more filter bands typically increases the quality of the results, but I wouldn’t consider the Metavox or V256 to be anywhere near the best in terms of intelligibility. The EMS Vocoder 5000 and Sennheiser VSM201 are often considered to be the best vocoders ever made.</p>
<p>The DigiTech Talker and Eletrix Warp Factory use linear predictive coding (LPC), rather than bandpass filters. The value shown in the bands column for those models is the LPC order.</p>
<h3 id="vocoder-comparison">Vocoder comparison&nbsp;<a title="Link to heading" href="#vocoder-comparison">#</a></h3>
<p>If you’d like to hear many of these vocoders in action, I’ve created a <a href="https://www.youtube.com/watch?v=bkTFCPQMJkc">vocoder comparison video</a>. The comparison includes a Sennheiser VSM201, DigiTech Talker, the XILS 201 plugin, a Dromedary Modular Ultimate VoIS, and many other vododers. It compares the actual models used by Daft Punk as well as software clones, and the best modern analogue vocoder.</p>
<p><a href="https://www.youtube.com/watch?v=bkTFCPQMJkc"><img src="https://bjango.com/images/articles/daftpunkvocaleffects/video-vocoders.jpg"></a></p>
<hr>
<h3 id="harmonisers">Harmonisers&nbsp;<a title="Link to heading" href="#harmonisers">#</a></h3>
<p>Harmonisers are very different to talk boxes and vocoders. There’s no filtering involved, and they don’t require two audio sources — they work directly with one audio signal, often a vocal, altering its pitch. Harmonisers are a digital effect.</p>
<p>There’s two main parts to the digital algorithm used by harmonisers — pitch detection, and pitch shifting. Pitch detection figures out the fundamental frequency of the signal. Once the pitch is known, lots of possibilities open up.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/digitech-vocalist-family-2.jpg" alt="A photo of several harmonizers, including two DigiTech Studio Vocalist EX modesl."></p>
<p>If the pitch is rapidly shifted to a nearby note in the song’s key, the result sounds like <a href="https://www.youtube.com/watch?v=FGBhQbmPwH8&amp;t=47s">One More Time</a> or <a href="https://www.youtube.com/watch?v=a5uQMwRMHcs&amp;t=81">Instant Crush</a>. Using <a href="https://www.antarestech.com/">Auto-Tune</a> with extreme settings is one way to achieve this effect, but it can be done with most harmonisers and is often called “hard tune”.</p>
<p>Some harmonisers can have keyboards connected to them, and use the notes played on the keyboard to determine which note to shift the vocal to. Holding multiple keys can create harmonies. <a href="https://www.youtube.com/watch?v=UYIAfiVGluk">Hide and Seek</a> by Imogen Heap is a great example of this effect in action. In Hide and Seek, the original main vocal can also be heard, blended in with the harmonies.</p>
<p><a href="https://www.youtube.com/watch?v=FxzBvqY5PP0">Digital Love</a> and <a href="https://www.youtube.com/watch?v=sOS9aOIXPEk">Something About Us</a> were created using the same technique, where a MIDI keyboard or sequencer was used to control the pitch of the vocal. Daft Punk have said they used a DigiTech Vocalist, which is a strong indication that’s what was used for those two tracks. Both songs were released on 12 March 2001, as part of the Discovery album. Given the time required for mixing, mastering, and physical media production, I’d guess they must have used a model from before late 2000. There’s 7 DigiTech Vocalist models with MIDI pitch control that were released before then.</p>
<p>The earlier models incorrectly call the MIDI control feature “vocoder”, and later models call it “MIDI notes mode” or “notes harmony mode”. The list below shows all the DigiTech Vocalist models with MIDI notes mode. There’s another 20 or so models that can’t be controlled via a keyboard.</p>
<p>Interestingly, Imogen Heap also used a DigiTech Vocalist to record Hide and Seek, and has used a <a href="https://youtu.be/-Scn74BnDz4?feature=shared&amp;t=1983">TC Helicon VoiceLive 2</a> when <a href="https://www.youtube.com/watch?v=dHk2lLaDzlM">performing the song live</a>. Which DigiTech Vocalist? I emailed Imogen’s team and to my surprise, they responded, confirming a DigiTech Vocalist Workstation EX was used for recording Hide and Seek.</p>
<table>
  <thead>
      <tr>
          <th>Year</th>
          <th>Model</th>
          <th>Sampling</th>
          <th>Mic power</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>1991</td>
          <td>Vocalist VHM5</td>
          <td>16-bit, 31.25kHz</td>
          <td>None</td>
      </tr>
      <tr>
          <td>1993</td>
          <td>Studio Vocalist</td>
          <td>18-bit, 48kHz</td>
          <td>+48V</td>
      </tr>
      <tr>
          <td>1995</td>
          <td>Vocalist II</td>
          <td>16-bit, 31.25kHz</td>
          <td>None</td>
      </tr>
      <tr>
          <td>1996</td>
          <td>MIDI Vocalist</td>
          <td>16-bit, 31.25kHz</td>
          <td>None</td>
      </tr>
      <tr>
          <td>1996</td>
          <td>Vocalist Workstation</td>
          <td>18-bit, 48kHz</td>
          <td>None</td>
      </tr>
      <tr>
          <td>1998</td>
          <td>Vocalist Access</td>
          <td>16-bit, 44.1kHz</td>
          <td>None</td>
      </tr>
      <tr>
          <td>1998</td>
          <td>Studio Vocalist EX</td>
          <td>18-bit, 48kHz</td>
          <td>+48V</td>
      </tr>
      <tr>
          <td>2000</td>
          <td>Vocalist VR</td>
          <td>16-bit, 44.1kHz</td>
          <td>None</td>
      </tr>
      <tr>
          <td>2001</td>
          <td>Vocalist Workstation EX</td>
          <td>18-bit, 48kHz</td>
          <td>None</td>
      </tr>
      <tr>
          <td>2008</td>
          <td>Vocalist Live Pro</td>
          <td>24-bit, 44.1kHz</td>
          <td>+48V</td>
      </tr>
      <tr>
          <td>2009</td>
          <td>Vocalist VL3D</td>
          <td>24-bit, 44.1kHz</td>
          <td>+48V</td>
      </tr>
  </tbody>
</table>
<p>Remember IVL Technologies from earlier in the article? Their logo is on all those Vocalist models, except the Vocalist Live Pro. Yes, IVL also designed and built almost all the DigiTech Vocalist units. The Vocalist Live Pro has a different logo on the back — 3db Research. And, the Vocalist VL3D has IVL and 3db Research’s logos on it. I don’t fully understand what went down, but 3db Research was created by ex-IVL staff, and Harman International accused TC Helicon of infringing <a href="https://patents.google.com/patent/US20140109751A1/en">patents relating to harmonisers</a>. TC Helicon counter sued, and won.</p>
<p>Hold up. Why is Harman involved? While IVL were busy merging with TC Group, forming TC Helicon, and being sold to Behringer, DigiTech were experiencing their own dramas — DigiTech sold to Harman International in 1990, then Samsung bought Harman in 2017. After a restructure, DigiTech ceased to exist in mid 2018. In 2022, <a href="https://digitech.com/announcing-the-comeback-of-the-dod-pedals/">DigiTech was purchased and revived by Cor-Tek</a>, who seem to be doing a great job. That’s why it’s possible to buy DigiTech pedals today. Unfortunately, they don’t currently sell any DigiTech Vocalists or Talkers, so you’ll have to hit the second hand market if you’re after either. <a href="https://www.youtube.com/live/RkzdNmBsyxw?feature=shared&amp;t=664">This JHS Pedals video has a full recap of the DOD and DigiTech history</a>.</p>
<p>IVL partnered with other companies, including Korg. The Korg ih Interactive Vocal Harmony appears to run the same algorithm as the DigiTech Vocalist series, so I purchased one of those to add to the tests. I also purchased a TC Helicon Perform VE and TC Helicon VoiceWorks. As noted above, TC Helicon is the modern incarnation of IVL, so maybe their harmonisers sound the same as the older DigiTech ones?</p>
<table>
  <thead>
      <tr>
          <th>Year</th>
          <th>Model</th>
          <th>Sampling</th>
          <th>Mic power</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>1995</td>
          <td>Korg ih</td>
          <td>16-bit, 31.25kHz</td>
          <td>None</td>
      </tr>
      <tr>
          <td>2003</td>
          <td>TC Helicon VoiceWorks</td>
          <td>24-bit, 44.1kHz or 48kHz</td>
          <td>+48V</td>
      </tr>
      <tr>
          <td>2017</td>
          <td>TC Helicon Perform VE</td>
          <td>24-bit, 44.1kHz</td>
          <td>+48V</td>
      </tr>
  </tbody>
</table>
<p>IVL also worked with DigiTech to create the Whammy WH-1, Whammy II, and Bass Whammy pedals, which makes sense, given the Whammy is a harmoniser. Those models likely shared code and algorithms with the Vocalist line. Later versions of the DigiTech Whammy weren’t built in collaboration with IVL. The earlier IVL Whammy models are held in high regard and their prices on the used market reflect that. However, the latest model, the Whammy V, does have a “classic” mode that is supposed to replicate the earler models.</p>
<h3 id="ex-models">EX models&nbsp;<a title="Link to heading" href="#ex-models">#</a></h3>
<p>Now we have the full history of DigiTech and IVL covered, we can talk about how the “EX” models fit into the timeline. In 1998, the Studio Vocalist EX was released, becoming the new flagship Vocalist model. The main upgrades were more patch storage, more presets, and an updated algorithm with better pitch recognition.</p>
<p>For the effect used on Digital Love and Something About Us, the improved pitch recognition would make an audible difference. But, in my tests, the difference between the non-EX and EX models is fairly subtle. Audible and better, but subtle. If I had to guess which unit Daft Punk used, my money would be on the Studio Vocalist EX, but a Vocalist Workstation or one of the other earlier models could have also been used.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/digitech-vocalist-workstation-ex.jpg" alt="A photo of a DigiTech Vocalist Workstation EX."></p>
<p>To confuse things even further, the Vocalist Workstation can run the EX firmware, and I was able to upgrade one of my own units from firmware 1.02 to 2.02 (2.02 is the EX version, with additional presets and patch storage). A Vocalist Workstation running firmware 2.02 sounds identical to a Vocalist Workstation EX to me. The internals to the Vocalist Workstation EX are a bit different to the Vocalist Workstation, but I don’t think any of the differences relate to the audio path. The EX is also heavier, at 840 grams vs the non-EX’s 700 grams. The weight difference is due to a useless strip of metal in the EX. No, I’m not kidding.</p>
<p>Without more evidence, it seems impossible to know which model Daft Punk used. They may have used a Vocalist VHM5, Studio Vocalist, Vocalist II, MIDI Vocalist, Vocalist Workstation, Vocalist Access, Studio Vocalist EX, or Vocalist VR. I’m not sure it matters, given how similar they all sound.</p>
<p>If you are looking for a device to replicate the effect, a Studio Vocalist EX, Vocalist Workstation EX, or Vocalist Workstation with the EX firmware are good choices. Some of the newer TC Helicon devices, like the VoiceLive 3 Extreme, Perform VE, and Perform VG are great, too. Just be aware that only specific TC Helicon vocal effect models have MIDI notes mode. Also, the Perform VE and Perform VG have been discontinued.</p>
<p>I would recommend against the Vocalist Live Pro or Vocalist VL3D. They don’t run IVL’s algorithm and they sound terrible. I would also recommend against getting a Korg ih — it does use IVL’s algorithm and sounds like a Vocalist Workstation, but there is a permanent chorus effect that can not be disabled.</p>
<h3 id="pitch-shifting">Pitch shifting&nbsp;<a title="Link to heading" href="#pitch-shifting">#</a></h3>
<p>The pitch shifting effect used on Teachers, Oh Yeah, and Technologic is similar to the harmoniser effect we’ve been discussing, but there’s no pitch detection involved — the audio is just shifted without trying to make it match any specific note. It’s highly likely Daft Punk used their Ensoniq DP/4+ to achieve the pitch shifting on Homework and Discovery.</p>
<h3 id="harmoniser-comparison">Harmoniser comparison&nbsp;<a title="Link to heading" href="#harmoniser-comparison">#</a></h3>
<p>If you’d like to hear many of these harmonisers in action, I’ve created a <a href="https://www.youtube.com/watch?v=10o0hCybeq4">harmoniser comparison video</a>, which also covers some of the modern TC Helicon models. Another interesting fact is that all the harmonisers tested use last note priority when more than one note is played at a time via MIDI.</p>
<p><a href="https://www.youtube.com/watch?v=10o0hCybeq4"><img src="https://bjango.com/images/articles/daftpunkvocaleffects/video-harmonizers.jpg"></a></p>
<hr>
<h3 id="synth-wah-vs-bass-synth-wah">Synth Wah vs Bass Synth Wah&nbsp;<a title="Link to heading" href="#synth-wah-vs-bass-synth-wah">#</a></h3>
<p>Human After All sounds like Human After All because of yet another DigiTech product — a digital envelope filter guitar pedal called the Synth Wah. Or, the Bass Synth Wah. It’s unclear which pedal was used, because they’re incredibly similar. Strangely, the Bass Synth Wah weights a lot more (340 vs 635 grams). I didn’t open the pedals up to find out why, but that’s a huge difference, given how alike they are. It might just be an internal metal weight, like the Vocalist Workstation EX has.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/digitech-synth-wah-bass-synth-wah.jpg" alt="A photo of DigiTech Synth Wah and DigiTech Bass Synth Wah guitar pedals."></p>
<p>There’s 7 effect types on each pedal, selected via the rightmost knob. Daft Punk favoured “filter 1” and “filter 2”. Here’s a list of the songs on Human After All, and the effect they probably used.</p>
<table>
  <thead>
      <tr>
          <th>Song</th>
          <th>Effect number</th>
          <th>Effect type</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Human After All</td>
          <td>6</td>
          <td>Filter 2</td>
      </tr>
      <tr>
          <td>The Prime Time Of Your Life</td>
          <td>6</td>
          <td>Filter 2</td>
      </tr>
      <tr>
          <td>Steam Machine</td>
          <td>5</td>
          <td>Filter 1</td>
      </tr>
      <tr>
          <td>The Brainwasher</td>
          <td>6</td>
          <td>Filter 2</td>
      </tr>
      <tr>
          <td>Television Rules The Nation</td>
          <td>2 and 6</td>
          <td>Env Down and Filter 2</td>
      </tr>
      <tr>
          <td>Technologic</td>
          <td>6</td>
          <td>Filter 2</td>
      </tr>
  </tbody>
</table>
<p>If <a href="https://www.youtube.com/watch?v=YKaN7a19jLc">Television Rules The Nation</a> does in fact use “env down”, then that’s a vital clue — the Synth Wah has “env down”, but the Bass Synth Wah does not. The pedals share effect types 3, 4, 5, and 6, but 1, 2, and 7 are different. It was the Synth Wah after all.</p>
<h3 id="credits">Credits&nbsp;<a title="Link to heading" href="#credits">#</a></h3>
<p>These amazing people helped make this article and the related videos a reality: Vocals performed by <a href="https://www.fiverr.com/soloheadmusic">Solohead</a>. Sennheiser VSM201 vocoder recording by <a href="https://www.youtube.com/@TalhaVocoding">Talha Vocoding</a>. Ensoniq DP/4+ vocoder recording by <a href="https://www.youtube.com/@_floeter">@_floeter</a>. DigiTech Studio Vocalist recording by Spencer D. Carson. Technical guidance, repairs, and EPROM programming by Cam Sanderson.</p>
<p>Keep living the gold and the silver dream.</p>
<p><img src="https://bjango.com/images/articles/daftpunkvocaleffects/daft-punk-2.jpg" alt="A photo of Daft Punk in front of Eiffel Tower at night."></p>

		<p><small>Published 5 May 2025.</small></p>
	</article>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trump announces 100% tariffs on movies ‘produced in foreign lands’ (200 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2025/may/04/trump-tariffs-foreign-movies</link>
            <guid>43893310</guid>
            <pubDate>Mon, 05 May 2025 09:47:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2025/may/04/trump-tariffs-foreign-movies">https://www.theguardian.com/us-news/2025/may/04/trump-tariffs-foreign-movies</a>, See on <a href="https://news.ycombinator.com/item?id=43893310">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><a href="https://www.theguardian.com/us-news/donaldtrump" data-link-name="in body link">Donald Trump</a> on Sunday announced on his Truth Social platform a 100% <a href="https://www.theguardian.com/us-news/trump-tariffs" data-link-name="in body link">tariff</a> on all movies “produced in Foreign Lands”, saying the US film industry was dying a “very fast death” due to the incentives that other countries were offering to draw American film-makers.</p><p>In his post, he claimed to have authorised the commerce department and the US trade representative to immediately begin instituting such a tariff.</p><p>“This is a concerted effort by other Nations and, therefore, a National Security threat,” Trump said in <a href="https://truthsocial.com/@realDonaldTrump/posts/114452117143235155" data-link-name="in body link">the Truth Social post</a>. “It is, in addition to everything else, messaging and propaganda!</p><p>“WE WANT MOVIES MADE IN AMERICA, AGAIN!” Trump added.</p><figure id="8ae2b780-47ed-4829-af63-0195c20dd2b3" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:4,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Republicans in Congress pull out all the stops to protect Trump tariffs&quot;,&quot;elementId&quot;:&quot;8ae2b780-47ed-4829-af63-0195c20dd2b3&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2025/may/04/republicans-congress-trump-tariffs&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>The commerce secretary, Howard Lutnick, posting on X said: “We’re on it.” Neither Lutnick nor Trump provided any details on the implementation. It was not immediately clear whether the move would target production companies, foreign or American, producing films overseas.</p><p>Film and television production in Los Angeles has fallen by nearly 40% over the last decade, according to FilmLA, a non-profit that tracks the region’s production. At the same time, governments around the world have offered more generous tax credits and cash rebates to lure productions, and capture a greater share of the $248bn that Ampere Analysis predicts will be spent globally in 2025 to produce content.</p><p>Politicians in Australia and New Zealand said on Monday they would advocate for their respective film industries, after the president’s announcement.</p><p>Australia’s home affairs minister, Tony Burke, said he had spoken to the head of the government body Screen Australia about the proposed tariffs. “Nobody should be under any doubt that we will be standing up unequivocally for the rights of the Australian screen industry,” he said in a statement.</p><figure id="c4a5d97c-26f1-4298-ba8d-fb8ca979211b" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:9,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;‘It feels empty’: is Hollywood film and TV production in a death spiral?&quot;,&quot;elementId&quot;:&quot;c4a5d97c-26f1-4298-ba8d-fb8ca979211b&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/film/2025/apr/26/los-angeles-film-and-tv-production&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>New Zealand’s prime minister, Christopher Luxon, told a news conference the government was awaiting further detail of the proposed tariffs. “We’ll have to see the detail of what actually ultimately emerges. But we’ll be obviously a great advocate, great champion of that sector in that industry,” he said.</p><p>The announcement from Trump comes after he triggered a trade war with China, and imposed global tariffs which have roiled markets and led to fears of a US recession. The film industry has already been feeling the effects of the tariffs, as China in April responded to the announcements by <a href="https://www.theguardian.com/film/2025/apr/10/china-to-restrict-us-film-releases-after-trumps-tariff-hike" data-link-name="in body link">reducing the quota</a> of American movies allowed into that country.</p><p>China is the world’s second largest film market after the US, although in recent years domestic offerings have outshone Hollywood imports.</p><p>A former senior commerce department official, William Reinsch, a senior fellow with the Center for Strategic and International Studies, said retaliation against Trump’s foreign movies tariffs would be devastating.</p><p>“The retaliation will kill our industry. We have a lot more to lose than to gain,” he said, adding that it would be difficult to make a national security or national emergency case for movies.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modern Latex (214 pts)]]></title>
            <link>https://github.com/mrkline/modern-latex</link>
            <guid>43892119</guid>
            <pubDate>Mon, 05 May 2025 05:18:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mrkline/modern-latex">https://github.com/mrkline/modern-latex</a>, See on <a href="https://news.ycombinator.com/item?id=43892119">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Modern LaTeX</h2><a id="user-content-modern-latex" aria-label="Permalink: Modern LaTeX" href="#modern-latex"></a></p>
<p dir="auto">LaTeX is a tool for creating beautiful writing, or a torture device that drives
users to the brink of madness every time they see bad spacing for the rest
of their lives. One of the two. Despite origins that can be traced back four
decades, it remains one of the best typesetting programs around. Many of its
guides, however, haven't aged as well. This short book will get you started with
LaTeX without bogging you down in arcana that lost its relevance back in the 90s.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Where do I get it?</h2><a id="user-content-where-do-i-get-it" aria-label="Permalink: Where do I get it?" href="#where-do-i-get-it"></a></p>
<p dir="auto">An up-to-date version should be available at
<a href="https://assets.bitbashing.io/modern-latex.pdf" rel="nofollow">https://assets.bitbashing.io/modern-latex.pdf</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How do I build it?</h2><a id="user-content-how-do-i-build-it" aria-label="Permalink: How do I build it?" href="#how-do-i-build-it"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install LuaLaTeX, a modern, Unicode-aware version of LaTeX.
On Linux, this is usually as simple as installing your distro's TeX Live
package, e.g., <code>texlive-base</code> or <code>texlive-core</code>.
The same package should also provide the <code>latexmk</code> script.
(See below)</p>
</li>
<li>
<p dir="auto">Check out the <code>online</code> branch of the source repository,
which is optimized for digital display instead of a printed book.
Changes include even margins, centered page numbers, a lack of blank pages
between chapters, and so on.</p>
</li>
<li>
<p dir="auto">Change the fonts as-needed.</p>
<p dir="auto">The official version of this book is typeset with Garamond Premier,
Neue Haas Grotesk, URW Futura, Drive Mono, Noto, and (of course) Latin Modern.
In the likely case that you don't have all of these typefaces,
change the <code>fontspec</code> commands (e.g., <code>setmainfont</code>, etc.) appropriately,
then modify or remove the colophon at the back of the book.</p>
</li>
<li>
<p dir="auto">Build the book using</p>
<div data-snippet-clipboard-copy-content="latexmk -lualatex -latexoption=-halt-on-error modern-latex.tex"><pre><code>latexmk -lualatex -latexoption=-halt-on-error modern-latex.tex
</code></pre></div>
<p dir="auto">Note that <code>latexmk</code> will run LuaLaTeX multiple times, since
TeX generates cross references in one pass, then links them in a second.</p>
<p dir="auto">If you can't use <code>latexmk</code> for some reason, you can manually invoke</p>
<div data-snippet-clipboard-copy-content="lualatex -halt-on-error -shell-escape modern-latex.tex"><pre><code>lualatex -halt-on-error -shell-escape modern-latex.tex
</code></pre></div>
<p dir="auto">until it no longer warns,
"Label(s) may have changed. Rerun to get cross-references right."</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feedback</h2><a id="user-content-feedback" aria-label="Permalink: Feedback" href="#feedback"></a></p>
<p dir="auto">...is welcome!
Please issue pull requests on this book's Github page,
or contact the author via matt &lt;at&gt; bitbashing.io</p>
<p dir="auto">Enjoy!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Meets WinDBG (276 pts)]]></title>
            <link>https://svnscha.de/posts/ai-meets-windbg/</link>
            <guid>43892096</guid>
            <pubDate>Mon, 05 May 2025 05:11:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://svnscha.de/posts/ai-meets-windbg/">https://svnscha.de/posts/ai-meets-windbg/</a>, See on <a href="https://news.ycombinator.com/item?id=43892096">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><h2 id="old-meets-new-bringing-crash-analysis-into-2025">Old Meets New: Bringing Crash Analysis into 2025</h2><p>Let's face it - while the rest of software development has evolved at warp speed, crash dump analysis feels like it's been preserved in digital amber for decades. We've got self-driving cars and pocket-sized supercomputers, yet here we are, still pecking away at command prompts like it's the dawn of the internet. Why is debugging the only area where we cling to tools that look like they belong in a computer history museum?</p><p>Picture this: You, a professional software engineer in 2025, hunched over a terminal, manually typing arcane commands like <code>!analyze -v</code> and <code>.ecxr</code>, squinting at hexadecimal memory addresses, and mentally translating stack traces. All while your friends in other industries are delegating their work to AI assistants that can write entire documents, create art, or automate complex workflows.</p><p>Something's wrong with this picture, right?</p><p><strong>What if I told you we can throw that ancient workflow into the dustbin of computing history?</strong> That's exactly what I've done. And I'm not talking about slightly better syntax highlighting or prettier UI for WinDBG. I'm talking about a fundamental transformation where you simply have a conversation with your debugger.</p><h2 id="when-inspiration-strikes">When Inspiration Strikes</h2><p>During a debugging session at work, I had one of those lightning bolt moments. What if - and stick with me here - we could apply the same AI-assisted "vibe coding" approach to crash dump analysis?</p><p>Picture this: instead of manually slogging through memory dumps and command outputs, you simply ask, "Hey, why did this application crash?" and get an intelligent, contextual answer that actually helps you solve the problem.</p><p><strong>The idea was too compelling not to pursue. So I built it.</strong></p><h2 id="see-it-in-action-ai-powered-crash-analysis">See It In Action: AI-Powered Crash Analysis</h2><p>Before diving into the technical details, let me show you what this looks like in practice. I have prepared a demo application to showcase two different use cases:</p><h3 id="video-1-crash-analysis-and-automated-bugfix">Video 1: Crash Analysis and Automated Bugfix</h3><p>In this video, I show how Copilot can analyze a crash dump, identify the bug and auto-fix the issue.</p><video controls="" src="https://svnscha.de/casts/2025-05-03-CrashDump1.webm">Your browser does not support the video tag.</video><p>As you can see in the video, instead of manually running WinDBG commands and interpreting the cryptic output, I'm having a natural conversation with GitHub Copilot. The AI quickly identifies that the application crashed, explains which specific conditions led to the crash, and suggests a fix.</p><h3 id="video-2-automated-crash-dump-analysis-of-multiple-crash-dump-files">Video 2: Automated Crash Dump Analysis of multiple crash dump files</h3><p>This video demonstrates a different capability: analyzing multiple crash dump files at once. It shows how the tool can quickly identify which dumps belong to your application and which don't.</p><video controls="" src="https://svnscha.de/casts/2025-05-03-CrashDump2.webm">Your browser does not support the video tag.</video><p>Worth noting, it takes just a few seconds until you get your first useful answer. I've played around with this for many hours and let me tell you one thing: You can really go deep. If you ask the right questions, the AI runs WinDBG/CDB commands that I haven't seen in all these years of debugging, and that is simply amazing.</p><h2 id="how-can-this-help-the-industry">How can this help the industry?</h2><p>I believe this is one of the really good examples of how AI can boost productivity. Analyzing crash dumps is a very tedious task. It begins with quickly checking and identifying whether crashes are the same or different, and often requires very advanced knowledge when a crash is challenging - really challenging.</p><p>Copilot can help here tremendously; it knows how to:</p><ul><li>Interpret assembly code (without you having to remember what EAX stands for)</li><li>Check memory contents (so you don't have to count hex bytes on your fingers)</li><li>Traverse structures with symbols (goodbye to manual pointer arithmetic!)</li><li>And so much more</li></ul><p>This is a game changer - not just for engineers, but also for support, QA, and everyone involved with crash dumps. It's like going from hunting with a stone spear to using a guided missile.</p><h2 id="how-did-i-build-this">How did I build this?</h2><p>If you've ever worked with WinDBG, you know the drill: cryptic commands, obscure syntax, and endless scrolling through memory addresses and stack traces that make your eyes glaze over. It's the kind of specialized knowledge that takes years to master and feels like speaking an alien language even when you do.</p><p>The trick here is connecting WinDBG with AI. To do that, you first need to programmatically control a debugging session, right? There are plenty of options on how to do this. I prefer to keep things simple, so I have chosen <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/debugger/debugging-using-cdb-and-ntsd">CDB</a>, which is Microsoft's Console Debugger. It operates on standard input and output, and that's so much more fun to deal with than setting up COM APIs or similar approaches.</p><p>The second part is "connecting with AI." That's where Model Context Protocol Servers come into the game.</p><h2 id="understanding-model-context-protocol-servers">Understanding Model Context Protocol Servers</h2><p>MCP is an open standard developed by Anthropic, released in November 2024. This protocol allows AI models to interact with external tools and data sources - think of it as giving AI assistants "hands" to work with other software. It defines a way for AI assistants to discover, access, and use tools through a consistent interface. In essence, it's what allows GitHub Copilot to "talk" to external programs like WinDBG.</p><p>An MCP server acts as the intermediary between the AI model and the tool. It:</p><ol><li>Registers available tools with the client</li><li>Handles requests from AI models to use these tools</li><li>Executes the tool operations and returns results</li><li>Maintains context across interactions</li></ol><p>This architecture means that any tool can be made available to AI models if someone builds an MCP server for it. And that's exactly what I did for WinDBG (CDB).</p><h3 id="why-mcp-instead-of-languagemodeltool-api">Why MCP Instead of LanguageModelTool API?</h3><p>The <a href="https://code.visualstudio.com/api/extension-guides/tools">LanguageModelTool API</a> might eventually be a better fit for this specific use-case. Creating a Visual Studio Extension that "just works" out of the box would potentially simplify the integration process significantly.</p><p>However, using MCP directly offers several notable advantages. It works with any AI model, not just limiting itself to Copilot. The server can be used outside VS Code, functioning with various other tools. New features can be easily added without necessitating changes to the core integration. Moreover, it remains platform-independent, avoiding lock-in to any single company's implementation.</p><h2 id="the-mcp-windbg-project">The MCP-WinDBG Project</h2><p>I've implemented a <a href="https://www.anthropic.com/news/model-context-protocol">Model Context Protocol</a> server that wraps WinDBG/CDB and exposes its capabilities to AI models within VS Code. Better yet, I've made it open source so everyone can experience this new workflow.</p><p>The project, called <a href="https://github.com/svnscha/mcp-windbg">mcp-windbg</a>, creates a seamless bridge between VS Code, GitHub Copilot, and the powerful analysis capabilities of WinDBG.</p><p>The actual "hard part" was implementing the CDB (Command-Line WinDBG) interaction layer. And by "hard", I mean vibe-coding with two coffees on a Saturday morning, where I spent more time being annoyed by pyTest failures than actual coding difficulties. The core implementation came together surprisingly quickly!</p><p>The rest is primarily wrapper code that implements the Model Context Protocol specifications. Now that I've established and defined the core WinDBG interaction logic, I'm considering refactoring the project to TypeScript. This would enable me to create both an MCP Server in TypeScript and a dedicated Visual Studio Extension, with both implementations leveraging the same underlying CDB interaction layer.</p><h2 id="what-does-this-mean-in-practice">What Does This Mean In Practice?</h2><p>Let me walk you through what this enables:</p><ol><li><p><strong>Natural language crash analysis</strong>: "Why is this application crashing with an access violation at this address?" (Instead of: "What the $%#@ is this heap corruption!?")</p></li><li><p><strong>Contextual debugging</strong>: "Show me the stack trace for thread 5 and explain what each function is doing based on the symbols." (Instead of staring at call stacks like they're ancient hieroglyphics)</p></li><li><p><strong>Root cause identification</strong>: "What's causing this null pointer dereference and where should I look in the code to fix it?" (Instead of playing detective with memory addresses)</p></li></ol><p>Instead of typing obscure commands like <code>!analyze -v</code> followed by a series of manual investigations, you simply ask questions in plain language, and the AI interprets the crash data for you. It's like having a WinDBG expert whispering in your ear, except it doesn't get annoyed when you ask the same question five times.</p><h2 id="how-it-works">How It Works</h2><p>The MCP server functions as a bridge between GitHub Copilot and WinDBG's powerful analysis capabilities:</p><ol><li>It provides a set of tools that Copilot can use to interact with crash dumps</li><li>It translates natural language questions into appropriate WinDBG commands</li><li>It parses and interprets the often cryptic WinDBG output into more useful information</li><li>It maintains context throughout a debugging session, enabling follow-up questions to work naturally</li></ol><p>The technical implementation uses Python to spawn and communicate with CDB (the command-line version of WinDBG), parses the output, and exposes the functionality through the Model Context Protocol to VS Code.</p><h2 id="getting-started-with-mcp-windbg">Getting Started With mcp-windbg</h2><p>Ready to try it yourself? Here's how to get started:</p><ol><li>First, make sure you have the Windows SDK installed with Debugging Tools for Windows</li><li>Clone the repository: <code>git clone https://github.com/svnscha/mcp-windbg.git</code></li><li>Set up a Python virtual environment and install the package</li><li>Configure VS Code to use the MCP server</li></ol><p>For complete details, check out the <a href="https://github.com/svnscha/mcp-windbg">repository README</a>.</p><p>Once configured, create a <code>.vscode/mcp.json</code> file in your project that points to the server:</p><pre data-lang="json"><code data-lang="json"><span>{
</span><span>    </span><span>"servers"</span><span>: {
</span><span>        </span><span>"mcp_server_windbg"</span><span>: {
</span><span>            </span><span>"type"</span><span>: </span><span>"stdio"</span><span>,
</span><span>            </span><span>"command"</span><span>: </span><span>"python"</span><span>,
</span><span>            </span><span>"args"</span><span>: [
</span><span>                </span><span>"-m"</span><span>,
</span><span>                </span><span>"mcp_server_windbg"
</span><span>            ],
</span><span>            </span><span>"env"</span><span>: {
</span><span>                </span><span>"_NT_SYMBOL_PATH"</span><span>: </span><span>"SRV*C:</span><span>\\</span><span>Symbols*https://msdl.microsoft.com/download/symbols"
</span><span>            }
</span><span>        },
</span><span>    }
</span><span>}
</span></code></pre><p>You might need to update the command, depending on where and how you have installed the mcp_server_windbg to.</p><h2 id="the-human-touch-still-matters">The Human Touch Still Matters</h2><p>Just like with <a href="https://svnscha.de/posts/vscode-vibe-coding/">code refactoring</a>, the AI assistance isn't perfect. The human element - your experience, intuition, and domain knowledge - remains crucial. Sometimes you'll need to guide the analysis, ask follow-up questions, or provide additional context.</p><p>But that's exactly what makes this approach so powerful: it combines the best of both worlds - AI's ability to quickly process and analyze large amounts of data with your human expertise in interpreting what truly matters for your specific application. Think of it as having a brilliant but occasionally confused intern who can do incredible things but sometimes needs you to point them in the right direction. "No, not that pointer... the OTHER pointer."</p><h2 id="join-the-experience">Join The Experience</h2><p>I'd love for you to try this out, contribute to the project, and share your experiences. If you're interested:</p><ol><li>Star the <a href="https://github.com/svnscha/mcp-windbg">GitHub repository</a></li><li>Try it on your own crash dumps</li><li>Report issues, suggest improvements, or contribute code</li><li>Share your success stories (or even failures - we learn from those too!)</li></ol><h2 id="the-magic-is-in-the-flow">The Magic Is In The Flow</h2><p>Just like with my code refactoring experience, the real magic isn't about any single capability - it's about the flow. When debugging stops being a tedious chore and becomes a fluid conversation, something fundamentally changes in how you approach problem-solving.</p><p>Gone are the days of dreading crash analysis. Instead, each debugging session becomes an opportunity for collaboration with an AI partner that helps you understand what's happening at a deeper level.</p><h2 id="wrapping-up">Wrapping Up</h2><p>Crash dump analysis has traditionally been one of the most technically demanding and least enjoyable parts of software development. It's like archaeology with a keyboard—painstakingly excavating through layers of memory and CPU state to unearth what went wrong. With AI assistance through tools like mcp-windbg, it becomes another area where we can experience that perfect "vibe state" of frictionless problem-solving.</p><p>If you're still manually typing WinDBG commands and squinting at memory dumps in 2025, you're not just missing out on productivity - you're missing out on a fundamentally more enjoyable way to work.</p><p>Try it. Debug it. Vibe it.</p></section></div>]]></description>
        </item>
    </channel>
</rss>