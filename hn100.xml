<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 29 Aug 2025 20:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The web does not need gatekeepers: Cloudflare’s new “signed agents” pitch (251 pts)]]></title>
            <link>https://positiveblue.substack.com/p/the-web-does-not-need-gatekeepers</link>
            <guid>45066258</guid>
            <pubDate>Fri, 29 Aug 2025 16:35:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://positiveblue.substack.com/p/the-web-does-not-need-gatekeepers">https://positiveblue.substack.com/p/the-web-does-not-need-gatekeepers</a>, See on <a href="https://news.ycombinator.com/item?id=45066258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Do you register with Google, Amazon or Microsoft to use the web?</p><p><span>Cloudflare’s new “</span><strong>signed agents</strong><span>” pitch sounds like safety but it’s a wolf in sheep’s clothing. They’ve built an allowlist for the open web and told builders to apply for permission. That’s not how the internet works. An </span><a href="https://dash.cloudflare.com/?to=/:account/configurations/verified-bots" rel="nofollow ugc noopener">application form</a><span> is not a standard.</span></p><p>Yes, identity for agents is a real problem. But Cloudflare is solving it like a border checkpoint. Get on their list or get treated like a trespasser. That’s vendor approval  not an internet protocol. An allowlist run by ONE company?</p><p>Authentication for that world isn’t “ask Cloudflare for a hall pass.” It’s verifiable chains of delegation and request-level proof: open, portable, and independent of any one company.</p><p><span>The web thrived because </span><strong>no one owned it</strong><span>. </span></p><p>In the 90s, Microsoft tried to “embrace and extend” the web, but failed. And that failure was a blessing. Because no single company controlled it, anyone could publish, anyone could innovate, and protocols carried more weight than corporate policies.</p><p><span>We’ve seen this movie before. </span><strong>Open standards beat closed plug-ins.</strong><span> HTML5 and the Open Web Platform displaced proprietary runtimes like Flash (Adobe) and Silverlight (Microsoft). Flash was formally ended in 2020 and Silverlight in 2021, while HTML5 became a W3C Recommendation back in 2014.</span></p><p>The pattern is consistent: when the commons defines the interface, innovation compounds; when a vendor hands out permission slips, it stalls.</p><p><span>Agents are inevitable. They will be the next major users of the web: retrieving information, automating workflows, making purchases, negotiating contracts. </span><strong>Sometimes ai agents will be explicitly directed by humans, other times they’ll act as subroutines inside bigger tasks</strong><span>.</span></p><p>The line between human and agent action will blur. </p><p><span>When I’m driving, I hand my phone to a friend and say, “Reply ‘on my way’ to my Mom.” They act on my behalf, through my identity, even though the software has no built-in concept of delegation. </span><strong>That is the world we are entering.</strong></p><p><strong>Authentication </strong><span>asks: </span><em>who is acting?</em></p><p><strong>Authorization</strong><span> asks: </span><em>what are they allowed to do?</em></p><div><p><span>They are not the same. Yet Cloudflare treats them as if a single passport could solve both. It can’t. In the real world, showing a passport is not enough to open a bank account, the actual person must be present!. </span></p><p><span>The same is true online. A cryptographic signature that claims “I am acting on behalf of X” means nothing unless it is tied to something real, like a verifiable infrastructure or a range of IPs. Without that, I can simply hand the passport to another agent, and they can act as if they were me. The passport becomes nothing more than a token anyone can pass around.</span></p></div><p>This is why the whole idea of a “bot passport” is deeply flawed. </p><p>Authentication and authorization matter more than ever but they must be rethought for the era of agents and for an authentic web.</p><p>And here’s the truth: on the internet, nobody knows you’re a dog.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kD6s!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kD6s!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic 424w, https://substackcdn.com/image/fetch/$s_!kD6s!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic 848w, https://substackcdn.com/image/fetch/$s_!kD6s!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic 1272w, https://substackcdn.com/image/fetch/$s_!kD6s!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!kD6s!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic" width="299" height="334" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:334,&quot;width&quot;:299,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:34738,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://positiveblue.substack.com/i/172206112?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!kD6s!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic 424w, https://substackcdn.com/image/fetch/$s_!kD6s!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic 848w, https://substackcdn.com/image/fetch/$s_!kD6s!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic 1272w, https://substackcdn.com/image/fetch/$s_!kD6s!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc83fa8-5429-4d44-8edb-a3ba065f8c9d_299x334.heic 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><p><span>A single signature proves nothing if it can simply be passed along. What we need is a way to prove both the chain of delegation and the authenticity of each request. The chain is like a certificate :</span></p><p><em>User X on Service Y delegated to Agent Z, who delegated to Agent K</em><span>. </span></p><p><span>But when Agent K actually makes a request to Service Y, it must add its own signature to prove it is truly Agent K. Without both pieces, authentication collapses.</span></p></div><p>The system must have a few key features:</p><ul><li><p><strong>Verifiable</strong><span>: you can check the claim independently.</span></p></li><li><p><strong>Composable</strong><span>: it works across chains of delegation.</span></p></li><li><p><strong>Decentralized</strong><span>: no single gatekeeper decides who is “valid.”</span></p></li></ul><p><span>Public key cryptography already gives us a model. Companies prove ownership today through DNS; they could publish public keys in the same way. That would let a service authenticate a third party simply by checking DNS (without anyone filling forms, asking permission, or registering with a central directory). Sites could still blacklist or whitelist as they choose, but the </span><strong>default is open</strong><span>.</span></p><p><strong>This is what authentication for the agentic era should look like: open, verifiable, and decentralized.</strong></p><p><span>Until now, software usually had a </span><strong>narrow scope</strong><span>. </span></p><p>Think a weekly cron job that emails a “new signups” report: it gets read-only access to the analytics DB, nothing else. Or a finance app via Plaid to fund your trading account: it can initiate transfers within limits but can’t browse your transaction history. </p><div><p><span>OAuth scopes worked because the software had a clear, predictable purpose.</span></p><p><span>Agents are different. They are </span><strong>general-purpose</strong><span>. The same agent might book a flight, pay for dinner, and then summarize your bank statement. They may also be short-lived, spun up for a single task and gone after it.</span></p></div><p>One way to make this work is to give the agent an “admin key”: full access to everything  It’s convenient, but dangerous. We must resist this pattern.</p><p><strong>Agents should not hold permanent credentials, authorizations must be per-task, not per-agent.</strong></p><p><span>Think of a bank account: I might tell my agent </span><em>“pay for dinner.”</em><span> That token should allow payment. But when I ask </span><em>“show me three months of spending,”</em><span> the agent should not be able to move money. Same agent, different task, different token. </span><strong>Credentials must follow tasks, not agents.</strong></p><p><span>Fortunately, cryptography and authorization models have evolved a lot in the last last years. We now have tools that allow us to issue tokens with </span><strong>constraints</strong><span>: granular, short-lived, and delegable (like </span><a href="https://en.wikipedia.org/wiki/Macaroons_(computer_science)" rel="nofollow ugc noopener">macaroons</a><span> or </span><a href="https://github.com/eclipse-biscuit/biscuit" rel="nofollow ugc noopener">biscuits</a><span>) and Open policy engines (like </span><a href="https://www.openpolicyagent.org/" rel="nofollow ugc noopener">OPA</a><span> or </span><a href="https://www.cedarpolicy.com/en" rel="nofollow ugc noopener">AWS Cedar</a><span>) can also be used for RBAC/ABAC for this use case.</span></p><p>Imagine:</p><ul><li><p>User X on Service Y holds an admin token.</p></li><li><p>They derive a narrower token for Agent Z to perform one task.</p></li><li><p>Agent Z can then derive an even narrower token for a sub-agent, all without bothering the service.</p></li><li><p>Each request can be verified against the chain.</p></li></ul><p>Coupled with the authentication model above, this approach gives us a foundation for managing agents without creating new gatekeepers.</p><p>This challenge is bigger than Cloudflare, Google, Microsoft or any single company. The future of the web cannot hinge on who controls the keys. We need protocols, not gatekeepers.</p><p><strong>Authentication, authorization, and monetization must remain open, interoperable, and standardized.</strong></p><p>Cloudflare’s launch is useful only because it exposes the danger. If we let a handful of companies decide which agents are “valid,” the agentic web will collapse into walled gardens. We’ve seen this movie before.</p><p>Here’s the line in the sand: I’m open-sourcing a first cut of these ideas chains of delegation,  request-level authorization , and task-scoped authorization so anyone can implement them, today.</p><p>If this resonates with you, if you want to collaborate, criticize, or help shape the protocols that will keep the web open for agents, please reach out jordi@fewsats.com</p><p><span>The future should not be about who holds the gates. It should be about </span><strong>protocols that let everyone build, share, and innovate</strong><span>.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia as a Graph (116 pts)]]></title>
            <link>https://wikigrapher.com/paths</link>
            <guid>45066060</guid>
            <pubDate>Fri, 29 Aug 2025 16:19:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wikigrapher.com/paths">https://wikigrapher.com/paths</a>, See on <a href="https://news.ycombinator.com/item?id=45066060">Hacker News</a></p>
Couldn't get https://wikigrapher.com/paths: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Essential Coding Theory [pdf] (196 pts)]]></title>
            <link>https://cse.buffalo.edu/faculty/atri/courses/coding-theory/book/web-coding-book.pdf</link>
            <guid>45065705</guid>
            <pubDate>Fri, 29 Aug 2025 15:53:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cse.buffalo.edu/faculty/atri/courses/coding-theory/book/web-coding-book.pdf">https://cse.buffalo.edu/faculty/atri/courses/coding-theory/book/web-coding-book.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45065705">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Taco Bell rethinks AI drive-through after man orders 18,000 waters (111 pts)]]></title>
            <link>https://www.bbc.com/news/articles/ckgyk2p55g8o</link>
            <guid>45065391</guid>
            <pubDate>Fri, 29 Aug 2025 15:28:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/ckgyk2p55g8o">https://www.bbc.com/news/articles/ckgyk2p55g8o</a>, See on <a href="https://news.ycombinator.com/item?id=45065391">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content"><article><div data-testid="byline-new" data-component="byline-block"><p><span data-testid="byline-new-contributors"><p><span>Shiona McCallum</span><span data-testid="undefined-role-location">Senior Tech Reporter </span></p></span></p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20250828-055926-c20c0c03f5-2.28.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/2a96/live/a8b53400-84b5-11f0-b98d-d308c56c1729.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/2a96/live/a8b53400-84b5-11f0-b98d-d308c56c1729.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/2a96/live/a8b53400-84b5-11f0-b98d-d308c56c1729.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/2a96/live/a8b53400-84b5-11f0-b98d-d308c56c1729.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/2a96/live/a8b53400-84b5-11f0-b98d-d308c56c1729.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/2a96/live/a8b53400-84b5-11f0-b98d-d308c56c1729.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/2a96/live/a8b53400-84b5-11f0-b98d-d308c56c1729.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/2a96/live/a8b53400-84b5-11f0-b98d-d308c56c1729.jpg.webp" loading="eager" alt="Getty Images A Taco Bell Drive Thru sign showing the pink and yellow bell and purple text saying 'Taco Bell, Drive Thru'."><span>Getty Images</span></p></div></figure><div data-component="text-block"><p>Taco Bell is rethinking its use of artificial intelligence (AI) to power drive-through restaurants in the US after comical videos of the tech making mistakes were viewed millions of times. </p><p>In one clip, a customer seemingly crashed the system by ordering 18,000 water cups, while in another a person got increasingly angry as the AI repeatedly asked him to add more drinks to his order.</p><p>Since 2023, the fast-food chain has introduced the technology at over 500 locations in the US, with the aim of reducing mistakes and speeding up orders. </p><p>But the AI seems to have served up the complete opposite. </p></div><div data-component="text-block"><p>Taco Bell's Chief Digital and Technology Officer Dane Mathews told <a target="_blank" href="https://www.wsj.com/articles/taco-bell-rethinks-future-of-voice-ai-at-the-drive-through-72990b5a?mod=rss_Technology">The Wall Street Journal</a> that deploying the voice AI has had its challenges. </p><p>"Sometimes it lets me down, but sometimes it really surprises me," he said.</p><p>He said the firm was "learning a lot" - but he would now think carefully about where to use AI going forwards, including not using it at drive-throughs.</p><p>In particular, Mr Matthews said, there are times when humans are better placed to take orders, especially when the restaurants get busy. </p><p>"We'll help coach teams on when to use voice AI and when it's better to monitor or step in," he said. </p><p>The issues have been building online as disgruntled customers take to social media to complain about the service - with many pointing out glitches and issues.</p><p>One clip on Instagram, which has been viewed over 21.5 million times, shows a man ordering "a large Mountain Dew" and the AI voice continually replying "and what will you drink with that?".</p><p>It isn't the first time there has been issues with AI not getting it right when it comes to processing food and drink orders.  </p><p>Last year <a target="_self" href="https://www.bbc.co.uk/news/articles/c722gne7qngo">McDonald's withdrew AI from its own drive-throughs</a> as the tech misinterpreted customer orders - resulting in one person getting bacon added to their ice cream in error, and another having hundreds of dollars worth of chicken nuggets mistakenly added to their order.</p><p>But despite some of the viral glitches facing Taco Bell, it says two million orders have been successfully processed using the voice AI since its introduction.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20250828-055926-c20c0c03f5-2.28.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/41d3/live/348b21e0-26a8-11f0-8f57-b7237f6a66e6.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/41d3/live/348b21e0-26a8-11f0-8f57-b7237f6a66e6.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/41d3/live/348b21e0-26a8-11f0-8f57-b7237f6a66e6.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/41d3/live/348b21e0-26a8-11f0-8f57-b7237f6a66e6.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/41d3/live/348b21e0-26a8-11f0-8f57-b7237f6a66e6.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/41d3/live/348b21e0-26a8-11f0-8f57-b7237f6a66e6.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/41d3/live/348b21e0-26a8-11f0-8f57-b7237f6a66e6.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/41d3/live/348b21e0-26a8-11f0-8f57-b7237f6a66e6.png.webp" loading="lazy" alt="A green promotional banner with black squares and rectangles forming pixels, moving in from the right. The text says: “Tech Decoded: The world’s biggest tech news in your inbox every Monday.”"></p></div></figure></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why AI Isn't Ready to Be a Real Coder (110 pts)]]></title>
            <link>https://spectrum.ieee.org/ai-for-coding</link>
            <guid>45065343</guid>
            <pubDate>Fri, 29 Aug 2025 15:24:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/ai-for-coding">https://spectrum.ieee.org/ai-for-coding</a>, See on <a href="https://news.ycombinator.com/item?id=45065343">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Why AI Isn’t Ready to Be a Real Coder"><p><a href="https://spectrum.ieee.org/topic/artificial-intelligence/">Artificial intelligence</a> (AI) has transformed the coding sphere, with <a href="https://spectrum.ieee.org/best-ai-coding-tools" target="_self">AI coding tools</a> completing <a href="https://spectrum.ieee.org/tag/source-code">source code</a>, correcting syntax errors, creating inline documentation, and understanding and answering questions about a codebase. As the technology advances beyond automating <a href="https://spectrum.ieee.org/tag/programming" target="_self">programming</a> tasks, the idea of full autonomy looms large. Is AI ready to be a <em><em>real</em></em> coder?</p><p>A <a href="https://arxiv.org/pdf/2503.22625" rel="noopener noreferrer" target="_blank">new paper</a> says not yet—and maps out exactly why. Researchers from <a href="https://www.cornell.edu/" rel="noopener noreferrer" target="_blank">Cornell University</a>, <a href="https://www.csail.mit.edu/node/2873" rel="noopener noreferrer" target="_blank">MIT CSAIL</a>, <a href="https://www.stanford.edu/" rel="noopener noreferrer" target="_blank">Stanford University</a>, and <a href="https://www.berkeley.edu/" rel="noopener noreferrer" target="_blank">UC Berkeley</a> highlight key challenges that today’s <a href="https://spectrum.ieee.org/tag/ai-models">AI models</a> face and outline promising research directions to tackle them. They presented their work at the <a href="https://icml.cc/Conferences/2025" rel="noopener noreferrer" target="_blank">2025 International Conference on Machine Learning</a>.</p><p>The study offers a clear-eyed reality check amid all the hype. “At some level, the technology is powerful and useful already, and it has gotten to the point where <a href="https://spectrum.ieee.org/tag/programming">programming</a> without these tools just feels primitive,” says <a href="https://www.csail.mit.edu/person/armando-solar-lezama" rel="noopener noreferrer" target="_blank">Armando Solar-Lezama</a>, a co-author of the paper and an associate director at <a href="https://spectrum.ieee.org/tag/mit">MIT</a> <a href="https://spectrum.ieee.org/tag/csail">CSAIL</a>, where he leads the computer-aided programming group. He argues, however, that AI-powered <a href="https://spectrum.ieee.org/tag/software-development" target="_self">software development</a> has yet to reach “the point where you can really collaborate with these tools the way you can with a human programmer.”</p><h2>Challenges With AI Coding Tools</h2><p>According to the study, AI still struggles with several crucial facets of coding: sweeping scopes involving huge codebases, the extended context lengths of millions of lines of code, higher levels of logical complexity, and long-horizon or long-term planning about the structure and design of code to maintain code quality.</p><p><a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/ksen.html" rel="noopener noreferrer" target="_blank">Koushik Sen</a>, a professor of computer science at <a href="https://spectrum.ieee.org/tag/uc-berkeley">UC Berkeley</a> and also a co-author of the paper, cites fixing a <a href="https://spectrum.ieee.org/memory-safe-programming-languages" target="_self">memory safety</a> bug as an example. (Such bugs can cause crashes, corrupt data, and open security vulnerabilities.) <a href="https://spectrum.ieee.org/tag/software-engineers" target="_self">Software engineers</a> might approach debugging by first determining where the error originates, “which might be far away from where it’s crashing, especially in a large codebase,” Sen explains. They’ll also have to understand the semantics of the code and how it works, and make changes based on that understanding. “You might have to not only fix that bug but change the entire memory management,” he adds.</p><p>These kinds of complex tasks can be difficult for AI development tools to navigate, resulting in <a href="https://spectrum.ieee.org/ai-hallucination" target="_self">hallucinations</a> about where the bug is or its root cause, as well as irrelevant suggestions or code fixes with subtle problems. “There are many failure points, and I don’t think the current <a href="https://spectrum.ieee.org/tag/llms" target="_self">LLMs</a> [<a href="https://spectrum.ieee.org/tag/large-language-models">large language models</a>] are good at handling that,” says Sen.</p><p>Among the various paths suggested by the researchers toward solving these AI coding challenges—such as training code LLMs to better collaborate with humans and ensuring human oversight for machine-generated code—the human element endures.</p><p>“A big part of <a href="https://spectrum.ieee.org/tag/software-development">software development</a> is building a shared vocabulary and a shared understanding of what the problem is and how we want to describe these features. It’s about coming up with the right metaphor for the architecture of our system,” Solar-Lezama says. “It’s something that can be difficult to replicate by a machine. Our interfaces with these tools are still quite narrow compared to all the things that we can do when interacting with real colleagues.”</p><h2>Enhancing AI-Human Collaboration in Coding</h2><p>Creating better interfaces, which today are driven by <a href="https://spectrum.ieee.org/prompt-engineering-is-dead" target="_self">prompt engineering</a>, is integral for developer productivity in the long run. “If it takes longer to explain to the system all the things you want to do and all the details of what you want to do, then all you have is just programming by another name,” says Solar-Lezama.</p><p><a href="https://cse.nd.edu/faculty/shreya-kumar/" rel="noopener noreferrer" target="_blank">Shreya Kumar</a>, a software engineer and an associate teaching professor in computer science at the <a href="https://www.nd.edu/" rel="noopener noreferrer" target="_blank">University of Notre Dame</a> who was not involved in the research, echoes the sentiment. “The reason we have a programming language is because we need to be unambiguous. But right now, we’re trying to adjust the prompt [in a way] that the tool will be able to understand,” she says. “We’re adapting to the tool, so instead of the tool serving us, we’re serving the tool. And it is sometimes more work than just writing the code.”</p><p>As the study notes, one way to address the dilemma of human-AI interaction is for AI systems to learn to quantify uncertainty and communicate proactively, asking for clarification or more information when faced with vague instructions or unclear scenarios. Sen adds that AI models might also be “missing context that I have in my mind as a developer—hidden concepts that are embedded in the code but hard to decipher from it. And if I give any hint to the LLM about what is happening, it might actually make better progress.”</p><p>For <a href="https://www.comp.nus.edu.sg/cs/people/abhik/" rel="noopener noreferrer" target="_blank">Abhik Roychoudhury</a>, a professor of computer science at the <a href="https://www.nus.edu.sg/" rel="noopener noreferrer" target="_blank">National University of Singapore</a> who was also not involved in the research, a crucial aspect missing from the paper and from most AI-backed software development tools entails capturing user intent.</p><p>“A software engineer is doing a lot of thinking in understanding the intent of the code. This intent inference—what the program is trying to do, what the program is supposed to do, and the deviation between the two—is what helps in a lot of <a href="https://spectrum.ieee.org/tag/software-engineering">software engineering</a> tasks. If this outlook can be brought in future AI offerings for software engineering, then it will get closer to what the software engineer does.”</p><h2>Where Does AI Coding Go From Here? </h2><p>Roychoudhury also assumes that many of the challenges identified in the paper are either being worked on now or “would be solved relatively quickly” due to the rapid pace of progress in AI for software engineering. Additionally, he believes that an <a href="https://spectrum.ieee.org/tag/agentic-ai" target="_self">agentic AI</a> approach can help, viewing significant promise in <a href="https://spectrum.ieee.org/tag/agentic-ai">AI agents</a> for processing requirements specifications and ensuring they can be enforced at the code level.</p><p>“I feel the <a href="https://spectrum.ieee.org/tag/automation">automation</a> of software engineering via agents is probably irreversible. I would dare say that it is going to happen,” Roychoudhury says.</p><p>Sen is of the same view but looks beyond agentic AI initiatives. He pinpoints ideas such as <a href="https://spectrum.ieee.org/evolutionary-ai-coding-agents" target="_self">evolutionary algorithms to enhance AI coding skills</a> and projects like <a href="https://spectrum.ieee.org/deepmind-alphaevolve" target="_self">AlphaEvolve</a> that employ <a href="https://spectrum.ieee.org/fighting-buggy-code-with-genetic-algorithms" target="_self">genetic algorithms</a> “to shuffle the solutions, pick the best ones, and then continue improving those solutions. We need to adopt a similar technology for coding agents, where the code is continuously improving in the background.”</p><p>However, Roychoudhury cautions that the bigger question lies in “whether you can <a href="https://spectrum.ieee.org/chatgpt-reliability" target="_self">trust</a> the agent, and this issue of trust will be further exacerbated as more and more of the coding gets automated.”</p><p>That’s why human supervision remains vital. “There should be a check and verify process. If you want a trustworthy system, you do need to have humans in the loop,” says Notre Dame’s Kumar.</p><p>Solar-Lezama agrees. “I think it’s always going to be the case that we’re ultimately going to want to build software for people, and that means we have to figure out what it is we want to write,” he says. “In some ways, achieving full automation really means that we get to now work at a different level of abstraction.”</p><p>So while AI may become a “real coder” in the near future, Roychoudhury acknowledges that it probably won’t gain software developers’ complete trust as a team member, and thus might not be allowed to do its tasks fully autonomously. “That team dynamics—when an AI agent can become a member of the team, what kind of tasks will it be doing, and how the rest of the team will be interacting with the agent—is essentially where the human-AI boundary lies,” he says.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deploying DeepSeek on 96 H100 GPUs (153 pts)]]></title>
            <link>https://lmsys.org/blog/2025-05-05-large-scale-ep/</link>
            <guid>45064329</guid>
            <pubDate>Fri, 29 Aug 2025 14:07:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lmsys.org/blog/2025-05-05-large-scale-ep/">https://lmsys.org/blog/2025-05-05-large-scale-ep/</a>, See on <a href="https://news.ycombinator.com/item?id=45064329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which uses Multi-head Latent Attention (MLA) and Mixture of Experts (MoE), require an advanced system for efficient serving at scale. In this blog, we explain how we match DeepSeek's inference system performance with SGLang.</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/overall-arch.png"></p>
<p>Our implementation, shown in the figure above, runs on 12 nodes in the Atlas Cloud, each equipped with 8 H100 GPUs.
It uses prefill-decode disaggregation and large-scale expert parallelism (EP), achieving a speed of <strong>52.3k input tokens per second and 22.3k output tokens per second per node</strong> for 2000-token input sequences.
To the best of our knowledge, this represents <strong>the first open-source implementation to nearly match the throughput reported in the official DeepSeek blog</strong> at large scale.
By deploying this implementation locally, it translates to a cost of $0.20/1M output tokens, which is about one-fifth the cost of the official DeepSeek Chat API.
Compared to vanilla tensor parallelism using the same resources, this optimized strategy improves the output throuhgput by up to 5x.
This blog dives into our parallelism design, optimization methods, and results. All components of our work are fully open-source, allowing others to explore and build on our efforts. The instructions for reproducing our experiments are fully available <a href="https://github.com/sgl-project/sglang/issues/6017">here</a>.</p>
<h2>Highlight</h2>
<p>✅ SGLang now supports prefill-decode (PD) disaggregation and large-scale EP, including the full functionality of <a href="https://github.com/deepseek-ai/DeepEP">DeepEP</a>, <a href="https://github.com/deepseek-ai/DeepGEMM">DeepGEMM</a>, and <a href="https://github.com/deepseek-ai/eplb">EPLB</a>.</p>
<p>✅ Leveraging these new features, our team successfully replicated DeepSeek's inference system using 12 nodes, each with 8 H100 GPUs. In total, SGLang achieves a throughput of 52.3k input tokens per second and 22.3k output tokens per second per node for input sequences of 2000 tokens.</p>
<p>✅ This blog explains technical details of our approach, focusing on optimizations for efficiency, peak memory usage reduction, and workload balancing. The profile results show that our implementation achieves nearly on-par performance with the official DeepSeek’s report.</p>
<p>✅ All experiments and code are fully open-sourced for community access and further development.</p>
<h2>Outline</h2>
<ul>
<li><a href="#parallelism-design">Parallelism Design</a></li>
<li><a href="#prefill-and-decode-disaggregation">Prefill and Decode Disaggregation</a></li>
<li><a href="#large-scale-expert-parallelism">Large-scale Expert Parallelism</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#toolkits">Toolkits</a></li>
<li><a href="#limitations-and-future-work">Limitations and Future Work</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#acknowledgment">Acknowledgment</a></li>
</ul>
<h2>Parallelism Design</h2>
<p>Efficient parallelism is essential to manage the computational complexity and memory demands of DeepSeek's architecture. This section outlines our approach to optimizing key components: attention layers, dense feed-forward networks (FFNs), sparse FFNs, and the language model (LM) head. Each component leverages tailored parallelism strategies to enhance scalability, memory efficiency, and performance.</p>
<h3>Attention Layers</h3>
<p>DeepSeek employs <strong>Multi-head Latent Attention (MLA)</strong> to effectively model complex dependencies within input sequences. To optimize this mechanism, we implement <strong>DP Attention</strong>, a data parallelism strategy that eliminates KV cache duplication across devices, significantly reducing memory overhead. Introduced in <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models">SGLang v0.4</a>, this approach has been extended to support <strong>hybrid data and tensor parallelism</strong>, offering flexibility for processing small batch sizes efficiently.</p>
<h3>Dense FFNs</h3>
<p>Despite using only three dense FFN layers, DeepSeek-V3's computation can significantly increase peak memory usage, potentially leading to system crashes if not carefully managed. To address this, we adopt <strong>Data Parallelism (DP)</strong> over tensor parallelism (TP), leveraging the following advantages:</p>
<ul>
<li><strong>Enhanced Scalability</strong>: With an intermediate dimension of 18,432, high TP degrees (e.g., TP32) result in inefficient fragmentation into small-unit segments (e.g., 576 units), which are not divisible by 128—a common alignment boundary for modern GPUs such as H100. This misalignment hampers computational efficiency and memory utilization. DP provides a more scalable solution by avoiding fragmentation, ensuring balanced workload distribution across devices.</li>
<li><strong>Optimized Memory Efficiency</strong>: Traditionally, TP reduces memory usage as worker size increases, but this advantage diminishes under DP attention. In a pure TP setup, memory demand for a single-layer Transformer model scales with DP size as: $$\text{Memory}=\frac{N_{\text{param}}}{\text{TP}}+(1+k)N_{\text{hidden_state}}\cdot \text{DP}\notag$$ Here, $N_{\text{hidden_state}}=n_\text{token}\times n_\text{hidden_size}$ is the size of the hidden state on each device (DP rank), $N_{\text{param}}=n_\text{intermediate_size}\times n_\text{hidden_size}$ is the number of model parameters, and $k$ is a coefficient representing extra memory overhead from CUDA Graph duplication. By assuming $\text{DP}=\text{TP}$, this memory usage function is minimized when $\text{TP}=\sqrt{\frac{N_{\text{param}}}{(1+k)N_{\text{hidden_state}}}}$. DeepSeek-V3 uses an intermediate size of 18,432. During the prefill phase, CUDA Graph is typically disabled, so $k = 0$. However, the token size per device can easily exceed 2,048, resulting in an optimal TP size of 3 or less. In the decode phase, a practical configuration might use 128 tokens per device and set $k = 3$. In this case, the memory-optimal TP size is 6. In both phases, a lower TP degree minimizes memory usage per device. As a result, DP may offer a more memory-efficient approach for scaling compared to relying solely on TP.</li>
<li><strong>Minimized Communication Overhead</strong>: In pure TP, each FFN necessitates two all-reduce operations, resulting in substantial communication overhead. By leveraging DP, we optimize this process to a single reduce-scatter following the prior attention layer and an all-gather before the next, reducing communication costs by 50%. Furthermore, when attention is also computed under pure DP, inter-device communication is entirely eliminated, significantly enhancing overall efficiency.</li>
</ul>
<p>The integration of DP dense FFN with DP attention is illustrated in the left figure below. Users can enable this feature by setting <code>--moe-dense-tp-size=1</code>.</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/parallel-design.png"></p>
<h3>Sparse FFNs</h3>
<p>In DeepSeek-V3's Mixture of Experts (MoE) architecture, sparse FFNs require substantial expert weights, creating a significant memory bottleneck. To address this, we implement <strong>Expert Parallelism (EP)</strong>, which distributes expert weights across multiple devices. This approach effectively scales memory capacity while maintaining high performance, though it does introduce challenges like irregular all-to-all communication and workload imbalance.</p>
<p>The figure in the right figure above illustrates our EP implementation using the DeepEP framework, with further details on our EP design and optimizations provided in <a href="#large-scale-expert-parallelism">the following sections</a>.</p>
<h3>LM Head</h3>
<p>The LM head computes output probabilities over a large vocabulary, a resource-intensive operation traditionally handled with vocabulary parallelism to aggregate token logits from TP groups. To enhance scalability and efficiency, we adopt <strong>Data Parallelism (DP)</strong>, mirroring our dense FFN strategy. This reduces memory overhead and simplifies communication across devices, delivering a more streamlined solution.</p>
<h2>Prefill and Decode Disaggregation</h2>
<p>LLM inference comprises two distinct phases: <strong>Prefill</strong> and <strong>Decode</strong>. The Prefill phase is computation-intensive, processing the entire input sequence, while the Decode phase is memory-intensive, managing the Key-Value (KV) cache for token generation. Traditionally, these phases are handled within a unified engine, where combined scheduling of prefill and decode batches introduces inefficiencies. To address these challenges, we introduce <strong>Prefill and Decode (PD) Disaggregation</strong> in SGLang.</p>
<h3>Issues with Unified Scheduling</h3>
<p>The conventional unified engine, which processes prefill and decode batches together, results in three significant problems:</p>
<ol>
<li><strong>Prefill Interruption</strong>: Incoming prefill batches frequently interrupt ongoing decode batches, causing substantial delays in token generation.</li>
<li><strong>DP Attention Imbalance</strong>: In DP attention, one DP worker may process a prefill batch while another handles a decode batch simultaneously, leading to increased decode latency.</li>
<li><strong>Incompatible with DeepEP</strong>: As we will discuss in <a href="#expert-parallelism-with-deepep">a later section</a>, DeepEP executes different dispatch modes for prefill and decode, making unified scheduling imcompatible with DeepEP.</li>
</ol>
<p>PD Disaggregation resolves these by separating the two stages, enabling tailored optimizations for each.</p>
<h3>Implementation Details</h3>
<p>The PD Disaggregation design in SGLang, depicted in the diagram below, interleaves execution between a Prefill Server and a Decode Server:</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/pd-disaggregation.png"></p>
<p>Upon receiving an input request, the workflow proceeds as follows:</p>
<ol>
<li>A Prefill Server and a Decode Server pair via a handshake, establishing a local sender and receiver, respectively.</li>
<li>The Decode Server pre-allocates the KV cache, signaling the Prefill Server to begin the model forward pass and compute the KV caches.</li>
<li>Once computed, the data transfers to the Decode Server, which handles iterative token generation.</li>
</ol>
<p>This separation ensures each phase operates under optimal conditions, maximizing GPU resource utilization. To further enhance performance, our implementation incorporates:</p>
<ul>
<li><strong>Non-blocking Transfer</strong>: Data send and receive operations run in a background thread, keeping the scheduler’s event loop uninterrupted.</li>
<li><strong>RDMA-Based Transfer</strong>: Remote Direct Memory Access (RDMA) leverages queue pairs for connections and scatter-gather elements (SGE) for efficient transfer of non-contiguous memory chunks.</li>
<li><strong>Flexible API Integration</strong>: SGLang offers adaptable APIs that integrate high-performance RDMA libraries like Mooncake and NIXL, streamlining data transfers.</li>
</ul>
<p>More details can be found in our <a href="https://docs.google.com/document/d/1rQXJwKd5b9b1aOzLh98mnyMhBMhlxXA5ATZTHoQrwvc/edit?tab=t.0">design document</a>.</p>
<h2>Large-scale Expert Parallelism</h2>
<h3>Expert Parallelism with DeepEP</h3>
<p><a href="https://github.com/deepseek-ai/DeepEP">DeepEP</a>, implemented by the DeepSeek team, is a communication library designed to streamline EP in MoE models. It tackles the challenge of efficiently routing tokens to specific experts across multiple GPUs. By providing optimized communication kernels, DeepEP reduces latency and boosts throughput, making it ideal for large-scale inference tasks.</p>
<p>DeepEP provides two specialized dispatch modes to address varying workload demands:</p>
<ul>
<li><strong>Normal Dispatch</strong>: Optimized for handling long input sequences, such as during the prefill phase, this mode prioritizes maximum computational throughput. However, it generates symbolic shapes that are incompatible with CUDA Graph, rendering it less effective for the decode phase, where kernel launch overhead becomes a significant bottleneck.</li>
<li><strong>Low-Latency Dispatch</strong>: Tailored for generating output tokens during the decode phase, this mode prioritizes minimal delay to ensure real-time performance. It supports CUDA Graph but requires preallocating a fixed memory size. If the memory demand exceeds this preallocation, a runtime error occurs.</li>
</ul>
<p>In SGLang, the integration of DeepEP provides <strong>auto mode</strong> that dynamically selects between these two dispatch modes based on the workload. However, without PD disaggregation, the auto mode faces a limitation: it cannot simultaneously support both normal dispatch (for prefill) and low-latency dispatch (for decode) within the same communication group. This restriction hinders its compatibility with DP attention, which is crucial for memory-efficient inference. The compatibility of each mode is outlined in the table below:</p>
<table>
<thead>
<tr>
<th><strong>Mode</strong></th>
<th><strong>Long Input</strong></th>
<th><strong>Long Output</strong></th>
<th><strong>DP Attention</strong></th>
<th><strong>CUDA Graph</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Normal</td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>Low-Latency</td>
<td>❌</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Auto</td>
<td>✅</td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
</tr>
</tbody>
</table>
<p>PD disaggregation addresses this by separating prefill and decode phases, allowing normal dispatch for the prefill phase and low-latency dispatch for the decode phase, both under DP attention. This integration optimizes resource utilization and enhances overall performance by aligning the dispatch mode with the specific needs of each phase.</p>
<h3>DeepGEMM Integration</h3>
<p><a href="https://github.com/deepseek-ai/DeepGEMM">DeepGEMM</a> is another high-efficient library developed by the DeepSeek team, specifically designed to optimize computations in MoE models. It provides two specialized functions for handling MoE-related matrix multiplications (Grouped GEMMs), each tailored to different phases of the inference process.</p>
<ul>
<li><strong>Grouped GEMMs (contiguous layout):</strong> This kernel is designed for dynamic input shapes, making it ideal for the prefill phase of MoE inference. It processes inputs where the data for different experts is concatenated contiguously, allowing for flexible handling of varying input sizes.</li>
<li><strong>Grouped GEMMs (masked layout):</strong> This kernel assumes a fixed input shape and uses a mask tensor to compute only the valid portions of the input. It is compatible with CUDA Graph, which optimizes kernel launches, making it well-suited for the decode phase where reducing overhead is critical.</li>
</ul>
<p>DeepGEMM integrates smoothly with the dispatch modes of DeepEP:</p>
<ul>
<li>For the <strong>contiguous layout kernel</strong>, which is used with <strong>normal dispatch</strong> in the prefill phase, an additional step is required. Since normal dispatch outputs a symbolic shape, a permutation is needed to transform the output into the contiguous format expected by the kernel. We referred to the LightLLM project and implemented a custom Triton kernel for efficient permutation. This kernel ensures that the output from normal dispatch is correctly rearranged, enabling smooth integration with the contiguous GEMM kernel.</li>
<li>The <strong>masked layout kernel</strong> pairs seamlessly with DeepEP’s <strong>low-latency dispatch</strong>, as both are optimized for the decode phase and support CUDA Graph.</li>
</ul>
<p>SGLang also integrates DeepGEMM for MoE computation under tensor parallelism. Additionally, DeepGEMM provides a highly efficient general GeMM kernel, which can be activated in SGLang by setting the environment variable <code>SGL_ENABLE_JIT_DEEPGEMM</code> to 1, offering even greater computational efficiency for non-MoE operations.</p>
<h3>Two-batch Overlap</h3>
<p>In multi-node environments, limited communication bandwidth can significantly increase overall latency. To tackle this challenge, we implemented <strong>Two-batch Overlap (TBO)</strong> following <a href="https://github.com/deepseek-ai/profile-data">DeepSeek's system design</a>. TBO splits a single batch into two micro-batches, allowing computation and communication to overlap, which also lowers peak memory usage by halving the effective batch size. However, putting TBO into practice introduces specific implementation difficulties.</p>
<h5>Implementation Challenges</h5>
<p>Although DeepSeek released the design framework of TBO, there are two slight implementation challenges.</p>
<ul>
<li><strong>Code Complexity</strong>: Directly coding TBO can lead to duplicated logic for managing multiple micro-batches. This increases the complexity of the codebase, making it harder to maintain and prone to errors, especially as the number of micro-batches or overlapping scenarios grows.</li>
<li><strong>Synchronization Issues in the Prefill Phase</strong>: Achieving effective overlap between computation and communication needs consideration when the normal dispatch in DeepEP block the CPU. This blocking behavior can stall the pipeline, leaving the GPU idle and undermining the performance benefits of TBO.</li>
</ul>
<h5>Abstraction for Clean Implementation</h5>
<p>To create a more maintainable and reusable codebase, we use an abstraction layer consisting of operations and yield points. This method simplifies development by allowing us to write code as if handling a single micro-batch, while strategically pausing execution by inserting yield points to let other micro-batches proceed. It eliminates code duplication, reduces the potential need for variable postfixes, and efficiently manages cases where some executions complete at a layer's end while others have not. Additionally, it supports easy adaptation to different overlapping region choices or future enhancements, like a three-batch overlap, with minimal code changes. Below is a concise demonstration of this approach:</p>
<pre><code>operations = [
    <span>self</span>._forward_attn,
    YieldOperation(),  <span># Pause execution for other micro-batches</span>
    <span>self</span>._forward_dispatch,
    <span>self</span>._forward_mlp,
    YieldOperation(),  <span># Another pause point</span>
    <span>self</span>._forward_combine,
]

<span># Process a single micro-batch without duplicating code</span>
<span>def</span> <span>_forward_attn</span>(<span>self, state</span>):
    state.hidden_states = <span>self</span>.self_attn(state.hidden_states, ...)
</code></pre>
<h5>Prefill Overlapping Implementation</h5>
<p>We refine the launch order during the prefill phase to avoid CPU-blocking via the dispatch operation in DeepEP, even though we are using its asynchronous mode. Specifically:</p>
<ul>
<li>The dispatch operation blocks the CPU until the GPU receives metadata from other ranks to allocate correctly sized tensors.</li>
<li>An improper implementation would leave the computation stream idle during this period, as no computation tasks are submitted to the GPU.</li>
</ul>
<p>To optimize, we prioritize submitting computation tasks to the GPU before launching CPU-blocking communication. This ensures the GPU remains active during communication. As illustrated in the figure below, TBO with a proper launch order, indicated by bolded borders, avoids bubble caused by a CPU-blocking operation (i.e., normal dispatch).</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/tbo-prefill.png"></p>
<h3>Expert Parallelism Load Balancer</h3>
<p>In MoE models, EP often leads to uneven workload distribution across GPUs. This imbalance forces the system to wait for the slowest GPU computation or communication, wasting compute cycles and increasing memory usage due to expert activations. As the number of GPUs (EP size) increases, the imbalance issue gets more severe.</p>
<p>To address this, DeepSeek developed the <a href="https://github.com/deepseek-ai/EPLB">Expert Parallelism Load Balancer (EPLB)</a>. EPLB takes expert distribution statistics as input and computes an optimal arrangement of experts to minimize imbalance. Users can allocate redundant experts (e.g., 32 additional experts), which, when combined with the original 256, create a pool of 288 experts. This pool allows EPLB to strategically place or replicate experts—for instance, duplicating the most frequently used expert multiple times or grouping a moderately used expert with rarely used ones on a single GPU.</p>
<p>Beyond balancing workloads, EPLB offers greater flexibility in parallelism design. With the original 256 experts, parallelism sizes are restricted to powers of two. EPLB’s use of 288 experts enables more diverse configurations, such as parallelism sizes of 12 or 72.</p>
<p>In the figure below, we demonstrate the effects of scale and EPLB algorithm to the imbalance issue via simulation. We compute GPU balancedness as the ratio between mean computation time and maximum computation time for a MoE layer among GPUs, and we use the number of tokens for a GPU to estimate the computation time for it. As can be seen, utilization rate decreases when the system scales with the number of nodes, and enabling EPLB significantly improves the utilization.</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/eplb-balancedness.png"></p>
<h5>EPLB for Real-World Serving</h5>
<p>For EPLB to be effective, the input distribution must closely match the actual serving workload. Two strategies enhance this alignment:</p>
<ul>
<li><strong>Increasing Batch Size</strong>: Larger batches reduce random fluctuations in expert usage, which improves balance, which can be achieved by scaling the cluster or using techniques like Multi-Token Prediction (MTP).</li>
<li><strong>Periodic Rebalancing</strong>: Regularly updating the expert arrangement leverages temporal locality but requires efficient reloading of experts. This necessitates minimizing the cost of expert reloading operations.</li>
</ul>
<p>Even with EPLB, some imbalance is inevitable, making further optimization a valuable future direction.</p>
<h5>Implementation of Rebalancing</h5>
<p>SGLang implements expert rebalancing in three stages to ensure efficiency and minimal disruption:</p>
<ol>
<li><strong>System Loading Stage</strong>: Weights are optionally preloaded from disk to main memory for faster rebalancing or kept on disk with memory mapping (mmap) for reduced memory usage.</li>
<li><strong>Rebalance Preparation Stage</strong>: Required weights are asynchronously transferred to device memory in the background, utilizing free DMA hardware engines without interrupting ongoing GPU operations.</li>
<li><strong>Rebalance Execution Stage</strong>: A device-to-device copy updates the weights. This step can be further optimized through physical memory rebinding techniques.</li>
</ol>
<p>This staged approach ensures that rebalancing is both efficient and non-disruptive, maintaining system performance during updates.</p>
<h2>Evaluation</h2>
<h3>End-to-end Performance</h3>
<h5>Experimental Setup</h5>
<p>We evaluated the end-to-end performance of different configurations of SGLang using DeepSeek-V3 on a cluster of 12 nodes, connected via InfiniBand and each equipped with 8 H100 GPUs. This evaluation highlights the throughput improvements enabled by our advanced optimization techniques. We compared the following four settings:</p>
<ul>
<li><strong>SGLang with TP16 x 6</strong>: Every two nodes are paired with an independent group, running DeepSeek-V3 inference with a TP size of 16 and DP attention.</li>
<li><strong>SGLang with PD Disaggregation</strong>: This version incorporates PD disaggregation and full EP optimization. For the EPLB, we adopt a distribution matching the input/output data, as real-time serving statistics are unavailable.</li>
<li><strong>SGLang with PD Disaggregation and simulated MTP</strong>: To simulate MTP’s effects, we firstly double the batch size and halve the Key-Value KV cache length to maintain the same workload for GroupedGeMM computation and memory access. Moreover, we insert dummy kernels after the real attention computation to ensure the attention phase takes the same time as in DeepSeek’s profile, accurately reflecting the slowdown caused by MTP’s attention mechanism. We conservatively assume a 70% acceptance rate under MTP.</li>
<li><strong>DeepSeek Profile Results</strong>: Throughput estimates are derived from <a href="https://github.com/deepseek-ai/profile-data">DeepSeek’s official profiling data</a>.</li>
</ul>
<h5>Performance Analysis of Prefill and Decode Phases</h5>
<p>To accommodate varying workload demands, we independently evaluated the prefill (P) and decode (D) phases, assuming unlimited resources for the non-tested phase to isolate and maximize the load on the tested nodes—mirroring the setup used by DeepSeek. The results are summarized below:</p>
<ul>
<li><strong>Prefill Phase</strong>: On 4 nodes (4×8×H100, EP32), the system achieved per-node throughputs of 57,674, 54,543, and 50,302 tokens per second for prompt lengths of 1K, 2K, and 4K, respectively. As shown in the bar chart below, this represents up to a 3.3× improvement over the TP16 baseline, largely attributable to the optimized GroupedGeMM kernel (DeepGEMM) and two-batch overlap. Assuming a perfectly balanced workload, our system’s throughput is within 5.6% of DeepSeek's official profile.</li>
<li><strong>Decode Phase</strong>: Evaluated on 9 nodes (9×8×H100, EP72; half the scale of DeepSeek), the system achieved 22,282 tokens/sec per node for 2K inputs—representing a 5.2× speedup over the TP16 baseline. Under simulated MTP conditions—with attention kernels intentionally slowed to reflect real-world latency—the system sustained a high throughput of 17,373 tokens/sec per node for 4K inputs, just 6.6% below DeepSeek’s official profile. As shown in the figure on the right, these performance gains are largely attributed to 4× larger batch sizes enabled by EP, which enhances scalability by significantly reducing per-GPU memory consumption of model weights.</li>
</ul>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/e2e-prefill-decode.png"></p>
<h3>Profile Results</h3>
<p>This section compares SGLang’s performance with DeepSeek’s inference system, aligning our experimental setup as closely as possible to DeepSeek’s production environment. We analyze overall throughput and detailed kernel breakdowns, benchmarking against DeepSeek’s blog and public profile data.</p>
<h5>Overall Throughput</h5>
<p>For prefill, we tested a scenario with 16,384 tokens per device and an input length of 4,096. Due to uncertainty in DeepSeek’s expert distribution, we evaluated two cases: one with default expert distribution and another with simulated perfect EPLB (random expert selection following group-limited routing semantics) as a performance upper bound.</p>
<p>The results are presented below:</p>
<table>
<thead>
<tr>
<th></th>
<th>DeepSeek Blog (excl. cache hit)</th>
<th>DeepSeek Profile</th>
<th>SGLang (Default)</th>
<th>SGLang + Simulated Perfect EPLB</th>
</tr>
</thead>
<tbody>
<tr>
<td>Batch Size</td>
<td>N/A</td>
<td>16,384</td>
<td>16,384</td>
<td>16,384</td>
</tr>
<tr>
<td>Input Length</td>
<td>N/A</td>
<td>4,096</td>
<td>4,096</td>
<td>4,096</td>
</tr>
<tr>
<td>Throughput (per node)</td>
<td>32,206</td>
<td>62,713</td>
<td>50,302</td>
<td>59,337</td>
</tr>
</tbody>
</table>
<p>DeepSeek’s profile reports a throughput roughly twice that of its production environment. SGLang with default expert imbalance is 20% slower than DeepSeek’s profile, while the simulated perfect EPLB case narrows the gap to 6%.</p>
<p>For decode, the results are shown below:</p>
<table>
<thead>
<tr>
<th></th>
<th>DeepSeek Blog</th>
<th>DeepSeek Profile</th>
<th>SGLang (Default)</th>
<th>SGLang + Simulated MTP (Slow Attention)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Batch Size</td>
<td>N/A</td>
<td>128</td>
<td>256</td>
<td>128</td>
</tr>
<tr>
<td>KV Cache Length</td>
<td>4,989</td>
<td>4,096</td>
<td>2,000</td>
<td>4,000</td>
</tr>
<tr>
<td>Number of Nodes</td>
<td>18</td>
<td>16</td>
<td>9</td>
<td>9</td>
</tr>
<tr>
<td>Throughput (per node)</td>
<td>14,800</td>
<td>18,598</td>
<td>22,282</td>
<td>17,373</td>
</tr>
</tbody>
</table>
<p>Using half the nodes of DeepSeek, SGLang with simulated MTP is only slightly slower than DeepSeek’s profile. In a higher batch size setting (256 sequences, 2,000 input length), SGLang achieves 22,282 tokens per second per node, demonstrating strong scalability.</p>
<h5>Detail Breakdown</h5>
<p>The figure below breaks down kernel execution times for prefill, including unit test results as a theoretical upper bound:</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/profile-prefill.png"></p>
<ul>
<li><strong>Default EPLB</strong>: Communication kernels exhibit longer execution times and higher variance compared to DeepSeek’s profile, likely due to greater expert imbalance. This leads to extended computation stream bubbles, slowing down overall performance.</li>
<li><strong>Simulated Perfect EPLB</strong>: This setup aligns more closely with DeepSeek’s profile, though discrepancies remain, indicating potential areas for optimization.</li>
<li><strong>Comparison with Unit Tests</strong>: Both DeepSeek and SGLang have a communication time slower than unit test results, while the latter is achievable when disabling TBO, revealing a potential optimization direction if communication is the bottleneck.</li>
</ul>
<p>SGLang’s decode kernel breakdown aligns closely with DeepSeek’s, as shown below:</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/profile-decode.png"></p>
<p>Key observations include:</p>
<ul>
<li><strong>Combine Time Discrepancy</strong>: SGLang’s combine operation appears 2x slower than DeepSeek’s due to shorter attention computation, causing communication kernels to busy-wait. In the simulated slow attention experiment, combine time matches DeepSeek’s, confirming this hypothesis.</li>
<li><strong>MoE Performance</strong>: SGLang’s MoE kernels are 25% slower, possibly because DeepSeek’s 18 nodes (versus our 9) distribute experts more efficiently, reducing memory access overhead for GEMM operations.</li>
<li><strong>Dispatch Optimization Potential</strong>: Both DeepSeek and SGLang show dispatch times of ~0.17ms per layer, but unit tests with DeepEP reveal a potential of 0.06ms occupying SMs. Currently, dispatch spends significant time busy-waiting for data. Inserting slow dummy kernels between send/receive operations reduces dispatch time to 0.09ms, and in-flight duration analysis using unit test data suggests further improvements are possible.</li>
</ul>
<p>While minor enhancements remain—primarily in kernel fusion under "Other Kernels"—SGLang’s decode performance is largely aligned with DeepSeek’s, with prefill optimization as the next focus.</p>
<h3>Ablation Study: Two-batch Overlap</h3>
<h5>Impact of Batch Size and Attention Time</h5>
<p>This section investigates TBO performance across varying batch sizes and simulated MTP scenarios.</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/tbo-overall.png"></p>
<p>TBO delivers two significant benefits in the prefill phase, as evidenced by throughput comparisons and memory usage optimizations:</p>
<ul>
<li><strong>Support for Larger Batch Sizes</strong>: In the vanilla configuration, each device processes up to 8,192 tokens before encountering out-of-memory (OOM) errors at 16,384 tokens. TBO mitigates this by optimizing memory usage for input tokens, enabling inference with batches as large as 16,384 tokens per device. This further boosts performance to 40.5% increase when comparing the TBO flag with all other configurations made optimal.</li>
<li><strong>Enhanced Throughput</strong>: By overlapping computation (e.g., attention and MLP phases) with communication (e.g., DeepEP Combine and Dispatch), TBO achieves a 27% to 35% throughput increase compared to the vanilla setup, even when processing the same token count per device.</li>
</ul>
<p>TBO’s impact in the decode phase varies by scenario, with performance tied to batch size and attention processing time:</p>
<ul>
<li><strong>Real Test Cases</strong>: Speedup in practical scenarios is contingent on batch size exceeding a threshold between 64 and 128 tokens. Below this, TBO yields minimal or negative gains (e.g., -27% at 32 tokens/device), as small decode batch sizes hinder kernel efficiency. The speedup reaches 25.5% at 256 tokens with a performance of 22,310 tokens per second.</li>
<li><strong>Simulated MTP Scenario</strong>: TBO provides the most substantial speedup in simulated MTP cases when processing 128 requests to generate 256 tokens per decode step. This is due to prolonged attention processing time, which aligns computation (e.g., DP Attention layers) with DeepEP communication overhead (e.g., combine and dispatch steps). The evaluation shows a 35% speedup at 128 sequences/device, with throughput 17,552 tokens per second compared to 12,929 without TBO.</li>
</ul>
<h5>Detail Breakdown</h5>
<p>We evaluated three prefill scenarios: TBO with 16k tokens per batch, TBO with 8k tokens, and no-TBO with 8k tokens. The figure below reveals key insights:</p>
<ul>
<li><strong>TBO Efficiency</strong>: Comparing the 8k cases, TBO improves overall efficiency by overlapping computation and communication, as expected.</li>
<li><strong>Batch Size Impact</strong>: Reducing the batch size from 16k to 8k with TBO results in a slight slowdown, reflecting diminished kernel efficiency with smaller batches.</li>
<li><strong>Kernel Performance</strong>: Interestingly, the no-TBO 8k case outperforms the TBO 16k case in per-kernel speed, despite both having an effective batch size of 8k for kernels. This may stem from reduced streaming multiprocessors (SMs) with TBO, potential noisy neighbor effects during overlap, or kernel incompatibility between computation and communication. These findings suggest future optimization directions for SGLang.</li>
</ul>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/tbo-breakdown-prefill.png"></p>
<p>For the decode phase, we analyzed three configurations: TBO with a batch size of 256, no-TBO with 256, and no-TBO with 128. The time breakdown is shown below:</p>
<ul>
<li><strong>TBO vs. No-TBO (Batch Size 256)</strong>: Without TBO, communication time increases significantly due to the lack of overlap. However, computation kernels, particularly GEMM, benefit from a larger effective batch size, resulting in faster execution.</li>
<li><strong>TBO (256) vs. No-TBO (128)</strong>: Comparing cases with the same kernel batch size, only non-overlapped communication slows down in the no-TBO setup, while computation remains consistent. Unlike prefill, decode communication kernels either fully utilize SMs (during send/receive) or none (during inflight waiting), avoiding resource contention with computation kernels.</li>
</ul>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/tbo-breakdown-decode.png"></p>
<h3>Ablation Study: EPLB</h3>
<p>This section evaluates the impact of the EPLB on system performance through overall throughput analysis and detailed case studies. Given EPLB's sensitivity to workload distribution and distribution shifts in production environments, we focus on qualitative and generalizable insights rather than real-world performance, which requires production data.</p>
<h5>Overall Results</h5>
<p>The figure below illustrates EPLB's effect on throughput in large-scale settings. EPLB delivers a significant speedup of 1.49x (prefill) and 2.54x (decode), as expected, due to its ability to mitigate workload imbalances across GPUs. As the number of ranks scales, imbalances grow, and EPLB effectively addresses this in our large-scale experiments, leading to notable throughput improvements.</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/eplb-throughput.png"></p>
<h5>Case Study: Workload Imbalance Versus Overall Throughput</h5>
<p>To explore the relationship between workload imbalance and throughput, we conducted a case study using a decode experiment with 1800 input tokens, 100 output tokens, and a batch size of 256. Throughput and balancedness (average token count divided by maximum token count across experts) were plotted against decoding steps:</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/eplb-throughput-vs-imbalance.png"></p>
<p>The results reveal a strong correlation between balancedness and throughput, emphasizing the importance of maintaining high balancedness for optimal performance.</p>
<h5>Case Study: Expert Distribution Statistics</h5>
<p>The following figure presents expert distribution statistics for prefill and decode sample data:</p>
<p><img src="https://lmsys.org/images/blog/large_scale_ep/eplb-stat.png"></p>
<p>Key observations include:</p>
<ul>
<li><strong>Imbalance in Expert Usage</strong>: Most experts are infrequently used, while a small subset is heavily utilized, underscoring the inherent imbalance in MoE models.</li>
<li><strong>Prefill vs. Decode Differences</strong>: Although prefill and decode distributions share similarities, notable differences exist. This supports the use of PD disaggregation, which enables distinct expert placements for each phase, optimizing performance.</li>
</ul>
<p>These findings highlight EPLB's role in addressing workload imbalances and the value of tailoring expert placement to phase-specific demands.</p>
<h2>Toolkits</h2>
<h3>Disposable Tensor</h3>
<p>Memory management in PyTorch can be challenging due to persistent object references, especially in GPU-intensive workflows where CUDA memory is a scarce resource. Consider the following example:</p>
<pre><code><span>def</span> <span>ffn</span>(<span>hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear</span>):
    intermediate_state = linear1(hidden_state)
    <span>del</span> hidden_state  <span># Attempt to free memory, but no effect due to external reference</span>
    <span>return</span> linear2(nn.ReLU(intermediate_state))

hidden_state = ffn(hidden_state, linear1, linear2)
</code></pre>
<p>In this code, <code>del hidden_state</code> is intended to release the memory occupied by <code>hidden_state</code> after <code>intermediate_state</code> is computed. However, as <code>hidden_state</code> is still referenced outside the function, the <code>del</code> operation has no effect. This increases peak memory usage, risking performance slowdowns or out-of-memory errors.</p>
<p>SGLang addresses this with the DisposableTensor class, a subclass of <code>torch.Tensor</code> which introduces a dispose() method to explicitly and immediately release a tensor’s memory, circumventing Python’s reference counting limitations. Here’s how it works:</p>
<pre><code><span>def</span> <span>ffn</span>(<span>hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear</span>):
    intermediate_state = linear1(hidden_state)
    hidden_state.dispose()  <span># Immediately releases CUDA memory</span>
    <span>return</span> linear2(nn.ReLU(intermediate_state))

<span># Wrap the tensor in DisposableTensor</span>
hidden_state = DisposableTensor(hidden_state)
hidden_state = ffn(hidden_state, linear1, linear2)
</code></pre>
<p>By wrapping <code>hidden_state</code> in a <code>DisposableTensor</code> and calling <code>dispose()</code> when it’s no longer needed, the CUDA memory is freed right away. This ensures that memory is released as soon as the tensor’s role in the computation is complete, reducing peak memory usage and improving overall efficiency.</p>
<h3>Expert Workload Extraction and Simulation</h3>
<p>SGLang also includes a toolset for analyzing and simulating expert workload distribution in MoE models. This feature enables users to:</p>
<ul>
<li><strong>Dump Expert Workload Statistics</strong>: Extract either accumulated statistics or per-batch workload data. Accumulated stats support the EPLB manager for real-time optimization, while per-batch data provides granular insights for analysis and simulation.</li>
<li><strong>Simulate Expert Utilization</strong>: Model expert balance across various configurations without requiring costly hardware or repeated trials. For instance, users can gather workload data from a modest setup (e.g., 2x8xH100 or 8xH200) and simulate the performance for a large-scale 22-node deployment.</li>
</ul>
<p>This simulation capability allows users to evaluate how factors like rebalancing frequency, node count, or batch size impact system performance. It’s a cost-effective way to fine-tune configurations before scaling up.</p>
<h2>Limitations and Future Work</h2>
<p>While our implementation of SGLang for DeepSeek-V3 inference demonstrates significant throughput improvements, several limitations and areas for future enhancement remain:</p>
<ol>
<li><strong>Latency Optimization</strong>: The current focus on throughput leaves Time to First Token (TTFT) at 2–5 seconds and Inter-Token Latency (ITL) at approximately 100ms, requiring further optimizations for real-time use cases.</li>
<li><strong>Sequence Length Constraints</strong>: Limited to shorter sequences due to the use of 96 GPUs. Expanding GPU resources would support longer sequences, essential for specific applications.</li>
<li><strong>Multi-Token Prediction (MTP) Integration</strong>: SGLang supports MTP but lacks full integration with DP attention, reducing efficiency in mixed parallelism configurations.</li>
<li><strong>EPLB Distribution</strong>: The experiments in this blog utilizes in-distribution data for Expert Parallelism Load Balancer (EPLB), which may not reflect real-world variability. Future work should experiment performances when having distribution shifts.</li>
<li><strong>Flexible Tensor Parallelism (TP) Sizes</strong>: For DeepSeek-V3, memory-optimal TP sizes for dense FFNs are small but larger than 1. Currently, SGLang only supports pure TP or DP, leading to suboptimal memory use. Flexible TP options are needed.</li>
<li><strong>Blackwell Support</strong>: Currently, our implementation supports only the NVIDIA Hopper architecture. We are actively working to extend compatibility to the next-generation Blackwell architecture. If you are interested in supporting or sponsoring this development, welcome to contact <a href="mailto:lmsys.org@gmail.com">lmsys.org@gmail.com</a>.</li>
</ol>
<h2>Conclusion</h2>
<p>By leveraging PD disaggregation, EP, and a carefully crafted parallelism design, we’ve reproduced DeepSeek’s inference framework in SGLang with exceptional performance. Our open-source efforts—achieving 52.3k input tokens per second and 22.3k output tokens per second—demonstrate SGLang’s power for large-scale LLM inference. We invite the community to explore, replicate, and extend this work to push the boundaries of efficient AI deployment.</p>
<h2>Acknowledgment</h2>
<p>We would like to express our heartfelt gratitude to the following teams and collaborators:</p>
<ul>
<li><strong>SGLang Core Team and Community Contributors</strong> — Jingyi Chen, Cheng Wan, Liangsheng Yin, Baizhou Zhang, Ke Bao, Jiexin Liang, Xiaoyu Zhang, Yanbo Yang, Fan Yin, Chao Wang, Laixin Xie, Runkai Tao, Yuhong Guo, Kaihong Zhang, Lei Yu, Yu-Hsuan Tseng, Qilin Tian, Peng Zhang, Yi Zhang, Yineng Zhang, Byron Hsu, and many others.</li>
<li><strong><a href="https://www.atlascloud.ai/">Atlas Cloud</a> Team</strong> —  Jerry Tang, Wei Xu, Simon Xue, Harry He, Eva Ma, and colleagues — for providing a 96-device NVIDIA H100 cluster and offering responsive engineering support.</li>
<li><strong>NVIDIA Solution Architect Team</strong> — Xuting Zhou, Jinyan Chen, and colleagues — for their work on the seamless integration of expert parallelism.</li>
<li><strong>NVIDIA Enterprise Product Team</strong> — Trevor Morris, Elfie Guo, Kaixi Hou, Kushan Ahmadian, and colleagues — for optimizing the DeepSeek R1 kernels.</li>
<li><strong>LinkedIn Team</strong> — Biao He, Qingquan Song, Chunan Zeng, Yun Dai, Yubo Wang, and colleagues — for optimizing the Flash-Attention 3 backend.</li>
<li><strong>Mooncake Team</strong> — Shangming Cai, Teng Ma, Mingxing Zhang, and colleagues — for their collaboration on PD disaggregation in SGLang.</li>
<li><strong>FlashInfer Team</strong> — Zihao Ye, Yong Wu, Yaxing Cai — for additional DeepSeek R1 kernel optimizations.</li>
<li><strong>Dynamo Team</strong> - Kyle Kranen, Vikram Sharma Mailthody, and colleagues - for extra support on PD disaggregation in SGLang.</li>
</ul>
<p>Thank you all for your invaluable support and collaboration.</p>
<h2>Appendix</h2>
<p><strong>Related PRs</strong>: <a href="https://github.com/sgl-project/sglang/pull/1970">#1970</a> <a href="https://github.com/sgl-project/sglang/pull/2925">#2925</a> <a href="https://github.com/sgl-project/sglang/pull/4068">#4068</a> <a href="https://github.com/sgl-project/sglang/pull/4165">#4165</a> <a href="https://github.com/sgl-project/sglang/pull/4232">#4232</a> <a href="https://github.com/sgl-project/sglang/pull/4390">#4390</a> <a href="https://github.com/sgl-project/sglang/pull/4435">#4435</a> <a href="https://github.com/sgl-project/sglang/pull/4521">#4521</a> <a href="https://github.com/sgl-project/sglang/pull/4654">#4654</a> <a href="https://github.com/sgl-project/sglang/pull/4767">#4767</a> <a href="https://github.com/sgl-project/sglang/pull/4770">#4770</a> <a href="https://github.com/sgl-project/sglang/pull/4836">#4836</a> <a href="https://github.com/sgl-project/sglang/pull/4880">#4880</a> <a href="https://github.com/sgl-project/sglang/pull/4957">#4957</a> <a href="https://github.com/sgl-project/sglang/pull/5068">#5068</a> <a href="https://github.com/sgl-project/sglang/pull/5085">#5085</a> <a href="https://github.com/sgl-project/sglang/pull/5295">#5295</a> <a href="https://github.com/sgl-project/sglang/pull/5415">#5415</a> <a href="https://github.com/sgl-project/sglang/pull/5432">#5432</a> <a href="https://github.com/sgl-project/sglang/pull/5435">#5435</a> <a href="https://github.com/sgl-project/sglang/pull/5530">#5530</a> <a href="https://github.com/sgl-project/sglang/pull/5558">#5558</a> <a href="https://github.com/sgl-project/sglang/pull/5561">#5561</a> <a href="https://github.com/sgl-project/sglang/pull/5626">#5626</a> <a href="https://github.com/sgl-project/sglang/pull/5657">#5657</a> <a href="https://github.com/sgl-project/sglang/pull/5805">#5805</a> <a href="https://github.com/sgl-project/sglang/pull/5819">#5819</a> <a href="https://github.com/sgl-project/sglang/pull/5890">#5890</a> <a href="https://github.com/deepseek-ai/DeepEP/pull/142">DeepEP#142</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flunking my Anthropic interview again (168 pts)]]></title>
            <link>https://taylor.town/flunking-anthropic</link>
            <guid>45064284</guid>
            <pubDate>Fri, 29 Aug 2025 14:02:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taylor.town/flunking-anthropic">https://taylor.town/flunking-anthropic</a>, See on <a href="https://news.ycombinator.com/item?id=45064284">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Here's a vague overview of what just happened:</p>
<ol>
<li>I recently applied for
<a href="https://job-boards.greenhouse.io/anthropic/jobs/4816735008">Anthropic's Developer Relations role</a>.</li>
<li>My friend who works there gave me a glowing recommendation (thanks again,
dude!).</li>
<li>I completed their secret take-home assignment.</li>
<li>On top of their secret take-home assignment, I independently published
<a href="https://diggit.dev/">diggit.dev</a> and <a href="https://taylor.town/diggit-000">a companion blogpost</a>
about my [sincerely] positive experiences with Claude. I was hoping that
some unsolicited "extra credit" would make me look like an
exceptional/ambitious candidate.</li>
<li>I
<a href="https://news.ycombinator.com/item?id=45012776">posted diggit.dev to HackerNews</a>
and it hit the frontpage!</li>
<li>I submitted my take-home assignment and my unsolicited extra credit.</li>
<li>They sent me
<a href="https://www.reddit.com/r/jobs/comments/1j98vu0/the_word_unfortunately_is_literally_traumatizing/">the "unfortunately" email</a>.</li>
</ol>
<p>Anthropic obviously didn't do anything wrong. I'm just bummed.</p>
<p>Claude Code truly is one of my favorite dev tools ever, and if you've suffered
through <a href="https://taylor.town/about#appearances">my talks/interviews</a>, you're probably sick of my
enthusiasm for software. I was particularly excited to interview with Anthropic
because I respect their approach to responsible AI adoption. <a href="https://taylor.town/">This very blog</a>
is too often a crazed celebration of humans, of software, of AI, of progress, of
sincerity -- I, I felt like I was a perfect fit.</p>
<p>The first time I flunked an Anthropic interview (ca. 2022), I accidentally
clicked a wrong button during their automated coding challenge. It was easy to
swallow that failure. I made an honest mistake; I expect companies to reject
candidates who make honest mistakes during interviews.</p>
<p>This is different. I didn't misclick any buttons. My best wasn't good enough.
I'm not good enough.</p>
<p>This essay started as a fantasy: some hero at Anthropic reads this on HackerNews
and vouches for me and I get the job and I help them guide humanity toward
post-scarity AI abundance, forever and ever, amen. I'm ashamed of these
thoughts. It's the same folly of explaining to an ex-girlfriend why she's wrong
about <em>her own experience</em>.</p>
<p>Dating was difficult for me. I don't mind feeling ugly or low-status or whatever
-- I know my place. But it hurts to feel seen, feel considered, but ultimately
rejected due to mysterious forces: "He's cute, but he's too weird."</p>
<p>Yes, I'm weird. My eccentric habits have been an overall boon for my career, for
my relationships, for my well-being. But it's moments like these when I just
want to turn all my weird off. I want to be a square peg for this square hole
and do honest work and feed my family and help humanity thrive.</p>
<p>I can't turn my weird off, so I think I defensively dial it up sometimes. I
exaggerate my eccentricities. It's easy to swallow criticism when it isn't the
<em>real</em> me, when it isn't <em>my best</em>, when it's <em>honest mistakes</em> -- what a load
of crap. This is me. This is my best. Hello, world.</p>
<p>Now it's all coming back in waves, in gasps -- I spent so much of my life being
an unlikable jerk. Becoming somebody else has been slow/painful and I'm so
deeply afraid of regressing. Over the past decade, I've been striving to spread
joy, to do good, to be better. I'm trying so hard.</p>
<p>And all this keyboard vomit is a promise to myself that I'm not giving up. I
hate this feeling, and I'm staring these nightmares straight in their stupid
eyeballs, and they're not blinking. I am still alive, and I have so much more to
do.</p>
<p>I'm okay. I mean it. I don't need (or deserve) your sympathy. I'm so lucky to be
alive, at this time, at this place, in this body, with these people. My life is
great, and it will get even better if I keep putting in this effort.</p>
<p>Spewing my insides like this onto The Internet is terrifying, but I suspect many
strangers are facing similar feelings. It's rough out there. Whatever it is,
wherever you are, I hope this helps. You've got this. You're not alone, and
we're only human.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Private Equity Snaps Up Disability Services, Challenging Regulators (158 pts)]]></title>
            <link>https://www.governing.com/management-and-administration/private-equity-snaps-up-disability-services-challenging-regulators</link>
            <guid>45063972</guid>
            <pubDate>Fri, 29 Aug 2025 13:37:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.governing.com/management-and-administration/private-equity-snaps-up-disability-services-challenging-regulators">https://www.governing.com/management-and-administration/private-equity-snaps-up-disability-services-challenging-regulators</a>, See on <a href="https://news.ycombinator.com/item?id=45063972">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
                                
                                    Private equity companies have gobbled up group homes and other services for people with disabilities, attracting the attention of state and federal regulators across the nation and alarming advocates.</p><p>People with intellectual or developmental disabilities have suffered abuse, neglect and even death while under the care of private equity-owned providers, according to a recent <a href="https://pestakeholder.org/wp-content/uploads/2025/03/PESP_Report_IDD_2025.pdf" target="_blank" data-cms-ai="0">report</a> from watchdog group Private Equity Stakeholder Project.</p><p>“Private equity firms are, more than many other types of investors, laser-focused on maximizing their cash flow, often trying to double or triple their investment over a relatively short period of time, usually just a handful of years,” said Eileen O’Grady, the report’s author. “The way that private equity firms will often do that is to cut costs.”</p><p><span data-bsp-pv="4f8e492c-6f2f-390e-bc61-f176d3a37ab9"></span><span data-bsp-pv="00000196-d9df-d2f6-ab96-dddfa7e50000"></span><br>For companies that provide essential services for people with disabilities, she said, “those cuts can have really harmful impacts on people’s lives.”</p><p>In late 2023, Florida moved to revoke the license of NeuroRestorative, one branch of the private equity-owned health services company Sevita, which provides services for people with disabilities. State regulators cited repeat violations by NeuroRestorative and a failure to “protect the rights of its clients to be free from physical abuse.” Ultimately the state opted not to revoke the license and fined the company $13,000 in a settlement.</p><p>But in recent years regulators have documented instances of patient harm at Sevita’s affiliates in multiple other states, including Colorado, Indiana, Iowa, Massachusetts and Utah. In 2019, a U.S. Senate committee <a href="https://www.finance.senate.gov/chairmans-news/grassley-wyden-probe-reports-of-abuse-at-group-homes-in-iowa-and-oregon" target="_blank" data-cms-ai="0">conducted a probe</a> into the company’s operations in <a href="https://www.finance.senate.gov/imo/media/doc/2020-12-03%20Investigative%20Report%20(REM%20Iowa).pdf" target="_blank" data-cms-ai="0">Iowa</a> and <a href="https://www.finance.senate.gov/imo/media/doc/120220%20Life%20at%20Cypress%20House%20-%20An%20Examination%20of%20Care%20Provided%20by%20MENTOR%20Oregon.pdf" target="_blank" data-cms-ai="0">Oregon</a> following multiple reports of patient abuse and neglect.</p><p>“Any entity that receives taxpayer dollars, but especially those charged with caring for our fellow Americans who may have an intellectual disability, ought to be doing everything under the sun to ensure quality care and continually improve,” U.S. Sen. Chuck Grassley, an Iowa Republican, said in a <a href="https://www.finance.senate.gov/chairmans-news/wyden-grassley-issue-reports-on-developmental-disability-care-facilities-in-oregon-and-iowa" target="_blank" data-cms-ai="0">statement</a> in 2020 following his investigation.</p><p>In a statement to Stateline, Sevita did not address the sanctions directly, but avowed its commitment to providing services and supports to give people greater independence, regardless of their intellectual or physical challenges.</p><p>“Since 2019, when new ownership acquired the company, there has been significant capital investment to improve and expand our services, enhance facilities, implement robust training and new technologies, and strengthen our workforce — all with the goal of better serving our individuals and communities,” the statement said.</p><p>The disability care industry has proven increasingly attractive to private equity.</p><p>In recent years, a handful of large private equity-owned companies such as Sevita have snapped up hundreds of smaller providers of disability services — often community nonprofits, mom-and-pop businesses and religious organizations — and rolled them into larger corporations.</p><p>From 2013 to 2023, private equity firms acquired more than 1,000 disability and elder care providers, <a href="https://pestakeholder.org/wp-content/uploads/2025/03/PESP_Report_IDD_2025.pdf" target="_blank" data-cms-ai="0">according to the report</a> by the Private Equity Stakeholder Project. That’s likely an undercount because they’re generally not required to disclose acquisitions, the report said.</p><h3 id="cash-cow">Cash Cow</h3><p>Private equity firms use pooled investments from pension funds, sovereign wealth funds, endowments and wealthy individuals to buy a controlling stake in a company. They seek to maximize its value — often by cutting costs — and then sell it at a profit.</p><p>Most of Sevita’s revenue comes from providing disability services. It operates companies in 40 states under various brands, including Mentor Network, NeuroRestorative and REM.</p><p>Sevita is currently owned by private equity firms Centerbridge Partners and Vistria Group, which also own Help at Home, a home health company with more than 200 locations across about a dozen states.</p><p><a href="https://disclosure.spglobal.com/ratings/en/regulatory/article/-/view/type/HTML/id/3320219#:~:text=Sevita%20generating%20approximately%2085%25%2D90%25%20of%20its%20revenues%20from%20Medicaid" target="_blank" data-cms-ai="0">Nearly all of Sevita’s revenue comes from Medicaid</a>, according to a February 2025 report from S&amp;P Global.</p><p>Through Medicaid and Medicare, the government pays for most services for people with intellectual or developmental disabilities. The two programs cover services such as group homes, adult day programs, in-home care, and physical and occupational therapy.</p><p>“Sevita has been owned by private equity firms for over a decade now, and has been under investigation and scrutiny at the federal and state level for basically that entire time,” O’Grady said.</p><p>In 2022, Iowa fined a NeuroRestorative group home $10,500 after a resident was <a href="https://iowacapitaldispatch.com/2022/03/29/care-facility-is-fined-after-woman-left-unattended-in-liquor-store-consumes-vodka/" target="_blank" data-cms-ai="0">left unattended in a liquor store</a> and drank three-quarters of a bottle of vodka. The same year, Massachusetts temporarily removed Sevita’s license to operate group homes after regulators reported inadequate staff training and supervision, and a “myriad of issues that were uncovered onsite,” according to a Massachusetts Department of Developmental Services <a href="https://www.mass.gov/doc/neurorestorative-2022/download" target="_blank" data-cms-ai="0">report</a>.</p><p>The federal Centers for Medicare &amp; Medicaid Services has fined a NeuroRestorative facility in Utah <a href="https://www.medicare.gov/care-compare/details/nursing-home/465179?state=UT&amp;measure=nursing-home-penalties" target="_blank" data-cms-ai="0">four times since 2022</a>. A <a href="https://www.medicare.gov/care-compare/inspections/pdf/nursing-home/465179/health/complaint-inspection?date=2024-02-20" target="_blank" data-cms-ai="0">February 2024 inspection report</a> by the agency found the facility “failed to prevent abuse, neglect … and exploitation” of residents.</p><p>Last year, Florida fined another Sevita brand, Florida Mentor, for improper use of restraints. More issues have been documented in Sevita-owned locations in Arkansas, California, Colorado, Illinois, Indiana, New Hampshire and Nevada.</p><p>Meanwhile, Sevita’s owners, Centerbridge and Vistria, have collected nearly half a billion dollars since 2019 by loading Sevita and Help at Home with debt in order to pay dividends to investors, according to Moody’s, a financial services company.</p><p>Similar financial maneuvering <a href="https://www.pbs.org/newshour/show/investigation-reveals-how-investors-made-millions-as-steward-health-care-system-collapsed" target="_blank" data-cms-ai="0">contributed to the recent collapse of Steward Health Care</a>, a private equity-owned hospital system that once had more than 30 hospitals nationwide. Steward has become a cautionary tale about the harm that profit-driven private equity firms can do to a state’s health system.</p><p>“Before Steward Health Care ultimately collapsed, executives spent years hiding their financial information from state regulators, putting patients and our health care system at risk,” Massachusetts Democratic House Speaker Ron Mariano said in a statement earlier this year announcing a new state law that beefs up reporting and financial requirements for private investors.</p><p>“That’s why ensuring that our institutions are equipped to monitor the health care landscape, and to guard against trends and transactions that drive up costs without improving patient outcomes, is so important.”</p><h3 id="david-vs-goliath">David vs. Goliath</h3><p>After two residents of a New Jersey group home died from choking on food in 2017, attorney Cory Bernstein became interested in private equity’s involvement in disability services. The residents had been living in homes operated by AdvoServ, a company then owned by the private equity firm Wellspring Capital Management. The state had cited AdvoServ more times than any other operator in New Jersey for abuse, neglect and unsafe conditions.</p><p>AdvoServ later ceased operations in 2019 after multiple state agencies, including in New Jersey, Florida and Maryland, launched investigations.</p><p>But even when state regulators are doing all they can to protect people with disabilities from substandard care, they’re limited in how much they can hold a company accountable, Bernstein told Stateline.</p><p>“It’s state-level oversight on a national entity with not much [help] coming from the federal side,” said Bernstein, who is now a staff attorney at the National Disability Rights Network, a membership organization of federally mandated state disability advocacy programs.</p><p>“States just don’t really have the resources or tools to do what needs to be done.”</p><p>A regulatory agency in Georgia might shut down all the group homes owned by a certain company, for example, but those regulators can’t do anything about the company’s abuses in, say, Montana. With branches in multiple states, a company is better able to withstand sanctions or even a loss of license in one state, he said.</p><p>“[States] are not set up to go up against a national operator with billions of dollars in resources in a regulatory or oversight battle,” Bernstein said.</p><p>Further complicating things for state regulators and for consumers is that a large services company such as Sevita might operate under multiple brand names, even in one state. It can be hard to parse out who owns a sanctioned business. Multiple brand names can also obscure a company’s monopoly on a particular regional market.</p><p>When Florida regulators reached a settlement agreement with Sevita’s NeuroRestorative last year, the state dismissed its proposed license revocation. O’Grady believes one reason the state chose to settle is the difficulty of finding alternative facilities to relocate the residents who would have been displaced from the 13 locations the company operated around the state.</p><p>“Because of that dearth of alternatives and the impotence of the state to act more fully, this company will continue to be allowed to operate,” she said.</p><p>Further complicating oversight: Large companies often operate various services that are overseen by different agencies. Group homes might be regulated under the state’s Medicaid program, while facilities that provide more intensive care might come under federal Medicare oversight.</p><p>There could be “two completely different oversight systems for facilities serving the same population in the same state with the same name,” Bernstein said.</p><h3 id="state-solutions">State Solutions</h3><p>Some states have moved to <a href="https://stateline.org/2024/01/18/shell-game-when-private-equity-comes-to-town-hospitals-can-see-cutbacks-closures/" target="_blank" data-cms-ai="0">address problems</a> with private equity involvement in health care by passing tighter restrictions on mergers and acquisitions of health care companies.</p><p>In Rhode Island, where private equity companies’ mismanagement of health care providers <a href="https://stateline.org/2024/01/18/shell-game-when-private-equity-comes-to-town-hospitals-can-see-cutbacks-closures/#:~:text=Rhode%20Island%20roadblock" target="_blank" data-cms-ai="0">threatened the future of local hospitals</a>, a robust oversight law allowed the state attorney general to impose conditions to protect the hospitals’ finances.</p><p>More states are following suit. In 2023 alone, <a href="https://stateline.org/2024/01/18/shell-game-when-private-equity-comes-to-town-hospitals-can-see-cutbacks-closures/#:~:text=In%202023%20alone%2C%2024%20states%20enacted%20laws%20related%20to%20health%20system%20consolidation%20and%20competition" target="_blank" data-cms-ai="0">24 states</a> enacted laws related to health system consolidation and competition, while this year <a href="https://www.axios.com/2025/03/17/private-equity-health-care-state-legislation" target="_blank" data-cms-ai="0">at least half a dozen</a> have considered legislation to check private equity-fueled health care mergers.</p><p><i>This article was published by Stateline. Read the original </i><a href="https://stateline.org/2025/05/16/private-equity-snaps-up-disability-services-challenging-state-regulators/" target="_blank" data-cms-ai="0">here</a><i>.</i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grok Code Fast 1 (233 pts)]]></title>
            <link>https://x.ai/news/grok-code-fast-1</link>
            <guid>45063559</guid>
            <pubDate>Fri, 29 Aug 2025 13:01:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://x.ai/news/grok-code-fast-1">https://x.ai/news/grok-code-fast-1</a>, See on <a href="https://news.ycombinator.com/item?id=45063559">Hacker News</a></p>
Couldn't get https://x.ai/news/grok-code-fast-1: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Gun Maker Sig Sauer Citing National Security to Keep Documents from Public (258 pts)]]></title>
            <link>https://practicalshootinginsights.com/eighth-circuit-fmeca-update/</link>
            <guid>45063431</guid>
            <pubDate>Fri, 29 Aug 2025 12:51:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://practicalshootinginsights.com/eighth-circuit-fmeca-update/">https://practicalshootinginsights.com/eighth-circuit-fmeca-update/</a>, See on <a href="https://news.ycombinator.com/item?id=45063431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
        <header>
          
          

  <p>
    
      
      <span>
        
        
        <time datetime="2025-08-28T00:00:00+00:00">August 28, 2025</time>
      </span>
    

    <span></span>

    
      
      

      <span>
        
        
          4 minute read
        
      </span>
    
  </p>


        </header>
      

      <section itemprop="text">
        
        <p><em>The secrecy battle over the Army’s Failure Modes, Effects, and Criticality Analysis (FMECA) for Sig Sauer’s P320 has followed <strong>Glasscock v. Sig Sauer</strong> to the Eighth Circuit. A media intervenor is now asking the appellate court to keep key records open—and their brief places Practical Shooting Insights (this site) squarely in the middle of the story.</em></p>

<h2 id="whats-new">What’s new</h2>

<ul>
  <li><strong>The Trace intervenes in the appeal.</strong> The newsroom moved to intervene <strong>for the limited purpose of opposing</strong> sealed filings tied to class certification and the FMECA, arguing the public’s right of access and noting the district court cited the FMECA <strong>nine times</strong> when it certified the class.</li>
</ul>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-01.png" alt="Screenshot 1"></p>

<ul>
  <li><strong>Sig Sauer says “national security” and asks for deference to the Army.</strong> In opposing intervention, Sig Sauer urges the court to leave FMECA-related material sealed and to give the Army time to weigh in, framing the dispute in terms of protecting “military secrets.”</li>
</ul>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-02.png" alt="Screenshot 2"></p>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-03.png" alt="Screenshot 3"></p>

<ul>
  <li><strong>A second FMECA document emerges.</strong> Sig Sauer’s opposition confirms there are <strong>two</strong> FMECA records in the class-certification exhibits: a <strong>FMECA Spreadsheet</strong> and a <strong>FMECA Memorandum</strong>—the latter not previously described in public filings—raising fresh questions about what the memo contains and who authored it.</li>
</ul>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-04.png" alt="Screenshot 4"></p>

<ul>
  <li><strong>PSI’s reporting is part of the record.</strong> The Trace’s filing tells the court the unredacted FMECA was found on CourtListener, de‑obscured, and published on Practical Shooting Insights, where it “remains available”—and it recounts Sig Sauer’s own executive discussing it on a podcast while pointing viewers to this website.</li>
</ul>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-05.png" alt="Screenshot 5"></p>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-06.png" alt="Screenshot 6"></p>

<p>The FMECA document was previously published <a href="https://practicalshootinginsights.com/the-document-sig-sauer-doesnt-want-you-to-see-about-the-p320/">here</a>.</p>

<hr>

<h2 id="the-traces-pitch-this-isnt-secret-anymore">The Trace’s pitch: This isn’t secret anymore</h2>

<p>The Trace walks the appellate court through how the FMECA left the bottle: it was posted on this website and then widely republished; a YouTube explainer discussing it surpassed <strong>100,000 views</strong>. The filing quotes Sig Sauer’s VP of Consumer Affairs <strong>Phil Strader</strong> being asked on the <em>Behind the Lens</em> podcast why the FMECA shouldn’t be public and responding, <strong>“No, there’s not”</strong> (nothing to hide), while <strong>directing viewers to this website to see the document</strong> and describing its contents.</p>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-07.png" alt="Screenshot 7"></p>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-08.png" alt="Screenshot 8"></p>

<p>The reporting regarding Phil Strader’s interview was previously published <a href="https://practicalshootinginsights.com/sig-sauer-vp-consumer-affairs-discusses-the-document-they-didnt-want-you-to-see/">here</a></p>

<p><strong>How many times has the unredacted FMECA been “shared”?</strong> The filings don’t give a precise share count. What they <em>do</em> document is widespread republication and discussion, including the 100k‑plus video and multiple re‑posts mirroring the PSI copy. In other words, the genie is out of the bottle.</p>

<p>The Trace also points to <strong>DoD Instruction 5230.24</strong>, the policy Sig Sauer invokes, noting it <strong>does not authorize withholding unclassified information</strong> about <strong>evaluations of performance and reliability</strong> of military equipment—and that the PSI‑hosted FMECA <strong>bears no DoD distribution markings</strong>.</p>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-09.png" alt="Screenshot 9"></p>

<hr>

<h2 id="sig-sauers-response-let-the-army-decideand-keep-the-lid-on">Sig Sauer’s response: Let the Army decide—and keep the lid on</h2>

<p>Sig Sauer tells the Eighth Circuit The Trace <strong>lacks standing</strong> and that parallel briefing is already underway in the district court. Substantively, Sig Sauer leans on <strong>military‑secrets</strong> concerns, requests time for the <strong>Army to opine</strong> on release, and characterizes the FMECA as controlled technical information created under the MHS contract. (The company also recounts how the spreadsheet briefly became public in another case before being pulled down.)</p>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-10.png" alt="Screenshot 10"></p>

<p>Two details in Sig Sauer’s papers matter going forward:</p>

<p>1) <strong>The “FMECA Memorandum.”</strong> Sig Sauer identifies the memo alongside the previously published spreadsheet. If the memo is Sig Sauer‑authored, it could reveal how the company framed the Army analysis internally—an issue directly relevant to notice, risk mitigation, and marketing claims.</p>

<p>2) <strong>Ongoing Army communications.</strong> Sig Sauer’s litigation counsel filed a declaration stating he asked the Army about the FMECA’s distribution status and that key Army decision‑makers were <strong>unavailable</strong> the week of the deadline; Sig Sauer says the Army may submit information and seeks additional time.</p>

<p><img src="https://practicalshootinginsights.com/assets/images/25-08-28-01-11.png" alt="Screenshot 11"></p>

<hr>

<h2 id="the-transparency-question-distilled">The transparency question, distilled</h2>

<ul>
  <li>
    <p><strong>Is the FMECA “national‑security” material?</strong> The Trace says <strong>no</strong>—and points to DoDI 5230.24’s carve‑out: it <strong>does not provide authority to withhold</strong> unclassified evaluations of <strong>performance and reliability</strong>—exactly what a FMECA is. It also underscores the lack of any DoD marking on the PSI copy.</p>
  </li>
  <li>
    <p><strong>Is secrecy even possible at this point?</strong> The record shows the unredacted spreadsheet is <strong>online on this website</strong>, has been <strong>reposted broadly</strong>, and has been discussed by Sig Sauer’s own executive on air—who <strong>told listeners where to find it</strong>. One video discussing it has <strong>100,000+ views</strong>.</p>
  </li>
</ul>

<hr>

<h2 id="why-this-matters-to-the-class-actionand-to-owners">Why this matters to the class action—and to owners</h2>

<p>The district court relied on the FMECA repeatedly when certifying the Missouri class, including on <strong>notice</strong> and <strong>risk‑mitigation</strong> questions—the very issues consumers care about. Keeping the heart of that analysis under seal on appeal would blunt the public’s ability to scrutinize a product‑safety fight with real‑world consequences.</p>

<hr>

<h2 id="my-role-plainly">My role, plainly</h2>

<p>Practical Shooting Insights is an independent site covering the shooting‑sports and firearms industry. The Trace’s filing names PSI as the first publisher of the unredacted spreadsheet and quotes Strader pointing viewers here. I will continue to publish filings and analysis so readers can compare the arguments to the documents themselves.</p>

<hr>

<h2 id="what-to-watch-next">What to watch next</h2>

<p>1) <strong>Whether the Eighth Circuit permits intervention</strong> and applies the strong presumption of public access to <strong>class‑certification</strong> records.<br>
2) <strong>If the Army weighs in</strong>—and on what basis—regarding the FMECA’s distribution status.<br>
3) <strong>Disclosure of the FMECA Memorandum.</strong> If it’s Sig Sauer-authored, it could illuminate internal framing of hazards and fixes—material at the core of consumer‑protection claims.</p>

        
      </section>

      

      


      
  


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta might be secretly scanning your phone's camera roll (391 pts)]]></title>
            <link>https://www.zdnet.com/article/meta-might-be-secretly-scanning-your-phones-camera-roll-how-to-check-and-turn-it-off/</link>
            <guid>45062910</guid>
            <pubDate>Fri, 29 Aug 2025 12:01:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zdnet.com/article/meta-might-be-secretly-scanning-your-phones-camera-roll-how-to-check-and-turn-it-off/">https://www.zdnet.com/article/meta-might-be-secretly-scanning-your-phones-camera-roll-how-to-check-and-turn-it-off/</a>, See on <a href="https://news.ycombinator.com/item?id=45062910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="infoCardDek"><h3>   Why is Facebook cloud-processing my device's camera roll?   </h3><p>Meta is uploading and analyzing your camera roll photos and videos, even ones you haven't posted, in its cloud in order to generate AI-powered suggestions like collages, monthly recaps, themed albums, or AI-restyled versions of your images.   </p><h3>   Where is this feature being tested?   </h3><p>Meta has confirmed the feature is a test, saying, "We're exploring ways to make content sharing easier for people on Facebook by testing suggestions of ready-to-share and curated content from a person's camera roll."   </p><p>The test is currently available in the US and Canada, but it's not available in <span><a href="https://www.facebook.com/legal/ai-terms" rel="noopener nofollow sponsored" target="_blank"><span>Illinois or Texas</span><!----></a></span> due to those states' privacy laws.   </p><h3>   Did Facebook ask for my consent before turning this on?   </h3><p>Meta is showing a pop-up asking users if they want to enable cloud processing, but some users claim they haven't seen it. Instead, they found the toggles in their settings already switched on by default, raising questions about whether clear consent was given.   </p><figure><div><picture><!----> <img src="" alt="Is Meta scanning your camera roll? How to check and turn it off - right now" width="1280" height="737.28" fetchpriority="low"></picture></div> <figcaption> <span>Elyse Betters Picaro / ZDNET</span></figcaption></figure><h3> Can I remove my photos once they've been uploaded? </h3><p>ZDNET's sister site, <a href="https://www.cnet.com/tech/services-and-software/prepare-to-share-all-your-pics-with-meta-if-you-turn-on-facebooks-new-ai-photo-tool/">CNET</a>, reports that Meta pulls from your newer pictures (roughly the last 30 days) and if you disable the feature, your uploaded photos will be deleted after 30 days. The only way to confirm is by <a href="https://www.zdnet.com/article/how-to-protect-your-privacy-from-facebook-and-what-doesnt-work/">downloading your Facebook account data.</a></p><h3>   Why is this a potential privacy issue?   </h3><p>It expands Meta's reach beyond the content you've chosen to upload and share online -- into your private, unposted photos and videos. For many, that's a major red flag and a line they're not comfortable crossing, understandably so.   </p><p><strong>Also:&nbsp;</strong><a href="https://www.zdnet.com/article/metas-personal-superintelligence-dream-leaves-much-to-the-imagination/"><strong>What Zuckerberg's 'personal superintelligence' sales pitch leaves out</strong></a></p><p>Even if Meta is asking for consent to access your camera roll in order to analyze your phone's photos and provide AI-powered suggestions, the company could have done a better job of being clear and explicit about what it's trying to do.   </p><p>How many users, like me, simply dismissed the consent pop-up without fully realizing what they'd just agreed to?   </p><p><em>Editor's note: This article was updated on Aug. 24, 2025 to clarify that Meta's camera roll sharing suggestions are not turned on by default and are entirely opt-in. Still, some users say they never knowingly agreed and are finding the features enabled in their settings</em>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[If you have a Claude account, they're going to train on your data moving forward (331 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/</link>
            <guid>45062738</guid>
            <pubDate>Fri, 29 Aug 2025 11:39:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/">https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/</a>, See on <a href="https://news.ycombinator.com/item?id=45062738">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Anthropic sent out an email, saying they will train on personal data.  They made it sound like you have to opt in, but when I click the privacy link it defaults to on.  If you don’t want your data trained on, you better manually turn it off.</p>

<p>Email:</p>

<p>Hello,</p>

<p>We're writing to inform you about important updates to our Consumer Terms and Privacy Policy. These changes will take effect on September 28, 2025, or you can choose to accept the updated terms before this date when you log in to Claude.ai. </p>

<p>These changes only affect Consumer accounts (Claude Free, Pro, and Max plans). If you use Claude for Work, via the API, or other services under our Commercial Terms or other Agreements, then these changes don't apply to you. </p>

<p>What's changing?</p>

<ol>
<li>Help improve Claude by allowing us to use your chats and coding sessions to improve our models</li>
</ol>

<p>With your permission, we will use your chats and coding sessions to train and improve our AI models. If you accept the updated Consumer Terms before September 28, your preference takes effect immediately. </p>

<p>If you choose to allow us to use your data for model training, it helps us:
Improve our AI models and make Claude more helpful and accurate for everyone
Develop more robust safeguards to help prevent misuse of Claude
We will only use chats and coding sessions you initiate or resume after you give permission. You can change your preference anytime in your Privacy Settings.</p>

<ol>
<li>Updates to data retention– your choices and controls</li>
</ol>

<p>If you choose to allow us to use your data for model training, we’ll retain this data for 5 years. This enables us to improve Claude through deeper model training as described above, while strengthening our safety systems over time. You retain full control over how we use your data: if you change your training preference, delete individual chats, or delete your account, we'll exclude your data from future model training. Learn more about our data retention practices here.</p>

<p>Learn more and next steps
For detailed information about these changes:
Read our blog post about these updates
Review the updated Consumer Terms and Privacy Policy
Visit our Privacy Center for more information about our practices
See our Help Center articles on how to manage your privacy settings
Next time you log into Claude, review the terms and confirm your settings
If you have questions about these updates, please visit our Help Center.</p>

<p>–The Anthropic Team</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic reverses privacy stance, will train on Claude chats (687 pts)]]></title>
            <link>https://www.perplexity.ai/page/anthropic-reverses-privacy-sta-xH4KWU9nS3KH4Aj9F12dvQ</link>
            <guid>45062683</guid>
            <pubDate>Fri, 29 Aug 2025 11:29:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.perplexity.ai/page/anthropic-reverses-privacy-sta-xH4KWU9nS3KH4Aj9F12dvQ">https://www.perplexity.ai/page/anthropic-reverses-privacy-sta-xH4KWU9nS3KH4Aj9F12dvQ</a>, See on <a href="https://news.ycombinator.com/item?id=45062683">Hacker News</a></p>
Couldn't get https://www.perplexity.ai/page/anthropic-reverses-privacy-sta-xH4KWU9nS3KH4Aj9F12dvQ: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla said it didn't have key data in a fatal crash. Then a hacker found it (545 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2025/08/29/tesla-autopilot-crashes-evidence-testimony-wrongful-death/</link>
            <guid>45062614</guid>
            <pubDate>Fri, 29 Aug 2025 11:15:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2025/08/29/tesla-autopilot-crashes-evidence-testimony-wrongful-death/">https://www.washingtonpost.com/technology/2025/08/29/tesla-autopilot-crashes-evidence-testimony-wrongful-death/</a>, See on <a href="https://news.ycombinator.com/item?id=45062614">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/technology/2025/08/29/tesla-autopilot-crashes-evidence-testimony-wrongful-death/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The Synology End Game (333 pts)]]></title>
            <link>https://lowendbox.com/blog/they-used-to-be-good-but-now-theyve-turned-to-evil-the-synology-end-game/</link>
            <guid>45060920</guid>
            <pubDate>Fri, 29 Aug 2025 06:37:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lowendbox.com/blog/they-used-to-be-good-but-now-theyve-turned-to-evil-the-synology-end-game/">https://lowendbox.com/blog/they-used-to-be-good-but-now-theyve-turned-to-evil-the-synology-end-game/</a>, See on <a href="https://news.ycombinator.com/item?id=45060920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMDAiIGhlaWdodD0iMzAwIiB2aWV3Qm94PSIwIDAgMzAwIDMwMCI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgZmlsbD0iI2NmZDRkYiIvPjwvc3ZnPg==" decoding="async" data-src="https://lowendbox.com/wp-content/uploads/2025/08/synology-horror-300x300.png" alt="Synology Horror" width="300" height="300" data-srcset="https://lowendbox.com/wp-content/uploads/2025/08/synology-horror-300x300.png 300w, https://lowendbox.com/wp-content/uploads/2025/08/synology-horror.png 2000w" data-sizes="(max-width: 300px) 100vw, 300px">I’ve been a Synology fan for many years.&nbsp; I used to roll my own NAS servers for home, but eventually decided that quieter, more energy-friendly dedicated NAS solutions were a better path forward.&nbsp; I don’t use a lot of their on-board apps, just basic file storage.</p><p>Right now I’ve got a DS920, a DS418, and a DS1522…but I probably won’t be buying another Synology again.</p><p>Why?</p><p>Their abusive, customer-hostile policies.</p><h2>Samba Limits</h2><p>I started getting queasy when I read earlier this year that on some models, they limit how many concurrent connections you can make.&nbsp; I though this was just something setup by default in smb.conf, but in fact Synology has a proprietary wrapper around the daemon that artificially limits it.</p><p>Whiskey.&nbsp; Tango.&nbsp; Foxtrot.</p><h2>You Must Buy Your Hard Drives From Us</h2><p>For a long time, Synology has only officially supported certain hard drives.&nbsp; I don’t have a problem with this, for three reasons.&nbsp; First, it was a pretty extensive list and included all the major players (WD, Seagate, etc.).&nbsp; Second, it’s unreasonable to expect Synology to certify every single hard drive from every maker on the planet.&nbsp; And finally, it was just a support limit.&nbsp; In other words, you could use whatever hard drives you wanted, but if there was a problem, they wouldn’t be able to support you if the drive wasn’t on their list.</p><p>I could live with that.&nbsp; What I can’t live with is the new policy, implemented this year, where you&nbsp;<strong>must buy your drives from Synology</strong>.&nbsp; This only affects new models from this year forward.&nbsp; Details still seem sketchy, but rumor is that it’s going to be along the lines of “we don’t recognize your WD Black hard drive, therefore we won’t use it.”</p><p>And by the way, Synology’s hard drives aren’t all that great.&nbsp; My WD Blacks come with a 5 year warranty.&nbsp; Synology’s <a href="https://www.amazon.com/Synology-HAT3300-Plus-SATA-HAT3300-4T/dp/B0C7FPRSK3/?_encoding=UTF8&amp;pd_rd_w=9yklA&amp;content-id=amzn1.sym.9071a05a-ab8a-4eb5-82ca-461a3b81eab8%3Aamzn1.symc.a68f4ca3-28dc-4388-a2cf-24672c480d8f&amp;pf_rd_p=9071a05a-ab8a-4eb5-82ca-461a3b81eab8&amp;pf_rd_r=18V51JSBCZQK9H73NEJX&amp;pd_rd_wg=uAo2o&amp;pd_rd_r=9dd71443-966d-45b5-8873-4cc5165fefb7&amp;ref_=pd_hp_d_atf_ci_mcx_mr_ca_hp_atf_d&amp;th=1" target="_blank" rel="noopener">only come with 3 years</a>.</p><p>Golf.&nbsp; Foxtrot.&nbsp; Yankee.</p><h2>Where to Now?</h2><p>I could go back to building my own, with TrueNAS.&nbsp; In the past, my home-build NAS boxes were hand-me-down gaming PCs (because they were big enough towers) but I have to imagine one can find a case that allows tons of drives and is still powered by something modest.</p><p>Or I may look at UGREEN.&nbsp; Or Buffalo.&nbsp; Or someone else.</p><div><h3>Related Posts:</h3></div><div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMDAiIGhlaWdodD0iMTAwIiB2aWV3Qm94PSIwIDAgMTAwIDEwMCI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgZmlsbD0iI2NmZDRkYiIvPjwvc3ZnPg==" referrerpolicy="no-referrer" alt="Unknown's avatar" data-src="https://lowendbox.com/wp-content/litespeed/avatar/03b70db75435180e78e97b9a775a1568.jpg?ver=1756407522" data-srcset="https://lowendbox.com/wp-content/litespeed/avatar/b518b0434cd85c23c2be1565f12f7e84.jpg?ver=1756407522 2x" height="100" width="100" itemprop="image"></p><div itemprop="description"><p data-start="196" data-end="633">Raindog308 is a longtime LowEndTalk community administrator, technical writer, and self-described techno polymath. With deep roots in the *nix world, he has a passion for systems both modern and vintage, ranging from Unix, Perl, Python, and Golang to shell scripting and mainframe-era operating systems like MVS. He’s equally comfortable with relational database systems, having spent years working with Oracle, PostgreSQL, and MySQL.</p><p data-start="635" data-end="926">As an avid user of LowEndBox providers, Raindog runs an empire of LEBs, from tiny boxes for VPNs, to mid-sized instances for application hosting, and heavyweight servers for data storage and complex databases. He brings both technical rigor and real-world experience to every piece he writes.</p><p data-start="928" data-end="1090">Beyond the command line, Raindog is a lover of German Shepherds, high-quality knives, target shooting, theology, tabletop RPGs, and hiking in deep, quiet forests.</p><p data-start="1092" data-end="1245">His goal with every article is to help users, from beginners to seasoned sysadmins, get more value, performance, and enjoyment out of their infrastructure.</p><p data-start="1247" data-end="1357">You can find him daily in the forums at <a href="https://lowendtalk.com/" target="_new" rel="noopener" data-start="1287" data-end="1323">LowEndTalk</a> under the handle <a href="https://lowendtalk.com/profile/raindog308">@raindog308</a>.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Strange CW Keys (130 pts)]]></title>
            <link>https://sites.google.com/site/oh6dccw/strangecwkeys</link>
            <guid>45060161</guid>
            <pubDate>Fri, 29 Aug 2025 04:22:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sites.google.com/site/oh6dccw/strangecwkeys">https://sites.google.com/site/oh6dccw/strangecwkeys</a>, See on <a href="https://news.ycombinator.com/item?id=45060161">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jscontroller="zPx2U" jsaction="rcuQ6b:WYd;FaOgy:nkegzf;vD0JD:Mq3Rnf;MxH79b:Mq3Rnf" data-cookie-path="/site/oh6dccw" jsmodel="QxOCld"><p>This site uses cookies from Google to deliver its services and to analyze traffic. Information about your use of this site is shared with Google. By using this site, you agree to its use of cookies.</p><div><p><a target="_blank" href="https://www.google.com/policies/technologies/cookies/"><span>Learn more</span></a></p><p><span>Got it</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PSA: Libxslt is unmaintained and has 5 unpatched security bugs (125 pts)]]></title>
            <link>https://vuxml.freebsd.org/freebsd/b0a3466f-5efc-11f0-ae84-99047d0a6bcc.html</link>
            <guid>45060004</guid>
            <pubDate>Fri, 29 Aug 2025 03:57:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vuxml.freebsd.org/freebsd/b0a3466f-5efc-11f0-ae84-99047d0a6bcc.html">https://vuxml.freebsd.org/freebsd/b0a3466f-5efc-11f0-ae84-99047d0a6bcc.html</a>, See on <a href="https://news.ycombinator.com/item?id=45060004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Alan Coopersmith reports:</p>
<blockquote cite="https://www.openwall.com/lists/oss-security/2025/07/11/2">
	  <p>On 6/16/25 15:12, Alan Coopersmith wrote:</p>
	  <p><em>
	    BTW, users of libxml2 may also be using its sibling project, libxslt,
	    which currently has no active maintainer, but has three unfixed security issues
	    reported against it according to
	    <a href="https://gitlab.gnome.org/Teams/Releng/security/-/wikis/2025#libxml2-and-libxslt">
		https://gitlab.gnome.org/Teams/Releng/security/-/wikis/2025#libxml2-and-libxslt</a>
	  </em></p>
	  <p>2 of the 3 have now been disclosed:</p>
	  <p>(CVE-2025-7424) libxslt: Type confusion in xmlNode.psvi between stylesheet and source nodes<br>
	    <a href="https://gitlab.gnome.org/GNOME/libxslt/-/issues/139">https://gitlab.gnome.org/GNOME/libxslt/-/issues/139</a>
	    <a href="https://project-zero.issues.chromium.org/issues/409761909">https://project-zero.issues.chromium.org/issues/409761909</a></p>
	  <p>(CVE-2025-7425) libxslt: heap-use-after-free in xmlFreeID caused by `atype` corruption<br>
	    <a href="https://gitlab.gnome.org/GNOME/libxslt/-/issues/140">https://gitlab.gnome.org/GNOME/libxslt/-/issues/140</a><br><a href="https://project-zero.issues.chromium.org/issues/410569369">https://project-zero.issues.chromium.org/issues/410569369</a></p>
	  <p>Engineers from Apple &amp; Google have proposed patches in the GNOME gitlab issues,
	  but neither has had a fix applied to the git repo since there is currently no
	    maintainer for libxslt.</p>
	
</blockquote>
<p>Note that a fourth vulnerability was reported on June 18, 2025, which remains undisclosed to date (GNOME libxslt issue 148, link below), see
	  <a href="https://gitlab.gnome.org/Teams/Releng/security/-/wikis/2025#libxml2-and-libxslt">
	    https://gitlab.gnome.org/Teams/Releng/security/-/wikis/2025#libxml2-and-libxslt</a>
	</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A deep dive into Debian 13 /tmp: What's new, and what to do if you don't like it (138 pts)]]></title>
            <link>https://lowendbox.com/blog/a-deep-dive-into-debian-13s-tmp-whats-new-and-what-to-do-if-you-dont-like-it/</link>
            <guid>45059470</guid>
            <pubDate>Fri, 29 Aug 2025 02:39:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lowendbox.com/blog/a-deep-dive-into-debian-13s-tmp-whats-new-and-what-to-do-if-you-dont-like-it/">https://lowendbox.com/blog/a-deep-dive-into-debian-13s-tmp-whats-new-and-what-to-do-if-you-dont-like-it/</a>, See on <a href="https://news.ycombinator.com/item?id=45059470">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMDAiIGhlaWdodD0iMzAwIiB2aWV3Qm94PSIwIDAgMzAwIDMwMCI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgZmlsbD0iI2NmZDRkYiIvPjwvc3ZnPg==" decoding="async" data-src="https://lowendbox.com/wp-content/uploads/2025/08/debian-trixie-300x300.png" alt="Debian Trixie" width="300" height="300" data-srcset="https://lowendbox.com/wp-content/uploads/2025/08/debian-trixie-300x300.png 300w, https://lowendbox.com/wp-content/uploads/2025/08/debian-trixie-1024x1024.png 1024w, https://lowendbox.com/wp-content/uploads/2025/08/debian-trixie-150x150.png 150w, https://lowendbox.com/wp-content/uploads/2025/08/debian-trixie-768x768.png 768w, https://lowendbox.com/wp-content/uploads/2025/08/debian-trixie-1536x1536.png 1536w, https://lowendbox.com/wp-content/uploads/2025/08/debian-trixie.png 2000w" data-sizes="(max-width: 300px) 100vw, 300px">Debian 13 “Trixie” introduces an important change to /tmp.&nbsp; Traditionally, it’s been just another filesystem, albeit with some special permissions that allows everyone on the system to use it without being able to remove each other’s files.</p><p>In Trixie, it’s been moved&nbsp;<em>off</em> the disk into&nbsp;<em>memory </em>– specifically a type of memory called tmpfs.&nbsp; To quote the tmpfs man page:</p><blockquote><p><span>The<span>&nbsp; </span></span><span>tmpfs<span>&nbsp; </span></span><span>facility<span>&nbsp; </span>allows<span>&nbsp; </span>the<span>&nbsp; </span>creation of filesystems whose contents reside in virtual memory. </span><span>Since the files on such filesystems typically reside in RAM, file access is extremely fast.</span></p></blockquote><p>They’re also extremely temporary…which is what you really want.&nbsp; There’s an old story about a user who was assigned to work on the <span>T</span>ransportation <span>M</span>anagement <span>P</span>roject.&nbsp; He logged into the server where he was supposed to store his work, saw the /tmp directory, found he could upload files there, and happily spent a couple months putting all his work there.&nbsp; Alas, when the server was rebooted…</p><p>Now that is undoubtedly an urban legend, but it illustrates the true nature of /tmp.&nbsp; It’s fine if you need a disposable log fine, a PHP session file, space for sorting something, etc.&nbsp; But you shouldn’t be <em>storing</em> anything there.</p><p>This isn’t a new thing in the Linux world.&nbsp; RedHat and its ilk have used tmps for /tmp for some time.</p><p>A more serious problem than people losing files is people who use too much /tmp.&nbsp;&nbsp;The system&nbsp;<em>needs</em> /tmp to do basic functions, so if it hits 100%, things will break.&nbsp; It’s really easy to think “I’m going to download and untar this big zip file into /tmp, and then I’ll remove it after I pull out the one file I need”…and forget to remove it.&nbsp; Now you’re hogging /tmp and over time, /tmp can be filled up with junk.</p><h2>Debian 13’s tmpfs Comes With…Challenges.&nbsp; And Solutions</h2><p>Now instead of filling up disk, you’re filling up&nbsp;<em>memory</em>.&nbsp; If you download a 300MB .zip file, expand it to 1GB, and forget it, now you’re chewing up 1GB of RAM.&nbsp; Ouch.</p><p>There are two mitigating factors.&nbsp; First, by default, Debian will only allocate a maximum of 50% of RAM to the tmpfs for /tmp.&nbsp; You can change this.&nbsp; To do so, type</p><pre>systemctl edit tmp.mount</pre><p>You’ll be popped into your editor (controlled by the EDITOR environment variable) with a form to update the settings.&nbsp; At the very bottom you’ll see a template, which you can copy and edit:</p><pre><span># [Mount]</span>
<span># What=tmpfs</span>
<span># Where=/tmp</span>
<span># Type=tmpfs</span>
<span># Options=mode=1777,strictatime,nosuid,nodev,size=50%%,nr_inodes=1m</span>
</pre><p>Go back up to the part before the line “<span>Edits below this comment will be discarded” and paste in something like this:</span></p><pre>[Mount]
<span>What=tmpfs
Where=/tmp
Type=tmpfs 
Options=mode=1777,strictatime,nosuid,nodev,size=25%%,nr_inodes=1m</span></pre><p>to change it to 25% or if you want a number:</p><pre>[Mount]
<span>What=tmpfs
Where=/tmp
Type=tmpfs 
Options=mode=1777,strictatime,nosuid,nodev,size=1G,nr_inodes=1m</span></pre><p>to change it to 1GB.</p><p>For example, I have a Debian 13 VPS with 4GB of RAM.&nbsp; After a fresh install, I see it’s using 2GB max for tmpfs:</p><pre><span># findmnt --target /tmp</span>
<span>TARGET SOURCE FSTYPE OPTIONS</span>
<span>/tmp <span>&nbsp; </span>tmpfs<span>&nbsp; </span>tmpfs<span>&nbsp; </span>rw,nosuid,nodev,size=2007704k,nr_inodes=1048576,inode64</span></pre><p>Note that this is a&nbsp;<strong>maximum</strong>.&nbsp; If there’s nothing in /tmp, /tmp does not use any memory.</p><p>After doing the systemctl edit, like this:</p><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyODA4IiBoZWlnaHQ9IjI2MCIgdmlld0JveD0iMCAwIDI4MDggMjYwIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjY2ZkNGRiIi8+PC9zdmc+" width="2808" height="260" decoding="async" data-src="https://lowendbox.com/wp-content/uploads/2025/08/systemctl-edit-tmp.mount_.png" alt="systemctl edit tmp.mount"></p><p>I get the message:</p><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMDI0IiBoZWlnaHQ9IjIwNiIgdmlld0JveD0iMCAwIDEwMjQgMjA2Ij48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjY2ZkNGRiIi8+PC9zdmc+" decoding="async" data-src="https://lowendbox.com/wp-content/uploads/2025/08/systemctl-edit-tmp.mount2_-1024x206.png" alt="sysctmctl edit tmp.mount output" width="1024" height="206" data-srcset="https://lowendbox.com/wp-content/uploads/2025/08/systemctl-edit-tmp.mount2_-1024x206.png 1024w, https://lowendbox.com/wp-content/uploads/2025/08/systemctl-edit-tmp.mount2_-300x60.png 300w, https://lowendbox.com/wp-content/uploads/2025/08/systemctl-edit-tmp.mount2_-768x155.png 768w" data-sizes="(max-width: 1024px) 100vw, 1024px"></p><p>Before this change, /tmp was at 2GB (half of the 4GB RAM):</p><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNzA4IiBoZWlnaHQ9IjIxNCIgdmlld0JveD0iMCAwIDI3MDggMjE0Ij48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjY2ZkNGRiIi8+PC9zdmc+" width="2708" height="214" decoding="async" data-src="https://lowendbox.com/wp-content/uploads/2025/08/tmp-before.png" alt="/tmp before "></p><p>Now, after reloading systemd and restarting tmp.mount, I see /tmp is limited to 1GB:</p><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNzkyIiBoZWlnaHQ9IjI2OSIgdmlld0JveD0iMCAwIDI3OTIgMjY5Ij48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjY2ZkNGRiIi8+PC9zdmc+" width="2792" height="269" decoding="async" data-src="https://lowendbox.com/wp-content/uploads/2025/08/tmp-after.png" alt="/tmp after"></p><h2>Cleanup</h2><p>The second mitigating factor is that /tmp is now automatically cleaned up.&nbsp; Quoting the <a href="https://www.debian.org/releases/trixie/release-notes/issues.en.html#the-directories-tmp-and-var-tmp-are-now-regularly-cleaned" target="_blank" rel="noopener">release notes</a>:</p><blockquote><p>The new default behavior is for files in&nbsp;<code><span>/tmp</span></code>&nbsp;to be automatically deleted after 10 days from the time they were last used (as well as after a reboot). Files in&nbsp;<code><span>/var/tmp</span></code>&nbsp;are deleted after 30 days (but not deleted after a reboot).</p></blockquote><p>You can modify these policies, exclude certain files (why?&nbsp; they’re temporary!), or even apply it to other directories.&nbsp; Consult the <a href="https://manpages.debian.org/testing/systemd-standalone-tmpfiles/tmpfiles.d.5.en.html" target="_blank" rel="noopener">fine manual</a> but I think for 99% of people, the defaults are just fine.&nbsp; I might be tempted to make the cleanup a little more aggressive, like 3 days.</p><h2>Thinking in a LowEnd Context</h2><p>One concern is for very low-memory systems.&nbsp; While 1GB has become the smallest VM for a lot of people, 512s are still sold.&nbsp; Allowing /tmp to consume 256MB out of 512 (which is really only 470-480 after the kernel and vital system processes are loaded) is a lot more impactful than consuming 256MB on a 10GB or 20GB filesystem.</p><p>Fortunately, opting out of the new tmpfs world is easy if you don’t like it:</p><pre><span>systemctl</span>&nbsp;<span>mask</span>&nbsp;<span>tmp.mount</span></pre><p>and reboot.&nbsp; I did that on the test box above:</p><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI3NDIiIGhlaWdodD0iODciIHZpZXdCb3g9IjAgMCA3NDIgODciPjxyZWN0IHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiIGZpbGw9IiNjZmQ0ZGIiLz48L3N2Zz4=" decoding="async" data-src="https://lowendbox.com/wp-content/uploads/2025/08/tmp-without-tmpfs-1024x120.png" alt="tmp without tmpfs" width="742" height="87" data-srcset="https://lowendbox.com/wp-content/uploads/2025/08/tmp-without-tmpfs-1024x120.png 1024w, https://lowendbox.com/wp-content/uploads/2025/08/tmp-without-tmpfs-300x35.png 300w, https://lowendbox.com/wp-content/uploads/2025/08/tmp-without-tmpfs-768x90.png 768w, https://lowendbox.com/wp-content/uploads/2025/08/tmp-without-tmpfs.png 1496w" data-sizes="(max-width: 742px) 100vw, 742px"></p><p>Now I can put 17GB of junk there.&nbsp; Fortunately, it will be cleaned up as described above.</p><p><strong>So how are you planning to handle Debian 13’s new tmpfs-based /tmp?</strong></p><div><h3>Related Posts:</h3></div><div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMDAiIGhlaWdodD0iMTAwIiB2aWV3Qm94PSIwIDAgMTAwIDEwMCI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgZmlsbD0iI2NmZDRkYiIvPjwvc3ZnPg==" referrerpolicy="no-referrer" alt="Unknown's avatar" data-src="https://lowendbox.com/wp-content/litespeed/avatar/03b70db75435180e78e97b9a775a1568.jpg?ver=1756407522" data-srcset="https://lowendbox.com/wp-content/litespeed/avatar/b518b0434cd85c23c2be1565f12f7e84.jpg?ver=1756407522 2x" height="100" width="100" itemprop="image"></p><div itemprop="description"><p data-start="196" data-end="633">Raindog308 is a longtime LowEndTalk community administrator, technical writer, and self-described techno polymath. With deep roots in the *nix world, he has a passion for systems both modern and vintage, ranging from Unix, Perl, Python, and Golang to shell scripting and mainframe-era operating systems like MVS. He’s equally comfortable with relational database systems, having spent years working with Oracle, PostgreSQL, and MySQL.</p><p data-start="635" data-end="926">As an avid user of LowEndBox providers, Raindog runs an empire of LEBs, from tiny boxes for VPNs, to mid-sized instances for application hosting, and heavyweight servers for data storage and complex databases. He brings both technical rigor and real-world experience to every piece he writes.</p><p data-start="928" data-end="1090">Beyond the command line, Raindog is a lover of German Shepherds, high-quality knives, target shooting, theology, tabletop RPGs, and hiking in deep, quiet forests.</p><p data-start="1092" data-end="1245">His goal with every article is to help users, from beginners to seasoned sysadmins, get more value, performance, and enjoyment out of their infrastructure.</p><p data-start="1247" data-end="1357">You can find him daily in the forums at <a href="https://lowendtalk.com/" target="_new" rel="noopener" data-start="1287" data-end="1323">LowEndTalk</a> under the handle <a href="https://lowendtalk.com/profile/raindog308">@raindog308</a>.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lucky 13: a look at Debian trixie (192 pts)]]></title>
            <link>https://lwn.net/Articles/1033474/</link>
            <guid>45059160</guid>
            <pubDate>Fri, 29 Aug 2025 01:55:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/1033474/">https://lwn.net/Articles/1033474/</a>, See on <a href="https://news.ycombinator.com/item?id=45059160">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<b>Benefits for LWN subscribers</b>
<p>
The primary benefit from <a href="https://lwn.net/Promo/nst-nag5/subscribe">subscribing to LWN</a>
       is helping to keep us publishing, but, beyond that, subscribers get
       immediate access to all site content and access to a number of extra
       site features.  Please sign up today!
</p></blockquote>

<p>After more than two years of development, the Debian Project has <a href="https://www.debian.org/News/2025/20250809">released</a> its new stable version, Debian&nbsp;13 ("trixie"). The release comes with the usual bounty of
upgraded packages and more than 14,000 new packages; it also debuts <a href="https://lwn.net/Articles/1017315/">Advanced Package Tool
(APT)&nbsp;3.0</a> as the default package manager and makes 64-bit
<span>RISC-V</span> a supported architecture. There are few surprises with trixie,
which is exactly what many Linux users are hoping for—a free
operating system that just works as expected.</p>

<p>Debian's <a href="https://www.debian.org/releases/">stable</a>
releases are aptly named; the project prioritizes stability over
shipping the latest software. The <a href="https://release.debian.org/trixie/freeze_policy.html">freeze
schedule for trixie</a> called for a soft freeze in April, which meant
that (for example) the <a href="https://kde.org/announcements/plasma/6/6.4.0/">KDE&nbsp;Plasma&nbsp;6.4</a>
release in June was too late to make the cut—even though trixie
was not released until August. Users who prefer to live on the edge
will want to run another distribution or follow Debian development by
running the <a href="https://www.debian.org/releases/testing/">testing</a> release
that previews the next stable <span>version—<a href="https://wiki.debian.org/DebianForky">Debian&nbsp;14&nbsp;("forky")</a></span>. Truly
adventurous users may take their chances with the <a href="https://www.debian.org/releases/sid/">unstable ("sid")
release</a>.</p>

<p>That said, trixie is up-to-date enough for many folks; it includes
GNOME&nbsp;48, KDE&nbsp;Plasma&nbsp;6.3, Xfce&nbsp;4.20, GNU
Emacs&nbsp;30.1, GnuPG&nbsp;2.4.7, LibreOffice&nbsp;25.2, and
more. Under the hood, it includes the most recent Linux LTS kernel
(6.12.41), GNU Compiler Collection (GCC) 14.2, GNU C Library (glibc)
2.41, LLVM/Clang 19, Python&nbsp;3.13, Rust&nbsp;1.85, and
systemd&nbsp;257. The release notes have a <a href="https://www.debian.org/releases/trixie/release-notes/whats-new.en.html#desktops-and-well-known-packages">section
for well-known software</a> that compares the version in Debian 12
against Debian 13. While some of the versions lag a bit behind the
upstream, they are not woefully outdated.</p>

<p>The project now <a href="https://www.debian.org/releases/stable/armhf/ch02s01.en.html">supports
six major hardware architectures</a>: x86-64/amd64, 32-bit Arm with a
hardware FPU (armhf), 64-bit Arm (arm64), IBM POWER8 or newer
(ppc64el), IBM S/390 (s390x), and 64-bit RISC-V. The i386 architecture
is not supported for trixie, though the project continues to
build some i386 packages to run on 64-bit systems; users with i386 systems cannot upgrade to
trixie. The <a href="https://www.debian.org/releases/trixie/release-notes/issues.en.html#mips-architectures-removed">MIPS</a>
architectures (mipsel and mis64el) have also been removed in trixie.</p>

<p>The <a href="https://wiki.debian.org/ArmEabiPort">Arm EABI</a>
(armel) port that targets older 32-bit Arm devices prior to Arm v7 is
still supported with trixie, but this release is the end of the
line. There is no installation media for armel systems, but users who
have bookworm installed can upgrade to trixie if they have supported
hardware: the Raspberry&nbsp;Pi&nbsp;1, Zero, and Zero&nbsp;W are the
only devices mentioned <a href="https://www.debian.org/releases/trixie/release-notes/issues.html#last-release-for-armel">in
the release notes</a>.</p>

<p>Upgrades from bookworm are supported, of course. The release
notes <a href="https://www.debian.org/releases/trixie/release-notes/upgrading.en.html#preparing-apt-sources-files">suggest</a> that users convert APT source files to the <a href="https://repolib.readthedocs.io/en/latest/deb822-format.html">DEB822&nbsp;format</a>
before the upgrade. APT&nbsp;3.0
includes an "<tt>apt&nbsp;modernize-sources</tt>" command to convert APT data
source files to DEB822, but that is not available in bookworm. Users are
also expected to <a href="https://www.debian.org/releases/trixie/release-notes/upgrading.en.html#start-from-pure-debian">remove
all third-party packages</a> prior to running the upgrade. I tested
the upgrade on one of my servers, after taking a snapshot to roll back
to if needed, and all went smoothly. Users who are considering an
upgrade should read the release notes carefully before forging ahead;
in particular, users should be aware that it's possible (but not
certain) for <a href="https://www.debian.org/releases/trixie/release-notes/issues.html#network-interface-names-may-change">network interface names to change</a> on upgrade.</p>

<h4>Installation</h4>

<p>For users who want to start fresh, Debian offers <a href="https://www.debian.org/releases/trixie/debian-installer/">a
variety of installer images</a> and download methods; users can choose
a 64MB minimal ISO image with the <a href="https://wiki.debian.org/DebianInstaller/Netboot">netboot</a>
installer, all the way up to a set of Blu-ray images. The project
recommends using <a href="https://en.wikipedia.org/wiki/BitTorrent">BitTorrent</a> or <a href="https://www.einval.com/~steve/software/jigdo/">Jigsaw
Download</a> (jigdo) for the largest images. BitTorrent probably needs
no introduction, but jigdo is not as well-known. Jigdo is a method of
downloading all of the individual packages for an image from multiple
mirrors and then assembling them into an ISO image on the user's
machine. It was a bit fiddly to use jigdo to download an image, but
not overly so—and the speed of the whole process was comparable
to simply downloading an ISO of the same size.</p>

<p>Debian's <a href="https://www.debian.org/CD/netinst/">network
install</a> ("netinst") image is probably the best option for server
installations and for experienced Linux users; it includes the
packages required for a base install and then fetches the remaining
software from Debian mirrors. Unlike the tiny netboot image, it
includes the option of using either the graphical installer or the
text-based installer.</p>

<p>The installer is a bit of a throwback to an earlier era when users
were expected to know a lot more about the workings of a Linux system. 
Users who have only worked with distributions like Fedora and Ubuntu
will notice that installing Debian requires many more steps than other
popular distributions. For example, many desktop distributions have
eliminated the step of setting a password for the root
user—instead, it is generally assumed that the primary user will
also be the system administrator, so the default is to give the
primary user sudo privileges instead. Debian does not take that
approach; <del>in fact, there is no way to give a user sudo privileges
during installation. Setting up sudo has to be done manually after
the installation is completed</del> <strong>Update:</strong> Users can skip creation of a root account and the installer will then set up the regular user as an administrator with sudo permissions. Apologies for the error.</p>

<p>For some folks, installing Debian will be a bit of a chore and may
even be confusing for users who are new to Linux. For example, the
text-mode installer requires users to specify the device for GRUB boot
loader installation, without providing a default. If one chooses an
invalid partition, the installer tells the user that the operation has
failed and drops back to a menu listing all the installation
steps. Presumably if one picks the <em>wrong</em> partition it will
happily install GRUB to that and render the system unbootable. This is
not insurmountable for experienced Linux users, but it would no doubt
be a hurdle for many users.</p>

<p>More experienced Linux users are likely to appreciate the
amount of control offered by the installer. For example, Fedora's
recent web-based installer makes it difficult to even find the option to
perform custom partitioning. Debian has a guided partitioning option
for those who do not want to fuss with it, but the option to create
custom partitions is not hidden from the user.</p>

<p>Debian has a better installation option for newer Linux users,
though it is easy to miss: the <a href="https://www.debian.org/CD/live/">live install images</a>, which
use the <a href="https://calamares.io/">Calamares</a> installer. Its
workflow is more akin to the installation process one finds with
Fedora and Ubuntu; it also sets up the primary user with sudo
privileges rather than creating a root password. Unfortunately,
the live images are not listed on the main page for installer
images—though they are mentioned, briefly, in the release
notes.</p>

<blockquote>
<a href="https://lwn.net/Articles/1034312/#id">
<img src="https://static.lwn.net/images/2025/calamares-sm.png" alt="[Debian Calamares installer]" title="Debian Calamares installer">
</a>
</blockquote>

<p>The Debian installer also has the option of using a Braille display
and/or speech synthesizer voice for the installation. I have not tried
these options, but they are available for users who need them.</p>

<h4>X.org</h4>

<p>Many distributions are in the process of phasing out X.org support
for GNOME and KDE as the upstream projects have started doing so.
For example, Fedora <a href="https://pagure.io/fesco/issue/3408">will remove X.org session support
for GNOME in Fedora&nbsp;43</a>, and the plan is for <a href="https://lwn.net/Articles/1024758/">Ubuntu to do the same</a>
in its upcoming 25.10 release. GNOME will be completely removing X.org
support in GNOME&nbsp;49, which is planned for September.</p>

<p>Much has already been said about this, of course, and there is
likely little new left to be said or that <em>needs</em> to be
said. However, for users who still need or want X.org support,
Debian&nbsp;13 includes X.org sessions for GNOME and KDE. In testing
trixie, I've spent some time in the GNOME and KDE X.org sessions as
well as the Wayland sessions; if there are any gotchas or horrible
bugs, I haven't encountered them (yet). This might be a compelling
reason for some folks to switch to (or stick with) Debian.</p>

<h4>Trying trixie</h4>

<p>I use Debian for my personal web site and blogs, but it has been
quite some time since I used it as my primary desktop operating
system. Debian (and Ubuntu) derivatives, such as <a href="https://linuxmint.com/">Linux&nbsp;Mint</a> and <a href="https://system76.com/pop/">Pop!_OS</a>, yes—but it's been
several years since I've used vanilla Debian on the desktop for
more than casual tinkering.</p>

<p>The Debian release announcement boasts about the number of packages
included in trixie: 64,419 packages total, with 14,100 added and more
than 6,000 removed as <a href="https://www.debian.org/releases/trixie/release-notes/upgrading.en.html#obsolete">obsolete</a>
since bookworm. That is quite a few packages, but falls short of some
other distributions. For example, "<tt>dnf repoquery --repo=fedora
--available</tt>" shows more than 76,000 packages available for
Fedora&nbsp;42.</p>

<p>After installing Debian, I went to install some of my preferred
software, such as <a href="https://lwn.net/Articles/993498/">aerc</a>,
<a href="https://lwn.net/Articles/1004377/">Ghostty</a>, <a href="https://lwn.net/Articles/1025866/">niri</a>, and <a href="https://lwn.net/Articles/987315/">Speech Note</a>. The aerc
packages in trixie are current, but Ghostty and niri are not packaged
for Debian at all. Ghostty is written in Zig, which is also not
available, so users who want to build it from source will need to
install Zig separately and then build Ghostty. Speech Note is packaged
as a Flatpak, but Debian does not enable Flatpaks or Flathub in the
GNOME Software Store by default. Users who want Flatpaks on Debian via
Flathub will need to install the <tt>flatpak</tt> package and manually
add the Flathub repo:</p>

<pre>&nbsp;&nbsp;&nbsp;&nbsp;flatpak remote-add --if-not-exists flathub \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;https://dl.flathub.org/repo/flathub.flatpakrepo
</pre>

<p>Users will need to add the <tt>gnome-software-plugin-flatpak</tt>
package for Flatpak support in GNOME&nbsp;Software, and
<tt>plasma-discover-backend-flatpak</tt> to add it to
KDE&nbsp;Discover.</p>

<p>Trixie ships with the Firefox extended-support release (ESR) by
default: <a href="https://www.firefox.com/en-US/firefox/128.0esr/releasenotes/">Firefox
128</a>, which was released in July 2024. Happily,
Mozilla offers a <a href="https://support.mozilla.org/en-US/kb/install-firefox-linux">Debian
repository</a> for those who want to run more current versions. Even
better, there is a little-advertised utility called <a href="https://packages.debian.org/trixie/extrepo">extrepo</a> that
has a curated list of external repositories users might want to enable
for Debian. To enable the Mozilla repository, for example, a user only
needs to install extrepo, run
"<tt>extrepo&nbsp;enable&nbsp;mozilla</tt>" as root (or with
sudo), update the package cache, and look for the regular
Firefox package. In all, extrepo includes more than 160 external
repositories for applications like <a href="https://docs.docker.com/engine/install/">Docker&nbsp;CE</a>, <a href="https://signal.org/download/">Signal</a>, and <a href="https://syncthing.net/">Syncthing</a>. Unfortunately, the
extrepo utility does not have a separate "list" command to show the
available repositories, though running "<tt>extrepo&nbsp;search</tt>"
with no search parameter will return all of its DEB822-formatted
repository entries. Some of the software is
in an external repository due to a non-free license, other software (like
Firefox) just has a development cycle that outpaces Debian's.</p>

<p>As one might expect, the Debian desktop experience is not
dramatically different from other distributions; GNOME&nbsp;48 on
Debian is little different than GNOME&nbsp;48 on Fedora, and the same
is true for KDE, Xfce, etc. The primary difference is that users can
expect more or less the same desktop experience running Debian stable
in two years that they have today, which is not necessarily true for
other distributions.</p>

<h4>Miscellaneous</h4>

<p>One of the features in Debian&nbsp;13 is something that most users
won't notice or appreciate at all: a <a href="https://wiki.debian.org/ReleaseGoals/64bit-time">transition to
64-bit <tt>time_t</tt></a> on 32-bit architectures, to avoid the <a href="https://theyear2038problem.com/">Year 2038</a> problem. The
short version is that 32-bit integers cannot hold a Unix epoch
timestamp for dates after January&nbsp;19,&nbsp;2038. That <em>may</em> seem
like a distant concern, even irrelevant for Debian trixie; after all,
Debian&nbsp;13 is only supported by the project until 2030. However,
the project expects that some 32-bit embedded systems will still be running
trixie in 2038, so Debian developers did the heavy lifting to complete
the transition to 64-bit <tt>time_t</tt> now. LWN <a href="https://lwn.net/Articles/938149/">covered</a> the early planning
for this in 2023.</p>

<p>By now, most users have retired their <a href="https://en.wikipedia.org/wiki/Digital_Signature_Algorithm">DSA</a>
SSH keys; if not, now is the time to do so. DSA keys were disabled by
default with OpenSSH in 2015, and they are entirely disabled now with
the <tt>openssh-client</tt> and <tt>openssh-server</tt> packages in
trixie. If there is a device that can, for some reason, only be
connected to with DSA, users can install the
<tt>openssh-client-ssh1</tt> package and use <tt>ssh1</tt> to make the
connection.</p>

<p>As we <a href="https://lwn.net/Articles/975565/">covered</a> in
June&nbsp;2024, Debian&nbsp;13 has switched to using a tmpfs
filesystem for the <tt>/tmp</tt> directory. By default, Debian
allocates up to 50% of memory to <tt>/tmp</tt>, but this can be
changed by following the <a href="https://www.debian.org/releases/trixie/release-notes/issues.en.html#the-temporary-files-directory-tmp-is-now-stored-in-a-tmpfs">instructions
in the release notes</a>. Note that this also applies to systems that
are upgraded to trixie from bookworm.</p>

<h4>Forward to forky</h4>

<p>Debian Project Leader (DPL) Andreas Tille <a href="https://lwn.net/ml/all/aKAHHO30ZhM7zz4v@an3as.eu/">recently
announced</a> "<q>Debian's 100000th birthday</q>", so clearly the project has a
bit of experience with putting out solid releases. Granted, he was
reporting the number in binary, but even when converted to decimal 
numbers (32 years), it's an impressive track record.</p>

<p>While testing, I installed trixie on a couple of systems, including
a new <a href="https://frame.work/laptop12">Framework&nbsp;12-inch laptop</a>. My original intent was to just see
whether Debian had any problems with the new hardware (it didn't), but
now I'm leaning toward sticking with Debian on this system for a while
to see if stability suits me.</p>

<p>With trixie out the door, the Debian Project has already turned its
attention to working on forky, which has no release date set. Debian has
stuck to a loose schedule of a new stable release roughly every two
years. Most likely we will see Debian&nbsp;14 sometime in 2027. After
the forky release, trixie will still receive updates from Debian's
security team through 2028, and then from its <a href="https://wiki.debian.org/LTS">LTS team</a> through 2030.</p>

<p>As of yet, there are no major new features or changes announced for
forky; it seems likely that those will be coming to light in the
coming months now that the project has trixie out the door. LWN will,
of course, be reporting on those developments as they happen.</p>

<br clear="all"><hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Sonnet Will Ship in Xcode (469 pts)]]></title>
            <link>https://developer.apple.com/documentation/xcode-release-notes/xcode-26-release-notes</link>
            <guid>45058688</guid>
            <pubDate>Fri, 29 Aug 2025 00:44:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.apple.com/documentation/xcode-release-notes/xcode-26-release-notes">https://developer.apple.com/documentation/xcode-release-notes/xcode-26-release-notes</a>, See on <a href="https://news.ycombinator.com/item?id=45058688">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Python: The Documentary (275 pts)]]></title>
            <link>https://lwn.net/Articles/1035537/</link>
            <guid>45058171</guid>
            <pubDate>Thu, 28 Aug 2025 23:27:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/1035537/">https://lwn.net/Articles/1035537/</a>, See on <a href="https://news.ycombinator.com/item?id=45058171">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Attendees at EuroPython had the chance to preview part of
<em>Python: The Documentary</em> during <a href="https://programme.europython.eu/europython-2025/talk/JKZDUY/">a
keynote panel</a>. The full film, created by <a href="https://www.cultrepo.com/">CultRepo</a>, is now available <a href="https://www.youtube.com/watch?v=GfH4QL4VqJ0">on YouTube</a>:</p>

<blockquote>
<p>This is the story of the world's most beloved programming language:
Python. What began as a side project in Amsterdam during the 1990s
became the software powering artificial intelligence, data science and
some of the world's biggest companies. But Python's future wasn't
certain; at one point it almost disappeared.</p>

<p>This 90-minute documentary features Guido van Rossum, Travis
Oliphant, Barry Warsaw, and many more, and they tell the story of
Python's rise, its community-driven evolution, the conflicts that
almost tore it apart, and the language's impact on... well...
everything.</p>
</blockquote>

<p>The <a href="https://www.youtube.com/watch?v=Sf2AqQ5a38Y">video</a>
of the keynote is also available.</p>
<br clear="all"><hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RSS is awesome (273 pts)]]></title>
            <link>https://evanverma.com/rss-is-awesome</link>
            <guid>45058024</guid>
            <pubDate>Thu, 28 Aug 2025 23:04:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://evanverma.com/rss-is-awesome">https://evanverma.com/rss-is-awesome</a>, See on <a href="https://news.ycombinator.com/item?id=45058024">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <!-- HTML_TAG_START --><p><a target="_blank" href="https://netnewswire.com/">
    <span>
      NetNewsWire
    </span>
  </a>
    <span>
       is my latest most-used iPhone app. It is a simple, free RSS reader. 
    </span>
  </p><p>
    <span>
      RSS is an old technology that it seems most people have forgotten about. Here's how it works: you enter a link to an RSS "feed", and your app pulls data from this feed every few minutes or so. When there is a new post from your feed, that post is pulled directly to your app. 
    </span>
  </p><p>
    <span>
      RSS is really simple, so it is still very well supported. Notably, all substack publications automatically have an RSS feed included at 
    </span>
  <code>https://{{substack-domain}}/feed</code>
    <span>
      . 
    </span>
  </p><p>
    <span>
      Blogs are great but I don't enjoy reading posts in my email, having to remember the websites each one is hosted at, or reading from each publications' different typesetting opinions with varying pop-ups and advertisements. An RSS reader centralizes all content from your blogs into a single place for reading. 
    </span>
  </p><p>
    <span>
      Since I started using this app I spend much more of my "mindless phone time" reading blog posts, which I think is somewhere between marginally good and good. 
    </span>
  </p><!-- HTML_TAG_END --> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Space Shuttle Columbia disaster and the over-reliance on PowerPoint (2019) (179 pts)]]></title>
            <link>https://mcdreeamiemusings.com/blog/2019/4/13/gsux1h6bnt8lqjd7w2t2mtvfg81uhx</link>
            <guid>45057404</guid>
            <pubDate>Thu, 28 Aug 2025 21:44:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mcdreeamiemusings.com/blog/2019/4/13/gsux1h6bnt8lqjd7w2t2mtvfg81uhx">https://mcdreeamiemusings.com/blog/2019/4/13/gsux1h6bnt8lqjd7w2t2mtvfg81uhx</a>, See on <a href="https://news.ycombinator.com/item?id=45057404">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="siteWrapper">

      

      

      
        
          
            
              
            
          
        
      


      
      
      

      <main id="page" role="main">
        
        <!--
        --><!--
        --><div id="content" data-content-field="main-content" data-collection-id="5b84290a8a922df35c5e3d70" data-edit-main-image="">

  
  <article id="article-5cb1bd934e17b62f29c7fbce" data-item-id="5cb1bd934e17b62f29c7fbce">

    

    <div data-layout-label="Post Body" data-type="item" data-updated-on="1555152384325" id="item-5cb1bd934e17b62f29c7fbce"><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1555152390818_2157">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>The space shuttle Columbia disintegrating in the atmosphere (Creative Commons)</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-sqsp-text-block-content="" data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1555166931302_13429">
  <p>We’ve all sat in those presentations.  A speaker with a stream of slides full of text, monotonously reading them off as we read along.  We’re so used to it we expect it.  We accept it.  We even consider it ‘learning’. As an educator I push against ‘death by PowerPoint’ and I'm fascinated with how we can improve the way we present and teach.  The fact is we know that PowerPoint kills.  Most often the only victims are our audience’s inspiration and interest.  This, however, is the story of a PowerPoint slide that actually helped kill seven people.</p><p>January 16th 2003.  NASA Mission STS-107 is underway. The Space Shuttle Columbia launches carrying its crew of seven to low orbit.  Their objective was to study the effects of microgravity on the human body and on ants and spiders they had with them.  Columbia had been the first Space Shuttle, first launched in 1981 and had been on 27 missions prior to this one.  Whereas other shuttle crews had focused on work to the Hubble Space Telescope or to the International Space Station this mission was one of pure scientific research.  </p><p>The launch proceeded as normal.  The crew settled into their mission.  They would spend 16 days in orbit, completing 80 experiments.  One day into their mission it was clear to those back on Earth that something had gone wrong.  </p><p>As a matter of protocol NASA staff reviewed footage from an external camera mounted to the fuel tank.  At eighty-two seconds into the launch a piece of spray on foam insulation (SOFI) fell from one of the ramps that attached the shuttle to its external fuel tank.  As the crew rose at 28,968 kilometres per hour the piece of foam collided with one of the tiles on the outer edge of the shuttle’s left wing.  </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1555166931302_17451">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>Frame of NASA launch footage showing the moment the foam struck the shuttle’s left wing (Creative Commons)</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-sqsp-text-block-content="" data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1555166931302_17738">
  <p>It was impossible to tell from Earth how much damage this foam, falling nine times faster than a fired bullet, would have caused when it collided with the wing.   Foam falling during launch was nothing new.  It had happened on four previous missions and was one of the reasons why the camera was there in the first place.  But the tile the foam had struck was on the edge of the wing designed to protect the shuttle from the heat of Earth’s atmosphere during launch and re-entry.  In space the shuttle was safe but NASA didn’t know how it would respond to re-entry.  There were a number of options.  The astronauts could perform a spacewalk and visually inspect the hull.  NASA could launch another Space Shuttle to pick the crew up.  Or they could risk re-entry.  </p><p>NASA officials sat down with Boeing Corporation engineers who took them through three reports; a total of 28 slides.    The salient point was whilst there was data showing that the tiles on the shuttle wing could tolerate being hit by the foam this was based on test conditions using foam more than 600 times smaller than that that had struck Columbia.  This is the slide the engineers chose to illustrate this point:</p>
</div><div data-sqsp-text-block-content="" data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1555166931302_26426">
  <p>NASA managers listened to the engineers and their PowerPoint.  The engineers felt they had communicated the potential risks.  NASA felt the engineers didn’t know what would happen but that all data pointed to there not being enough damage to put the lives of the crew in danger.  They rejected the other options and pushed ahead with Columbia re-entering Earth’s atmosphere as normal.  Columbia was scheduled to land at 0916 (EST) on February 1st 2003.  Just before 0900, 61,170 metres above Dallas at 18 times the speed of sound, temperature readings on the shuttle’s left wing were abnormally high and then were lost.  Tyre pressures on the left side were soon lost as was communication with the crew.  At 0912, as Columbia should have been approaching the runway, ground control heard reports from residents near Dallas that the shuttle had been seen disintegrating.  Columbia was lost and with it her crew of seven.  The oldest crew member was 48.  </p><p>The shuttle programme was on lock down, grounded for two years as the investigation began.  The cause of the accident became clear: <em>a hole in a tile on the left wing caused by the foam let the wing dangerously overheat until the shuttle disintegrated</em>.  </p><p>The questions to answer included a very simple one: <strong>Why, given that the foam strike had occurred at a force massively out of test conditions had NASA proceeded with re-entry? </strong> </p><p>Edward Tufte, a Professor at Yale University and expert in communication reviewed the slideshow the Boeing engineers had given NASA, in particular the above slide.  His findings were tragically profound.</p>
</div><div data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1555166931302_66458">

<p> Firstly, the slide had a misleadingly reassuring title claiming that test data pointed to the tile being able to withstand the foam strike.  This was not the case but the presence of the title, centred in the largest font makes this seem the salient, summary point of this slide.  This helped Boeing’s message be lost almost immediately.</p>






















</div><div data-sqsp-text-block-content="" data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1555166931302_70933">
  <p>Secondly, the slide contains four different bullet points with no explanation of what they mean.  This means that interpretation is left up to the reader.  Is number 1 the main bullet point?  Do the bullet points become less important or more?  It’s not helped that there’s a change in font sizes as well.  In all with bullet points and indents six levels of hierarchy were created.  This allowed NASA managers to imply a hierarchy of importance in their head: the writing lower down and in smaller font was ignored.  Actually, this had been where the contradictory (and most important) information was placed.  </p>
</div><div data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1555332030872_26463">

<p>Thirdly, there is a huge amount of text, more than 100 words or figures on one screen.   Two words, ‘SOFI’ and ‘ramp’ both mean the same thing: the foam.  Vague terms are used.  <em>Sufficient</em> is used once, <em>significant </em>or <em>significantly</em>, five times with little or no quantifiable data.  As a result this left a lot open to audience interpretation.  How much is significant?  Is it statistical significance you mean or something else?  </p>






















</div><div data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1555166931302_75625">

<p>Finally the single most important fact, that the foam strike had occurred at forces massively out of test conditions, is hidden at the very bottom.  Twelve little words which the audience would have had to wade through more than 100 to get to.  If they even managed to keep reading to that point.  In the middle it does say that it is possible for the foam to damage the tile.  This is in the smallest font, lost. </p>






















</div><div data-sqsp-text-block-content="" data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1555332030872_55842">
  <p>NASA’s subsequent report criticised technical aspects along with human factors.  <a href="http://s3.amazonaws.com/akamai.netstorage/anon.nasa-global/CAIB/CAIB_lowres_full.pdf" target="_blank">Their report</a> mentioned an over-reliance on PowerPoint:</p><p><em> “The Board views the endemic use of PowerPoint briefing slides instead of technical papers as an illustration of the problematic methods of technical communication at NASA.” </em></p><p> Edward Tufte’s full report makes for<a href="https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001yB" target="_blank"> fascinating reading</a>. Since being released in 1987 PowerPoint has grown exponentially to the point where <a href="https://www.thinkoutsidetheslide.com/are-we-wasting-250-million-per-day-due-to-bad-powerpoint/" target="_blank">it is now estimated</a> than thirty million PowerPoint presentations are made every day.  Yet, PowerPoint is <a href="https://www.theguardian.com/commentisfree/2015/sep/23/powerpoint-thought-students-bullet-points-information" target="_blank">blamed</a> by academics for killing critical thought.  Amazon’s CEO Jeff Bezos has <a href="https://www.forbes.com/sites/quora/2018/08/22/jeff-bezos-banned-powerpoint-presentations-at-amazon-meetings-heres-what-replaced-them/#4578b4773b5f" target="_blank">banned it from meetings</a>.   Typing text on a screen and reading it out loud does not count as teaching.  An audience reading text off the screen does not count as learning.  Imagine if the engineers had put up a slide with just: “<strong>foam strike more than 600 times bigger than test data</strong>.”  Maybe NASA would have listened.  Maybe they wouldn’t have attempted re-entry.  Next time you’re asked to give a talk remember Columbia. Don’t just jump to your laptop and write out slides of text.  Think about your message.  Don’t let that message be lost amongst text.  Death by PowerPoint is a real thing.  Sometimes literally.</p><p>Thanks for reading</p><p> - Jamie </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1555166931302_61502">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>Columbia’s final crew (from https://www.space.com/19436-columbia-disaster.html)</p>
          </figcaption>
        
      
        </figure>
      

    </div></div>

    

    

  </article>

  

  

  
  
  

</div><!--
        -->
        
        
      </main>

      

      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sometimes CPU cores are odd (113 pts)]]></title>
            <link>https://anubis.techaro.lol/blog/2025/cpu-core-odd/</link>
            <guid>45057346</guid>
            <pubDate>Thu, 28 Aug 2025 21:39:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anubis.techaro.lol/blog/2025/cpu-core-odd/">https://anubis.techaro.lol/blog/2025/cpu-core-odd/</a>, See on <a href="https://news.ycombinator.com/item?id=45057346">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container"><p><img decoding="async" loading="lazy" src="https://anubis.techaro.lol/assets/images/parc-dsilence-81fae4a5101e65a9842678688cc8ffbc.webp" width="1024" height="576"></p>
<p>One of the biggest lessons that I've learned in my career is that all software has bugs, and the more complicated your software gets the more complicated your bugs get. A lot of the time those bugs will be fairly obvious and easy to spot, validate, and replicate. Sometimes, the process of fixing it will uncover your core assumptions about how things work in ways that will leave you feeling like you just got trolled.</p>
<p>Today I'm going to talk about a single line fix that prevents people on a large number of devices from having weird irreproducible issues with Anubis rejecting people when it frankly shouldn't. Stick around, it's gonna be a wild ride.</p>
<!-- -->
<h2 id="how-this-happened">How this happened<a href="#how-this-happened" aria-label="Direct link to How this happened" title="Direct link to How this happened">​</a></h2>
<p>Anubis is a web application firewall that tries to make sure that the client is a browser. It uses a few <a href="https://anubis.techaro.lol/docs/admin/configuration/challenges/">challenge methods</a> to do this determination, but the main method is the <a href="https://anubis.techaro.lol/docs/admin/configuration/challenges/proof-of-work/">proof of work</a> challenge which makes clients grind away at cryptographic checksums in order to rate limit clients from connecting too eagerly.</p>
<div><p><span><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</p><p>In retrospect implementing the proof of work challenge may have been a mistake and it's likely to be supplanted by things like <a href="https://github.com/TecharoHQ/anubis/pull/1038" target="_blank" rel="noopener noreferrer">Proof of React</a> or other methods that have yet to be developed. Your patience and polite behaviour in the bug tracker is appreciated.</p></div>
<p>In order to make sure the proof of work challenge screen <em>goes away as fast as possible</em>, the <a href="https://github.com/TecharoHQ/anubis/tree/main/web/js/worker" target="_blank" rel="noopener noreferrer">worker code</a> is optimized within an inch of its digital life. One of the main ways that this code is optimized is with how it's run. Over the last 10-20 years, the main way that CPUs have gotten fast is via increasing multicore performance. Anubis tries to make sure that it can use as many cores as possible in order to take advantage of your device's CPU as much as it can.</p>
<p>This strategy sometimes has some issues though, for one Firefox seems to get <em>much slower</em> if you have Anubis try to absolutely saturate all of the cores on the system. It also has a fairly high overhead between JavaScript JIT code and <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Crypto_API" target="_blank" rel="noopener noreferrer">WebCrypto</a>. I did some testing and found out that Firefox's point of diminishing returns was about half of the CPU cores.</p>
<h2 id="another-invalid-response-bug">Another "invalid response" bug<a href="#another-invalid-response-bug" aria-label="Direct link to Another &quot;invalid response&quot; bug" title="Direct link to Another &quot;invalid response&quot; bug">​</a></h2>
<p>One of the complaints I've been getting from users and administrators using Anubis is that they've been running into issues where users get randomly rejected with an error message only saying "invalid response". This happens when the challenge validating process fails. This issue has been blocking the release of the next version of Anubis.</p>
<p>In order to demonstrate this better, I've made a little interactive diagram for the proof of work process:</p>
<div><div><div><h2>1. Challenge</h2><p>3e2c67c9ef91d81fff589db473a2f996</p></div><div><h2>3. Combined Data</h2><p>3e2c67c9ef91d81fff589db473a2f9960</p></div></div><div><h2>4. Resulting Hash (SHA-256)</h2><p><span>...</span></p></div></div>
<p>I've fixed a lot of the easy bugs in Anubis by this point. A lot of what's left is the hard bugs, but also specifically the kinds of hard bugs that involve weird hardware configurations. In order to try and catch these issues before software hits prod, I test Anubis against a bunch of hardware I have locally. Any issues I find and fix before software ships are issues that you don't hit in production.</p>
<p>Let's consider <a href="https://github.com/TecharoHQ/anubis/blob/main/web/js/algorithms/fast.mjs" target="_blank" rel="noopener noreferrer">the line of code</a> that was causing this issue:</p>
<div><pre tabindex="0"><code><span><span>threads </span><span>=</span><span> </span><span>Math</span><span>.</span><span>max</span><span>(</span><span>navigator</span><span>.</span><span>hardwareConcurrency</span><span> </span><span>/</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>)</span><span>,</span><br></span></code></pre></div>
<p>This is intended to make your browser spawn a proof of work worker for <em>half</em> of your available CPU cores. If you only have one CPU core, you should only have one worker. Each thread is given this number of threads and uses that to increment the nonce so that each thread doesn't try to find a solution that another worker has already performed.</p>
<p>One of the subtle problems here is that all of the parts of this assume that the thread ID and nonce are integers without a decimal portion. Famously, <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number" target="_blank" rel="noopener noreferrer">all JavaScript numbers are IEEE 754 floating point numbers</a>. Surely there wouldn't be a case where the thread count could be a <em>decimal</em> number, right?</p>
<p>Here's all the devices I use to test Anubis <em>and their core counts</em>:</p>
<table><thead><tr><th>Device Name</th><th>Core Count</th></tr></thead><tbody><tr><td>MacBook Pro M3 Max</td><td>16</td></tr><tr><td>MacBook Pro M4 Max</td><td>16</td></tr><tr><td>AMD Ryzen 9 7950x3D</td><td>32</td></tr><tr><td>Google Pixel 9a (GrapheneOS)</td><td>8</td></tr><tr><td>iPhone 15 Pro Max</td><td>6</td></tr><tr><td>iPad Pro (M1)</td><td>8</td></tr><tr><td>iPad mini</td><td>6</td></tr><tr><td>Steam Deck</td><td>8</td></tr><tr><td>Core i5 10600 (homelab)</td><td>12</td></tr><tr><td>ROG Ally</td><td>16</td></tr></tbody></table>
<p>Notice something? All of those devices have an <em>even</em> number of cores. Some devices such as the <a href="https://www.gsmarena.com/google_pixel_8_pro-12545.php" target="_blank" rel="noopener noreferrer">Pixel 8 Pro</a> have an <em>odd</em> number of cores. So what happens with that line of code as the JavaScript engine evaluates it?</p>
<p>Let's replace the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Navigator/hardwareConcurrency" target="_blank" rel="noopener noreferrer"><code>navigator.hardwareConcurrency</code></a> with the Pixel 8 Pro's 9 cores:</p>
<div><pre tabindex="0"><code><span><span>threads </span><span>=</span><span> </span><span>Math</span><span>.</span><span>max</span><span>(</span><span>9</span><span> </span><span>/</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>)</span><span>,</span><br></span></code></pre></div>
<p>Then divide it by two:</p>
<div><pre tabindex="0"><code><span><span>threads </span><span>=</span><span> </span><span>Math</span><span>.</span><span>max</span><span>(</span><span>4.5</span><span>,</span><span> </span><span>1</span><span>)</span><span>,</span><br></span></code></pre></div>
<p>Oops, that's not ideal. However <code>4.5</code> is bigger than <code>1</code>, so <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/max" target="_blank" rel="noopener noreferrer"><code>Math.max</code></a> returns that:</p>

<p>This means that each time the proof of work equation is calculated, there is a 50% chance that a valid solution would include a nonce with a decimal portion in it. If the client finds a solution with such a nonce, then it would think the client was successful and submit the solution to the server, but the server only expects whole numbers back so it rejects that as an invalid response.</p>
<p>I keep telling more junior people that when you have the weirdest, most inconsistent bugs in software that it's going to boil down to the dumbest possible thing you can possibly imagine. People don't believe me, then they encounter bugs like this. Then they suddenly believe me.</p>
<p>Here is the fix:</p>
<div><pre tabindex="0"><code><span><span>threads </span><span>=</span><span> </span><span>Math</span><span>.</span><span>trunc</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>navigator</span><span>.</span><span>hardwareConcurrency</span><span> </span><span>/</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>)</span><span>)</span><span>,</span><br></span></code></pre></div>
<p>This uses <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/trunc" target="_blank" rel="noopener noreferrer"><code>Math.trunc</code></a> to truncate away the decimal portion so that the Pixel 8 Pro has <code>4</code> workers instead of <code>4.5</code> workers.</p>
<h2 id="today-i-learned-this-was-possible">Today I learned this was possible<a href="#today-i-learned-this-was-possible" aria-label="Direct link to Today I learned this was possible" title="Direct link to Today I learned this was possible">​</a></h2>
<p>This was a total "today I learned" moment. I didn't actually think that hardware vendors shipped processors with an odd number of cores, however if you look at the core geometry of the Pixel 8 Pro, it has <em>three</em> tiers of processor cores:</p>
<table><thead><tr><th>Core type</th><th>Core model</th><th>Number</th></tr></thead><tbody><tr><td>High performance</td><td>3 Ghz Cortex X3</td><td>1</td></tr><tr><td>Medium performance</td><td>2.45 Ghz Cortex A715</td><td>4</td></tr><tr><td>High efficiency</td><td>2.15 Cortex A510</td><td>4</td></tr><tr><td>Total</td><td></td><td>9</td></tr></tbody></table>
<p>I guess every assumption that developers have about CPU design is probably wrong.</p>
<p>This probably isn't helped by the fact that for most of my career, the core count in phones has been largely irrelevant and most of the desktop / laptop CPUs I've had (where core count does matter) uses <a href="https://en.wikipedia.org/wiki/Simultaneous_multithreading" target="_blank" rel="noopener noreferrer">simultaneous multithreading</a> to "multiply" the core count by two.</p>
<p>The client side fix is a bit of an "emergency stop" button to try and mitigate the badness as early as possible. In general I'm quite aware of the terrible UX involved with this flow failing and I'm still noodling through ways to make that UX better and easier for users / administrators to debug.</p>
<p>I'm looking into the following:</p>
<ol>
<li>This could have been prevented on the server side by doing less strict input validation in compliance with <a href="https://en.wikipedia.org/wiki/Robustness_principle" target="_blank" rel="noopener noreferrer">Postel's Law</a>. I feel nervous about making such a security-sensitive endpoint <em>more liberal</em> with the inputs it can accept, but it may be fine? I need to consult with a security expert.</li>
<li>Showing an encrypted error message on the "invalid response" page so that the user and administrator can work together to fix or report the issue. I remember Google doing this at least once, but I can't recall where I've seen it in the past. Either way, this is probably the most robust method even though it would require developing some additional tooling. I think it would be worth it.</li>
</ol>
<p>I'm likely going to go with the second option. I will need to figure out a good flow for this. It's likely going to involve <a href="https://github.com/FiloSottile/age" target="_blank" rel="noopener noreferrer">age</a>. I'll say more about this when I have more to say.</p>
<p>In the meantime though, looks like I need to expense a used Pixel 8 Pro to add to the testing jungle for Anubis. If anyone has a deal out there, please let me know!</p>
<p>Thank you to the people that have been polite and helpful when trying to root cause and fix this issue.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Expert: LSP for Elixir (218 pts)]]></title>
            <link>https://github.com/elixir-lang/expert</link>
            <guid>45057322</guid>
            <pubDate>Thu, 28 Aug 2025 21:36:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/elixir-lang/expert">https://github.com/elixir-lang/expert</a>, See on <a href="https://news.ycombinator.com/item?id=45057322">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Expert</h2><a id="user-content-expert" aria-label="Permalink: Expert" href="#expert"></a></p>
<p dir="auto">Expert is the official language server implementation for the Elixir programming language.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">You can download Expert from the <a href="https://github.com/elixir-lang/expert/releases">releases page</a> for your
operating system and architecture. Put the executable somewhere on your <code>$PATH</code>, like <code>~/.local/bin/expert</code></p>
<p dir="auto">For editor specific installation instructions, please refer to the <a href="https://github.com/elixir-lang/expert/blob/main/pages/installation.md">Installation Instructions</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Nightly Builds</h3><a id="user-content-nightly-builds" aria-label="Permalink: Nightly Builds" href="#nightly-builds"></a></p>
<p dir="auto">If you want to try out the latest features, you can download a nightly build.</p>
<p dir="auto">Using the GH CLI, you can run the following command to download the latest nightly build:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gh release download nightly --pattern 'expert_linux_amd64'"><pre>gh release download nightly --pattern <span><span>'</span>expert_linux_amd64<span>'</span></span></pre></div>
<p dir="auto">Then point your editor to the downloaded binary.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building from source</h3><a id="user-content-building-from-source" aria-label="Permalink: Building from source" href="#building-from-source"></a></p>
<p dir="auto">To build Expert from source, you need Zig <code>0.14.1</code> installed on your system.</p>
<p dir="auto">Then you can run the following command or follow the instructions in the <a href="https://github.com/elixir-lang/expert/blob/main/pages/installation.md">Installation Instructions</a>:</p>

<p dir="auto">This will build the Expert binary and place it in the <code>apps/expert/burrito_out</code> directory. You can then point your
editor to this binary.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sponsorship</h2><a id="user-content-sponsorship" aria-label="Permalink: Sponsorship" href="#sponsorship"></a></p>
<p dir="auto">Thank you to our corporate sponsors! If you'd like to start sponsoring the project, please read more below.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/elixir-lang/expert/blob/main/assets/sponsors/fly.png"><img height="100" src="https://github.com/elixir-lang/expert/raw/main/assets/sponsors/fly.png"></a>
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/elixir-lang/expert/blob/main/assets/sponsors/tauspace.png"><img height="100" src="https://github.com/elixir-lang/expert/raw/main/assets/sponsors/tauspace.png"></a>
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/elixir-lang/expert/blob/main/assets/sponsors/river.png"><img height="100" src="https://github.com/elixir-lang/expert/raw/main/assets/sponsors/river.png"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Corporate</h3><a id="user-content-corporate" aria-label="Permalink: Corporate" href="#corporate"></a></p>
<p dir="auto">For companies wanting to directly sponsor full time work on Expert, please reach out to Dan Janowski: EEF Chair of Sponsorship WG at <a href="mailto:danj@erlef.org">danj@erlef.org</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Individual</h3><a id="user-content-individual" aria-label="Permalink: Individual" href="#individual"></a></p>
<p dir="auto">Individuals can donate using GitHub sponsors. Team members are listed in the sidebar.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other resources</h2><a id="user-content-other-resources" aria-label="Permalink: Other resources" href="#other-resources"></a></p>
<ul dir="auto">
<li><a href="https://github.com/elixir-lang/expert/blob/main/pages/architecture.md">Architecture</a></li>
<li><a href="https://github.com/elixir-lang/expert/blob/main/pages/development.md">Development Guide</a></li>
<li><a href="https://github.com/elixir-lang/expert/blob/main/pages/glossary.md">Glossary</a></li>
<li><a href="https://github.com/elixir-lang/expert/blob/main/pages/installation.md">Installation Instructions</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">LICENSE</h2><a id="user-content-license" aria-label="Permalink: LICENSE" href="#license"></a></p>
<p dir="auto">Expert source code is released under Apache License 2.0.</p>
<p dir="auto">Check LICENSE file for more information.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fuck up my site – Turn any website into beautiful chaos (308 pts)]]></title>
            <link>https://www.fuckupmysite.com/?url=https%3A%2F%2Fnews.ycombinator.com&amp;torchCursor=true&amp;comicSans=true&amp;fakeCursors=true&amp;peskyFly=true</link>
            <guid>45057020</guid>
            <pubDate>Thu, 28 Aug 2025 21:04:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fuckupmysite.com/?url=https%3A%2F%2Fnews.ycombinator.com&#x26;torchCursor=true&#x26;comicSans=true&#x26;fakeCursors=true&#x26;peskyFly=true">https://www.fuckupmysite.com/?url=https%3A%2F%2Fnews.ycombinator.com&#x26;torchCursor=true&#x26;comicSans=true&#x26;fakeCursors=true&#x26;peskyFly=true</a>, See on <a href="https://news.ycombinator.com/item?id=45057020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><div><div><div><h2><p><img alt="FUCKUPMYSITE" width="100" height="100" decoding="async" data-nimg="1" src="https://www.fuckupmysite.com/fuckupmysite/fuckupmysite.png"></p><span data-word="fuck"><span>fuck</span></span><span data-word="up"><span>up</span></span><span data-word="my"><span>my</span></span><span data-word="site"><span>site</span></span></h2></div><p>Some people just want to watch the web <span data-text="burn">burn</span></p><form><div><p><span>Try:</span></p></div></form></div><div><div><h2><span>😈</span><span>Chaos Settings</span></h2></div><div><p><span>3<!-- --> of <!-- -->6<!-- --> agents of chaos enabled</span></p></div></div><p>Heads up: Not every site plays nice with the chaos. Got feedback or discovered something broken?<!-- --> <a href="https://twitter.com/jshchnz" target="_blank" rel="noopener noreferrer">Let me know on Twitter</a></p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You no longer need JavaScript: an overview of what makes modern CSS so awesome (313 pts)]]></title>
            <link>https://lyra.horse/blog/2025/08/you-dont-need-js/</link>
            <guid>45056878</guid>
            <pubDate>Thu, 28 Aug 2025 20:49:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lyra.horse/blog/2025/08/you-dont-need-js/">https://lyra.horse/blog/2025/08/you-dont-need-js/</a>, See on <a href="https://news.ycombinator.com/item?id=45056878">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
  
  <p><time datetime="2025-08-28T20:40:00Z">2025-08-28</time> <span id="tags"><ul><li> ¦ <a href="https://lyra.horse/blog/tags/css/">css</a></li></ul></span>
</p>

  <p>So much of the web these days is ruined by the bloat that is modern JavaScript frameworks. React apps that take several seconds to load. NextJS sites that throw random hydration errors. The <em>node_modules</em> folder that takes up gigabytes on your hard drive.</p>
<p>It’s awful. And you don’t need it.</p>

<art-frame flex="" id="cover-art" aria-label="Cover art of the Chrome DevTools showing a bunch of requests to assets, most of which are JavaScript files" role="img">
<div>
<table>
  <thead>
    <tr>
      <td>Name</td>
      <td>Status</td>
      <td>Type</td>
      <td>Size</td>
      <td>Time</td>
    </tr>
  </thead>
  <tbody>
    <tr><td><dev-icon html=""></dev-icon>app</td><td>200</td><td>document</td><td>153.8 kB</td><td>51 ms</td></tr>
    <tr><td><dev-icon font=""></dev-icon>6920616d20612066<span>-s.p.6f6e7421</span>.woff2</td><td>200</td><td>font</td><td>31.5 kB</td><td>32 ms</td></tr>
    <tr><td><dev-icon font=""></dev-icon>686579206d652074<span>-s.p.6f6f2121</span>.woff2</td><td>200</td><td>font</td><td>28.5 kB</td><td>116 ms</td></tr>
    <tr><td><dev-icon css=""></dev-icon>77687920646f6573.css</td><td>200</td><td>stylesheet</td><td>253 kB</td><td>47 ms</td></tr>
    <tr><td><dev-icon js=""></dev-icon>2074686520646566.js</td><td>200</td><td>script</td><td>648 kB</td><td>83 ms</td></tr>
    <tr><td><dev-icon js=""></dev-icon>61756c74206e6578.js</td><td>200</td><td>script</td><td>166 kB</td><td>363 ms</td></tr>
    <tr><td><dev-icon js=""></dev-icon>746a732074616b65.js</td><td>200</td><td>script</td><td>83.3 kB</td><td>46 ms</td></tr>
    <tr><td><dev-icon js=""></dev-icon>turbopack-20757020302e354d.js</td><td>200</td><td>script</td><td>38.0 kB</td><td>95 ms</td></tr>
    <tr><td><dev-icon js=""></dev-icon>423f207468617427.js</td><td>200</td><td>script</td><td>414 B</td><td>34 ms</td></tr>
    <tr><td><dev-icon js=""></dev-icon>73206d6f72652074.js</td><td>200</td><td>script</td><td>32.6 kB</td><td>49 ms</td></tr>
    <tr><td><dev-icon js=""></dev-icon>68616e206d792065.js</td><td>200</td><td>script</td><td>15.1 kB</td><td>71 ms</td></tr>
    <tr><td><dev-icon js=""></dev-icon>6e7469726520626c.js</td><td>200</td><td>script</td><td>143 kB</td><td>48 ms</td></tr>
    <tr><td><dev-icon js=""></dev-icon>6f6721 hey there!</td><td>200</td><td>script</td><td>4.1 kB</td><td>103 ms</td></tr>
  </tbody>
</table>
</div>
</art-frame>
<p>The intro paragraph of this post is tongue-in-cheek. It’s there to get you to read the rest of the post. I suspect the megabytes of tracking scripts intertwined with bad code is far more likely to be the real culprit behind all the terrible sites out there. Web frameworks have their time and place. And despite my personal distaste for them, I know they are used by many teams to build awesome well-optimized apps.</p>
<p>Despite that, I think there’s some beauty in leaving it all behind. Not just the frameworks, but JavaScript altogether.<!-- [Figmas](https://www.figma.com/) and [Photopeas](https://www.photopea.com/) need their JavaScript. [Interactive](https://x3c.tf/archive/) [experiences](https://lyra.horse/antonymph/) do[^interactive]. But does your homepage or blog need it?--> Not every site needs JavaScript. Perhaps your <a href="https://justfuckingusereact.com/">e-commerce site needs it for its complex carts and data visualization dashboards</a>, but is it really a necessity for most of what’s out there?</p>
<p>It’s actually <a href="https://lyra.horse/css-clicker/">pretty incredible</a> what HTML and CSS alone can achieve.</p>
<!--I don't think that everyone should switch to a strict CSS-only diet, but I do believe that the average web developer could benefit from knowing just a little more about our cascading little friend.-->
<h2 id="so-what-do-you-say">So, what do you say?</h2>
<p>My goal with this article is to share my perspectives on the web, as well as introduce many aspects of modern HTML/CSS you may not be familiar with. I’m not trying to make you give up JavaScript, I’m just trying to show you everything that’s possible, leaving it up to you to pick what works best for whatever you’re working on.</p>
<p>I think there’s a lot most web developers don’t know about CSS.</p>
<p>And I think JS is often used where better alternatives exist.</p>
<p>So, let me show you what’s out there.</p>

<h2 id="but-css-sucks">“But CSS sucks”</h2>
<p>I believe a lot of the negativity towards CSS stems from not really knowing how to use it. Many developers kind of just skip learning the CSS fundamentals in favor of the more interesting Java- and TypeScript, and then go on to complain about a styling language they don’t understand.</p>
<p>I suspect this is due to many treating CSS as this silly third wheel for adding borders and box-shadows to a webapp. <!--rephrase-->It’s undervalued and often compared to glorified crayons, rather than what it really is - a powerful domain-specific programming language.</p>
<p>It’s telling when to this day the only CSS joke in the webdev circles is centering a div.</p>
<!-- it was fun recreating all of the flexbox icons in css haha -->
<code-compare id="center-gadget" aria-label="a centered div and its code" role="figure"><fake-frame id="demo-center-gadget">
<p>i am a div</p>
</fake-frame>

</code-compare>



<p>Yes, the syntax isn’t the prettiest, but is it <em>really</em> that hard?</p>
<p>Besides, your devtools probably<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> come with a fun little gadget that lets you fiddle with the flexbox by just clicking around. You don’t even need to remember the syntax.</p>
<!--i wanted to write about: maybe something about box-model and clock directions-->
<p>I don’t think CSS is fundamentally any more difficult than JS, but if you skip the basics on one and only focus on the other, it’s no surprise it feels that way.</p>
<h2 id="but-its-painful-to-write">“But it’s painful to write”</h2>
<p>Another source of disdain for CSS is how awful it has been to write in the past. This is very much true, and is probably why things like <a href="https://sass-lang.com/">Sass</a> and <a href="https://tailwindcss.com/">Tailwind</a><sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> exist.</p>
<p>But that’s the thing, it <em>used</em> to be bad.</p>
<twoot-embed aria-label="a twoot from rebane2001" role="figure">

<div><p>btw u should write css like
<span role="code">
cool-thing {
    display: flex;
    &amp;[shadow] {
        box-shadow: 1px 1px #0007;
    }<!-- edit: changed screen to width -->
    @media (width &lt; 480px) {
        flex-direction: column;
    }
}
</span>
and html like
</p><!----><p>
&lt;cool-thing shadow&gt;wow&lt;/cool-thing&gt;
</p><!----><p>
because it's allowed &amp; modern &amp; neat!</p></div>
<time datetime="2025-04-08T08:58:14.000Z">11:58 AM · Apr 8, 2025</time>
<hr>
<p><span>❤️</span> 1.5K</p>
</twoot-embed>

<p><em>(yes! the code above is standards compliant<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>)</em></p>
<p>In the past few years, CSS has received a ton of awesome quality-of-life additions, making it nice to do stuff that has historically required preprocessors or JavaScript.</p>
<p>Nesting is definitely one of my favorite additions!</p>
<p>In the past, you’ve had to write code that looks like this:</p>
<pre><code>:<sx-l>root</sx-l> {
  <sx-e>--like-color</sx-e>: <sx-n>#24A4F3</sx-n>;
  <sx-e>--like-color-hover</sx-e>: <sx-n>#54B8F5</sx-n>;
  <sx-e>--like-color-active</sx-e>: <sx-n>#0A6BA8</sx-n>;
}

.<sx-y>post</sx-y> {
  <sx-p>display</sx-p>: <sx-a>block</sx-a>;
  <sx-p>background</sx-p>: <sx-n>#EEE</sx-n>;
  <sx-p>color</sx-p>: <sx-n>#111</sx-n>;
}

.<sx-y>post</sx-y> .<sx-y>avatar</sx-y> {
  <sx-p>width</sx-p>: <sx-n>48px</sx-n>;
  <sx-p>height</sx-p>: <sx-n>48px</sx-n>;
}

.<sx-y>post</sx-y> &gt; .<sx-y>buttons</sx-y> {
  <sx-p>display</sx-p>: <sx-a>flex</sx-a>;
}

.<sx-y>post</sx-y> &gt; .<sx-y>buttons</sx-y> .<sx-y>label</sx-y> {
  <sx-p>font-size</sx-p>: <sx-n>24px</sx-n>;
  <sx-p>padding</sx-p>: <sx-n>8px</sx-n>;
}

.<sx-y>post</sx-y> &gt; .<sx-y>buttons</sx-y> .<sx-y>like</sx-y> {
  <sx-p>cursor</sx-p>: <sx-a>pointer</sx-a>;
  <sx-p>color</sx-p>: <sx-k>var</sx-k>(<sx-e>--like-color</sx-e>);
}

.<sx-y>post</sx-y> &gt; .<sx-y>buttons</sx-y> .<sx-y>like</sx-y>:<sx-l>hover</sx-l> {
  <sx-p>color</sx-p>: <sx-k>var</sx-k>(<sx-e>--like-color-hover</sx-e>);
}

.<sx-y>post</sx-y> &gt; .<sx-y>buttons</sx-y> .<sx-y>like</sx-y>:<sx-l>active</sx-l> {
  <sx-p>color</sx-p>: <sx-k>var</sx-k>(<sx-e>--like-color-active</sx-e>);
}

<sx-z>@media</sx-z> <sx-k>screen</sx-k> (<sx-p>max-width</sx-p>: <sx-n>800px</sx-n>) {
  .<sx-y>post</sx-y> &gt; .<sx-y>buttons</sx-y> .<sx-y>label</sx-y> {
    <sx-p>font-size</sx-p>: <sx-n>16px</sx-n>;
    <sx-p>padding</sx-p>: <sx-n>4px</sx-n>;
  }
}

<sx-z>@media</sx-z> (<sx-x>prefers-color-scheme</sx-x>: <sx-a>dark</sx-a>) {
  .<sx-y>post</sx-y> {
    <sx-p>background</sx-p>: <sx-n>#222</sx-n>;
    <sx-p>color</sx-p>: <sx-n>#FFF</sx-n>;
  }
}
</code></pre>

<p>And yeah, that’s pretty awful to work with. For anything that involves multiple chained selectors, you kind of have to keep a mental map of how every parent selector relates to its children, and the more CSS you add the harder it gets.</p>
<p>But let’s try it with nesting:</p>
<pre><code>:<sx-l>root</sx-l> {
  <sx-e>--like-color</sx-e>: <sx-n>#24A4F3</sx-n>;
  <sx-e>--like-color-hover</sx-e>: <sx-k>hsl</sx-k>(<sx-a>from</sx-a> <sx-k>var</sx-k>(<sx-e>--like-color</sx-e>) <sx-a>h</sx-a> <sx-a>s</sx-a> <sx-k>calc</sx-k>(<sx-a>l</sx-a> + <sx-n>10</sx-n>));
  <sx-e>--like-color-active</sx-e>: <sx-k>hsl</sx-k>(<sx-a>from</sx-a> <sx-k>var</sx-k>(<sx-e>--like-color</sx-e>) <sx-a>h</sx-a> <sx-a>s</sx-a> <sx-k>calc</sx-k>(<sx-a>l</sx-a> - <sx-n>20</sx-n>));
}

.<sx-y>post</sx-y> {
  <sx-p>display</sx-p>: <sx-a>block</sx-a>;
  <sx-p>background</sx-p>: <sx-n>#EEE</sx-n>;
  <sx-p>color</sx-p>: <sx-n>#111</sx-n>;
  <sx-z>@media</sx-z> (<sx-x>prefers-color-scheme</sx-x>: <sx-a>dark</sx-a>) {
    <sx-p>background</sx-p>: <sx-n>#222</sx-n>;
    <sx-p>color</sx-p>: <sx-n>#FFF</sx-n>;
  }
  .<sx-y>avatar</sx-y> {
    <sx-p>width</sx-p>: <sx-n>48px</sx-n>;
    <sx-p>height</sx-p>: <sx-n>48px</sx-n>;
  }
  &amp; &gt; .<sx-y>buttons</sx-y> {
    <sx-p>display</sx-p>: <sx-a>flex</sx-a>;
    .<sx-y>label</sx-y> {
      <sx-p>font-size</sx-p>: <sx-n>24px</sx-n>;
      <sx-p>padding</sx-p>: <sx-n>8px</sx-n>;
      <sx-z>@media</sx-z> (<sx-p>width</sx-p> &lt;= <sx-n>800px</sx-n>) {
        <sx-p>font-size</sx-p>: <sx-n>16px</sx-n>;
        <sx-p>padding</sx-p>: <sx-n>4px</sx-n>;
      }
    }
    .<sx-y>like</sx-y> {
      <sx-p>cursor</sx-p>: <sx-a>pointer</sx-a>;
      <sx-p>color</sx-p>: <sx-k>var</sx-k>(<sx-e>--like-color</sx-e>);
      &amp;:<sx-l>hover</sx-l> { <sx-p>color</sx-p>: <sx-k>var</sx-k>(<sx-e>--like-color-hover</sx-e>); }
      &amp;:<sx-l>active</sx-l> { <sx-p>color</sx-p>: <sx-k>var</sx-k>(<sx-e>--like-color-active</sx-e>); }
    }
  }
}
</code></pre>
<p>That is way nicer to read<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>! All the relevant parts are right next to each other, so it’s a lot easier to understand what’s going on. Seeing the <code>&amp;:hover</code> and <code>&amp;:active</code> right next to the <code>.like</code> button is especially nice imo.</p>
<p>And since you can sort of see the structure - the parent selectors “guarding” the child ones - it also makes it a lot easier to get away with short and simple class names (or even referring to elements themselves).</p>
<p>You may have noticed that I’m also making use of relative colors in the second example. I think the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_colors/Relative_colors">MDN article</a> has a lot of awesome examples, but the jist of it is that you can take an existing color, modify it in many different ways across multiple color spaces, and mix it with other colors using <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/color_value/color-mix">color-mix()</a>.</p>
<pre><code><sx-c>/* remove blue from a color */</sx-c>
<sx-k>rgb</sx-k>(<sx-a>from</sx-a> <sx-n>#123456</sx-n> <sx-a>r</sx-a> <sx-a>g</sx-a> <sx-n>0</sx-n>);
<sx-c>/* make a color transparent */</sx-c>
<sx-k>rgb</sx-k>(<sx-a>from</sx-a> <sx-n>#123456</sx-n> <sx-a>r</sx-a> <sx-a>g</sx-a> <sx-a>b</sx-a> / <sx-n>0.5</sx-n>);
<sx-c>/* make a color lighter */</sx-c>
<sx-k>hsl</sx-k>(<sx-a>from</sx-a> <sx-n>#123456</sx-n> <sx-a>h</sx-a> <sx-a>s</sx-a> <sx-k>calc</sx-k>(<sx-a>l</sx-a> + <sx-n>10</sx-n>));
<sx-c>/* change the hue in oklch color space */</sx-c>
<sx-k>oklch</sx-k>(<sx-a>from</sx-a> <sx-n>#123456</sx-n> <sx-a>l</sx-a> <sx-a>c</sx-a> <sx-k>calc</sx-k>(<sx-a>h</sx-a> + <sx-n>10</sx-n>));
<sx-c>/* mix two colors in oklab color space */</sx-c>
<sx-k>color-mix</sx-k>(<sx-t>in</sx-t> <sx-t>oklab</sx-t>, <sx-n>#8CFFDB</sx-n>, <sx-n>#04593B</sx-n> 25%);
</code></pre>
<p>These snippets are really useful for when you want something to be just ever so slightly darker or brighter, such as a button hover effect or a matching border color, and they’re way nicer to use than doing all those color conversions in JavaScript. If you’re feeling particularly adventurous, you could even go ahead and generate your entire color scheme in just CSS.</p>
<color-demo id="color-picker" role="img" aria-label="A color picker written in CSS that lets you pick a color, and generates color swatches of varying brightness, varying hue, and complimentary/secondary colors."><color-bg></color-bg><color-picker><color-result><div><color-style>
<color-swatch monochrome=""><p>100</p><p>200</p><p>300</p><p>400</p><p>500</p><p>600</p><p>700</p><p>800</p><p>900</p></color-swatch>
<color-swatch analogous=""><p>-40°</p><p>-20°</p><p>0°</p><p>+20°</p><p>+40°</p></color-swatch>
<color-swatch primary=""><p>primary</p><p>complimentary</p><p>secondary</p></color-swatch>
<color-swatch success=""><p>success</p><p>danger</p><p>warning</p><p>info</p></color-swatch>
</color-style></div></color-result><color-point></color-point><color-indicator></color-indicator></color-picker></color-demo>
<details><summary>view-source</summary>

</details>
<p><em>(yes! the color picker above is written in just css)</em></p>
<p>Safari is currently broken when handling of cqw/cqh units, therefore the demo above may not work correctly. If this happens, try using Firefox or Chrome instead.</p>
<p>There are so many cool new CSS features that make writing it just that little bit nicer. Things like letting you use <code>(width &lt;= 768px)</code> instead of <code>(max-width: 768px)</code> in your <a href="https://web.dev/articles/media-query-range-syntax"><em>@media</em> query</a>, the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/length#lh">lh unit</a> that matches the line-height, the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/scrollbar-gutter">scrollbar-gutter</a> property that solves the little scrollbar-related layout shifts, or the ability to finally <a href="https://web.dev/blog/align-content-block">center stuff vertically</a> without flex/grid.</p>
<art-frame aria-label="Baseline logo" role="img" flex="">

Baseline</art-frame>
<p>And all of this is brought together by the cherry on top that is <a href="https://web-platform-dx.github.io/web-features/">Baseline</a>. It’s a guarantee that a specific feature works in every major browser<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>, and it also lets you know since when - <strong>newly available</strong> features work in all the latest browsers, and <strong>widely available</strong> ones work in browsers up to 2.5 years old. <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Nesting_selector">Nesting</a>, for example, has been fully supported in all browsers since December 2023, and thus will become <em>widely available</em> in June 2026. You can find the Baseline symbols in various places, such as the MDN docs<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>.</p>
<p>These are just a few examples of what makes modern CSS so much nicer to write than what we had even just 5 years ago. It almost feels like comparing ES3<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup> to ECMAScript 2025 - and I wouldn’t blame your grudge if the former is what you’re used to.</p>
<h2 id="why-bother">Why bother?</h2>
<p>Okay, so CSS has more quality-of-life stuff than before. Still, why would one choose to use it over something else? Doesn’t JavaScript already let us do everything just fine?</p>
<art-frame aria-label="You need to disable JavaScript to run this app" role="img" center=""><fake-frame>You need to <span>disable</span> JavaScript to run this app.</fake-frame></art-frame>
<p>I think my reasons for using CSS fall into two main categories - because some users don’t want to use JavaScript, and because doing things in CSS can be genuinely better.</p>
<p>My blog, for example, focuses on infosec topics. Many security researchers (myself included) use a hardened browser configuration to protect themselves, which often means disabling JavaScript by default. I think it’s nice that they can fully experience my blog without changing their security settings or running a separate, sandboxed browser.</p>
<p>The same goes for privacy-conscious users, and it makes sense! As an experiment, I opened up a local Estonian news site in a web browser with JavaScript enabled. Can you guess how many js files it fetched? <em>(answer in footnote<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>)</em> That’s crazy! You do not want that running on your computer.</p>
<p>But surely, you are not <em>one of the evil devs</em> who loads a double-digit number of analytics scripts on your site - is there still any reason to reach for CSS?</p>
<p>Well, I think a lot of things are just plain nicer to make in HTML/CSS, both from the developer and end-user perspectives, be it for ease of use, accessibility, or performance.</p>
<p>Hover effects for your buttons? Toast animations? Input validation? All of these things <em>just work</em> in CSS, and you won’t have to reinvent the wheel, or throw kilobytes of someone else’s code at it. There will always be some cases where you do need that extra flexibility JavaScript often provides, but if you don’t need that, and doing it in CSS is easier, then why not save yourself the trouble?</p>
<art-frame aria-label="A fun pink button demo with shading, gradient, and shadows." role="figure" id="cool-anim">
<art-strs>
  <art-str></art-str>
  <art-str></art-str>
  <art-str></art-str>
  <art-str></art-str>
  <art-str></art-str>
  <art-str></art-str>
  <art-str></art-str>
  <art-str></art-str>
  <art-str></art-str>
  <art-str></art-str>
  <art-str></art-str>
  <art-str></art-str>
</art-strs>
<art-box></art-box>
<art-box></art-box>
<art-box></art-box>
<art-box></art-box>
<art-box></art-box>
<art-box></art-box>
<art-style>

</art-style>

<art-target></art-target>
</art-frame>
<p>And the performance of CSS is so much better! Every JavaScript interaction has to go through an event loop that wastes CPU cycles, eats some battery, and adds that tiny bit of stutter to everything.</p>
<p>Sure, in the grand scale of things it isn’t <em>that</em> bad, APIs like <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/requestAnimationFrame">requestAnimationFrame</a> are really good at keeping things smooth. But CSS animations run in the separate compositor thread, and <a href="https://web.dev/articles/animations-and-performance#css_vs_javascript_performance">aren’t affected</a> by stutters and blocking in the event loop.</p>
<p>It makes quite a difference on low-end devices, but feels nice even on high-end ones. CSS animations on my 240hz monitor look amazing<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup> - JS can look pretty good too, but it has that tiny bit of stutter to it that keeps it from being perfect, especially if you plan on running other heavy code at the same time.</p>
<p>It also means you won’t have to worry as much about optimization, as the browser takes care of a lot more of the rendering side of things, and often runs your stuff on the GPU if possible.</p>
<p><em>Pro tip! Wanna trigger animations from JS anyways? Use the modern <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Animations_API/Using_the_Web_Animations_API">Web Animations API</a> to easily play the smooth CSS animations from JS.</em></p>
<h2 id="transitioning">Transitioning</h2>
<!--Speaking of which, let's get to the first show & tell section of the post, where I show you something cool and tell you how it's made.-->
<p>Speaking of which, I think it’s time I start showing you practical examples, and a good place to <em>start</em> showing the <em>styles</em> is well, <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@starting-style"><em>@starting-style</em></a>.</p>
<p>In the past it has been pretty annoying to add start animations (such as fade-ins) to elements. You’ve had to either set up an entire CSS animation with a separate <em>@keyframes</em> block to go with it, or do a transition using JavaScript where you first add an element to the page, then wait a frame, and then add a class to the element.</p>
<code-compare aria-label="Demo: A toast message fading in from bottom" role="figure" id="starting-style-toast"><code-frame role="code">.<sx-y>toast</sx-y> {
  <sx-p>transition</sx-p>: <sx-a>opacity</sx-a> <sx-n>1s</sx-n>, <sx-a>translate</sx-a> <sx-n>1s</sx-n>;
  <sx-p>opacity</sx-p>: <sx-n>1</sx-n>;
  <sx-p>translate</sx-p>: <sx-n>0</sx-n> <sx-n>0</sx-n>;
  <sx-z>@starting-style</sx-z> {
    <sx-p>opacity</sx-p>: <sx-n>0</sx-n>;
    <sx-p>translate</sx-p>: <sx-n>0</sx-n> <sx-n>10px</sx-n>;
  }
}</code-frame>
<fake-frame><span></span><p>Success!</p></fake-frame></code-compare>

<p>But this has all changed thanks to the new <em>@starting-style</em> at-rule!</p>
<p>Pretty much all you have to do is set your properties as usual, add the initial transition states to <em>@starting-style</em>, and add those properties to a transition. It’s pretty simple and it kind of <em>just works</em> without having to trigger the animation in any way.</p>
<h2 id="lunalover">Lunalover</h2>
<p>Another good example of where CSS shines is theming. Many sites <em>need</em> separate light and dark modes, and modern CSS makes dealing with that pretty easy.</p>
<code-compare aria-label="Demo: Various elements affected by theme setting" role="figure" id="dark-example-1"><code-frame role="code">:<sx-l>root</sx-l> {
  <sx-p>color-scheme</sx-p>: <sx-a>light</sx-a> <sx-a>dark</sx-a>;
  <sx-e>--text</sx-e>: <sx-k>light-dark</sx-k>(<sx-n>#000</sx-n>, <sx-n>#FFF</sx-n>);
  <sx-e>--bg</sx-e>: <sx-k>light-dark</sx-k>(<sx-n>#EEE</sx-n>, <sx-n>#242936</sx-n>);
}</code-frame>
<fake-frame><p>hi there!</p><a href="https://karnaboy.bandcamp.com/track/lunalover" target="_blank"></a><p>you are awesome!<br><label>i am!</label></p></fake-frame></code-compare>

<p>By setting the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/color-scheme">color-scheme</a> property to <code>light dark</code>, you are telling the browser to automatically pick the theme according to the user preference, and you can then make use of that by setting color values with the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/color_value/light-dark">light-dark()</a> function.</p>
<p>Not only does it set your own colors, but also those of the native components, such as the default buttons, form elements, and scrollbars. It kind of just <a href="https://infosec.exchange/@rebane2001/115060623979682479">makes stuff work</a> by default, and that’s nice!</p>
<code-compare aria-label="Demo: Buttons to change theme between auto, light, and dark" role="figure" id="dark-example-2"><code-frame role="code">:<sx-l>root</sx-l> {
  <sx-p>color-scheme</sx-p>: <sx-a>light</sx-a> <sx-a>dark</sx-a>;
  &amp;:<sx-l>has</sx-l>(#<sx-a>theme-light</sx-a>:<sx-l>checked</sx-l>) {
    <sx-p>color-scheme</sx-p>: <sx-a>light</sx-a>;
  }
  &amp;:<sx-l>has</sx-l>(#<sx-a>theme-dark</sx-a>:<sx-l>checked</sx-l>) {
    <sx-p>color-scheme</sx-p>: <sx-a>dark</sx-a>;
  }
}</code-frame>
<fake-frame><theme-picker aria-label="Theme picker" role="radiogroup">
    <label>Auto</label>
    <label>Light</label>
    <label>Dark</label>
  </theme-picker></fake-frame></code-compare>

<p>You can then add some way of overriding the <em>color-scheme</em> property to let the user pick a theme different from their system setting. Here I am using radio buttons to accomplish that.</p>
<p><em>Pro tip! CSS can’t save the theme preference, but you can still do progressive enhancement. Make the themes work CSS-only, and then add the saving/loading of preference as an optional extra in JavaScript or server-side code.</em></p>
<h2 id="lyres-and-accordions">Lyres and accordions</h2>
<p><em>“But those don’t look like radio buttons”</em> I hear you cry.</p>
<p>Input elements such as radio buttons and checkboxes are a great foundation to build other stuff on top of - the example above consists of labels for the buttons and invisible radio buttons that can be checked for with the <em>:checked</em> pseudo-class.</p>
<code-compare aria-label="Demo: Making radio buttons look like normal buttons" role="figure" vertical="" id="radio-example"><code-frame role="code"><sx-t>&lt;radio-picker</sx-t> <sx-r>aria-label</sx-r><sx-t>=</sx-t><sx-v>"Radio buttons example"</sx-v> <sx-r>role</sx-r><sx-t>=</sx-t><sx-v>"radiogroup"</sx-v><sx-t>&gt;</sx-t>
  <sx-t>&lt;label&gt;&lt;input</sx-t> <sx-r>type</sx-r><sx-t>=</sx-t><sx-v>"radio"</sx-v> <sx-r>name</sx-r><sx-t>=</sx-t><sx-v>"demo"</sx-v> <sx-r>id</sx-r><sx-t>=</sx-t><sx-v>"veni"</sx-v> <sx-r>checked</sx-r><sx-t>&gt;</sx-t>veni<sx-t>&lt;/label&gt;</sx-t>
  <sx-t>&lt;label&gt;&lt;input</sx-t> <sx-r>type</sx-r><sx-t>=</sx-t><sx-v>"radio"</sx-v> <sx-r>name</sx-r><sx-t>=</sx-t><sx-v>"demo"</sx-v> <sx-r>id</sx-r><sx-t>=</sx-t><sx-v>"vidi"</sx-v><sx-t>&gt;</sx-t>vidi<sx-t>&lt;/label&gt;</sx-t>
  <sx-t>&lt;label&gt;&lt;input</sx-t> <sx-r>type</sx-r><sx-t>=</sx-t><sx-v>"radio"</sx-v> <sx-r>name</sx-r><sx-t>=</sx-t><sx-v>"demo"</sx-v> <sx-r>id</sx-r><sx-t>=</sx-t><sx-v>"vici"</sx-v><sx-t>&gt;</sx-t>vici<sx-t>&lt;/label&gt;</sx-t>
<sx-t>&lt;/radio-picker&gt;</sx-t>
<sx-t>&lt;style&gt;</sx-t>
  <sx-t>radio-picker</sx-t> {
    <sx-p>display</sx-p>: <sx-a>flex</sx-a>;
    <sx-t>label</sx-t> {
      &amp;:<sx-l>has</sx-l>(<sx-t>input</sx-t>:<sx-l>checked</sx-l>) {
        <sx-p>box-shadow</sx-p>: <sx-a>inset</sx-a> <sx-n>0px</sx-n> <sx-n>0px</sx-n> <sx-n>8px</sx-n> <sx-n>0px</sx-n> <sx-n>#888</sx-n>;
      }
      &amp;:<sx-l>has</sx-l>(<sx-t>input</sx-t>:<sx-l>focus-visible</sx-l>) {
        <sx-p>outline</sx-p>: <sx-n>2px</sx-n> <sx-a>solid</sx-a> <sx-n>#000</sx-n>;
      }
      <sx-p>box-shadow</sx-p>: <sx-a>inset</sx-a> <sx-n>0px</sx-n> <sx-n>0px</sx-n> <sx-n>1.2px</sx-n> <sx-n>0px</sx-n> <sx-n>#000</sx-n>;
      <sx-p>padding</sx-p>: <sx-n>10px</sx-n>;
      <sx-p>cursor</sx-p>: <sx-a>pointer</sx-a>;
      <sx-p>background</sx-p>: <sx-n>#0002</sx-n>;
      &amp;:<sx-l>hover</sx-l> { <sx-p>background</sx-p>: <sx-n>#0004</sx-n>; }
      &amp;:<sx-l>active</sx-l> { <sx-p>background</sx-p>: <sx-n>#0006</sx-n>; }
    }
    <sx-t>input</sx-t> {
      <sx-c>/* To allow screen reader to still access these. */</sx-c>
      <sx-p>opacity</sx-p>: <sx-n>0</sx-n>;
      <sx-p>position</sx-p>: <sx-a>absolute</sx-a>;
      <sx-p>pointer-events</sx-p>: <sx-a>none</sx-a>;
    }
  }
<sx-t>&lt;/style&gt;</sx-t>
</code-frame>
<fake-frame><radio-picker aria-label="Radio buttons example" role="radiogroup">
    <label>veni</label>
    <label>vidi</label>
    <label>vici</label>
  </radio-picker></fake-frame></code-compare>

<p>This is how I made the theme selector from the previous example. I’ve made the radio buttons half-visible in the demo for clarity, but with the <code>opacity: 0</code> they would not actually be visible.</p>
<p>There’s a whole lot going on here, so let’s break it down.</p>
<pre><code><sx-t>&lt;radio-picker</sx-t> <sx-r>aria-label</sx-r><sx-t>=</sx-t><sx-v>"Radio buttons example"</sx-v> <sx-r>role</sx-r><sx-t>=</sx-t><sx-v>"radiogroup"</sx-v><sx-t>&gt;</sx-t>
</code></pre>
<p>We start off with the <em>radio-picker</em> element - I just made it up, you can use a div instead if you’d prefer. We give it an aria-label to give the group an accessible name, and the aria role of <em>radiogroup</em> to make it work as a group for the radio buttons.</p>
<p>You could also use the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/fieldset"><em>fieldset</em></a> element instead of doing the aria roles if that’d fit your use case better.</p>
<pre><code><sx-t>&lt;label&gt;&lt;input</sx-t> <sx-r>type</sx-r><sx-t>=</sx-t><sx-v>"radio"</sx-v> <sx-r>name</sx-r><sx-t>=</sx-t><sx-v>"demo"</sx-v> <sx-r>id</sx-r><sx-t>=</sx-t><sx-v>"veni"</sx-v> <sx-r>checked</sx-r><sx-t>&gt;</sx-t>veni<sx-t>&lt;/label&gt;</sx-t>
<sx-t>&lt;label&gt;&lt;input</sx-t> <sx-r>type</sx-r><sx-t>=</sx-t><sx-v>"radio"</sx-v> <sx-r>name</sx-r><sx-t>=</sx-t><sx-v>"demo"</sx-v> <sx-r>id</sx-r><sx-t>=</sx-t><sx-v>"vidi"</sx-v><sx-t>&gt;</sx-t>vidi<sx-t>&lt;/label&gt;</sx-t>
<sx-t>&lt;label&gt;&lt;input</sx-t> <sx-r>type</sx-r><sx-t>=</sx-t><sx-v>"radio"</sx-v> <sx-r>name</sx-r><sx-t>=</sx-t><sx-v>"demo"</sx-v> <sx-r>id</sx-r><sx-t>=</sx-t><sx-v>"vici"</sx-v><sx-t>&gt;</sx-t>vici<sx-t>&lt;/label&gt;</sx-t>
</code></pre>
<p>Next, we add the radio buttons with their respective labels - usually you’d have to use the <em>for</em> attribute on labels to define which element they’re referring to, but since we have the <em>input</em> inside the <em>label</em> we don’t have to do that.</p>
<p>All the <code>type="radio"</code> inputs should also have a <em>name</em> value set to the same thing so that they are grouped together (you still need<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup> the radiogroup though). And then you can give them values or ids however you want.</p>
<pre><code><sx-t>label</sx-t> {
  &amp;:<sx-l>has</sx-l>(<sx-t>input</sx-t>:<sx-l>checked</sx-l>) {
    <sx-p>box-shadow</sx-p>: <sx-a>inset</sx-a> <sx-n>0px</sx-n> <sx-n>0px</sx-n> <sx-n>8px</sx-n> <sx-n>0px</sx-n> <sx-n>#888</sx-n>;
  }
  &amp;:<sx-l>has</sx-l>(<sx-t>input</sx-t>:<sx-l>focus-visible</sx-l>) {
    <sx-p>outline</sx-p>: <sx-n>2px</sx-n> <sx-a>solid</sx-a> <sx-n>#000</sx-n>;
  }
  <sx-p>box-shadow</sx-p>: <sx-a>inset</sx-a> <sx-n>0px</sx-n> <sx-n>0px</sx-n> <sx-n>1.2px</sx-n> <sx-n>0px</sx-n> <sx-n>#000</sx-n>;
  <sx-p>padding</sx-p>: <sx-n>10px</sx-n>;
  <sx-p>cursor</sx-p>: <sx-a>pointer</sx-a>;
  <sx-p>background</sx-p>: <sx-n>#0002</sx-n>;
  &amp;:<sx-l>hover</sx-l> { <sx-p>background</sx-p>: <sx-n>#0004</sx-n>; }
  &amp;:<sx-l>active</sx-l> { <sx-p>background</sx-p>: <sx-n>#0006</sx-n>; }
}
</code></pre>
<p>We then style the labels as we wish - the <em>:hover</em> and <em>:active</em> pseudo-classes can be used to make the buttons more fun to click, the <em>:has(input:checked)</em> selector can be used to define the style of the selected button, and the <em>:has(input:focus-visible)</em> selector can be used to add an outline when someone tabs over to the button.</p>
<p>The difference between <em>:focus</em> and <em>:focus-visible</em> is that the former shows up even if you use your mouse, while the latter only shows up when you use keyboard navigation, so it’s often visually more clean to use the latter.</p>
<pre><code><sx-t>input</sx-t> {
  <sx-p>opacity</sx-p>: <sx-n>0</sx-n>;
  <sx-p>position</sx-p>: <sx-a>absolute</sx-a>;
  <sx-p>pointer-events</sx-p>: <sx-a>none</sx-a>;
}
</code></pre>
<p>And last, we make the radio button input <em>exist</em> while not being visible. This is a bit hacky, but it’s how you can keep this control accessible to keyboard navigation and screen readers.</p>
<p>And that’s how we get the cool-looking radio buttons!</p>
<code-compare vertical="" id="tab-example"><code-frame role="code"><sx-t>&lt;radio-tabs&gt;</sx-t>
  <sx-t>&lt;div</sx-t> <sx-r>tabindex</sx-r><sx-t>=</sx-t><sx-v>0</sx-v> <sx-r>id</sx-r><sx-t>=</sx-t><sx-v>"tab-veni"</sx-v><sx-t>&gt;</sx-t>veni...<sx-t>&lt;/div&gt;</sx-t>
  <sx-t>&lt;div</sx-t> <sx-r>tabindex</sx-r><sx-t>=</sx-t><sx-v>0</sx-v> <sx-r>id</sx-r><sx-t>=</sx-t><sx-v>"tab-vidi"</sx-v><sx-t>&gt;</sx-t>vidi...<sx-t>&lt;/div&gt;</sx-t>
  <sx-t>&lt;div</sx-t> <sx-r>tabindex</sx-r><sx-t>=</sx-t><sx-v>0</sx-v> <sx-r>id</sx-r><sx-t>=</sx-t><sx-v>"tab-vici"</sx-v><sx-t>&gt;</sx-t>vici...<sx-t>&lt;/div&gt;</sx-t>
<sx-t>&lt;/radio-tabs&gt;</sx-t>
<sx-t>&lt;style&gt;</sx-t>
  <sx-t>body</sx-t>:<sx-l>has</sx-l>(#<sx-a>veni</sx-a>:<sx-l>not</sx-l>(:<sx-l>checked</sx-l>)) #<sx-a>tab-veni</sx-a>,
  <sx-t>body</sx-t>:<sx-l>has</sx-l>(#<sx-a>vidi</sx-a>:<sx-l>not</sx-l>(:<sx-l>checked</sx-l>)) #<sx-a>tab-vidi</sx-a>,
  <sx-t>body</sx-t>:<sx-l>has</sx-l>(#<sx-a>vici</sx-a>:<sx-l>not</sx-l>(:<sx-l>checked</sx-l>)) #<sx-a>tab-vici</sx-a> {
    <sx-p>display</sx-p>: <sx-a>none</sx-a>;
  }
<sx-t>&lt;/style&gt;</sx-t></code-frame>
<fake-frame><div><radio-picker aria-label="Tabs example" role="radiogroup">
    <label>veni</label>
    <label>vidi</label>
    <label>vici</label>
</radio-picker>
<radio-tabs>
  <p><strong>veni</strong><br><em>/ˈveɪni/</em><br>(intransitive) to come</p>
  <p><strong>vidi</strong><br><em>/ˈviːdi/</em><br>(intransitive) to see</p>
  <p><strong>vici</strong><br><em>/ˈviːt͡ʃi/</em><br>(intransitive) to conquer</p>
</radio-tabs></div></fake-frame></code-compare>

<p>We can now use them in the CSS however we want by just seeing if they’re <em>:checked</em>. Here I made tabs with separate divs for the content by using a <em>:has</em> selector on a parent element to find out which radio button is currently selected.</p>
<p>The <em>:has</em> selector has to be on a parent element that contains both the radio button and the target element - you can simply use <em>html</em> or <em>body</em> if you want it to work across the entire page. You should <strong>never</strong> use something like <code>:has(…)</code> by itself as it’ll run the selector for every element of the page, which can cause performance issues (<code>body:has(…)</code> is okay).</p>
<code-compare aria-label="Demo: Details elements in an FAQ format" role="figure" id="details-example"><code-frame role="code"><sx-t>&lt;div&gt;</sx-t>
  <sx-t>&lt;details</sx-t> <sx-r>name</sx-r><sx-t>=</sx-t><sx-v>"deets"</sx-v><sx-t>&gt;</sx-t>
    <sx-t>&lt;summary&gt;</sx-t>What's your name?<sx-t>&lt;/summary&gt;</sx-t>
    My name is Lyra Rebane.
  <sx-t>&lt;/details&gt;</sx-t>
  <sx-t>&lt;details</sx-t> <sx-r>name</sx-r><sx-t>=</sx-t><sx-v>"deets"</sx-v><sx-t>&gt;</sx-t>
    ...
  <sx-t>&lt;/details&gt;</sx-t>
<sx-t>&lt;/div&gt;</sx-t>
<sx-t>&lt;style&gt;</sx-t>
  <sx-t>div</sx-t> {
    <sx-p>border</sx-p>: <sx-n>1px</sx-n> <sx-a>solid</sx-a> <sx-n>#AAA</sx-n>;
    <sx-p>border-radius</sx-p>: <sx-n>8px</sx-n>;
    <sx-c>/* based on the MDN example */</sx-c>
    <sx-t>summary</sx-t> {
      <sx-p>font-weight</sx-p>: <sx-a>bold</sx-a>;
      <sx-p>margin</sx-p>: <sx-n>-0.5em</sx-n> <sx-n>-0.5em</sx-n> <sx-n>0</sx-n>;
      <sx-p>padding</sx-p>: <sx-n>0.5em</sx-n>;
      <sx-p>cursor</sx-p>: <sx-a>pointer</sx-a>;
    }
    <sx-t>details</sx-t> {
      &amp;:<sx-l>last-child</sx-l> { <sx-p>border</sx-p>: <sx-a>none</sx-a> }
      <sx-p>border-bottom</sx-p>: <sx-n>1px</sx-n> <sx-a>solid</sx-a> <sx-n>#aaa</sx-n>;
      <sx-p>padding</sx-p>: <sx-n>0.5em</sx-n> <sx-n>0.5em</sx-n> <sx-n>0</sx-n>;
      &amp;[<sx-r>open</sx-r>] {
        <sx-p>padding</sx-p>: <sx-n>0.5em</sx-n>;
        <sx-t>summary</sx-t> {
          <sx-p>border-bottom</sx-p>: <sx-n>1px</sx-n> <sx-a>solid</sx-a> <sx-n>#aaa</sx-n>;
          <sx-p>margin-bottom</sx-p>: <sx-n>0.5em</sx-n>;
        }
      }
    }
  }
<sx-t>&lt;/style&gt;</sx-t></code-frame>
<fake-frame>
  <p>
    <!-- for some reason, the name thing doesn't work in tor browser -->
    <details name="deets"><summary>What's your name?</summary>My name is Lyra Rebane.</details>
    <details name="deets"><summary>Cool name!</summary>I know ^_^</details>
    <details name="deets"><summary>Where can I learn more?</summary>On my website, <a href="https://lyra.horse/">lyra.horse</a>!</details>
  </p>
</fake-frame></code-compare>

<p>Finally, before we move on, I want to give you a quick introduction to the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/details">details</a> element. It’s great for if you want an accordion-style menu, such as for a FAQ section. The details open and close independently of each other, but you can set their name attribute to the same value to have only one open at a time.</p>
<p>Using them is pretty easy, put your content and a <em>summary</em> tag inside a <em>details</em> tag, and put the title inside the <em>summary</em> tag. The example above is a bit more convoluted for the visual flair, but all you <em>really</em> need is the html part of it.</p>
<p>The details elements are pretty stylable! You can add animations depending on the <em>[open]</em> state, and you can also get rid of the arrow by setting <code>list-style: none</code> on the <em>summary</em>.</p>
<p>Also, ctrl+f works with it, which is a big win in my book!</p>
<h2 id="validation">Validation</h2>
<p>And lastly, I want to show you the power of input validation in HTML and CSS.</p>
<code-compare aria-label="Demo: Input validation" role="figure" vertical="" id="validation-example-1"><code-frame role="code"><sx-t>&lt;label</sx-t> <sx-r>for</sx-r><sx-t>=</sx-t><sx-v>"usrname"</sx-v><sx-t>&gt;</sx-t>Username<sx-t>&lt;/label&gt;</sx-t>
<sx-t>&lt;input</sx-t> <sx-r>type</sx-r><sx-t>=</sx-t><sx-v>"text"</sx-v> <sx-r>id</sx-r><sx-t>=</sx-t><sx-v>"usrname"</sx-v> <sx-r>pattern</sx-r><sx-t>=</sx-t><sx-v>"\w{3,16}"</sx-v> <sx-r>required</sx-r><sx-t>&gt;</sx-t>
<sx-t>&lt;small&gt;</sx-t>3-16 letters, only alphanum and _.<sx-t>&lt;/small&gt;</sx-t>
<sx-t>&lt;style&gt;</sx-t>
 <sx-t>input</sx-t>:<sx-l>valid</sx-l> {
   <sx-p>border</sx-p>: <sx-n>1px</sx-n> <sx-a>solid</sx-a> <sx-a>green</sx-a>;
 }
 <sx-t>input</sx-t>:<sx-l>invalid</sx-l> {
   <sx-p>border</sx-p>: <sx-n>1px</sx-n> <sx-a>solid</sx-a> <sx-a>red</sx-a>;
 }
<sx-t>&lt;/style&gt;</sx-t></code-frame>
<fake-frame>
  <p><label for="usrname">Username</label>
    
    <small>3-16 letters, only alphanum and _.</small>
  </p>
</fake-frame></code-compare>

<p>This is a simple example of how you can validate an input field with a regex pattern. If you set a <em>pattern</em> attribute like above, a form that contains the input cannot be submitted unless the field matches the pattern. If you’re submitting something like <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/input/email">an e-mail address</a>, <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/input/tel">a phone number</a>, or <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/input/url">a url</a>, it might make sense to use the respective input types instead of writing your own regex.</p>
<p>Now, where CSS comes in is styling the input to show whether its value is valid. In the example above, I’m using <em>:valid</em> and <em>:invalid</em> to set a border color, but that comes with the downside of <em>always</em> having your input marked, even if the user hasn’t entered anything yet.</p>
<code-compare aria-label="Demo: Input validation" role="figure" id="validation-example-2"><code-frame role="code"><sx-t>input</sx-t> {
  <sx-p>border</sx-p>: <sx-a>none</sx-a>;
  <sx-p>border-radius</sx-p>: <sx-n>2px</sx-n>;
  <sx-p>outline</sx-p>: <sx-n>1px</sx-n> <sx-a>solid</sx-a> <sx-n>#000</sx-n>;
  &amp;:<sx-l>focus</sx-l> { <sx-p>outline-width</sx-p>: <sx-n>2px</sx-n>; }
  &amp;:<sx-l>user-valid</sx-l> { <sx-p>outline-color</sx-p>: <sx-a>green</sx-a>; }
  &amp;:<sx-l>user-invalid</sx-l> { <sx-p>outline-color</sx-p>: <sx-a>red</sx-a>; }
}</code-frame>
<fake-frame>
  <p><label for="usrname2">Username</label>
    
    <small>3-16 letters, only alphanum and _.</small>
  </p>
</fake-frame></code-compare>

<p>An easy win here is to instead use <em>:user-valid</em> and <em>:user-invalid</em> - these pseudo-classes only become active once you’ve interacted with input field. I also made this example use an outline instead of a border, which I think looks a lot nicer.</p>
<p>It may sometimes even make sense to use a combination of <em>:valid</em> and <em>:user-invalid</em>.</p>
<p>And of course, you can use the <em>:has</em> selector to style other elements depending on the input too!</p>
<code-compare aria-label="Demo: Specific requirements password challenge for fun" role="figure" id="validation-example-3">
<fake-frame>
  <p><label for="paswd">Password</label>
    
    <small>The password must:<br>
- be 8-16 characters<br>
- contain at least ⅰ roman numeral<br>
- not end with a letter<br>
</small>
  </p>
</fake-frame></code-compare>

<p>This one's just for fun ^_-! <span title="you've now unlocked a scrapped animation in the button thing!">you win! yay<a href="#cool-anim">!</a></span></p>
<p>I do want to mention that for some stuff, such as date pickers <no-wrap>()</no-wrap> or datalists <no-wrap>()</no-wrap>, there are built-in elements that do the job, but you may find them limited in one way or the other. If you’re making an input like that with specific requirements, you may still need to dip your feet in a bit of JavaScript.</p>
<datalist id="ponies">
  <option value="Twilight Sparkle"></option>
  <option value="Pinkie Pie"></option>
  <option value="Fluttershy"></option>
  <option value="Rainbow Dash"></option>
  <option value="Rarity"></option>
  <option value="Lyra Heartstrings"></option>
  <option value="Bonbon"></option>
  <option value="DJ Pon-3"></option>
  <option value="Octavia"></option>
  <option value="Colgate"></option>
  <option value="Carrot Top"></option>
  <option value="Berry Punch"></option>
  <option value="Derpy Hooves"></option>
  <option value="Dr. Hooves"></option>
  <option value="Jasmine Leaf"></option>
</datalist>
<!--
## Container queries
-->
<h2 id="do-not-the-vwvh">Do not the vw/vh</h2>
<p>This section is kind of random but I wanted to include it here because I think a lot of people are messing this one up and I want more people to know how to do this stuff right.</p>
<p>So CSS has vw/vh units that correspond to 1% of the viewport width and height respectively, which makes perfect sense for desktop browsers.</p>

<div>
<epic-phone aria-label="A phone displaying a web page cut off from the top and bottom" role="figure"><phone-screen>
<!--
<clickbait-circle style="top:46px;left:134px;scale:1.25 0.75"></clickbait-circle>
<clickbait-circle style="bottom:-3px;left:70px;scale:1.75 0.75"></clickbait-circle>
-->
<phone-notifications>
  <div><p><span>CB</span></p><div><p>Signal chat</p><p>Are you feeling encrypted?</p></div></div>
  <div><p><span>M</span></p><div><p>Marat</p><p>it smells of onions in here...</p></div></div>
  <div><p><span>bm</span></p><div><p>blackle mori</p><p>what's the scoop in yer smacker, horseberry?</p></div></div>
  
  <div><p><span>P</span></p><div><p>PatTheHyruler</p><p>I just lost the game</p></div></div>
  <div><p><span>M</span></p><div><p>Malk</p><p>I can't wait to taste the sorbet!</p></div></div>
</phone-notifications>
<phone-content>
<phone-urlbar aria-label="URL bar" role="figure"><phone-urlbar-inner><p>🔒</p><svg xmlns="http://www.w3.org/2000/svg"><path d="M11.55 13.52a2.27 2.27 0 0 1 -1.68 -0.69a2.29 2.29 0 0 1 -0.69 -1.68c0 -0.66 0.23 -1.22 0.7 -1.68a2.3 2.3 0 0 1 1.68 -0.69c0.66 0 1.22 0.23 1.68 0.69c0.46 0.46 0.69 1.02 0.69 1.68a2.27 2.27 0 0 1 -0.69 1.68c-0.46 0.46 -1.02 0.69 -1.68 0.69Zm0 -1.45c0.25 0 0.47 -0.09 0.65 -0.27a0.88 0.88 0 0 0 0.27 -0.64a0.89 0.89 0 0 0 -0.27 -0.65a0.88 0.88 0 0 0 -0.65 -0.27a0.88 0.88 0 0 0 -0.65 0.27a0.88 0.88 0 0 0 -0.26 0.64c0 0.25 0.09 0.47 0.27 0.65c0.18 0.18 0.4 0.27 0.65 0.27Zm-9.47 -0.1v-1.63H7.98v1.63Zm2.37 -4.75a2.27 2.27 0 0 1 -1.67 -0.69a2.29 2.29 0 0 1 -0.69 -1.68c0 -0.66 0.23 -1.22 0.7 -1.68a2.3 2.3 0 0 1 1.68 -0.69c0.66 0 1.22 0.23 1.68 0.69c0.46 0.46 0.69 1.02 0.69 1.68c0 0.66 -0.23 1.22 -0.69 1.68c-0.46 0.46 -1.02 0.69 -1.68 0.69Zm0 -1.46a0.88 0.88 0 0 0 0.65 -0.27a0.88 0.88 0 0 0 0.27 -0.64a0.89 0.89 0 0 0 -0.26 -0.65a0.88 0.88 0 0 0 -0.65 -0.27a0.88 0.88 0 0 0 -0.65 0.27a0.88 0.88 0 0 0 -0.27 0.65c0 0.25 0.09 0.47 0.27 0.65c0.18 0.18 0.39 0.27 0.65 0.27Zm3.57 -0.1V4.03h5.9v1.63Zm0 0Z"></path></svg><span><span>lyra.horse</span>/blog/</span></phone-urlbar-inner><!--<div style="border-radius:6px;border: 2px solid;width:15px;height:12px;font-size:9px;user-select:none;text-align: center;font-weight:600;line-height: 12px;">:D</div>--><a href="https://youtu.be/7zbNBCb_AOU" target="_blank">•••</a></phone-urlbar>

<div><ul><li><a href="https://lyra.horse/blog/" target="_blank">lyra's epic blog</a></li> <li><a href="https://lyra.horse/blog/posts/" target="_blank">posts</a></li> <li><a href="https://lyra.horse/blog/tags/" target="_blank">tags</a></li></ul></div>
<p>You no longer need JavaScript</p>
<p>yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap</p>
</phone-content>

</phone-screen></epic-phone>
</div>
<p>Where it becomes a bit more nuanced is on mobile devices. For example, mobile versions of both Firefox and Chrome will hide the URL bar when scrolling down on a page.</p>
<p>This causes the vw/vh units to be a bit ambigous - do they represent the <em>entire</em> available screen, only the area that’s visible with the URL bar, or something in between?</p>
<p>If it’s the first option, you might end up with buttons or links off-screen<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup>! If it’s the second, you may end up with a background div that doesn’t cover the entire background.</p>

<div>
<epic-phone aria-label="A phone displaying the differences between the svh, dvh, and lvh units" role="figure"><phone-screen>
<phone-notifications>
  <div><p><span>p</span></p><div><p>pingotux</p><p>css spec so good i transitioned</p></div></div>
  
  
  
  
  
  
</phone-notifications>

<smol-arrow></smol-arrow>
<smol-arrow line=""></smol-arrow>
<smol-arrow down=""></smol-arrow>
<smol-arrow></smol-arrow>
<smol-arrow line=""></smol-arrow>
<smol-arrow down=""></smol-arrow>
<smol-arrow></smol-arrow>
<smol-arrow line=""></smol-arrow>
<smol-arrow down=""></smol-arrow>
<phone-content>
<phone-urlbar aria-label="URL bar" role="figure"><phone-urlbar-inner><p>🔒</p><svg xmlns="http://www.w3.org/2000/svg"><path d="M11.55 13.52a2.27 2.27 0 0 1 -1.68 -0.69a2.29 2.29 0 0 1 -0.69 -1.68c0 -0.66 0.23 -1.22 0.7 -1.68a2.3 2.3 0 0 1 1.68 -0.69c0.66 0 1.22 0.23 1.68 0.69c0.46 0.46 0.69 1.02 0.69 1.68a2.27 2.27 0 0 1 -0.69 1.68c-0.46 0.46 -1.02 0.69 -1.68 0.69Zm0 -1.45c0.25 0 0.47 -0.09 0.65 -0.27a0.88 0.88 0 0 0 0.27 -0.64a0.89 0.89 0 0 0 -0.27 -0.65a0.88 0.88 0 0 0 -0.65 -0.27a0.88 0.88 0 0 0 -0.65 0.27a0.88 0.88 0 0 0 -0.26 0.64c0 0.25 0.09 0.47 0.27 0.65c0.18 0.18 0.4 0.27 0.65 0.27Zm-9.47 -0.1v-1.63H7.98v1.63Zm2.37 -4.75a2.27 2.27 0 0 1 -1.67 -0.69a2.29 2.29 0 0 1 -0.69 -1.68c0 -0.66 0.23 -1.22 0.7 -1.68a2.3 2.3 0 0 1 1.68 -0.69c0.66 0 1.22 0.23 1.68 0.69c0.46 0.46 0.69 1.02 0.69 1.68c0 0.66 -0.23 1.22 -0.69 1.68c-0.46 0.46 -1.02 0.69 -1.68 0.69Zm0 -1.46a0.88 0.88 0 0 0 0.65 -0.27a0.88 0.88 0 0 0 0.27 -0.64a0.89 0.89 0 0 0 -0.26 -0.65a0.88 0.88 0 0 0 -0.65 -0.27a0.88 0.88 0 0 0 -0.65 0.27a0.88 0.88 0 0 0 -0.27 0.65c0 0.25 0.09 0.47 0.27 0.65c0.18 0.18 0.39 0.27 0.65 0.27Zm3.57 -0.1V4.03h5.9v1.63Zm0 0Z"></path></svg><span><span>lyra.horse</span>/blog/</span></phone-urlbar-inner><a href="https://soundcloud.com/prodlightnex/anything-but-job" target="_blank">•••</a></phone-urlbar>

<smol-arrow></smol-arrow>

<div>
  <p>lvh</p>
  <p>svh</p>
  <p>dvh</p>
  <p>lvh</p>
  <p>svh</p>
  <p>dvh</p>
  <p>lvh</p>
  <p>svh</p>
  <p>dvh</p>
  <p>lvh</p>
  <p>svh</p>
  <p>dvh</p>
  <p>lvh</p>
  <p>svh</p>
  <p>dvh</p>
</div>
<p>Your values</p>
<table>
  <thead>
    <tr>
      <th>Unit</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>vh</th>
      <td><unit-value vh=""></unit-value>px</td>
    </tr>
    <tr>
      <th>lvh</th>
      <td><unit-value lvh=""></unit-value>px</td>
    </tr>
    <tr>
      <th>dvh</th>
      <td><unit-value dvh=""></unit-value>px</td>
    </tr>
    <tr>
      <th>svh</th>
      <td><unit-value svh=""></unit-value>px</td>
    </tr>
  </tbody>
</table>
<p>Above is a table of values your browser reports - if you're on mobile, try scrolling the blogpost up and down so that the URL bar hides and see how the numbers change.</p>
<p>The values are multiplied by 100 (eg 100vh is used instead of 1vh).</p>

</phone-content>
</phone-screen></epic-phone>
</div>
<p>The solution to this is to use the new responsive viewport units: <strong>lvh</strong>, <strong>svh</strong>, and <strong>dvh</strong>.</p>
<p><strong>lvh</strong> stands for <em>largest</em> viewport height, and thus is useful for things like backgrounds that you’d want to cover the entire screen with, and wouldn’t care about getting cut off.</p>
<p><strong>svh</strong> stands for <em>smallest</em> viewport height, and should be used for things that must always fit on the screen, such as buttons and links.</p>
<p>And <strong>dvh</strong> stands for <em>dynamic</em> viewport height - this one will update to whatever the current viewport height is. It might seem like the obvious choice, but it should not be used for elements you don’t want resizing or moving around as the user scrolls the page, as it could become quite annoying and possibly even laggy otherwise.</p>
<p>Of course, the respective <strong>lvw</strong>, <strong>svw</strong>, and <strong>dvw</strong> units exist too :).</p>
<h3 id="keyboard-cat">Keyboard cat</h3>
<p>By default, the viewport units do not account for the keyboard overlaying the page.</p>
<p>There are two ways to deal with that: the <em><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Guides/Viewport_meta_element#interactive-widget">interactive-widget</a></em> attribute, and the <em><a href="https://developer.mozilla.org/en-US/docs/Web/API/VirtualKeyboard_API">VirtualKeyboard</a></em> API.</p>
<p>The former option is widely supported across browsers, works without JS, and goes in the meta viewport tag. It makes it so that opening the keyboard will change <em>all</em> of the viewport units.</p>
<pre><code><sx-t>&lt;meta</sx-t> <sx-r>name</sx-r><sx-t>=</sx-t><sx-v>"viewport"</sx-v> <sx-r>content</sx-r><sx-t>=</sx-t><sx-v>"width=device-width, interactive-widget=resizes-content"</sx-v><sx-t>&gt;</sx-t>
</code></pre>
<p>The latter option is currently only supported in Chromium-based browsers, and requires a single line of JavaScript to use:</p>
<pre><code><sx-e>navigator</sx-e>.<sx-p>virtualKeyboard</sx-p>.<sx-p>overlaysContent</sx-p> = <sx-a>true</sx-a>;
</code></pre>
<p>The advantage of the second option is that it allows you to use <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/env">environment variables</a> in CSS to get the position and size of the keyboard, which is pretty cool.</p>
<pre><code><sx-t>floating-button</sx-t> {
  <sx-p>margin-bottom</sx-p>: <sx-k>env</sx-k>(<sx-a>keyboard-inset-height</sx-a>, <sx-n>0px</sx-n>);
}
</code></pre>
<p>But considering the fact that it doesn’t work cross-browser, I’d avoid it.</p>
<h2 id="css-wishlist">CSS wishlist</h2>
<p>Alright, so this is a little different from the rest of the post, but I wanted to bring up some things that I wish were in CSS. I haven’t fully fleshed out all of them, so some definitely wouldn’t fit the spec as-is, but maybe they can inspire some other stuff at least.</p>
<p>They are just fun ideas, don’t take them too seriously.</p>
<h3 id="reusable-blocks">Reusable blocks</h3>
<p>I wish it was possible to put classes in other classes in CSS, so that you could write something like:</p>
<pre><code>.<sx-y>border</sx-y> {
  <sx-p>border</sx-p>: <sx-n>2px</sx-n> <sx-a>solid</sx-a>;
  <sx-p>border-radius</sx-p>: <sx-n>4px</sx-n>;
}

.<sx-y>button</sx-y> {
  <sx-z>@apply</sx-z> <sx-y>border</sx-y>;
}

.<sx-y>card</sx-y> {
  <sx-z>@apply</sx-z> <sx-y>border</sx-y>;
}
</code></pre>
<p>This is something that <a href="https://tailwindcss.com/docs/functions-and-directives#apply-directive">Tailwind already has</a>, and that makes me jealous.</p>
<!--
### Scoped variables

I wish we could scope variables, so that an inner element could set the value for an outer element.

<pre class="sx-block"><code><sx-t>foo</sx-t> {
  <sx-k>@property</sx-k> <sx-k>--color</sx-k> {
    <sx-p>syntax</sx-p>: <sx-s>'&lt;color&gt;'</sx-s>;
    <sx-p>initial-value</sx-p>: <sx-a>red</sx-a>;
    <sx-p>scoped</sx-p>: <sx-a>true</sx-a>;
  }
  <sx-t>bar</sx-t> {
    &amp;:<sx-l>hover</sx-l> {
      <sx-e>--color</sx-e>: <sx-a>blue</sx-a>;
    }
  }
  <sx-c>/* foo will be blue on bar hover */</sx-c>
  <sx-p>color</sx-p>: <sx-k>var</sx-k>(<sx-e>--color</sx-e>);
}
</code></pre>
This idea has a lot of problems in practice, from little things such as nested statements getting reordered, to bigger issues that'd require rewriting and rethinking big parts of CSS. I don't see this ever being added, but it would be fun to have.
-->
<h3 id="combined-media-selectors">Combined @media selectors</h3>
<p>We can currently do nested <em>@media</em> queries, and also multiple selectors at the same time:</p>
<pre><code><sx-t>div</sx-t> {
  &amp;.<sx-y>foo</sx-y>, &amp;.<sx-y>bar</sx-y> {
    <sx-p>color</sx-p>: <sx-a>red</sx-a>;
    <sx-p>padding</sx-p>: <sx-n>8px</sx-n>;
    <sx-p>font-size</sx-p>: <sx-n>2em</sx-n>;
  }
  <sx-z>@media</sx-z> (<sx-p>width</sx-p> &lt; <sx-n>480px</sx-n>) {
    <sx-p>color</sx-p>: <sx-a>red</sx-a>;
    <sx-p>padding</sx-p>: <sx-n>8px</sx-n>;
    <sx-p>font-size</sx-p>: <sx-n>2em</sx-n>;
  }
}
</code></pre>
<p>But we cannot combine the two into a single selector:</p>
<pre><code><sx-t>div</sx-t> {
  <sx-z>@media</sx-z> (<sx-p>width</sx-p> &lt; <sx-n>480px</sx-n>), &amp;.<sx-y>foo</sx-y> {
    <sx-p>color</sx-p>: <sx-a>red</sx-a>;
    <sx-p>padding</sx-p>: <sx-n>8px</sx-n>;
    <sx-p>font-size</sx-p>: <sx-n>2em</sx-n>;
  }
}
</code></pre>
<p>Which means if you want to do that you’ll inevitably have to repeat code or do some silly variable hacks, neither of which is ideal.</p>
<h3 id="n-th-child-variable">n-th child variable</h3>
<p>For many of the CSS crimes I like to commit, I often end up writing code like:</p>
<pre><code><sx-t>div</sx-t> {
  <sx-t>span</sx-t>:<sx-l>nth-child</sx-l>(<sx-n>1</sx-n>) { <sx-e>--nth</sx-e>: <sx-n>1</sx-n>; }
  <sx-t>span</sx-t>:<sx-l>nth-child</sx-l>(<sx-n>2</sx-n>) { <sx-e>--nth</sx-e>: <sx-n>2</sx-n>; }
  <sx-t>span</sx-t>:<sx-l>nth-child</sx-l>(<sx-n>3</sx-n>) { <sx-e>--nth</sx-e>: <sx-n>3</sx-n>; }
  <sx-t>span</sx-t>:<sx-l>nth-child</sx-l>(<sx-n>4</sx-n>) { <sx-e>--nth</sx-e>: <sx-n>4</sx-n>; }
  <sx-t>span</sx-t>:<sx-l>nth-child</sx-l>(<sx-n>5</sx-n>) { <sx-e>--nth</sx-e>: <sx-n>5</sx-n>; }
  ...
  <sx-y>span</sx-y> {
    <sx-p>top</sx-p>: <sx-k>calc</sx-k>(<sx-e>--nth</sx-e> * <sx-n>24px</sx-n>);
    <sx-p>color</sx-p>: <sx-k>hsl</sx-k>(<sx-k>calc</sx-k>(<sx-k>var</sx-k>(<sx-e>--nth</sx-e>) * <sx-n>90deg</sx-n>) <sx-n>100</sx-n> <sx-n>90</sx-n>);
  }
}
</code></pre>
<p>And I think it would be a lot nicer if we could instead just do:</p>
<pre><code><sx-t>div</sx-t> {
  <sx-t>span</sx-t> {
    <sx-e>--nth</sx-e>: <sx-k>nth-child</sx-k>();
    <sx-p>top</sx-p>: <sx-k>calc</sx-k>(<sx-e>--nth</sx-e> * <sx-n>24px</sx-n>);
    <sx-p>color</sx-p>: <sx-k>hsl</sx-k>(<sx-k>calc</sx-k>(<sx-k>var</sx-k>(<sx-e>--nth</sx-e>) * <sx-n>90deg</sx-n>) <sx-n>100</sx-n> <sx-n>90</sx-n>);
  }
}
</code></pre>
<h3 id="n-th-letter-targeting">n-th letter targeting</h3>
<p>CSS has the ability to style the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/::first-letter">::first-letter</a> of text. It’d be cool if were was also a <strong>::nth-letter(…)</strong> selector, similar to <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/:nth-child">:nth-child</a>. I suspect the reason this isn’t a thing is because the <em>::first-letter</em> selector is a pseudo-element, which would be a bit tricky to implement with the nth-letter idea.</p>
<code-compare aria-label="Demo: the letter i in hi there is made red" role="figure" id="nth-example"><code-frame role="code"><sx-c>/* not a real feature */</sx-c>
<sx-t>p</sx-t>::<sx-l>nth-letter</sx-l>(2) {
  <sx-p>color</sx-p>: <sx-a>red</sx-a>;
}</code-frame>
<fake-frame>
<p>h<span>i</span> there~</p>
</fake-frame></code-compare>

<p><a href="https://www.blackle-mori.com/">Blackle</a> suggested that combining the nth-child() variable with :nth-letter targeting would also be fun for certain effects, such as putting the value in the sin() function to create wavy text.</p>
<code-compare aria-label="Demo: the text waves vertically, and has a trans-colored gradient going across it" role="figure" vertical="" id="wavey-example"><code-frame role="code"><sx-t>div</sx-t> {
  <sx-c>/* not a real feature */</sx-c>
  <sx-e>--nth</sx-e>: <sx-k>nth-child</sx-k>(<sx-a>nth-letter</sx-a>);
  <sx-p>will-change</sx-p>: <sx-a>transform</sx-a>;
  <sx-p>translate</sx-p>: <sx-n>0</sx-n> <sx-k>calc</sx-k>(<sx-k>sin</sx-k>(<sx-k>var</sx-k>(<sx-e>--nth</sx-e>) * <sx-n>0.35</sx-n> - <sx-k>var</sx-k>(<sx-e>--wave</sx-e>) * <sx-n>3</sx-n>) * <sx-n>5px</sx-n>);
  <sx-p>color</sx-p>: <sx-k>color-mix</sx-k>(<sx-a>in</sx-a> <sx-a>oklch</sx-a>, <sx-n>#58C8F2</sx-n>, <sx-n>#EDA4B2</sx-n> <sx-k>calc</sx-k>(<sx-k>sin</sx-k>(<sx-k>var</sx-k>(<sx-e>--nth</sx-e>) * <sx-n>0.5</sx-n> - <sx-k>var</sx-k>(<sx-e>--wave</sx-e>)) * <sx-n>50%</sx-n> + <sx-n>50%</sx-n>));
}</code-frame>
<fake-frame>
  <a href="https://blog.polly.computer/untuck_NOW_queen/" aria-label="untuck now queen" target="_blank">
    <p>u</p><p>n</p><p>t</p><p>u</p><p>c</p><p>k</p><p>n</p><p>o</p><p>w</p><p>q</p><p>u</p><p>e</p><p>e</p><p>n</p>
  </a>
  <p>(<span>tap</span><span>hover</span> to play animation)</p>
</fake-frame></code-compare>

<h3 id="unit-removal">Unit removal</h3>
<p>I wish you could easily remove units from values, for example by dividing them.</p>

<pre><code><sx-t>div</sx-t> {
  <sx-c>/* Turns into: <screen-size></screen-size> (no unit) */</sx-c>
  <sx-e>--screen-width</sx-e>: <sx-k>calc</sx-k>(<sx-n>100vw</sx-n> / <sx-n>1px</sx-n>);
  <sx-p>color</sx-p>: <sx-k>hsl</sx-k>(<sx-k>var</sx-k>(<sx-e>--screen-width</sx-e>) <sx-n>100</sx-n>, <sx-n>50</sx-n>);
}</code></pre>
<p>This would allow you to use the size of the viewport or container as a numeric variable for things other than length. For example, the <a href="#color-picker">color picker</a> from earlier uses it to convert the location of the color picker dot to a number to be used in a color value instead.</p>
<p>Uh, but wait? Does that mean this feature already exists?</p>
<p>Yeah, lol! We already have the ability to get unitless values in CSS, but it involves doing hacky stuff such as <code>tan(atan2(var(--vw), 1px))</code> with a custom <em>@property</em>. It’d be nice to have this as just a division, for example.</p>
<p>Oh, and good news, this one we might actually be <a href="https://www.w3.org/TR/css-values-4/#calc-type-checking">getting soon</a>!</p>
<p><em>Also if you do something like <code>calc(1px + sqrt(1px * 1px))</code> your browser will crash<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup>.</em></p>
<h3 id="a-better-image-function">A better image function</h3>
<p>The <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/image/image"><strong>image()</strong> function</a> exists, but no browsers implement it. It’s similar to just using <em>url()</em>, but adds some really cool features such as a fallback color, and image fragments to crop a smaller section out of a bigger image (think spritesheets).</p>
<p>We can already do both fallbacks and spritesheets with the various background properties, but it’d be nice to have this pretty syntax. I’d honestly love this syntax even more for &lt;img&gt; tags than CSS.</p>
<h3 id="style-tags-in-body">style tags in body</h3>
<p>I make heavy use of &lt;style&gt; tags in &lt;body&gt; for my projects. On my blog, for example, I write the relevant CSS close to their graphics so that you can <a href="https://infosec.exchange/@rebane2001/114931064484832451">start reading the blog</a> before the entire page (or the entire CSS) has finished loading<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup>. And it works great!</p>
<p>But what’s unfortunate is that despite browsers supporting this, and major sites using this, it’s <a href="https://github.com/whatwg/html/issues/1605">not officially spec-compliant</a>. I suspect it’s in the spec to avoid the <a href="https://en.wikipedia.org/wiki/Flash_of_unstyled_content">FOUC</a> footgun, but there are so many reasons you would want/need style in body that I don’t think it justifies it.</p>
<p>I think an HTML validator should warn for this, but not error.</p>
<h2 id="the-art">The art</h2>
<p>I want to end this article by saying that to me, web development is an art, and thus, CSS is too. I often have a hard time relating to people who do webdev solely to earn money or build a startup - web development is very different when you’re on a team and are given tasks from above instead of having free will over what you create for fun.</p>
<p>It’s probably most apparent with things like AI<sup id="fnref:14"><a href="#fn:14" role="doc-noteref">14</a></sup>, that for me take all the fun and creativity out of my work. But it also applies to build chain tooling such as linters and minifiers - the way I write my code is part of the art, and I don’t want a tool to erase that. I don’t even use an IDE<sup id="fnref:15"><a href="#fn:15" role="doc-noteref">15</a></sup>.</p>
<p>Among the practical reasons for sticking to CSS listed throughout this post, there’s a secret extra reason I like to do everything in CSS, and that’s expression and art. Art isn’t always practical, and using CSS isn’t either. But it’s how I like to express myself, and it’s why I do what I do.</p>
<p>I tried to keep this post approachable and practical for all web developers. But there is so much more to CSS that I’d like to talk about, so expect another post about the stuff that isn’t practical, and is instead just cool as fuck. I think <em>CSS is a programming language</em>, and <a href="https://lyra.horse/css-clicker/">I made a game to prove it</a>.</p>
<p>But that’s a topic for another time.</p>
<h2 id="afterword">afterword</h2>
<p>it’s been almost a year since my last post, but i hope it’s been worth the wait ^_^</p>
<p>as usual, this post is a self-contained html file with no javascript, images, or other external resources - everything on the page is handwritten html/css, weighing in at around 49kB gzipped. it was really fun creating all the little interactive widgets and visuals this time around, i think i’ve improved in css a lot since the last time i posted.</p>
<p>this entire post turned out to be a bit of a fun mess (as did i!), it’s almost like a chaotic gradient of tone throughout, i hope it was still interesting and enjoyable to read though.</p>
<p>i have a few new posts in the works: in addition to the second css one mentioned earlier, i also have one about a new web vulnerability subclass i discovered, and one about a trans topic. i’m not sure when these posts will come out, but we’ll see! make sure to add me to your rss reader if that sounds fun.</p>
<p>i’ll also be giving <a href="https://pretalx.com/bsides-tallinn-2025/talk/S3V8UY/">a talk</a> at bsides tallinn in september! i’m hoping to also do css-related talks at the next ccc and disobey, but we’ll have to see whether i get accepted and have the travel budget for those.</p>
<p>thank you so much for reading &lt;3</p>
<p id="youAre">you're awesome!! (i can tell because you checked <a href="#awesome">that checkbox</a> from earlier)</p>

<p><strong>Discuss this post on:</strong> <a href="https://twitter.com/rebane2001/status/1961167342530183535">twitter</a>, <a href="https://infosec.exchange/@rebane2001/115108329158660789">mastodon</a>, <a href="https://lobste.rs/s/xx7dbi/you_no_longer_need_javascript_overview">lobsters</a></p>
<!--[^interactive]: They actually don't.-->



  </div></div>]]></description>
        </item>
    </channel>
</rss>