<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 12 Nov 2023 19:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HTML First – Six principles for building simple, maintainable, web software (212 pts)]]></title>
            <link>https://html-first.com/</link>
            <guid>38241304</guid>
            <pubDate>Sun, 12 Nov 2023 16:00:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://html-first.com/">https://html-first.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38241304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
  

<p>HTML First is a set of principles that aims to make building web software <strong>easier</strong>, <strong>faster</strong>, more <strong>inclusive</strong>, and more <strong>maintainable</strong> by...</p>

<ol>
<li>Leveraging the default capabilities of modern web browsers.</li>
<li>Leveraging the extreme simplicity of HTML's attribute syntax.</li>
<li>Leveraging the web's ViewSource affordance.</li>
</ol>

<h2>Goals</h2>

<p>The main goal of HTML First is to <strong>substantially widen the pool of people who can work on web software codebases</strong>. This is good from an individual perspective because it allows a greater number of people to become web programmers, to build great web software, and increase their income. It's also good from a business perspective as it decreases the cost of building software, and decreases the amount of resources required to hire - a notoriously resource intensive process. </p>

<p>A second goal of HTML First is to make it more <ins>enjoyable</ins> and <ins>seamless</ins> to build web software. Most web programmers are familiar with the excitement of seeing their product come together rapidly as they transition smoothly between the text editor and the browser, with very few unexpected potholes or context switches. But today it takes several years of mastering tools and frameworks to get to that stage. HTML First principles should allow people to unlock that feeling, and level of mastery, much earlier on in their coding journey.</p>

<p>The way we achieve these goals is by acknowledging that <a href="https://new.tonyennis.com/blog/M3WoiPA5P-comparing-the-readability-and-learning-curve-of-html" target="_blank">HTML is very easy to understand</a>, and thus using HTML as the bedrock of our product - not only to define content and structure, but also to set styling and behaviours.</p>

<h2>Principles</h2>

<ul>
<li><a href="#vanilla-approaches">Prefer Vanilla approaches</a></li>
<li><a href="#attributes-for-styling-behaviour">Use HTML attributes for styling and behaviour</a></li>
<li><a href="#attributes-for-libraries">Use libraries that leverage HTML attributes</a></li>
<li><a href="#build-steps">Avoid Build Steps</a></li>
<li><a href="#naked-html">Prefer Naked HTML</a></li>
<li><a href="#view-source">Be View-Source Friendly</a></li>
</ul>



<h2><strong>Use "vanilla" approaches to achieve desired functionality over external frameworks</strong></h2>

<p>The range of things that browsers support out of the box is large, and growing. Before adding a library or framework to your codebase, check whether you can achieve it using plain old html/css.<br>
<strong>Encouraged</strong></p>

<pre><code>&lt;details&gt;
  &lt;summary&gt;Click to toggle content&lt;/summary&gt;
  &lt;p&gt;This is the full content that is revealed when a user clicks on the summary&lt;/p&gt;
&lt;/details&gt;    
</code></pre>

<p><strong>Discouraged</strong></p>

<pre><code>import React, { useState } from 'react';

const DetailsComponent = () =&gt; {
  const [isContentVisible, setContentVisible] = useState(false);

  const toggleContent = () =&gt; {
    setContentVisible(!isContentVisible);
  };

  return (
    &lt;details&gt;
      &lt;summary onClick={toggleContent}&gt;Click to toggle content&lt;/summary&gt;
      {isContentVisible &amp;&amp; &lt;p&gt;This is the full content that is revealed when a user clicks on the summary&lt;/p&gt;}
    &lt;/details&gt;
  );
};

export default DetailsComponent;
</code></pre>



<h2>Where possible, default to defining style and behaviour with inline HTML attributes</h2>

<p>For styling this can be enabled with an SPC library like <a href="https://github.com/tonyennis145/dumb-tailwind" target="_blank">Tailwind</a> or <a href="http://tachyons.io/" target="_blank">Tachyons</a>. For behaviour, you can use libraries like <a href="https://hyperscript.org/" target="_blank">hyperscript</a>, <a href="https://alpinejs.dev/" target="_blank">Alpine</a>, or similar. Yes, this does mean your HTML will <em>look</em> busy. But it also means it will be easier for other developers to find and understand behaviour, navigate it, and make changes to it.</p>

<p><strong>Encouraged</strong></p>

<pre><code>&lt;div onlick="this.classList.add('bg-green')"&gt;
  Click Me
&lt;/div&gt;
</code></pre>

<p><strong>Discouraged</strong></p>

<pre><code>&lt;div id="results-pane"&gt;
  Click Me
&lt;/div&gt;
</code></pre>

<pre><code>#results-pane.active {
  background-color: green;
}
</code></pre>

<pre><code>var resultsPane = document.getElementById("myDiv");
resultsPane.addEventListener("click", function() {
    this.classList.add("active");
});
</code></pre>

<p>You may notice that this approach seems to violate <a href="https://en.wikipedia.org/wiki/Separation_of_concerns" target="_blank">Separation of Concerns</a> - one of the most commonly-touted software design principles. We believe an all-or-nothing approach to SoC is flawed, and instead advocate an approach that accounts for Locality of Behaviour and acknoweldges the trade-offs between the two.   </p>

<ul>
<li><a href="https://htmx.org/essays/locality-of-behaviour/" target="_blank">HTMX on The Locality of Behaviour Principle</a></li>
<li><a href="https://adamwathan.me/css-utility-classes-and-separation-of-concerns/" target="_blank">Adam Wathan on separation of styling concerns</a>.</li>
</ul>



<h2><strong>Where libraries are necessary, use libraries that leverage html attributes over libraries built around javascript or custom syntax</strong></h2>

<p><strong>Encouraged</strong></p>

<pre><code>&lt;script src="https://unpkg.com/<a href="https://html-first.com/cdn-cgi/l/email-protection" data-cfemail="d8b0a1a8bdaaabbbaab1a8ac98e8f6e8f6ef">[email&nbsp;protected]</a>/dist/hyperscript.min.js"&gt;&lt;/script&gt;
&lt;div&gt;
  &lt;input type="text" _="on input put me into #output"&gt;
  &lt;div id="output"&gt;&lt;/div&gt;
&lt;/div&gt;
</code></pre>

<p><strong>Discouraged</strong></p>

<pre><code>
&lt;script src="https://cdn.jsdelivr.net/npm/<a href="https://html-first.com/cdn-cgi/l/email-protection" data-cfemail="bbc8cfd2d6ced7cec8fb89958b958b">[email&nbsp;protected]</a>/dist/stimulus.umd.js"&gt;&lt;/script&gt;

&lt;div data-controller="echo"&gt;
  &lt;input type="text" data-echo-target="source" data-action="input-&gt;echo#update"&gt;
  &lt;div data-echo-target="output"&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script&gt;
  const application = Stimulus.Application.start();

  application.register("echo", class extends Stimulus.Controller {
      static targets = ["source", "output"]

      update() {
          this.outputTarget.textContent = this.sourceTarget.value;
      }
  });
&lt;/script&gt;
</code></pre>


      

<h2>Steer Clear of Build Steps</h2>

<p>Libraries that require transforming your files from one format to another add significant maintenance overhead, remove or heavily impair the ViewSource affordance , and usually dictate that developers learn new tooling in order to use them. <a href="https://stackoverflow.com/questions/36517829/what-does-multiplexing-mean-in-http-2/36519379#36519379" target="_blank">Modern browsers don't have the same performance constraints</a> that they did when these practices were introduced. And if we use HTML First libraries like static tailwind or htmx, the amount of additional CSS and JS needed is usually minimal.</p>

<p><strong>Encouraged</strong></p>

<pre><code>&lt;link rel="stylesheet" href="/styles.css"&gt;
</code></pre>

<p><strong>Discouraged</strong></p>

<pre><code>&lt;link href="/dist/output.css" rel="stylesheet"&gt;
</code></pre>

<pre><code>npx css-compile -i ./src/input.css -o ./dist/output.css --watch
</code></pre>

<p>Aside: The build step practice is so deeply ingrained that even one year ago this opinion was considered <a href="https://twitter.com/tonyennis/status/1579610085499998208" target="_blank">extremely fringe</a>. But in the last year has begun to gain significant steam. Some recent examples:</p>

<ul>
<li><a href="https://twitter.com/dhh/status/1719041666412347651" target="_blank">@dhh - "We've gone #NoBuild on CSS with 37signals"</a></li>
<li><a href="https://gomakethings.com/are-build-tools-an-anti-pattern/" target="_blank">Are build tools an anti-pattern by Chris Ferdinandi</a></li>
<li><a href="https://gomakethings.com/how-do-build-tools-break-backwards-compatibility/" target="_blank">How do build tools break backwards compatibility</a></li>
<li><a href="https://social.lol/@bw/111293266036805485" target="_blank">Blake Watson - "There has never been a better time to ditch build steps"</a></li>
</ul>



<h2>Prefer "naked" HTML to obfuscation layers that compile down to HTML</h2>

<p>This principle is most applicable to backend implementation. The underlying idea again here is readability. If a developer who has familiarity with HTML but not with your backend framework looks through your view files, they should still be able to understand 90%+ of what they see. As with above, this means sacrificing brevity for understandability. </p>

<p><strong>Encouraged</strong></p>

<pre><code>&lt;form action="&lt;%= new_signup_path %&gt;" method="post"&gt;
  &lt;div class="field"&gt;
    &lt;label for="first_name"&gt;First Name&lt;/label&gt;
    &lt;input id="first_name" type="text" value="&lt;%= @signup&amp;.first_name %&gt;" /&gt;
  &lt;/div&gt;
  &lt;div class="field"&gt;
    &lt;label for="last_name"&gt;Last Name&lt;/label&gt;
    &lt;input id="last_name" type="text" value="&lt;%= @signup&amp;.last_name %&gt;" /&gt;
  &lt;/div&gt;
  &lt;div class="field"&gt;
    &lt;label for="email"&gt;Last Name&lt;/label&gt;
    &lt;input id="email" type="text" value="&lt;%= @signup&amp;.email %&gt;" /&gt;
  &lt;/div&gt;
&lt;/form&gt;
</code></pre>

<p><strong>Discouraged</strong></p>

<pre><code>&lt;%= form_with url: "#", local: true do |form| %&gt;
  &lt;div class="field"&gt;
    &lt;%= form.label :first_name %&gt;
    &lt;%= form.text_field :first_name %&gt;
  &lt;/div&gt;

  &lt;div class="field"&gt;
    &lt;%= form.label :last_name %&gt;
    &lt;%= form.text_field :last_name %&gt;
  &lt;/div&gt;

  &lt;div class="field"&gt;
    &lt;%= form.label :email %&gt;
    &lt;%= form.email_field :email %&gt;
  &lt;/div&gt;

  &lt;div class="actions"&gt;
    &lt;%= form.submit %&gt;
  &lt;/div&gt;
&lt;% end %&gt;
</code></pre>



<h2>Where possible, maintain the right-click-view-source affordance</h2>

<p>The beauty of the early web was that it was always possible to "peek behind the curtains" and see the code that was responsible for any part of any web page. This was a gift to aspiring developers, as it allowed us to bridge the gap between the theoretical (reading about how code works) and the practical - seeing both code and interface alongside each other. For many sites, we could copy and paste the html or css and run it in ourselves to get a close-to-identical replica. "Remixing" existing snippets was not only a way to learn, but often formed the basis of our new creations.</p>

<p>In the time since, the industry has adopted several "improvements" which have made this practice much rarer. For example, if we use React - the most popular frontend framework, we cannot hit "View Source", copy the code, and remix it, because 1. React has a build step, meaning the code we see in the developer tools is different to the code that the developer wrote, and 2. React code snippets must be wrapped in a react application in order for them to work.</p>

<p>For sites that follow HTML First principles, we regain the ViewSource affordance again. In fact, HTML First sites often go one step further. Because if you define your <strong>UI interactions</strong> using HTML attributes, you can now also preserve these interactions when copy pasting into a new codebase, (provided your destination file includes the same js library). At some point we intend to levearge this to build an HTML First code snippet library. </p>

<ul>
<li><a href="https://htmx.org/essays/right-click-view-source/" target="_blank">HTMX.org on the ViewSource affordance</a></li>
</ul>

<h2>Wrapping Up</h2>

<p>The practices and principles described on this site are still considered niche in the industry as a whole, and the community of people using them small. One of my hopes with creating this site is to act as a Honeypot to find and connect like minded people with whom we can discuss and sharpen these ideas. If any of this resonates with you, I'd love to <a href="https://twitter.com/tonyennis" target="_blank">hear from you</a>.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPU Survival Toolkit for the AI age: The bare minimum every developer must know (201 pts)]]></title>
            <link>https://journal.hexmos.com/gpu-survival-toolkit/</link>
            <guid>38240421</guid>
            <pubDate>Sun, 12 Nov 2023 14:37:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journal.hexmos.com/gpu-survival-toolkit/">https://journal.hexmos.com/gpu-survival-toolkit/</a>, See on <a href="https://news.ycombinator.com/item?id=38240421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        <p><span><svg width="16" height="17" viewBox="0 0 16 17" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M4.49365 4.58752C3.53115 6.03752 2.74365 7.70002 2.74365 9.25002C2.74365 10.6424 3.29678 11.9778 4.28134 12.9623C5.26591 13.9469 6.60127 14.5 7.99365 14.5C9.38604 14.5 10.7214 13.9469 11.706 12.9623C12.6905 11.9778 13.2437 10.6424 13.2437 9.25002C13.2437 6.00002 10.9937 3.50002 9.16865 1.68127L6.99365 6.25002L4.49365 4.58752Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
</svg> Featured</span>
        </p>

        


        <section>

            <ul>
                <li>
                    <a href="https://journal.hexmos.com/author/rijul/">
                        <img src="https://journal.hexmos.com/content/images/size/w100/2023/08/3a30bb804e1845504626b40d9fb136d2.jpeg" alt="Rijul Rajesh">
                    </a>
                </li>
            </ul>

            <div>
                
                <p><time datetime="2023-11-12">Nov 12, 2023</time>
                        <span><span>•</span> 13 min read</span>
                </p>
            </div>

        </section>

            <figure>
                <img srcset="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/a230a3ffafd791a0973337dbab3db132061e9d02787646049eb519aae1cb0f72.png 300w,
                            https://journal-wa6509js.s3.ap-south-1.amazonaws.com/a230a3ffafd791a0973337dbab3db132061e9d02787646049eb519aae1cb0f72.png 600w,
                            https://journal-wa6509js.s3.ap-south-1.amazonaws.com/a230a3ffafd791a0973337dbab3db132061e9d02787646049eb519aae1cb0f72.png 1000w,
                            https://journal-wa6509js.s3.ap-south-1.amazonaws.com/a230a3ffafd791a0973337dbab3db132061e9d02787646049eb519aae1cb0f72.png 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/a230a3ffafd791a0973337dbab3db132061e9d02787646049eb519aae1cb0f72.png" alt="GPU Survival Toolkit for the AI age: The bare minimum every developer must know">
            </figure>

    </header>

    <section>
        <!--kg-card-begin: html--><h3 id="why-cpu-knowledge-is-no-longer-enough">Why CPU Knowledge Is No Longer Enough</h3>

<p>In today's AI age, the majority of developers train in the CPU way. This knowledge has been part of our academics as well, so it's obvious to think and problem-solve in a <strong>CPU-oriented way</strong>.</p>

<p>However, the problem with CPUs is that they rely on a <strong>sequential architecture</strong>. In today's world, where we are dependent on numerous parallel tasks, CPUs are unable to work well in these scenarios.</p>

<p>Some problems faced by developers include:</p>

<h4 id="executing-parallel-tasks">Executing Parallel Tasks</h4>

<p>CPUs traditionally operate linearly, executing one instruction at a time. This limitation stems from the fact that CPUs typically feature a few powerful cores optimized for single-threaded performance.</p>

<p>When faced with multiple tasks, a CPU allocates its resources to address each task one after the other, leading to a sequential execution of instructions. This approach becomes inefficient in scenarios where numerous tasks need simultaneous attention.</p>

<p>While we make efforts to enhance CPU performance through techniques like multi-threading, the fundamental design philosophy of CPUs prioritizes sequential execution.</p>

<h4 id="running-ai-models-efficiently">Running AI Models Efficiently</h4>

<p>AI models, employing advanced architectures like transformers, leverage parallel processing to enhance performance. Unlike older <strong>recurrent neural networks (RNNs)</strong> that operate sequentially, modern transformers such as <strong>GPT</strong> can concurrently process multiple words, increasing efficiency and capability in training. Because when we train in parallel, it will result in bigger models, and bigger models will yield better outputs.</p>

<p>The concept of parallelism extends beyond natural language processing to other domains like <strong>image recognition</strong>. For instance, <a href="https://medium.com/analytics-vidhya/concept-of-alexnet-convolutional-neural-network-6e73b4f9ee30?ref=journal.hexmos.com">AlexNet</a>, an architecture in image recognition, demonstrates the power of parallel processing by processing different parts of an image simultaneously, allowing for accurate pattern identification.</p>

<p>However, CPUs, designed with a focus on single-threaded performance, struggle to fully exploit parallel processing potential. They face difficulties efficiently distributing and executing the numerous parallel computations required for intricate AI models.</p>

<p>As a result, the development of GPUs has become prevalent to address the specific needs of parallel processing in AI applications, unlocking higher efficiency and faster computation.</p>

<h4 id="how-gpu-driven-development-solves-these-issues">How GPU Driven Development Solves These Issues</h4>

<p><strong>Massive Parallelism With GPU Cores</strong></p>

<p>Engineers design GPUs with <strong>smaller, highly specialized cores</strong> compared to the <strong>larger, more powerful cores</strong> found in CPUs. This architecture allows GPUs to execute a multitude of parallel tasks simultaneously.</p>

<p>The high number of cores in a GPU are well-suited for workloads depending on parallelism, such as graphics rendering and complex mathematical computations.</p>

<p>We will soon demonstrate how using GPU parallelism can reduce the time taken for complex tasks.</p>

<p><img alt="GPUDemo1" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/ee0bdbd2103cbed5d0fa894432cc476065984dcd3921bb9158e608a50fe62f7b.png"></p>

<p><strong>Parallelism Used In AI Models</strong></p>

<p>AI models, particularly those built on deep learning frameworks like <a href="https://www.tensorflow.org/?ref=journal.hexmos.com">TensorFlow</a>, exhibit a high degree of parallelism. Neural network training involves numerous matrix operations, and GPUs, with their expansive core count, excel in parallelizing these operations. TensorFlow, along with other popular deep learning frameworks, optimizes to leverage GPU power for accelerating model training and inference.</p>

<p>We will show a demo soon how to train a neural network using the power of the GPU.</p>

<p><img alt="GPUDemo1" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/5078635f14fc266b48058cafe1f214608eecd89f0ddfebfc481df24704a7bba9.png"></p>

<h3 id="cpus-vs-gpus-whats-the-difference">CPUs Vs GPUs: What’s the Difference?</h3>

<h4 id="cpu">CPU</h4>

<p><strong>Sequential Architecture</strong></p>

<p>Central Processing Units (CPUs) are designed with a focus on sequential processing. They excel at executing a single set of instructions linearly.</p>

<p>CPUs are optimized for tasks that require high single-threaded performance, such as</p>

<ul>
<li>General-purpose computing</li>
<li>System operations</li>
<li>Handling complex algorithms that involve conditional branching</li>
</ul>

<p><strong>Limited Cores For Parallel Tasks</strong></p>

<p>CPUs feature a smaller number of cores, often in the range of <strong>2-16</strong> cores in consumer-grade processors. Each core is capable of handling its own set of instructions independently.</p>

<h4 id="gpu">GPU</h4>

<p><strong>Parallelized Architecture</strong></p>

<p>Graphics Processing Units (GPUs) are designed with a parallel architecture, making them highly efficient for parallel processing tasks.</p>

<p>This is beneficial for</p>

<ul>
<li>Rendering graphics</li>
<li>Performing complex mathematical calculations</li>
<li>Running parallelizable algorithms</li>
</ul>

<p>GPUs handle multiple tasks simultaneously by breaking them into smaller, parallel sub-tasks.</p>

<p><strong>Thousands Of Cores For Parallel Tasks</strong></p>

<p>Unlike CPUs, GPUs boast a significantly larger number of cores, often numbering in the thousands. These cores are organized into streaming multiprocessors (SMs) or similar structures.</p>

<p>The abundance of cores allows GPUs to process a massive amount of data concurrently, making them well-suited for parallelisable tasks, such as image and video processing, deep learning, and scientific simulations.</p>

<h3 id="aws-gpu-instances-a-beginners-guide">AWS GPU Instances: A Beginner's Guide</h3>

<p>Amazon Web Services (AWS) offers a variety of GPU instances used for things like machine learning.</p>

<p>Here are the different types of AWS GPU instances and their use cases:</p>

<p><strong>General-Purpose Gpu Instances</strong></p>

<ul>
<li>
<p><a href="https://aws.amazon.com/ec2/instance-types/p3/?ref=journal.hexmos.com">P3</a> and <a href="https://aws.amazon.com/ec2/instance-types/p4/?ref=journal.hexmos.com">P4</a> instances serve as versatile general-purpose GPU instances, well-suited for a broad spectrum of workloads.</p>
</li>
<li>
<p>These include machine learning training and inference, image processing, and video encoding. Their balanced capabilities make them a solid choice for diverse computational tasks.</p>
</li>
<li>
<p><strong>Pricing:</strong> The p3.2xlarge instance costs <strong>$3.06</strong> per hour.</p>
</li>
<li>This provides 1 <a href="https://www.nvidia.com/en-gb/data-center/tesla-v100/?ref=journal.hexmos.com">NVIDIA Tesla V100 GPU</a> of 16 GB GPU memory</li>
</ul>

<p><strong>Inference-optimized GPU instances</strong></p>

<ul>
<li>
<p>Inference is the process of running live data through a trained AI model to make a prediction or solve a task.</p>
</li>
<li>
<p><a href="https://aws.amazon.com/ec2/instance-types/p5/?ref=journal.hexmos.com">P5</a> and <a href="https://aws.amazon.com/ec2/instance-types/inf1/?ref=journal.hexmos.com">Inf1</a> instances specifically cater to machine learning inference, excelling in scenarios where low latency and cost efficiency are essential.</p>
</li>
<li>
<p><strong>Pricing:</strong> the p5.48xlarge instance costs <strong>$98.32</strong> per hour.</p>
</li>
<li>This provides 8 <a href="https://www.nvidia.com/en-in/data-center/h100/?ref=journal.hexmos.com">NVIDIA H100 GPUs</a> of 80 GB memory each, totalling upto 640 GB Video Memory.</li>
</ul>

<p><strong>Graphics-optimized GPU instances</strong></p>

<ul>
<li>
<p><a href="https://aws.amazon.com/ec2/instance-types/g4/?ref=journal.hexmos.com">G4 instances</a> instances are engineered to handle graphics-intensive tasks.</p>
</li>
<li>
<p>A video game developer might use a G4 instance to render 3D graphics for a video game.</p>
</li>
<li><strong>Pricing:</strong> g4dn.xlarge costs <strong>$0.526</strong> to run per hour.</li>
<li>Uses 1 <a href="https://www.nvidia.com/en-in/data-center/tesla-t4/?ref=journal.hexmos.com">NVIDIA T4 GPU</a> of 16 GB Memory.</li>
</ul>

<p><strong>Managed GPU Instances</strong></p>

<ul>
<li>
<p><a href="https://aws.amazon.com/sagemaker/?ref=journal.hexmos.com">Amazon SageMaker</a> is a managed service for machine learning. It provides access to a variety of GPU-powered instances, including P3, P4, and P5 instances.</p>
</li>
<li>
<p>SageMaker is a good choice for organizations that wants to begin machine learning easily without having to manage the underlying infrastructure.</p>
</li>
<li>
<p><a href="https://aws.amazon.com/sagemaker/pricing/?ref=journal.hexmos.com">Pricing of Amazon Sagemaker</a></p>
</li>
</ul>

<h3 id="using-nvidias-cuda-for-gpu-driven-development">Using Nvidia's CUDA for GPU-Driven Development</h3>

<h4 id="what-is-cuda">What Is Cuda?</h4>

<p><strong>CUDA</strong> is a parallel computing platform and programming model developed by NVIDIA, enabling developers to accelerate their applications by harnessing the power of GPU accelerators.</p>

<p>The Practical examples in the demo will use CUDA.</p>

<h4 id="how-to-setup-cuda-on-your-machine">How to Setup Cuda on Your Machine</h4>

<p>To setup CUDA on your machine you can follow these steps.</p>

<ul>
<li>Download <a href="https://developer.nvidia.com/cuda-downloads?ref=journal.hexmos.com">CUDA</a></li>
<li>From the above link download the base installer as well as the driver installer</li>
<li>Go to .bashrc in home folder</li>
<li>
<p>Add the following lines below</p>
</li>
<li>
<p><code>export PATH="/usr/local/cuda-12.3/bin:$PATH"</code></p>
</li>
<li>
<p><code>export LD_LIBRARY_PATH="/usr/local/cuda-12.3/lib64:$LD_LIBRARY_PATH"</code></p>
</li>
<li>
<p>Execute the following commands</p>
</li>
<li><code>sudo apt-get install cuda-toolkit</code></li>
<li>
<p><code>sudo apt-get install nvidia-gds</code></p>
</li>
<li>
<p>Reboot the system for the changes to take effect</p>
</li>
</ul>

<h4 id="basic-commands-to-use">Basic Commands to Use</h4>

<p>Once you have CUDA installed, here are some helpful commands.</p>

<p><code>lspci | grep VGA</code></p>

<p>The purpose of this command is to identify and list the GPUs in your system.
<img alt="Alt text" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/ea108a4de53720d8e4de22247f92b43481a000cfe41026826e52f4af020b9c0f.png"></p>

<p><code>nvidia-smi</code></p>

<p>It stands for "NVIDIA System Management Interface"
It provides detailed information about the NVIDIA GPUs in your system, including utilization, temperature, memory usage and more.</p>

<p><img alt="Alt text" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/b467adbefdfe4fced1552ca63134994c2924dc648955adb585c80d1a6356f10f.png"></p>

<p><code>sudo lshw -C display</code></p>

<p>The purpose is to provide detailed information about the display controllers in your system, including graphics cards.
<img alt="Alt text" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/0af5ca02e576f5da48a561ebffb4bd20e4051899cb1b1e69caca78aeff58cff7.png"></p>

<p><code>inxi -G</code></p>

<p>This command provides information about the graphics subsystem, including details about the GPU and the display.
<img alt="Alt text" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/8352bf05d3be3b8085e837de0f69f244ab6c34041c6b462cf78513fcae6373c0.png"></p>

<p><code>sudo hwinfo --gfxcard</code></p>

<p>Its purpose is to obtain detailed information about the graphics cards in your system.</p>

<p><img alt="Alt text" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/70fa1b931593c213a1a949dbc700d47cca2e933cee331d63e03e0806d6327baa.png"></p>

<h3 id="get-started-with-the-cuda-framework">Get Started with the Cuda Framework</h3>

<p>As we have installed the CUDA Framework, let's start executing operations that showcases its functionality.</p>

<h4 id="array-addition-problem">Array Addition Problem</h4>

<p>A suitable problem to demonstrate the parallelization of GPUs is the <strong>Array addition problem</strong>.</p>

<p>Consider the following arrays:</p>

<ul>
<li>
<p>Array A = [1,2,3,4,5,6]</p>
</li>
<li>
<p>Array B = [7,8,9,10,11,12]</p>
</li>
<li>
<p>We need to store the sum of each element and store it in Array C.</p>
</li>
<li>
<p>Like C = [1+7,2+8,3+9,4+10,5+11,6+12] = [8,10,12,14,16,18]</p>
</li>
<li>
<p>If the CPU is to execute such operation, it would be executing the operation like the below code.</p>
</li>
</ul>

<div><pre><span></span><span>#include</span><span> </span><span>&lt;stdio.h&gt;</span>
<span>int</span><span> </span><span>a</span><span>[]</span><span> </span><span>=</span><span> </span><span>{</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>,</span><span>4</span><span>,</span><span>5</span><span>,</span><span>6</span><span>};</span>
<span>int</span><span> </span><span>b</span><span>[]</span><span> </span><span>=</span><span> </span><span>{</span><span>7</span><span>,</span><span>8</span><span>,</span><span>9</span><span>,</span><span>10</span><span>,</span><span>11</span><span>,</span><span>12</span><span>};</span>
<span>int</span><span> </span><span>c</span><span>[</span><span>6</span><span>];</span>

<span>int</span><span> </span><span>main</span><span>()</span><span> </span><span>{</span>
<span>    </span><span>int</span><span> </span><span>N</span><span> </span><span>=</span><span> </span><span>6</span><span>;</span><span>  </span><span>// Number of elements</span>

<span>    </span><span>for</span><span> </span><span>(</span><span>int</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span>)</span><span> </span><span>{</span>
<span>        </span><span>c</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>a</span><span>[</span><span>i</span><span>]</span><span> </span><span>+</span><span> </span><span>b</span><span>[</span><span>i</span><span>];</span>
<span>    </span><span>}</span>

<span>    </span><span>for</span><span> </span><span>(</span><span>int</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span>)</span><span> </span><span>{</span>
<span>        </span><span>printf</span><span>(</span><span>"c[%d] = %d"</span><span>,</span><span> </span><span>i</span><span>,</span><span> </span><span>c</span><span>[</span><span>i</span><span>]);</span>
<span>    </span><span>}</span>

<span>    </span><span>return</span><span> </span><span>0</span><span>;</span>
<span>}</span>
</pre></div>

<p>The previous method involves traversing the array elements one by one and performing the additions sequentially. However, when dealing with a <strong>substantial volume of numbers</strong>, this approach becomes sluggish due to its <strong>sequential nature</strong>.</p>

<p>To address this limitation, GPUs offer a solution by <strong>parallelizing the addition process</strong>. Unlike CPUs, which execute operations one after the other, GPUs can concurrently perform multiple additions.</p>

<p>For instance, the operations 1+7, 2+8, 3+9, 4+10, 5+11 and 6+12 can be executed simultaneously through parallelization with the assistance of a GPU.</p>

<p>Utilizing CUDA, the code to achieve this parallelized addition is as follows:</p>

<p>We will use a kernel file (.cu) for the demonstration.</p>

<p>Let's go through the code one by one.</p>

<div><pre><span></span><span>__global__</span><span> </span><span>void</span><span> </span><span>vectorAdd</span><span>(</span><span>int</span><span>*</span><span> </span><span>a</span><span>,</span><span> </span><span>int</span><span>*</span><span> </span><span>b</span><span>,</span><span> </span><span>int</span><span>*</span><span> </span><span>c</span><span>)</span>
<span>{</span>
<span>    </span><span>int</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>threadIdx</span><span>.</span><span>x</span><span>;</span>
<span>    </span><span>c</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>a</span><span>[</span><span>i</span><span>]</span><span> </span><span>+</span><span> </span><span>b</span><span>[</span><span>i</span><span>]</span><span>;</span>
<span>    </span><span>return</span><span>;</span>
<span>}</span>
</pre></div>

<ul>
<li>
<p><code>__global__</code> specifier indicates that this function is a kernel function, which will be called on the GPU.</p>
</li>
<li>
<p><code>vectorAdd</code> takes three integer pointers (a, b, and c) as arguments, representing vectors to be added.</p>
</li>
<li>
<p><code>threadIdx.x</code> retrieves the index of the current thread (in a one-dimensional grid).</p>
</li>
<li>
<p>The sum of the corresponding elements from vectors a and b is stored in vector c.</p>
</li>
</ul>

<p>Now lets go through the main function.</p>

<p>Pointers <code>cudaA</code>, <code>cudaB</code> and <code>cudaC</code> are created to point to memory on the GPU.</p>

<div><pre><span></span><span>// Uses CUDA to use functions that parallelly calculates the addition</span>
<span>int</span><span> </span><span>main</span><span>(){</span>
<span>    </span><span>int</span><span> </span><span>a</span><span>[]</span><span> </span><span>=</span><span> </span><span>{</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>};</span>
<span>    </span><span>int</span><span> </span><span>b</span><span>[]</span><span> </span><span>=</span><span> </span><span>{</span><span>4</span><span>,</span><span>5</span><span>,</span><span>6</span><span>};</span>
<span>    </span><span>int</span><span> </span><span>c</span><span>[</span><span>sizeof</span><span>(</span><span>a</span><span>)</span><span> </span><span>/</span><span> </span><span>sizeof</span><span>(</span><span>int</span><span>)]</span><span> </span><span>=</span><span> </span><span>{</span><span>0</span><span>};</span>
<span>    </span><span>// Create pointers into the GPU</span>
<span>    </span><span>int</span><span>*</span><span> </span><span>cudaA</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span>
<span>    </span><span>int</span><span>*</span><span> </span><span>cudaB</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span>
<span>    </span><span>int</span><span>*</span><span> </span><span>cudaC</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span>
</pre></div>

<p>Using <code>cudaMalloc</code>, memory is allocated on the GPU for the vectors cudaA, cudaB, and cudaC.</p>

<div><pre><span></span><span>// Allocate memory in the GPU</span>
<span>cudaMalloc</span><span>(</span><span>&amp;</span><span>cudaA</span><span>,</span><span>sizeof</span><span>(</span><span>a</span><span>));</span>
<span>cudaMalloc</span><span>(</span><span>&amp;</span><span>cudaB</span><span>,</span><span>sizeof</span><span>(</span><span>b</span><span>));</span>
<span>cudaMalloc</span><span>(</span><span>&amp;</span><span>cudaC</span><span>,</span><span>sizeof</span><span>(</span><span>c</span><span>));</span>
</pre></div>

<p>The content of vectors a and b is copied from the host to the GPU using <code>cudaMemcpy</code>.</p>

<div><pre><span></span><span>// Copy the vectors into the gpu</span>
<span>cudaMemcpy</span><span>(</span><span>cudaA</span><span>,</span><span> </span><span>a</span><span>,</span><span> </span><span>sizeof</span><span>(</span><span>a</span><span>),</span><span> </span><span>cudaMemcpyHostToDevice</span><span>);</span>
<span>cudaMemcpy</span><span>(</span><span>cudaB</span><span>,</span><span> </span><span>b</span><span>,</span><span> </span><span>sizeof</span><span>(</span><span>b</span><span>),</span><span> </span><span>cudaMemcpyHostToDevice</span><span>);</span>
</pre></div>

<p>The kernel function <code>vectorAdd</code> is launched with one block and a number of threads equal to the size of the vectors.</p>

<div><pre><span></span><span>// Launch the kernel with one block and a number of threads equal to the size of the vectors</span>
<span>vectorAdd</span><span> </span><span>&lt;&lt;&lt;</span><span>1</span><span>,</span><span> </span><span>sizeof</span><span>(</span><span>a</span><span>)</span><span> </span><span>/</span><span> </span><span>sizeof</span><span>(</span><span>a</span><span>[</span><span>0</span><span>])</span><span>&gt;&gt;&gt;</span><span> </span><span>(</span><span>cudaA</span><span>,</span><span> </span><span>cudaB</span><span>,</span><span> </span><span>cudaC</span><span>);</span>
</pre></div>

<p>The result vector <code>cudaC</code> is copied from the GPU back to the host.</p>

<div><pre><span></span><span>// Copy the result vector back to the host</span>
<span>cudaMemcpy</span><span>(</span><span>c</span><span>,</span><span> </span><span>cudaC</span><span>,</span><span> </span><span>sizeof</span><span>(</span><span>c</span><span>),</span><span> </span><span>cudaMemcpyDeviceToHost</span><span>);</span>
</pre></div>

<p>We can then print the results as usual</p>

<div><pre><span></span><span>    </span><span>// Print the result</span>
<span>    </span><span>for</span><span> </span><span>(</span><span>int</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>sizeof</span><span>(</span><span>c</span><span>)</span><span> </span><span>/</span><span> </span><span>sizeof</span><span>(</span><span>int</span><span>);</span><span> </span><span>i</span><span>++</span><span>)</span>
<span>    </span><span>{</span>
<span>        </span><span>printf</span><span>(</span><span>"c[%d] = %d"</span><span>,</span><span> </span><span>i</span><span>,</span><span> </span><span>c</span><span>[</span><span>i</span><span>]);</span>
<span>    </span><span>}</span>

<span>    </span><span>return</span><span> </span><span>0</span><span>;</span>
<span>}</span>
</pre></div>

<p>For executing this code, we will use <code>nvcc</code> command.</p>

<p>We will get the output as</p>

<p><img alt="GPU Output" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/144c2193b0d5ad71eb7bd4da490534fb278ed174df6ec8c6411b039f16757116.png"></p>

<p>Here's the <a href="https://github.com/RijulTP/GPUToolkit/tree/main/array-addition?ref=journal.hexmos.com">full code</a> for your reference.</p>

<h4 id="optimize-image-generation-in-python-using-the-gpu">Optimize Image Generation in Python Using the GPU</h4>

<ul>
<li>
<p>This section explores the optimization of performance-intensive tasks, such as image generation, using GPU processing.</p>
</li>
<li>
<p><strong>Mandelbrot set</strong> is a mathematical construct that forms intricate visual patterns based on the behavior of specific numbers in a prescribed equation. Generating one is a resource intensive operation.</p>
</li>
<li>
<p>In the following code snippet, you can observe the conventional method of generating a Mandelbrot set using CPU processing, which is slow.</p>
</li>
</ul>

<div><pre><span></span><span># Import necessary libraries</span>
<span>from</span> <span>matplotlib</span> <span>import</span> <span>pyplot</span> <span>as</span> <span>plt</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>pylab</span> <span>import</span> <span>imshow</span><span>,</span> <span>show</span>
<span>from</span> <span>timeit</span> <span>import</span> <span>default_timer</span> <span>as</span> <span>timer</span>

<span># Function to calculate the Mandelbrot set for a given point (x, y)</span>
<span>def</span> <span>mandel</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>max_iters</span><span>):</span>
    <span>c</span> <span>=</span> <span>complex</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span>
    <span>z</span> <span>=</span> <span>0.0</span><span>j</span>
    <span># Iterate to check if the point is in the Mandelbrot set</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>max_iters</span><span>):</span>
        <span>z</span> <span>=</span> <span>z</span><span>*</span><span>z</span> <span>+</span> <span>c</span>
        <span>if</span> <span>(</span><span>z</span><span>.</span><span>real</span><span>*</span><span>z</span><span>.</span><span>real</span> <span>+</span> <span>z</span><span>.</span><span>imag</span><span>*</span><span>z</span><span>.</span><span>imag</span><span>)</span> <span>&gt;=</span> <span>4</span><span>:</span>
            <span>return</span> <span>i</span>
    <span># If within the maximum iterations, consider it part of the set</span>
    <span>return</span> <span>max_iters</span>

<span># Function to create the Mandelbrot fractal within a specified region</span>
<span>def</span> <span>create_fractal</span><span>(</span><span>min_x</span><span>,</span> <span>max_x</span><span>,</span> <span>min_y</span><span>,</span> <span>max_y</span><span>,</span> <span>image</span><span>,</span> <span>iters</span><span>):</span>
    <span>height</span> <span>=</span> <span>image</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
    <span>width</span> <span>=</span> <span>image</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]</span>

    <span># Calculate pixel sizes based on the specified region</span>
    <span>pixel_size_x</span> <span>=</span> <span>(</span><span>max_x</span> <span>-</span> <span>min_x</span><span>)</span> <span>/</span> <span>width</span>
    <span>pixel_size_y</span> <span>=</span> <span>(</span><span>max_y</span> <span>-</span> <span>min_y</span><span>)</span> <span>/</span> <span>height</span>

    <span># Iterate over each pixel in the image and compute the Mandelbrot value</span>
    <span>for</span> <span>x</span> <span>in</span> <span>range</span><span>(</span><span>width</span><span>):</span>
        <span>real</span> <span>=</span> <span>min_x</span> <span>+</span> <span>x</span> <span>*</span> <span>pixel_size_x</span>
        <span>for</span> <span>y</span> <span>in</span> <span>range</span><span>(</span><span>height</span><span>):</span>
            <span>imag</span> <span>=</span> <span>min_y</span> <span>+</span> <span>y</span> <span>*</span> <span>pixel_size_y</span>
            <span>color</span> <span>=</span> <span>mandel</span><span>(</span><span>real</span><span>,</span> <span>imag</span><span>,</span> <span>iters</span><span>)</span>
            <span>image</span><span>[</span><span>y</span><span>,</span> <span>x</span><span>]</span> <span>=</span> <span>color</span>

<span># Create a blank image array for the Mandelbrot set</span>
<span>image</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>1024</span><span>,</span> <span>1536</span><span>),</span> <span>dtype</span><span>=</span><span>np</span><span>.</span><span>uint8</span><span>)</span>

<span># Record the start time for performance measurement</span>
<span>start</span> <span>=</span> <span>timer</span><span>()</span>

<span># Generate the Mandelbrot set within the specified region and iterations</span>
<span>create_fractal</span><span>(</span><span>-</span><span>2.0</span><span>,</span> <span>1.0</span><span>,</span> <span>-</span><span>1.0</span><span>,</span> <span>1.0</span><span>,</span> <span>image</span><span>,</span> <span>20</span><span>)</span>

<span># Calculate the time taken to create the Mandelbrot set</span>
<span>dt</span> <span>=</span> <span>timer</span><span>()</span> <span>-</span> <span>start</span>

<span># Print the time taken to generate the Mandelbrot set</span>
<span>print</span><span>(</span><span>"Mandelbrot created in </span><span>%f</span><span> s"</span> <span>%</span> <span>dt</span><span>)</span>

<span># Display the Mandelbrot set using matplotlib</span>
<span>imshow</span><span>(</span><span>image</span><span>)</span>
<span>show</span><span>()</span>
</pre></div>

<p>The above code produces the output in <code>4.07</code> seconds.</p>

<p><img alt="Mandelbrot without GPU" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/14af8fd709215ad96688a3364deefea9cfc2fa13497810587106cef04c5a413c.png"></p>

<ul>
<li>
<p>To make this faster, we can parallelize the code with GPU by using <a href="https://numba.pydata.org/?ref=journal.hexmos.com">Numba library</a>, Lets see how its done.</p>
</li>
<li>
<p>We will import Just-In-Time compilation, CUDA for GPU acceleration, and other utilities from numba</p>
</li>
</ul>

<div><pre><span></span><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>numba</span> <span>import</span> <span>jit</span><span>,</span> <span>cuda</span><span>,</span> <span>uint32</span><span>,</span> <span>f8</span><span>,</span> <span>uint8</span>
<span>from</span> <span>pylab</span> <span>import</span> <span>imshow</span><span>,</span> <span>show</span>
<span>from</span> <span>timeit</span> <span>import</span> <span>default_timer</span> <span>as</span> <span>timer</span>
</pre></div>

<ul>
<li>The <code>@jit</code> decorator signals Numba to perform <strong>Just-In-Time compilation</strong>, translating the Python code into machine code for improved execution speed.</li>
</ul>

<div><pre><span></span><span>@jit</span>
<span>def</span> <span>mandel</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>max_iters</span><span>):</span>
    <span>c</span> <span>=</span> <span>complex</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span>
    <span>z</span> <span>=</span> <span>0.0</span><span>j</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>max_iters</span><span>):</span>
        <span>z</span> <span>=</span> <span>z</span><span>*</span><span>z</span> <span>+</span> <span>c</span>
        <span>if</span> <span>(</span><span>z</span><span>.</span><span>real</span><span>*</span><span>z</span><span>.</span><span>real</span> <span>+</span> <span>z</span><span>.</span><span>imag</span><span>*</span><span>z</span><span>.</span><span>imag</span><span>)</span> <span>&gt;=</span> <span>4</span><span>:</span>
            <span>return</span> <span>i</span>

    <span>return</span> <span>max_iters</span>

<span>@jit</span>
<span>def</span> <span>create_fractal</span><span>(</span><span>min_x</span><span>,</span> <span>max_x</span><span>,</span> <span>min_y</span><span>,</span> <span>max_y</span><span>,</span> <span>image</span><span>,</span> <span>iters</span><span>):</span>
    <span>height</span> <span>=</span> <span>image</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
    <span>width</span> <span>=</span> <span>image</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]</span>

    <span>pixel_size_x</span> <span>=</span> <span>(</span><span>max_x</span> <span>-</span> <span>min_x</span><span>)</span> <span>/</span> <span>width</span>
    <span>pixel_size_y</span> <span>=</span> <span>(</span><span>max_y</span> <span>-</span> <span>min_y</span><span>)</span> <span>/</span> <span>height</span>

    <span>for</span> <span>x</span> <span>in</span> <span>range</span><span>(</span><span>width</span><span>):</span>
        <span>real</span> <span>=</span> <span>min_x</span> <span>+</span> <span>x</span> <span>*</span> <span>pixel_size_x</span>
        <span>for</span> <span>y</span> <span>in</span> <span>range</span><span>(</span><span>height</span><span>):</span>
            <span>imag</span> <span>=</span> <span>min_y</span> <span>+</span> <span>y</span> <span>*</span> <span>pixel_size_y</span>
            <span>color</span> <span>=</span> <span>mandel</span><span>(</span><span>real</span><span>,</span> <span>imag</span><span>,</span> <span>iters</span><span>)</span>
            <span>image</span><span>[</span><span>y</span><span>,</span> <span>x</span><span>]</span> <span>=</span> <span>color</span>
</pre></div>

<ul>
<li><code>mandel_gpu</code> is a GPU-compatible version of the mandel function created using cuda.jit. This allows the mandel logic to be offloaded to the GPU.</li>
<li>This is done by using <code>@cuda.jit</code> decorator along with specifying the data types (f8 for float, uint32 for unsigned integer) for the function arguments.</li>
<li>The <code>device=True</code> argument indicates that this function will run on the GPU.</li>
</ul>

<div><pre><span></span><span>mandel_gpu</span> <span>=</span> <span>cuda</span><span>.</span><span>jit</span><span>((</span><span>f8</span><span>,</span> <span>f8</span><span>,</span> <span>uint32</span><span>),</span> <span>device</span><span>=</span><span>True</span><span>)(</span><span>mandel</span><span>)</span>
</pre></div>

<ul>
<li>The mandel_kernel function is defined to be executed on the CUDA GPU. It is responsible for parallelizing the Mandelbrot set generation across GPU threads.</li>
</ul>

<div><pre><span></span><span>@cuda</span><span>.</span><span>jit</span><span>((</span><span>f8</span><span>,</span> <span>f8</span><span>,</span> <span>f8</span><span>,</span> <span>f8</span><span>,</span> <span>uint8</span><span>[:,:],</span> <span>uint32</span><span>))</span>
<span>def</span> <span>mandel_kernel</span><span>(</span><span>min_x</span><span>,</span> <span>max_x</span><span>,</span> <span>min_y</span><span>,</span> <span>max_y</span><span>,</span> <span>image</span><span>,</span> <span>iters</span><span>):</span>
    <span>height</span> <span>=</span> <span>image</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
    <span>width</span> <span>=</span> <span>image</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]</span>

    <span>pixel_size_x</span> <span>=</span> <span>(</span><span>max_x</span> <span>-</span> <span>min_x</span><span>)</span> <span>/</span> <span>width</span>
    <span>pixel_size_y</span> <span>=</span> <span>(</span><span>max_y</span> <span>-</span> <span>min_y</span><span>)</span> <span>/</span> <span>height</span>

    <span>startX</span><span>,</span> <span>startY</span> <span>=</span> <span>cuda</span><span>.</span><span>grid</span><span>(</span><span>2</span><span>)</span>
    <span>gridX</span> <span>=</span> <span>cuda</span><span>.</span><span>gridDim</span><span>.</span><span>x</span> <span>*</span> <span>cuda</span><span>.</span><span>blockDim</span><span>.</span><span>x</span>
    <span>gridY</span> <span>=</span> <span>cuda</span><span>.</span><span>gridDim</span><span>.</span><span>y</span> <span>*</span> <span>cuda</span><span>.</span><span>blockDim</span><span>.</span><span>y</span>

    <span>for</span> <span>x</span> <span>in</span> <span>range</span><span>(</span><span>startX</span><span>,</span> <span>width</span><span>,</span> <span>gridX</span><span>):</span>
        <span>real</span> <span>=</span> <span>min_x</span> <span>+</span> <span>x</span> <span>*</span> <span>pixel_size_x</span>
        <span>for</span> <span>y</span> <span>in</span> <span>range</span><span>(</span><span>startY</span><span>,</span> <span>height</span><span>,</span> <span>gridY</span><span>):</span>
            <span>imag</span> <span>=</span> <span>min_y</span> <span>+</span> <span>y</span> <span>*</span> <span>pixel_size_y</span>
            <span>image</span><span>[</span><span>y</span><span>,</span> <span>x</span><span>]</span> <span>=</span> <span>mandel_gpu</span><span>(</span><span>real</span><span>,</span> <span>imag</span><span>,</span> <span>iters</span><span>)</span>
</pre></div>

<p>The above code gets executed in <code>0.43 seconds</code>. Which is a lot faster the CPU Based code we had earlier.</p>

<p><img alt="Mandelbrot without GPU" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/cff7881a179a0a27ce03882b3f75a8c8d2a984fe308c84e87bd0446c8871452d.png"></p>

<p>Here's the <a href="https://github.com/RijulTP/GPUToolkit/tree/main/mandelbrot?ref=journal.hexmos.com">full code</a> for your reference.</p>

<h4 id="training-a-cat-vs-dog-neural-network-using-the-gpu">Training a Cat VS Dog Neural Network Using the GPU</h4>

<p>One of the hot topics we see nowadays is how GPUs are getting used in AI, So to demonstrate that we will be creating a <strong>neural network</strong> to differentiate between cats and dogs.</p>

<p><strong>Prerequisites</strong></p>

<ul>
<li>CUDA</li>
<li>
<p>Tensorflow -&gt; Can be installed via
  <code>pip install tensorflow[and-cuda]</code></p>
</li>
<li>
<p>We will use a data set of cats and dogs from <a href="https://www.kaggle.com/competitions/dogs-vs-cats/overview?ref=journal.hexmos.com">kaggle</a></p>
</li>
<li>
<p>Once you have downloaded it, Unzip them, organize the pictures of cats and dogs in the training folder into different subfolders, Like so.</p>
</li>
</ul>

<p><img alt="CNN File Structure" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/47b0cc05fa99283bd8dcd39a801809bdf5521a285df558c1436460ddf4d1ebdd.png"></p>

<p>This is the code we will use for training and using the Cat vs Dog Model.</p>

<p>The below code uses a convolutional neural network, you can <a href="https://www.analyticsvidhya.com/blog/2021/06/beginner-friendly-project-cat-and-dog-classification-using-cnn/?ref=journal.hexmos.com">read more details about it</a></p>

<p><strong>Importing Libraries</strong></p>

<ul>
<li>pandas and numpy for data manipulation.</li>
<li>Sequential for creating a linear stack of layers in the neural network.</li>
<li>Convolution2D, MaxPooling2D, Dense, and Flatten are layers used in building the Convolutional Neural Network (CNN).</li>
<li>ImageDataGenerator for real-time data augmentation during training.</li>
</ul>

<div><pre><span></span><span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>keras.models</span> <span>import</span> <span>Sequential</span>
<span>from</span> <span>keras.layers</span> <span>import</span> <span>Convolution2D</span><span>,</span> <span>MaxPooling2D</span><span>,</span> <span>Dense</span><span>,</span> <span>Flatten</span>
<span>from</span> <span>keras.preprocessing.image</span> <span>import</span> <span>ImageDataGenerator</span>
</pre></div>

<p><strong>Initializing the Convolutional Neural Network</strong></p>

<div><pre><span></span><span>classifier</span> <span>=</span> <span>Sequential</span><span>()</span>
</pre></div>

<p><strong>Loading the data for training</strong></p>

<div><pre><span></span><span>train_datagen</span> <span>=</span> <span>ImageDataGenerator</span><span>(</span>
    <span>rescale</span><span>=</span><span>1.</span><span>/</span><span>255</span><span>,</span>
    <span>shear_range</span><span>=</span><span>0.2</span><span>,</span>
    <span>zoom_range</span><span>=</span><span>0.2</span><span>,</span>
    <span>horizontal_flip</span><span>=</span><span>True</span>
<span>)</span>
<span>test_datagen</span> <span>=</span> <span>ImageDataGenerator</span><span>(</span><span>rescale</span><span>=</span><span>1.</span><span>/</span><span>255</span><span>)</span>

<span>training_set</span> <span>=</span> <span>train_datagen</span><span>.</span><span>flow_from_directory</span><span>(</span>
    <span>'./training_set'</span><span>,</span>
    <span>target_size</span><span>=</span><span>(</span><span>64</span><span>,</span> <span>64</span><span>),</span>
    <span>batch_size</span><span>=</span><span>32</span><span>,</span>
    <span>class_mode</span><span>=</span><span>'binary'</span>
<span>)</span>

<span>test_set</span> <span>=</span> <span>test_datagen</span><span>.</span><span>flow_from_directory</span><span>(</span>
    <span>'./test_set'</span><span>,</span>
    <span>target_size</span><span>=</span><span>(</span><span>64</span><span>,</span> <span>64</span><span>),</span>
    <span>batch_size</span><span>=</span><span>32</span><span>,</span>
    <span>class_mode</span><span>=</span><span>'binary'</span>
<span>)</span>
</pre></div>

<p><strong>Building the CNN Architecture</strong></p>

<div><pre><span></span><span>classifier</span><span>.</span><span>add</span><span>(</span><span>Convolution2D</span><span>(</span><span>32</span><span>,</span> <span>3</span><span>,</span> <span>3</span><span>,</span> <span>input_shape</span><span>=</span><span>(</span><span>64</span><span>,</span> <span>64</span><span>,</span> <span>3</span><span>),</span> <span>activation</span><span>=</span><span>'relu'</span><span>))</span>
<span>classifier</span><span>.</span><span>add</span><span>(</span><span>MaxPooling2D</span><span>(</span><span>pool_size</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>)))</span>
<span>classifier</span><span>.</span><span>add</span><span>(</span><span>Flatten</span><span>())</span>
<span>classifier</span><span>.</span><span>add</span><span>(</span><span>Dense</span><span>(</span><span>units</span><span>=</span><span>128</span><span>,</span> <span>activation</span><span>=</span><span>'relu'</span><span>))</span>
<span>classifier</span><span>.</span><span>add</span><span>(</span><span>Dense</span><span>(</span><span>units</span><span>=</span><span>1</span><span>,</span> <span>activation</span><span>=</span><span>'sigmoid'</span><span>))</span>
</pre></div>

<p><strong>Compiling the model</strong></p>

<div><pre><span></span><span>classifier</span><span>.</span><span>compile</span><span>(</span><span>optimizer</span><span>=</span><span>'adam'</span><span>,</span> <span>loss</span><span>=</span><span>'binary_crossentropy'</span><span>,</span> <span>metrics</span><span>=</span><span>[</span><span>'accuracy'</span><span>])</span>
</pre></div>

<p><strong>Training the model</strong></p>

<div><pre><span></span><span>classifier</span><span>.</span><span>fit</span><span>(</span><span>training_set</span><span>,</span> <span>epochs</span><span>=</span><span>25</span><span>,</span> <span>validation_data</span><span>=</span><span>test_set</span><span>,</span> <span>validation_steps</span><span>=</span><span>2000</span><span>)</span>
<span>classifier</span><span>.</span><span>save</span><span>(</span><span>'trained_model.h5'</span><span>)</span>
</pre></div>

<p>Once we have trained the model, The model is stored in a .h5 file using <code>classifier.save</code></p>

<p>In the below code, we will use this <code>trained_model.h5</code> file to recognize cats and dogs.</p>

<div><pre><span></span><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>keras.models</span> <span>import</span> <span>load_model</span>
<span>import</span> <span>keras.utils</span> <span>as</span> <span>image</span>

<span>def</span> <span>predict_image</span><span>(</span><span>imagepath</span><span>,</span> <span>classifier</span><span>):</span>
    <span>predict</span> <span>=</span> <span>image</span><span>.</span><span>load_img</span><span>(</span><span>imagepath</span><span>,</span> <span>target_size</span><span>=</span><span>(</span><span>64</span><span>,</span> <span>64</span><span>))</span>
    <span>predict_modified</span> <span>=</span> <span>image</span><span>.</span><span>img_to_array</span><span>(</span><span>predict</span><span>)</span>
    <span>predict_modified</span> <span>=</span> <span>predict_modified</span> <span>/</span> <span>255</span>
    <span>predict_modified</span> <span>=</span> <span>np</span><span>.</span><span>expand_dims</span><span>(</span><span>predict_modified</span><span>,</span> <span>axis</span><span>=</span><span>0</span><span>)</span>
    <span>result</span> <span>=</span> <span>classifier</span><span>.</span><span>predict</span><span>(</span><span>predict_modified</span><span>)</span>

    <span>if</span> <span>result</span><span>[</span><span>0</span><span>][</span><span>0</span><span>]</span> <span>&gt;=</span> <span>0.5</span><span>:</span>
        <span>prediction</span> <span>=</span> <span>'dog'</span>
        <span>probability</span> <span>=</span> <span>result</span><span>[</span><span>0</span><span>][</span><span>0</span><span>]</span>
        <span>print</span><span>(</span><span>"Probability = "</span> <span>+</span> <span>str</span><span>(</span><span>probability</span><span>))</span>
        <span>print</span><span>(</span><span>"Prediction = "</span> <span>+</span> <span>prediction</span><span>)</span>
    <span>else</span><span>:</span>
        <span>prediction</span> <span>=</span> <span>'cat'</span>
        <span>probability</span> <span>=</span> <span>1</span> <span>-</span> <span>result</span><span>[</span><span>0</span><span>][</span><span>0</span><span>]</span>
        <span>print</span><span>(</span><span>"Probability = "</span> <span>+</span> <span>str</span><span>(</span><span>probability</span><span>))</span>
        <span>print</span><span>(</span><span>"Prediction = "</span> <span>+</span> <span>prediction</span><span>)</span>

<span># Load the trained model</span>
<span>loaded_classifier</span> <span>=</span> <span>load_model</span><span>(</span><span>'trained_model.h5'</span><span>)</span>

<span># Example usage</span>
<span>dog_image</span> <span>=</span> <span>"dog.jpg"</span>
<span>predict_image</span><span>(</span><span>dog_image</span><span>,</span> <span>loaded_classifier</span><span>)</span>

<span>cat_image</span> <span>=</span> <span>"cat.jpg"</span>
<span>predict_image</span><span>(</span><span>cat_image</span><span>,</span> <span>loaded_classifier</span><span>)</span>
</pre></div>

<p>Let's see the output
<img alt="Alt text" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/d546542af4055bc7163bc49055d5a623a44ad0cc3022d02d0aaa791b9a83d75b.png"></p>

<p>Here's the <a href="https://github.com/RijulTP/GPUToolkit/tree/main/neural-network?ref=journal.hexmos.com">full code</a> for your reference</p>

<h3 id="conclusion">Conclusion</h3>

<p>In the upcoming AI age, GPUs are not a thing to be ignored, We should be more aware of its capabilities.</p>

<p>As we transition from traditional <strong>sequential algorithms</strong> to increasingly prevalent <strong>parallelized algorithms</strong>, GPUs emerge as indispensable tools that empower the acceleration of complex computations. The parallel processing prowess of GPUs is particularly advantageous in handling the massive datasets and intricate neural network architectures inherent to artificial intelligence and machine learning tasks.</p>

<p>Furthermore, the role of GPUs extends beyond traditional machine learning domains, finding applications in scientific research, simulations, and data-intensive tasks. The parallel processing capabilities of GPUs have proven instrumental in addressing challenges across diverse fields, ranging from drug discovery and climate modelling to financial simulations.</p>

<h3 id="reference">Reference</h3>

<ul>
<li><a href="https://noahgift.github.io/cloud-data-analysis-at-scale/topics/end-of-moores-law.html?ref=journal.hexmos.com">Using Numba for mandelbrot generation</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2021/06/beginner-friendly-project-cat-and-dog-classification-using-cnn/?ref=journal.hexmos.com">Using Convolutional Neural Networks</a></li>
</ul>

<p><a href="https://twitter.com/HexmosTech?ref=journal.hexmos.com"><img alt="Twitter" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/e706462dc4b473f96955889657e3893beca1e6cba15daa89dca171d732709b87.png"></a></p>

<p><a href="https://hexmos.com/?ref=journal.hexmos.com"><img alt="Hexmos" src="https://journal-wa6509js.s3.ap-south-1.amazonaws.com/62b98356b4386f9c312f586af9c3606f9bd67f7116b4031c1bf2f5fb0fc73e0a.png"></a></p>

<p><a href="https://news.ycombinator.com/item?id=38240421&amp;ref=journal.hexmos.com">Hackernews post</a></p>

<p><a href="https://www.linkedin.com/feed/update/urn:li:activity:7129475708971618305?ref=journal.hexmos.com">Linkedin post</a></p><!--kg-card-end: html-->
    </section>


        
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple might have to pay that €13B EU tax bill after all (110 pts)]]></title>
            <link>https://www.theregister.com/2023/11/11/apple_tax_bill_hiring/</link>
            <guid>38240363</guid>
            <pubDate>Sun, 12 Nov 2023 14:31:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/11/11/apple_tax_bill_hiring/">https://www.theregister.com/2023/11/11/apple_tax_bill_hiring/</a>, See on <a href="https://news.ycombinator.com/item?id=38240363">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Apple managed to escape a whopping €13 billion ($13.9 billion) tax bill in the European Union a few years ago, though now the advocate general of the Court of Justice of the EU (CJEU) is asking judges to take another look.&nbsp;</p>
<p><a href="https://curia.europa.eu/jcms/upload/docs/application/pdf/2023-11/cp230171en.pdf" rel="nofollow">According</a> [PDF] to the CJEU's Advocate General Giovanni Pitruzzella, a <a href="https://www.theregister.com/2020/07/15/apple_13bn_irish_tax_ruling_overturned/">2020 ruling</a> that absolved Apple of the expense should be set aside due to "a series of errors in law" when it made the ruling.</p>
<p>"The General Court failed to assess correctly the substance and consequences of certain methodological errors," the AG's office said.</p>

    

<p>For those having trouble recalling the nature of the disagreement, it reaches back a few years, beginning with a decision in 2018 that told Apple to hand over the aforementioned €13 billion to Irish tax officials after the European Commission <a href="https://www.theregister.com/2018/09/19/apple_alleged_state_aid_ireland/">decided</a> Apple and Irish authorities had together broken state aid rules.</p>

        


        

<p>Ireland, where Apple has its European headquarters, is known for its favorable tax rules that have attracted a number of large US-based tech companies to set up shop on the Emerald Isle. Apple, EU officials report, split itself into two entities – Apple Sales International (ASI) and Apple Operations International (AOE) – after obtaining permission from Irish tax authorities.&nbsp;</p>
<p>Those officials gave that permission in two rulings issued in 1991 and 2007, the AG's office said. In 2016, the European Commission figured those two rulings allowed Apple to exclude profits derived from intellectual property licenses held by ASI and AOE. That meant Apple didn't have to pay as much tax in Europe as one might expect, and the EC was OK with that at the time.</p>

        

<p>Then, as we said, in 2018, the commission had a change of heart, felt rules had been broken, and wanted that €13 billion in tax from Apple.</p>
<p>Fast-forward to 2020, and the General Court of the European Union decided Apple didn't actually need to pay the tax due to the arrangement the iPhone maker had in Ireland seemingly being above board. We got a hint of how the EC planned to appeal against that general court decision, and extract billions from Apple, <a href="https://www.theregister.com/2021/02/01/european_commission_outlines_its_appeal/">in 2021</a>, and the filing by Pitruzzella's office makes largely the same argument.</p>
<p>Whether Apple will end up having to cough up the cash again (a considerable amount - the iGiant's <a href="https://www.apple.com/newsroom/2023/11/apple-reports-fourth-quarter-results/" rel="nofollow">calendar Q3 profit</a> this year amounted to $23 billion) is unknown. As the AG's office notes, its opinions aren't binding on the CJEU, and will require the court to review the case and issue a new decision.&nbsp;</p>
<h3>US hiring discrimination charges mean pocket-change payout for Apple</h3>
<p>The iMaker will also cough up $25 million (with an M) the US Department of Justice <a href="https://www.justice.gov/opa/pr/justice-department-secures-25-million-landmark-agreement-apple-resolve-employment" rel="nofollow">announced</a> this week, settling claims the tech titan "illegally discriminated in hiring and recruitment against&nbsp;US citizens and certain non-US citizens whose permission to live in and work in the United States does not expire."</p>
<p>According to the DoJ, Apple violated the Immigration and Nationality Act (<a href="https://www.uscis.gov/laws-and-policy/legislation/immigration-and-nationality-act" rel="nofollow">INA</a>) when recruiting for jobs that fell under the permanent labor certification program (<a href="https://www.dol.gov/agencies/eta/foreign-labor/programs/permanent" rel="nofollow">PERM</a>), a US Department of Labor scheme that allows employers to sponsor workers for permanent residence status, provided program requirements are met. Among those requirements are rules that employers can't illegally discriminate in hiring or recruitment based on citizenship or immigration status.&nbsp;</p>

        

<p>According to the DoJ, that's just what Apple did.&nbsp;</p>
<ul>

<li><a href="https://www.theregister.com/2023/11/02/apple_batterygate_uk/">Batterygate bound for Blighty as UK court approves billion-dollar Apple compensation case</a></li>

<li><a href="https://www.theregister.com/2023/10/06/apple_sales_wages_settlement/">Apple pays $500K to make sales bods' complaint about wage theft go away</a></li>

<li><a href="https://www.theregister.com/2023/10/11/microsoft_irs_tax_bill/">Hell no, we won't pay, says Microsoft as Uncle Sam sends $29B bill for back taxes</a></li>

<li><a href="https://www.theregister.com/2023/01/28/apple_sued_privacy/">Apple sued for promising privacy, failing at it</a></li>
</ul>
<p>"The department's investigation found that Apple did not advertise positions Apple sought to fill through the PERM program on its external job website … [and] required all PERM position applicants to mail paper applications," the DoJ said.&nbsp;</p>
<p>"These less effective recruitment procedures nearly always resulted in few or no applications to PERM positions from applicants whose permission to work does not expire."&nbsp;</p>
<p>Basically, if a US company wants to sponsor an employee's green card, so that the staffer can get permanent residency in America, the biz usually (but not always) has to apply for a PERM certificate from the Dept of Labor so that the foreign worker's immigration process can be completed. To get that PERM cert, the business has to demonstrate that no US citizen or someone already with a green card is available and qualified to do that employee's role in America and thus the foreigner needs permanent status in the country to fill the position.</p>
<p>By demonstrate, we mean that the employer is expected to re-advertise that foreign staffer's job widely so that US persons have a chance to come forward to show they ought to have it instead; if you put barriers up, such as only accepting mailed applications or hiding the job ads in little-seen corners of the internet, it's not a fair situation, and that makes Uncle Sam unhappy. And the government is upset at the iPhone giant doing stuff like that, allegedly.</p>
<p>We reached out to Apple for comment, and haven't heard back. Cook and Co did <a href="https://www.wsj.com/us-news/law/apple-to-pay-up-to-25-million-to-settle-u-s-discrimination-charges-3f81fad4?st=df2t03ajuss4fo1&amp;reflink=article_copyURL_share" rel="nofollow">tell</a> the Wall Street Journal that its violation of government standards was an accident.&nbsp;</p>
<p>"When we realized we had unintentionally not been following the DOJ standard, we agreed to a settlement addressing their concerns," an Apple spokesperson said. "We have implemented a robust remediation plan to comply with the requirements of various government agencies as we continue to hire American workers and grow in the US."</p>
<p>Apple's payout will be split between $6.75 million in civil penalties and an $18.25 million back pay fund for discrimination victims. As noted above, Apple made $23 billion in profit in one quarter alone. That $25 million fine amounts to a tenth of one percent of its Q3 net income. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub Actions Are a Problem (124 pts)]]></title>
            <link>https://felix-knorr.net/posts/2023-11-11-github-actions.html</link>
            <guid>38240278</guid>
            <pubDate>Sun, 12 Nov 2023 14:22:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://felix-knorr.net/posts/2023-11-11-github-actions.html">https://felix-knorr.net/posts/2023-11-11-github-actions.html</a>, See on <a href="https://news.ycombinator.com/item?id=38240278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>The Problem</h2>
<p>You push (or better, merge) to master, and the deployment happens automagically. No
further work required. That's the promise of GitHub-Actions (GHA), and probably
every other CI service provider.</p>
<p>However, things are not as shiny as they seem at first glance. There are many
problems with the approach that GHA and co. use. Some are addressable by more
disciplined usage behavior<sup id="fnref:1"><a href="#fn:1">1</a></sup>, but others are not.</p>
<!--more-->

<p>My first and foremost problem with them is: you cannot run them locally. There
are some projects, that attempt to enable you to do that, e.g. 
<a href="https://github.com/nektos/act">act</a>, but they are very limited. And that is because
of my second-biggest issue with GHA: the runner is closed source.</p>
<p>Hosting a git repo is hardly more than providing a file system and SSH access.
The actual mechanism they use to keep you on their platform is the CI-pipelines
(and maybe the issue system and wiki, but less so). Half a year ago, I started
porting my teams Azure-Devops pipelines to GHA, investing ~1 day per week. AND I
AM STILL NOT DONE.</p>
<p>The fact that you have to push a commit to the repo to test a change, and then 
wait for a runner delays everything indefinitely, and makes iterating really
painful.</p>
<p>Additionally, writing pipelines in YAML is just painful. It's super verbose,
and you have a lot of "code" duplication. Quite soon in the process I invested
3 days to write a small DSL that compiles to GHA<sup id="fnref:2"><a href="#fn:2">2</a></sup>. This already took care
of the code duplication, and I integrated <a href="https://github.com/rhysd/actionlint">actionlint</a> to reduce the number of pushes I'd need. Sadly, actionlint
catches a similar percentage of your bugs like a C-compiler (which is not very
high). Originally, that DSL hat two backends: One to compile to GHA, and one
to run stuff locally. The second one is not in use anymore. Why? Because there
are tons of actions<sup id="fnref:3"><a href="#fn:3">3</a></sup> that you want to use, e.g. for terraform. To use those
locally, I'd have to reimplement the complete GHA runner, or at least, locally
reimplement all the actions I want to use.</p>
<p>Many of those actions are written by 3rd parties, not by GitHub/Microsoft,
but it benefits them a lot by strengthening their position against their
competition. It's a network effect. And it's bad for everyone except MS. I'm
only somewhat knowledgeable in Azure-Devops and GitHub actions (much to my own
dismay), but I assume that more or less every CI provider does its own thing,
and there is a lot of duplicated efforts. </p>
<h2>An attempt at a solution</h2>
<p>The base idea is the same for every CI provider: You define a "workflow" that
runs some scripts. Some of which run in parallel, some of which require others
to finish first. They need access to a git repository, and they might have
to interact with it. Defining a language to run scripts in parallel with some
constraints is actually quite easy if you just pass the script to an existing
interpreter and let the language mainly deal with the constraints around order
and parallelism (and code reuse). However, if this is then just a new solution,
little is gained. So it would be necessary to have different "generators" that
generate small wrappers for your CI-provider of choice that call the actual
definitions, and maybe to call a subset of functionality that is shared by
more or less everyone (e.g. creating issues, merging a branch, etc.). If people
who currently write "plugins" for different CI providers instead focus on this
hypothetical language, we would end up with an ecosystem of CI tools that can run
everywhere, including your PC. I'm thinking of something like this:</p>
<p><img alt="Graph of the hypothetical System" src="https://felix-knorr.net/assets/img/common_action_runtime.svg"></p>
<p>And yes, I'm aware of <a href="https://xkcd.com/927/">XKCD #927</a>, but It's actually not the same
as a CI-Provider, and I'm not aware of a similar project. Corrections are welcome.
And while I'd love to work on this, I don't see myself having enough free time in the
foreseeable future to do this. So I hope, someone feels inspired.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Casino-like apps have drained people of millions (205 pts)]]></title>
            <link>https://www.nbcnews.com/tech/tech-news/addicted-losing-how-casino-apps-have-drained-people-millions-n1239604</link>
            <guid>38240212</guid>
            <pubDate>Sun, 12 Nov 2023 14:13:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/tech/tech-news/addicted-losing-how-casino-apps-have-drained-people-millions-n1239604">https://www.nbcnews.com/tech/tech-news/addicted-losing-how-casino-apps-have-drained-people-millions-n1239604</a>, See on <a href="https://news.ycombinator.com/item?id=38240212">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Shellz, 37, a nurse from Houston, spends at least two hours a day with her husband playing a casino-style smartphone game called Jackpot Magic.</p><p>The app offers a variety of typical casino games to play, including their favorite, called Reel Rivals, a game in which players accrue points by playing a virtual slot machine. As in a real casino, players exchange money for coins to bet.</p><p>Unlike in a real casino, there is no way to win money back or earn a payout on coins.</p><p>But that has not stopped Shellz and her husband from spending about $150,000 in the game in just two years. She asked to use her in-game username so her family does not find out how much money they have spent on the game.</p><p>"We lie in bed next to each other, we have two tablets, two phones and a computer and all these apps spinning Reel Rivals at the same time," she said. "We normalize it with each other."</p><p>Jackpot Magic is an app made by Big Fish Games of Seattle, one of the leaders in an industry of "free-to-play" social games into which some people have plowed thousands of dollars. Big Fish Games also operates a similar app, Big Fish Casino. Both are labeled as video games, which allows the company and others like it to skirt the tightly regulated U.S. gambling market.</p><p>But unlike the gambling market, apps like Jackpot Magic and Big Fish Casino are under little oversight to determine whether they are fair or whether their business practices are predatory.</p><p>NBC News spoke to 21 people, including Shellz and her husband, who said they were hooked on the casino-style games and had spent significant sums of money. They described feelings of helplessness and wanting to quit but found themselves addicted to the games and tempted by the company's aggressive marketing tactics.</p><p>Most of the 21 players wished to remain anonymous, as they were ashamed of their addictions and did not want their loved ones to find out about their behavior.</p><p>A 42-year-old Pennsylvania woman said she felt saddened that she spent $40,000 on Big Fish Casino while working as an addiction counselor.</p><p>"The whole time I was working as an addiction counselor, I was addicted to gambling and with no hope of winning any money back," she said.</p><p>Big Fish Games did not make anyone available for an interview, nor did the company respond to detailed questions. The company has said in previous <a href="https://www.documentcloud.org/documents/6878099-206-2.html#document/p4/a561949">court filings</a> that only a fraction of the game's players actually spend money.</p><p>In a response to NBC News' inquiries, the company issued a <a href="https://www.documentcloud.org/documents/6896112-BF.html">statement</a> saying its games are not gambling and should not be regulated as such.</p><p>"These games are not gambling because, among other reasons, they offer no opportunity for players to win money or anything of value," the statement said in part.</p><p>"Our games are offered for free purely for entertainment, with an opportunity for customers to spend money within the game to enhance their gameplay experience," it said. "The vast majority of Big Fish Casino and Jackpot Magic Slots customers play without ever paying any money. No court has yet considered all of the facts relating to how these games operate."</p><p>Players have had some recourse in recent months thanks to successful lawsuits.</p><p>After a long legal battle, 2 million players, including Shellz and her husband, will be eligible to get a small part of their losses back — about 20 percent for those who lost $10,000 to $100,000.</p><p>The money will come in a $155 million class-action settlement, announced at the end of July, that will cover two major lawsuits filed against Big Fish Games; its former owner, Churchill Downs; and its current Australian parent company, Aristocrat Leisure, alleging that they were operating "unlawful gambling devices."</p><p>The preliminary agreement was recently approved by a federal judge in Tacoma, Washington. Churchill Downs and Aristocrat Leisure both declined to comment on the settlement. Aristocrat Leisure released a public <a href="https://www.documentcloud.org/documents/7205179-21354134-b761-4dd7-a3a2-266d8a52056c-1.html">statement</a> in May outlining the general contours of the settlement, but it has not said anything further.</p><p><em><strong><a href="https://smart.link/5d5ad16083f88" target="_blank">Download the NBC News app for breaking news and politics</a></strong></em></p><p>While Big Fish Games admits no wrongdoing, it has <a href="https://www.documentcloud.org/documents/7006464-171.html#document/p8/a573737">agreed</a> to implement "addiction-related resources" and a "self-exclusion policy" that would allow players who feel out of control to opt in to be blacklisted from playing the game.</p><p>Big Fish Games also declined to comment on the settlement.</p><p>While some players are happy to recoup some of their losses, gambling addiction experts and some lawmakers say it does not go far enough to help those whose lives have spiraled out of control after they got hooked on social casino games. They call for further regulation of the industry.</p><p>"What we would have welcomed as part of this settlement as a wake-up call for the industry is a change in practices," said Keith Whyte, the executive director of the <a href="https://www.ncpgambling.org/">National Council on Problem Gambling</a>.</p><p>"I think their model is so lucrative and in some ways so aggressive that they're doubling down, and it's going to do a lot more harm. I think it's going to eventually be reined in, but it appears they are prioritizing short-term profit over long-term sustainability and responsibility," Whyte said.</p><h2><strong>The game</strong></h2><p>Joann, 46, who lives in southwest Florida, said she began playing Big Fish Casino about eight years ago.</p><p>She estimates that she has spent $100,000 on the game.</p><p>"You know what I tell people? It's a cult, and they suck you in, and once you're in you can't get out," said Joann, who asked to use only her middle name. "You want to play, and you want to spin."</p><p>One of the named plaintiffs in the settlement is Crystal Fair of Texas, who said in a sworn declaration that she has <a href="https://www.documentcloud.org/documents/6782706-128-8.html#document/p3/a551787">spent $500,000</a> and described herself as being "addicted" to Big Fish Casino, playing it sometimes "nearly 24 hours a day."</p><p>"I have considered walking away for good but then I think of all my time and more importantly all my money and it's hard to walk away," she wrote. "That's how I know I'm addicted."</p><p>She concluded: "But if I could go back to the point in time when I installed Big Fish Casino, I'd never ever have done it."</p><p>Several people said they felt the apps were engineered to keep them spending money in a variety of ways, including tiered clubs for players who spend significant amounts of money and free chips for people who try to quit.</p><p>Suzie Kelly of Dallas previously told <a href="https://www.revealnews.org/episodes/harpooned-by-facebook/">Reveal News</a> how she spent about $400,000 on the game. She took out a home equity loan and used the money she inherited when her mother died to fund her habit.</p><p>When she tried to cancel her account on several occasions, Kelly said, a "VIP representative" would call her and offer her free chips so she would continue playing.</p><p>The Big Fish apps in some ways are similar to many other apps that offer casino games that can be played on smartphones.</p><p>The home screen of Big Fish Casino, known as the "lobby," offers players a chance to try various types of casino-style games, including roulette, blackjack, Texas Hold 'em, Video Poker and the most prominently featured game: slots.</p><p>Shortly after they install the app, players are encouraged to join clubs — Big Fish Casino even offers a "one time join bonus" of 50,000 chips for joining a club. Once they are in a club, players can use a chat feature to strike up conversations with their counterparts and develop friendships. While anyone can create a club, the real action is in the invitation-only ranked clubs that compete against one another.</p><p>Winning more chips and playing at higher stakes unlock new features, like high-roller rooms. There is also a <a href="https://www.bigfishgames.com/game/jackpotmagicslots/help/articles/115000134954-What-is-the-VIP-Rewards-Program-/">system</a> of "levels" and "tiers" to unlock as players spend and win more. Higher tiers come with larger potential winnings and bigger bets, which makes it easier to lose chips faster.</p><p>Players who lose but want to continue playing in high-roller rooms can do so by either rebuilding their digital fortunes through hours of gameplay or taking a shorter route: buying more chips.</p><p>Most people who play the Big Fish games do not end up spending real money. Less than 10 percent of users have ever bought virtual items while playing the games, <a href="https://www.documentcloud.org/documents/6878099-206-2.html#document/p4/a561949">according to an October court filing</a>.</p><p>But that 10 percent has translated into a lucrative business. According to data provided by Apptopia, an app analytics company, Big Fish Games took in an estimated $139.3 million from Big Fish Casino and Jackpot Magic players from February 2019 through July 2020.</p><p>The app's tier system, along with its social functions, can be a powerful trap for some players. Joann said she continues to play, as Big Fish Casino grants her a set of free chips (known as a "boost") every day. Even so, she said, she spends at least $600 a month, largely to maintain her status within her club.</p><p>"I want to quit the club, and I want to stop, but I have friends," she said.</p><h2>No recourse</h2><p>Big Fish Games is one of the clearest examples of the <a href="https://arstechnica.com/gaming/2018/09/multinational-regulators-join-together-to-fight-gambling-in-video-games/">convergence</a> of the small-time harmless fun of video games and the rapidly expanding world of real-money gambling.</p><p>While many video games have added premium features in recent years, including <a href="https://www.nbcnews.com/tech/tech-news/loot-boxes-gambling-video-games-ftc-look-it-n941256">loot boxes</a> — a mechanism to pay small, fixed fees for chances to win in-game prizes that has attracted the ire of <a href="https://www.nbcnews.com/tech/video-games/video-game-loot-boxes-face-final-boss-missouri-senator-n1003471">some lawmakers</a> — no other type of game appears to allow players to lose so much credit so quickly and be constantly encouraged to spend more.</p><p>But for the time being, there does not seem to be anything stopping these gambling-style smartphone games from continuing. No federal legislation would halt this model, nor would any state-level legislation mitigate losses created by this type of game. Washington state legislators considered a bill that would have formally defined games like Big Fish Casino to not be considered gambling, but the bill did not pass.</p><p>Some players of Big Fish games have filed class-action lawsuits against the company, arguing that its games should be regulated just like traditional gambling, which is unlikely to happen any time soon.</p><p>Conventional slot machines, for example, are subject to rigorous outside testing to ensure that the odds are consistent for all players. In Nevada, there are rules about how many slot machines can be <a href="https://www.documentcloud.org/documents/6922257-All-Regulations-as-of-01-30-20.html#document/p46/a565021">placed in liquor stores</a>, among hundreds of pages of regulations.</p><p>In Washington state — where Big Fish Games is located — slot machines are <a href="https://www.wsgc.wa.gov/regulation-enforcement/illegal-activities">banned</a> outright. But Washingtonians can download a smartphone game that offers would-be gamblers the chance to spend money on an experience nearly identical to that of an in-person slot machine, only without any chance of actually winning money.</p><p>Still, the recent legal victories are a welcome bit of help for some players, particularly because many have lost their jobs because of the COVID-19 pandemic.</p><p>Neva Barker, 58, a retiree in Portland, Oregon, estimated that she had spent $80,000 on Big Fish Casino and said she was thrilled to hear that she likely would get some of her money back.</p><p>It is particularly needed now, because, Barker says, she lost some of her income to coronavirus-related cutbacks.</p><p>"This has been going through so many ups and downs," she said. "I thought it was a myth that it was ever going to happen. That would be life-changing for me at this point."</p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/cyrus-farivar-ncpn973741"><picture data-testid="picture"><source srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:eco,dpr_2.0/newscms/2019_11/2784541/cyrus-farivar-1500.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_11/2784541/cyrus-farivar-1500.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_11/2784541/cyrus-farivar-1500.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/cyrus-farivar-ncpn973741">Cyrus Farivar</a></span><span><a href="https://twitter.com/cfarivar" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:cyrus.farivar@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Cyrus Farivar is a reporter on the tech investigations unit of NBC News in San Francisco.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stanford Data Science Handbook (161 pts)]]></title>
            <link>https://datascience.stanford.edu/programs/stanford-data-science-scholars-program/data-science-handbook</link>
            <guid>38239728</guid>
            <pubDate>Sun, 12 Nov 2023 13:04:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://datascience.stanford.edu/programs/stanford-data-science-scholars-program/data-science-handbook">https://datascience.stanford.edu/programs/stanford-data-science-scholars-program/data-science-handbook</a>, See on <a href="https://news.ycombinator.com/item?id=38239728">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-content">

      <div>
            
  <h2 id="menu-blockmain-menu">Main navigation</h2>
  

        
<p><a href="#main-content">Skip to main content</a></p>

  </div>
  
      <div><h2>Open, rigorous and reproducible research: A practitioner’s handbook</h2>

</div>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Paid $1k for an iPhone, but Apple Still Controls It (142 pts)]]></title>
            <link>https://www.nytimes.com/2023/11/12/technology/iphone-repair-apple-control.html</link>
            <guid>38239640</guid>
            <pubDate>Sun, 12 Nov 2023 12:45:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2023/11/12/technology/iphone-repair-apple-control.html">https://www.nytimes.com/2023/11/12/technology/iphone-repair-apple-control.html</a>, See on <a href="https://news.ycombinator.com/item?id=38239640">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2023/11/12/technology/iphone-repair-apple-control.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Speaker Support in Asahi Linux (166 pts)]]></title>
            <link>https://github.com/AsahiLinux/docs/wiki/SW:Speakers</link>
            <guid>38239503</guid>
            <pubDate>Sun, 12 Nov 2023 12:20:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/AsahiLinux/docs/wiki/SW:Speakers">https://github.com/AsahiLinux/docs/wiki/SW:Speakers</a>, See on <a href="https://news.ycombinator.com/item?id=38239503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <h2 id="user-content-speaker-support-in-asahi-linux"><a href="#speaker-support-in-asahi-linux">Speaker support in Asahi Linux</a></h2>
<p>We are progressively enabling speaker support on Asahi Fedora Remix.</p>
<p>Currently supported models:</p>
<ul>
<li>M1 MacBook Air 13" (J313)</li>
</ul>
<p>Proper speaker support has been a multi-year development effort involving many people across all parts of the Linux audio stack. We want to offer the best speaker support of any Linux laptop platform, and that has meant driving Linux desktop audio forward a couple decades so that we can do what is expected of a modern laptop audio subsystem!</p>
<h2 id="user-content-current-state"><a href="#current-state">Current state</a></h2>
<p>We are ready to begin releasing preliminary speaker support to users. Keep in mind that as Asahi Linux is the first desktop Linux platform with integrated advanced speaker DSP, there will likely be bugs. In addition, the DSP profiles will be improved and adjusted over time, and do not represent the absolute best possible results.</p>
<p>Speaker DSP is currently only supported on Fedora Asahi Remix (things are moving fast), however alternative distros should be able to integrate this work relatively painlessly once things settle down (see the section below).</p>
<h2 id="user-content-known-limitations"><a href="#known-limitations">Known limitations</a></h2>
<p>The DSP processing uses too much power, due to limitations of the default scheduler. Uclamp support will be enabled with an upcoming PipeWire release, and it should greatly reduce the power consumption of the DSP processing, even doubling battery life while playing audio in some cases.</p>
<h2 id="user-content-known-bugs"><a href="#known-bugs">Known bugs</a></h2>
<ul>
<li>In KDE Plasma, if you toggle mute using the keyboard hotkey while the master volume is set at anything other than 100%, on unmute the speaker volume will be too low. Touching the volume control (or pressing a volume hotkey) will restore the intended volume.</li>
<li>The DSP chain introduces excessive delay, and trailing audio is "buffered" (if you stop playing something and start something else, you get a bit of the end of the first when the second starts).</li>
<li>There is no final limiter/compressor in the current DSP chains (although there is an input compressor), so inputs with content in high-gain regions of the EQ curve might cause distortion or clipping (and speakersafetyd limiting). This is most prominent in the 200Hz region right now. This should not cause damage, but we recommed lowering the volume if you notice the sound is noticeably distorted.</li>
<li>The 13" MacBook Air EQ curve might be a bit harsh on the treble; pending re-calibration with an individually calibrated microphone to confirm/fix.</li>
<li>Bankstown (the "fake bass" plugin) uses a relatively naïve algorithm right now, which doesn't work well for all music.</li>
</ul>
<h2 id="user-content-project-goals"><a href="#project-goals">Project goals</a></h2>
<p>Our DSP profiles aim to provide a balanced sound, with the features that people expect from high-quality laptop/small-speaker audio. In particular, we aim for:</p>
<ul>
<li>A balanced (neutral) tone at moderate listening volumes, with a mostly flat frequency response from a typical listening position</li>
<li>Reasonably high peak volume with acceptable sound degradation (compression, limiting, etc.)</li>
<li>
<a href="https://en.wikipedia.org/wiki/Missing_fundamental#Audio_processing_applications" rel="nofollow">"Fake bass" processing</a> to make audible frequencies that cannot be physically reproduced by the speakers, extending the perceived frequency response of the system.</li>
<li>
<a href="https://en.wikipedia.org/wiki/Equal-loudness_contour" rel="nofollow">Equal-loudness volume compensation</a>, so that the sound does not become noticeably tinny as the system master volume is lowered.</li>
</ul>
<p>These are all techniques that are in wide use in consumer microspeaker systems in tablets and phones, though sadly not common on laptops from most non-Apple brands. All of these processing tricks are also used by macOS.</p>
<p>Our goal is explicitly <em>not</em> to clone the full/exact macOS audio experience. We consider the macOS speaker DSP processing to be too tryhard; some of the things it does work well, some do not (or are outright buggy!), and some are just strange. Although audio is ultimately subjective, and we recognize that some people might prefer the "macOS sound", we aim for a more objectively neutral and balanced sound than macOS as a baseline. Users are encouraged to try adding their own effects (e.g. with EasyEffects) if they want to customize their experience and achieve certain sonic profiles. We believe that a balanced baseline that allows users to shape the sound to their own preference if they so desire is a better option than hard-coding specific effects (such as macOS' spatial audio processing) with no option to turn them off. There is no user-friendly (GUI) way to modify or tweak our DSP chains, so it's best if additional effects are left to existing utilities like EasyEffects that can be easily customized by the user.</p>
<p>That said, for the types of processing we <em>do</em> intend to apply, failing to work properly where macOS does (e.g. objectively bad audio quality for certain kinds of inputs in particular, such as bad-sounding compression or distortion at high volumes where macOS sounds better at an equal output volume) is considered a bug. Feel free to file issues if you find test cases where macOS does a clearly better job. We're particularly talking about program-dependent problems here, not "I like the macOS EQ/spatial/whatever stuff better in general".</p>
<h2 id="user-content-smart-amp--safety-support"><a href="#smart-amp--safety-support">"Smart Amp" / safety support</a></h2>
<p>In addition to the DSP processing, we also have the world's first (as far as we know) open source "smart amp" implementation. This allows the speakers to be driven to much higher peak levels than the worst-case safe volume level, greatly extending their dynamic range. Our implementation, <a href="https://github.com/AsahiLinux/speakersafetyd">speakersafetyd</a>, monitors feedback signals from the amplifiers, estimates speaker voice coil temperature using a two-stage thermal model, and reduces the hardware speaker volumes when the speakers get too warm. We also have kernel-side interlocks to disable or limit speaker volumes if speakersafetyd is not running or non-responsive.</p>
<p>Right now, we are shipping with a hard volume reduction limit of -7dBFS to catch potential bugs or misbehavior. If you notice that your speakers cut off for approximately one second and then come back at a reduced volume level, it is likely that you triggered this safety limit. Stop playback to let the speakers cool down (just in case) and let the limit re-arm, and then check <code>/var/lib/speakersafetyd/blackbox</code> for any blackbox dumps and <a href="">file a bug</a> attaching them together with any speakersafetyd logs (try <code>journalctl -S '10m ago' -u speakersafetyd</code>). If you play full-scale test tones or "extreme" music at maximum volume, it is expected that you will hit this limit and trigger a blackbox dump / panic. This is intended to help catch any badness while playing normal music, and we will remove this conservative limit once we are confident in the software stack (speakersafetyd has other safety limits built in and will gain more over time).</p>
<p>You can watch speakersafetyd in action by using <code>sudo journalctl -fu speakersafetyd</code>. As a rule of thumb, with the speaker volume at max, you should expect speakersafetyd not to kick in when playing loudness-normalized music (e.g. YouTube). Playing overly loud music without normalization (e.g. increasing the browser app volume to 100% in the Plasma applet while using YouTube, which bypasses the normalization limit it applies, or using media players without normalization) is likely to trigger some gain reduction from speakersafetyd, but it should not hit the -7 dB panic limit unless it's something <a href="https://open.spotify.com/album/6uvGw7zcCyMzYKKqXp9D3z" rel="nofollow">ridiculous</a>.</p>
<p><strong>WARNING:</strong> Speaker safety models have yet to be fully validated on all models. We are enabling audio only on models where we are confident things are ready and safe to use. If you use any undocumented overrides to force-enable speakers on any other machine models, <strong>you are entirely on your own</strong> and may very well blow up your speakers.</p>
<h2 id="user-content-distro-integration-notes"><a href="#distro-integration-notes">Distro integration notes</a></h2>
<p>You need:</p>
<ul>
<li>The latest Asahi kernel (tag asahi-6.5-25 or later)</li>
<li><a href="https://github.com/AsahiLinux/alsa-ucm-conf-asahi">alsa-ucm-conf-asahi</a></li>
<li><a href="https://github.com/asahilinux/asahi-audio">asahi-audio</a></li>
<li><a href="https://github.com/asahilinux/speakersafetyd">speakersafetyd</a></li>
<li><a href="https://github.com/chadmed/bankstown/">bankstown</a></li>
<li>
<a href="https://lsp-plug.in/" rel="nofollow">LSP plug-ins</a> (LV2 version)</li>
<li>PipeWire 0.3.84. Depending on your packaging rules, you might need your package to create a few empty directories (see <a href="https://src.fedoraproject.org/rpms/pipewire/commits/rawhide" rel="nofollow">here</a>) so <code>asahi-audio</code> can put files there.</li>
<li>PipeWire <code>module-filter-chain-lv2</code> (this may be a separate package or flags depending on distro)</li>
<li>WirePlumber 0.4.15 with an <a href="https://gitlab.freedesktop.org/pipewire/wireplumber/-/merge_requests/558" rel="nofollow">unreleased patch</a>. Please do not enable speakers without that patch, as it'll leave the raw speaker device visible which is confusing and could trigger bugs elsewhere. Either get your WirePlumber package to add the patch, or wait for the next release.</li>
</ul>
<p>The correct deployment order is asahi-audio/speakersafetyd &gt; (whatever you use to get those installed for users, e.g. metapackage) &gt; kernel. If you push the kernel first before asahi-audio, users will get either a nonfunctional (if no speakersafetyd) or functional but bad-sounding (if speakersafetyd is installed) raw speaker device with no DSP.</p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Archive Your Old Projects (142 pts)]]></title>
            <link>https://arne.me/articles/archive-your-old-projects</link>
            <guid>38239358</guid>
            <pubDate>Sun, 12 Nov 2023 11:40:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arne.me/articles/archive-your-old-projects">https://arne.me/articles/archive-your-old-projects</a>, See on <a href="https://news.ycombinator.com/item?id=38239358">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><article><header></header><p>Yesterday, while looking through a folder called <em>old things lol glhf</em>, I fell
into a rabbit hole of old abandoned projects—mostly websites and graphic
design, but I also found one or two Flash<sup><a href="#fn-1" id="fnref-1" data-footnote-ref="">1</a></sup> projects and compiled <code>.exe</code>
files<sup><a href="#fn-2" id="fnref-2" data-footnote-ref="">2</a></sup>.</p>
<p>And, while it was really fun remembering projects I’ve long forgotten, there
was no structure, and it was often difficult to figure out what a project did
and what it looked like—some even missed crucial data.
This made me think about how I want to archive my projects going forward.</p>
<!-- raw HTML omitted -->
<p>Here is my new strategy:</p>
<h2>Leave it online</h2>
<p>If the project is on the web, doesn’t require maintenance and doesn’t cost you
money, leave it online.</p>
<p>Occasionally, you’ll need to move to a different domain, for example when re-doing
a website.
When talking to <a href="https://flbn.sh/">Ollie</a> about this, he told me that some people
leave their old websites online at <code>&lt;year&gt;.&lt;domain&gt;</code> and I love that idea<sup><a href="#fn-3" id="fnref-3" data-footnote-ref="">3</a></sup>.</p>
<p>You can look up old content and redirect links, so your
<a href="https://www.w3.org/Provider/Style/URI">URIs stay cool</a>.
And in ten years it’ll probably still be online.</p>
<h2>Archive it</h2>
<p>If you can’t leave it online, save it to your file system.
You don’t have to go all <a href="https://johnnydecimal.com/">Johnny.Decimal</a>, but at
least create a dedicated folder and subfolders for every year.</p>
<p>But instead of just copying your project to the archive folder and be done with
it, consider these points to make life easier for your future self:</p>
<dl><dt>Make screenshots</dt>
<dd>
<p>Having a screenshot allows you to relive your memories more easily without
going through the hassle of setting up a project.
If you want to go the extra mile, do a screen recording showcasing your
project—this has the bonus effect of hearing your voice from years ago, and it
has more context.</p>
</dd>
<dt>Add a README</dt>
<dd>
<p>Explain what the project did, when it was created and abandoned, who
contributed and how to get it running again.</p>
</dd>
<dt>Back up the database</dt>
<dd>
<p>Some of my projects are missing a database dump, so all the actual content is
gone. Run <code>sqldump</code> or whatever export functionality your database supports
and add it next to your files.</p>
</dd>
<dt>Keep generated assets</dt>
<dd>
<p>When using static site generators, add the folder containing the built HTML,
CSS &amp; JS to the archive.
That way, all you have to do is run a static file server to be able to browse
the complete website.</p>
</dd>
</dl>
<h2>Save it to the Internet Archive</h2>
<p>If you don’t own your platform (maybe you’re publishing to Substack or Notion),
you can at least save your website to the
<a href="https://web.archive.org/">Wayback Machine</a>.
I would also advise saving your content somewhere you control.</p>
<h2>Show me yours</h2>
<p>So that’s it, that’s my new project archival strategy.</p>
<p>How are you archiving your projects?
Am I missing anything?
<a href="https://arne.me/contact">Let me know.</a></p>
<section data-footnotes="">
<ol>
<li id="fn-1">
<p>If you don’t know what this is, <a href="https://en.wikipedia.org/wiki/Adobe_Flash">Wikipedia’s got your back</a> <a href="#fnref-1" data-footnote-backref="" data-footnote-backref-idx="1" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="fn-2">
<p>Not sure what to do with these besides deleting. <a href="#fnref-2" data-footnote-backref="" data-footnote-backref-idx="2" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="fn-3">
<p>You can find the previous version of this very website at <a href="https://2023.arne.me/">2023.arne.me</a>. <a href="#fnref-3" data-footnote-backref="" data-footnote-backref-idx="3" aria-label="Back to reference 3">↩</a></p>
</li>
</ol>
</section>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RIP Google Groups Dejanews.com Archive (134 pts)]]></title>
            <link>http://dejanews.com/</link>
            <guid>38238796</guid>
            <pubDate>Sun, 12 Nov 2023 09:43:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://dejanews.com/">http://dejanews.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38238796">Hacker News</a></p>
Couldn't get http://dejanews.com/: Error: Request failed with status code 404]]></description>
        </item>
        <item>
            <title><![CDATA[Map of Space Invader Mosaics in Paris (195 pts)]]></title>
            <link>https://pnote.eu/projects/invaders/map.html</link>
            <guid>38238658</guid>
            <pubDate>Sun, 12 Nov 2023 09:11:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pnote.eu/projects/invaders/map.html">https://pnote.eu/projects/invaders/map.html</a>, See on <a href="https://news.ycombinator.com/item?id=38238658">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
     <p><a href="https://pnote.eu/projects/invaders/">What is this?</a> | 
     <a href="https://newsletter.pnote.eu/">Newsletter</a> 

      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Overview of Nix in Practice (113 pts)]]></title>
            <link>https://www.slice.zone/blog/nix-in-practice</link>
            <guid>38237696</guid>
            <pubDate>Sun, 12 Nov 2023 05:54:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.slice.zone/blog/nix-in-practice">https://www.slice.zone/blog/nix-in-practice</a>, See on <a href="https://news.ycombinator.com/item?id=38237696">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-apply-variations="true">
<!-- -->
<!-- -->
<!-- -->

<blockquote>
<p><code>&lt;skip&gt;</code> i would really appreciate it if someone who knows nix could help tho
<span role="img" aria-label="pleading face emoji">🥺</span><br> <code>&lt;skip&gt;</code> i asked in nix server and multiple people were
like “yea this should be working but it doesn’t” <br><code>&lt;skip&gt;</code> maybe i should
post on <a href="https://discourse.nixos.org/">the discourse</a> hm <br> <code>&lt;skip&gt;</code> i’ve
lost sleep over this lol</p>
<small>— July 25, 2021</small>
</blockquote>
<p>I was introduced to Nix in late 2021 by a friend in a Discord channel, but it
wasn’t my first encounter with the operating system/package
manager/configuration system/programming language hodgepodge.</p>
<p>As a Linux-obsessed child who constantly hopped between distributions, it
inevitably registered on my radar. I was always looking for the next ISO file to
victimize and copy onto my tired USB stick, which would probably suffocate me in
my sleep if it grew arms. But NixOS was never one of those distros. It looked
too <em>weird</em> to my 14-year-old brain.</p>
<p>I still believe this. NixOS, and Nix in general, is <em>really</em> weird. I’ve
described it as what ends up writhing out of a malfunctioning industrial mixer
that someone accidentally dropped Haskell and Bash into. When I, like many other
computer programmers, tell my friends who don’t write code that “it’s a miracle
that modern technology even works”, Nix is one of those things that I’m
referring to.</p>
<p>Despite this, NixOS is the only distro I install on my servers. I’m dual-booting
it on my MacBook<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>. I build and set up the development environments for all of
my projects with Nix. I use Nix to set up my dotfiles and take care of my shell
configuration and personal set of installed packages. Nix makes it unflinchingly
simple to replicate my cozy user environment anywhere Nix is available, so I can
feel at home no matter where I am. Nix even turned building ffmpeg into a
cakewalk for that one time I needed
<a href="https://en.wikipedia.org/wiki/Fraunhofer_FDK_AAC">better AAC encoding</a><sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>.</p>
<p>Nix is a really neat (in my opinion), but the learning “curve” is comparable to
climbing a brick wall with nothing but your fingernails, determination, and the
incredibly thin shred that is your remaining patience. I’m still not great at
Nix, and I’m terrified to submit anything to the official package collection,
but I hope to demystify it and emphasize what makes it so dang useful.</p>
<h2 id="behind-the-curtain">Behind the curtain</h2>
<p>“Nix”, generally speaking, refers to any or all of these things:</p>
<ol>
<li>A ~purely functional programming language with immutability, laziness, floaty
Haskellesque syntax, and some interesting design choices to make
configuration and configuration-adjacent tasks particularly easy.</li>
<li>A package manager and build system with the primary goal of reproducibility
that consumes package definitions and build instructions written in the Nix
programming language.</li>
<li>A massive repository that houses more than 80,000 Nix package definitions
written and maintained by over 5,000 contributors, called “nixpkgs”.</li>
<li>A Linux distribution (“NixOS”) that uses Nix<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup> to declaratively specify the
total configuration of the system, such as: which packages are to be
installed, which user accounts exist on the system, and what services are
active.</li>
</ol>
<p>All of these puzzle pieces fit together to create something that is truly unique
and terrifying. Hooray!</p>
<p>Nix is different from other package managers in that it’s truly and utterly
obsessed with making things entirely self-contained. When Nix builds a package,
the build environment is isolated such that nothing outside of what you declare
is accessible. Having globally installed libraries that are recognized by
<code>configure</code> scripts, CMake, etc. can be a massive pain, especially when you need
a specific version or find yourself needing to apply bespoke patches. Sometimes
the exact version of a library you require just isn’t available easily. Having
multiple versions installed can be either difficult or nigh impossible.</p>
<p>Nix avoids this by having binaries reference their dependencies <em>explicitly</em>.
List the shared libraries needed by a binary that was built by Nix, and you’ll
see something like this:</p>

<pre><code>$ ldd "$(which curl)"
	linux-vdso.so.1 (0x00007fffeeb86000)
	libcurl.so.4 =&gt; /nix/store/rirzp6ijbcwnxlf0b2n286n587r3z9jw-curl-7.86.0/lib/libcurl.so.4 (0x00007ffb5ce40000)
	libssl.so.3 =&gt; /nix/store/4mxnw95jcm5a27qk60z7yc0gvxp42b9a-openssl-3.0.7/lib/libssl.so.3 (0x00007ffb5cd93000)
	libcrypto.so.3 =&gt; /nix/store/4mxnw95jcm5a27qk60z7yc0gvxp42b9a-openssl-3.0.7/lib/libcrypto.so.3 (0x00007ffb5c914000)
	libz.so.1 =&gt; /nix/store/026hln0aq1hyshaxsdvhg0kmcm6yf45r-zlib-1.2.13/lib/libz.so.1 (0x00007ffb5c8f6000)
	libc.so.6 =&gt; /nix/store/4nlgxhb09sdr51nc9hdm8az5b08vzkgx-glibc-2.35-163/lib/libc.so.6 (0x00007ffb5c6ed000)
	libnghttp2.so.14 =&gt; /nix/store/qz400bwshaqikj5s2qyvh0c9qffgmqik-nghttp2-1.49.0-lib/lib/libnghttp2.so.14 (0x00007ffb5c6bc000)
	libidn2.so.0 =&gt; /nix/store/5mh5019jigj0k14rdnjam1xwk5avn1id-libidn2-2.3.2/lib/libidn2.so.0 (0x00007ffb5c69a000)
	libssh2.so.1 =&gt; /nix/store/vqq9s0d6fw6kqf3sr5nrzqbys9rhygqd-libssh2-1.10.0/lib/libssh2.so.1 (0x00007ffb5c659000)
	libgssapi_krb5.so.2 =&gt; /nix/store/r7gl900my2fw6k33nxh2r7rzv8nv0s25-libkrb5-1.20/lib/libgssapi_krb5.so.2 (0x00007ffb5c606000)
	libzstd.so.1 =&gt; /nix/store/w10in9diaqrcqqxi5lg20n3q2jfpk6pq-zstd-1.5.2/lib/libzstd.so.1 (0x00007ffb5c540000)
	libbrotlidec.so.1 =&gt; /nix/store/9iy1ng7h1l6jdmjk157jra8n4hkrfdj1-brotli-1.0.9-lib/lib/libbrotlidec.so.1 (0x00007ffb5c532000)
	libdl.so.2 =&gt; /nix/store/4nlgxhb09sdr51nc9hdm8az5b08vzkgx-glibc-2.35-163/lib/libdl.so.2 (0x00007ffb5c52d000)
	libpthread.so.0 =&gt; /nix/store/4nlgxhb09sdr51nc9hdm8az5b08vzkgx-glibc-2.35-163/lib/libpthread.so.0 (0x00007ffb5c528000)
	/nix/store/4nlgxhb09sdr51nc9hdm8az5b08vzkgx-glibc-2.35-163/lib/ld-linux-x86-64.so.2 =&gt; /nix/store/4nlgxhb09sdr51nc9hdm8az5b08vzkgx-glibc-2.35-163/lib64/ld-linux-x86-64.so.2 (0x00007ffb5cee7000)
	libunistring.so.2 =&gt; /nix/store/34xlpp3j3vy7ksn09zh44f1c04w77khf-libunistring-1.0/lib/libunistring.so.2 (0x00007ffb5c37a000)
	libkrb5.so.3 =&gt; /nix/store/r7gl900my2fw6k33nxh2r7rzv8nv0s25-libkrb5-1.20/lib/libkrb5.so.3 (0x00007ffb5c29f000)
	libk5crypto.so.3 =&gt; /nix/store/r7gl900my2fw6k33nxh2r7rzv8nv0s25-libkrb5-1.20/lib/libk5crypto.so.3 (0x00007ffb5c270000)
	libcom_err.so.3 =&gt; /nix/store/r7gl900my2fw6k33nxh2r7rzv8nv0s25-libkrb5-1.20/lib/libcom_err.so.3 (0x00007ffb5c26a000)
	libkrb5support.so.0 =&gt; /nix/store/r7gl900my2fw6k33nxh2r7rzv8nv0s25-libkrb5-1.20/lib/libkrb5support.so.0 (0x00007ffb5c25a000)
	libkeyutils.so.1 =&gt; /nix/store/816qwr4xy058451rbxr0ccyh1v1akhb6-keyutils-1.6.3-lib/lib/libkeyutils.so.1 (0x00007ffb5c251000)
	libresolv.so.2 =&gt; /nix/store/4nlgxhb09sdr51nc9hdm8az5b08vzkgx-glibc-2.35-163/lib/libresolv.so.2 (0x00007ffb5c23f000)
	libm.so.6 =&gt; /nix/store/4nlgxhb09sdr51nc9hdm8az5b08vzkgx-glibc-2.35-163/lib/libm.so.6 (0x00007ffb5c15f000)
	libbrotlicommon.so.1 =&gt; /nix/store/9iy1ng7h1l6jdmjk157jra8n4hkrfdj1-brotli-1.0.9-lib/lib/libbrotlicommon.so.1 (0x00007ffb5c13c000)
</code></pre>
<p>Assuming that the package is built correctly and you aren’t playing any runtime
linker tricks on your end, it’s virtually impossible to run into library errors.
Having multiple versions of a library installed is a nonissue, and dependency
hell falls into irrelevancy.</p>

<p>Binaries are vacuum packed in that they only ever refer to exactly what they
need, and this applies for both runtime and build dependencies alike.</p>
<p>Because the Nix store is essentially an append-only graph database<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup> with
nodes being packages and edges being dependency relations, I can even query it
to see e.g. ffmpeg’s runtime dependencies, both direct and indirect:</p>
<figure><img alt="An end-to-end graph of ffmpeg's dependencies." title="An end-to-end graph of ffmpeg's dependencies." loading="lazy" width="550" height="321" decoding="async" data-nimg="1" sizes="(max-width: 800px) 90vw, (min-width: 800px) 570px" srcset="https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fffmpeg-graph.993e883a.png&amp;w=640&amp;q=75 640w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fffmpeg-graph.993e883a.png&amp;w=750&amp;q=75 750w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fffmpeg-graph.993e883a.png&amp;w=828&amp;q=75 828w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fffmpeg-graph.993e883a.png&amp;w=1080&amp;q=75 1080w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fffmpeg-graph.993e883a.png&amp;w=1200&amp;q=75 1200w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fffmpeg-graph.993e883a.png&amp;w=1920&amp;q=75 1920w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fffmpeg-graph.993e883a.png&amp;w=2048&amp;q=75 2048w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fffmpeg-graph.993e883a.png&amp;w=3840&amp;q=75 3840w" src="https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fffmpeg-graph.993e883a.png&amp;w=3840&amp;q=75"><p>Brought to you by <code>nix-store</code> and <code>graphviz</code>.</p></figure>

<p>At the top, we see <code>CoreFoundation</code>, which is a core macOS system framework that
a lot of packages depend on. We can also spot <code>dejavu-fonts-minimal</code>, which is
needed by <code>fontconfig</code>, which is needed by <code>libass</code> to render subtitles, which
is needed by this default configuration of <code>ffmpeg</code>. Emphasis on default: we can
override options provided by the package definition and specify <code>.patch</code> files
to be applied to the source code.</p>
<p>To maximize reproducibility, Nix works on the source code of a package. But
because every package is built in its own universe, it is extremely cacheable.
Nix package installations are capital F <em>Fast</em>, because there’s no dependency
resolution to be done. It amounts to downloading data from a CDN and unpacking
it, and it can’t get any faster than that.</p>

<p>Executing “over 350,000 builds each week”, the NixOS foundation maintains
<a href="https://hydra.nixos.org/">a massive Nix build farm for <code>x86-64</code> and <code>aarch64</code> Linux and macOS</a>,
uploading the build artifacts to
<a href="https://cache.nixos.org/">a widely-available binary cache</a>.</p>
<p>As Nix evaluates the build instructions locally, it hashes the result and
queries the cache for it. In theory<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="true" aria-describedby="footnote-label">5</a></sup>, between systems of the same platform,
there’s no difference between building the desired package and copying the
results from the cache. Taking an extreme amount of care to isolate builds from
one another makes this practical.</p>
<p>This means that in ordinary circumstances, you won’t have to build packages from
scratch. If you impose an override onto a package that changes the resulting
build instructions, Nix doesn’t find a cached result from the cache due to the
differing hashes, and the package is built locally.</p>
<p>Nix’s approach to builds also has some other neat effects:</p>
<ol>
<li><a href="https://nix.dev/tutorials/cross-compilation">Cross-compilation becomes easier</a>,
which unfortunately complicates the overall infrastructure but makes
approaching this problem comparatively less scary…probably.</li>
<li><a href="https://nixos.org/manual/nix/stable/advanced-topics/distributed-builds.html">Remote build</a>
support lets you evaluate a package’s (potentially customized) build
instructions on your local box, then send them to a more powerful machine
over the network to build. This is not only cool, but also necessary should
the package not be supported on your local platform. Distributed builds, CI,
et cetera! The sky is the limit.</li>
<li><a href="https://nixos.org/manual/nix/stable/command-ref/nix-copy-closure.html"><code>nix-copy-closure</code></a>
copies the results of a build and its total runtime dependency tree (called a
“closure”) between machines of the same platform, avoiding unnecessary work.</li>
<li>Cache your own stuff!
<a href="https://nixos.wiki/wiki/Binary_Cache">Host your own binary cache</a>, or use a
service like <a href="https://www.cachix.org/">Cachix</a> to avoid rebuilding.</li>
</ol>
<h2 id="ephemerality">Ephemerality</h2>
<figure><img alt="xkcd 1987" title="xkcd 1987" loading="lazy" width="550" height="545" decoding="async" data-nimg="1" sizes="(max-width: 800px) 90vw, (min-width: 800px) 570px" srcset="https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxkcd-1987.6b704c51.png&amp;w=640&amp;q=75 640w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxkcd-1987.6b704c51.png&amp;w=750&amp;q=75 750w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxkcd-1987.6b704c51.png&amp;w=828&amp;q=75 828w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxkcd-1987.6b704c51.png&amp;w=1080&amp;q=75 1080w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxkcd-1987.6b704c51.png&amp;w=1200&amp;q=75 1200w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxkcd-1987.6b704c51.png&amp;w=1920&amp;q=75 1920w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxkcd-1987.6b704c51.png&amp;w=2048&amp;q=75 2048w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxkcd-1987.6b704c51.png&amp;w=3840&amp;q=75 3840w" src="https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fxkcd-1987.6b704c51.png&amp;w=3840&amp;q=75"><p>This sums it up, I think.</p></figure>

<p>Managing Python environments is not only painful, but actively damaging to the
psyche.</p>
<p>Setting up an ephemeral Python environment with OpenCV can be done in Nix like
so:</p>
<pre><code>$ nix-shell --pure -p "python3.withPackages(packages: [ packages.opencv4 ])" --run python3
Python 3.10.10 (main, Feb 17 2023, 05:25:10) [Clang 11.1.0 ] on darwin
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import cv2
&gt;&gt;&gt;
</code></pre>
<p>I can run a similar command to drop myself into a REPL with NumPy or SciPy, or
any package in general: a certain version of Java, or even a Node or Haskell
environment.</p>
<p>Dissecting the command:</p>
<ol>
<li>
<p>The <code>--pure</code> flag clears the environment before dropping you into the shell,
preventing e.g. <code>$PATH</code> from leaking through.</p>
</li>
<li>
<p><code>-p</code> specifies which package(s) we are interested in. In this case, <code>python3</code>
(which currently resolves to <code>python310</code> on the 22.11 channel). We use
<code>withPackages</code> to indicate that we’d like to create an environment with the
<code>opencv4</code> Python package available.</p>
<pre><code><span>python3</span><span>.</span><span>withPackages</span>(<span>packages</span>: [ <span>packages</span><span>.</span><span>opencv4</span> ])
</code></pre>
<p>This is Nix <em>code</em> that uses package definitions from nixpkgs, which has
specific infrastructure for Python, Python packages, and managing Python
environments. Nix has similar infrastructure for Rust, Node.js, etc.</p>
</li>
<li>
<p><code>--run</code> drops us directly into the Python REPL instead of Bash.</p>
</li>
</ol>
<p>Invoking this command causes Nix to compute which packages is needed and what
Python environment trickery needs to be done. If the necessary packages aren’t
in my Nix store already, they’re downloaded from the binary cache. When I exit
the shell, my local Python user environment remains completely untouched. The
fundamental nature of Nix allows this to happen in a frictionless way—no
<code>virtualenv</code> futzing required.</p>
<p>Nix’s modular nature allows it to cooperate with other technologies, too. Take
Docker, for instance:
<a href="https://xeiaso.net/blog/i-was-wrong-about-nix-2020-02-10">Nix is seemingly better at generating smaller images</a>;
because
<a href="https://grahamc.com/blog/nix-and-layered-docker-images/">it knows everything about every package</a>,
it can purge superfluous data down to the nanometer and keep only the bits that
matter.</p>
<h2 id="declarative-systems">Declarative systems</h2>
<p>NixOS takes wacky declarative package management to its next logical conclusion
by using the same technology to determine to service topology of your Linux
system. In essence, your system configuration becomes a package that is managed
and installed by Nix just like any other.</p>
<p>This is what a minimal NixOS configuration looks like (in practice, real
configurations are much larger):</p>
<pre><code>{ <span>config</span><span>,</span> <span>pkgs</span><span>,</span> <span>... </span>}: {
  <span>imports</span> <span>=</span> [
    <span># Import another Nix module that contains the results of the hardware scan.</span>
    <span># (Includes important kernel modules for hardware, the filesystem table, etc.)</span>
    <span>./hardware-configuration.nix</span>
  ];

  <span># Enable a bootloader for UEFI systems.</span>
  <span>boot</span>.<span>loader</span>.<span>systemd-boot</span>.<span>enable</span> <span>=</span> <span>true</span>;

  <span># Enable the OpenSSH server.</span>
  <span>services</span>.<span>sshd</span>.<span>enable</span> <span>=</span> <span>true</span>;

  <span># Add some useful packages for everyone.</span>
  <span>environment</span>.<span>systemPackages</span> <span>=</span> <span>with</span> <span>pkgs</span>; [ <span>git</span> <span>curl</span> <span>wget</span> ];

  <span># ...</span>
}
</code></pre>
<p>As someone with ADHD and relatively poor memory retention, NixOS is a godsend
for managing my fleet of servers. After not having logged into a box for a
while, I easily forget what services exist on the server. I’m too lazy to
maintain this in writing, but it’s OK because Nix enables declarative management
of these services. It acts as the definitive source of truth.</p>
<p>Figuring out exactly what a server does without fear of documentation
desynchronizing with what actually exists on the server is a single
<code>$EDITOR /etc/nixos/configuration.nix</code> away.</p>
<p>NixOS differs from something like Ansible in that it’s inherently declarative,
through and through. If you remove the <code>services.sshd.enable = true;</code> line from
your configuration, NixOS will tear down the OpenSSH server upon a rebuild.
It’ll be as if it was never there (subtracting any leftover data), because there
isn’t a practical difference between installing NixOS for the first time and
building it again. Ansible has the notion of “idempotency”, but Nix (and NixOS)
is idempotent by nature.</p>
<p>NixOS isn’t special:</p>
<ol>
<li>
<p>You can build a NixOS system and copy the closure to another machine to use.</p>
</li>
<li>
<p>You can
<a href="https://nixos.wiki/wiki/Creating_a_NixOS_live_CD">make a custom version of the NixOS installer ISO</a>
with your desired customizations to craft your ideal Linux rescue USB.</p>
</li>
<li>
<p>You can
<a href="https://nixos.org/manual/nixos/stable/#sec-changing-config">test experimental changes to your system</a>
by sandboxing it within a QEMU virtual machine:</p>
<pre><code>$ nixos-rebuild build-vm

<span># Run your new system inside of a VM to see if something broke.</span>
$ ./result/bin/run-<span>*</span>-vm
</code></pre>
</li>
<li>
<p>Because virtualization is made easy with declarative configurations, NixOS
maintainers
<a href="https://nixos.org/manual/nixos/stable/index.html#sec-nixos-tests">test their own code with it</a>.</p>
</li>
<li>
<p>Because Nix packages are fully self-contained,
<a href="https://nixos.org/manual/nix/stable/package-management/profiles.html">previous generations of the system are tracked</a>,
so you can rollback anytime in the boot menu. I used this once after I
accidentally hosed my network configuration and wasn’t sure how to fix it.</p>
</li>
<li>
<p>Not only are previous generations kept around, system upgrades are
essentially fully atomic. New version of <code>libc</code> (or other Important<span role="img" aria-label="tm emoji">™️</span>
library)? On an ordinary system, replacing the file without rebooting would
probably summon flesh-eating demons. But with Nix, this isn’t an
issue—every package was already referencing the version it depended on
through an absolute path into the Nix store.</p>
</li>
</ol>
<p>Saving disk space is easy, too. Being a feature of Nix in general,
<a href="https://nixos.org/manual/nix/stable/command-ref/nix-collect-garbage.html">the garbage collector</a>
can clean up computed build instructions, old profile/system generations, and
other build dependencies that aren’t needed for a functioning system. This runs
pretty fast and is a nice crutch for those times you just need more room.</p>
<pre><code>$ sudo nix-collect-garbage -d
<span># ...</span>
16730 store paths deleted, 11828.97 MiB freed <span># :O</span>
</code></pre>
<h2 id="declarative-modular-systems">Declarative, modular systems</h2>

<p>nixpkgs comes with a huge number of included modules that port other software
and services to be usable with NixOS.</p>
<p>Something that is genuinely terrifying to set up to me is GitLab. It just has
too many moving parts. To deploy it with NixOS, I have this in my configuration:</p>
<pre><code>{
  <span>services</span>.<span>gitlab</span> <span>=</span> <span>let</span>
    <span>secretFile</span> <span>=</span> <span><span>"</span>/var/lib/gitlab-secret<span>"</span></span>;
    <span>rsaSecretFile</span> <span>=</span> <span><span>"</span>/var/lib/gitlab-secret-rsa<span>"</span></span>;
    <span>initialRootPasswordFile</span> <span>=</span> <span><span>"</span>/var/lib/gitlab-initial-root-password<span>"</span></span>;
  <span>in</span> {
    <span>enable</span> <span>=</span> <span>true</span>;
    <span>host</span> <span>=</span> <span>...</span>;
    <span>port</span> <span>=</span> <span>80</span>;
    <span>https</span> <span>=</span> <span>true</span>;
    <span>inherit</span> <span>initialRootPasswordFile</span>;
    <span>secrets</span> <span>=</span> {
      <span>secretFile</span> <span>=</span> <span>secretFile</span>;
      <span>dbFile</span> <span>=</span> <span>secretFile</span>;
      <span>otpFile</span> <span>=</span> <span>secretFile</span>;
      <span>jwsFile</span> <span>=</span> <span>rsaSecretFile</span>;
    };
  };
}
</code></pre>
<p>Other than specifying some paths to files containing secrets (a caveat which
I’ll elaborate on later), this is all I need for a functioning GitLab setup. If
I change my mind on this in the future, all I have to do is flip
<code>enable = true;</code> to <code>false</code>, or outright evict this code block from my
configuration.</p>
<p>By doing this, all required services immediately stop and become disabled. If I
run the garbage collector, all GitLab packages are deleted. All that remains is
any user data that was created by the services in question. This is seriously
awesome to me, because GitLab is exemplary of being somewhat of a behemoth to
set up, despite <a href="https://docs.gitlab.com/omnibus/">Omnibus</a>.</p>
<p>What is even better is the inherent benefit of a modular and declarative
configuration. For example, one of my servers is also responsible for hosting an
authoritative DNS server, which has a corresponding NixOS module in nixpkgs:</p>
<pre><code>{
  <span>services</span>.<span>nsd</span> <span>=</span> {
    <span>enable</span> <span>=</span> <span>true</span>;
    <span>interfaces</span> <span>=</span> [ <span><span>"</span>0.0.0.0<span>"</span></span> ];
    <span>verbosity</span> <span>=</span> <span>2</span>;
    <span>zones</span>.<span>howl</span>.<span>children</span> <span>=</span> {
      <span><span>"</span>howl.<span>"</span></span>.<span>data</span> <span>=</span> <span><span>''</span></span>
<span>        $ORIGIN howl.</span>
<span>        $TTL 3600</span>
<span></span>
<span>        @ IN SOA howl. tinyslices@gmail.com. ( 2021122201 28800 7200 864000 60 )</span>
<span>        @ IN NS louie.howl.</span>
<span></span>
<span>        <span>${</span><span>lib</span><span>.</span><span>concatStringsSep</span> <span>"</span><span>\n</span><span>"</span> <span>config</span><span>.</span><span>howl</span><span>.</span><span>records</span><span>}</span></span>
<span>      <span>''</span></span>;
    };
  };
}
</code></pre>
<p>An authoritative DNS server vends records instead of simply resolving and
caching them. Here, I’m directing
<a href="https://www.nlnetlabs.nl/projects/nsd/about/"><code>nsd</code></a> (the DNS server in
question) to listen on the unspecified address<sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="true" aria-describedby="footnote-label">6</a></sup>. I’m using it with
<a href="https://tailscale.com/">Tailscale</a>, which is a really neat service that enables
a zero-config mesh VPN between all of my servers and devices. (NixOS also has a
module for it—just <code>services.tailscale.enable = true;</code>.)</p>
<p>Most importantly, I’m defining a custom NixOS option: <code>howl.records</code>, which lets
me easily add lines to the zone from other modules. This is important.</p>
<pre><code>{
  <span>options</span>.<span>howl</span> <span>=</span> <span>with</span> <span>lib</span>; {
    <span>records</span> <span>=</span> <span>mkOption</span> {
      <span>type</span> <span>=</span> <span>types</span><span>.</span><span>listOf</span> <span>types</span><span>.</span><span>str</span>;
      <span>description</span> <span>=</span>
        <span><span>"</span>Authoritative DNS records exposed to every device in the tailnet<span>"</span></span>;
    };
  };
}
</code></pre>

<p>When I write the code to enable the <code>nsd</code> module above, I make sure to reference
the final value of this custom option (as <code>config.howl.records</code>) in the zone
configuration, which is collected from all instances of the option being
modified in other modules. Nix’s lazy evaluation crucially enables this.</p>
<p>Now, in another NixOS module—say, <code>monitoring.nix</code>, I can enable Grafana and
other services such as Prometheus. I can also tweak Nginx and tell it to expose
its status page, and also create virtual hosts for Grafana and Prometheus. And
in the same breath, I can create DNS records for all of these new services, so
every device connected to the tailnet can access them painlessly:</p>

<pre><code>{ <span>lib</span><span>,</span> <span>... </span>}:

<span>let</span>
  <span>inherit</span> (<span>import</span> <span>./net.nix</span>) <span>localhost</span> <span>internal</span> <span>ports</span>;
<span>in</span>
{
  <span>services</span>.<span>grafana</span> <span>=</span> {
    <span>enable</span> <span>=</span> <span>true</span>;
    <span>port</span> <span>=</span> <span>ports</span><span>.</span><span>grafana</span>;
    <span>domain</span> <span>=</span> <span><span>"</span>grafana.howl<span>"</span></span>;
    <span>rootUrl</span> <span>=</span> <span><span>"</span>http://grafana.howl<span>"</span></span>;
  };

  <span>services</span>.<span>prometheus</span> <span>=</span> {
    <span>enable</span> <span>=</span> <span>true</span>;
    <span>port</span> <span>=</span> <span>ports</span><span>.</span><span>prometheus</span><span>.</span><span>prometheus</span>;
    <span>globalConfig</span>.<span>scrape_interval</span> <span>=</span> <span><span>"</span>10s<span>"</span></span>;
    <span>scrapeConfigs</span> <span>=</span> [{
      <span># ...</span>
    }];
    <span>exporters</span> <span>=</span> {
      <span># ...</span>
      <span>nginx</span>.<span>enable</span> <span>=</span> <span>true</span>;
    };
  };

  <span># Instruct Nginx to expose its status page for Prometheus.</span>
  <span>services</span>.<span>nginx</span>.<span>statusPage</span> <span>=</span> <span>true</span>;

  <span># Configure Nginx vhosts for these new services.</span>
  <span>services</span>.<span>nginx</span>.<span>virtualHosts</span>.<span><span>"</span>grafana.howl<span>"</span></span>.<span>locations</span>.<span><span>"</span>/<span>"</span></span> <span>=</span> <span>internal</span> {
    <span>proxyPass</span> <span>=</span> <span>localhost</span> <span>ports</span><span>.</span><span>grafana</span>;
  };
  <span>services</span>.<span>nginx</span>.<span>virtualHosts</span>.<span><span>"</span>prometheus.howl<span>"</span></span>.<span>locations</span>.<span><span>"</span>/<span>"</span></span> <span>=</span> <span>internal</span> {
    <span>proxyPass</span> <span>=</span> <span>localhost</span> <span>ports</span><span>.</span><span>prometheus</span><span>.</span><span>prometheus</span>;
  };

  <span># Add lines to the DNS zone for these new services.</span>
  <span>howl</span>.<span>records</span> <span>=</span> [
    <span><span>"</span>grafana.howl. IN CNAME louie.howl.<span>"</span></span>
    <span><span>"</span>prometheus.howl. IN CNAME louie.howl.<span>"</span></span>
  ];
}
</code></pre>
<p>I import this new module in my main configuration:</p>
<pre><code>{
  <span># ...</span>
  <span>imports</span> <span>=</span> [
    <span># ...</span>
    <span>./monitoring.nix</span>
  ];
}
</code></pre>
<p>And after a rebuild, Nix pulls in the required packages and builds the system
closure, enabling and configuring every service that I declared to be active.</p>
<p>In my opinion, this is where NixOS really shines. I’m able to enable and
configure Grafana, Prometheus, Nginx, and the DNS records that have to do with
those services in a single file. I’m defining shared constants in a separate
place and using them across configurations that are written in <em>completely
different languages and syntaxes</em>.</p>

<p>With traditional server management, I’d have to install the correct packages,
look up the paths to their configuration files, and maybe even run some commands
to modify some mutable state. I’ll forget all of this in a few days or even
hours, but Nix lets me authoritatively describe it in code.</p>
<p>If I decide to stop importing that file in my configuration, then the DNS and
Nginx virtual hosts are removed; Nginx stops exposing its status page;
Prometheus and Grafana’s <code>systemd</code> units are disabled; and all unneeded packages
are no longer reachable from a garbage collector root, making them eligible for
deletion.</p>
<p>NixOS lets me unify and modularize what would otherwise be disconnected
configurations across packages in a highly idempotent, cacheable, and
declarative way, essentially homogenizing every service on the system. <strong>That’s
amazing to me.</strong></p>
<h2 id="diving-in">Diving in</h2>
<p>I’m intentionally avoiding the details here because I believe it’s important to
first grasp the big picture behind Nix and NixOS, and understand what it’s truly
useful for.</p>
<p>I write and maintain a few Discord bots. To ease the maintenance and deployment
of these critters, I’ve created a base that lets me share the same development
environment and automatically generate a NixOS module for all of them, with
customization and extension points whenever necessary. I’m interested in writing
some detailed case studies on how I accomplished this, but this post is getting
really long, and we still aren’t finished!</p>
<p>For now, here are some resources that I personally recommend for perusal should
you be interested:</p>
<ol>
<li><a href="https://zero-to-nix.com/">Zero to Nix</a>: New kid on the block; gentle and
more detailed introduction to using Nix in practice.</li>
<li><a href="https://www.youtube.com/watch?v=6iVXaqUfHi4">“Nix: What Even is it Though”</a>,
a presentation by Burke Libbey.</li>
<li><a href="https://nixos.org/guides/nix-pills/">Nix Pills</a>: Learn Nix from the bottom
up.</li>
<li>The
<a href="https://nixos.org/manual/nix/stable/introduction.html">Nix Reference Manual</a></li>
<li><a href="https://nixos.org/manual/nixpkgs/stable/">nixpkgs manual</a> and
<a href="https://nixos.org/manual/nixos/stable/">NixOS manual</a>: The official
documentation for those projects.</li>
<li><a href="https://nix.dev/">nix.dev</a>: An opinionated handbook and “survival guide”.</li>
<li><a href="https://search.nixos.org/packages">NixOS Search</a>: An invaluable tool for
quickly looking up packages and NixOS options. Keep this on speed dial.</li>
<li><a href="https://noogle.dev/">noogle</a> eases the looking up of library functions.</li>
</ol>
<p>When docs aren’t amazing, learning becomes akin to patchwork: you slowly and
incrementally stitch squares into your quilt until you reach a certain point
where everything becomes clear, and you end up with something nice and soft to
keep you warm during winter.</p>
<h2 id="rougher-edges">Rougher edges</h2>
<p>Every technology involves a set of tradeoffs, and Nix is no exception.</p>
<p>One particular pain point is managing secrets: the Nix store is world-readable,
and this includes any configuration files that may or may not contain secret
keys and passwords. This problem is tackled via projects such as <a href="https://github.com/ryantm/agenix">agenix</a> and
others, but keep this in mind when blueprinting your system. I’ve experimented
with committing my nixfiles with their encrypted secrets in public before, but
this ended up being such a massive pain that I gave up. Maybe I’ll try again
soon.</p>
<p>When it comes to libraries and scaffolding your own projects with Nix, it can be
hard to find what’s appropriate for use.</p>
<p>Two good examples: packaging Rust code, and deploying NixOS. I used to wield
<a href="https://github.com/nix-community/naersk">Naersk</a> to blast my Rust packages with the Nix beam, but was later recommended
<a href="https://crane.dev/">Crane</a> by a friend, which seems to require less ceremony. I never would’ve
found out about it otherwise. nixpkgs also seems to have built-in support for
Rust via <code>buildRustPackage</code>. I’m not sure which one is the best to use; it’s
unclear to me, but I’ve settled on Crane for now.</p>
<figure><img alt="A screenshot of a successful colmena command invocation." title="A screenshot of a successful colmena command invocation." loading="lazy" width="550" height="102" decoding="async" data-nimg="1" sizes="(max-width: 800px) 90vw, (min-width: 800px) 570px" srcset="https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcolmena.2ebc19dc.png&amp;w=640&amp;q=75 640w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcolmena.2ebc19dc.png&amp;w=750&amp;q=75 750w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcolmena.2ebc19dc.png&amp;w=828&amp;q=75 828w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcolmena.2ebc19dc.png&amp;w=1080&amp;q=75 1080w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcolmena.2ebc19dc.png&amp;w=1200&amp;q=75 1200w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcolmena.2ebc19dc.png&amp;w=1920&amp;q=75 1920w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcolmena.2ebc19dc.png&amp;w=2048&amp;q=75 2048w, https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcolmena.2ebc19dc.png&amp;w=3840&amp;q=75 3840w" src="https://www.slice.zone/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcolmena.2ebc19dc.png&amp;w=3840&amp;q=75"><p>Build and apply your Nix config with a single command.</p></figure>
<p>Deployment to other servers from your own box is peak comfort. There are many
Nix deployment solutions, but I’ve settled on <a href="https://github.com/zhaofengli/colmena">Colmena</a>, because it supports
macOS and Flakes. There’s also <a href="https://github.com/rapenne-s/bento">Bento</a> (which tries to keep it simple), as well
as <a href="https://github.com/DBCDK/morph">Morph</a> and <a href="https://github.com/NixOS/nixops">nixops</a>. And there’s probably more I’m missing. There’s a lot of
choice here.</p>
<p>I run into a similar feeling when I write Nix code myself. The documentation
story here could use some improvement: finding the right function to use can
feel like navigating a dense jungle with a machete. I feel like reading other
people’s Nix code is one of the most effective ways to learn Nix, because you
get to see how its used (and how people solve their specific problems) in
practice.</p>
<p><a href="https://github.com/features/code-search/">GitHub’s code search</a> is an
invaluable tool here, as well as poking around in nixpkgs and asking questions
in community channels. A lot of learning Nix is done hands-on; you’ll need to
throw things at a wall to see what sticks, and that’s going to be frustrating at
first.</p>
<h2 id="the-nature-of-nix">The nature of Nix</h2>
<p>Nix is infectious, which can be both good and bad. Having a universal,
omnipresent existence on your system is what enables Nix’s niceties, but also
can make it frustrating to just <em>do things</em>.</p>
<p>You can’t follow <code>README</code> instructions verbatim anymore: if you want to use
something, it has to be packaged by Nix. Or, at least, it should. If there
happens to be a Nix package maintained by someone in nixpkgs, great! If not, you
better be in a mood to write some Nix code.</p>
<p>My dotfiles are Nix-managed, too. This means that my entire user environment is
easily reproducible everywhere Nix is available. However, this means that I need
to rebuild my user package every time I make a change, because the
configurations need to be built, copied into the Nix store, and then symlinked
into my own home directory.</p>
<p>Any change I make—big or small—has to go through Nix before programs take
notice, which can be annoying.</p>
<p>Fortunately, you aren’t forced to adopt Nix to such a high degree. You can even
install it on other Linux distributions, since it’s just a package manager. By
doing this, you get to pick and choose what falls under Nix’s reign. The more
you decide to use, though, the more you’ll have to zap things with the Nix ray.</p>
<p>Take black box binaries, for example: tarballs of pre-built blobs, sans source
code, that you “just” have to unpack and run. Nix inherently doesn’t play well
with these, and that can make certain things harder to do (see: games).</p>
<p>If patching the binaries in question is viable, <a href="https://nixos.org/manual/nixpkgs/stable/#setup-hook-autopatchelfhook">patchelf</a> can come to the
rescue. A friend of mine who is trying out NixOS is using it to patch binaries
in order to run Garry’s Mod servers, which are able to interface with external
3rd-party plugins via <code>dlsym</code> and other runtime loading.</p>
<p>When patching is unfeasible, such as programs that perform integrity checking or
simply make too many assumptions about the outside world,
<a href="https://nixos.org/manual/nixpkgs/stable/#sec-fhs-environments"><code>buildFHSUserEnv</code></a> enables you to run lightweight sandboxes
that are compatible with the Filesystem Hierarchy Standard (<code>/usr/lib</code> and
friends), made possible through Linux namespaces. This is done to <a href="https://github.com/NixOS/nixpkgs/tree/nixos-22.11/pkgs/games/steam">support
Steam</a>, which is pretty damn clever.</p>
<p>Steam has always felt pretty fragile to me. For example, it requires its own
bespoke set of libraries that it downloads and maintains outside of the system
package manager. This sucks, but it’s cool that it was possible for Nix to shove
it into a reproducible box of sorts that basically can’t break (let’s hope I
don’t jinx it).</p>
<p>When packaging other software that we have a reduced amount of control over,
things can get a little wonky. When I installed NixOS on my Mac, I ran into an
issue where <a href="https://github.com/ArmCord/ArmCord">ArmCord</a>—a neat little project that wraps Discord natively for
ARM devices—wouldn’t open links in my web browser.</p>
<p>After some poking around with Chromium’s logging levels, I eventually got it to
spew out enough debug information to see why <code>xdg-open</code> (which is what is being
spawned to open my web browser) was failing:</p>
<pre><code>XPCOMGlueLoad error for file /nix/store/5mndwvvbdz07kllj6bs0pp1n82cx260i-firefox-110.0.1/lib/firefox/libxul.so:
/nix/store/p9ggv8qkdv0s7pckz2xkxxs68ras07g3-nss-3.79.4/lib/libssl3.so: version `NSS_3.80' not found (required by /nix/store/5mndwvvbdz07kllj6bs0pp1n82cx260i-firefox-110.0.1/lib/firefox/libxul.so)
Couldn't load XPCOM.
/home/slice/.nix-profile/bin/xdg-open: line 881: x-www-browser: command not found
XPCOMGlueLoad error for file /nix/store/5mndwvvbdz07kllj6bs0pp1n82cx260i-firefox-110.0.1/lib/firefox/libxul.so:
/nix/store/p9ggv8qkdv0s7pckz2xkxxs68ras07g3-nss-3.79.4/lib/libssl3.so: version `NSS_3.80' not found (required by /nix/store/5mndwvvbdz07kllj6bs0pp1n82cx260i-firefox-110.0.1/lib/firefox/libxul.so)
Couldn't load XPCOM.
</code></pre>
<p>To my surprise, there were… linking errors going on! Firefox seemed to be
loading the incorrect version of OpenSSL for some reason. I thought Nix was
supposed to be reproducible, self-contained, etc. etc. This shouldn’t be
happening!</p>
<p>More investigation led to the realization that the ArmCord package actually
<a href="https://github.com/NixOS/nixpkgs/blob/cd749f58ba83f7155b7062dd49d08e5e47e44d50/pkgs/applications/networking/instant-messengers/armcord/default.nix#L112">injects <code>LD_LIBRARY_PATH</code> at runtime</a> so that it can find the
right libraries—needed because the package reuses the <code>.deb</code> binaries—and it
was bleeding into the <code>xdg-open</code> subprocess that was being spawned. Whoops.</p>
<p>I was able to fix this by <a href="https://github.com/ArmCord/ArmCord/issues/354#issuecomment-1480432789">wrapping <code>xdg-open</code> with a pristine script that unset
<code>LD_LIBRARY_PATH</code>, and overriding the ArmCord package to add a wrapper that
invoked the main binary with the pristine script prepended to <code>PATH</code> before
anything else</a>. Then, when <code>xdg-open</code> was spawned by Chromium,
<code>LD_LIBRARY_PATH</code> would no longer be clobbered.</p>
<p>In this situation, overriding the package definition here was enough to solve my
problem, but that might not always be the case: NixOS modules, and Nix packages
in general, are only as flexible as they are written to be<sup><a href="#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="true" aria-describedby="footnote-label">7</a></sup>. So far, I’ve
never had to straight-up fork a package or NixOS module definition, but I’m
definitely not ruling that out from ever happening.</p>
<p>Luckily, there tends to be a lot of escape hatches: nixpkgs is very configurable
and extensible with support for overlays, overrides, and source code patches.
It’s also totally possible to do something such as creating a package that
solely takes the output of another package and patches it in a certain complex
way (however you want!)</p>
<p>Lastly, I’d like to reiterate that because Nix is a lazily evaluated (and
dynamically typed) language, it’s very possible to run into fairly cryptic
errors often, especially when recursion is involved. Lazy evaluation means that
it’s possible to introduce a ticking time bomb that can be detonated from
seemingly unrelated code.</p>
<p>While this can be a pain, it’s also what enables Nix to avoid unnecessary
computation: the root of nixpkgs is a huge tree of every package contained
within the repository, but only those that are actually needed are ever
evaluated. Deferring evaluation to the last possible moment is what makes things
such as overrides and overlays possible.</p>
<h2 id="bottom-line">Bottom line</h2>
<p>Nix is quirky, unique, and a little rough around the edges. Debugging it, like a
lot of other things, is frustrating. But the benefits I get from using it
currently outweigh the disadvantages, providing enough incentive for me to keep
on investing in it.</p>
<p>What I find to be most useful for me is declarative system management via NixOS.
My memory span is virtually nonexistent nowadays, and having a way to declare my
servers completely idempotently is an incredible way to avoid confusion and
stress. I like having that philosophy extended to my personal projects and user
environment, as it ensures a level of consistency and reproducibility that I
haven’t found anywhere else.</p>
<hr>
<section data-footnotes="true">
<ol>
<li id="user-content-fn-1">
<p>Made possible via <a href="https://asahilinux.org/">the Asahi Linux project</a> and
<a href="https://github.com/tpwrules/nixos-apple-silicon">this effort, maintained by someone who I can only describe as a heaven-sent angel</a>. <a href="#user-content-fnref-1" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>I’ve built ffmpeg from source sans Nix and it wasn’t actually that hard, but
what’s actually cool here is that that exact build configuration (ffmpeg
with that non-free library) is available across all machines with my user
environment—so I only had to figure it out once. <a href="#user-content-fnref-2" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>Look, recursion! And yes, I’m referring to all of the other items in this
list. <a href="#user-content-fnref-3" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-4">
<p>I’m stealing this quote from Burke Libbey, who has made
<a href="https://www.youtube.com/watch?v=6iVXaqUfHi4">a great presentation</a>
explaining what Nix actually is at a high level. I recommend you watch it if
you’re interested. <a href="#user-content-fnref-4" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-5">
<p>Hey, gotta cover all bases. <a href="#user-content-fnref-5" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-6">
<p>Apparently this is what <code>0.0.0.0</code> is named. <a href="#user-content-fnref-6" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-7">
<p>This was pointed out to me in a talk by
<a href="https://xeiaso.net/talks/nixos-pain-2021-11-10">Xe Iaso</a>, which is
definitely worth a watch and or read if you’re interested in Nix’s rougher
edges. They point out a lot of the same roadblocks that I ran into
personally. <a href="#user-content-fnref-7" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
</ol>
</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Win-Vind: Vim powers with speed of thought throughout Windows 11 (200 pts)]]></title>
            <link>https://pit-ray.github.io/win-vind/</link>
            <guid>38236684</guid>
            <pubDate>Sun, 12 Nov 2023 02:32:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pit-ray.github.io/win-vind/">https://pit-ray.github.io/win-vind/</a>, See on <a href="https://news.ycombinator.com/item?id=38236684">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <main>
        















    <h2 id="what-is-this">What is this?</h2>
<p><strong>win-vind</strong> provides a lightweight hybrid UI system of CUI and GUI for Windows.<br>
By installing this tool, you will be able to control the Windows GUI in the same way as Vim.</p>

<h3 id="1-vim-user-friendly">1. Vim-user friendly.</h3>
<p>All configuration methods and mode concepts are derived from Vim, allowing for Vim-like UI operation.<br>
Vim users only need to understand win-vind’s macro features and additional mode concepts with little learning cost.</p>

<h3 id="2-there-are-many-useful-built-in-commands">2. There are many useful built-in commands.</h3>
<p>You don’t have to worry about complicated scripts and dependencies like with traditional key binding tools.<br>
You can freely create user-defined commands by combining low-level optimized built-in commands.</p>

<h3 id="3-very-portable-and-fully-open-source">3. Very portable and fully open source.</h3>
<p>win-vind is a small single binary with no dependencies that runs with user permissions. <br>
It is also available from the command line as a command for UI operations like <code>$ win-vind -c "ggyyGp"</code>.</p>

<h3 id="top-feature-demo">Top Feature Demo</h3>

<video src="https://user-images.githubusercontent.com/42631247/215270933-3365065b-53db-4eca-9fc6-cd03d13e5ab0.mp4" controls=""></video>

<h3 id="configuration-file-example">Configuration File Example</h3>

<p>You can configure it in .vimrc style. What you can do in .vindrc is switch options, set parameters, remap low-level keys, and define function bindings.</p>

<div><pre><code><span>" options</span>
<span>set</span> <span>shell</span> <span>=</span> cmd
<span>set</span> cmd_fontsize <span>=</span> <span>14</span>
<span>set</span> cmd_fontname <span>=</span> Consolas
<span>set</span> blockstylecaret
<span>set</span> blockstylecaret_mode <span>=</span> solid

<span>" bindings</span>
imap <span>&lt;</span>capslock<span>&gt;</span> <span>{&lt;</span>ctrl<span>&gt;}</span>

inoremap <span>&lt;</span>ralt<span>&gt;&lt;</span>ralt<span>&gt;</span> <span>&lt;</span>easyclick<span>&gt;&lt;</span>click_left<span>&gt;</span>
inoremap <span>&lt;</span>rctrl<span>&gt;&lt;</span>rctrl<span>&gt;</span> <span>&lt;</span>gridmove<span>&gt;&lt;</span>click_left<span>&gt;</span>
inoremap <span>&lt;</span><span>win</span><span>-</span>enter<span>&gt;</span> <span>&lt;</span>window_resizer<span>&gt;</span>

<span>noremap</span> <span>&lt;</span>ctrl<span>-1</span><span>&gt;</span> <span>:!</span> <span>gvim</span><span>&lt;</span><span>cr</span><span>&gt;</span>
<span>noremap</span> <span>&lt;</span>ctrl<span>-2</span><span>&gt;</span> <span>:</span><span>e</span> http<span>:</span><span>//</span>example<span>.</span><span>com</span><span>&lt;</span><span>cr</span><span>&gt;</span>

enoremap <span>t</span> ggyyGp

autocmd AppLeave * <span>&lt;</span>to_insert<span>&gt;</span>
autocmd AppEnter<span>,</span>EdiNormalEnter <span>vim</span><span>.</span>exe <span>&lt;</span>to_resident<span>&gt;</span>
</code></pre></div>

<h2 id="license">License</h2>
<p>This software is provided by <a href="https://github.com/pit-ray/win-vind/blob/master/LICENSE.txt">MIT License</a>.</p>





    </main>

    
    
    
    

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rats have an imagination, new research suggests (103 pts)]]></title>
            <link>https://www.sciencedaily.com/releases/2023/11/231102162557.htm</link>
            <guid>38235745</guid>
            <pubDate>Sat, 11 Nov 2023 23:43:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencedaily.com/releases/2023/11/231102162557.htm">https://www.sciencedaily.com/releases/2023/11/231102162557.htm</a>, See on <a href="https://news.ycombinator.com/item?id=38235745">Hacker News</a></p>
Couldn't get https://www.sciencedaily.com/releases/2023/11/231102162557.htm: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[It's OK if your code is just good enough (106 pts)]]></title>
            <link>https://shiftmag.dev/code-quality-good-enough-2034/</link>
            <guid>38234653</guid>
            <pubDate>Sat, 11 Nov 2023 21:20:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shiftmag.dev/code-quality-good-enough-2034/">https://shiftmag.dev/code-quality-good-enough-2034/</a>, See on <a href="https://news.ycombinator.com/item?id=38234653">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>
      Good enough code is a nice middle ground between implementing a feature fast and maintaining the code quality.    </p>
  </div><div>
          <figure><img fetchpriority="high" decoding="async" width="1200" height="630" src="https://shiftmag.dev/wp-content/uploads/2023/11/Code-quality.png?x35993" alt="" srcset="https://shiftmag.dev/wp-content/uploads/2023/11/Code-quality.png 1200w, https://shiftmag.dev/wp-content/uploads/2023/11/Code-quality-300x158.png 300w, https://shiftmag.dev/wp-content/uploads/2023/11/Code-quality-1024x538.png 1024w, https://shiftmag.dev/wp-content/uploads/2023/11/Code-quality-768x403.png 768w" sizes="(max-width: 1200px) 100vw, 1200px"></figure>


<p>I’ve heard a story about a student who had to write an essay to pass the exam. He had a three-month deadline. During those months he struggled a lot<strong> rewriting pages again and again</strong>. Every now and then the professor checked on him and he would always respond with “I’m not done yet, it’s simply not good enough”. </p>



<p>Finally, the professor suggested: “Write the essay as if it would be graded <em>good enough</em>“. As much as this approach was strange to him, he took the advice. Surprisingly, he <strong>finished the essay in a whip </strong>earning a <strong>higher grade than expected.</strong></p>



<p>Where I come from, <em>good enough</em> would be a “three”, simply an average grade on a 1 to 5 scale. Solid, acceptable, good.</p>



<h2><span id="the-five-shades-of-code-quality">The five shades of code quality</span></h2>



<p>We, as software developers, not only write essays but dissertations to bring our pile of zeros and ones to life. <strong>Most of us would want our code to be 5 out of 5.</strong> But is this something worth pursuing? Let’s explore five shades of code quality:</p>



<p>1 – Shit</p>



<p>2 – Proof of concept</p>



<p>3 – Good enough</p>



<p>4 – Very good&nbsp;</p>



<p>5 – Perfection</p>



<h2><span id="grade-1-shit">Grade 1: Shit</span></h2>



<p>This is the type of code that makes you sweat. Your heart sinks when you hear about a new requirement. You know it will take up ages to implement it. The best thing you can do is to throw everything away and rewrite it. Such code<strong> shouldn’t be acceptable</strong>, we shouldn’t ship shit.</p>



<h2><span id="grade-2-poc">Grade 2: PoC</span></h2>



<p>Typical PoC code. You know that this code is <strong>not good but you can live with it for a while</strong>.&nbsp;</p>



<p>Usual problems:</p>



<ul>
<li>doesn’t have clear architecture, clear boundaries</li>



<li>many things are tightly coupled</li>



<li>many edge cases are not covered</li>



<li>missing validations</li>



<li>inadequate domain object modeling</li>



<li>too complex or too tiny test suite</li>



<li>code is not clean</li>
</ul>



<p>While this may not sound ideal, it can serve to <strong>quickly determine the direction you want to go</strong>. You can move really fast and iterate as you go. This code quality is particularly suitable for PoC initiatives.</p>





<h2><span id="grade-3-good-enough">Grade 3: Good enough</span></h2>



<p>Good enough code is a nice middle ground between implementing a feature fast and maintaining the code quality.</p>



<p>It addresses many of the issues typically found in grade 2 code, with some exceptions:</p>



<ul>
<li>some unnecessary levels of abstraction</li>



<li>some unclear naming</li>



<li>few larger functions or classes (nothing too big)</li>



<li>misuse of exceptions here and there</li>



<li>some code duplications</li>



<li>redundant commenting</li>



<li>readability issues in tests</li>
</ul>



<p>This code is suited for most applications. It’s probably the best for those dealing with CRUD operations. You can also achieve good results using this approach on components involved in traffic flow.&nbsp;</p>



<h2><span id="grade-4-very-good">Grade 4: Very good</span></h2>



<div><p>Grade 4 is a <strong>highly maintainable clean code paradise</strong>. Doesn’t have any problems that <em>good enough</em> has. </p><p>Take a look at the&nbsp;<a href="https://github.com/infobip/infobip-spring-data-querydsl" target="_blank" rel="noreferrer noopener">infobip-spring-data-querydsl&nbsp;</a>library. <strong>Although it sounds perfect, it’s not.</strong> Everything is by the book, yet you may not like it, or dislike some parts of it. For example, you may find interfaces to be too generic, or you might be bothered with the usage of primitives instead of objects or something else. There will always be something to dislike and that is ok. :D.&nbsp;</p></div>



<p>The problem with this grade is that<strong> it is difficult to achieve.</strong> We have different backgrounds and views about how a really good code should look like.</p>



<p>I’ve worked with a freelancing company whose code was on this level. In order to achieve it, they enforced <strong>highly strict static code analysis</strong> and <strong>really challenging code reviews</strong>. Static code analysis was the cornerstone to stop most of the common problems from&nbsp;<em>good enough</em> entering the codebase. You can view them&nbsp;<a href="https://www.elegantobjects.org/" target="_blank" rel="noreferrer noopener">here</a>&nbsp;– under principles. Code review consisted of <strong>two reviewers</strong>, <strong>one developer and an architect, plus a QA </strong>person who would check the quality of the review.&nbsp;</p>



<p>If you want to tackle with grade 4 you need to <strong>build infrastructure for it</strong> and have everyone onboard with it. Aiming for <strong>this grade will slow you down</strong> so it should be taken into consideration when doing estimations with PD. Clear benefits will be seen in the long run.</p>



<p>This grade might be suited for building libraries that would be used by multiple projects/teams or when building critical parts of a system.</p>



<h2><span id="grade-5-perfection">Grade 5: Perfection</span></h2>



<p>Doesn’t exist.</p>



<h2><span id="good-enough-is-the-way-to-go">Good enough is the way to go</span></h2>



<p>Daily we are faced with different requirements, different deadlines, scopes, and so on. They are, of course, not always quick and simple to solve but most of the time,&nbsp;the <em>good enough</em>&nbsp;approach is the way to go. </p>



<p>To me, the code that has a clear architecture, understandable names of services, and good tests ticks all the boxes. And I like it! It doesn’t have to be a clean-code candy. If you prefer candies, then you have to convince PD and your team that this extra sugar will benefit the project.</p>



<p>What would you say, what would be your preferred coding approach?</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OCaml: a Rust developer's first impressions (168 pts)]]></title>
            <link>https://pthorpe92.github.io/ocaml/ocaml-first-thoughts/</link>
            <guid>38234580</guid>
            <pubDate>Sat, 11 Nov 2023 21:12:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pthorpe92.github.io/ocaml/ocaml-first-thoughts/">https://pthorpe92.github.io/ocaml/ocaml-first-thoughts/</a>, See on <a href="https://news.ycombinator.com/item?id=38234580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" role="main">
    
      
    
    <h3 id="ocaml-first-impressions">OCaml: First impressions</h3>

<h4 id="the-why">The Why</h4>

<p>I’m somewhat of a language nerd to begin with, and it was Rust that originally got me interested in the whole functional paradigm.
Not due to it’s lineage, but the heavy use of chained iterator methods in favor over traditional loops. This is one of the more intimidating hurdles
for newcomers to the language, but after getting used to it, rarely will you see anyone write a <code>for</code> loop again.</p>

<p><strong>Rant:</strong> The argument can be made that what I’m talking about is more similar to Javascript-land where a bunch of pseudo-functional methods and terminology are 
used, but it in reality is as far away from functional programming as <code>useState()</code> would imply it is. 
At least Rust has a few things to back it up, as at least a language that will support you should you desire to write pure functional code. I would argue that for beginners, Rust will simply force you to write functional, immutable code, whether they really understand it or otherwise, as the amount of <code>.clone()</code>‘ing required for a newcomer to compile a non-trivial program will practically almost guarantee that nothing was actually <code>&amp;mut</code>ated*.</p>

<p>So this got me interested in watching some <em>GOTO</em>, <em>strangeloop</em>, and other conference talks about functional programming, and upon discovering @artemslab
(definitely go check it out, awesome content) on YouTube and watching some in-depth videos about Ocaml and it’s runtime integer representations and memory
allocation, that really got me fascinated with the language.</p>

<p>In the past, I typically wouldn’t hesitate to <code>sudo pacman -Sy </code> whatever compiler or interpreter I needed, to get started playing around with a new language I was interested in. However since getting full-time employment, on top of the existing OSS stuff,
as well as doing contracted gigs for the facility I’m in, I haven’t had so much as a moment to spend on anything un-related to these things in quite some time.</p>

<p>Anyways, as usual I’m off track here. My point is that I <em>thought</em> I had a good introduction to the world of FP, and I <em>thought</em> since I’d heard that many of the
features and some of the syntax we love from Rust came directly from tha ‘Caml itself, I thought I would be love it and picking it up would be second nature.</p>

<p>My plan was to do the entire Advent of Code in Ocaml, and I was also going to pick an ambitious project that I would otherwise use Rust for, and force myself
to use Ocaml. I find this is the best way to learn a language, is just take on a project that is significantly more ambitious than your current skill level, and by thetime you finish it, you’ll have not only saved yourself a bunch of wasted time on tutorials, but you’ll have made something cool (hopefully).</p>

<h3 id="i-have-two-gripes-with-the-language-so-far">I have two gripes with the language (so far)..</h3>

<h3 id="where-are-the-types-">Where are the types? 👀</h3>

<p>I underestimated how much I would be thrown off by the fact that the types are <em>heavily</em> inferred, and are almost never explicitly declared, from what I can tell. I found that the combination of this and the strange syntax really screws me up, as now I am having problems inferring my <em>own</em> types at this point…</p>

<p>When dealing with lots of recursive functions, you find yourself looking up often at the function signature to view the parameters, but the concise syntax makes it difficult to mentally parse quickly.</p>

<p>Example, random code snippet from ocaml.org:</p>
<pre><code>  let group list sizes =
    let initial = List.map (fun size -&gt; size, []) sizes in

  let prepend p list =
    let emit l acc = l :: acc in
    let rec aux emit acc = function
      | [] -&gt; emit [] acc
      | (n, l) as h :: t -&gt;
         let acc = if n &gt; 0 then emit ((n - 1, p :: l) :: t) acc
                   else acc in
         aux (fun l acc -&gt; emit (h :: l) acc) acc t
    in
    aux emit [] list
  in
  let rec aux = function
    | [] -&gt; [initial]
    | h :: t -&gt; List.concat_map (prepend h) (aux t)
  in
  let all = aux list in
  let complete = List.filter (List.for_all (fun (x, _) -&gt; x = 0)) all in
    List.map (List.map snd) complete;;
val group : 'a list -&gt; int list -&gt; 'a list list list = &lt;fun&gt;
;;
</code></pre>
<p>Even when you learn what it actually going on here, and it makes immediate sense to you.. It must take a while before it becomes easier to read.</p>

<h3 id="remember-recursion-how-about-linked-lists">Remember recursion? How about linked lists?</h3>

<p>Two things every Computer science student learns, and then are immediately told to never use again*…</p>

<p>Anything that can be done recursively can be done iteratively (and most optimizing compilers will replace your recursion anyway), and linked lists are slow and inefficient with modern CPU caches, and you should almost never use them (and if you’re a web dev, well, you just wouldn’t anyway).</p>

<p>You could describe Ocaml as a language that relies <strong>heavily</strong> on <em>both</em> of these things. This feels even more strange that it might seem it would, and I have
found myself stumbling on absolutely remedial tasks.</p>

<p>Yes, there are libraries with other data structures, and there are in fact <code>for</code> loops (and even <code>while</code> loops), However I feel like I will end up learning more of the language and find that iterator methods will be used more so than any of these, much as in Rust.</p>

<h2 id="the-good">The Good:</h2>

<ul>
  <li>
    <p>Thought provoking. If you don’t believe there is anything to gain from being forced to think about something you do every day in a completely different way… well I’m surprised you made it this far into the post.</p>
  </li>
  <li>
    <p>Awesome mixture of !significant-whitespace and !brackets. And how cool is the comment syntax? (* doesn’t it just feel like.. middle-eastern-ish *);;</p>
  </li>
  <li>
    <p>Tooling: is way better than it has any right to be, considering the size of it’s user-base and community.</p>
  </li>
  <li>
    <p>Great conversation starter, really cool and helpful community.</p>
  </li>
</ul>

<p>Lots more to learn for sure but these were my initial thoughts anyway, I do really want to like it so we will see how much that helps. I just definitely did not get the head-start I was expecting, but I don’t think that’s a negative thing it just means there is more to learn.</p>

<p>*obviously I know this isn’t true and an extreme exaggeration, but I thought it was funny. There’s always someone who will ‘well ackshually’ you when 
you think it’s obvious.</p>


<ul>
  
  
    <li>
      <a href="#2023">
        <strong>2023</strong> <span>3</span>
      </a>
    </li>
  
</ul>




  <section id="2023">
    <h2>2023</h2>
    
    <a href="#page-title">Back to Top ↑</a>
  </section>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CTF Writeup: Abusing select() to factor RSA (129 pts)]]></title>
            <link>https://threadreaderapp.com/thread/1723398619313603068.html</link>
            <guid>38233938</guid>
            <pubDate>Sat, 11 Nov 2023 19:59:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://threadreaderapp.com/thread/1723398619313603068.html">https://threadreaderapp.com/thread/1723398619313603068.html</a>, See on <a href="https://news.ycombinator.com/item?id=38233938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-controller="mentions">

<div>
<p><a href="https://threadreaderapp.com/user/moyix"><img src="https://pbs.twimg.com/profile_images/851124181995589638/67s_2qnD_bigger.jpg" alt="Brendan Dolan-Gavitt Profile picture" data-controller="twitter-profile" data-twtrid="15194897" data-action="error->twitter-profile#error"></a>
</p>

</div> 
<p>
Will still try to do a blog post on my @CSAW_NYUTandon CTF challenge, NERV Center, but for now here's a thread explaining the key mechanics. I put a lot of work into the aesthetics, like this easter egg credit sequence (all ANSI colors+unicode text) that contains key hints: <span><video controls="" poster="https://pbs.twimg.com/ext_tw_video_thumb/1723398329193603073/pu/img/BOjilHxex46DHgNz.jpg"><source src="https://video.twimg.com/ext_tw_video/1723398329193603073/pu/vid/avc1/320x270/T7o7Rg69CJYAflcu.mp4?tag=12" type="video/mp4"><br>
<source src="https://video.twimg.com/ext_tw_video/1723398329193603073/pu/vid/avc1/426x360/hDsE9J-Tm7U449X_.mp4?tag=12" type="video/mp4"><br>
<source src="https://video.twimg.com/ext_tw_video/1723398329193603073/pu/vid/avc1/854x720/mTfO5bxXSMMK00GR.mp4?tag=12" type="video/mp4"><br>
<source src="https://video.twimg.com/ext_tw_video/1723398329193603073/pu/pl/Dt3lV1ZNZ4_XzXfS.m3u8?tag=12&amp;container=fmp4" type="application/x-mpegURL"></video></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon (Note the karaoke subtitles timed to the credits at the bottom 😁)
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon First, the vulnerability. If you read the man page for select(), you'll see this warning: select() is limited to monitoring file descriptors numbered less than 1024. But modern systems can have many more open files, and importantly the kernel select() interface is NOT limited. <span><a href="https://pbs.twimg.com/media/F-q-tpjXwAAKWQk.jpg" target="_blank"><img alt="DESCRIPTION  WARNING: select() can monitor only file descriptors numbers  that  are  less than  FD_SETSIZE  (1024)—an  unreasonably low limit for many modern applications—and this limitation will not change.  All modern  applications  should instead use poll(2) or epoll(7), which do not suffer this limitation." src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-q-tpjXwAAKWQk.jpg"></a></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon But IMO the man page understates the severity of the problem. So what happens if you *do* try to monitor fds higher than 1024? Well, the fd_set struct is just a bitset of 1024 bits (128 bytes). So attempting to monitor fds larger than 1024 will cause memory corruption!
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon And the really cool part (to me) is that a) the corruption is bitwise, since the data structure is a bitset, and b) the exact bits that will be written out of bounds depend on the *state* of the file descriptors being monitored.
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon So if the fds correspond to, say, network connections, an attacker can make a large number of connections, arrange for them to be in just the right state, and thereby get precise control over the bit pattern that gets written.
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon (Side note: I discovered this because very old versions of QEMU—including the one used by our PANDA 1.0-based malware sandbox—had this bug and guests could trigger it in the SLIRP user mode networking by making a large number of connections. Yikes! )<a data-preview="true" href="https://bugzilla.redhat.com/show_bug.cgi?id=892977">bugzilla.redhat.com/show_bug.cgi?i…</a>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon select() takes up to three bitsets: readfds, writefds, and exceptfds, so each is susceptible to this overflow. I decided to have the intended overflow occur on exceptfds.
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon Why? Two reasons: 1) it's third in the argument list so it naturally would be the last one listed in, say, a struct; 2) the fd states can be controlled more easily—with TCP connections, exceptfds is used to indicate connections with pending TCP out-of-band (OOB) data.
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon So now the basic exploit strategy is clear: make more than 1024 connections to the server, an send TCP OOB data to the ones with fds higher than 1024 to set your desired bit pattern. But exactly what should get overwritten?
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon That's when this challenge moved from just Pwn to Pwn+Crypto :D I decided to place an RSA public key used for ssh-style challenge/response auth right after the exceptfds bitset, so you can control the upper 64 bits using the vuln. <span><a href="https://pbs.twimg.com/media/F-rD9ZUXEAAySUm.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rD9ZUXEAAySUm.jpg"></a></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon What can you do with the ability to control the top 64 bits of a 1024-bit RSA public key N? Well, N = pq, which is hard to factor. But the *corrupted* N' is very likely to be a product of a bunch of smaller primes—which is easy to factor!
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon I first learned of this trick from the super cool USENIX Security 2016 paper Flip Feng Shui by Kaveh Razavi, @bjg et al., who used it in conjunction with RowHammer and memory deduplication to bypass SSH authentication. <a data-preview="true" href="https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/razavi">usenix.org/conference/use…</a><br>
<span><a href="https://pbs.twimg.com/media/F-rFbfPXgAA3Ezz.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rFbfPXgAA3Ezz.jpg"></a></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon (One of the players who solved the challenge during CSAW CTF finals actually found an even sweeter way to exploit this vuln: figure out a bit pattern that makes N *prime* and set that. Then the corresponding private key d is just: pow(65537, -1, n-1). Slick!)
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon Once you've factored the key (or made it prime) and figured out the secret key d for the corrupted public key, you can use it to forge signatures and bypass the authentication, allowing you to download the flag (encrypted with RSA+AES-256-GCM). <span><a href="https://pbs.twimg.com/media/F-rInsJXEAAuE0Q.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rInsJXEAAuE0Q.jpg"></a></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon Okay, but now on to the important stuff: ~aesthetics~. The theme of the challenge was based around Episode 13 of Neon Genesis Evangelion, which features an Angel that hacks NERV's MAGI supercomputer cluster.
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon I realized that three computers in the MAGI cluster (Casper, Balthasar, and Melchior) correspond neatly to the three fd_sets readfds, writefds, and exceptds. So I was able to make a cute UI that shows the state of the fd_sets in real-time. <span><video controls="" poster="https://pbs.twimg.com/ext_tw_video_thumb/1723411409579429888/pu/img/8Bjj0p8WU4HfW3jo.jpg"><source src="https://video.twimg.com/ext_tw_video/1723411409579429888/pu/vid/avc1/320x270/qYrDaeco3ZV7Csnc.mp4?tag=12" type="video/mp4"><br>
<source src="https://video.twimg.com/ext_tw_video/1723411409579429888/pu/vid/avc1/426x360/4XP5JkjnGojXyxTF.mp4?tag=12" type="video/mp4"><br>
<source src="https://video.twimg.com/ext_tw_video/1723411409579429888/pu/vid/avc1/854x720/nZLphAqzNrSxhGKy.mp4?tag=12" type="video/mp4"><br>
<source src="https://video.twimg.com/ext_tw_video/1723411409579429888/pu/pl/oCl-SK_b9xgSrZDR.m3u8?tag=12&amp;container=fmp4" type="application/x-mpegURL"></video></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon Aside from being my best attempt at recreating the UI from the show (pictured here), the UI elements also have some key clues to the vulnerability. <span><a href="https://pbs.twimg.com/media/F-rKhHpWsAAmxom.png" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rKhHpWsAAmxom.png"></a></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon - The CODE field shows the highest file descriptor in use, and the EXTENTION (sic) shows the server's max fd ulimit.<br>
- The box on the right turns red when the highest FD is &gt; 1024.<br>
- The UI gets slightly corrupted precisely when there is actual memory corruption happening. <span><a href="https://pbs.twimg.com/media/F-rMd-3XMAAn0aj.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rMd-3XMAAn0aj.jpg"></a></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon All of the visuals in the challenge were created using plain unicode characters and ANSI colors, so you can just cat the text files to see them (or dump them over a network socket). <span><video controls="" poster="https://pbs.twimg.com/ext_tw_video_thumb/1723415475378393089/pu/img/nUXZ6x6EkzpNC3jm.jpg"><source src="https://video.twimg.com/ext_tw_video/1723415475378393089/pu/vid/avc1/320x270/gcU1d77FxNPgoyFt.mp4?tag=12" type="video/mp4"><br>
<source src="https://video.twimg.com/ext_tw_video/1723415475378393089/pu/vid/avc1/426x360/hxKT8RRyQX9bRjUc.mp4?tag=12" type="video/mp4"><br>
<source src="https://video.twimg.com/ext_tw_video/1723415475378393089/pu/vid/avc1/854x720/QQVbqPP-j9C9iEkN.mp4?tag=12" type="video/mp4"><br>
<source src="https://video.twimg.com/ext_tw_video/1723415475378393089/pu/pl/0Tl_-kzEedv_y_ce.m3u8?tag=12&amp;container=fmp4" type="application/x-mpegURL"></video></span>
<sup><i></i></sup>
</p>
<div id="tweet_21" data-controller="thread" data-action="click->thread#showTweet" data-screenname="moyix" data-tweet="1723416020134564123" dir="auto"><p>
@CSAW_NYUTandon The credit sequence was made with some rather, uh, "rough and ready" techniques. <span><span><blockquote data-conversation="none" data-align="center" data-dnt="true"><a href="https://twitter.com/moyix/status/1723409266051051904">https://twitter.com/moyix/status/1723409266051051904</a></blockquote></span></span>
<sup><i></i></sup>
</p></div>
<p>
@CSAW_NYUTandon The code for credits generation is also available, though it is not what I would call production-quality <a data-preview="true" href="https://github.com/moyix/csaw23_nervcenter_credits">github.com/moyix/csaw23_n…</a>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon Finally, a bit on the mechanics of developing this challenge. First, it ended up being rather a lot of C code, so I was super worried about accidentally introducing an unintended vuln that would make the challenge boring. <span><a href="https://pbs.twimg.com/media/F-rPWU8WIAAVCx8.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rPWU8WIAAVCx8.jpg"></a></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon To guard against this I wrote a bunch of libfuzzer targets, network torture tests in Python, and traditional CTest unit tests. I think it worked! I didn't hear of anyone finding a vuln in the challenge except the one I intended. <br>
<span><span><span><a href="https://pbs.twimg.com/media/F-rQQcYWIAAvqP2.png" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rQQcYWIAAvqP2.png"></a></span></span><br>
<span><span><a href="https://pbs.twimg.com/media/F-rQpoqXIAAxFbT.png" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rQpoqXIAAxFbT.png"></a></span></span></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon Another bit that took some work was making the challenge consistently solvable. With select(), you fill the fd_set with 1s, call select, and the kernel fills the set with the actual fd state. But that means that 1/2 the time the key is corrupted by all 1s instead of your bits!
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon This makes exploitation annoying, to say the least. So I introduced a mechanism that lets players pause and resume the thread calling select(), so the corrupted key stays in place during factoring/signing/encrypting. <span><a href="https://pbs.twimg.com/media/F-rThOzXcAAPWzh.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rThOzXcAAPWzh.jpg"></a></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon Oh, a couple more bits of aesthetics. The sensor port (the one you actually use for the overflow, lets you look at some cards for the various angels: <br>
<span><span><span><a href="https://pbs.twimg.com/media/F-rUrwoWAAAD5kP.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rUrwoWAAAD5kP.jpg"></a></span></span><br>
<span><span><a href="https://pbs.twimg.com/media/F-rUzzCW4AA5yaK.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rUzzCW4AA5yaK.jpg"></a></span></span></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon That functionality also hides the easter egg: by calling EXAMINE on three angels in a row where the first letter of their names spell out "RSA" (e.g., Ramiel, Sandalphon, Adam), you can activate the credit sequence shown in the first tweet. <span><span><blockquote data-conversation="none" data-align="center" data-dnt="true"><a href="https://twitter.com/moyix/status/1723398619313603068">https://twitter.com/moyix/status/1723398619313603068</a></blockquote></span></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon And I think it's always fun to taunt the players a little bit. That's why when you fail the challenge-response authentication, Asuka shows up to make fun of you. (There are a few other images included for other errors, but they're harder to trigger.) <span><a href="https://pbs.twimg.com/media/F-rVjxLW0AAlWIQ.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rVjxLW0AAlWIQ.jpg"></a></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon I learned a few fun things while making this challenge. First, Python crypto libraries like pycryptodome do NOT enjoy working with prime or multi-prime keys. I used OpenSSL directly instead in my solver, but some players monkeypatched the Python library. <a data-preview="true" href="https://github.com/moyix/csaw23_nervcenter/tree/main/solver">github.com/moyix/csaw23_n…</a>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon Second, one variant of the challenge I tried ran into an interesting issue. I considered byte-swapping the key so that players could only set the LSB. But it turns out half of those keys are unusable, because OpenSSL uses Montgomery multiplication, which requires an odd modulus.
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon Tommaso Gagliardoni of Kudelski Security also suggested a variant on the core RSA overwrite that would also have been fun - allow the overwrite to make the key BIGGER by extending into adjacent data in memory. Sadly I didn't have time to implement this. <span><a href="https://pbs.twimg.com/media/F-rYbWoW8AA561k.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/F-rYbWoW8AA561k.jpg"></a></span>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon Okay this thread is now officially WAY too long, so I'll wrap up by saying that I had a great time writing the challenge, and I'm thrilled that it all came together and people enjoyed playing it! The challenge source and solver can be found here: <a data-preview="true" href="https://github.com/moyix/csaw23_nervcenter">github.com/moyix/csaw23_n…</a>
<sup><i></i></sup>
</p>
<p>
@CSAW_NYUTandon @threadreaderapp unroll
<sup><i></i></sup>
</p>
<p>• • •</p>
<p><span>
Missing some Tweet in this thread? You can try to
<a id="force-click" href="#" data-category="refresh" data-action="1723398619313603068">force a refresh</a>
</span>
</p>
　
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Outlook sends passwords, mails and other data to Microsoft (148 pts)]]></title>
            <link>https://mailbox.org/en/post/warning-new-outlook-sends-passwords-mails-and-other-data-to-microsoft?nl=e</link>
            <guid>38233694</guid>
            <pubDate>Sat, 11 Nov 2023 19:35:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mailbox.org/en/post/warning-new-outlook-sends-passwords-mails-and-other-data-to-microsoft?nl=e">https://mailbox.org/en/post/warning-new-outlook-sends-passwords-mails-and-other-data-to-microsoft?nl=e</a>, See on <a href="https://news.ycombinator.com/item?id=38233694">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        
  
      <div><p><a title="" href="https://www.heise.de/news/Microsoft-krallt-sich-Zugangsdaten-Achtung-vorm-neuen-Outlook-9357691.html" target="_blank" rel="noopener">"Microsoft steals access data"</a> - When the well-known German IT portal "Heise Online" uses such drastic words in its headline, then something is up. If Microsoft has its way, all Windows users will have to switch to the latest version of Microsoft Outlook. But: Not only can the IMAP and SMTP access data of your e-mail account be transferred to Microsoft, but all e-mails in the INBOX can also be copied to the Microsoft servers, even if you have your mailbox with a completely different provider such as mailbox.org.</p></div>
<h2>Main risk: Transferring your data to Microsoft "Synchronisation with the Microsoft server" - and everything is copied!</h2>
<p>If you set up a new account in the software, Microsoft offers a supposed security function: It says that non-Microsoft accounts are synchronised with the Microsoft cloud and that copies of "emails, calendars and contacts are therefore synchronised between your email provider and Microsoft data centres".</p>
<p>Anyone who reads this carefully may be perplexed, no question. But we all know how easy it is to agree to supposed banalities without reading them and to click away notices, especially when setting up software. In view of the drastic consequences of giving consent here, the warnings and explanations from Microsoft are probably too inconspicuous. Only a few users will realise that they are giving Microsoft comprehensive access to passwords, mail and more. Therefore, once again clearly:</p>
<p><strong>Microsoft gets full access to mails, calendars and contacts!</strong></p>
<div><p>But not only Windows users are at risk: Outlook versions for iOS, Mac and even Android are also affected, according to Heise.</p></div>
<h2>mailbox.org warns against using the new Microsoft Outlook</h2>
<p>mailbox.org warns its users: there is a high risk that sensitive data may be transmitted to Microsoft when using the new Outlook! And by the way: this compromised data includes not only emails, but also calendar and contact data.</p>
  
  
  

</div><p>For <strong>business customers</strong>, storing personal data in this way (albeit unintentionally) may constitute a GDPR offence that is subject to fines. After all, storing data in the Microsoft cloud legally constitutes data processing that requires the conclusion of an order data processing agreement (DPA) with Microsoft - and companies may have to identify this as such in their data protection declarations and in the data processing directory. It is irrelevant whether this is done intentionally by the company management or ultimately through the uninformed consent of an individual employee.</p><div>

        
  
      
<h2>Our recommendation</h2>
<p>Whether business or private: We strongly advise all our customers not to use the new Outlook! And we have the following alternatives for you:</p>
<ul>
<li><strong>Another e-mail client:</strong> We advise you to switch to the popular e-mail client "Thunderbird" on your computer. This is compatible with Windows and easy to set up. On mobile devices, there are a number of different IMAP mail clients, such as FairEmail and K9 Mail (which will also be called Thunderbird in the future).</li>
<li><strong>Using the webmailer:</strong> As a mailbox.org customer, you can use our secure webmail portal at any time, which offers an excellent alternative to desktop email clients. In addition to mail, calendar and contacts, you also have secure access to files and Office documents - and your personal video conference with OpenTalk is just a click away.</li>
</ul>
<p>We do everything we can to protect the security and privacy of your e-mail communication. But we also need your help: make sure you use apps from providers that respect and protect your privacy and security.</p>

<h2>Update</h2>
<p>The German Federal Commissioner for Data Protection and Freedom of Information, Ulrich Kelber, is also alarmed: On the <a title="social media network Mastodon" href="https://social.mailbox.org/@bfdi@social.bund.de/111381793879390891" target="_blank" rel="noopener">social media network Mastodon</a>, he described the data collection as "alarming" and announced his intention to pursue the issue at European level through the data protection authorities as early as next Tuesday.</p>  
  
  

</div></div>]]></description>
        </item>
    </channel>
</rss>