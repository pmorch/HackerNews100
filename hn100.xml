<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 11 Apr 2024 17:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Apple alerts users in 92 nations to mercenary spyware attacks (134 pts)]]></title>
            <link>https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/</link>
            <guid>40002987</guid>
            <pubDate>Thu, 11 Apr 2024 15:06:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/">https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/</a>, See on <a href="https://news.ycombinator.com/item?id=40002987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Apple sent threat notifications to iPhone users in 92 countries on Wednesday, warning them that they may have been targeted by mercenary spyware attacks.</p>
<p>The company said it sent the alerts to individuals in 92 nations at 12 p.m. Pacific Time Wednesday. The notification, which TechCrunch has seen, did not disclose the attackers‚Äô identities or the countries where users received notifications.</p>
<p>‚ÄúApple detected that you are being targeted by a mercenary spyware attack that is trying to remotely compromise the iPhone associated with your Apple ID -xxx-,‚Äù it wrote in the warning to affected customers.</p>
<p>‚ÄúThis attack is likely targeting you specifically because of who you are or what you do. Although it‚Äôs never possible to achieve absolute certainty when detecting such attacks, Apple has high confidence in this warning ‚Äî please take it seriously,‚Äù Apple added in the text.</p>
<p>The iPhone maker sends these kind of notifications <a href="https://techcrunch.com/2023/10/30/indian-opposition-leaders-says-apple-has-warned-them-of-state-sponsored-iphone-attacks/" target="_blank" rel="noopener">multiple times a year</a> and has notified users to such threats in over 150 countries since 2021, per an updated Apple <a href="https://support.apple.com/en-in/102174" target="_blank" rel="noopener">support page</a>.</p>
<p>Apple also sent an identical warning to a number of journalists and politicians in India in October last year. Later, nonprofit advocacy group Amnesty International <a href="https://techcrunch.com/2023/12/27/india-pressed-apple-on-state-sponsored-warnings-report-says/" target="_blank" rel="noopener">reported</a> that it had found Israeli spyware maker NSO Group‚Äôs invasive spyware Pegasus on the iPhones of prominent journalists in India. (Users in India are among those who have received Apple‚Äôs latest threat notifications, according to people familiar with the matter.)</p>
<p>The spyware alerts arrive at a time when many nations are preparing for elections. In recent months, many tech firms have cautioned about rising state-sponsored efforts to sway certain electoral outcomes. Apple‚Äôs alerts, however, did not remark on their timing.</p>
<p>‚ÄúWe are unable to provide more information about what caused us to send you this notification, as that may help mercenary spyware attackers adapt their behavior to evade detection in the future,‚Äù Apple told affected customers.</p>
<p>Apple <a href="https://web.archive.org/web/20240101053644/https://support.apple.com/en-in/102174" target="_blank" rel="noopener">previously</a> described the attackers as ‚Äústate-sponsored‚Äù but has replaced all such references with ‚Äúmercenary spyware attacks.‚Äù</p>
<p>The warning to customers adds: ‚ÄúMercenary spyware attacks, such as those using Pegasus from the NSO Group, are exceptionally rare and vastly more sophisticated than regular cybercriminal activity or consumer malware.‚Äù</p>
<p>Apple said it relies solely on ‚Äúinternal threat-intelligence information and investigations to detect such attacks.‚Äù</p>
<p>‚ÄúAlthough our investigations can never achieve absolute certainty, Apple threat notifications are high-confidence alerts that a user has been individually targeted by a mercenary spyware attack and should be taken very seriously,‚Äù it added.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anyone got a contact at OpenAI. They have a spider problem (281 pts)]]></title>
            <link>https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html</link>
            <guid>40001971</guid>
            <pubDate>Thu, 11 Apr 2024 13:34:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html">https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html</a>, See on <a href="https://news.ycombinator.com/item?id=40001971">Hacker News</a></p>
Couldn't get https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Transformers.js ‚Äì  Run Transformers directly in the browser (156 pts)]]></title>
            <link>https://github.com/xenova/transformers.js</link>
            <guid>40001193</guid>
            <pubDate>Thu, 11 Apr 2024 11:57:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/xenova/transformers.js">https://github.com/xenova/transformers.js</a>, See on <a href="https://news.ycombinator.com/item?id=40001193">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <br>
    <themed-picture data-catalyst-inline="true"><picture> 
        <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/e55d794af470c9b405283b492e6a07aee928185a752fc6e613679fa6bde93478/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f58656e6f76612f7472616e73666f726d6572732e6a732d646f63732f7261772f6d61696e2f7472616e73666f726d6572736a732d6461726b2e737667" width="500" data-canonical-src="https://huggingface.co/datasets/Xenova/transformers.js-docs/raw/main/transformersjs-dark.svg">
        <source media="(prefers-color-scheme: light)" srcset="https://camo.githubusercontent.com/10bd3c5284d4104279c2b977506ea5e9bebf83fbd45715830a39d4a44def28b3/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f58656e6f76612f7472616e73666f726d6572732e6a732d646f63732f7261772f6d61696e2f7472616e73666f726d6572736a732d6c696768742e737667" width="500" data-canonical-src="https://huggingface.co/datasets/Xenova/transformers.js-docs/raw/main/transformersjs-light.svg">
        <img alt="transformers.js javascript library logo" src="https://camo.githubusercontent.com/10bd3c5284d4104279c2b977506ea5e9bebf83fbd45715830a39d4a44def28b3/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f58656e6f76612f7472616e73666f726d6572732e6a732d646f63732f7261772f6d61696e2f7472616e73666f726d6572736a732d6c696768742e737667" width="500" data-canonical-src="https://huggingface.co/datasets/Xenova/transformers.js-docs/raw/main/transformersjs-light.svg">
    </picture></themed-picture>
    <br>
</p>
<p dir="auto">
    <a href="https://www.npmjs.com/package/@xenova/transformers" rel="nofollow">
        <img alt="NPM" src="https://camo.githubusercontent.com/4f705dd79f06a166012e671df1d8456cdb17bc1ba7c7410ca1dce062e758b1a7/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f4078656e6f76612f7472616e73666f726d657273" data-canonical-src="https://img.shields.io/npm/v/@xenova/transformers">
    </a>
    <a href="https://www.npmjs.com/package/@xenova/transformers" rel="nofollow">
        <img alt="NPM Downloads" src="https://camo.githubusercontent.com/a4c5b6ffd4d92bf1b8f68b1446c397649a2acdbeb4f360a4144e7cf73175ef0d/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f64772f4078656e6f76612f7472616e73666f726d657273" data-canonical-src="https://img.shields.io/npm/dw/@xenova/transformers">
    </a>
    <a href="https://www.jsdelivr.com/package/npm/@xenova/transformers" rel="nofollow">
        <img alt="jsDelivr Hits" src="https://camo.githubusercontent.com/3b81259823a5e4da5cd7d8dd46b9a962e99547ccb610a18c59621a4a329fd670/68747470733a2f2f696d672e736869656c64732e696f2f6a7364656c6976722f6e706d2f68772f4078656e6f76612f7472616e73666f726d657273" data-canonical-src="https://img.shields.io/jsdelivr/npm/hw/@xenova/transformers">
    </a>
    <a href="https://github.com/xenova/transformers.js/blob/main/LICENSE">
        <img alt="License" src="https://camo.githubusercontent.com/a1df49724bc051547675d8e544f351f71a4ab2382771ca46af9d471107333050/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f78656e6f76612f7472616e73666f726d6572732e6a733f636f6c6f723d626c7565" data-canonical-src="https://img.shields.io/github/license/xenova/transformers.js?color=blue">
    </a>
    <a href="https://huggingface.co/docs/transformers.js/index" rel="nofollow">
        <img alt="Documentation" src="https://camo.githubusercontent.com/652b71cd0fafb5f0c2cec2d463539708b4e8b875772134430935ff3127655d47/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652f687474702f68756767696e67666163652e636f2f646f63732f7472616e73666f726d6572732e6a732f696e6465782e7376673f646f776e5f636f6c6f723d72656426646f776e5f6d6573736167653d6f66666c696e652675705f6d6573736167653d6f6e6c696e65" data-canonical-src="https://img.shields.io/website/http/huggingface.co/docs/transformers.js/index.svg?down_color=red&amp;down_message=offline&amp;up_message=online">
    </a>
</p>
<p dir="auto">State-of-the-art Machine Learning for the web. Run ü§ó Transformers directly in your browser, with no need for a server!</p>
<p dir="auto">Transformers.js is designed to be functionally equivalent to Hugging Face's <a href="https://github.com/huggingface/transformers">transformers</a> python library, meaning you can run the same pretrained models using a very similar API. These models support common tasks in different modalities, such as:</p>
<ul dir="auto">
<li>üìù <strong>Natural Language Processing</strong>: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.</li>
<li>üñºÔ∏è <strong>Computer Vision</strong>: image classification, object detection, and segmentation.</li>
<li>üó£Ô∏è <strong>Audio</strong>: automatic speech recognition and audio classification.</li>
<li>üêô <strong>Multimodal</strong>: zero-shot image classification.</li>
</ul>
<p dir="auto">Transformers.js uses <a href="https://onnxruntime.ai/" rel="nofollow">ONNX Runtime</a> to run models in the browser. The best part about it, is that you can easily <a href="#convert-your-models-to-onnx">convert</a> your pretrained PyTorch, TensorFlow, or JAX models to ONNX using <a href="https://github.com/huggingface/optimum#onnx--onnx-runtime">ü§ó Optimum</a>.</p>
<p dir="auto">For more information, check out the full <a href="https://huggingface.co/docs/transformers.js" rel="nofollow">documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick tour</h2><a id="user-content-quick-tour" aria-label="Permalink: Quick tour" href="#quick-tour"></a></p>
<p dir="auto">It's super simple to translate from existing code! Just like the python library, we support the <code>pipeline</code> API. Pipelines group together a pretrained model with preprocessing of inputs and postprocessing of outputs, making it the easiest way to run models with the library.</p>
<table>
<tbody><tr>
<th><b>Python (original)</b></th>
<th><b>Javascript (ours)</b></th>
</tr>
<tr>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import pipeline

# Allocate a pipeline for sentiment-analysis
pipe = pipeline('sentiment-analysis')

out = pipe('I love transformers!')
# [{'label': 'POSITIVE', 'score': 0.999806941}]"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>pipeline</span>

<span># Allocate a pipeline for sentiment-analysis</span>
<span>pipe</span> <span>=</span> <span>pipeline</span>(<span>'sentiment-analysis'</span>)

<span>out</span> <span>=</span> <span>pipe</span>(<span>'I love transformers!'</span>)
<span># [{'label': 'POSITIVE', 'score': 0.999806941}]</span></pre></div>
</td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="import { pipeline } from '@xenova/transformers';

// Allocate a pipeline for sentiment-analysis
let pipe = await pipeline('sentiment-analysis');

let out = await pipe('I love transformers!');
// [{'label': 'POSITIVE', 'score': 0.999817686}]"><pre><span>import</span> <span>{</span> <span>pipeline</span> <span>}</span> <span>from</span> <span>'@xenova/transformers'</span><span>;</span>

<span>// Allocate a pipeline for sentiment-analysis</span>
<span>let</span> <span>pipe</span> <span>=</span> <span>await</span> <span>pipeline</span><span>(</span><span>'sentiment-analysis'</span><span>)</span><span>;</span>

<span>let</span> <span>out</span> <span>=</span> <span>await</span> <span>pipe</span><span>(</span><span>'I love transformers!'</span><span>)</span><span>;</span>
<span>// [{'label': 'POSITIVE', 'score': 0.999817686}]</span></pre></div>
</td>
</tr>
</tbody></table>
<p dir="auto">You can also use a different model by specifying the model id or path as the second argument to the <code>pipeline</code> function. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Use a different model for sentiment-analysis
let pipe = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment');"><pre><span>// Use a different model for sentiment-analysis</span>
<span>let</span> <span>pipe</span> <span>=</span> <span>await</span> <span>pipeline</span><span>(</span><span>'sentiment-analysis'</span><span>,</span> <span>'Xenova/bert-base-multilingual-uncased-sentiment'</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To install via <a href="https://www.npmjs.com/package/@xenova/transformers" rel="nofollow">NPM</a>, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="npm i @xenova/transformers"><pre>npm i @xenova/transformers</pre></div>
<p dir="auto">Alternatively, you can use it in vanilla JS, without any bundler, by using a CDN or static hosting. For example, using <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules" rel="nofollow">ES Modules</a>, you can import the library with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="<script type=&quot;module&quot;>
    import { pipeline } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.0';
</script>"><pre><span>&lt;</span><span>script</span> <span>type</span>="<span>module</span>"<span>&gt;</span>
    <span>import</span> <span>{</span> <span>pipeline</span> <span>}</span> <span>from</span> <span>'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.0'</span><span>;</span>
<span>&lt;/</span><span>script</span><span>&gt;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Want to jump straight in? Get started with one of our sample applications/templates:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Links</th>
</tr>
</thead>
<tbody>
<tr>
<td>Whisper Web</td>
<td>Speech recognition w/ Whisper</td>
<td><a href="https://github.com/xenova/whisper-web">code</a>, <a href="https://huggingface.co/spaces/Xenova/whisper-web" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Doodle Dash</td>
<td>Real-time sketch-recognition game</td>
<td><a href="https://huggingface.co/blog/ml-web-games" rel="nofollow">blog</a>, <a href="https://github.com/xenova/doodle-dash">code</a>, <a href="https://huggingface.co/spaces/Xenova/doodle-dash" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Code Playground</td>
<td>In-browser code completion website</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/code-completion/">code</a>, <a href="https://huggingface.co/spaces/Xenova/ai-code-playground" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Semantic Image Search (client-side)</td>
<td>Search for images with text</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/semantic-image-search-client/">code</a>, <a href="https://huggingface.co/spaces/Xenova/semantic-image-search-client" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Semantic Image Search (server-side)</td>
<td>Search for images with text (Supabase)</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/semantic-image-search/">code</a>, <a href="https://huggingface.co/spaces/Xenova/semantic-image-search" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Vanilla JavaScript</td>
<td>In-browser object detection</td>
<td><a href="https://scrimba.com/scrim/cKm9bDAg" rel="nofollow">video</a>, <a href="https://github.com/xenova/transformers.js/tree/main/examples/vanilla-js/">code</a>, <a href="https://huggingface.co/spaces/Scrimba/vanilla-js-object-detector" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>React</td>
<td>Multilingual translation website</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/react-translator/">code</a>, <a href="https://huggingface.co/spaces/Xenova/react-translator" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Text to speech (client-side)</td>
<td>In-browser speech synthesis</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/text-to-speech-client/">code</a>, <a href="https://huggingface.co/spaces/Xenova/text-to-speech-client" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Browser extension</td>
<td>Text classification extension</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/extension/">code</a></td>
</tr>
<tr>
<td>Electron</td>
<td>Text classification application</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/electron/">code</a></td>
</tr>
<tr>
<td>Next.js (client-side)</td>
<td>Sentiment analysis (in-browser inference)</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/next-client/">code</a>, <a href="https://huggingface.co/spaces/Xenova/next-example-app" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Next.js (server-side)</td>
<td>Sentiment analysis (Node.js inference)</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/next-server/">code</a>, <a href="https://huggingface.co/spaces/Xenova/next-server-example-app" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Node.js</td>
<td>Sentiment analysis API</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/node/">code</a></td>
</tr>
<tr>
<td>Demo site</td>
<td>A collection of demos</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/demo-site/">code</a>, <a href="https://xenova.github.io/transformers.js/" rel="nofollow">demo</a></td>
</tr>
</tbody>
</table>
<p dir="auto">Check out the Transformers.js <a href="https://huggingface.co/new-space?template=static-templates%2Ftransformers.js" rel="nofollow">template</a> on Hugging Face to get started in one click!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Custom usage</h2><a id="user-content-custom-usage" aria-label="Permalink: Custom usage" href="#custom-usage"></a></p>
<p dir="auto">By default, Transformers.js uses <a href="https://huggingface.co/models?library=transformers.js" rel="nofollow">hosted pretrained models</a> and <a href="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.0/dist/" rel="nofollow">precompiled WASM binaries</a>, which should work out-of-the-box. You can customize this as follows:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Settings</h3><a id="user-content-settings" aria-label="Permalink: Settings" href="#settings"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { env } from '@xenova/transformers';

// Specify a custom location for models (defaults to '/models/').
env.localModelPath = '/path/to/models/';

// Disable the loading of remote models from the Hugging Face Hub:
env.allowRemoteModels = false;

// Set location of .wasm files. Defaults to use a CDN.
env.backends.onnx.wasm.wasmPaths = '/path/to/files/';"><pre><span>import</span> <span>{</span> <span>env</span> <span>}</span> <span>from</span> <span>'@xenova/transformers'</span><span>;</span>

<span>// Specify a custom location for models (defaults to '/models/').</span>
<span>env</span><span>.</span><span>localModelPath</span> <span>=</span> <span>'/path/to/models/'</span><span>;</span>

<span>// Disable the loading of remote models from the Hugging Face Hub:</span>
<span>env</span><span>.</span><span>allowRemoteModels</span> <span>=</span> <span>false</span><span>;</span>

<span>// Set location of .wasm files. Defaults to use a CDN.</span>
<span>env</span><span>.</span><span>backends</span><span>.</span><span>onnx</span><span>.</span><span>wasm</span><span>.</span><span>wasmPaths</span> <span>=</span> <span>'/path/to/files/'</span><span>;</span></pre></div>
<p dir="auto">For a full list of available settings, check out the <a href="https://huggingface.co/docs/transformers.js/api/env" rel="nofollow">API Reference</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Convert your models to ONNX</h3><a id="user-content-convert-your-models-to-onnx" aria-label="Permalink: Convert your models to ONNX" href="#convert-your-models-to-onnx"></a></p>
<p dir="auto">We recommend using our <a href="https://github.com/xenova/transformers.js/blob/main/scripts/convert.py">conversion script</a> to convert your PyTorch, TensorFlow, or JAX models to ONNX in a single command. Behind the scenes, it uses <a href="https://huggingface.co/docs/optimum" rel="nofollow">ü§ó Optimum</a> to perform conversion and quantization of your model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.convert --quantize --model_id <model_name_or_path>"><pre>python -m scripts.convert --quantize --model_id <span>&lt;</span>model_name_or_path<span>&gt;</span></pre></div>
<p dir="auto">For example, convert and quantize <a href="https://huggingface.co/bert-base-uncased" rel="nofollow">bert-base-uncased</a> using:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.convert --quantize --model_id bert-base-uncased"><pre>python -m scripts.convert --quantize --model_id bert-base-uncased</pre></div>
<p dir="auto">This will save the following files to <code>./models/</code>:</p>
<div data-snippet-clipboard-copy-content="bert-base-uncased/
‚îú‚îÄ‚îÄ config.json
‚îú‚îÄ‚îÄ tokenizer.json
‚îú‚îÄ‚îÄ tokenizer_config.json
‚îî‚îÄ‚îÄ onnx/
    ‚îú‚îÄ‚îÄ model.onnx
    ‚îî‚îÄ‚îÄ model_quantized.onnx"><pre><code>bert-base-uncased/
‚îú‚îÄ‚îÄ config.json
‚îú‚îÄ‚îÄ tokenizer.json
‚îú‚îÄ‚îÄ tokenizer_config.json
‚îî‚îÄ‚îÄ onnx/
    ‚îú‚îÄ‚îÄ model.onnx
    ‚îî‚îÄ‚îÄ model_quantized.onnx
</code></pre></div>
<p dir="auto">For the full list of supported architectures, see the <a href="https://huggingface.co/docs/optimum/main/en/exporters/onnx/overview" rel="nofollow">Optimum documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported tasks/models</h2><a id="user-content-supported-tasksmodels" aria-label="Permalink: Supported tasks/models" href="#supported-tasksmodels"></a></p>
<p dir="auto">Here is the list of all tasks and architectures currently supported by Transformers.js.
If you don't see your task/model listed here or it is not yet supported, feel free
to open up a feature request <a href="https://github.com/xenova/transformers.js/issues/new/choose">here</a>.</p>
<p dir="auto">To find compatible models on the Hub, select the "transformers.js" library tag in the filter menu (or visit <a href="https://huggingface.co/models?library=transformers.js" rel="nofollow">this link</a>).
You can refine your search by selecting the task you're interested in (e.g., <a href="https://huggingface.co/models?pipeline_tag=text-classification&amp;library=transformers.js" rel="nofollow">text-classification</a>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tasks</h3><a id="user-content-tasks" aria-label="Permalink: Tasks" href="#tasks"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Natural Language Processing</h4><a id="user-content-natural-language-processing" aria-label="Permalink: Natural Language Processing" href="#natural-language-processing"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/fill-mask" rel="nofollow">Fill-Mask</a></td>
<td><code>fill-mask</code></td>
<td>Masking some of the words in a sentence and predicting which words should replace those masks.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FillMaskPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=fill-mask&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/question-answering" rel="nofollow">Question Answering</a></td>
<td><code>question-answering</code></td>
<td>Retrieve the answer to a question from a given text.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.QuestionAnsweringPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=question-answering&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/sentence-similarity" rel="nofollow">Sentence Similarity</a></td>
<td><code>sentence-similarity</code></td>
<td>Determining how similar two texts are.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=feature-extraction&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/summarization" rel="nofollow">Summarization</a></td>
<td><code>summarization</code></td>
<td>Producing a shorter version of a document while preserving its important information.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.SummarizationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=summarization&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/table-question-answering" rel="nofollow">Table Question Answering</a></td>
<td><code>table-question-answering</code></td>
<td>Answering a question about information from a given table.</td>
<td>‚ùå</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/text-classification" rel="nofollow">Text Classification</a></td>
<td><code>text-classification</code> or <code>sentiment-analysis</code></td>
<td>Assigning a label or class to a given text.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=text-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/text-generation#completion-generation-models" rel="nofollow">Text Generation</a></td>
<td><code>text-generation</code></td>
<td>Producing new text by predicting the next word in a sequence.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextGenerationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=text-generation&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/text-generation#text-to-text-generation-models" rel="nofollow">Text-to-text Generation</a></td>
<td><code>text2text-generation</code></td>
<td>Converting one text sequence into another text sequence.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.Text2TextGenerationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=text2text-generation&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/token-classification" rel="nofollow">Token Classification</a></td>
<td><code>token-classification</code> or <code>ner</code></td>
<td>Assigning a label to each token in a text.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TokenClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=token-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/translation" rel="nofollow">Translation</a></td>
<td><code>translation</code></td>
<td>Converting text from one language to another.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TranslationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=translation&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/zero-shot-classification" rel="nofollow">Zero-Shot Classification</a></td>
<td><code>zero-shot-classification</code></td>
<td>Classifying text into classes that are unseen during training.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=zero-shot-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/feature-extraction" rel="nofollow">Feature Extraction</a></td>
<td><code>feature-extraction</code></td>
<td>Transforming raw data into numerical features that can be processed while preserving the information in the original dataset.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=feature-extraction&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Vision</h4><a id="user-content-vision" aria-label="Permalink: Vision" href="#vision"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/depth-estimation" rel="nofollow">Depth Estimation</a></td>
<td><code>depth-estimation</code></td>
<td>Predicting the depth of objects present in an image.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DepthEstimationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=depth-estimation&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/image-classification" rel="nofollow">Image Classification</a></td>
<td><code>image-classification</code></td>
<td>Assigning a label or class to an entire image.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=image-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/image-segmentation" rel="nofollow">Image Segmentation</a></td>
<td><code>image-segmentation</code></td>
<td>Divides an image into segments where each pixel is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageSegmentationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=image-segmentation&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/image-to-image" rel="nofollow">Image-to-Image</a></td>
<td><code>image-to-image</code></td>
<td>Transforming a source image to match the characteristics of a target image or a target image domain.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToImagePipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=image-to-image&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/mask-generation" rel="nofollow">Mask Generation</a></td>
<td><code>mask-generation</code></td>
<td>Generate masks for the objects in an image.</td>
<td>‚ùå</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/object-detection" rel="nofollow">Object Detection</a></td>
<td><code>object-detection</code></td>
<td>Identify objects of certain defined classes within an image.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ObjectDetectionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=object-detection&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/video-classification" rel="nofollow">Video Classification</a></td>
<td>n/a</td>
<td>Assigning a label or class to an entire video.</td>
<td>‚ùå</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/unconditional-image-generation" rel="nofollow">Unconditional Image Generation</a></td>
<td>n/a</td>
<td>Generating images with no condition in any context (like a prompt text or another image).</td>
<td>‚ùå</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/image-feature-extraction" rel="nofollow">Image Feature Extraction</a></td>
<td><code>image-feature-extraction</code></td>
<td>Transforming raw data into numerical features that can be processed while preserving the information in the original image.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageFeatureExtractionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=image-feature-extraction&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Audio</h4><a id="user-content-audio" aria-label="Permalink: Audio" href="#audio"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/audio-classification" rel="nofollow">Audio Classification</a></td>
<td><code>audio-classification</code></td>
<td>Assigning a label or class to a given audio.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.AudioClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=audio-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/audio-to-audio" rel="nofollow">Audio-to-Audio</a></td>
<td>n/a</td>
<td>Generating audio from an input audio source.</td>
<td>‚ùå</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/automatic-speech-recognition" rel="nofollow">Automatic Speech Recognition</a></td>
<td><code>automatic-speech-recognition</code></td>
<td>Transcribing a given audio into text.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.AutomaticSpeechRecognitionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/text-to-speech" rel="nofollow">Text-to-Speech</a></td>
<td><code>text-to-speech</code> or <code>text-to-audio</code></td>
<td>Generating natural-sounding speech given text input.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextToAudioPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=text-to-audio&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Tabular</h4><a id="user-content-tabular" aria-label="Permalink: Tabular" href="#tabular"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/tabular-classification" rel="nofollow">Tabular Classification</a></td>
<td>n/a</td>
<td>Classifying a target category (a group) based on set of attributes.</td>
<td>‚ùå</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/tabular-regression" rel="nofollow">Tabular Regression</a></td>
<td>n/a</td>
<td>Predicting a numerical value given a set of attributes.</td>
<td>‚ùå</td>
</tr>
</tbody>
</table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Multimodal</h4><a id="user-content-multimodal" aria-label="Permalink: Multimodal" href="#multimodal"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/document-question-answering" rel="nofollow">Document Question Answering</a></td>
<td><code>document-question-answering</code></td>
<td>Answering questions on document images.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DocumentQuestionAnsweringPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=document-question-answering&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/image-to-text" rel="nofollow">Image-to-Text</a></td>
<td><code>image-to-text</code></td>
<td>Output text from a given image.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToTextPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=image-to-text&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/text-to-image" rel="nofollow">Text-to-Image</a></td>
<td><code>text-to-image</code></td>
<td>Generates images from input text.</td>
<td>‚ùå</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/visual-question-answering" rel="nofollow">Visual Question Answering</a></td>
<td><code>visual-question-answering</code></td>
<td>Answering open-ended questions based on an image.</td>
<td>‚ùå</td>
</tr>
<tr>
<td><a href="https://huggingface.co/learn/audio-course/chapter4/classification_models#zero-shot-audio-classification" rel="nofollow">Zero-Shot Audio Classification</a></td>
<td><code>zero-shot-audio-classification</code></td>
<td>Classifying audios into classes that are unseen during training.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotAudioClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?other=zero-shot-audio-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/zero-shot-image-classification" rel="nofollow">Zero-Shot Image Classification</a></td>
<td><code>zero-shot-image-classification</code></td>
<td>Classifying images into classes that are unseen during training.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotImageClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/zero-shot-object-detection" rel="nofollow">Zero-Shot Object Detection</a></td>
<td><code>zero-shot-object-detection</code></td>
<td>Identify objects of classes that are unseen during training.</td>
<td>‚úÖ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotObjectDetectionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?other=zero-shot-object-detection&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Reinforcement Learning</h4><a id="user-content-reinforcement-learning" aria-label="Permalink: Reinforcement Learning" href="#reinforcement-learning"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/reinforcement-learning" rel="nofollow">Reinforcement Learning</a></td>
<td>n/a</td>
<td>Learning from actions by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback.</td>
<td>‚ùå</td>
</tr>
</tbody>
</table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Models</h3><a id="user-content-models" aria-label="Permalink: Models" href="#models"></a></p>
<ol dir="auto">
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/albert" rel="nofollow">ALBERT</a></strong> (from Google Research and the Toyota Technological Institute at Chicago) released with the paper <a href="https://arxiv.org/abs/1909.11942" rel="nofollow">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a>, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer" rel="nofollow">Audio Spectrogram Transformer</a></strong> (from MIT) released with the paper <a href="https://arxiv.org/abs/2104.01778" rel="nofollow">AST: Audio Spectrogram Transformer</a> by Yuan Gong, Yu-An Chung, James Glass.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/bart" rel="nofollow">BART</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a> by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/beit" rel="nofollow">BEiT</a></strong> (from Microsoft) released with the paper <a href="https://arxiv.org/abs/2106.08254" rel="nofollow">BEiT: BERT Pre-Training of Image Transformers</a> by Hangbo Bao, Li Dong, Furu Wei.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/bert" rel="nofollow">BERT</a></strong> (from Google) released with the paper <a href="https://arxiv.org/abs/1810.04805" rel="nofollow">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/blenderbot" rel="nofollow">Blenderbot</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2004.13637" rel="nofollow">Recipes for building an open-domain chatbot</a> by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/blenderbot-small" rel="nofollow">BlenderbotSmall</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2004.13637" rel="nofollow">Recipes for building an open-domain chatbot</a> by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/bloom" rel="nofollow">BLOOM</a></strong> (from BigScience workshop) released by the <a href="https://bigscience.huggingface.co/" rel="nofollow">BigScience Workshop</a>.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/camembert" rel="nofollow">CamemBERT</a></strong> (from Inria/Facebook/Sorbonne) released with the paper <a href="https://arxiv.org/abs/1911.03894" rel="nofollow">CamemBERT: a Tasty French Language Model</a> by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su√°rez*, Yoann Dupont, Laurent Romary, √âric Villemonte de la Clergerie, Djam√© Seddah and Beno√Æt Sagot.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/chinese_clip" rel="nofollow">Chinese-CLIP</a></strong> (from OFA-Sys) released with the paper <a href="https://arxiv.org/abs/2211.01335" rel="nofollow">Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese</a> by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/clap" rel="nofollow">CLAP</a></strong> (from LAION-AI) released with the paper <a href="https://arxiv.org/abs/2211.06687" rel="nofollow">Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</a> by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/clip" rel="nofollow">CLIP</a></strong> (from OpenAI) released with the paper <a href="https://arxiv.org/abs/2103.00020" rel="nofollow">Learning Transferable Visual Models From Natural Language Supervision</a> by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/clipseg" rel="nofollow">CLIPSeg</a></strong> (from University of G√∂ttingen) released with the paper <a href="https://arxiv.org/abs/2112.10003" rel="nofollow">Image Segmentation Using Text and Image Prompts</a> by Timo L√ºddecke and Alexander Ecker.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/codegen" rel="nofollow">CodeGen</a></strong> (from Salesforce) released with the paper <a href="https://arxiv.org/abs/2203.13474" rel="nofollow">A Conversational Paradigm for Program Synthesis</a> by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/llama_code" rel="nofollow">CodeLlama</a></strong> (from MetaAI) released with the paper <a href="https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/" rel="nofollow">Code Llama: Open Foundation Models for Code</a> by Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D√©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/convbert" rel="nofollow">ConvBERT</a></strong> (from YituTech) released with the paper <a href="https://arxiv.org/abs/2008.02496" rel="nofollow">ConvBERT: Improving BERT with Span-based Dynamic Convolution</a> by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/convnext" rel="nofollow">ConvNeXT</a></strong> (from Facebook AI) released with the paper <a href="https://arxiv.org/abs/2201.03545" rel="nofollow">A ConvNet for the 2020s</a> by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/convnextv2" rel="nofollow">ConvNeXTV2</a></strong> (from Facebook AI) released with the paper <a href="https://arxiv.org/abs/2301.00808" rel="nofollow">ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</a> by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/deberta" rel="nofollow">DeBERTa</a></strong> (from Microsoft) released with the paper <a href="https://arxiv.org/abs/2006.03654" rel="nofollow">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a> by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/deberta-v2" rel="nofollow">DeBERTa-v2</a></strong> (from Microsoft) released with the paper <a href="https://arxiv.org/abs/2006.03654" rel="nofollow">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a> by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/deit" rel="nofollow">DeiT</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2012.12877" rel="nofollow">Training data-efficient image transformers &amp; distillation through attention</a> by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv√© J√©gou.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/model_doc/depth_anything" rel="nofollow">Depth Anything</a></strong> (from University of Hong Kong and TikTok) released with the paper <a href="https://arxiv.org/abs/2401.10891" rel="nofollow">Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</a> by Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/detr" rel="nofollow">DETR</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2005.12872" rel="nofollow">End-to-End Object Detection with Transformers</a> by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/dinov2" rel="nofollow">DINOv2</a></strong> (from Meta AI) released with the paper <a href="https://arxiv.org/abs/2304.07193" rel="nofollow">DINOv2: Learning Robust Visual Features without Supervision</a> by Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv√© Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/distilbert" rel="nofollow">DistilBERT</a></strong> (from HuggingFace), released together with the paper <a href="https://arxiv.org/abs/1910.01108" rel="nofollow">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into <a href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation">DistilGPT2</a>, RoBERTa into <a href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation">DistilRoBERTa</a>, Multilingual BERT into <a href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation">DistilmBERT</a> and a German version of DistilBERT.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/dit" rel="nofollow">DiT</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2203.02378" rel="nofollow">DiT: Self-supervised Pre-training for Document Image Transformer</a> by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/donut" rel="nofollow">Donut</a></strong> (from NAVER), released together with the paper <a href="https://arxiv.org/abs/2111.15664" rel="nofollow">OCR-free Document Understanding Transformer</a> by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/master/model_doc/dpt" rel="nofollow">DPT</a></strong> (from Intel Labs) released with the paper <a href="https://arxiv.org/abs/2103.13413" rel="nofollow">Vision Transformers for Dense Prediction</a> by Ren√© Ranftl, Alexey Bochkovskiy, Vladlen Koltun.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/efficientnet" rel="nofollow">EfficientNet</a></strong> (from Google Brain) released with the paper <a href="https://arxiv.org/abs/1905.11946" rel="nofollow">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a> by Mingxing Tan, Quoc V. Le.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/electra" rel="nofollow">ELECTRA</a></strong> (from Google Research/Stanford University) released with the paper <a href="https://arxiv.org/abs/2003.10555" rel="nofollow">ELECTRA: Pre-training text encoders as discriminators rather than generators</a> by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/esm" rel="nofollow">ESM</a></strong> (from Meta AI) are transformer protein language models.  <strong>ESM-1b</strong> was released with the paper <a href="https://www.pnas.org/content/118/15/e2016239118" rel="nofollow">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</a> by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. <strong>ESM-1v</strong> was released with the paper <a href="https://doi.org/10.1101/2021.07.09.450648" rel="nofollow">Language models enable zero-shot prediction of the effects of mutations on protein function</a> by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. <strong>ESM-2 and ESMFold</strong> were released with the paper <a href="https://doi.org/10.1101/2022.07.20.500902" rel="nofollow">Language models of protein sequences at the scale of evolution enable accurate structure prediction</a> by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/falcon" rel="nofollow">Falcon</a></strong> (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/flan-t5" rel="nofollow">FLAN-T5</a></strong> (from Google AI) released in the repository <a href="https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints">google-research/t5x</a> by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/glpn" rel="nofollow">GLPN</a></strong> (from KAIST) released with the paper <a href="https://arxiv.org/abs/2201.07436" rel="nofollow">Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth</a> by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/gpt_neo" rel="nofollow">GPT Neo</a></strong> (from EleutherAI) released in the repository <a href="https://github.com/EleutherAI/gpt-neo">EleutherAI/gpt-neo</a> by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/gpt_neox" rel="nofollow">GPT NeoX</a></strong> (from EleutherAI) released with the paper <a href="https://arxiv.org/abs/2204.06745" rel="nofollow">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a> by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/gpt2" rel="nofollow">GPT-2</a></strong> (from OpenAI) released with the paper <a href="https://blog.openai.com/better-language-models/" rel="nofollow">Language Models are Unsupervised Multitask Learners</a> by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/gptj" rel="nofollow">GPT-J</a></strong> (from EleutherAI) released in the repository <a href="https://github.com/kingoflolz/mesh-transformer-jax/">kingoflolz/mesh-transformer-jax</a> by Ben Wang and Aran Komatsuzaki.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/gpt_bigcode" rel="nofollow">GPTBigCode</a></strong> (from BigCode) released with the paper <a href="https://arxiv.org/abs/2301.03988" rel="nofollow">SantaCoder: don't reach for the stars!</a> by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc√≠a del R√≠o, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/herbert" rel="nofollow">HerBERT</a></strong> (from Allegro.pl, AGH University of Science and Technology) released with the paper <a href="https://www.aclweb.org/anthology/2020.acl-main.111.pdf" rel="nofollow">KLEJ: Comprehensive Benchmark for Polish Language Understanding</a> by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/hubert" rel="nofollow">Hubert</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2106.07447" rel="nofollow">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a> by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/longt5" rel="nofollow">LongT5</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2112.07916" rel="nofollow">LongT5: Efficient Text-To-Text Transformer for Long Sequences</a> by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/llama" rel="nofollow">LLaMA</a></strong> (from The FAIR team of Meta AI) released with the paper <a href="https://arxiv.org/abs/2302.13971" rel="nofollow">LLaMA: Open and Efficient Foundation Language Models</a> by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/llama2" rel="nofollow">Llama2</a></strong> (from The FAIR team of Meta AI) released with the paper <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX" rel="nofollow">Llama2: Open Foundation and Fine-Tuned Chat Models</a> by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/m2m_100" rel="nofollow">M2M100</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2010.11125" rel="nofollow">Beyond English-Centric Multilingual Machine Translation</a> by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/marian" rel="nofollow">MarianMT</a></strong> Machine translation models trained using <a href="http://opus.nlpl.eu/" rel="nofollow">OPUS</a> data by J√∂rg Tiedemann. The <a href="https://marian-nmt.github.io/" rel="nofollow">Marian Framework</a> is being developed by the Microsoft Translator Team.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mbart" rel="nofollow">mBART</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2001.08210" rel="nofollow">Multilingual Denoising Pre-training for Neural Machine Translation</a> by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mbart" rel="nofollow">mBART-50</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2008.00401" rel="nofollow">Multilingual Translation with Extensible Multilingual Pretraining and Finetuning</a> by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mistral" rel="nofollow">Mistral</a></strong> (from Mistral AI) by The <a href="https://mistral.ai/" rel="nofollow">Mistral AI</a> team: Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L√©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mms" rel="nofollow">MMS</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2305.13516" rel="nofollow">Scaling Speech Technology to 1,000+ Languages</a> by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mobilebert" rel="nofollow">MobileBERT</a></strong> (from CMU/Google Brain) released with the paper <a href="https://arxiv.org/abs/2004.02984" rel="nofollow">MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</a> by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mobilevit" rel="nofollow">MobileViT</a></strong> (from Apple) released with the paper <a href="https://arxiv.org/abs/2110.02178" rel="nofollow">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</a> by Sachin Mehta and Mohammad Rastegari.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mpnet" rel="nofollow">MPNet</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2004.09297" rel="nofollow">MPNet: Masked and Permuted Pre-training for Language Understanding</a> by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mpt" rel="nofollow">MPT</a></strong> (from MosaiML) released with the repository <a href="https://github.com/mosaicml/llm-foundry/">llm-foundry</a> by the MosaicML NLP Team.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mt5" rel="nofollow">MT5</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2010.11934" rel="nofollow">mT5: A massively multilingual pre-trained text-to-text transformer</a> by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/nllb" rel="nofollow">NLLB</a></strong> (from Meta) released with the paper <a href="https://arxiv.org/abs/2207.04672" rel="nofollow">No Language Left Behind: Scaling Human-Centered Machine Translation</a> by the NLLB team.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/nougat" rel="nofollow">Nougat</a></strong> (from Meta AI) released with the paper <a href="https://arxiv.org/abs/2308.13418" rel="nofollow">Nougat: Neural Optical Understanding for Academic Documents</a> by Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/master/model_doc/opt" rel="nofollow">OPT</a></strong> (from Meta AI) released with the paper <a href="https://arxiv.org/abs/2205.01068" rel="nofollow">OPT: Open Pre-trained Transformer Language Models</a> by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/owlvit" rel="nofollow">OWL-ViT</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2205.06230" rel="nofollow">Simple Open-Vocabulary Object Detection with Vision Transformers</a> by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/owlv2" rel="nofollow">OWLv2</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2306.09683" rel="nofollow">Scaling Open-Vocabulary Object Detection</a> by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/model_doc/phi" rel="nofollow">Phi</a></strong> (from Microsoft) released with the papers - <a href="https://arxiv.org/abs/2306.11644" rel="nofollow">Textbooks Are All You Need</a> by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S√©bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, <a href="https://arxiv.org/abs/2309.05463" rel="nofollow">Textbooks Are All You Need II: phi-1.5 technical report</a> by Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/qwen2" rel="nofollow">Qwen2</a></strong> (from the Qwen team, Alibaba Group) released with the paper <a href="https://arxiv.org/abs/2309.16609" rel="nofollow">Qwen Technical Report</a> by Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou and Tianhang Zhu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/resnet" rel="nofollow">ResNet</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/1512.03385" rel="nofollow">Deep Residual Learning for Image Recognition</a> by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/roberta" rel="nofollow">RoBERTa</a></strong> (from Facebook), released together with the paper <a href="https://arxiv.org/abs/1907.11692" rel="nofollow">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/roformer" rel="nofollow">RoFormer</a></strong> (from ZhuiyiTechnology), released together with the paper <a href="https://arxiv.org/abs/2104.09864" rel="nofollow">RoFormer: Enhanced Transformer with Rotary Position Embedding</a> by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/segformer" rel="nofollow">SegFormer</a></strong> (from NVIDIA) released with the paper <a href="https://arxiv.org/abs/2105.15203" rel="nofollow">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</a> by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/sam" rel="nofollow">Segment Anything</a></strong> (from Meta AI) released with the paper <a href="https://arxiv.org/pdf/2304.02643v1.pdf" rel="nofollow">Segment Anything</a> by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/model_doc/siglip" rel="nofollow">SigLIP</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2303.15343" rel="nofollow">Sigmoid Loss for Language Image Pre-Training</a> by Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/speecht5" rel="nofollow">SpeechT5</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2110.07205" rel="nofollow">SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</a> by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/squeezebert" rel="nofollow">SqueezeBERT</a></strong> (from Berkeley) released with the paper <a href="https://arxiv.org/abs/2006.11316" rel="nofollow">SqueezeBERT: What can computer vision teach NLP about efficient neural networks?</a> by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/stablelm" rel="nofollow">StableLm</a></strong> (from Stability AI) released with the paper <a href="https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo" rel="nofollow">StableLM 3B 4E1T (Technical Report)</a> by Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme Ruiz, Duy Phung, Maksym Zhuravinskyi, Nathan Cooper, Nikhil Pinnaparaju, Reshinth Adithyan, and James Baicoianu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/model_doc/starcoder2" rel="nofollow">Starcoder2</a></strong> (from BigCode team) released with the paper <a href="https://arxiv.org/abs/2402.19173" rel="nofollow">StarCoder 2 and The Stack v2: The Next Generation</a> by Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau√ü, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu√±oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/swin" rel="nofollow">Swin Transformer</a></strong> (from Microsoft) released with the paper <a href="https://arxiv.org/abs/2103.14030" rel="nofollow">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a> by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/swin2sr" rel="nofollow">Swin2SR</a></strong> (from University of W√ºrzburg) released with the paper <a href="https://arxiv.org/abs/2209.11345" rel="nofollow">Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration</a> by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/t5" rel="nofollow">T5</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/t5v1.1" rel="nofollow">T5v1.1</a></strong> (from Google AI) released in the repository <a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511">google-research/text-to-text-transfer-transformer</a> by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/table-transformer" rel="nofollow">Table Transformer</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2110.00061" rel="nofollow">PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents</a> by Brandon Smock, Rohith Pesala, Robin Abraham.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/trocr" rel="nofollow">TrOCR</a></strong> (from Microsoft), released together with the paper <a href="https://arxiv.org/abs/2109.10282" rel="nofollow">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models</a> by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/unispeech" rel="nofollow">UniSpeech</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2101.07597" rel="nofollow">UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data</a> by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/unispeech-sat" rel="nofollow">UniSpeechSat</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2110.05752" rel="nofollow">UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING</a> by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/vit" rel="nofollow">Vision Transformer (ViT)</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2010.11929" rel="nofollow">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a> by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/vitmatte" rel="nofollow">ViTMatte</a></strong> (from HUST-VL) released with the paper <a href="https://arxiv.org/abs/2305.15272" rel="nofollow">ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers</a> by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/vits" rel="nofollow">VITS</a></strong> (from Kakao Enterprise) released with the paper <a href="https://arxiv.org/abs/2106.06103" rel="nofollow">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</a> by Jaehyeon Kim, Jungil Kong, Juhee Son.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/wav2vec2" rel="nofollow">Wav2Vec2</a></strong> (from Facebook AI) released with the paper <a href="https://arxiv.org/abs/2006.11477" rel="nofollow">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a> by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/model_doc/wav2vec2-bert" rel="nofollow">Wav2Vec2-BERT</a></strong> (from Meta AI) released with the paper <a href="https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/" rel="nofollow">Seamless: Multilingual Expressive and Streaming Speech Translation</a> by the Seamless Communication team.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/wavlm" rel="nofollow">WavLM</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2110.13900" rel="nofollow">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing</a> by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/whisper" rel="nofollow">Whisper</a></strong> (from OpenAI) released with the paper <a href="https://cdn.openai.com/papers/whisper.pdf" rel="nofollow">Robust Speech Recognition via Large-Scale Weak Supervision</a> by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/xlm" rel="nofollow">XLM</a></strong> (from Facebook) released together with the paper <a href="https://arxiv.org/abs/1901.07291" rel="nofollow">Cross-lingual Language Model Pretraining</a> by Guillaume Lample and Alexis Conneau.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/xlm-roberta" rel="nofollow">XLM-RoBERTa</a></strong> (from Facebook AI), released together with the paper <a href="https://arxiv.org/abs/1911.02116" rel="nofollow">Unsupervised Cross-lingual Representation Learning at Scale</a> by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/yolos" rel="nofollow">YOLOS</a></strong> (from Huazhong University of Science &amp; Technology) released with the paper <a href="https://arxiv.org/abs/2106.00666" rel="nofollow">You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection</a> by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.</li>
</ol>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mental Health in Software Engineering (101 pts)]]></title>
            <link>https://vadimkravcenko.com/shorts/mental-health-in-software-engineering/</link>
            <guid>40001150</guid>
            <pubDate>Thu, 11 Apr 2024 11:50:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vadimkravcenko.com/shorts/mental-health-in-software-engineering/">https://vadimkravcenko.com/shorts/mental-health-in-software-engineering/</a>, See on <a href="https://news.ycombinator.com/item?id=40001150">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">


<p>I want to talk about something we don't discuss enough in our field: the mental health of software engineers, especially those of us who've taken on the challenge of leadership. As a technical co-founder, I've had my struggles with anxiety. It's something that comes with the territory but isn't part of the job description.</p>


<p>If we rewind back to 2017, it was not a pleasant year for me ‚Äî my days consisted of panic attacks, constant use of relaxing supplements, and trying to code while being severely under pressure with all the deadlines and new <a href="https://vadimkravcenko.com/shorts/what-cto-does/" title="What does a CTO actually do?">responsibilities</a>. During that time, I inherited the position of Head of IT from my predecessor. I was now in charge of a small team of developers, and our startup has made many promises to many partners. It was my job to make it happen. I was either going to break or deliver. I did both.</p>


<p>Mental health issues still carry a bit of stigma with them. You cannot take a sick day by telling your team, ‚ÄúI have mental issues and need a day off.‚Äù Not many people understand what panic attacks feel like and why would you need to take a day off because of them? I understand those people completely. Right until the moment I had my burnout (or a mental breakdown), I didn‚Äôt have any idea why you need to take drugs to manage your anxiety. I thought it was all in your head; can‚Äôt you control your thoughts? Apparently, sometimes, you can‚Äôt.</p>


<figure><img alt="" data-src="https://vadimkravcenko.com/wp-content/uploads/2024/03/panic-attacks-in-it.jpeg" src="https://vadimkravcenko.com/wp-content/uploads/2024/03/panic-attacks-in-it.jpeg"><figcaption>Author: <a href="https://sow-ay.tumblr.com/" rel="nofollow">Sow Ay</a></figcaption></figure>


<p>When I was first diagnosed with anxiety, I didn‚Äôt understand it, so I went to Reddit to read similar stories of people all around the world dealing with the same symptoms, trying to survive the same way I did. It calmed me down during panic episodes. Imagine sitting at your workplace, coding, and then a wave of primal fear running through you, an all-enveloping sense of doom starts consuming you, and you turn to Reddit to read stories about people experiencing the same things. That was my coping mechanism. And it worked.</p>


<p>My story is, sadly, not unique. It‚Äôs a story that any software engineer can relate to. I was a perfectionist, a high-achiever who liked to have things under control and in a specific way to ensure everything was running as efficiently as possible. I have my keyboard bindings set up just as I need them, I have my dotfiles organized in a specific way, and I have my automation that makes my life easier. I think most software developers are like that. We strive to be efficient.</p>


<p>It‚Äôs easy to have things under control when coding is the only thing that you do, for example, at the junior level. You have a clearly defined task, which your senior colleague refined based on some vague description that your product owner brought them. You debug it, have fun building things, get your next task, and solve some bugs. You have zero worries; your only job now is to get better and learn. Life is good.</p>


<p>As you grow, you understand the realities of business. In business, there‚Äôs no place for perfection. There‚Äôs no space for having everything under control. In fact, not only can‚Äôt you influence most of the things around you, but most of the things are uncertain. Thinking about your next month becomes a math problem with probability variables.</p>


<pre>ü•∏ An example of uncertainty in business is when your CEO tells you they promised a feature to your biggest client and it needs to be built ASAP as highest priority, so all hands on deck. Then a day later they tell you another feature, completely contradictory to the first one, needs to be built as well and is also highest priority. When you tell them they both can't be highest priority, the answer is: make it happen.</pre>


<p>This was my perfect storm in 2017 ‚Äî I was trying to control all of the uncertainty around me:</p>


<ul>
<li><em>Trying to control the looming unrealistic deadlines.</em></li>


<li><em>Writing a lot of the code myself to ensure we uphold our promises to stakeholders and none of our developers burn out. Which led to me working more and sleeping less.</em></li>


<li><em>Worrying about next month‚Äôs payroll and trying to control our runway.</em></li>


<li><em>Maintaining developer velocity and tight budgets, juggling future growth and current issues.</em></li>


<li><em>Trying to control our developer turnover and making sure our juniors grow.</em></li>


<li><em>There were days I'd be coding non-stop or in a series of back-to-back <a href="https://vadimkravcenko.com/shorts/proper-documentation/" title="The Surprising Power of Documentation">meetings</a>, forgetting meals, sleeping, and even what it felt like to relax.</em></li>
</ul>


<p>There wasn‚Äôt one big thing that led to the burnout. It‚Äôs more of a combination of factors contributing to my perpetual state of stress. (Side note: I remember waking up on the weekends, feeling great the first few minutes in my bed, and then anxiety taking over).</p>


<p>Eventually, I saw a doctor and am doing much better now. It took me a long while, though.</p>


<h2 id="h-not-all-deadlines-are-equal">Not all deadlines are equal</h2>


<p>I‚Äôm not going to philosophize about how you need to maintain a good work-life balance and how not doing so may negatively impact your non-job-related aspects of life. But I can tell you for a fact I did not have a good work-life balance before. I think I‚Äôm getting better, but it‚Äôs only due to the fact that my company is doing better ‚Äî we have better cash flow, more loyal clients, and a great team. Before that? I thought work-life balance was a myth. I thought every deadline was sacred, every project was critical, and if you‚Äôre not online 24/7, it all fell apart. Was that true? It was, sometimes.</p>


<pre>üí° Some people asked me ‚Äî why are you working so much? What‚Äôs the worst that can happen? People not getting their salaries and the company going bankrupt. I think that‚Äôs a big enough reason to risk burning out. At least, I thought it was. Now? Probably still is a good reason, but I would approach it differently. </pre>


<p>This kind of situation isn't rare. You're always on edge, thinking the whole company's fate rests on your next move. It's like being in a constant state of alert, where slowing down feels like you're letting everyone down. As a CTO or any other tech leadership position, you're making calls that could either make or break the whole operation. And yeah, it's thrilling, but it's also a breeding ground for anxiety.</p>


<p>This one time, for example, when our <a href="https://vadimkravcenko.com/shorts/database-migrations/" title="Database Migrations">deployment</a> crashed halfway through right before a major release. The CEO emphasized how important this project was, so we were all hands on deck, trying to get it back up, fearing the worst, that the client would go ballistic if he found out we were delaying the release. I was stressing big time, thinking we had to pull off a miracle, and of course we did.</p>


<p>But you know what? After all that chaos, it turned out the relevant stakeholders were away on vacation that week, and the release wasn't even checked for many days after that. All that stress, the mad rush, and for what? We often stress ourselves over deadlines as if they're set in stone. But are they really? We push ourselves to the limit, thinking we're doing what's best for the business. What if we just‚Ä¶ didn't? Not all deadlines are equal. That release could've waited. That deadline was one we could‚Äôve let slip. Our health, our sanity, can't always take a backseat.</p>


<figure><img alt="" data-src="https://vadimkravcenko.com/wp-content/uploads/2024/03/deadline-stress-721x1024.png" src="https://vadimkravcenko.com/wp-content/uploads/2024/03/deadline-stress-721x1024.png"><figcaption>Deadlines aren't created equal. Source: <a href="https://monkeyuser.com/">Monkeyuser.com</a></figcaption></figure>


<p>In tech, there's this unspoken rule that you've gotta be all in, all the time. But that's just not sustainable. I've learned the hard way that not every deadline is do-or-die. Sometimes, pushing a release back a week is the best call you can make‚Äîfor you and your team.</p>


<h2 id="h-what-worked-for-me">What worked for me</h2>


<p>So, as you might guess, I‚Äôve picked up a few tricks along the way that have seriously helped me out. I‚Äôm not going to tell you I‚Äôm an expert on mental health, and I‚Äôm not going to be giving out advice. I just thought I'd share the things that have helped me, just in case you find yourself in a similar boat.</p>


<p>First off, anxiety and burnout are real, and they don't just go away on their own. I learned this the hard way. So, recognizing when you're starting to burn out or get anxious is crucial. For me, it was about noticing when I started to dread work I usually enjoyed and the random sense of apocalyptic doom or when my sleep went sideways. I mean, I don‚Äôt think anyone can miss those signs, but it took me half a year to recognize them and go to a psychotherapist. I thought I had the winter blues. Go figure‚Ä¶</p>


<p>Saying ‚ÄúNo‚Äù to anything non-critical in your off time, as well as setting boundaries between work and the rest of your life. I‚Äôm not very good at this yet, but I am learning to shut off after work hours. No reading emails, no "quick checks" on projects, no MacBook even. It's still challenging, but it‚Äôs getting better. Also, sometimes my partners call me on the weekends to do something; unless it‚Äôs something critical, I tell them I will handle it on Monday. Trust me, the world won't end if you tell someone you‚Äôll do it 24 or 48 hours later.</p>


<figure><img alt="" data-src="https://vadimkravcenko.com/wp-content/uploads/2024/03/new-kid-on-the-block-work-life-integration.png" src="https://vadimkravcenko.com/wp-content/uploads/2024/03/new-kid-on-the-block-work-life-integration.png"><figcaption>I hope this doesn't become the norm. Source: <a href="https://todoist.com/inspiration/work-life-integration-comic" rel="nofollow">Todoist</a></figcaption></figure>


<p>I swapped my coffee for decaf and cut out alcohol. It wasn't easy, but it helped me feel less jittery and sleep better. I also got into walking. I aim for 12,500 steps a day, usually with a podcast in my ears. It's my new (old) obsession. The steps clear my head and get me out of the work <a href="https://vadimkravcenko.com/shorts/engineering-scarcity-mindset/" title="ü§ù Engineering Scarcity Mindset">mindset</a>. Plus, hitting my step goal feels like a win every day.</p>


<p>Putting things in perspective has been another big one for me. I always ask myself, "Will this matter in two years?" You'd be surprised how often the answer is no. It takes the edge off the pressure. I allow myself to miss a deadline every now and then. As I mentioned above, not every deadline is do-or-die. Sometimes, it‚Äôs entirely reasonable to push things back to worry less.‚Ä®</p>


<p>Notifications on my phone? Almost all gone. If something's truly urgent, it'll find its way to me. This alone has cut down on a lot of unnecessary stress. It's like I've reclaimed my attention span. I don‚Äôt get constant DING! DING!</p>


<p>But here's something I didn't expect to have such a big impact: educating myself on mental health and emotional intelligence. Understanding that someone might be fighting a battle I know nothing about made me a more empathetic leader and colleague. It's changed how I interact with my team. I've learned to listen more, assume less, and approach every situation with a bit more kindness and understanding. You never know when a little compassion during those stressful times will help someone avoid burnout.</p>


<p>When I felt overwhelmed, I didn‚Äôt immediately seek professional help. I thought I could handle it myself; I was ashamed of going to a psychotherapist, as if my seeking help was admitting that I was broken. Since then, I‚Äôve understood many things. First of all, it‚Äôs totally fine to be vulnerable. If you're feeling overwhelmed, don't hesitate to seek professional help. We‚Äôre all human. There's no shame in it. In fact, it's one of the bravest things you can do. We go to the doctor for a physical ailment; why should our mental health be any different?</p>


<p>In short, for me, it's been about making small, sustainable changes and being kinder to myself. It's not always easy, and I'm still learning, but these steps have helped me find a healthier balance.</p>


<h2 id="h-our-greatest-asset">Our greatest asset</h2>


<p>You've heard it before, but let me repeat it ‚Äî our greatest asset isn't the code we write. It's us. Our health, our minds, our ability to be present and enjoy life outside of the terminal window. Software Engineers and Tech co-founders, like us, are more prone to hitting the lows. Depression doesn't care about your GitHub stars or how scalable you managed to build your Kubernetes Cluster.</p>


<p>I've learned the hard way that not every problem at work is mine to solve. I used to take every customer issue personally, letting my stress levels hit the roof. But I've gotten better at recognizing what's within my control and what's not. Can't help a customer because of a time difference or because it's outside my expertise? That's okay. There's a team for that, and it's not all on me.</p>


<p>I‚Äôve been burnt out and stressed out, and it took a toll on my work, my relationships, everything. Only after I finally started prioritizing my well-being did things change for the better. I won‚Äôt say it‚Äôs all rainbows and ponies now, but things have changed. I became a better engineer, a better leader, a better friend, and a happier person than I was.</p>


<p>If you‚Äôve read this far ‚Äî know that you‚Äôre not alone. Things do get better. If you're overwhelmed, ask for help; there‚Äôs no shame. Find what makes you tick outside of work ‚Äî be it with your family, a new hobby, or just chilling with your pets ‚Äî and give it the time it deserves.</p>


<p>Any company that measures your worth by how quickly you burn out isn't worth your time or talent. <strong>So, I will repeat it again: our greatest asset isn't the code we write. It‚Äôs us, alive, and living the life.</strong></p>


<p><strong>UPDATE April 2024:</strong> If you prefer watching videos rather than reading, or if you just want to see my face ‚Äî I recorded a video version of this article. In the podcast-style video I talk more in-depth about the stuff that I wrote here. <a href="https://www.youtube.com/watch?v=anPb6X-sXxI">Here's the link</a>. </p>


<iframe height="415" width="100%" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" data-src="https://www.youtube.com/embed/anPb6X-sXxI" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></iframe>


<p>Other <a href="https://vadimkravcenko.com/newsletter/">Newsletter</a> Issues:</p>


<ul>
<li><a href="https://vadimkravcenko.com/shorts/why-software-projects-fail/">Why software projects fail</a></li>


<li><a href="https://vadimkravcenko.com/shorts/falsehoods-junior-developers-believe-about-becoming-senior/">Falsehoods Junior Developers believe about becoming Senior</a></li>


<li><a href="https://vadimkravcenko.com/shorts/habits-of-great-software-engineers/">Habits of great software engineers</a></li>


<li><a href="https://vadimkravcenko.com/shorts/project-estimates/">Proper Software Development Estimations</a></li>
</ul>
 <div>
<h3>Reactions</h3>

<div>
<p><span>Hot!</span> The last couple of years I've been writing about CTO / Tech lead job. I've compiled all my knowledge into a printable PDF. I called it <a href="https://vadimkravcenko.com/technical-manager-guide/" data-analytics="&quot;CTOGuideButton&quot;, {&quot;props&quot;:{&quot;page&quot;:&quot;single&quot;}}">"256 Pages of No Bullshit Guide for CTOs"</a>. So if you're interested, take a look.
</p><p>
<span>Hot!</span> If you're a software engineer looking for a job, I started a <a href="https://vadimkravcenko.com/roast-my-resume/" data-analytics="&quot;RoastMyResume&quot;, {&quot;props&quot;:{&quot;page&quot;:&quot;single&quot;}}">Roast my Resume</a> service, where I record a personalized video of me "roasting" your CV, which basically means taking a hard look at your resume as a CTO and commenting on all the good and the bad parts.
</p></div>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI-generated sad girl with piano performs the text of the MIT License (617 pts)]]></title>
            <link>https://suno.com/song/da6d4a83-1001-4694-8c28-648a6e8bad0a/</link>
            <guid>39998849</guid>
            <pubDate>Thu, 11 Apr 2024 06:01:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://suno.com/song/da6d4a83-1001-4694-8c28-648a6e8bad0a/">https://suno.com/song/da6d4a83-1001-4694-8c28-648a6e8bad0a/</a>, See on <a href="https://news.ycombinator.com/item?id=39998849">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-theme="dark"><div data-theme="dark"><h2 data-theme="dark">Permission is hereby granted</h2><div data-theme="dark"><p data-theme="dark">Sad girl piano ballad; jazz-trained female singer-songwriter</p><p><span data-theme="dark">v3</span></p></div><p data-theme="dark">April 4, 2024</p></div><p data-theme="dark">Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[End of the Line? Saudi Arabia to scale back plans for desert megacity (165 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/apr/10/the-line-saudi-arabia-scaling-back-plans-105-mile-long-desert-megacity-crown-prince</link>
            <guid>39998404</guid>
            <pubDate>Thu, 11 Apr 2024 04:33:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/apr/10/the-line-saudi-arabia-scaling-back-plans-105-mile-long-desert-megacity-crown-prince">https://www.theguardian.com/world/2024/apr/10/the-line-saudi-arabia-scaling-back-plans-105-mile-long-desert-megacity-crown-prince</a>, See on <a href="https://news.ycombinator.com/item?id=39998404">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>It was billed as a glass-walled city of the future, an ambitious centrepiece of the economic plan backed by Crown Prince <a href="https://www.theguardian.com/world/mohammed-bin-salman" data-link-name="in body link" data-component="auto-linked-tag">Mohammed bin Salman</a> to transition Saudi Arabia away from oil dependency.</p><p>Now, however, plans for the mirror-clad desert metropolis called the Line have been scaled down and the project, which was envisaged to stretch 105 miles (170km) is expected to reach just a mile and a half by 2030.</p><p>Dreamed up as a linear city that would eventually be home to about 9 million people on a footprint of just 13 sq miles, the Line is part of a wider Neom project. Now at least one contractor has begun dismissing workers.</p><figure id="b6dc75ec-f325-455e-927c-de89a8d9bcb9" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-1"><picture><source srcset="https://i.guim.co.uk/img/media/54af0d2b2f224981b58a7872217d08a97d398efb/333_0_3333_2000/master/3333.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/54af0d2b2f224981b58a7872217d08a97d398efb/333_0_3333_2000/master/3333.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/54af0d2b2f224981b58a7872217d08a97d398efb/333_0_3333_2000/master/3333.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/54af0d2b2f224981b58a7872217d08a97d398efb/333_0_3333_2000/master/3333.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/54af0d2b2f224981b58a7872217d08a97d398efb/333_0_3333_2000/master/3333.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/54af0d2b2f224981b58a7872217d08a97d398efb/333_0_3333_2000/master/3333.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="A promotional image of Saudi Arabia‚Äôs Neom shows the design plan for the parallel structures, known collectively as the Line." src="https://i.guim.co.uk/img/media/54af0d2b2f224981b58a7872217d08a97d398efb/333_0_3333_2000/master/3333.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="267.026702670267" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>A promotional image of Saudi Arabia‚Äôs Neom shows the design plan for the parallel structures, known collectively as the Line.</span> Photograph: NEOM/AFP/Getty Images</figcaption></figure><p>The scaling down of Prince Mohammed‚Äôs most grandiose project was <a href="https://www.bloomberg.com/news/articles/2024-04-05/saudis-scale-back-ambition-for-1-5-trillion-desert-project-neom" data-link-name="in body link">reported by Bloomberg</a>, which said it had seen documents relating to the project.</p><figure id="446d42b7-0ddd-4f9f-8042-963e1da3cc7a" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:5,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;‚ÄòIt‚Äôs being built on our blood‚Äô: the true cost of Saudi Arabia‚Äôs $500bn megacity&quot;,&quot;elementId&quot;:&quot;446d42b7-0ddd-4f9f-8042-963e1da3cc7a&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/global-development/2020/may/04/its-being-built-on-our-blood-the-true-cost-of-saudi-arabia-5bn-mega-city-neom&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>The project, which had been slated to cost $1.5tn (¬£1.2tn), was pitched as a reinvention of urban design. However, it has long attracted scepticism and criticism, not least after the reported execution of several members of the Howeitat tribe who had protested over plans to construct on their ancestral lands.</p><p>Then there were reports of Prince Mohammed‚Äôs changing vision for the project, budget overspends and an ever-changing roster of key staff, with some who have worked on the project describing it as ‚Äúuntethered from reality‚Äù.</p><figure id="de370cd5-5774-41f7-b6f5-6d8708947467" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/010bfa1cccfada89d0b3bdb2afe6648934b24cf9/0_0_2500_3333/master/2500.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/010bfa1cccfada89d0b3bdb2afe6648934b24cf9/0_0_2500_3333/master/2500.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/010bfa1cccfada89d0b3bdb2afe6648934b24cf9/0_0_2500_3333/master/2500.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/010bfa1cccfada89d0b3bdb2afe6648934b24cf9/0_0_2500_3333/master/2500.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/010bfa1cccfada89d0b3bdb2afe6648934b24cf9/0_0_2500_3333/master/2500.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/010bfa1cccfada89d0b3bdb2afe6648934b24cf9/0_0_2500_3333/master/2500.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="handout picture provided by Saudi‚Äôs Neom project in July 2022 shows the design plan for the 500-metre tall parallel structures, known collectively as The Line," src="https://i.guim.co.uk/img/media/010bfa1cccfada89d0b3bdb2afe6648934b24cf9/0_0_2500_3333/master/2500.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="593.274" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>The city was envisaged as being car-free and connected by one of the world‚Äôs fastest trains.</span> Photograph: NEOM/AFP/Getty Images</figcaption></figure><p>According to Bloomberg, the scaling back of the Line comes as the overall Neom budget for 2024 has yet to be approved by Saudi Arabia‚Äôs sovereign wealth fund amid declining cash reserves.</p><p>Promotional presentations had suggested something out of a science fiction novel running inland into Tabuk province from the mouth of the Gulf of Aqaba where it enters the Red Sea.</p><p>A few hundred metres wide, the linear city had been sold as the future of accessible urban planning, with amenities for residents within close walking distance to accommodation and districts connected by one of the world‚Äôs fastest trains.</p><p>Promotional material described the Line in almost mystical terms: a ‚Äúcognitive city‚Äù and a ‚Äúcivilisation revolution‚Äù where amenities would be provided by artificial intelligence.</p><p>Prince Mohammed, who has long been accused of involvement in the killing of the Washington Post journalist Jamal Khashoggi in Istanbul in 2018, had described the city project as ‚Äútackling the challenges facing humanity in urban life today‚Äù to ‚Äúshine a light on alternative ways to live‚Äù.</p><figure id="5b69d0cf-ba82-4f7e-99d6-c4a233b64e42" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-3"><picture><source srcset="https://i.guim.co.uk/img/media/f8e3beff8113b4039304dd8e5bed3cbf581f9424/0_34_3500_2100/master/3500.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/f8e3beff8113b4039304dd8e5bed3cbf581f9424/0_34_3500_2100/master/3500.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/f8e3beff8113b4039304dd8e5bed3cbf581f9424/0_34_3500_2100/master/3500.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/f8e3beff8113b4039304dd8e5bed3cbf581f9424/0_34_3500_2100/master/3500.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/f8e3beff8113b4039304dd8e5bed3cbf581f9424/0_34_3500_2100/master/3500.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/f8e3beff8113b4039304dd8e5bed3cbf581f9424/0_34_3500_2100/master/3500.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Saudi Crown Prince Mohammed Bin Salman announces the Line in January 2021." src="https://i.guim.co.uk/img/media/f8e3beff8113b4039304dd8e5bed3cbf581f9424/0_34_3500_2100/master/3500.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="267" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Saudi Crown Prince Mohammed Bin Salman announces the Line in January 2021.</span> Photograph: Reuters</figcaption></figure><p>Not everyone, however, has been convinced by the prince‚Äôs glossy prospectus. Writing in the New York Times in 2021 at the time Neom released a video describing the prospects of living between the city‚Äôs silvered walls, the US journalist and author Robert Worth said: ‚ÄúTo watch the crown prince‚Äôs promotional video is to be immersed in a distinctively Saudi form of arrogance, blending religious triumphalism and royal grandiosity.‚Äù</p><p>And hubris, too, apparently.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Going in circles without a real-time clock (187 pts)]]></title>
            <link>https://rachelbythebay.com/w/2024/04/10/rtc/</link>
            <guid>39998346</guid>
            <pubDate>Thu, 11 Apr 2024 04:22:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2024/04/10/rtc/">https://rachelbythebay.com/w/2024/04/10/rtc/</a>, See on <a href="https://news.ycombinator.com/item?id=39998346">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2024/04/10/rtc/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon owes $525M in cloud-storage patent fight, US jury says (205 pts)]]></title>
            <link>https://www.reuters.com/legal/amazon-owes-525-mln-cloud-storage-patent-fight-us-jury-says-2024-04-11/</link>
            <guid>39997556</guid>
            <pubDate>Thu, 11 Apr 2024 01:33:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/amazon-owes-525-mln-cloud-storage-patent-fight-us-jury-says-2024-04-11/">https://www.reuters.com/legal/amazon-owes-525-mln-cloud-storage-patent-fight-us-jury-says-2024-04-11/</a>, See on <a href="https://news.ycombinator.com/item?id=39997556">Hacker News</a></p>
Couldn't get https://www.reuters.com/legal/amazon-owes-525-mln-cloud-storage-patent-fight-us-jury-says-2024-04-11/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[ETag and HTTP Caching (136 pts)]]></title>
            <link>https://rednafi.com/misc/etag_and_http_caching/</link>
            <guid>39996521</guid>
            <pubDate>Wed, 10 Apr 2024 22:51:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rednafi.com/misc/etag_and_http_caching/">https://rednafi.com/misc/etag_and_http_caching/</a>, See on <a href="https://news.ycombinator.com/item?id=39996521">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>One neat use case for the HTTP <code>ETag</code> header is client-side HTTP caching for <code>GET</code> requests.
Along with the <code>ETag</code> header, the caching workflow requires you to fiddle with other
conditional HTTP headers like <code>If-Match</code> or <code>If-None-Match</code>. However, their interaction can
feel a bit confusing at times.</p><p>Every time I need to tackle this, I end up spending some time browsing through the relevant
MDN docs<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup><sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup><sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> to jog my memory. At this point, I‚Äôve done it enough times to justify
spending the time to write this.</p><h2 id="caching-the-response-of-a-get-endpoint">Caching the response of a <code>GET</code> endpoint</h2><p>The basic workflow goes as follows:</p><ul><li>The client makes a <code>GET</code> request to the server.</li><li>The server responds with a <code>200 OK</code> status, including the content requested and an
<code>ETag</code> header.</li><li>The client caches the response and the <code>ETag</code> value.</li><li>For subsequent requests to the same resource, the client includes the <code>If-None-Match</code>
header with the <code>ETag</code> value it has cached.</li><li>The server regenerates the <code>ETag</code> independently and checks if the <code>ETag</code> value sent by
the client matches the generated one.<ul><li>If they match, the server responds with a <code>304 Not Modified</code> status, indicating that
the client‚Äôs cached version is still valid, and the client serves the resource from
the cache.</li><li>If they don‚Äôt match, the server responds with a <code>200 OK</code> status, including the new
content and a new <code>ETag</code> header, prompting the client to update its cache.</li></ul></li></ul><div><pre tabindex="0"><code data-lang="txt"><span><span>Client                                 Server
</span></span><span><span>  |                                       |
</span></span><span><span>  |----- GET Request --------------------&gt;|
</span></span><span><span>  |                                       |
</span></span><span><span>  |&lt;---- Response 200 OK + ETag ----------|
</span></span><span><span>  |     (Cache response locally)          |
</span></span><span><span>  |                                       |
</span></span><span><span>  |----- GET Request + If-None-Match ----&gt;|  (If-None-Match == previous ETag)
</span></span><span><span>  |                                       |
</span></span><span><span>  |       Does ETag match?                |
</span></span><span><span>  |&lt;---- Yes: 304 Not Modified -----------|  (No body sent; Use local cache)
</span></span><span><span>  |       No: 200 OK + New ETag ----------|  (Update cached response)
</span></span><span><span>  |                                       |
</span></span></code></pre></div><p>We can test this workflow with GitHub‚Äôs REST API suite via the GitHub CLI<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>. If you‚Äôve
installed the CLI and authenticated yourself, you can make a request like this:</p><p>This asks for the data associated with the user <code>rednafi</code>. The response looks as follows:</p><div><pre tabindex="0"><code data-lang="txt"><span><span>HTTP/2.0 200 OK
</span></span><span><span>Etag: W/"b8fdfabd59aed6e0e602dd140c0a0ff48a665cac791dede458c5109bf4bf9463"
</span></span><span><span>
</span></span><span><span>{
</span></span><span><span>  "login":"rednafi",
</span></span><span><span>  "id":30027932,
</span></span><span><span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>I‚Äôve truncated the response body and omitted the headers that aren‚Äôt relevant to this
discussion. You can see that the HTTP status code is <code>200 OK</code> and the server has included an
ETag header.</p><p>The <code>W/</code> prefix indicates that a weak validator<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> is used to validate the content of the
cache. Using a weak validator means when the server compares the response payload to
generate the hash, it doesn‚Äôt do it bit-by-bit. So, if your response is JSON, then changing
the format of the JSON won‚Äôt change the value of the <code>ETag</code> header since two JSON payloads
with the same content but with different formatting are semantically the same thing.</p><p>Let‚Äôs see what happens if we make the same request again while passing the value of the
<code>ETag</code> in the <code>If-None-Match</code> header.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>gh api -i -H <span>\
</span></span></span><span><span><span></span>    <span>'If-None-Match: W/"b8fdfabd59aed6e0e602dd140c0a0ff48a665cac791dede458c5109bf4bf9463"'</span> <span>\
</span></span></span><span><span><span></span>    /users/rednafi
</span></span></code></pre></div><p>This returns:</p><div><pre tabindex="0"><code data-lang="txt"><span><span>HTTP/2.0 304 Not Modified
</span></span><span><span>Etag: "b8fdfabd59aed6e0e602dd140c0a0ff48a665cac791dede458c5109bf4bf9463"
</span></span><span><span>
</span></span><span><span>gh: HTTP 304
</span></span></code></pre></div><p>This means that the cached response in the client is still valid and it doesn‚Äôt need to
refetch that from the server. So, the client can be coded to serve the previously cached
data to the users when asked for.</p><p>A few key points to keep in mind:</p><ul><li><p>Always wrap your <code>ETag</code> values in double quotes when sending them with the
<code>If-None-Match</code> header, just as the spec says<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>.</p></li><li><p>Using the <code>If-None-Match</code> header to pass the <code>ETag</code> value means that the client request
is considered successful when the <code>ETag</code> value from the client doesn‚Äôt match that of the
server. When the values match, the server will return <code>304 Not Modified</code> with no body.</p></li><li><p>If we‚Äôre writing a compliant server, when it comes to <code>If-None-Match</code>, the spec tells
us<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup> to use a weak comparison for ETags. This means that the client will still be able
to validate the cache with weak ETags, even if there have been slight changes to the
representation of the data.</p></li><li><p>If the client is a browser, it‚Äôll automatically manage the cache and send conditional
requests without any extra work.</p></li></ul><h2 id="writing-a-server-that-enables-client-side-caching">Writing a server that enables client-side caching</h2><p>If you‚Äôre serving static content, you can configure your load balancer to enable this
caching workflow. But for dynamic <code>GET</code> requests, the server needs to do a bit more work to
allow client-side caching.</p><p>Here‚Äôs a simple server in Go that enables the above workflow for a dynamic <code>GET</code> request:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>package</span> <span>main</span>
</span></span><span><span>
</span></span><span><span><span>import</span> <span>(</span>
</span></span><span><span>    <span>"crypto/sha256"</span>
</span></span><span><span>    <span>"encoding/hex"</span>
</span></span><span><span>    <span>"fmt"</span>
</span></span><span><span>    <span>"net/http"</span>
</span></span><span><span>    <span>"strings"</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>// calculateETag generates a weak ETag by SHA-256-hashing the content
</span></span></span><span><span><span>// and prefixing it with W/ to indicate a weak comparison
</span></span></span><span><span><span></span><span>func</span> <span>calculateETag</span><span>(</span><span>content</span> <span>string</span><span>)</span> <span>string</span> <span>{</span>
</span></span><span><span>    <span>hasher</span> <span>:=</span> <span>sha256</span><span>.</span><span>New</span><span>()</span>
</span></span><span><span>    <span>hasher</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>content</span><span>))</span>
</span></span><span><span>    <span>hash</span> <span>:=</span> <span>hex</span><span>.</span><span>EncodeToString</span><span>(</span><span>hasher</span><span>.</span><span>Sum</span><span>(</span><span>nil</span><span>))</span>
</span></span><span><span>    <span>return</span> <span>fmt</span><span>.</span><span>Sprintf</span><span>(</span><span>"W/\"%s\""</span><span>,</span> <span>hash</span><span>)</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>func</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>    <span>http</span><span>.</span><span>HandleFunc</span><span>(</span><span>"/"</span><span>,</span> <span>func</span><span>(</span><span>w</span> <span>http</span><span>.</span><span>ResponseWriter</span><span>,</span> <span>r</span> <span>*</span><span>http</span><span>.</span><span>Request</span><span>)</span> <span>{</span>
</span></span><span><span>        <span>// Define the content within the handler
</span></span></span><span><span><span></span>        <span>content</span> <span>:=</span> <span>`{"message": "Hello, world!"}`</span>
</span></span><span><span>        <span>eTag</span> <span>:=</span> <span>calculateETag</span><span>(</span><span>content</span><span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>// Remove quotes and W/ prefix for If-None-Match header comparison
</span></span></span><span><span><span></span>        <span>ifNoneMatch</span> <span>:=</span> <span>strings</span><span>.</span><span>TrimPrefix</span><span>(</span>
</span></span><span><span>            <span>strings</span><span>.</span><span>Trim</span><span>(</span><span>r</span><span>.</span><span>Header</span><span>.</span><span>Get</span><span>(</span><span>"If-None-Match"</span><span>),</span> <span>"\""</span><span>),</span> <span>"W/"</span><span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>// Generate a hash of the content without the W/ prefix for comparison
</span></span></span><span><span><span></span>        <span>contentHash</span> <span>:=</span> <span>strings</span><span>.</span><span>TrimPrefix</span><span>(</span><span>eTag</span><span>,</span> <span>"W/"</span><span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>// Check if the ETag matches; if so, return 304 Not Modified
</span></span></span><span><span><span></span>        <span>if</span> <span>ifNoneMatch</span> <span>==</span> <span>strings</span><span>.</span><span>Trim</span><span>(</span><span>contentHash</span><span>,</span> <span>"\""</span><span>)</span> <span>{</span>
</span></span><span><span>            <span>w</span><span>.</span><span>WriteHeader</span><span>(</span><span>http</span><span>.</span><span>StatusNotModified</span><span>)</span>
</span></span><span><span>            <span>return</span>
</span></span><span><span>        <span>}</span>
</span></span><span><span>
</span></span><span><span>        <span>// If ETag does not match, return the content and the ETag
</span></span></span><span><span><span></span>        <span>w</span><span>.</span><span>Header</span><span>().</span><span>Set</span><span>(</span><span>"ETag"</span><span>,</span> <span>eTag</span><span>)</span> <span>// Send weak ETag
</span></span></span><span><span><span></span>        <span>w</span><span>.</span><span>Header</span><span>().</span><span>Set</span><span>(</span><span>"Content-Type"</span><span>,</span> <span>"application/json"</span><span>)</span>
</span></span><span><span>        <span>w</span><span>.</span><span>WriteHeader</span><span>(</span><span>http</span><span>.</span><span>StatusOK</span><span>)</span>
</span></span><span><span>        <span>fmt</span><span>.</span><span>Fprint</span><span>(</span><span>w</span><span>,</span> <span>content</span><span>)</span>
</span></span><span><span>    <span>})</span>
</span></span><span><span>
</span></span><span><span>    <span>fmt</span><span>.</span><span>Println</span><span>(</span><span>"Server is running on http://localhost:8080"</span><span>)</span>
</span></span><span><span>    <span>http</span><span>.</span><span>ListenAndServe</span><span>(</span><span>":8080"</span><span>,</span> <span>nil</span><span>)</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><ul><li><p>The server generates a weak <code>ETag</code> for its content by creating a SHA-256 hash and adding
<code>W/</code> to the front, indicating it‚Äôs meant for weak comparison.</p><p>You could make the <code>calculateETag</code> function format-agnostic, so the hash stays the same
if the JSON format changes but the content does not. The current <code>calculateETag</code>
implementation is susceptible to format changes, and I kept it that way to keep the code
shorter.</p></li><li><p>When delivering content, the server includes this weak <code>ETag</code> in the response headers,
allowing clients to cache the content along with the <code>ETag</code>.</p></li><li><p>For subsequent requests, the server checks if the client has sent an <code>ETag</code> in the
<code>If-None-Match</code> header and weakly compares it with the current content‚Äôs <code>ETag</code> by
independently generating the hash.</p></li><li><p>If the ETags match, indicating no significant content change, the server replies with a
<code>304 Not Modified</code> status. Otherwise, it sends the content again with a <code>200 OK</code> status
and updates the ETag. When this happens, the client knows that the existing cache is
still warm and can be served without any changes to it.</p></li></ul><p>You can spin up the server by running <code>go run main.go</code> and from a different console start
making requests to it like this:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>curl -i  http://localhost:8080/foo
</span></span></code></pre></div><p>This will return the ETag header along with the JSON response:</p><div><pre tabindex="0"><code data-lang="txt"><span><span>HTTP/1.1 200 OK
</span></span><span><span>Content-Type: application/json
</span></span><span><span>Etag: W/"1d3b4242cc9039faa663d7ca51a25798e91fbf7675c9007c2b0470b72c2ed2f3"
</span></span><span><span>Date: Wed, 10 Apr 2024 15:54:33 GMT
</span></span><span><span>Content-Length: 28
</span></span><span><span>
</span></span><span><span>{"message": "Hello, world!"}
</span></span></code></pre></div><p>Now, you can make another request with the value of</p><p>the <code>ETag</code> in the <code>If-None-Match</code> header:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>curl -i -H <span>\
</span></span></span><span><span><span></span>    <span>'If-None-Match: "1d3b4242cc9039faa663d7ca51a25798e91fbf7675c9007c2b0470b72c2ed2f3"'</span> <span>\
</span></span></span><span><span><span></span>    http://localhost:8080/foo
</span></span></code></pre></div><p>This will return a <code>304 Not Modified</code> response with no body:</p><div><pre tabindex="0"><code data-lang="txt"><span><span>HTTP/1.1 304 Not Modified
</span></span><span><span>Date: Wed, 10 Apr 2024 15:57:25 GMT
</span></span></code></pre></div><p>In a real-life scenario, you‚Äôll probably factor out the caching part in middleware so that
all of your HTTP <code>GET</code> requests can be cached from the client-side without repetition.</p><h2 id="one-thing-to-look-out-for">One thing to look out for</h2><p>While writing a cache-enabled server, make sure the system is set up so that the server
always sends back the same <code>ETag</code> for the same content, even when there are multiple servers
working behind a load balancer. If these servers give out different ETags for the same
content, it can mess up how clients cache that content.</p><p>Clients use ETags to decide if content has changed. If the <code>ETag</code> value hasn‚Äôt changed, they
know the content is the same and don‚Äôt download it again, saving bandwidth and speeding up
access. But if ETags are inconsistent across servers, clients might download content they
already have, wasting bandwidth and slowing things down.</p><p>This inconsistency also means servers end up dealing with more requests for content that
clients could have just used from their cache if ETags were consistent.</p><h2>Recent posts</h2><li><a href="https://rednafi.com/misc/crossing_the_cors_crossroad/">Crossing the CORS crossroad</a></li><li><a href="https://rednafi.com/go/dysfunctional_options_pattern/">Dysfunctional options pattern in Go</a></li><li><a href="https://rednafi.com/zephyr/einstellung_effect/">Einstellung effect</a></li><li><a href="https://rednafi.com/go/strategy_pattern/">Strategy pattern in Go</a></li><li><a href="https://rednafi.com/go/anemic_stack_traces/">Anemic stack traces in Go</a></li><li><a href="https://rednafi.com/go/retry_function/">Retry function in Go</a></li><li><a href="https://rednafi.com/go/type_assertion_vs_type_switches/">Type assertion vs type switches in Go</a></li><li><a href="https://rednafi.com/python/patch_pydantic_settings_in_pytest/">Patching pydantic settings in pytest</a></li><li><a href="https://rednafi.com/go/omit_dev_dependencies_in_binaries/">Omitting dev dependencies in Go binaries</a></li><li><a href="https://rednafi.com/misc/eschewing_black_box_api_calls/">Eschewing black box API calls</a></li></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EPA Says 'Forever Chemicals' Must Be Removed from Tap Water (217 pts)]]></title>
            <link>https://www.nytimes.com/2024/04/10/climate/epa-pfas-drinking-water.html</link>
            <guid>39996433</guid>
            <pubDate>Wed, 10 Apr 2024 22:37:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/04/10/climate/epa-pfas-drinking-water.html">https://www.nytimes.com/2024/04/10/climate/epa-pfas-drinking-water.html</a>, See on <a href="https://news.ycombinator.com/item?id=39996433">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/04/10/climate/epa-pfas-drinking-water.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Why can't my mom email me? (248 pts)]]></title>
            <link>https://matduggan.com/why-cant-my-mom-email-me/</link>
            <guid>39996314</guid>
            <pubDate>Wed, 10 Apr 2024 22:21:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matduggan.com/why-cant-my-mom-email-me/">https://matduggan.com/why-cant-my-mom-email-me/</a>, See on <a href="https://news.ycombinator.com/item?id=39996314">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>An investigation into Proton encrypted email. </p><h3 id="suddenly-silence">Suddenly Silence</h3><p>I'm a big user of email, preferring long chains to messaging apps for a lot of my friends and contacts. It's nice that it isn't tied to a single device or platform and since I own my domain, I can move it from service to service whenever I want and the sender doesn't have to learn some new address. However in the last two months I suddenly stopped getting emails from a percentage of my friends and even my mom. </p><p>What I was getting instead was PGP encrypted emails with blank bodies that looked like the following:</p><figure><img src="https://matduggan.com/content/images/2024/04/image-2.png" alt="" loading="lazy" width="839" height="336" srcset="https://matduggan.com/content/images/size/w600/2024/04/image-2.png 600w, https://matduggan.com/content/images/2024/04/image-2.png 839w" sizes="(min-width: 720px) 720px"></figure><p>If I inspected the message, it was clearly an encrypted email which Fastmail doesn't support. They have a whole blog post on why they don't here: <a href="https://www.fastmail.com/blog/why-we-dont-offer-pgp/">https://www.fastmail.com/blog/why-we-dont-offer-pgp/</a> but up to this point I haven't really cared one way or the other since nobody sends me encrypted emails. </p><p>Now I knew that Proton would send encrypted emails to <em>other Proton email addresses</em>, but obviously this isn't a Proton hosted email address which it would be able to tell pretty easily with DNS. Then it got even stranger when I tried my work email and <strong>got the same error. </strong></p><figure><img src="https://matduggan.com/content/images/2024/04/image-3.png" alt="" loading="lazy" width="736" height="364" srcset="https://matduggan.com/content/images/size/w600/2024/04/image-3.png 600w, https://matduggan.com/content/images/2024/04/image-3.png 736w" sizes="(min-width: 720px) 720px"></figure><p>Checking the raw message and there it is, Proton has encrypted this email. Now this address is hosted on Google Workspaces, so at this point I'm just baffled. Can Proton email users not send emails to people on Google Workspaces email addresses? That can't possibly be right? My friends and mom using Proton would have noticed that their emails seem to always disappear into the ether for the majority of the people they email. </p><p>I open a ticket with Fastmail hoping they've seen this problem before, but no luck. Then I opened a ticket with Proton but didn't hear back as of the time of me writing this. </p><h3 id="how-proton-seems-to-work">How Proton Seems To Work</h3><p>So the reason why so many people I know are moving to Proton is they seem to be the only game in town that has cracked sending encrypted emails in the least annoying way possible. Their encryption uses asymmetric PGP key pairs with lookup for other users public keys happening on their key server. This in conjunction with their Key Transparency technology that compares lookup requests by the client with requests on the server-side allows for easy encrypted message exchanges with a high degree of safety, at least according to them. </p><p>There seems to be three classes of keys at Proton. </p><ul><li>User keys: encrypt account-specific stuff like contacts. Not shared.</li><li>Address keys: for encrypting messages and data. </li><li>Other keys: part of a key tree that leads back to the address key as the primary external key for people to use. </li></ul><p>So that makes sense that Proton can lookup address keys for users on their system. But where are my keys coming from? So in their Proton Key Transparency whitepaper they have this little snippet on page 10:</p><blockquote>For External Addresses, the server may return email encryption keys that it<br>found in the Web Key Directory (WKD) [6] (since email is hosted elsewhere).<br>The server may also return data encryption keys, used e.g. for Proton Drive.<br>The former should have an absence proof in KT, and the latter should have an<br>inclusion proof.<br>For Non-Proton Addresses, the server may also return keys that it found in the<br>WKD. This way clients can automatically encrypt emails to it. These keys won‚Äôt<br>be in ProtonKT, thus KT should return an absence proof.</blockquote><h3 id="what-the-hell-is-wkd">What The Hell Is WKD?</h3><p>WKD, or OpenPGP Web Key Directory is an IETF draft by Werner Koch. It describes a service where you can lookup OpenPGP keys by mail addresses using a service. It also allows the key owner and the mail provider to publish and revoke keys. The whole thing is very clever, an interesting way to get around the annoying parts of PGP encryption of email. You can read it here: <a href="https://www.ietf.org/archive/id/draft-koch-openpgp-webkey-service-16.txt">https://www.ietf.org/archive/id/draft-koch-openpgp-webkey-service-16.txt</a></p><p>It outlines an enrollment process by which I would signal to a WKD service that I have a key that I want to enroll into the process. The only problem is I never did that, or at least certainly can't remember doing that. I'm certainly not hosting a page with any key verification stuff. </p><p>There seems to be a way to set a CNAME record to point towards keys.openpgp.org where I do have a key set, but that isn't set up on my domain. </p><pre><code>nslookup openpgpkey.matduggan.com
Server:		2a01:4f8:c2c:123f::1
Address:	2a01:4f8:c2c:123f::1#53

Non-authoritative answer:
*** Can't find openpgpkey.matduggan.com: No answer</code></pre><p>Source here: <a href="https://keys.openpgp.org/about/usage">https://keys.openpgp.org/about/usage</a></p><p>I can't seem to find why Proton thinks they can use this key BUT I can confirm this is the key they're encrypting the emails with. </p><figure><img src="https://matduggan.com/content/images/2024/04/image-4.png" alt="" loading="lazy" width="744" height="475" srcset="https://matduggan.com/content/images/size/w600/2024/04/image-4.png 600w, https://matduggan.com/content/images/2024/04/image-4.png 744w" sizes="(min-width: 720px) 720px"></figure><h3 id="what">What?</h3><p>So it seems if your email address returns a key from <code>keys.openpgp.org</code> then Proton will encrypt the message with your public key from there, even though (as far as I can tell) I haven't opted into them using this service. I also can't seem to figure out a way to signal to them they shouldn't do it. </p><p>Alright so what happens if I just remove my key from <code>keys.openpgp.org</code>. The process is pretty simple, just go to: <a href="https://keys.openpgp.org/manage">https://keys.openpgp.org/manage</a> and follow the instructions in the email. It seems to work more or less instantly. </p><figure><img src="https://matduggan.com/content/images/2024/04/image-5.png" alt="" loading="lazy" width="662" height="258" srcset="https://matduggan.com/content/images/size/w600/2024/04/image-5.png 600w, https://matduggan.com/content/images/2024/04/image-5.png 662w"></figure><p>Alright looks like we figured it out!</p><figure><img src="https://matduggan.com/content/images/2024/04/image-6.png" alt="" loading="lazy" width="870" height="218" srcset="https://matduggan.com/content/images/size/w600/2024/04/image-6.png 600w, https://matduggan.com/content/images/2024/04/image-6.png 870w" sizes="(min-width: 720px) 720px"></figure><h3 id="proton-seriously-what-the-hell">Proton Seriously What The Hell?</h3><p>I'm at a little bit of a loss here. I totally understand sending me encrypted emails if I've gone through the steps to set the CNAME that indicates that I want to do that, but it doesn't seem like that's how the service works. As far as I can tell, the act of uploading a OpenPGP-compatible key seems to trigger their service to send it as an end-to-end encrypted message. </p><p>I'll update this with whatever I hear back from Proton but in the meantime if you stumble across this post after getting blank emails from people for months, you'll at least be able to fix it. </p><p>Is there some flag I've accidentally set somewhere that tells Proton to send me encrypted emails? Let me know at: <a href="https://c.im/@matdevdug">https://c.im/@matdevdug</a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Aider: AI pair programming in your terminal (401 pts)]]></title>
            <link>https://github.com/paul-gauthier/aider</link>
            <guid>39995725</guid>
            <pubDate>Wed, 10 Apr 2024 21:06:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/paul-gauthier/aider">https://github.com/paul-gauthier/aider</a>, See on <a href="https://news.ycombinator.com/item?id=39995725">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">aider is AI pair programming in your terminal</h2><a id="user-content-aider-is-ai-pair-programming-in-your-terminal" aria-label="Permalink: aider is AI pair programming in your terminal" href="#aider-is-ai-pair-programming-in-your-terminal"></a></p>
<p dir="auto">Aider is a command line tool that lets you pair program with GPT-3.5/GPT-4,
to edit code stored in your local git repository.
Aider will directly edit the code in your local source files,
and <a href="https://aider.chat/docs/faq.html#how-does-aider-use-git" rel="nofollow">git commit the changes</a>
with sensible commit messages.
You can start a new project or work with an existing git repo.
Aider is unique in that it lets you ask for changes to <a href="https://aider.chat/docs/repomap.html" rel="nofollow">pre-existing, larger codebases</a>.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/paul-gauthier/aider/blob/main/assets/screencast.svg"><img src="https://github.com/paul-gauthier/aider/raw/main/assets/screencast.svg" alt="aider screencast"></a>
</p>
<p dir="auto">
  <a href="https://discord.gg/Tv2uQnR88V" rel="nofollow">
    <img src="https://camo.githubusercontent.com/dfd919a27895c138700ce36a0a5e888d3c603d28784f798648443ca0006d1418/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4a6f696e2d446973636f72642d626c75652e737667" data-canonical-src="https://img.shields.io/badge/Join-Discord-blue.svg">
  </a>
</p>
<ul dir="auto">
<li><a href="#getting-started">Getting started</a></li>
<li><a href="#example-chat-transcripts">Example chat transcripts</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="https://aider.chat/docs/install.html#tutorial-videos" rel="nofollow">Tutorial videos</a></li>
<li><a href="#in-chat-commands">In-chat commands</a></li>
<li><a href="#tips">Tips</a></li>
<li><a href="https://aider.chat/docs/install.html" rel="nofollow">Installation</a></li>
<li><a href="https://aider.chat/docs/voice.html" rel="nofollow">Voice-to-code</a></li>
<li><a href="https://aider.chat/docs/faq.html" rel="nofollow">FAQ</a></li>
<li><a href="https://discord.gg/Tv2uQnR88V" rel="nofollow">Discord</a></li>
<li><a href="https://aider.chat/blog/" rel="nofollow">Blog</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">See the
<a href="https://aider.chat/docs/install.html" rel="nofollow">installation instructions</a>
for more details, but you can
get started quickly like this:</p>
<div data-snippet-clipboard-copy-content="$ pip install aider-chat
$ export OPENAI_API_KEY=your-key-goes-here
$ aider hello.js

Using git repo: .git
Added hello.js to the chat.

hello.js> write a js script that prints hello world"><pre><code>$ pip install aider-chat
$ export OPENAI_API_KEY=your-key-goes-here
$ aider hello.js

Using git repo: .git
Added hello.js to the chat.

hello.js&gt; write a js script that prints hello world
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example chat transcripts</h2><a id="user-content-example-chat-transcripts" aria-label="Permalink: Example chat transcripts" href="#example-chat-transcripts"></a></p>
<p dir="auto">Here are some example transcripts that show how you can chat with <code>aider</code> to write and edit code with GPT-4.</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://aider.chat/examples/hello-world-flask.html" rel="nofollow"><strong>Hello World Flask App</strong></a>: Start from scratch and have GPT create a simple Flask app with various endpoints, such as adding two numbers and calculating the Fibonacci sequence.</p>
</li>
<li>
<p dir="auto"><a href="https://aider.chat/examples/2048-game.html" rel="nofollow"><strong>Javascript Game Modification</strong></a>: Dive into an existing open-source repo, and get GPT's help to understand it and make modifications.</p>
</li>
<li>
<p dir="auto"><a href="https://aider.chat/examples/complex-change.html" rel="nofollow"><strong>Complex Multi-file Change with Debugging</strong></a>: GPT makes a complex code change that is coordinated across multiple source files, and resolves bugs by reviewing error output and doc snippets.</p>
</li>
<li>
<p dir="auto"><a href="https://aider.chat/examples/add-test.html" rel="nofollow"><strong>Create a Black Box Test Case</strong></a>: GPT creates a "black box" test case without access to the source of the method being tested, using only a
<a href="https://aider.chat/docs/repomap.html" rel="nofollow">high level map of the repository based on tree-sitter</a>.</p>
</li>
</ul>
<p dir="auto">You can find more chat transcripts on the <a href="https://aider.chat/examples/" rel="nofollow">examples page</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Chat with GPT about your code by launching <code>aider</code> from the command line with set of source files to discuss and edit together. Aider lets GPT see and edit the content of those files.</li>
<li>GPT can write and edit code in most popular languages: python, javascript, typescript, html, css, etc.</li>
<li>Request new features, changes, improvements, or bug fixes to your code. Ask for new test cases, updated documentation or code refactors.</li>
<li>Aider will apply the edits suggested by GPT directly to your source files.</li>
<li>Aider will <a href="https://aider.chat/docs/faq.html#how-does-aider-use-git" rel="nofollow">automatically commit each changeset to your local git repo</a> with a descriptive commit message. These frequent, automatic commits provide a safety net. It's easy to undo changes or use standard git workflows to manage longer sequences of changes.</li>
<li>You can use aider with multiple source files at once, so GPT can make coordinated code changes across all of them in a single changeset/commit.</li>
<li>Aider can <a href="https://aider.chat/docs/repomap.html" rel="nofollow">give <em>GPT-4</em> a map of your entire git repo</a>, which helps it understand and modify large codebases.</li>
<li>You can also edit files by hand using your editor while chatting with aider. Aider will notice these out-of-band edits and keep GPT up to date with the latest versions of your files. This lets you bounce back and forth between the aider chat and your editor, to collaboratively code with GPT.</li>
<li>If you are using gpt-4 through openai directly, you can add image files to your context which will automatically switch you to the gpt-4-vision-preview model</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Run the <code>aider</code> tool by executing the following command:</p>
<div data-snippet-clipboard-copy-content="aider <file1> <file2> ..."><pre><code>aider &lt;file1&gt; &lt;file2&gt; ...
</code></pre></div>
<p dir="auto">If your pip install did not place the <code>aider</code> executable on your path, you can invoke aider like this:</p>
<div data-snippet-clipboard-copy-content="python -m aider.main <file1> <file2>"><pre><code>python -m aider.main &lt;file1&gt; &lt;file2&gt;
</code></pre></div>
<p dir="auto">Replace <code>&lt;file1&gt;</code>, <code>&lt;file2&gt;</code>, etc., with the paths to the source code files you want to work on.
These files will be "added to the chat session", so that GPT can see their contents and edit them according to your instructions.</p>
<p dir="auto">You can also just launch <code>aider</code> anywhere in a git repo without naming
files on the command line.  It will discover all the files in the
repo.  You can then add and remove individual files in the chat
session with the <code>/add</code> and <code>/drop</code> chat commands described below.
If you or GPT mention one of the repo's filenames in the conversation,
aider will ask if you'd like to add it to the chat.</p>
<p dir="auto">Think about the change you want to make and which files will need
to be edited -- add those files to the chat.
Don't add <em>all</em> the files in your repo to the chat.
Be selective, and just add the files that GPT will need to edit.
If you add a bunch of unrelated files, GPT can get overwhelmed
and confused (and it costs more tokens).
Aider will automatically
share snippets from other, related files with GPT so it can
<a href="https://aider.chat/docs/repomap.html" rel="nofollow">understand the rest of your code base</a>.</p>
<p dir="auto">Aider also has many
additional command-line options, environment variables or configuration file
to set many options. See <code>aider --help</code> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">In-chat commands</h2><a id="user-content-in-chat-commands" aria-label="Permalink: In-chat commands" href="#in-chat-commands"></a></p>
<p dir="auto">Aider supports commands from within the chat, which all start with <code>/</code>. Here are some of the most useful in-chat commands:</p>
<ul dir="auto">
<li><code>/add &lt;file&gt;</code>: Add matching files to the chat session.</li>
<li><code>/drop &lt;file&gt;</code>: Remove matching files from the chat session.</li>
<li><code>/undo</code>: Undo the last git commit if it was done by aider.</li>
<li><code>/diff</code>: Display the diff of the last aider commit.</li>
<li><code>/run &lt;command&gt;</code>: Run a shell command and optionally add the output to the chat.</li>
<li><code>/voice</code>: Speak to aider to <a href="https://aider.chat/docs/voice.html" rel="nofollow">request code changes with your voice</a>.</li>
<li><code>/help</code>: Show help about all commands.</li>
</ul>
<p dir="auto">See the <a href="https://aider.chat/docs/commands.html" rel="nofollow">full command docs</a> for more information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tips</h2><a id="user-content-tips" aria-label="Permalink: Tips" href="#tips"></a></p>
<ul dir="auto">
<li>Think about which files need to be edited to make your change and add them to the chat.
Aider has some ability to help GPT figure out which files to edit all by itself, but the most effective approach is to explicitly add the needed files to the chat yourself.</li>
<li>Large changes are best performed as a sequence of thoughtful bite sized steps, where you plan out the approach and overall design. Walk GPT through changes like you might with a junior dev. Ask for a refactor to prepare, then ask for the actual change. Spend the time to ask for code quality/structure improvements.</li>
<li>Use Control-C to safely interrupt GPT if it isn't providing a useful response. The partial response remains in the conversation, so you can refer to it when you reply to GPT with more information or direction.</li>
<li>Use the <code>/run</code> command to run tests, linters, etc and show the output to GPT so it can fix any issues.</li>
<li>Use Meta-ENTER (Esc+ENTER in some environments) to enter multiline chat messages. Or enter <code>{</code> alone on the first line to start a multiline message and <code>}</code> alone on the last line to end it.</li>
<li>If your code is throwing an error, share the error output with GPT using <code>/run</code> or by pasting it into the chat. Let GPT figure out and fix the bug.</li>
<li>GPT knows about a lot of standard tools and libraries, but may get some of the fine details wrong about APIs and function arguments. You can paste doc snippets into the chat to resolve these issues.</li>
<li>GPT can only see the content of the files you specifically "add to the chat". Aider also sends GPT-4 a <a href="https://aider.chat/docs/repomap.html" rel="nofollow">map of your entire git repo</a>. So GPT may ask to see additional files if it feels that's needed for your requests.</li>
<li>I also shared some general <a href="https://news.ycombinator.com/item?id=36211879" rel="nofollow">GPT coding tips on Hacker News</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">See the <a href="https://aider.chat/docs/install.html" rel="nofollow">installation instructions</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto">For more information, see the <a href="https://aider.chat/docs/faq.html" rel="nofollow">FAQ</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Kind words from users</h2><a id="user-content-kind-words-from-users" aria-label="Permalink: Kind words from users" href="#kind-words-from-users"></a></p>
<ul dir="auto">
<li><em>The best AI coding assistant so far.</em> -- <a href="https://www.youtube.com/watch?v=df8afeb1FY8" rel="nofollow">Matthew Berman</a></li>
<li><em>Hands down, this is the best AI coding assistant tool so far.</em> -- <a href="https://www.youtube.com/watch?v=MPYFPvxfGZs" rel="nofollow">IndyDevDan</a></li>
<li><em>Aider ... has easily quadrupled my coding productivity.</em> -- <a href="https://news.ycombinator.com/item?id=36212100" rel="nofollow">SOLAR_FIELDS</a></li>
<li><em>It's a cool workflow... Aider's ergonomics are perfect for me.</em> -- <a href="https://news.ycombinator.com/item?id=38185326" rel="nofollow">qup</a></li>
<li><em>It's really like having your senior developer live right in your Git repo - truly amazing!</em> -- <a href="https://github.com/paul-gauthier/aider/issues/124" data-hovercard-type="issue" data-hovercard-url="/paul-gauthier/aider/issues/124/hovercard">rappster</a></li>
<li><em>What an amazing tool. It's incredible.</em> -- <a href="https://github.com/paul-gauthier/aider/issues/6#issue-1722897858" data-hovercard-type="issue" data-hovercard-url="/paul-gauthier/aider/issues/6/hovercard">valyagolev</a></li>
<li><em>Aider is such an astounding thing!</em> -- <a href="https://github.com/paul-gauthier/aider/issues/82#issuecomment-1631876700" data-hovercard-type="issue" data-hovercard-url="/paul-gauthier/aider/issues/82/hovercard">cgrothaus</a></li>
<li><em>It was WAY faster than I would be getting off the ground and making the first few working versions.</em> -- <a href="https://twitter.com/d_feldman/status/1662295077387923456" rel="nofollow">Daniel Feldman</a></li>
<li><em>THANK YOU for Aider! It really feels like a glimpse into the future of coding.</em> -- <a href="https://news.ycombinator.com/item?id=38205643" rel="nofollow">derwiki</a></li>
<li><em>It's just amazing.  It is freeing me to do things I felt were out my comfort zone before.</em> -- <a href="https://discord.com/channels/1131200896827654144/1174002618058678323/1174084556257775656" rel="nofollow">Dougie</a></li>
<li><em>This project is stellar.</em> -- <a href="https://github.com/paul-gauthier/aider/issues/112#issuecomment-1637429008" data-hovercard-type="issue" data-hovercard-url="/paul-gauthier/aider/issues/112/hovercard">funkytaco</a></li>
<li><em>Amazing project, definitely the best AI coding assistant I've used.</em> -- <a href="https://github.com/paul-gauthier/aider/issues/84" data-hovercard-type="issue" data-hovercard-url="/paul-gauthier/aider/issues/84/hovercard">joshuavial</a></li>
<li><em>I am an aider addict. I'm getting so much more work done, but in less time.</em> -- <a href="https://discord.com/channels/1131200896827654144/1131200896827654149/1135913253483069470" rel="nofollow">dandandan</a></li>
<li><em>After wasting $100 on tokens trying to find something better, I'm back to Aider. It blows everything else out of the water hands down, there's no competition whatsoever.</em> -- <a href="https://discord.com/channels/1131200896827654144/1131200896827654149/1178736602797846548" rel="nofollow">SystemSculpt</a></li>
<li><em>Best agent for actual dev work in existing codebases.</em> -- <a href="https://twitter.com/NickADobos/status/1690408967963652097?s=20" rel="nofollow">Nick Dobos</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nimble: A new columnar file format by Meta [video] (111 pts)]]></title>
            <link>https://www.youtube.com/watch?v=bISBNVtXZ6M</link>
            <guid>39995112</guid>
            <pubDate>Wed, 10 Apr 2024 20:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=bISBNVtXZ6M">https://www.youtube.com/watch?v=bISBNVtXZ6M</a>, See on <a href="https://news.ycombinator.com/item?id=39995112">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Slay the Spire 2 Announced ‚Äì Using Godot (106 pts)]]></title>
            <link>https://www.ign.com/articles/slay-the-spire-2-officially-announced-first-gameplay-screens-revealed</link>
            <guid>39994723</guid>
            <pubDate>Wed, 10 Apr 2024 19:21:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ign.com/articles/slay-the-spire-2-officially-announced-first-gameplay-screens-revealed">https://www.ign.com/articles/slay-the-spire-2-officially-announced-first-gameplay-screens-revealed</a>, See on <a href="https://news.ycombinator.com/item?id=39994723">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 data-cy="article-sub-headline">The iconic roguelike deck builder returns in announcement in Triple-I showcase.</h2></div><section><p>Mega Crit has announced Slay the Spire 2, the sequel to the wildly popular indie that helped inspired an entire generation of digital deckbuilders.</p><output data-cy="article-video"></output><p>Revealed during the Triple-I Showcase on Wednesday, Slay the Spire 2 was shown in a short trailer that seemed to confirm the return of the Ironclad and Silent classes while introducing a brand-new character ‚Äî The Necrobinder. The official description for the new character reads, "A wandering lich who seeks to bind the forgotten corpse. Calls up on her trusty left hand, Osty in combat."</p><p>In addition to the trailer, Mega Crit included several gameplay screens, which are stylistically quite similar to the previous game. Check them out below.</p><output data-cy="article-slideshow"><div><h3>Slay the Spire 2 - Announcement Screens</h3></div></output><p>First released in 2019, Slay the Spire earned itself a large fanbase with its innovative combination of roguelite mechanics, turn-based combat, and deckbuilding. In our <a href="https://www.ign.com/articles/2019/01/25/slay-the-spire-review">original review</a> we wrote, "[Slay the Spire] encourages experimentation, gives you time to make mistakes, and will challenge you immensely as you navigate your way through floor after floor of entertaining, puzzle-like fights. It‚Äôs an idea so good that it‚Äôs inspired a dozen games like it before it even left early access, but is executed so well that none of them even come close to matching it."</p><p>Notably, Mega Crit was one of the developers to <a href="https://www.ign.com/articles/the-unity-games-silksong-cult-of-the-lamb-among-us">vocally condemn Unity's short-lived install fee policy</a>, threatening to migrate to a new engine unless Unity's changes were "completely reverted." A Mega Crit representative confirmed that Slay the Spire 2 has since fully migrated off Unity and will instead use Godot ‚Äî  a free and open-source cross-platform engine.</p><p>Slay the Spire 2 is enter early access sometime in 2025. You can find all of the <a href="https://www.ign.com/articles/video-game-release-dates-ps4-ps5-xbox-one-series-x-nintendo-switch">biggest game release dates for 2024</a> right here.</p><p><em>Kat Bailey is IGN's News Director as well as co-host of Nintendo Voice Chat. Have a tip? Send her a DM at @the_katbot.</em></p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Libgourou: A Free Implementation of Adobe's Adept DRM on ePub/PDF Files (210 pts)]]></title>
            <link>https://forge.soutade.fr/soutade/libgourou</link>
            <guid>39994339</guid>
            <pubDate>Wed, 10 Apr 2024 18:49:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forge.soutade.fr/soutade/libgourou">https://forge.soutade.fr/soutade/libgourou</a>, See on <a href="https://news.ycombinator.com/item?id=39994339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
				<h2 id="user-content-introduction" dir="auto">Introduction</h2>
<p dir="auto">libgourou is a free implementation of Adobe's ADEPT protocol used to add DRM on ePub/PDF files. It overcome the lacks of Adobe support for Linux platforms.</p>
<h2 id="user-content-architecture" dir="auto">Architecture</h2>
<p dir="auto">Like RMSDK, libgourou has a client/server scheme. All platform specific functions (crypto, network...) has to be implemented in a client class (that derives from DRMProcessorClient) while server implements ADEPT protocol.
A reference implementation using cURL, OpenSSL and libzip is provided (in <em>utils</em> directory).</p>
<p dir="auto">Main fucntions to use from gourou::DRMProcessor are :</p>
<ul dir="auto">
<li>Get an ePub from an ACSM file : <em>fulfill()</em> and <em>download()</em></li>
<li>Create a new device : <em>createDRMProcessor()</em></li>
<li>Register a new device : <em>signIn()</em> and <em>activateDevice()</em></li>
<li>Remove DRM : <em>removeDRM()</em></li>
<li>Return loaned book : <em>returnLoan()</em></li>
</ul>
<p dir="auto">You can import configuration from (at least) :</p>
<ul dir="auto">
<li>Kobo device    : .adept/device.xml, .adept/devicesalt  and .adept/activation.xml</li>
<li>Bookeen device : .adobe-digital-editions/device.xml, root/devkey.bin and .adobe-digital-editions/activation.xml</li>
</ul>
<p dir="auto">Or create a new one. Be careful : there is a limited number of devices that can be created bye one account.</p>
<p dir="auto">ePub are encrypted using a shared key : one account / multiple devices, so you can create and register a device into your computer and read downloaded (and encrypted) ePub file with your eReader configured using the same AdobeID account.</p>
<p dir="auto">For those who wants to remove DRM without adept_remove, you can export your private key and import it within <a href="https://calibre-ebook.com/" rel="nofollow">Calibre</a> an its DeDRM plugin.</p>
<h2 id="user-content-dependencies" dir="auto">Dependencies</h2>
<p dir="auto">For libgourou :</p>
<p dir="auto"><em>externals</em> :</p>
<ul dir="auto">
<li>libpugixml</li>
</ul>
<p dir="auto"><em>internals</em> :</p>
<ul dir="auto">
<li>uPDFParser</li>
</ul>
<p dir="auto">For utils :</p>
<ul dir="auto">
<li>libcurl</li>
<li>OpenSSL</li>
<li>libzip</li>
<li>libpugixml</li>
</ul>
<p dir="auto">Internal libraries are automatically fetched and statically compiled during the first run.
When you update libgourou's repository, <strong>don't forget to update internal libraries</strong> with :</p>
<pre><code>make update_lib
</code></pre>
<h2 id="user-content-compilation" dir="auto">Compilation</h2>
<p dir="auto">Use <em>make</em> command</p>
<pre><code>make [CROSS=XXX] [DEBUG=(0*|1)] [STATIC_UTILS=(0*|1)] [BUILD_UTILS=(0|1*)] [BUILD_STATIC=(0*|1)] [BUILD_SHARED=(0|1*)] [all*|clean|ultraclean|build_utils|install|uninstall]
</code></pre>
<p dir="auto">CROSS can define a cross compiler prefix (ie arm-linux-gnueabihf-)</p>
<p dir="auto">DEBUG can be set to compile in DEBUG mode</p>
<p dir="auto">BUILD_UTILS to build utils or not</p>
<p dir="auto">STATIC_UTILS to build utils with static library (libgourou.a) instead of default dynamic one (libgourou.so)</p>
<p dir="auto">BUILD_STATIC build libgourou.a if 1, nothing if 0, can be combined with BUILD_SHARED</p>
<p dir="auto">BUILD_SHARED build libgourou.so if 1, nothing if 0, can be combined with BUILD_STATIC</p>
<p dir="auto">other variables are DESTDIR and PREFIX to handle destination install directory</p>
<ul dir="auto">
<li>Default value</li>
</ul>
<h2 id="user-content-utils" dir="auto">Utils</h2>
<p dir="auto">First, add libgourou.so to your LD_LIBRARY_PATH</p>
<pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD
</code></pre>
<p dir="auto">You can optionaly specify your .adept directory</p>
<pre><code>export ADEPT_DIR=/home/XXX
</code></pre>
<p dir="auto">Then, use utils as following :</p>
<p dir="auto">You can import configuration from your eReader or create a new one with <em>utils/adept_activate</em> :</p>
<pre><code>./utils/adept_activate -u &lt;AdobeID USERNAME&gt;
</code></pre>
<p dir="auto">Then a <em>/home//.config/adept</em> directory is created with all configuration file</p>
<p dir="auto">To download an ePub/PDF :</p>
<pre><code>./utils/acsmdownloader &lt;ACSM_FILE&gt;
</code></pre>
<p dir="auto">To export your private key (for DeDRM software) :</p>
<pre><code>./utils/acsmdownloader --export-private-key [-o adobekey_1.der]
</code></pre>
<p dir="auto">To remove ADEPT DRM :</p>
<pre><code>./utils/adept_remove &lt;encryptedFile&gt;
</code></pre>
<p dir="auto">To list loaned books :</p>
<pre><code>./utils/adept_loan_mgt [-l]
</code></pre>
<p dir="auto">To return a loaned book :</p>
<pre><code>./utils/adept_loan_mgt -r &lt;id&gt;
</code></pre>
<p dir="auto">You can get utils full options description with -h or --help switch</p>
<h2 id="user-content-docker" dir="auto">Docker</h2>
<p dir="auto">A docker image (by bcliang) is available at <a href="https://github.com/bcliang/docker-libgourou/" rel="nofollow">https://github.com/bcliang/docker-libgourou/</a></p>
<h2 id="user-content-copyright" dir="auto">Copyright</h2>
<p dir="auto">Gr√©gory Soutad√©</p>
<h2 id="user-content-license" dir="auto">License</h2>
<p dir="auto">libgourou : LGPL v3 or later</p>
<p dir="auto">utils     : BSD</p>
<h2 id="user-content-special-thanks" dir="auto">Special thanks</h2>
<ul dir="auto">
<li><em>Jens</em> for all test samples and utils testing</li>
<li><em>Milian</em> for debug &amp; code</li>
<li><em>Berwyn H</em> for all test samples, feedbacks, patches and kind donation</li>
</ul>

			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lens sort, a masked pixel sort glitch effect (114 pts)]]></title>
            <link>https://github.com/BernardZhao/lenssort</link>
            <guid>39994301</guid>
            <pubDate>Wed, 10 Apr 2024 18:47:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/BernardZhao/lenssort">https://github.com/BernardZhao/lenssort</a>, See on <a href="https://news.ycombinator.com/item?id=39994301">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Lenssort</h2><a id="user-content-lenssort" aria-label="Permalink: Lenssort" href="#lenssort"></a></p>
<p dir="auto">Using facial recognition and pixelsorting on images to create glitched, Snapchat-like lenses.</p>
<p dir="auto">Utilizes <a href="https://github.com/satyarth/pixelsort">pixelsort</a>, another project I am involved in. Make sure to check it out!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">With Docker:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/BernardZhao/lenssort.git
cd lenssort
docker-compose up
# Make sure to include the -o flag, previews won't show up in the container.
docker-compose run lenssort python -m lenssort examples/example1.jpg -m face -o example_result.png"><pre>git clone https://github.com/BernardZhao/lenssort.git
<span>cd</span> lenssort
docker-compose up
<span><span>#</span> Make sure to include the -o flag, previews won't show up in the container.</span>
docker-compose run lenssort python -m lenssort examples/example1.jpg -m face -o example_result.png</pre></div>
<p dir="auto">Manually:</p>
<p dir="auto">Requires Python 3.6 &gt;=.
Make sure you have <a href="https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf">dlib Python bindings</a> installed!</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/BernardZhao/lenssort.git
cd lenssort

# Can skip virtual environment if desired
python -m venv venv 
source venv/bin/activate

pip install -r requirements.txt

python -m lenssort %PathToImage% [mask_type] [params]"><pre>git clone https://github.com/BernardZhao/lenssort.git
<span>cd</span> lenssort

<span><span>#</span> Can skip virtual environment if desired</span>
python -m venv venv 
<span>source</span> venv/bin/activate

pip install -r requirements.txt

python -m lenssort %PathToImage% [mask_type] [params]</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mask types:</h3><a id="user-content-mask-types" aria-label="Permalink: Mask types:" href="#mask-types"></a></p>
<table>
<thead>
<tr>
<th>Mask name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>eyes</code></td>
<td>Sort the eyes.</td>
</tr>
<tr>
<td><code>face</code></td>
<td>Sort the face, within the brows and chin.</td>
</tr>
<tr>
<td><code>shuffle</code></td>
<td>Sort a polygon randomly generated over facial features.</td>
</tr>
<tr>
<td><code>censored</code></td>
<td>Sort the eye area with a thick bar.</td>
</tr>
<tr>
<td><code>facemask</code></td>
<td>Sort the area of the face under the eyes.</td>
</tr>
<tr>
<td><code>tears</code></td>
<td>Sort tear-like lines below the eyes.</td>
</tr>
</tbody>
</table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Parameters:</h3><a id="user-content-parameters" aria-label="Permalink: Parameters:" href="#parameters"></a></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Flag</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Invert</td>
<td><code>-i</code></td>
<td>Inverts the mask. May produce cool results.</td>
</tr>
<tr>
<td>Angle</td>
<td><code>-a</code></td>
<td>Sorting angle. Overrides internal default for the mask.</td>
</tr>
<tr>
<td>Output path</td>
<td><code>-o</code></td>
<td>File output path. Previews image if not provided.</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><code>python -m lenssort examples/example1.jpg -m face</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BernardZhao/lenssort/blob/master/examples/results/example1_face.png"><img src="https://github.com/BernardZhao/lenssort/raw/master/examples/results/example1_face.png" alt="example1_face_i_a90"></a></p>
<p dir="auto"><code>python -m lenssort examples/example1.jpg -m face -i -a 90</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BernardZhao/lenssort/blob/master/examples/results/example1_face_i_a90.png"><img src="https://github.com/BernardZhao/lenssort/raw/master/examples/results/example1_face_i_a90.png" alt="example1_face_i_a90"></a></p>
<p dir="auto"><code>python -m lenssort examples/example1.jpg -m eyes -i</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BernardZhao/lenssort/blob/master/examples/results/example1_eyes_i.png"><img src="https://github.com/BernardZhao/lenssort/raw/master/examples/results/example1_eyes_i.png" alt="example1_eyes_i"></a></p>
<p dir="auto"><code>python -m lenssort examples/example3.jpg -m facemask</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BernardZhao/lenssort/blob/master/examples/results/example3_facemask.png"><img src="https://github.com/BernardZhao/lenssort/raw/master/examples/results/example3_facemask.png" alt="example1_eyes_i"></a></p>
<p dir="auto"><code>python -m lenssort examples/example3.jpg -m shuffle</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BernardZhao/lenssort/blob/master/examples/results/example3_shuffle.png"><img src="https://github.com/BernardZhao/lenssort/raw/master/examples/results/example3_shuffle.png" alt="example1_eyes_i"></a></p>
<p dir="auto"><code>python -m lenssort examples/example2.jpg -m censored</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BernardZhao/lenssort/blob/master/examples/results/example2_censored.png"><img src="https://github.com/BernardZhao/lenssort/raw/master/examples/results/example2_censored.png" alt="example1_eyes_i"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Todo</h3><a id="user-content-todo" aria-label="Permalink: Todo" href="#todo"></a></p>
<ul>
<li> Expose pixelsort args: sorting function, interval function, etc.</li>
<li> Validate mask: No out of bounds</li>
<li> Mask compositions: Ex: (face - eyes + ...)</li>
</ul>
<p dir="auto">And more masks ofc üò™</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Code search is hard (283 pts)]]></title>
            <link>https://blog.val.town/blog/search-notes/</link>
            <guid>39993976</guid>
            <pubDate>Wed, 10 Apr 2024 18:15:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.val.town/blog/search-notes/">https://blog.val.town/blog/search-notes/</a>, See on <a href="https://news.ycombinator.com/item?id=39993976">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-bvzihdzo=""> <article itemscope="" itemtype="https://schema.org/Article" data-astro-cid-bvzihdzo=""> <div data-astro-cid-bvzihdzo=""> <div data-astro-cid-bvzihdzo="">  <p><img src="https://blog.val.town/_astro/tom.Cx99M2g4_Z1L13eJ.webp" srcset="https://blog.val.town/_astro/tom.Cx99M2g4_Z2jf3OO.webp 1x, https://blog.val.town/_astro/tom.Cx99M2g4_Z2f6MW8.webp 2x, https://blog.val.town/_astro/tom.Cx99M2g4_Z2aXx4r.webp 3x" alt="Tom MacWright" }="true" data-astro-cid-bvzihdzo="" width="20" height="20" loading="lazy" decoding="async"> 
on
<time datetime="2024-04-10T00:00:00.000Z"> Apr 10, 2024 </time> </p> </div> <p>Val Town‚Äôs search functionality isn‚Äôt very good. Right now it‚Äôs built on the Postgres <a href="https://www.postgresql.org/docs/current/functions-matching.html">ILIKE</a> functionality, which just performs a substring search: if your search term is in the code, it appears in search results. There‚Äôs virtually no ranking involved, and queries with multiple words are pretty poorly supported.
Better search is <a href="https://github.com/val-town/val-town-product/discussions/69">one of our most-requested features</a>.</p>
<p>I‚Äôm working on improving this, but we haven‚Äôt found a solution that fits our needs yet.
Here are some notes from our research. So far what we‚Äôve learned is that:</p>
<ul>
<li>Mainstream search solutions are designed for natural language, not code.</li>
<li>Big companies with code search needs have spent a lot of time and money
building their own custom solutions.</li>
<li>We have a lot of data already, and need a solution that scales well.</li>
<li>The infrastructure and complexity tradeoffs involved in using a separate
search service instead of a database extension are important.</li>
</ul>
<h3 id="code-search-versus-natural-language-search">Code search versus natural language search</h3>
<p>A common issue with off-the shelf search solutions is that they‚Äôre designed to work with English and other natural languages. For example, here are some of the algorithms you get by default with a usual FTS setup:</p>
<ul>
<li><strong>Stop words removal</strong>: words like ‚Äúthe‚Äù and ‚Äúit‚Äù are removed from text before it is indexed, because they‚Äôre so common that they cause more problems for performance than they‚Äôre worth.</li>
<li><strong>Stemming</strong>: this mostly <a href="https://en.wikipedia.org/wiki/Grammatical_conjugation">reverses conjugation</a>, turning a word like ‚Äúrunning‚Äù into ‚Äúrun‚Äù before it is added to the index, and doing the same for search queries, so that you can search for ‚Äúruns‚Äù and get a search result for a document with the term ‚Äúrunning.‚Äù</li>
<li><strong>Lemmatization</strong>: some search indexes are even fancy enough to substitute synonyms for more common words, so that you can search for ‚Äúexcellent‚Äù and get results for documents including ‚Äúgreat.‚Äù</li>
</ul>
<p>All together, this means that the vector derived from a document that you‚Äôre storing in the index does not look like the document at all:</p>
<div><figure><pre tabindex="0"><code><div><p><span>select</span><span> </span><span>*</span><span> </span><span>from</span><span> to_tsvector(</span><span>'english'</span><span>, </span><span>'I am writing this example sentence'</span><span>);</span></p></div><div><p><span>--- 'exampl':5 'sentenc':6 'write':3</span></p></div></code></pre></figure></div>
<p>The problem with all of these rules is that they wreak havoc on code. <code>the</code> is not a stop-word in TypeScript: it‚Äôs a valid variable name that you might want to search for. Word boundaries aren‚Äôt the same, and stemming function names doesn‚Äôt make much sense.</p>
<div><figure><pre tabindex="0"><code><div><p><span>select</span><span> </span><span>*</span><span> </span><span>from</span><span> to_tsvector(</span><span>'english'</span><span>,</span></p></div><div><p><span>&nbsp; </span><span>'function stringifyNumber(a: number): string { return a.toString() }'</span><span>);</span></p></div><div><p><span>-- 'a.tostring':7 'function':1 'number':4 'return':6 'string':5 'stringifynumb':2</span></p></div></code></pre></figure></div>
<p>This is a pretty bad index: it has words that should be stop words, like <code>function</code>, and won‚Äôt split <code>a.toString()</code> into two tokens because <code>.</code> is not a default word boundary.</p>
<h3 id="full-text-search">Full Text Search</h3>
<p>Postgres has a <a href="https://www.postgresql.org/docs/current/textsearch.html">Full Text Search</a> extension which is supported by our hosting provider, <a href="https://docs.render.com/postgresql-extensions">Render</a>. I‚Äôve used FTS in previous projects, and for certain scales, it works great. You can try and <a href="https://www.amazingcto.com/postgres-for-everything/">use Postgres for everything</a>, and frankly, so far we have: we‚Äôve been using the heck out of Postgres. It‚Äôs a fantastic piece of technology with great documentation that is well-supported by our hosting provider.</p>
<p>If we can use Postgres for something, we will: keeping infrastructure as simple as possible is essential with a small team.</p>
<p>However, the previous projects I‚Äôve used FTS for have run into performance problems and struggled to scale - <a href="https://observablehq.com/">Observable</a> ended up
moving to Elasticsearch. We have a ton of vals, and are testing the limits of a single-node Postgres cluster. It‚Äôs hard to find any accounts of code-search using FTS, though people might be quietly succeeding with it. I wanted to avoid this as a first option but keep it in my back pocket.</p>
<h3 id="pg_trgrm">pg_trgrm</h3>
<p>The solution that we‚Äôve soft-launched as the v2 search algorithm is based on <a href="https://www.postgresql.org/docs/current/pgtrgm.html"><code>pg_trgrm</code></a>, which implements <a href="https://en.wikipedia.org/wiki/Trigram_search">trigram search</a> in Postgres. Code search <em>does</em> seem to succeed with trigrams: <a href="https://swtch.com/~rsc/regexp/regexp4.html">Russ Cox‚Äôs famous (to me?) piece from 2012 tells the story of how Google Code Search</a> used trigram indexes and a special regex implementation to succeed, technically. GitHub‚Äôs <a href="https://github.blog/2023-02-06-the-technology-behind-githubs-new-code-search/">new search system</a> uses trigram search too, in addition to a lot of technology that I‚Äôm jealous of. <a href="https://github.com/sourcegraph/zoekt">Sourcegraph</a> has a trigram-based search tool that they‚Äôve inherited from Google, too.</p>
<p>Our work with the Postgres <code>pg_trgrm</code> approach has been heavily informed by <a href="https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories/">Stephen Gutekanst‚Äôs blog post series about indexing repositories locally</a> in Postgres. We‚Äôve created a <a href="https://www.postgresql.org/docs/current/gin.html">GIN</a> index with <code>gin_trgm_ops</code> on a column containing search text.</p>
<p>The conclusion so far is that this is a great solution for regex search, but we‚Äôre not doing regex search: most searches are more freeform. We‚Äôre using <a href="https://www.postgresql.org/docs/current/pgtrgm.html#PGTRGM-FUNCS-OPS">word_similarity</a> for search ranking, and it has been very hard to coax the algorithm into giving us anything like a reasonable ranking.</p>
<h3 id="the-universe-of-options">The universe of options</h3>
<table><thead><tr><th>Option</th><th>Architecture</th><th>Language</th><th>Stars</th></tr></thead><tbody><tr><td><a href="https://www.meilisearch.com/">Meilisearch</a></td><td>Standalone</td><td>Rust</td><td>41k</td></tr><tr><td><a href="https://typesense.org/">Typesense</a></td><td>Standalone</td><td>C++</td><td>17k</td></tr><tr><td><a href="https://github.com/sourcegraph/zoekt">Zoekt</a></td><td>Standalone</td><td>Go</td><td>406</td></tr><tr><td><a href="https://www.paradedb.com/">ParadeDB</a></td><td>Postgres extension</td><td>Rust</td><td>3.2k</td></tr><tr><td><a href="https://github.com/valeriansaliou/sonic">Sonic</a></td><td>Standalone</td><td>Rust</td><td>19.4k</td></tr></tbody></table>
<p>There are code-specific tools that exist, but most of them are closed-source: GitHub‚Äôs search is excellent, but is obviously the work of a dedicated team with a real time budget.</p>
<ul>
<li>Sourcegraph‚Äôs maintained fork of <a href="https://github.com/google/zoekt">Zoekt</a> is pretty cool, but is pretty fearfully niche and would be a big, new infrastructure commitment.</li>
<li><a href="https://github.com/elastic/elasticsearch">Elasticsearch</a> might be the eventual,
unavoidable solution to this problem. It doesn‚Äôt have code-specific handling, but can be
customized in nearly infinite ways. We‚Äôre not excited to start learning about Java memory
tuning and to introduce the first persistent disk storage to our application, as well as
an additional source of truth for our data. Possibly we could use Elasticsearch
<a href="https://www.elastic.co/cloud">Cloud</a> to avoid the maintenance overhead.</li>
<li><a href="https://github.com/meilisearch/meilisearch">Meilisearch</a> seems like a promising ES alternative with the shininess of ‚ú®Rust‚ú®, but they <a href="https://blog.meilisearch.com/meilisearch-vs-elasticsearch/">seem to emphasize latency over scalability</a>, and we‚Äôre not sure if the infrastructure commitment would be any lower.</li>
<li><a href="https://www.paradedb.com/">ParadeDB</a> promises to be like Elasticsearch but ‚Äújust Postgres,‚Äù which is very appealing, but we <a href="https://docs.render.com/postgresql-extensions">can‚Äôt use their extension in Render yet</a>.</li>
</ul>
<hr>
<p>In short, we‚Äôre still working on it. Searching code instead of English makes the difficulty level a bit higher. For a small team, with an incentive to keep infrastructure simple, development environments easy to set up, and data in the same place, we‚Äôre trying to be careful not to commit to something that requires constant upkeep. There‚Äôs a reason why most mid and large-sized companies have a search ‚Äúteam,‚Äù not just a search service.</p> <a href="https://github.com/val-town/val-town-blog/edit/main/src/content/blog/search-notes.md" data-astro-cid-npoeh54f=""><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" data-astro-cid-npoeh54f=""><path stroke-linecap="round" stroke-linejoin="round" d="M16.862 4.487l1.687-1.688a1.875 1.875 0 112.652 2.652L6.832 19.82a4.5 4.5 0 01-1.897 1.13l-2.685.8.8-2.685a4.5 4.5 0 011.13-1.897L16.863 4.487zm0 0L19.5 7.125" data-astro-cid-npoeh54f=""></path></svg>
Edit this page
</a> </div> </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Udio: Generate music in your favorite styles with a text prompt (208 pts)]]></title>
            <link>https://twitter.com/udiomusic/status/1778045322654003448</link>
            <guid>39993930</guid>
            <pubDate>Wed, 10 Apr 2024 18:10:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/udiomusic/status/1778045322654003448">https://twitter.com/udiomusic/status/1778045322654003448</a>, See on <a href="https://news.ycombinator.com/item?id=39993930">Hacker News</a></p>
Couldn't get https://twitter.com/udiomusic/status/1778045322654003448: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Implementation of Google's Griffin Architecture ‚Äì RNN LLM (209 pts)]]></title>
            <link>https://github.com/google-deepmind/recurrentgemma</link>
            <guid>39993626</guid>
            <pubDate>Wed, 10 Apr 2024 17:47:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google-deepmind/recurrentgemma">https://github.com/google-deepmind/recurrentgemma</a>, See on <a href="https://news.ycombinator.com/item?id=39993626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">RecurrentGemma</h2><a id="user-content-recurrentgemma" aria-label="Permalink: RecurrentGemma" href="#recurrentgemma"></a></p>
<p dir="auto">RecurrentGemma is a family of open-weights Language Models by <a href="https://deepmind.google/" rel="nofollow">Google DeepMind</a>, based on the novel <a href="https://arxiv.org/abs/2402.19427" rel="nofollow">Griffin architecture</a>. This architecture achieves fast inference when generating long sequences by replacing global attention with a mixture of local attention and linear recurrences.</p>
<p dir="auto">This repository contains the model implementation and examples for sampling and fine-tuning. We recommend most users adopt the <a href="https://github.com/google/flax">Flax</a> implementation, which is highly optimized. We also provide an un-optimized <a href="https://github.com/pytorch/pytorch">PyTorch</a> implementation for reference.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Learn more about RecurrentGemma</h3><a id="user-content-learn-more-about-recurrentgemma" aria-label="Permalink: Learn more about RecurrentGemma" href="#learn-more-about-recurrentgemma"></a></p>
<ul dir="auto">
<li>The RecurrentGemma <a href="https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf" rel="nofollow">technical report</a> gives specific details on the training and evaluation of RecurrentGemma.</li>
<li>The <a href="https://arxiv.org/abs/2402.19427" rel="nofollow">Griffin paper</a> describes the underlying model architecture.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Using Poetry</h4><a id="user-content-using-poetry" aria-label="Permalink: Using Poetry" href="#using-poetry"></a></p>
<p dir="auto">RecurrentGemma uses <a href="https://python-poetry.org/docs/" rel="nofollow">Poetry</a> for dependency
management.</p>
<p dir="auto">To install dependencies for the full project:</p>
<ul dir="auto">
<li>Checkout the code.</li>
<li><code>poetry install -E full</code> to create a virtual environment with all dependencies.</li>
<li><code>poetry shell</code> to activate the created virtual environment.</li>
</ul>
<p dir="auto">If you only need to install a subset of dependencies use one of the alternative
library-specific commands below.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Using pip</h4><a id="user-content-using-pip" aria-label="Permalink: Using pip" href="#using-pip"></a></p>
<p dir="auto">If you want to use <code>pip</code> instead of Poetry,
then create a virtual environment (run <code>python -m venv recurrentgemma-demo</code> and <code>. recurrentgemma-demo/bin/activate</code>) and:</p>
<ul dir="auto">
<li>Checkout the code.</li>
<li><code>pip install .[full]</code></li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installing library-specific packages</h4><a id="user-content-installing-library-specific-packages" aria-label="Permalink: Installing library-specific packages" href="#installing-library-specific-packages"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">JAX</h5><a id="user-content-jax" aria-label="Permalink: JAX" href="#jax"></a></p>
<p dir="auto">To install dependencies only for the JAX pathway use:
<code>poetry install -E jax</code> or (<code>pip install .[jax]</code>).</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">PyTorch</h5><a id="user-content-pytorch" aria-label="Permalink: PyTorch" href="#pytorch"></a></p>
<p dir="auto">To install dependencies only for the PyTorch pathway use:
<code>poetry install -E torch</code> (or <code>pip install .[torch]</code>).</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Tests</h5><a id="user-content-tests" aria-label="Permalink: Tests" href="#tests"></a></p>
<p dir="auto">To install dependencies required for running unit tests use:
<code>poetry install -E test</code> (or <code>pip install .[test]</code>)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Downloading the models</h3><a id="user-content-downloading-the-models" aria-label="Permalink: Downloading the models" href="#downloading-the-models"></a></p>
<p dir="auto">The model checkpoints are available through Kaggle at
<a href="http://kaggle.com/models/google/recurrentgemma" rel="nofollow">http://kaggle.com/models/google/recurrentgemma</a>.
Select either the <strong>Flax</strong> or <strong>PyTorch</strong> model variations, click the ‚§ì button
to download the model archive, then extract the contents to a local directory.</p>
<p dir="auto">In both cases, the archive contains both the model weights and
the tokenizer.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running the unit tests</h3><a id="user-content-running-the-unit-tests" aria-label="Permalink: Running the unit tests" href="#running-the-unit-tests"></a></p>
<p dir="auto">To run the tests, install the optional <code>[test]</code> dependencies (e.g. using <code>pip install .[test]</code>) from the root of the source tree, then:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">To run the example sampling script, pass the paths to the weights directory and tokenizer:</p>
<div data-snippet-clipboard-copy-content="python examples/sampling_jax.py \
  --path_checkpoint=/path/to/archive/contents/2b/ \
  --path_tokenizer=/path/to/archive/contents/tokenizer.model"><pre><code>python examples/sampling_jax.py \
  --path_checkpoint=/path/to/archive/contents/2b/ \
  --path_tokenizer=/path/to/archive/contents/tokenizer.model
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Colab notebook tutorials</h3><a id="user-content-colab-notebook-tutorials" aria-label="Permalink: Colab notebook tutorials" href="#colab-notebook-tutorials"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://colab.sandbox.google.com/github/google-deepmind/recurrentgemma/blob/main/colabs/sampling_tutorial_jax.ipynb" rel="nofollow"><code>colabs/sampling_tutorial_jax.ipynb</code></a>
contains a <a href="http://colab.google/" rel="nofollow">Colab</a> notebook with a sampling example using JAX.</p>
</li>
<li>
<p dir="auto"><a href="https://colab.sandbox.google.com/github/google-deepmind/recurrentgemma/blob/main/colabs/sampling_tutorial_pytorch.ipynb" rel="nofollow"><code>colabs/sampling_tutorial_pytorch.ipynb</code></a>
contains a <a href="http://colab.google/" rel="nofollow">Colab</a> notebook with a sampling example using PyTorch.</p>
</li>
<li>
<p dir="auto"><a href="https://colab.sandbox.google.com/github/google-deepmind/recurrentgemma/blob/main/colabs/fine_tuning_tutorial_jax.ipynb" rel="nofollow"><code>colabs/fine_tuning_tutorial_jax.ipynb</code></a>
contains a <a href="http://colab.google/" rel="nofollow">Colab</a> with a basic tutorial on how to
fine-tune RecurrentGemma for a task, such as English to French translation, using JAX.</p>
</li>
</ul>
<p dir="auto">To run these notebooks you will need to have a Kaggle account and first read and accept
the Gemma license terms and conditions from the <a href="http://kaggle.com/models/google/recurrentgemma" rel="nofollow">RecurrentGemma page</a>.
After this you can run the notebooks, which will automatically download the weights and tokenizer from there.</p>
<p dir="auto">Currently different notebooks are supported under the following hardware:</p>
<table>
<thead>
<tr>
<th>Hardware</th>
<th>T4</th>
<th>P100</th>
<th>V100</th>
<th>A100</th>
<th>TPUv2</th>
<th>TPUv3+</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sampling in Jax</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Sampling in PyTorch</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Finetuning in Jax</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚úÖ</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">System Requirements</h2><a id="user-content-system-requirements" aria-label="Permalink: System Requirements" href="#system-requirements"></a></p>
<p dir="auto">RecurrentGemma code can run on CPU, GPU or TPU.
The code has been optimized for running on TPU using the Flax implementation,
which contains a low level <a href="https://jax.readthedocs.io/en/latest/pallas/index.html" rel="nofollow">Pallas</a> kernel to perform the linear scan in the recurrent layers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We are open to bug reports and issues. Please see
<a href="https://github.com/google-deepmind/recurrentgemma/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for details on PRs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright 2024 DeepMind Technologies Limited</p>
<p dir="auto">This code is licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License. You may obtain
a copy of the License at <a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">http://www.apache.org/licenses/LICENSE-2.0</a>.</p>
<p dir="auto">Unless required by applicable law or agreed to in writing, software distributed
under the License is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR
CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<p dir="auto">This is not an official Google product.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>