<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 27 Aug 2024 03:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Erasure Coding for Distributed Systems (138 pts)]]></title>
            <link>https://transactional.blog/blog/2024-erasure-coding</link>
            <guid>41361281</guid>
            <pubDate>Mon, 26 Aug 2024 20:03:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://transactional.blog/blog/2024-erasure-coding">https://transactional.blog/blog/2024-erasure-coding</a>, See on <a href="https://news.ycombinator.com/item?id=41361281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <hr>
    
    
    
    <div id="toc">

<ul>
<li><a href="#_erasure_coding_basics">Erasure Coding Basics</a></li>
<li><a href="#_applications_in_distributed_systems">Applications in Distributed Systems</a>
<ul>
<li><a href="#_space_and_tail_latency_improvements">Space and Tail Latency Improvements</a></li>
<li><a href="#_quorum_systems">Quorum Systems</a></li>
</ul>
</li>
<li><a href="#_usage_basics">Usage Basics</a></li>
<li><a href="#_usage_not_so_basics">Usage Not So Basics</a>
<ul>
<li><a href="#_decoding_cost_variability">Decoding Cost Variability</a></li>
<li><a href="#_library_differences">Library Differences</a></li>
</ul>
</li>
<li><a href="#_implementing_erasure_codes">Implementing Erasure Codes</a>
<ul>
<li><a href="#_algorithmic_efficiency">Algorithmic Efficiency</a></li>
<li><a href="#_implementation_efficiency">Implementation Efficiency</a></li>
</ul>
</li>
<li><a href="#_references">References</a></li>
</ul>
</div>
<div id="preamble">
  <p>Suppose one has \(N\) servers across which to store a file.  One extreme is to give each of the \(N\) servers a full copy of the file.  Any server can supply a full copy of the file, so even if \(N-1\) servers are destroyed, then the file hasn’t been lost.  This provides the best durability and fault tolerance but is the most expensive in terms of storage space used.  The other extreme is to carve the data up into \(N\) equal-sized chunks, and give each server one chunk.  Reading the file will require reading all \(N\) chunks and reassembling the file.  This will provide the best cost efficiency, as each server can contribute to the file read request while using the minimum amount of storage space.</p>
<p>Erasure codes are the way to more generally describe the space of trade-offs between storage efficiency and fault tolerance.  One can say "I’d like this file carved into \(N\) chunks, such that it can still be reconstructed with any \(M\) chunks destroyed", and there’s an erasure code with those parameters which will provide the minimum-sized chunks necessary to meet that goal.</p>
<p>The simplest intuition for there being middle points in this tradeoff is to consider a file replicated across three servers such that reading from any two should be able to yield the whole contents.  We can divide the file into two pieces, the first half of the file forms the first chunk (\(A\)) and the second half of the file forms the second chunk (\(B\)).  We can then produce a third equal-sized chunk (\(C\)) that’s the exclusive or of the first two (\(A \oplus B = C\)).  By reading any two of the three chunks, we can reconstruct the whole file:</p>
<table>
<colgroup>
<col>
<col>
</colgroup>
<tbody>
<tr>
<th><p>Chunks Read</p></th>
<th><p>Reconstruct Via</p></th>
</tr>
<tr>
<td><p>\(\{A, B\}\)</p></td>
<td><p>\(A :: B\)</p></td>
</tr>
<tr>
<td><p>\(\{A, C\}\)</p></td>
<td><p>\(A :: A \oplus C =&gt; (A \oplus (A \oplus B)) =&gt; A :: B\)</p></td>
</tr>
<tr>
<td><p>\(\{B, C\}\)</p></td>
<td><p>\(B \oplus C :: B =&gt; (B \oplus (A \oplus B)) :: B =&gt; A :: B\)</p></td>
</tr>
</tbody>
</table>
<p>And all erasure codes follow this same pattern of having separate data and parity chunks.</p>
</div>
<h2 id="_erasure_coding_basics">
Erasure Coding Basics
</h2> 
<p>Configuring an erasure code revolves around one formula:</p>

<div>
<table>
<tbody><tr>
<td>
\(k\)
</td>
<td>
<p>The number of pieces the data is split into.  One must read at least this many chunks in total to be able to reconstruct the value.  Each chunk in the resulting erasure code will be \(1/k\) of the size of the original file.</p>
</td>
</tr>
<tr>
<td>
\(m\)
</td>
<td>
<p>The number of parity chunks to generate.  This is the fault tolerance of the code, or the number of reads which can fail to complete.</p>
</td>
</tr>
<tr>
<td>
\(n\)
</td>
<td>
<p>The total number of chunks that are generated.</p>
</td>
</tr>
</tbody></table>
</div>
<p>Erasure codes are frequently referred to by their \(k+m\) tuple.  It is important to note that the variable names are not consistent across all literature.  The only constant is that an erasure code written as \(x+y\) means \(x\) data chunks and \(y\) parity chunks.</p>
<p>Please enjoy a little calculator to show the effects of different \(k\) and \(m\) settings:</p>
<div x-data="{k: 3, m: 2}">
<p>
Each chunk is \(1/k = \)<kbd x-text="(100/k).toFixed(2)"></kbd>% of the size of the original data.  There are \(k + m =\)<kbd x-text="k+m"></kbd> chunks total, and together they are equivalent to \((m + k) / k =\)<kbd x-text="((m+k)/k).toFixed(2)"></kbd> full copies of the data.
</p></div>
<p>Erasure codes are incredibly attractive to storage providers, as they offer a way to fault tolerance at minimal storage overhead.
Backblaze B2 runs with \(17+3\), allowing it to tolerate 3 failures using 1.18x the storage space.  OVH Cloud uses an \(8+4\) code, allowing it to tolerate 4 failures using 1.5x the storage space.  Scaleway uses a \(6+3\) code, tolerating three failures using 1.5x the storage space.  "Cloud storage reliability for Big Data applications"<a id="_sideref_1"></a><sup>[1]</sup> pays significant attention to the subject of erasure coding due to the fundamental role it plays in increasing durability for storage providers at a minimal cost of additional storage space.
<span><a id="_sidedef_1"></a>[1]: Rekha Nachiappan, Bahman Javadi, Rodrigo N. Calheiros, and Kenan M. Matawie. 2017. Cloud storage reliability for Big Data applications. <em>J. Netw. Comput. Appl.</em> 97, C (November 2017), 35–47. <a href="https://scholar.google.com/scholar?cluster=12723199345811969350">[scholar]</a></span></p>
<p>The main trade-off in erasure coding is a reduction in storage space used at the cost of an increase in requests issued to read data.  Rather than issuing one request to read a file-sized chunk from one disk, requests are issued to \(k+m\) disks.  Storage systems meant for infrequently accessed data, form ideal targets for erasure coding.  Infrequent access means issuing more IO operations per second won’t be a problematic tax, and the storage savings are significant when compared to storing multiple full copies of every file.</p>
<p>"Erasure coding" describes a general class of algorithms and not any one algorithm in particular.  In general, Reed-Solomon codes can be used to implement any \(k+m\) configuration of erasure codes.  Due to the prevalence of <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels">RAID</a>, special attention in erasure coding research has been paid to developing more efficient algorithms specialized for implementing these specific subsets of erasure coding. RAID-0 is \(k+0\) erasure coding.  RAID-1 is \(1+m\) erasure coding.  RAID-4 and RAID-5 are slightly different variations of \(k+1\) erasure coding.  RAID-6 is \(k+2\) erasure coding.  Algorithms specifically designed for these cases are mentioned in the implementation section below, but it’s also perfectly fine to not be aware of what exact algorithm is being used to implement the choice of a specific \(k+m\) configuration.</p>
<p>Everything described in this post is about <em>Minimum Distance Separable</em> (MDS) erasure codes, which are only one of many erasure code families.  MDS codes provide the quorum-like property that any \(m\) chunks can be used to reconstruct the full value.  Other erasure codes take other tradeoffs, where some combinations of less than \(m\) chunks can be used to reconstruct the full value, but other combinations require more than \(m\) chunks.  "Erasure Coding in Windows Azure Storage"<a id="_sideref_2"></a><sup>[2]</sup> nicely explains the motivation of why Azure devised Local Reconstruction Codes for their deployment.  "SD Codes: Erasure Codes Designed for How Storage Systems Really Fail"<a id="_sideref_3"></a><sup>[3]</sup> pitches specializing an erasure code towards recovering from sector failures, as the most common failure type.  Overall, if one has knowledge about the expected pattern of failures, then a coding scheme that allow recovering from expected failures with less than \(m\) chunks, and unexpected failures with more than \(m\) chunks would have a positive expected value.
<span><a id="_sidedef_2"></a>[2]: Cheng Huang, Huseyin Simitci, Yikang Xu, Aaron Ogus, Brad Calder, Parikshit Gopalan, Jin Li, and Sergey Yekhanin. 2012. Erasure Coding in Windows Azure Storage. In <em>2012 USENIX Annual Technical Conference (USENIX ATC 12)</em>, USENIX Association, Boston, MA, 15–26. <a href="https://scholar.google.com/scholar?cluster=7930684733311413322">[scholar]</a><br>
         <a id="_sidedef_3"></a>[3]: James S. Plank, Mario Blaum, and James L. Hafner. 2013. SD Codes: Erasure Codes Designed for How Storage Systems Really Fail. In <em>11th USENIX Conference on File and Storage Technologies (FAST 13)</em>, USENIX Association, San Jose, CA, 95–104. <a href="https://scholar.google.com/scholar?cluster=6762112190773483176">[scholar]</a></span></p>
<h2 id="_applications_in_distributed_systems">
Applications in Distributed Systems
</h2> 
<h3 id="_space_and_tail_latency_improvements">
Space and Tail Latency Improvements
</h3> 
<p>The most direct application is in reducing the storage cost and increasing the durability of data in systems with a known, fixed set of replicas.
Think of blob/object storage or NFS storage.  A metadata service maps a file path to a server that stores the file.  Instead of having 3 replicas storing the full file each, have 15 replicas store the chunks of the (10+5) erasure coded file.  Such a coding yields half the total amount of data to store, and more than double the fault tolerance.</p>
<p>More generally, this pattern translates to "instead of storing data across \(X\) servers, consider storing it across \(X+m\) replicas with an \(X+m\) erasure code".  Over on Marc Brooker’s blog, this is illustrated <a href="https://brooker.co.za/blog/2023/01/06/erasure.html">using a caching system</a>.  Instead of using consistent hashing to identify one of \(k\) cache servers to query, one can use a \(k+m\) erasure code with \(k+m\) cache servers and not have to wait for the \(m\) slowest responses.  This provides both a storage space and tail latency improvement.</p>
<p>Again, the space and latency savings do come at a cost, which is an increase in IOPS/QPS, or effectively CPU.  In both cases, we’re betting that the limiting resource which determines how many machines or disks we need to buy is storage capacity, and that we can increase our CPU usage to decrease the amount of data that needs to be stored.  If the system is already pushing its CPU limits, then erasure coding might not be a cost-saving idea.</p>
<h3 id="_quorum_systems">
Quorum Systems
</h3> 
<p>Consider a quorum system with 5 replicas, where one must read from and write to at least 3 of them, a simple majority.  Erasure codes are well matched on the read side, where a \(3+2\) erasure code equally represents that a read may be completed using the results from any 3 of the 5 replicas.  Unfortunately, the rule is that writes are allowed to complete as long as they’re received by any 3 replicas, so one could only use a \(1+2\) code, which is exactly the same as writing three copies of the file.  Thus, there are no trivial savings to be had by applying erasure coding.</p>
<p>RS-Paxos<a id="_sideref_4"></a><sup>[4]</sup> examined the applicability of erasure codes to Paxos, and similarly concluded that the only advantage is when there’s an overlap between two quorums of more than one replica.  A quorum system of 7 replicas, where one must read and write to at least 5 of them would have the same 2 replica fault tolerance, but would be able to apply a \(3+2\) erasure code.  In general, with \(N\) replicas and a desired fault tolerance of \(f\), the best one can do with a fixed erasure coding scheme is \((N-2f)+f\).
<span><a id="_sidedef_4"></a>[4]: Shuai Mu, Kang Chen, Yongwei Wu, and Weimin Zheng. 2014. When paxos meets erasure code: reduce network and storage cost in state machine replication. In <em>Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing</em> (HPDC '14), Association for Computing Machinery, New York, NY, USA, 61–72. <a href="https://scholar.google.com/scholar?cluster=16520033292975033789">[scholar]</a></span></p>
<p>HRaft<a id="_sideref_5"></a><sup>[5]</sup> explores that there is a way to get the desired improvement from a simple majority quorum, but adapting the coding to match the number of available replicas.  When all 5 replicas are available then we may use a \(3+2\) encoding, when 4 are available then use a \(2+2\) encoding, and when only 3 are available then use a \(1+2\) encoding<a id="_sideref_6"></a><sup>[6]</sup>.  Adapting the erasure code to the current replica availability yields our optimal improvement, but comes with a number of drawbacks.  Each write is optimistic in guessing the number of replicas that are currently available, and writes must be re-coded and resent to all replicas if one replica unexpectedly doesn’t acknowledge the write.  Additionally, one must still provision the system such that a replica storing the full value of every write is possible, so that after two failures, the system running in a \(1+2\) configuration won’t cause unavailability due to lacking disk space or throughput.  However, if failures are expected to be rare and will be recovered from quickly, then HRaft’s adaptive encoding scheme will yield significant improvements.
<span><a id="_sidedef_5"></a>[5]: Yulei Jia, Guangping Xu, Chi Wan Sung, Salwa Mostafa, and Yulei Wu. 2022. HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks. In <em>2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</em>, 1316–1326. <a href="https://scholar.google.com/scholar?cluster=15724086733201598850">[scholar]</a></span>
<span><a id="_sidedef_6"></a>[6]: And just to emphasize again, a \(1+2\) erasure encoding is just 3 full copies of the data.  It’s the same as not applying any erasure encoding.  The only difference is that it’s promised that only three full copies of the data are generated and sent to replicas.</span></p>
<h2 id="_usage_basics">
Usage Basics
</h2> 
<p>For computing erasure codings, there is a mature and standard <a href="https://jerasure.org/">Jerasure</a>.  If on a modern Intel processor, the Intel <a href="https://www.intel.com/content/www/us/en/developer/tools/isa-l/overview.html">Intelligent Storage Acceleration Library</a> is a SIMD-optimized library consistently towards the top of the benchmarks.</p>
<p>As an example, we can use <a href="https://pypi.org/project/pyeclib/">pyeclib</a> as a way to get easy access to an erasure coding implementation from python, and apply it to specifically to HRaft’s proposed adaptive erasure coding scheme:</p>
<details>
<summary>Python source code</summary>
<div>
<pre><code data-lang="python"><span>#!/usr/bin/env python
# Usage: ./ec.py &lt;K&gt; &lt;M&gt;
</span><span>import</span> <span>sys</span>
<span>K</span> <span>=</span> <span>int</span><span>(</span><span>sys</span><span>.</span><span>argv</span><span>[</span><span>1</span><span>])</span>
<span>M</span> <span>=</span> <span>int</span><span>(</span><span>sys</span><span>.</span><span>argv</span><span>[</span><span>2</span><span>])</span>

<span># Requires running the following to install dependencies:
# $ pip install --user pyeclib
# $ sudo dnf install liberasurecode-devel
</span><span>import</span> <span>pyeclib.ec_iface</span> <span>as</span> <span>ec</span>

<span># liberasurecode_rs_vand is built into liberasurecode, so this
# shouldn't have any other dependencies.
</span><span>driver</span> <span>=</span> <span>ec</span><span>.</span><span>ECDriver</span><span>(</span><span>ec_type</span><span>=</span><span>'liberasurecode_rs_vand'</span><span>,</span>
                     <span>k</span><span>=</span><span>K</span><span>,</span> <span>m</span><span>=</span><span>M</span><span>,</span> <span>chksum_type</span><span>=</span><span>'none'</span><span>)</span>
<span>data</span> <span>=</span> <span>bytes</span><span>([</span><span>i</span> <span>%</span> <span>100</span> <span>+</span> <span>32</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>10000</span><span>)])</span>
<span>print</span><span>(</span><span>f</span><span>"Erasure Code(K data chunks = </span><span>{</span><span>K</span><span>}</span><span>, M parity chunks = </span><span>{</span><span>M</span><span>}</span><span>)"</span>
      <span>f</span><span>" of </span><span>{</span><span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes"</span><span>)</span>

<span># Produce the coded chunks.
</span><span>chunks</span> <span>=</span> <span>driver</span><span>.</span><span>encode</span><span>(</span><span>data</span><span>)</span>

<span># There's some metdata that's prefixed onto each chunk to identify
# its position.  This isn't technically required, but there isn't
# an easy way to disable it.  There's also some additional bytes
# which I can't account for.
</span><span>metadata_size</span> <span>=</span> <span>len</span><span>(</span><span>driver</span><span>.</span><span>get_metadata</span><span>(</span><span>chunks</span><span>[</span><span>0</span><span>]))</span>
<span>chunk_size</span> <span>=</span> <span>len</span><span>(</span><span>chunks</span><span>[</span><span>0</span><span>])</span> <span>-</span> <span>metadata_size</span>
<span>print</span><span>(</span><span>f</span><span>"Encoded into </span><span>{</span><span>len</span><span>(</span><span>chunks</span><span>)</span><span>}</span><span> chunks of </span><span>{</span><span>chunk_size</span><span>}</span><span> bytes"</span><span>)</span>
<span>print</span><span>(</span><span>""</span><span>)</span>

<span># This replication scheme is X% less efficient than writing 1 copy
</span><span>no_ec_size</span> <span>=</span> <span>(</span><span>K</span><span>+</span><span>M</span><span>)</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>"No EC: </span><span>{</span><span>(</span><span>M</span><span>+</span><span>K</span><span>)</span><span>*</span><span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes, </span><span>{</span><span>1</span><span>/</span><span>(</span><span>K</span><span>+</span><span>M</span><span>)</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency"</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>"Expected: </span><span>{</span><span>(</span><span>M</span><span>+</span><span>K</span><span>)</span><span>/</span><span>K</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes,"</span>
      <span>f</span><span>" </span><span>{</span><span>1</span><span>/</span> <span>(</span><span>1</span><span>/</span><span>K</span> <span>*</span> <span>(</span><span>K</span><span>+</span><span>M</span><span>))</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency"</span><span>)</span>
<span>total_ec_size</span> <span>=</span> <span>chunk_size</span> <span>*</span> <span>len</span><span>(</span><span>chunks</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>"Actual: </span><span>{</span><span>total_ec_size</span><span>}</span><span> bytes,"</span>
      <span>f</span><span>" </span><span>{</span><span>len</span><span>(</span><span>data</span><span>)</span> <span>/</span> <span>total_ec_size</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency"</span><span>)</span>

<span># Validate that our encoded data decodes using minimal chunks
</span><span>import</span> <span>random</span>
<span>indexes</span> <span>=</span> <span>random</span><span>.</span><span>sample</span><span>(</span><span>range</span><span>(</span><span>K</span><span>+</span><span>M</span><span>),</span> <span>K</span><span>)</span>
<span># Prepended metadata is used to determine the chunk part number
# from the data itself.  Other libraries require this to be
# passed in as part of the decode call.
</span><span>decoded_data</span> <span>=</span> <span>driver</span><span>.</span><span>decode</span><span>([</span><span>chunks</span><span>[</span><span>idx</span><span>]</span> <span>for</span> <span>idx</span> <span>in</span> <span>indexes</span><span>])</span>
<span>assert</span> <span>decoded_data</span> <span>==</span> <span>data</span></code></pre>
</div>
</details>
<p>When there are 5/5 replicas available, HRaft would use a \(3+2\) erasure code:</p>
<div>
<pre>$ ./ec.py 3 2
Erasure Code(K data chunks = 3, M parity chunks = 2) of 10000 bytes
Encoded into 5 chunks of 3355 bytes

No EC: 50000 bytes, 20% efficiency
Expected: 16666.666666666668 bytes, 60.00000000000001% efficiency
Actual: 16775 bytes, 59.61251862891207% efficiency</pre>
</div>
<p>When there are 4/5 replicas available, HRaft would use a \(2+2\) erasure code:</p>
<div>
<pre>$ ./ec.py 2 2
Erasure Code(K data chunks = 2, M parity chunks = 2) of 10000 bytes
Encoded into 4 chunks of 5021 bytes

No EC: 40000 bytes, 25% efficiency
Expected: 20000.0 bytes, 50% efficiency
Actual: 20084 bytes, 49.790878311093406% efficiency</pre>
</div>
<p>When there are 3/5 replicas available, HRaft would use a \(1+2\) erasure code:</p>
<div>
<pre>$ ./ec.py 1 2
Erasure Code(K data chunks = 1, M parity chunks = 2) of 10000 bytes
Encoded into 3 chunks of 10021 bytes

No EC: 30000 bytes, 33.33333333333333% efficiency
Expected: 30000.0 bytes, 33.33333333333333% efficiency
Actual: 30063 bytes, 33.263480025280245% efficiency</pre>
</div>
<h2 id="_usage_not_so_basics">
Usage Not So Basics
</h2> 
<p>As always, things aren’t quite perfectly simple.</p>
<h3 id="_decoding_cost_variability">
Decoding Cost Variability
</h3> 
<p>Decoding performance varies with the number of data chunks that need to be recovered.  Decoding a \(3+2\) code from the three data chunks is computationally trivial.  Decoding the same file from two data chunks and one parity chunk involves solving a system of linear equations via Gaussian elimination, and the computational increases as the number of required parity chunks involved increases.  Thus, if using an erasure code as part of a quorum system, be aware that the CPU cost of decoding will vary depending on exactly which replicas reply.</p>
<p>There are a few different papers comparing different erasure code implementations and their performance across varying block size and number of data chunks to reconstruct.  I’ll suggest "Practical Performance Evaluation of Space Optimal Erasure Codes for High Speed Data Storage Systems"<a id="_sideref_7"></a><sup>[7]</sup> as the one I liked the most, from which the following figure was taken:
<span><a id="_sidedef_7"></a>[7]: Rui Chen and Lihao Xu. 2019. Practical Performance Evaluation of Space Optimal Erasure Codes for High-Speed Data Storage Systems. <em>SN Comput. Sci.</em> 1, 1 (December 2019). <a href="https://scholar.google.com/scholar?cluster=9222594581704961566">[scholar]</a></span></p>

<div>

<p><img src="https://transactional.blog/images/blog/2024-erasure-coding/decoding_performance-45d237e9.png" alt="decoding performance">
</p>
</div>
<h3 id="_library_differences">
Library Differences
</h3> 
<p>Liberasurecode abstracts over most common erasure coding implementation libraries, but be aware that does not mean that the implementations are equivalent.  Just because two erasure codes are both \(3+2\) codes doesn’t mean the same math was used to construct them.</p>
<p>Correspondingly, liberasurecode doesn’t <em>just</em> do the linear algebra work, it "helpfully" adds metadata necessary to configure which decoder to use and how, which you can’t disable or modify:</p>
<div>
<p>liberasurecode / erasurecode.h</p>
<div>
<pre><code data-lang="c"><span>struct</span> <span>__attribute__</span><span>((</span><span>__packed__</span><span>))</span>
<span>fragment_metadata</span>
<span>{</span>
    <span>uint32_t</span>    <span>idx</span><span>;</span>                <span>/* 4 */</span>
    <span>uint32_t</span>    <span>size</span><span>;</span>               <span>/* 4 */</span>
    <span>uint32_t</span>    <span>frag_backend_metadata_size</span><span>;</span>    <span>/* 4 */</span>
    <span>uint64_t</span>    <span>orig_data_size</span><span>;</span>     <span>/* 8 */</span>
    <span>uint8_t</span>     <span>chksum_type</span><span>;</span>        <span>/* 1 */</span>
    <span>uint32_t</span>    <span>chksum</span><span>[</span><span>LIBERASURECODE_MAX_CHECKSUM_LEN</span><span>];</span> <span>/* 32 */</span>
    <span>uint8_t</span>     <span>chksum_mismatch</span><span>;</span>    <span>/* 1 */</span>
    <span>uint8_t</span>     <span>backend_id</span><span>;</span>         <span>/* 1 */</span>
    <span>uint32_t</span>    <span>backend_version</span><span>;</span>    <span>/* 4 */</span>
<span>}</span> <span>fragment_metadata_t</span><span>;</span></code></pre>
</div>
</div>
<p>This is just a liberasurecode thing.  Using either Jerasure or ISA-L directly allows access to only the erasure coded data.  It <em>is</em> required as part of the APIs that each chunk must be provided along with if it was the Nth data or parity chunk, so the index must be maintained somehow as part of metadata.</p>
<p>As was noted in the <a href="https://www.youtube.com/watch?v=URAm-bbst-o">YDB talk at HydraConf</a>, Jerasure does a permutation of the output from what one would expect from just the linear algebra.  This means that it’s up to the specific implementation details of a library as to if reads must be aligned with writes — Jerasure cannot read a subset or superset of what was encoded.  ISA-L applies no permutation, so reads may decode unaligned subsets or supersets of encoded data.</p>
<p>Jerasure and ISA-L are, by far, the most popular libraries for erasure coding, but they’re not the only ones.  <a href="https://github.com/tahoe-lafs/zfec">tahoe-lafs/zfec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> is also a reasonably well-known implementation.  Christopher Taylor has written at least three MDS erasure coding implementations taking different tradeoffs (<a href="https://github.com/catid/cm256">catid/cm256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>, <a href="https://github.com/catid/longhair">catid/longhair<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>, <a href="https://github.com/catid/leopard">catid/leopard<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>), and a comparison and discussion of the differences can be found on <a href="https://github.com/catid/leopard/blob/master/Benchmarks.md">leopard’s benchmarking results page</a>.  If erasure coding becomes a bottleneck, a library more optimized for your specific use case can likely be found somewhere, but ISA-L is generally good enough.</p>
<h2 id="_implementing_erasure_codes">
Implementing Erasure Codes
</h2> 
<p>It is entirely acceptable and workable to treat erasure codes as a magic function that turns 1 file into \(n\) chunks and back.  You can stop reading here, and not knowing the details of what math is being performed will not hinder your ability to leverage erasure codes to great effect in distributed systems or databases.  (And if you continue, take what follows with a large grain of salt, as efficient erasure coding is a subject folk have spent years on, and the below is what I’ve collected from a couple of days of reading through papers I only half understand.)</p>
<p>The construction of the \(n\) chunks is some linear algebra generally involving a Galois Field, none of which is important to understand to be able to productively <em>use</em> erasure codes.  Backblaze published <a href="https://www.backblaze.com/blog/reed-solomon/">a very basic introduction</a>.  The best introduction to the linear algebra of erasure coding that I’ve seen is Fred Akalin’s <a href="https://www.akalin.com/intro-erasure-codes">"A Gentle Introduction to Erasure Codes"</a>.  <a href="https://tomverbeure.github.io/2022/08/07/Reed-Solomon.html">Reed-Solomon Error Correcting Codes from the Bottom Up</a> covers Reed-Solomon codes and Galois Field polynomials specifically.  There’s also a plethora of erasure coding-related questions on the Stack Overflow family of sites, so any question over the math that one might have has already likely been asked and answered there.</p>
<p>With the basics in place, there are two main dimensions to investigate: what is the exact MDS encoding and decoding algorithm to implement, and how can one implement that algorithm most efficiently?</p>
<h3 id="_algorithmic_efficiency">
Algorithmic Efficiency
</h3> 
<p>In general, most MDS codes are calculated as a matrix multiplication, where addition is replaced with XOR, and multiply is replaced with a more expensive multiplication over GF(256).  For the special cases of 1-3 parity chunks (\(m \in \{1,2,3\}\)), there are algorithms not derived from Reed-Solomon and which use only XORs:</p>
<div>
<ul>
<li>
<p>\(m=1\) is a trivial case of a single parity chunk, which is just the XOR of all data chunks.</p>
</li>
<li>
<p>\(m=2\) is also known as RAID-6, for which I would recommend Liberation codes<a id="_sideref_8"></a><sup>[8]</sup><a id="_sideref_9"></a><sup>[9]</sup> as <em>nearly</em> optimal with an implementation available as part of <a href="https://jerasure.org/">Jerasure</a>, and HDP codes<a id="_sideref_10"></a><sup>[10]</sup> and EVENODD<a id="_sideref_11"></a><sup>[11]</sup> as notable but patented.  If \(k+m+2\) is prime, then X-Codes<a id="_sideref_12"></a><sup>[12]</sup> are also optimal.</p>
</li>
<li>
<p>\(m=3\) can be done via STAR coding<a id="_sideref_13"></a><sup>[13]</sup>.</p>
</li>
</ul>
</div>

<p>Otherwise and more generally, a form of Reed-Solomon coding is used.  The encoding/decoding matrix is either a \(k \times n\) Vandermonde<a id="_sideref_14"></a><sup>[14]</sup> matrix with the upper \(k \times k\) of it Gaussian eliminated to form an identity matrix, or an \(k \times k\) identity matrix with a \(k \times m\) Cauchy<a id="_sideref_15"></a><sup>[15]</sup> matrix glued onto the bottom.  In both cases, the goal is to form a matrix where the top \(k \times k\) is an identity matrix (so that each data chunk is preserved), and any deletion of \(m\) rows yields an invertible matrix.  Encoding is multiplying by this matrix, and decoding deletes the rows corresponding to erased chunks, and then solves the matrix as a system of linear equations for the missing data.</p>
<p>Gaussian elimination, as used in ISA-L, is the simplest method of decoding, but also the slowest.  For Cauchy matrixes, this can be improved<a id="_sideref_16"></a><sup>[16]</sup>, as done in <a href="https://github.com/catid/cm256">catid/cm256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>.  The current fastest methods appear to be implemented in <a href="https://github.com/catid/leopard">catid/leopard<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>, which uses Fast Fourier Transforms<a id="_sideref_17"></a><sup>[17]</sup><a id="_sideref_18"></a><sup>[18]</sup> for encoding and decoding.</p>

<h3 id="_implementation_efficiency">
Implementation Efficiency
</h3> 
<p>There are levels of implementation efficiency for erasure codes that function over any \(k+m\) configuration:</p>
<div>
<ol>
<li>
<p>Implement the algorithm in C, and rely on the compiler for auto-vectorization.</p>
<p>This provides the most straightforward and most portable implementation, at acceptable performance.  Usage of <code>restrict</code> and ensuring the appropriate architecture-specific compilation flags have been specified (e.g. <code>-march=native</code>).</p>
</li>
<li>
<p>Rely on a vectorization library or compiler intrinsics to abstract the platform specifics.</p>
<p><a href="https://github.com/google/highway">google/highway<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> and <a href="https://github.com/xtensor-stack/xsimd">xtensor-stack/xsimd<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> appear to be reasonably commonly used libraries that try to use the best available SIMD instructions to accomplish general tasks.  There is also the upcoming <a href="https://en.cppreference.com/w/cpp/experimental/simd/simd"><code>std::experimental::simd</code></a>.  C/C++ compilers also offer <a href="https://gcc.gnu.org/onlinedocs/gcc/Vector-Extensions.html">builtins</a> for vectorization support.</p>
<p>The core of encoding and decoding is Galois field multiply and addition.  Optimized libraries for this can be found at <a href="https://github.com/catid/gf256">catid/gf256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> and <a href="https://web.eecs.utk.edu/~jplank/plank/papers/CS-07-593/">James Plank’s Fast Galois Field Arithmetic Library</a>.</p>
</li>
<li>
<p>Handwrite a vectorized implementation of the core encoding and decoding functions.</p>
<p>Further discussion of fast GF(256) operations can be found in the PARPAR project: <a href="https://github.com/animetosho/ParPar/blob/master/fast-gf-multiplication.md">fast-gf-multiplication</a> and the <a href="https://github.com/animetosho/ParPar/blob/master/xor_depends/info.md">xor_depends work</a>.  The consensus appears to be that a XOR-only GF multiply should be faster than a table-driven multiply.</p>

</li>
</ol>
</div>
<p>Optimizing further involves specializing the code to one specific \(k+m\) configuration by transforming the matrix multiplication with a constant into a linear series of instructions, and then:</p>
<div>
<ol start="4">
<li>
<p>Find an optimal coding matrix and XOR schedule for the specific GF polynomial and encoding matrix.</p>

</li>
<li>
<p>Apply further operation, memory, and cache optimizations.</p>

<p>The code is publicly available at <a href="https://github.com/yuezato/xorslp_ec">yuezato/xorslp_ec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>.</p>
</li>
<li>
<p>Programmatically explore an optimized instruction schedule for a specific architecture.</p>

<p>The code is publicly available at <a href="https://github.com/Thesys-lab/tvm-ec">Thesys-lab/tvm-ec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>.</p>
</li>
</ol>
</div>
<p>For a more fully explored treatment of this topic, please see <a href="https://www.usenix.org/conference/fast19/presentation/zhou">"Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques"</a><a id="_sideref_19"></a><sup>[19]</sup>, which also has a video of the presenter if that’s your preferred medium.
<span><a id="_sidedef_19"></a>[19]: Tianli Zhou and Chao Tian. 2020. Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques. <em>ACM Trans. Storage</em> 16, 1 (March 2020). <a href="https://scholar.google.com/scholar?cluster=15189943361362749273">[scholar]</a></span></p>
<h2 id="_references">
References
</h2> 
<p><a href="https://transactional.blog/blog/2024-erasure-coding.bib">References as BibTeX</a></p>
<p>And if you’re looking to broadly dive deeper, I’d suggest starting with reviewing <a href="https://dblp.org/pid/07/3005.html">James S. Plank’s publications</a>.</p>
    <!-- TODO: consider https://utteranc.es/ for in-page comments. -->
      <hr>
      
      <p><a href="https://discu.eu/?q=https://transactional.blog/blog/2024-erasure-coding.html&amp;submit_title=Erasure%20Coding%20for%20Distributed%20Systems">See discussion of this page on Reddit, HN, and lobsters.</a></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Snowden: The arrest of Durov is an assault on the basic human rights (121 pts)]]></title>
            <link>https://twitter.com/Snowden/status/1827695836832334169</link>
            <guid>41360808</guid>
            <pubDate>Mon, 26 Aug 2024 19:24:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Snowden/status/1827695836832334169">https://twitter.com/Snowden/status/1827695836832334169</a>, See on <a href="https://news.ycombinator.com/item?id=41360808">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Arrest of Pavel Durov Is a Reminder That Telegram Is Not Encrypted (136 pts)]]></title>
            <link>https://gizmodo.com/the-arrest-of-pavel-durov-is-a-reminder-that-telegram-is-not-encrypted-2000490960</link>
            <guid>41359502</guid>
            <pubDate>Mon, 26 Aug 2024 17:28:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/the-arrest-of-pavel-durov-is-a-reminder-that-telegram-is-not-encrypted-2000490960">https://gizmodo.com/the-arrest-of-pavel-durov-is-a-reminder-that-telegram-is-not-encrypted-2000490960</a>, See on <a href="https://news.ycombinator.com/item?id=41359502">Hacker News</a></p>
Couldn't get https://gizmodo.com/the-arrest-of-pavel-durov-is-a-reminder-that-telegram-is-not-encrypted-2000490960: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Intel SGX Fuse Key0, a.k.a. Root Provisioning Key Was Extracted by Researchers (145 pts)]]></title>
            <link>https://twitter.com/_markel___/status/1828112469010596347</link>
            <guid>41359152</guid>
            <pubDate>Mon, 26 Aug 2024 16:56:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/_markel___/status/1828112469010596347">https://twitter.com/_markel___/status/1828112469010596347</a>, See on <a href="https://news.ycombinator.com/item?id=41359152">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Eating the Birds of America: Audubon's Culinary Reviews of America's Birds (113 pts)]]></title>
            <link>https://usbirdhistory.com/audubon-eating-americas-birds/</link>
            <guid>41359132</guid>
            <pubDate>Mon, 26 Aug 2024 16:55:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://usbirdhistory.com/audubon-eating-americas-birds/">https://usbirdhistory.com/audubon-eating-americas-birds/</a>, See on <a href="https://news.ycombinator.com/item?id=41359132">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>On June 26, 1826, John James Audubon sat aboard the cotton schooner <em>Delos </em>off of Florida’s Gulf coast, en route from New Orleans to Liverpool, where he was hoping to find a publisher for his extensive portfolio of paintings of American birds.[1] On this particular day, the winds were still, leaving Audubon’s boat to rock with the waves, making no forward progress. Audubon sat on deck with the ship’s crew, watching a group of small black and white birds skimming above the waves. Since they had sailed beyond the mouth of the Mississippi and entered the Gulf, Audubon had observed growing numbers of these birds, watching as they searched the open seas for floating patches of vegetation. As they watched a small flock fly near their boat, the ship’s mate raised a gun and brought down four of the birds with one blast of birdshot. At Audubon’s request, he then brought them on board. This was the first time that Audubon was able to examine the birds up close, extend their wings with his hands, and run his fingers through their feathers. Audubon recorded careful notes of the details of its plumage, beak, eyes, wings, and claws. He skinned each of the birds before dissecting them and noting the contents of their stomach, finding them full of fish in various states of digestion. He then took one final measure of the bird, writing that “the flesh of this Petrel was fat, but tough, with a strong smell, and unfit for food; for, on tasting it, <em>as is my practice</em>, I found it to resemble that of the porpoises.”[2]</p>



<figure>
<figure><img loading="lazy" decoding="async" width="640" height="418" data-id="138" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-dusky-petrel.png?resize=640%2C418&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-dusky-petrel.png?w=893&amp;ssl=1 893w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-dusky-petrel.png?resize=300%2C196&amp;ssl=1 300w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-dusky-petrel.png?resize=768%2C501&amp;ssl=1 768w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-dusky-petrel.png?resize=850%2C555&amp;ssl=1 850w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="640" height="429" data-id="139" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-photo.png?resize=640%2C429&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-photo.png?w=929&amp;ssl=1 929w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-photo.png?resize=300%2C201&amp;ssl=1 300w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-photo.png?resize=768%2C515&amp;ssl=1 768w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-photo.png?resize=850%2C570&amp;ssl=1 850w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1"></figure>
<figcaption>Left, Audubon’s painting of his eponymous shearwater (which he called a Dusky Petrel), and right, Audubon’s Shearwater in real life (credit: <a href="https://commons.wikimedia.org/w/index.php?curid=4251231">Dominic Sherony CC BY-SA 2.0</a>). </figcaption></figure>



<p>In this passage, Audubon was describing a bird that he called the Dusky Petrel, which was not one that often made its way onto the plates of sailors or into market stalls. The bird was, in fact, not yet known to science, and if Audubon was not the first person to sample its meat, he was likely the first to describe the bird in writing. Audubon assumed that the slender, charcoal-colored bird was the same species as one that had first been described in the Pacific 40 years previously. In fact, he was taking detailed notes on a related bird that was not yet known to science. This fact was recognized in 1872, and the bird was renamed Audubon’s Shearwater in recognition of his discovery.[3]&nbsp;</p>









<p>Whether birds were large or small, familiar or obscure, palatable or nauseating, Audubon made sampling their meat a part of his extensive process of studying America’s birds for his<em> Ornithological Biography, or, An account of the habits of the birds of the United States of America</em>, a five-volume text that would accompany the 435 paintings that composed his <em>Birds of America</em>, for which he gained his fame. Alongside a description of each bird’s appearance, diet, behavior, and breeding habits, Audubon frequently included a reflection on the taste of the bird’s flesh.</p>


<div>
<figure><img loading="lazy" decoding="async" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-birds-of-america-volume-NYPL.png?resize=540%2C459&amp;ssl=1" alt="" width="540" height="459" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-birds-of-america-volume-NYPL.png?w=769&amp;ssl=1 769w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-birds-of-america-volume-NYPL.png?resize=300%2C255&amp;ssl=1 300w" sizes="(max-width: 540px) 100vw, 540px" data-recalc-dims="1"><figcaption>A first-edition copy of <em>Birds of America</em>, which featured life-sized paintings of 435 of America’s birds, hand-painted on “double elephant” size pages measuring more than two feet by three feet. This copy is held at the New York Public Library.</figcaption></figure></div>


<p>Throughout the 19th century, wild birds were a common part of both rural and urban cuisine. In 1867, Thomas De Voe included an inventory of all of the birds he had seen for sale in the markets of Boston, Philadelphia, and New York, which included some 36 species of waterfowl, 57 varieties of shorebird and upland game, and 27 kinds of songbirds, for a total of 120 species of birds.[4] Yet by sampling the majority of the 435 birds he painted for his <em>Birds of America </em>(mostly based on birds he had taxidermied and posed himself), Audubon far exceeded the variety of birds even the most accomplished gourmand might have possibly eaten. John James Audubon, without a doubt, holds the record for “most species of American birds eaten.”</p>


<div>
<figure><img loading="lazy" decoding="async" width="640" height="518" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/De-Voe-Market-Assistant-Title-Page-image-1.png?resize=640%2C518&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/De-Voe-Market-Assistant-Title-Page-image-1.png?w=661&amp;ssl=1 661w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/De-Voe-Market-Assistant-Title-Page-image-1.png?resize=300%2C243&amp;ssl=1 300w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1"><figcaption>Title page from Thomas De Voe’s 1867 book <em>The Market Assistant</em>, which listed 120 species of American birds found for sale in markets.</figcaption></figure></div>


<p>Since 1918, it has been a federal crime to “pursue, hunt, take, capture, kill, attempt to take, capture, or kill, possess, offer for sale, sell, offer to barter, barter, offer to purchase, purchase” or do really anything else with the bodies of most migratory birds,[5] with the exception of a few species designated as game, meaning that Audubon’s record is definitively safe. Safe too are most birds, at least from diners with adventurous palates. That being said, Audubon did provide a fascinating eater’s guide to America’s birds, which is an informative read for all of us who will (happily!) never have the opportunity to dine on Bald Eagle, or for that matter, Passenger Pigeon. So without further ado, here is a sampling of Audubon’s reviews of the culinary merits of America’s birds.[6]</p>



<h2>Game Birds</h2>



<p>First, Audubon shared his opinions about the many birds categorized as game, which would have been commonly eaten in his day. Audubon considered the meat from <strong>Ring-necked Ducks</strong> and <strong>Common Eiders</strong> to be “excellent” ; that of the <strong>Cackling Goose</strong> “exquisite”, and the <strong>Barnacle Goose</strong> “sweet and tender, and highly esteemed for the table.” <strong>Ruddy Ducks</strong> “provide good eating when fat and young.” <strong>Wood Ducks</strong> are at their best in the Fall, when Audubon found them to be tender and juicy. Audubon was much less fond of other waterfowl, finding the <strong>Bufflehead </strong>“fishy and disagreeable,” the <strong>Common Goldeneye</strong> “unfit for being eaten” (although he found their eggs delicious), and the <strong>Surf Scoter</strong> “tough, rank, and fishy, so as to be scarcely fit for food.” The <strong>Hooded Merganser</strong> too “has a fishy taste and odour, although it is relished by some persons.”</p>



<figure>
<figure><img loading="lazy" decoding="async" width="423" height="266" data-id="145" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/common-eider.png?resize=423%2C266&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/common-eider.png?w=423&amp;ssl=1 423w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/common-eider.png?resize=300%2C189&amp;ssl=1 300w" sizes="(max-width: 423px) 100vw, 423px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="378" height="249" data-id="143" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bufflehead.png?resize=378%2C249&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bufflehead.png?w=378&amp;ssl=1 378w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bufflehead.png?resize=300%2C198&amp;ssl=1 300w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bufflehead.png?resize=350%2C230&amp;ssl=1 350w" sizes="(max-width: 378px) 100vw, 378px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="365" height="267" data-id="147" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/hooded-merganser.png?resize=365%2C267&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/hooded-merganser.png?w=365&amp;ssl=1 365w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/hooded-merganser.png?resize=300%2C219&amp;ssl=1 300w" sizes="(max-width: 365px) 100vw, 365px" data-recalc-dims="1"></figure>
<figcaption>The Common Eider is “excellent” (Left, <a href="https://commons.wikimedia.org/w/index.php?curid=98887404">Rhododendrites – Own work, CC BY-SA 4.0</a>), the Bufflehead is “fishy and disagreeable” (Center, <a href="https://commons.wikimedia.org/w/index.php?curid=617245">CC BY-SA 3.0</a>), and the Hooded Merganser has “a fishy taste and odour” (Right, <a href="https://commons.wikimedia.org/w/index.php?curid=99102166">Rhododendrites – Own work, CC BY-SA 4.0</a>).</figcaption></figure>



<p>After the <strong><a href="https://usbirdhistory.com/transatlantic-turkeys/">Wild Turkey</a></strong>, Audubon’s favorite upland game bird to eat was the <strong>Ruffed Grouse</strong>, which were best taken in September after spending a summer feeding on mountain Huckleberries and Whortleberries. Meat from <strong>Mourning Doves</strong> was “remarkably fine, when they are obtained young and in the proper season.” The <strong>White-crowned Pigeon</strong>, found in the Florida Keys, is the most skittish bird which Audubon encountered, which he reasoned was due to the “continued war waged against them, their flesh being juicy, well flavoured, and generally tender, even in old birds.” When they feed on grasshoppers and strawberries, <strong>Upland Sandpipers</strong> are “truly delicious.” When fed on cantharides, however, they can become quite noxious if not cleaned properly, a situation which caused several New Orleans acquaintances of Audubon to leave their dinner expeditiously, “suffer[ing] greatly”, under circumstances which, Audubon wrote, “cannot well be described here.” Audubon claimed that <strong>Woodcocks </strong>were considered such fine dishes that some who he called epicures ate these birds “with all their viscera, worms and insects to boot, the intestines in fact being considered the most savoury parts.” Audubon, for his part, never joined in this practice. </p>



<h2>Backyard Birds</h2>



<p>Audubon also tried just about every songbird and backyard bird, big or small, whether they were commonly eaten at the time or not. <strong><a href="https://usbirdhistory.com/how-robins-got-their-name/">Robins</a></strong>, which he considered “fat and juicy, and afford excellent eating,” were hunted and eaten widely throughout the south, as were <strong>Bobolinks</strong>, which have “extremely tender and juicy” flesh. <strong>Brown-headed Cowbirds</strong> were also shot in large numbers, and Audubon considered them “more delicate and better flavoured than the species with which they associate, excepting the Robin”. The <strong>Louisiana Waterthrush</strong>, which is much less common of a bird, “becomes so plump as to be a pure mass of fat” during the winter, and “furnishes extremely delicate eating.” <strong>Orchard Orioles</strong> make good eating in the early fall, and according to Audubon are a favorite of the “Creoles of Louisiana.” The meat of <strong>Purple Finches </strong>was “equal to that of any other small bird.” <strong>Vesper Sparrows</strong> have “juicy, tender and savoury” flesh, but while <strong>Swamp Sparrows</strong> are fat and tender, their flesh is “sedgy.”&nbsp;</p>



<figure>
<figure><img loading="lazy" decoding="async" width="307" height="392" data-id="142" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bobolink.png?resize=307%2C392&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bobolink.png?w=307&amp;ssl=1 307w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bobolink.png?resize=235%2C300&amp;ssl=1 235w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bobolink.png?resize=300%2C383&amp;ssl=1 300w" sizes="(max-width: 307px) 100vw, 307px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="518" height="354" data-id="144" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/cedar-waxwing.png?resize=518%2C354&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/cedar-waxwing.png?w=518&amp;ssl=1 518w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/cedar-waxwing.png?resize=300%2C205&amp;ssl=1 300w" sizes="(max-width: 518px) 100vw, 518px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="343" height="330" data-id="152" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/swamp-sparrow.png?resize=343%2C330&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/swamp-sparrow.png?w=343&amp;ssl=1 343w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/swamp-sparrow.png?resize=300%2C289&amp;ssl=1 300w" sizes="(max-width: 343px) 100vw, 343px" data-recalc-dims="1"></figure>
<figcaption>Bobolinks are “extremely tender and juicy” (Left: <a href="https://commons.wikimedia.org/w/index.php?curid=113783228">Kshanti Greene – Own work, CC BY-SA 4.0</a>), Cedar Waxwings are good in pies (Center: <a href="https://commons.wikimedia.org/w/index.php?curid=93030737">Alan Rice – Own work, CC BY-SA 4.0</a>), and Swamp Sparrows are tender but “sedgy” (Right: <a href="https://commons.wikimedia.org/w/index.php?curid=19418982">Cephas – Own work, CC BY-SA 3.0</a>).</figcaption></figure>



<p><strong>Common Grackles</strong> have flesh that is “little better than that of the <strong>Crow</strong>, being dry and ill-flavoured” (although he found their eggs to be “quite delicate”). Nevertheless, Audubon reports with some curiosity that grackles were “frequently used, with the addition of one or two Golden-winged Woodpeckers or Redwings, to make what is here called <em>pot pie</em>, even amidst a profusion of so many better things.” Blackbirds weren’t the only birds that ended up baked in pies. An acquaintance of Audubon’s shipped a basket of <strong>Cedar Waxwings</strong> to New Orleans as a Christmas present, only to find that “the steward of the steamer, in which they were shipped, made pies of them for the benefit of the passengers.” Audubon found the flesh of most types of woodpeckers to taste too much like ants, but <strong>Northern Flickers</strong> were nevertheless frequently eaten by hunters and occasionally sold in markets. Although the <strong>Whip-poor-will</strong> was rarely killed because it is “too small to be sought as an article of food,” Audubon still sampled its meat, which he deemed “savoury.”</p>



<h2>Seabirds and Shorebirds</h2>



<p>Audubon found the meat from <strong>Wood Storks</strong> to be too tough and oily to eat, but wrote that in Louisiana they were skinned, cooked, and eaten. Meat from <strong>Roseate Spoonbills</strong> was “oily and poor eating.” <strong>Puffins </strong>were so tough and fishy that Audubon could only recommend their meat to those facing desperate circumstances (a judgment he also passed on the <strong>American Oystercatcher</strong> and <strong>American White Pelican</strong>), and their eggs were not much better. The <strong>Magnificent Frigatebird</strong> was “unfit for any other person than one in a state of starvation.” <strong>Belted Kingfisher </strong>eggs make fine eating, but their meat is “extremely fishy, oily, and disagreeable.” <strong>Cormorants </strong>were perhaps the only bird that Audubon did not taste, and he intended to avoid doing so until such a time as “nothing better could be procured” – a situation he must have considered truly desperate.</p>



<figure>
<figure><img loading="lazy" decoding="async" width="416" height="275" data-id="154" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/image.png?resize=416%2C275&amp;ssl=1" alt="This image has an empty alt attribute; its file name is roseate-spoonbill.png" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/image.png?w=416&amp;ssl=1 416w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/image.png?resize=300%2C198&amp;ssl=1 300w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/image.png?resize=350%2C230&amp;ssl=1 350w" sizes="(max-width: 416px) 100vw, 416px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="275" height="386" data-id="148" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/magnificent-frigatebird.png?resize=275%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/magnificent-frigatebird.png?w=275&amp;ssl=1 275w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/magnificent-frigatebird.png?resize=214%2C300&amp;ssl=1 214w" sizes="(max-width: 275px) 100vw, 275px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="384" height="286" data-id="141" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/belted-kingfisher.png?resize=384%2C286&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/belted-kingfisher.png?w=384&amp;ssl=1 384w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/belted-kingfisher.png?resize=300%2C223&amp;ssl=1 300w" sizes="(max-width: 384px) 100vw, 384px" data-recalc-dims="1"></figure>
<figcaption>Roseate Spoonbills are “oily and poor eating (left: <a href="https://commons.wikimedia.org/w/index.php?curid=42301356">Photo Dante – Own work, CC BY-SA 4.0</a>), Magnificent Frigatebirds are “unfit for any other person than one in a state of starvation” (center: <a href="https://commons.wikimedia.org/w/index.php?curid=12334028">John Picken, CC BY 2.0</a>), and Belted Kingfishers are “extremely fishy, oily, and disagreeable (right: <a href="https://commons.wikimedia.org/w/index.php?curid=116127806">MarshBunny – Own work, CC BY-SA 4.0</a>). </figcaption></figure>



<p>Audubon found <strong>Loons </strong>to be “not very palatable,” with their meat “being tough, rank, and dark coloured.” This opinion was not shared by all, however, as Audubon had “seen it much relished by many lovers of good-living, especially at Boston, where it was not infrequently served almost raw.” <strong>Black Terns</strong> are “tolerably good.” Fishermen and eggers would preserve <strong>Herring Gulls</strong> in salt to be eaten over the winter. Audubon was indifferent to mature <strong>Killdeer</strong>, but found the young in their first Autumn to be “fat, juicy, and tender.” <strong>Dunlins </strong>were also fat and juicy, for which reason they were shot in large numbers. <strong>Spotted Sandpipers</strong> are “delicious” and fat in the fall. Audubon had reached the limits of his descriptive capabilities by the time he described the meat from <strong>Black-necked Stilts</strong>, which he labeled neither good nor bad, but “ordinary.”</p>



<h2>Birds You Will Definitely Never Get To Eat</h2>



<p>Finally, Audubon left us his thoughts on birds that may have been considered food in his day, but are now endangered, extinct, or highly protected by federal law. Meat from adult <strong>Passenger Pigeons</strong> (extinct since 1914) was dark but tolerable. That of young birds taken from their nests, however, was “much esteemed.” When <strong>Carolina Parakeets</strong> (extinct since 1918) are young, “their flesh is tolerable food.” Similarly, young <strong>Whooping Cranes</strong> (current population: 543 birds) were “tender and juicy.” Audubon pronounced older birds of that species to be “tough and unfit for the table”, although he noted that the Seminoles of Florida hunted them for food. Audubon also sampled the meat of <strong>Peregrine Falcons</strong>, which he found to be very tough. Young <strong>Bald Eagles</strong> were like “veal in taste and tenderness,” while the flesh of <strong>Barred Owls</strong> is “palatable.” Of all the birds he ate, there was one he compared to our most common poultry. Meat from <strong>Snowy Owls, </strong>Audubon wrote, is like “that of a chicken, and not indelicate eating.”</p>



<figure>
<figure><img loading="lazy" decoding="async" width="493" height="624" data-id="150" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-passenger-pigeon.png?resize=493%2C624&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-passenger-pigeon.png?w=493&amp;ssl=1 493w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-passenger-pigeon.png?resize=237%2C300&amp;ssl=1 237w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-passenger-pigeon.png?resize=300%2C380&amp;ssl=1 300w" sizes="(max-width: 493px) 100vw, 493px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="410" height="594" data-id="149" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-carolina-parakeet.png?resize=410%2C594&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-carolina-parakeet.png?w=410&amp;ssl=1 410w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-carolina-parakeet.png?resize=207%2C300&amp;ssl=1 207w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-carolina-parakeet.png?resize=300%2C435&amp;ssl=1 300w" sizes="(max-width: 410px) 100vw, 410px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="521" height="821" data-id="151" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-whooping-crane.png?resize=521%2C821&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-whooping-crane.png?w=521&amp;ssl=1 521w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-whooping-crane.png?resize=190%2C300&amp;ssl=1 190w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-whooping-crane.png?resize=300%2C473&amp;ssl=1 300w" sizes="(max-width: 521px) 100vw, 521px" data-recalc-dims="1"></figure>
<figcaption>Audubon’s paintings of the Passenger Pigeon (left – their meat was dark, but tolerable), Carolina Parakeet (center – also “tolerable food”), and the Whooping Crane (right – “tender and juicy”). </figcaption></figure>







<div>




<p>Enter your email to receive new posts any time they’re published.</p>
</div>



<div>




<p>Enter your email to receive new posts any time they’re published.</p>
</div>



<hr>



<p>[1] T. S. Palmer, Audubon’s Shearwater in the United States, <em>The Auk</em>, Volume 48, Issue 2, 1 April 1931, Pages 198–206, <a href="https://doi.org/10.2307/4076787">https://doi.org/10.2307/4076787</a><br>[2] Audubon,&nbsp;John James.&nbsp;Ornithological Biography, Or an Account of the Habits of the Birds of the United States of America: Accompanied by Descriptions of the Objects Represented in the Work Entitled The Birds of America, and Interspersed with Delineations of American Scenery and Manners.&nbsp;United Kingdom:&nbsp;Black,&nbsp;1835.<br>[3] Palmer, “Audubon’s Shearwater”<br>[4] De Voe,&nbsp;Thomas Farrington.&nbsp;The Market Assistant: Containing a Brief Description of Every Article of Human Food Sold in the Public Markets of the Cities of New York, Boston, Philadelphia, and Brooklyn; Including the Various Domestic and Wild Animals, Poultry, Game, Fish, Vegetables, Fruits &amp;c., &amp;c. with Many Curious Incidents and Anecdotes.&nbsp;United States:&nbsp;Hurd and Houghton,&nbsp;1867.<br>[5] Migratory Bird Treaty Act, 16 U.S.C. §703<br>[6] All quotes are from Audubon, Ornithological Biography, vol. I-V. </p>



<hr>



<div data-v="2" data-block-id="7bf3034"><div><article><h4><a href="https://usbirdhistory.com/we-used-to-have-parrots/">We Used to Have Parrots</a></h4><a href="https://usbirdhistory.com/we-used-to-have-parrots/"></a><p>At the northern tip of Kentucky there’s a mineral spring called Big Bone Lick. Some of the earliest Europeans to arrive at the spring found massive bones sticking out of the mud, left by enormous animals that had evidently gotten stuck while…</p><a href="https://usbirdhistory.com/we-used-to-have-parrots/">Continue Reading</a></article></div><div><article><h4><a href="https://usbirdhistory.com/canaries-in-coal-mines/">Canaries in Coal Mines</a></h4><a href="https://usbirdhistory.com/canaries-in-coal-mines/"><figure><img loading="lazy" decoding="async" width="441" height="414" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/07/mine-rescuers-with-a-canary.png?fit=441%2C414&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/07/mine-rescuers-with-a-canary.png?w=441&amp;ssl=1 441w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/07/mine-rescuers-with-a-canary.png?resize=300%2C282&amp;ssl=1 300w" sizes="(max-width: 441px) 100vw, 441px"></figure></a><p>Sometime between 7:00 and 7:30AM the morning of November 6, 1922, a stray spark ignited a pocket of gas deep beneath the town of Spangler, Pennsylvania. Several men working inside the Reilly coal mine were immediately killed by the blast, thrown against…</p><a href="https://usbirdhistory.com/canaries-in-coal-mines/">Continue Reading</a></article></div><div><article><h4><a href="https://usbirdhistory.com/americas-favorite-bird/">America’s Favorite Bird</a></h4><a href="https://usbirdhistory.com/americas-favorite-bird/"></a><p>From New York Public Library’s collection Aviary and Cage Birds: A Series of 50 (Player’s Cigarettes). Shrewdness. Intelligence. Capability to endure a vast amount of exposure and hardship. Skill at telling male and female birds apart. These were the most important characteristics…</p><a href="https://usbirdhistory.com/americas-favorite-bird/">Continue Reading</a></article></div><div><article><h4><a href="https://usbirdhistory.com/when-birds-meant-food/">When Birds Meant Food</a></h4><a href="https://usbirdhistory.com/when-birds-meant-food/"></a><p>Looking back at the pieces I’ve written since I started this newsletter last September, I realize that an uncomfortably large number of posts deal with killing birds, often for their meat, sometimes out of necessity, and many times just for fun. These…</p><a href="https://usbirdhistory.com/when-birds-meant-food/">Continue Reading</a></article></div><div><article><h4><a href="https://usbirdhistory.com/deadly-ceilometers/">Deadly Pillars of Light</a></h4><a href="https://usbirdhistory.com/deadly-ceilometers/"><figure><img loading="lazy" decoding="async" width="640" height="591" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/04/ceilometer-this-machine-kills-birds.png?fit=640%2C591&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/04/ceilometer-this-machine-kills-birds.png?w=641&amp;ssl=1 641w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/04/ceilometer-this-machine-kills-birds.png?resize=300%2C277&amp;ssl=1 300w" sizes="(max-width: 640px) 100vw, 640px"></figure></a><p>To make a safe landing, one of the many pieces of data that pilots need is the height of clouds above the landing strip. Because clouds block a pilot’s vision, a low cloud bank can make it difficult, even dangerous, for planes…</p><a href="https://usbirdhistory.com/deadly-ceilometers/">Continue Reading</a></article></div><div><article><h4><a href="https://usbirdhistory.com/birds-and-slavery/">Birds and Slavery</a></h4><a href="https://usbirdhistory.com/birds-and-slavery/"><figure><img loading="lazy" decoding="async" width="640" height="243" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?fit=640%2C243&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?w=1242&amp;ssl=1 1242w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?resize=300%2C114&amp;ssl=1 300w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?resize=1024%2C388&amp;ssl=1 1024w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?resize=768%2C291&amp;ssl=1 768w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?resize=850%2C322&amp;ssl=1 850w" sizes="(max-width: 640px) 100vw, 640px"></figure></a><p>From 1936 to 1938, an army of formerly-unemployed writers hired under the Works Progress Administration traveled South to collect the life histories of aging women and men born into slavery. The Federal Writers Project was just one of many depression-era initiatives designed…</p><a href="https://usbirdhistory.com/birds-and-slavery/">Continue Reading</a></article></div></div>




			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Remove-bg – open-source remove background using WebGPU (176 pts)]]></title>
            <link>https://bannerify.co/tools/remove-bg</link>
            <guid>41358490</guid>
            <pubDate>Mon, 26 Aug 2024 15:58:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bannerify.co/tools/remove-bg">https://bannerify.co/tools/remove-bg</a>, See on <a href="https://news.ycombinator.com/item?id=41358490">Hacker News</a></p>
Couldn't get https://bannerify.co/tools/remove-bg: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Using AI to fight insurance claim denials (182 pts)]]></title>
            <link>https://sfstandard.com/2024/08/23/holden-karau-fight-health-insurance-appeal-claims-denials/</link>
            <guid>41358132</guid>
            <pubDate>Mon, 26 Aug 2024 15:31:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sfstandard.com/2024/08/23/holden-karau-fight-health-insurance-appeal-claims-denials/">https://sfstandard.com/2024/08/23/holden-karau-fight-health-insurance-appeal-claims-denials/</a>, See on <a href="https://news.ycombinator.com/item?id=41358132">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>For San Francisco tech worker Holden Karau, paperwork had become a hobby.&nbsp;</p><p>Specifically, the forms and letters required to fight back when her health insurance provider denied a claim for a covered service, surgery, or pharmaceutical.&nbsp;</p><p>As a trans woman who loves motorcycles, she has required gender-affirming care and treatments for an accident in recent years, and received a spate of insurance denials along the way.&nbsp;</p><p>Instead of passively accepting the providers’ decisions, she’d spend hours writing letters and filling out forms to appeal. It usually worked: Out of roughly 40 denials, she won more than 90% of her appeals, she estimates. (She also successfully fought an insurance denial for her dog, Professor Timbit.)&nbsp;</p><p>“Part of that is an unreasonable willingness to take things too far,” Karau said. “There’s an enjoyment in getting a counterparty to follow the rules that they don’t seem to want to have to follow.”</p></div><figure id=""><div><p><span><span></span><img alt="A person wearing a pink shirt and a smartwatch is working on a laptop. They are using their right hand to point at the screen and their left hand is resting on the laptop." loading="lazy" decoding="async" data-nimg="responsive" sizes="(min-width: 1001px) 600px, (min-width: 768px) 700px, 100vw" srcset="https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=640&amp;q=75 640w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=750&amp;q=75 750w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=768&amp;q=75 768w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=828&amp;q=75 828w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=1024&amp;q=75 1024w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=1080&amp;q=75 1080w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=1200&amp;q=75 1200w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=1920&amp;q=75 1920w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=2048&amp;q=75 2048w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=3840&amp;q=75 3840w" src="https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-09.jpg?w=3840&amp;q=75" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div><figcaption>Karau demonstrates the platform, which crafts appeals to health insurance denials. | <span>Source: </span>Emily Steinberger/The Standard</figcaption></figure><div><p>She began helping friends file appeals, too, then asked herself a question that’s typical for engineers: Could she figure out a way to automate the process?&nbsp;</p><p>After a year of tinkering, she just launched her answer: <a href="https://fighthealthinsurance.com/">Fight Health Insurance</a>, an open-source platform that takes advantage of large language models to help users generate health insurance appeals with AI.</p><p>With the slogan “Make your health insurance company cry too,” Karau’s site makes filing appeals faster and easier. <a href="https://www.kff.org/private-insurance/issue-brief/claims-denials-and-appeals-in-aca-marketplace-plans/">A recent study found that</a> Affordable Care Act patients appeal only about 0.1% of rejected claims, and she hopes her platform will encourage more people to fight back.&nbsp;</p></div><figure id=""><div><p><span><span></span><img alt="The image shows a tangle of multicolored cables connected to networking devices in a server rack, with wires coiled around various points and additional equipment nearby." loading="lazy" decoding="async" data-nimg="responsive" sizes="(min-width: 1001px) 600px, (min-width: 768px) 700px, 100vw" srcset="https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=640&amp;q=75 640w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=750&amp;q=75 750w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=768&amp;q=75 768w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=828&amp;q=75 828w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=1024&amp;q=75 1024w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=1080&amp;q=75 1080w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=1200&amp;q=75 1200w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=1920&amp;q=75 1920w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=2048&amp;q=75 2048w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=3840&amp;q=75 3840w" src="https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-11.jpg?w=3840&amp;q=75" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div><figcaption>One of the servers that operates the platform is in Karau's basement office. | <span>Source: </span>Emily Steinberger/The Standard</figcaption></figure><div><p>“Most of the time, my relationship with my health insurance company is more adversarial than collaborative,” she said. “You’re trying to force them to comply with the rules, and they’re trying to spend the <a href="https://www.propublica.org/article/unitedhealth-healthcare-insurance-denial-ulcerative-colitis">least amount of money</a>.”&nbsp;</p><p>A Fight Health Insurance user can scan their insurance denial, and the system will craft several appeal letters to choose from and modify.&nbsp;</p><p>The “dirty secret” of the insurance industry is that most denials can be successfully appealed, according to Dr. Harley Schultz, a patient advocate in the Bay Area.</p><p>“Very few people know about the process, and even fewer take advantage of it, because it’s rather cumbersome, arcane, and confusing, by design,” he said. “But if you fight hard enough and long enough, most denials get overturned.”&nbsp;</p><p>It’s often assumed&nbsp; that only doctors can file appeals, but patients can do it too, he added. <a href="https://www.propublica.org/article/unitedhealth-healthcare-insurance-denial-ulcerative-colitis">Insurers reject about 1 in 7 claims</a> for treatment (Schultz estimates that it could be as high as 25% for some companies), and the reality is that physicians just don’t have time for all that filing.&nbsp;&nbsp;</p><p>“I was in practice for many years, and if I fought every insurance denial, there wouldn’t be any time to do anything else,” Schultz said.&nbsp;</p><p>While some doctors have <a href="https://www.nytimes.com/2024/07/10/health/doctors-insurers-artificial-intelligence.html">turned to artificial intelligence themselves</a> to fight claims, Karau’s service puts the power in the hands of patients, who likely have more time and motivation to dedicate to their claims.&nbsp;</p><p>“In an ideal world, we would have a different system, but we don’t live in an ideal world, so what I’m shooting for here is incremental progress and making the world suck a little less,” she said.&nbsp;</p><p>So far, dozens have used the platform to generate an appeal, and Karau is assessing their feedback to fine-tune the platform and make it more effective and easier to use.&nbsp;</p></div><figure id=""><div><p><span><span></span><img alt="The image shows a computer screen displaying a draft appeal for an insurance company to cover PrEP medication. The document argues for coverage based on medical necessity." loading="lazy" decoding="async" data-nimg="responsive" sizes="(min-width: 1001px) 600px, (min-width: 768px) 700px, 100vw" srcset="https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=640&amp;q=75 640w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=750&amp;q=75 750w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=768&amp;q=75 768w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=828&amp;q=75 828w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=1024&amp;q=75 1024w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=1080&amp;q=75 1080w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=1200&amp;q=75 1200w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=1920&amp;q=75 1920w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=2048&amp;q=75 2048w, https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=3840&amp;q=75 3840w" src="https://content.sfstandard.com/wp-content/uploads/2024/08/20240821-techworkerhealthcare-es-10.jpg?w=3840&amp;q=75" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div><figcaption>An example of an appeal letter generated by the AI platform. | <span>Source: </span>Emily Steinberger/The Standard</figcaption></figure><div><p>She&nbsp;estimates that she has spent about $10,000 building the platform. It’s free for users, though she might eventually charge for added services like faxing appeals.</p><p>At this point, she’s not planning on leaving her tech job to work on the platform full time (she has held gigs at IBM, Apple, Google, and Netflix, where she currently works) but hopes it can become a self-sustaining business, in addition to a cause about which she’s wildly passionate.&nbsp;</p><p>“The best-case scenario —&nbsp;which is, admittedly, incredibly unlikely —&nbsp;is that this increases the accessibility of appeals to the point that insurance companies stop denying so much tiny bullshit,” she said. “I suspect that they would still try to be dicks about big things, but hopefully we can get them to stop being dicks about small things.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dokku: My favorite personal serverless platform (610 pts)]]></title>
            <link>https://hamel.dev/blog/posts/dokku/</link>
            <guid>41358020</guid>
            <pubDate>Mon, 26 Aug 2024 15:21:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hamel.dev/blog/posts/dokku/">https://hamel.dev/blog/posts/dokku/</a>, See on <a href="https://news.ycombinator.com/item?id=41358020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">




<div>
<figure>
<p><img src="https://hamel.dev/blog/posts/dokku/images/serverless.png"></p>
<figcaption>With Dokku, you can turn a VPS into a powerful serverless platform</figcaption>
</figure>
</div>
<section id="what-is-dokku">
<h2 data-anchor-id="what-is-dokku">What is Dokku?</h2>
<p><a href="https://dokku.com/">Dokku</a> is an open-source Platform as a Service (PaaS) that runs on a single server of your choice. <strong>It’s like <a href="https://www.heroku.com/">Heroku</a>, but you own it.</strong> It is a great way to get the benefits of Heroku without the costs (Heroku can get quite expensive!). I need to deploy many applications for my <a href="https://hamel.dev/hire.html">LLM consulting work</a>. Having a cost-effective, easy-to-use serverless platform is essential for me.</p>
<p><strong>I run a Dokku server on a $7/month VPS on <a href="https://us.ovhcloud.com/">OVHcloud</a></strong> for non-gpu workloads. These applications include things like <a href="https://nbsanity.com/">nbsanity</a> and <a href="https://langfree.parlance-labs.com/tutorials/shiny.html#run-the-shiny-app-locally">data cleaning tools for LLMs</a>.</p>
<p>Some of the features I love about Dokku:</p>
<ul>
<li>Easy to use (like Heroku).</li>
<li>Automatic SSL certificate management via <a href="https://letsencrypt.org/">Let’s Encrypt</a>.</li>
<li>Basic Auth support so I can password-protect sites.</li>
<li>Scale up and down with a single command.</li>
<li>Flexibility to handle any application (Node, Python, etc), including defining a Docker container.</li>
<li>Lots of <a href="https://dokku.com/docs/community/plugins/?h=plugins#official-plugins">official plugins</a> that do almost anything I want.</li>
<li>Easily deploy with git commands.</li>
</ul>
</section>
<section id="minimal-dokku-examples">
<h2>Minimal Dokku Examples</h2>
<p>Make sure you <a href="https://dokku.com/docs/getting-started/installation/">install Dokku</a> on your VPS. As I mentioned, I use <a href="https://us.ovhcloud.com/">OVH</a>.</p>
<section id="deploying-apps-as-a-docker-container">
<h2 data-anchor-id="deploying-apps-as-a-docker-container">Deploying Apps as A Docker Container</h2>
<p>An easy way to deploy applications is with a Docker container.</p>
<p>To deploy a Docker container, I put a Dockerfile in the root of my git repo like this:</p>
<div id="cb1" data-filename="Dockerfile"><pre><code><span id="cb1-1"><span>FROM</span> python:3.10</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span>COPY</span> . /app</span>
<span id="cb1-4"><span>WORKDIR</span> /app</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span># Install the local package</span></span>
<span id="cb1-7"><span>RUN</span> <span>pip</span> install .</span>
<span id="cb1-8"></span>
<span id="cb1-9"><span># This directory contains app.py, a FastApi app</span></span>
<span id="cb1-10"><span>WORKDIR</span> /app/</span>
<span id="cb1-11"></span>
<span id="cb1-12"><span>ENTRYPOINT</span> [<span>"./entrypoint.sh"</span>]</span></code></pre></div>
<div>
<p>The <code>entrypoint.sh</code> script allows me to easily run the app locally or in a Docker container. It looks like this:</p>
<div id="cb2" data-filename="entrypoint.sh"><pre><code><span id="cb2-1"><span>#!/bin/bash</span></span>
<span id="cb2-2"><span>exec</span> uvicorn main:app <span>--port</span> <span>"</span><span>$PORT</span><span>"</span> <span>--host</span> 0.0.0.0</span></code></pre></div>
</div>
<p>On the Dokku host, create the app:</p>

<p><strong>Locally</strong>, set up access to the Dokku host and name it <code>dokku</code> in your <code>~/.ssh/config</code> file. For example, here is mine:</p>
<pre><code>Host dokku
  HostName &lt;The external IP address of your Dokku host&gt;
  User ubuntu
  IdentityFile /Users/hamel/.ssh/dokku</code></pre>
<p>Locally, add the Dokku host as a remote and push to it:</p>
<div id="cb5"><pre><code><span id="cb5-1"><span>git</span> remote add dokku dokku@dokku:myapp</span>
<span id="cb5-2"><span>git</span> push dokku main</span></code></pre></div>
<p>That’s it - your app should be running on the Dokku host! Your local logs will print the URL that your application is served on, which by default will be <code>myapp.yourdomain.com</code>. You can also scale it up/down with the following command:</p>
<div id="cb6"><pre><code><span id="cb6-1"><span>#scale to two workers</span></span>
<span id="cb6-2"><span>dokku</span> ps:scale myapp web=2</span></code></pre></div>
<p>We are just scratching the surface. For more details, see the <a href="https://dokku.com/docs/">Dokku docs</a>.</p>
</section>
<section id="static-sites">
<h2 data-anchor-id="static-sites">Static Sites</h2>
<p>GitHub Pages is annoying in that you can’t easily deploy private static sites without paying for an expensive Enterprise account. With Dokku, you can easily deploy a static site from a private GitHub Repo and password-protect it.</p>
<p>We will assume that you have a static site in a git repo in a folder named <code>_site</code>.</p>
<p><strong>On the Dokku host</strong>, create an app named <code>mysite</code> and set the <code>NGINX_ROOT</code> environment variable to <code>_site</code>:</p>
<div id="cb7"><pre><code><span id="cb7-1"><span>dokku</span> apps:create mysite</span>
<span id="cb7-2"><span>dokku</span> config:set static-site NGINX_ROOT=_site</span></code></pre></div>
<p>Also on the Dokku host, install <a href="https://github.com/dokku/dokku-http-auth">basic auth</a> and <a href="https://github.com/dokku/dokku-http-auth/issues/15#issuecomment-1637058437">set permissions</a> so the plugin can work properly.</p>
<div id="cb8"><pre><code><span id="cb8-1"><span># do setup for the auth plugin that we will use later</span></span>
<span id="cb8-2"><span>sudo</span> dokku plugin:install https://github.com/dokku/dokku-http-auth.git</span>
<span id="cb8-3"><span>sudo</span> chmod +x /home/dokku</span></code></pre></div>
<p>Then execute the following commands from the root of your git repo that contains the static site. :</p>
<div id="annotated-cell-8"><pre><code><a data-target-cell="annotated-cell-8" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-8-1"><span>touch</span> .static</span>
<a data-target-cell="annotated-cell-8" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-8-2"><span>echo</span> BUILDPACK_URL=https://github.com/dokku/buildpack-nginx <span>&gt;</span> .env</span>
<a data-target-cell="annotated-cell-8" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-8-3"><span>git</span> remote add dokku dokku@dokku:mysite</span></code></pre></div>
<dl>
<dt data-target-cell="annotated-cell-8" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="1" data-code-annotation="1">tells <code>dokku</code> that this is a static site</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="2" data-code-annotation="2">tells <code>dokku</code> to use the nginx buildpack for static sites (it will usually automatically detect this, but if you have a project with code and a static site, you need to tell it to use the nginx buildpack so it doesn’t get confused).</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="3" data-code-annotation="3">add the <code>dokku</code> host as a remote. For this to work, make sure <code>dokku</code> is a hostname in your <code>~/.ssh/config</code> file as described <a href="#deploying-apps-as-a-docker-container">in the previous section</a>.</span>
</dd>
</dl>
<p>Finally, deploy your application:</p>

<p>You can now add auth by running the following command on the Dokku host:</p>
<div id="cb10"><pre><code><span id="cb10-1"><span>dokku</span> http-auth:enable mysite <span>&lt;</span>username<span>&gt;</span> <span>&lt;</span>password<span>&gt;</span></span></code></pre></div>
<div>

<p>You can add multiple usernames/passwords and even filter specific IPs. See <a href="https://github.com/dokku/dokku-http-auth">the docs</a>.</p>
</div>
<div>
<p>It’s often desirable to have HTTPS for your site. Dokku makes this easy with the <a href="https://github.com/dokku/dokku-letsencrypt">Let’s Encrypt Plugin</a>, which will even auto-renew for you. I don’t use this, because I’m letting <a href="https://developers.cloudflare.com/dns/manage-dns-records/reference/proxied-dns-records/">Cloudflare handle this with its proxy</a>.</p>
<p>If you are using Cloudflare this way, activating this plugin will mess things up (don’t worry its easy to disable). Honestly, I think it’s easier to let Cloudflare handle it if you are already doing so.</p>
</div>
</section>
</section>
<section id="deploying-with-github-actions">
<h2>Deploying With GitHub Actions</h2>
<p>You can automatically deploy Dokku apps with GitHub Actions, which is helpful if you don’t want to fiddle with pushing to the Dokku host. Here is an example GitHub Action workflow that does this:</p>
<div id="cb11" data-filename="deploy-dokku.yml"><pre><code><span id="cb11-1"><span>name</span><span>:</span><span> CI</span></span>
<span id="cb11-2"><span>on</span><span>:</span></span>
<span id="cb11-3"><span>  </span><span>workflow_dispatch</span><span>:</span></span>
<span id="cb11-4"><span>  </span><span>push</span><span>:</span></span>
<span id="cb11-5"><span>    </span><span>branches</span><span>:</span><span> </span><span>[</span><span>main</span><span>]</span></span>
<span id="cb11-6"></span>
<span id="cb11-7"><span>concurrency</span><span>:</span><span> # Cancel previous jobs to avoid deploy locks on dokku</span></span>
<span id="cb11-8"><span>  </span><span>group</span><span>:</span><span> ${{ github.ref }}</span></span>
<span id="cb11-9"><span>  </span><span>cancel-in-progress</span><span>:</span><span> </span><span>true</span></span>
<span id="cb11-10"></span>
<span id="cb11-11"><span>jobs</span><span>:</span></span>
<span id="cb11-12"><span>  </span><span>deploy-dokku</span><span>:</span></span>
<span id="cb11-13"><span>    </span><span>runs-on</span><span>:</span><span> ubuntu-latest</span></span>
<span id="cb11-14"><span>    </span><span>steps</span><span>:</span></span>
<span id="cb11-15"><span>      </span><span>-</span><span> </span><span>name</span><span>:</span><span> Checkout code</span></span>
<span id="cb11-16"><span>        </span><span>uses</span><span>:</span><span> actions/checkout@v2</span></span>
<span id="cb11-17"><span>        </span><span>with</span><span>:</span></span>
<span id="cb11-18"><span>          </span><span>fetch-depth</span><span>:</span><span> </span><span>0</span></span>
<span id="cb11-19"><span>      </span></span>
<span id="cb11-20"><span>      </span><span>-</span><span> </span><span>name</span><span>:</span><span> Install SSH key</span></span>
<span id="cb11-21"><span>        run</span><span>: </span><span>|</span></span>
<span id="cb11-22">          echo "${{ secrets.DOKKU_SSH_PRIVATE_KEY }}" &gt; private_key.pem</span>
<span id="cb11-23">          chmod 600 private_key.pem</span>
<span id="cb11-24"></span>
<span id="cb11-25"><span>      </span><span>-</span><span> </span><span>name</span><span>:</span><span> Add remote and push</span></span>
<span id="cb11-26"><span>        run</span><span>: </span><span>|</span></span>
<span id="cb11-27">          git remote add dokku dokku@rechat.co:llm-eval</span>
<span id="cb11-28">          GIT_SSH_COMMAND="ssh -i private_key.pem -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no" git push dokku main -f</span></code></pre></div>
</section>
<section id="miscellaneous-tips">
<h2>Miscellaneous Tips</h2>
<p>These are things I often forget, so I’m writing them down here. For these examples, assume my app is named <code>llm-eval</code> and my host is <code>rechat.co</code>.</p>
<section id="run-commands-remotely">
<h2 data-anchor-id="run-commands-remotely">Run commands remotely</h2>
<p>You don’t have to ssh into the Dokku host just to execute commands. You can execute them remotely via the <code>dokku</code> user like this:</p>
<div id="cb12"><pre><code><span id="cb12-1"><span># https://dokku.com/docs/deployment/application-management/</span></span>
<span id="cb12-2"><span>ssh</span> dokku@rechat.co apps:list</span></code></pre></div>
</section>
<section id="docker-cache">
<h2 data-anchor-id="docker-cache">Docker cache</h2>
<p>This is how you can <a href="https://dokku.com/docs/advanced-usage/repository-management/">invalidate the docker cache</a> for a fresh build:</p>
<div id="cb13"><pre><code><span id="cb13-1"><span>ssh</span> dokku@rechat.co repo:purge-cache llm-eval</span></code></pre></div>
</section>
<section id="rebuild-without-pushing">
<h2 data-anchor-id="rebuild-without-pushing">Rebuild without pushing</h2>
<p>Sometimes you want to rebuild without pushing. There are <a href="https://dokku.com/docs/processes/process-management/">many ways to do this</a>, but one way is like this:</p>
<div id="cb14"><pre><code><span id="cb14-1"><span>ssh</span> dokku@rehcat.co ps:rebuild llm-eval</span></code></pre></div>
</section>
</section>
<section id="why-did-i-write-this">
<h2>Why Did I Write This?</h2>
<p>I had to dig up these details whenever I wanted to deploy a new app, so I had to write it up anyway. I hope you find it useful, too!</p>


</section>

</main> <!-- /main -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux: We need tiling desktop environments (148 pts)]]></title>
            <link>https://linuxblog.io/linux-tiling-desktop-environments/</link>
            <guid>41357853</guid>
            <pubDate>Mon, 26 Aug 2024 15:05:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linuxblog.io/linux-tiling-desktop-environments/">https://linuxblog.io/linux-tiling-desktop-environments/</a>, See on <a href="https://news.ycombinator.com/item?id=41357853">Hacker News</a></p>
Couldn't get https://linuxblog.io/linux-tiling-desktop-environments/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DOJ Files Antitrust Suit Against RealPage, Maker of Rent-Setting Algorithm (159 pts)]]></title>
            <link>https://www.propublica.org/article/realpage-lawsuit-doj-antitrustdoj-files-antitrust-suit-against-maker-of-rent-setting-algorithm</link>
            <guid>41357557</guid>
            <pubDate>Mon, 26 Aug 2024 14:36:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.propublica.org/article/realpage-lawsuit-doj-antitrustdoj-files-antitrust-suit-against-maker-of-rent-setting-algorithm">https://www.propublica.org/article/realpage-lawsuit-doj-antitrustdoj-files-antitrust-suit-against-maker-of-rent-setting-algorithm</a>, See on <a href="https://news.ycombinator.com/item?id=41357557">Hacker News</a></p>
Couldn't get https://www.propublica.org/article/realpage-lawsuit-doj-antitrustdoj-files-antitrust-suit-against-maker-of-rent-setting-algorithm: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[NSA releases 1982 Grace Hopper lecture (619 pts)]]></title>
            <link>https://www.nsa.gov/helpful-links/nsa-foia/declassification-transparency-initiatives/historical-releases/view/article/3880193/capt-grace-hopper-on-future-possibilities-data-hardware-software-and-people-1982/</link>
            <guid>41356528</guid>
            <pubDate>Mon, 26 Aug 2024 12:37:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nsa.gov/helpful-links/nsa-foia/declassification-transparency-initiatives/historical-releases/view/article/3880193/capt-grace-hopper-on-future-possibilities-data-hardware-software-and-people-1982/">https://www.nsa.gov/helpful-links/nsa-foia/declassification-transparency-initiatives/historical-releases/view/article/3880193/capt-grace-hopper-on-future-possibilities-data-hardware-software-and-people-1982/</a>, See on <a href="https://news.ycombinator.com/item?id=41356528">Hacker News</a></p>
Couldn't get https://www.nsa.gov/helpful-links/nsa-foia/declassification-transparency-initiatives/historical-releases/view/article/3880193/capt-grace-hopper-on-future-possibilities-data-hardware-software-and-people-1982/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Coolify’s rise to fame, and why it could be a big deal (141 pts)]]></title>
            <link>https://blog.api-fiddle.com/posts/coolify-revolution</link>
            <guid>41356239</guid>
            <pubDate>Mon, 26 Aug 2024 11:50:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.api-fiddle.com/posts/coolify-revolution">https://blog.api-fiddle.com/posts/coolify-revolution</a>, See on <a href="https://news.ycombinator.com/item?id=41356239">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>To this day, I remember deploying my first Next.js app on Vercel. It was a magical moment.</p>
<p>Before that, I had only known pain. From renting a server, dockerizing, setting up a proxy, SSL certificates, monitoring—you name it!</p>
<p>Then came Next.js and Vercel. Sure, they weren’t the first to promise seamless deployments. Products like Heroku, AWS Elastic Beanstalk, etc. all tried to
remove the pain from deploying software on the internet.</p>
<p>However, it never felt as magical as connecting my repository to Vercel and pressing that sweet, sweet deploy button.</p>
<p>What happened next with Next.js and Vercel is far less magical... but back to the blog post.</p>
<h2><span>#</span>Coolify Clicked</h2>
<p>While the internet is in an AI craze, I quietly had another magical moment. This time, it wasn’t
fueled by a multimillion-dollar company like Google, AWS, or Vercel.</p>
<p>It was fueled by <a rel="nofollow" href="https://x.com/heyandras?lang=en" target="_blank">Andras Bacsai</a>, who turned down over 30 investors to build Coolify as a
community-funded project. It is a true revolution as I will try to explain later in this blog post.</p>
<p>During the last weeks, I was on the lookout for the best solution to deploy the <a rel="nofollow" href="https://api-fiddle.com/" target="_blank">Api-Fiddle</a> REST API. After considering <a rel="nofollow" href="https://fly.io/" target="_blank">fly.io</a>, a friend pointed me to <a rel="nofollow" href="https://github.com/coollabsio/coolify" target="_blank">Coolify</a>.
The name rang familiar—I’d seen it on X before!</p>
<p>Coolify describes itself as:</p>
<blockquote>
<p>An open-source &amp; self-hostable Heroku / Netlify / Vercel alternative.</p>
</blockquote>
<p>I was hooked and ready to give it a try. I rented a Virtual Private Server (VPS) on <a rel="nofollow" href="https://hetzner.com/" target="_blank">Hetzner</a> and set it up with Coolify. I followed this fantastic
<a rel="nofollow" href="https://www.youtube.com/watch?v=taJlPG82Ucw&amp;t=3431s&amp;pp=ygUHY29vbGlmeQ%3D%3D" target="_blank">YouTube tutorial</a>.</p>
<p>While setting up a VPS isn’t exactly magical, it was simple enough, and it only took couple of hours to get Coolify running on my server.</p>
<h2><span>#</span>My Coolify Moment</h2>
<p>My Coolify moment happened right then and there.</p>
<p>Coolify is still in it's early days and the UI can be a bit rough. But with little effort I became the CEO of my own Vercel-like,
fully-featured deployment platform.</p>
<p>Deploying a PostgreSQL instance took three clicks and 30 seconds. Configuring a running instance of Grafana? Another 30 seconds.</p>
<p>When it was time to deploy my app. I pointed Coolify to my GitHub repository and specified the domain.</p>
<p>After a bit of fiddling my app was deployed with a database, hourly backups, and a <a rel="nofollow" href="https://caddyserver.com/" target="_blank">Caddy</a> proxy. Just like that.</p>
<p>Once you get the hang of it, deploying any app that is either dockerized or <a rel="nofollow" href="https://nixpacks.com/docs/getting-started" target="_blank">Nixpacks-compatible</a> is a breeze.</p>
<p>This alone makes me talk about Coolify with excitement. But I believe there is more to this story.</p>
<p>Coolify has the potential to change how small to mid-sized projects and companies -
deploy internal and external tooling.</p>
<h2><span>#</span>The Rise to Fame</h2>
<p>Looking back, I’ve only had one moment in my software career that felt like a turning point: My first deployment to Vercel.
Now, I’ve had another one with Coolify. Before you disagree, let me explain.</p>
<p>No, Coolify isn’t something entirely new. Like many new things, it’s a wrapper around existing technologies. Now and then, you could piece
together these technologies yourself and, depending on your skill and luck, end up with something very similar to what Coolify offers.</p>
<p>You can achieve this with foundational tools like Kubernetes or Docker Compose and additional layers to support rolling updates, webhooks, etc.</p>
<p>Additionally, Coolify isn’t the first attempt to build an open-source Platform as a Service (PaaS). Tools like Dokku have been
around since 2014 (Coolify began in 2021).</p>
<p>However, Coolify’s explosive growth in 2024 suggests we’re witnessing a different level of adoption and impact on the wider software community.</p>
<p><img src="https://blog.api-fiddle.com/post-media/0008-graph.png" alt="Comparison of github starts between Coolify and Dokky" width="864" height="549"></p><p><a rel="nofollow" href="https://star-history.com/#dokku/dokku&amp;coollabsio/coolify&amp;Date" target="_blank">Check out this graph</a>.</p>
<p>To paint the full picture of Coolify's rise to fame, we also need to talk about the recent rise of self-hosted software in general.</p>
<p>There’s a wide variety of open-source, self-hostable software out there—from Calendly alternative <a rel="nofollow" href="https://cal.com/" target="_blank">Cal.com</a>
to email and marketing tools like <a rel="nofollow" href="https://www.mailcoach.app/" target="_blank">Mailcoach</a>.</p>
<p>In recent years, many software companies have developed a business model around self-hosted versions of their products.</p>
<p>While altruism surely plays a role in open soucring software, it can be good for business too. Small comapnies often open source
products so they get adopted by large organizations quicker! This model is called
<a rel="nofollow" href="https://cal.com/blog/open-source" target="_blank">Commercial Open Source Software</a> (COSS).</p>
<p>Large enterprises self-host tools and retain ownership of their data. In return, they sign a service contract with the vendor.
This saves companies years of security certifications and enterprise sales cycles. All for the price of open sourcing their product that
no small company in their right mind would ever self-host...<strong>Right? Right!</strong></p>
<p>In the past, most self-hosting (aside from WordPress and a few other examples) was cumbersome for small companies. It was easier to wire $100 to Salesforce and
hope Slack will do right by them and their data.</p>
<p>I belive Coolify has the potential to change that. Coolify can enable organizations of any size to host an arbitrary number of free,
self-hosted software easier than ever.</p>
<p>Let’s look at this tweet from Jarek Ceborski, an indie creator, who self-hosted Mailcoach in just a couple of hours (I'm not sure if he uses Coolify):</p>

<p>Here's another Tweet from Theo, a larger creator:</p>

<p>More and more developers seem to take advantage of free, self-hostable software.</p>
<p>My argument is this: The increasing availability of well-designed, self-hostable software combined with tools that make self-hosting
trivial has the potential to shake things up.</p>
<h2><span>#</span>Dorothy, are we still in Kansas?</h2>
<p>One could be tempted to call this a full circle. We came from a world of on-premise (On-Prem) Software, moved to the Cloud, and ended up
in a Software as a Service (SaaS) world. Now, are we really considering leaving SaaS behind to return to cloud instances?</p>
<p>It may look like we're one step away from dusting off our old on-prem servers.</p>
<p>While this perspective has merit, there's another way to view it. We can choose to celebrate the wealth of tools and deployment options
available, allowing us to optimize for cost efficiency and data ownership.</p>
<h2><span>#</span>Final Thoughts</h2>
<p>This is an overwhelmingly positive review of Coolify, but it's important to remember that despite its wide usage, Coolify is still in its early stages.</p>
<p>This article was not sponsored by Coolify, and I have no affiliation with the product other than being a user.</p>
<p>If you’re looking to get started with Coolify, the best place to start is likely <a rel="nofollow" href="https://www.youtube.com/watch?v=taJlPG82Ucw&amp;t=3431s&amp;pp=ygUHY29vbGlmeQ%3D%3D" target="_blank">this video</a>
along with their <a rel="nofollow" href="https://coolify.io/" target="_blank">website</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fixing a Bug in Google Chrome as a First-Time Contributor (490 pts)]]></title>
            <link>https://cprimozic.net/blog/fixing-a-bug-in-google-chrome/</link>
            <guid>41355303</guid>
            <pubDate>Mon, 26 Aug 2024 09:10:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cprimozic.net/blog/fixing-a-bug-in-google-chrome/">https://cprimozic.net/blog/fixing-a-bug-in-google-chrome/</a>, See on <a href="https://news.ycombinator.com/item?id=41355303">Hacker News</a></p>
Couldn't get https://cprimozic.net/blog/fixing-a-bug-in-google-chrome/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Dutch DPA fines Uber 290M euro because of transfers of drivers' data to the US (292 pts)]]></title>
            <link>https://www.autoriteitpersoonsgegevens.nl/en/current/dutch-dpa-imposes-a-fine-of-290-million-euro-on-uber-because-of-transfers-of-drivers-data-to-the-us</link>
            <guid>41355021</guid>
            <pubDate>Mon, 26 Aug 2024 08:15:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.autoriteitpersoonsgegevens.nl/en/current/dutch-dpa-imposes-a-fine-of-290-million-euro-on-uber-because-of-transfers-of-drivers-data-to-the-us">https://www.autoriteitpersoonsgegevens.nl/en/current/dutch-dpa-imposes-a-fine-of-290-million-euro-on-uber-because-of-transfers-of-drivers-data-to-the-us</a>, See on <a href="https://news.ycombinator.com/item?id=41355021">Hacker News</a></p>
Couldn't get https://www.autoriteitpersoonsgegevens.nl/en/current/dutch-dpa-imposes-a-fine-of-290-million-euro-on-uber-because-of-transfers-of-drivers-data-to-the-us: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Avante.nvim: Use Your Neovim Like Using Cursor AI IDE (255 pts)]]></title>
            <link>https://github.com/yetone/avante.nvim</link>
            <guid>41353835</guid>
            <pubDate>Mon, 26 Aug 2024 03:44:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yetone/avante.nvim">https://github.com/yetone/avante.nvim</a>, See on <a href="https://news.ycombinator.com/item?id=41353835">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">avante.nvim</h2><a id="user-content-avantenvim" aria-label="Permalink: avante.nvim" href="#avantenvim"></a></p>
<p dir="auto"><strong>avante.nvim</strong> is a Neovim plugin designed to emulate the behaviour of the <a href="https://www.cursor.com/" rel="nofollow">Cursor</a> AI IDE. It provides users with AI-driven code suggestions and the ability to apply these recommendations directly to their source files with minimal effort.</p>
<div dir="auto"><p dir="auto">Note</p>
<p dir="auto">🥰 This project is undergoing rapid iterations, and many exciting features will be added successively. Stay tuned!</p>
</div>
<details open="">
  <summary>
    
    <span aria-label="Video description avante-2.mp4">avante-2.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/1206493/357962425-510e6270-b6cf-459d-9a2f-15b397d1fe53.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjQ2NzIxMDIsIm5iZiI6MTcyNDY3MTgwMiwicGF0aCI6Ii8xMjA2NDkzLzM1Nzk2MjQyNS01MTBlNjI3MC1iNmNmLTQ1OWQtOWEyZi0xNWIzOTdkMWZlNTMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDgyNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA4MjZUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDg4ZTIzYjU1Y2FlNDI5MjFlNDQxMGI1OGRkZTY2MGY0MmM0OTE1YmJiMGRmZTU1ZmRmN2M0N2VlMDIxNmY0NiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.0gv_uhlSjDbCgdHxz524wbNG9AWaIOjET2DF30C7aiM" data-canonical-src="https://private-user-images.githubusercontent.com/1206493/357962425-510e6270-b6cf-459d-9a2f-15b397d1fe53.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjQ2NzIxMDIsIm5iZiI6MTcyNDY3MTgwMiwicGF0aCI6Ii8xMjA2NDkzLzM1Nzk2MjQyNS01MTBlNjI3MC1iNmNmLTQ1OWQtOWEyZi0xNWIzOTdkMWZlNTMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDgyNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA4MjZUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDg4ZTIzYjU1Y2FlNDI5MjFlNDQxMGI1OGRkZTY2MGY0MmM0OTE1YmJiMGRmZTU1ZmRmN2M0N2VlMDIxNmY0NiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.0gv_uhlSjDbCgdHxz524wbNG9AWaIOjET2DF30C7aiM" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    
    <span aria-label="Video description avante-3.mp4">avante-3.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/1206493/358215978-86140bfd-08b4-483d-a887-1b701d9e37dd.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjQ2NzIxMDIsIm5iZiI6MTcyNDY3MTgwMiwicGF0aCI6Ii8xMjA2NDkzLzM1ODIxNTk3OC04NjE0MGJmZC0wOGI0LTQ4M2QtYTg4Ny0xYjcwMWQ5ZTM3ZGQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDgyNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA4MjZUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTdjODAxMGQ5ZDFjMDg2NDBjMGZjZTg0N2FiY2ZiYTY0NWRkMjU5YTgyNTFjYTAyNTllYjNiNzIzMmJjNWMxOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.imMakzMmOSIjDueTLrg3ZsLKzhV_gb2Ue5zKYjzYSEs" data-canonical-src="https://private-user-images.githubusercontent.com/1206493/358215978-86140bfd-08b4-483d-a887-1b701d9e37dd.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjQ2NzIxMDIsIm5iZiI6MTcyNDY3MTgwMiwicGF0aCI6Ii8xMjA2NDkzLzM1ODIxNTk3OC04NjE0MGJmZC0wOGI0LTQ4M2QtYTg4Ny0xYjcwMWQ5ZTM3ZGQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDgyNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA4MjZUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTdjODAxMGQ5ZDFjMDg2NDBjMGZjZTg0N2FiY2ZiYTY0NWRkMjU5YTgyNTFjYTAyNTllYjNiNzIzMmJjNWMxOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.imMakzMmOSIjDueTLrg3ZsLKzhV_gb2Ue5zKYjzYSEs" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>AI-Powered Code Assistance</strong>: Interact with AI to ask questions about your current code file and receive intelligent suggestions for improvement or modification.</li>
<li><strong>One-Click Application</strong>: Quickly apply the AI's suggested changes to your source code with a single command, streamlining the editing process and saving time.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Install <code>avante.nvim</code> using <a href="https://github.com/folke/lazy.nvim">lazy.nvim</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;yetone/avante.nvim&quot;,
  event = &quot;VeryLazy&quot;,
  build = &quot;make&quot;,
  opts = {
    -- add any opts here
  },
  dependencies = {
    &quot;nvim-tree/nvim-web-devicons&quot;, -- or echasnovski/mini.icons
    &quot;stevearc/dressing.nvim&quot;,
    &quot;nvim-lua/plenary.nvim&quot;,
    &quot;MunifTanjim/nui.nvim&quot;,
    --- The below is optional, make sure to setup it properly if you have lazy=true
    {
      'MeanderingProgrammer/render-markdown.nvim',
      opts = {
        file_types = { &quot;markdown&quot;, &quot;Avante&quot; },
      },
      ft = { &quot;markdown&quot;, &quot;Avante&quot; },
    },
  },
}"><pre>{
  <span><span>"</span>yetone/avante.nvim<span>"</span></span>,
  <span>event</span> <span>=</span> <span><span>"</span>VeryLazy<span>"</span></span>,
  <span>build</span> <span>=</span> <span><span>"</span>make<span>"</span></span>,
  <span>opts</span> <span>=</span> {
    <span><span>--</span> add any opts here</span>
  },
  <span>dependencies</span> <span>=</span> {
    <span><span>"</span>nvim-tree/nvim-web-devicons<span>"</span></span>, <span><span>--</span> or echasnovski/mini.icons</span>
    <span><span>"</span>stevearc/dressing.nvim<span>"</span></span>,
    <span><span>"</span>nvim-lua/plenary.nvim<span>"</span></span>,
    <span><span>"</span>MunifTanjim/nui.nvim<span>"</span></span>,
    <span><span>---</span> The below is optional, make sure to setup it properly if you have lazy=true</span>
    {
      <span><span>'</span>MeanderingProgrammer/render-markdown.nvim<span>'</span></span>,
      <span>opts</span> <span>=</span> {
        <span>file_types</span> <span>=</span> { <span><span>"</span>markdown<span>"</span></span>, <span><span>"</span>Avante<span>" </span></span>},
      },
      <span>ft</span> <span>=</span> { <span><span>"</span>markdown<span>"</span></span>, <span><span>"</span>Avante<span>" </span></span>},
    },
  },
}</pre></div>
<p dir="auto">For Windows users, change the build command to the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;yetone/avante.nvim&quot;,
  event = &quot;VeryLazy&quot;,
  build = &quot;powershell -ExecutionPolicy Bypass -File Build-LuaTiktoken.ps1&quot;,
  -- rest of the config
}"><pre>{
  <span><span>"</span>yetone/avante.nvim<span>"</span></span>,
  <span>event</span> <span>=</span> <span><span>"</span>VeryLazy<span>"</span></span>,
  <span>build</span> <span>=</span> <span><span>"</span>powershell -ExecutionPolicy Bypass -File Build-LuaTiktoken.ps1<span>"</span></span>,
  <span><span>--</span> rest of the config</span>
}</pre></div>
<div dir="auto"><p dir="auto">Important</p>
<p dir="auto"><code>avante.nvim</code> is currently only compatible with Neovim 0.10.0 or later. Please ensure that your Neovim version meets these requirements before proceeding.</p>
</div>
<div dir="auto"><p dir="auto">Important</p>
<p dir="auto">If your neovim doesn't use LuaJIT, then change <code>build</code> to <code>make lua51</code>. By default running make will install luajit.
For ARM-based setup, make sure to also install cargo as we will have to build the tiktoken_core from source.</p>
</div>
<div dir="auto"><p dir="auto">Note</p>
<p dir="auto">Recommended <strong>Neovim</strong> options:</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- views can only be fully collapsed with the global statusline
vim.opt.laststatus = 3
-- Default splitting will cause your main splits to jump when opening an edgebar.
-- To prevent this, set `splitkeep` to either `screen` or `topline`.
vim.opt.splitkeep = &quot;screen&quot;"><pre><span><span>--</span> views can only be fully collapsed with the global statusline</span>
<span>vim</span>.<span>opt</span>.<span>laststatus</span> <span>=</span> <span>3</span>
<span><span>--</span> Default splitting will cause your main splits to jump when opening an edgebar.</span>
<span><span>--</span> To prevent this, set `splitkeep` to either `screen` or `topline`.</span>
<span>vim</span>.<span>opt</span>.<span>splitkeep</span> <span>=</span> <span><span>"</span>screen<span>"</span></span></pre></div>
</div>
<div dir="auto"><p dir="auto">Note</p>
<p dir="auto"><code>render-markdown.nvim</code> is an optional dependency that is used to render the markdown content of the chat history. Make sure to also include <code>Avante</code> as a filetype
to its setup:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;MeanderingProgrammer/render-markdown.nvim&quot;,
  opts = {
    file_types = { &quot;markdown&quot;, &quot;Avante&quot; },
  },
  ft = { &quot;markdown&quot;, &quot;Avante&quot; },
}"><pre>{
  <span><span>"</span>MeanderingProgrammer/render-markdown.nvim<span>"</span></span>,
  <span>opts</span> <span>=</span> {
    <span>file_types</span> <span>=</span> { <span><span>"</span>markdown<span>"</span></span>, <span><span>"</span>Avante<span>" </span></span>},
  },
  <span>ft</span> <span>=</span> { <span><span>"</span>markdown<span>"</span></span>, <span><span>"</span>Avante<span>" </span></span>},
}</pre></div>
</div>
<p dir="auto">Default setup configuration:</p>
<p dir="auto"><em>See <a href="https://github.com/yetone/avante.nvim/blob/main/lua/avante/config.lua">config.lua#L9</a> for the full config</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  ---@alias Provider &quot;openai&quot; | &quot;claude&quot; | &quot;azure&quot;  | &quot;copilot&quot; | [string]
  provider = &quot;claude&quot;,
  claude = {
    endpoint = &quot;https://api.anthropic.com&quot;,
    model = &quot;claude-3-5-sonnet-20240620&quot;,
    temperature = 0,
    max_tokens = 4096,
  },
  mappings = {
    ask = &quot;<leader>aa&quot;,
    edit = &quot;<leader>ae&quot;,
    refresh = &quot;<leader>ar&quot;,
    --- @class AvanteConflictMappings
    diff = {
      ours = &quot;co&quot;,
      theirs = &quot;ct&quot;,
      none = &quot;c0&quot;,
      both = &quot;cb&quot;,
      next = &quot;]x&quot;,
      prev = &quot;[x&quot;,
    },
    jump = {
      next = &quot;]]&quot;,
      prev = &quot;[[&quot;,
    },
    submit = {
      normal = &quot;<CR>&quot;,
      insert = &quot;<C-s>&quot;,
    },
    toggle = {
      debug = &quot;<leader>ad&quot;,
      hint = &quot;<leader>ah&quot;,
    },
  },
  hints = { enabled = true },
  windows = {
    wrap = true, -- similar to vim.o.wrap
    width = 30, -- default % based on available width
    sidebar_header = {
      align = &quot;center&quot;, -- left, center, right for title
      rounded = true,
    },
  },
  highlights = {
    ---@type AvanteConflictHighlights
    diff = {
      current = &quot;DiffText&quot;,
      incoming = &quot;DiffAdd&quot;,
    },
  },
  --- @class AvanteConflictUserConfig
  diff = {
    debug = false,
    autojump = true,
    ---@type string | fun(): any
    list_opener = &quot;copen&quot;,
  },
}"><pre>{
  <span><span>---</span><span>@alias</span> <span>Provider</span> <span><span>"</span>openai<span>" </span></span><span>| </span><span><span>"</span>claude<span>" </span></span><span>| </span><span><span>"</span>azure<span>"  </span></span><span>| </span><span><span>"</span>copilot<span>" </span></span><span>| </span><span>[string]</span></span>
  <span>provider</span> <span>=</span> <span><span>"</span>claude<span>"</span></span>,
  <span>claude</span> <span>=</span> {
    <span>endpoint</span> <span>=</span> <span><span>"</span>https://api.anthropic.com<span>"</span></span>,
    <span>model</span> <span>=</span> <span><span>"</span>claude-3-5-sonnet-20240620<span>"</span></span>,
    <span>temperature</span> <span>=</span> <span>0</span>,
    <span>max_tokens</span> <span>=</span> <span>4096</span>,
  },
  <span>mappings</span> <span>=</span> {
    <span>ask</span> <span>=</span> <span><span>"</span>&lt;leader&gt;aa<span>"</span></span>,
    <span>edit</span> <span>=</span> <span><span>"</span>&lt;leader&gt;ae<span>"</span></span>,
    <span>refresh</span> <span>=</span> <span><span>"</span>&lt;leader&gt;ar<span>"</span></span>,
    <span><span>---</span><span> @class</span> <span>AvanteConflictMappings</span></span>
    <span>diff</span> <span>=</span> {
      <span>ours</span> <span>=</span> <span><span>"</span>co<span>"</span></span>,
      <span>theirs</span> <span>=</span> <span><span>"</span>ct<span>"</span></span>,
      <span>none</span> <span>=</span> <span><span>"</span>c0<span>"</span></span>,
      <span>both</span> <span>=</span> <span><span>"</span>cb<span>"</span></span>,
      <span>next</span> <span>=</span> <span><span>"</span>]x<span>"</span></span>,
      <span>prev</span> <span>=</span> <span><span>"</span>[x<span>"</span></span>,
    },
    <span>jump</span> <span>=</span> {
      <span>next</span> <span>=</span> <span><span>"</span>]]<span>"</span></span>,
      <span>prev</span> <span>=</span> <span><span>"</span>[[<span>"</span></span>,
    },
    <span>submit</span> <span>=</span> {
      <span>normal</span> <span>=</span> <span><span>"</span>&lt;CR&gt;<span>"</span></span>,
      <span>insert</span> <span>=</span> <span><span>"</span>&lt;C-s&gt;<span>"</span></span>,
    },
    <span>toggle</span> <span>=</span> {
      <span>debug</span> <span>=</span> <span><span>"</span>&lt;leader&gt;ad<span>"</span></span>,
      <span>hint</span> <span>=</span> <span><span>"</span>&lt;leader&gt;ah<span>"</span></span>,
    },
  },
  <span>hints</span> <span>=</span> { <span>enabled</span> <span>=</span> <span>true</span> },
  <span>windows</span> <span>=</span> {
    <span>wrap</span> <span>=</span> <span>true</span>, <span><span>--</span> similar to vim.o.wrap</span>
    <span>width</span> <span>=</span> <span>30</span>, <span><span>--</span> default % based on available width</span>
    <span>sidebar_header</span> <span>=</span> {
      <span>align</span> <span>=</span> <span><span>"</span>center<span>"</span></span>, <span><span>--</span> left, center, right for title</span>
      <span>rounded</span> <span>=</span> <span>true</span>,
    },
  },
  <span>highlights</span> <span>=</span> {
    <span><span>---</span><span>@type</span> <span>AvanteConflictHighlights</span></span>
    <span>diff</span> <span>=</span> {
      <span>current</span> <span>=</span> <span><span>"</span>DiffText<span>"</span></span>,
      <span>incoming</span> <span>=</span> <span><span>"</span>DiffAdd<span>"</span></span>,
    },
  },
  <span><span>---</span><span> @class</span> <span>AvanteConflictUserConfig</span></span>
  <span>diff</span> <span>=</span> {
    <span>debug</span> <span>=</span> <span>false</span>,
    <span>autojump</span> <span>=</span> <span>true</span>,
    <span><span>---</span><span>@type</span> <span>string </span><span>| </span><span>fun</span><span>(): </span><span>any</span></span>
    <span>list_opener</span> <span>=</span> <span><span>"</span>copen<span>"</span></span>,
  },
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Given its early stage, <code>avante.nvim</code> currently supports the following basic functionalities:</p>
<div dir="auto"><p dir="auto">Important</p>
<p dir="auto">Avante will only support OpenAI (and its variants including copilot and azure), and Claude out-of-the-box due to its high code quality generation.
For all OpenAI-compatible providers, see <a href="https://github.com/yetone/avante.nvim/wiki">wiki</a> for more details.</p>
</div>
<div dir="auto"><p dir="auto">Important</p>
<p dir="auto">For most consistency between neovim session, it is recommended to set the environment variables in your shell file.
By default, <code>Avante</code> will prompt you at startup to input the API key for the provider you have selected.</p>
<p dir="auto">For Claude:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export ANTHROPIC_API_KEY=your-api-key"><pre><span>export</span> ANTHROPIC_API_KEY=your-api-key</pre></div>
<p dir="auto">For OpenAI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export OPENAI_API_KEY=your-api-key"><pre><span>export</span> OPENAI_API_KEY=your-api-key</pre></div>
<p dir="auto">For Azure OpenAI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export AZURE_OPENAI_API_KEY=your-api-key"><pre><span>export</span> AZURE_OPENAI_API_KEY=your-api-key</pre></div>
</div>
<ol dir="auto">
<li>Open a code file in Neovim.</li>
<li>Use the <code>:AvanteAsk</code> command to query the AI about the code.</li>
<li>Review the AI's suggestions.</li>
<li>Apply the recommended changes directly to your code with a simple command or key binding.</li>
</ol>
<p dir="auto"><strong>Note</strong>: The plugin is still under active development, and both its functionality and interface are subject to significant changes. Expect some rough edges and instability as the project evolves.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Bindings</h2><a id="user-content-key-bindings" aria-label="Permalink: Key Bindings" href="#key-bindings"></a></p>
<p dir="auto">The following key bindings are available for use with <code>avante.nvim</code>:</p>
<ul dir="auto">
<li><kbd>Leader</kbd><kbd>a</kbd><kbd>a</kbd> — show sidebar</li>
<li><kbd>Leader</kbd><kbd>a</kbd><kbd>r</kbd> — show sidebar</li>
<li><kbd>c</kbd><kbd>o</kbd> — choose ours</li>
<li><kbd>c</kbd><kbd>t</kbd> — choose theirs</li>
<li><kbd>c</kbd><kbd>b</kbd> — choose both</li>
<li><kbd>c</kbd><kbd>0</kbd> — choose none</li>
<li><kbd>]</kbd><kbd>x</kbd> — move to previous conflict</li>
<li><kbd>[</kbd><kbd>x</kbd> — move to next conflict</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Highlight Groups</h2><a id="user-content-highlight-groups" aria-label="Permalink: Highlight Groups" href="#highlight-groups"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Highlight Group</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>AvanteTitle</td>
<td>Title</td>
</tr>
<tr>
<td>AvanteReversedTitle</td>
<td>Used for rounded border</td>
</tr>
<tr>
<td>AvanteSubtitle</td>
<td>Selected code title</td>
</tr>
<tr>
<td>AvanteReversedSubtitle</td>
<td>Used for rounded border</td>
</tr>
<tr>
<td>AvanteThirdTitle</td>
<td>Prompt title</td>
</tr>
<tr>
<td>AvanteReversedThirdTitle</td>
<td>Used for rounded border</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">TODOs</h2><a id="user-content-todos" aria-label="Permalink: TODOs" href="#todos"></a></p>
<ul>
<li> Chat with current file</li>
<li> Apply diff patch</li>
<li> Chat with the selected block</li>
<li> Slash commands</li>
<li> Edit the selected block</li>
<li> Smart Tab (Cursor Flow)</li>
<li> Chat with project</li>
<li> Chat with selected files</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<ul dir="auto">
<li><strong>Enhanced AI Interactions</strong>: Improve the depth of AI analysis and recommendations for more complex coding scenarios.</li>
<li><strong>LSP + Tree-sitter + LLM Integration</strong>: Integrate with LSP and Tree-sitter and LLM to provide more accurate and powerful code suggestions and analysis.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions to avante.nvim are welcome! If you're interested in helping out, please feel free to submit pull requests or open issues. Before contributing, ensure that your code has been thoroughly tested.</p>
<p dir="auto">See <a href="https://github.com/yetone/avante.nvim/wiki">wiki</a> for more recipes and tricks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">avante.nvim is licensed under the Apache License. For more details, please refer to the <a href="https://github.com/yetone/avante.nvim/blob/main/LICENSE">LICENSE</a> file.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Helen Fisher, who researched the brain’s love circuitry, has died (108 pts)]]></title>
            <link>https://www.nytimes.com/2024/08/23/science/helen-fisher-dead.html</link>
            <guid>41353811</guid>
            <pubDate>Mon, 26 Aug 2024 03:41:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/08/23/science/helen-fisher-dead.html">https://www.nytimes.com/2024/08/23/science/helen-fisher-dead.html</a>, See on <a href="https://news.ycombinator.com/item?id=41353811">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/08/23/science/helen-fisher-dead.html: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>