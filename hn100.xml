<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 12 Apr 2024 20:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: 5 Years Ago I made the Recovery Kit, I just made the RK2 (114 pts)]]></title>
            <link>https://www.doscher.com/recovery-kit-version-2/</link>
            <guid>40014937</guid>
            <pubDate>Fri, 12 Apr 2024 16:45:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.doscher.com/recovery-kit-version-2/">https://www.doscher.com/recovery-kit-version-2/</a>, See on <a href="https://news.ycombinator.com/item?id=40014937">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                <p>If you're not familiar with the Recovery Kit, it's something I made back in 2019 and posted to my original back7.co blog.  I created it before I knew what a "cyberdeck" was beyond a simple part of William Gibson's <a href="https://amzn.to/3VRuXKo?ref=doscher.com" rel="noreferrer">Neuromancer</a>.  I wanted a rugged computer enclosure for my Raspberry Pi, and I'd been unhappy with overly simplistic and cheap cases for the Raspberry Pi at the time.  I had some projects in mind for GPIO, but sadly many of those haven't come to light since I made the Recovery Kit 5 years ago.</p><p>I have been making other stuff since then, but the Recovery Kit got picked up by <a href="https://www.theverge.com/circuitbreaker/2019/11/27/20983472/raspberry-pi-recovery-kit-apocalypse-cyberdeck-build-jay-doscher?ref=doscher.com" rel="noreferrer">The Verge</a>, the <a href="https://magpi.raspberrypi.com/articles/raspberry-pi-recovery-kit-project-showcase?ref=doscher.com" rel="noreferrer">Raspberry Pi Magazine</a>, Hackster, and more- but the bucket list items were from <a href="https://news.ycombinator.com/item?id=21647398&amp;ref=doscher.com" rel="noreferrer">Hackernews</a>, <a href="https://hackaday.com/2019/11/11/a-mobile-terminal-for-the-end-of-the-world/?ref=doscher.com" rel="noreferrer">Hackaday</a>, and <a href="https://uncrate.com/off-grid-cyberdeck-computer/?ref=doscher.com" rel="noreferrer">Uncrate</a>.  I'm still a daily reader of all three!  In the last 5 years I've lost count of all the variations I've made in CAD to try and make something I felt was an improvement on the well-rounded Recovery Kit.  I am grateful for the springboard that the exposure this project has granted me.  It's allowed me to connect and share with a pretty special audience.  However, my designs over the last few years for an updated RK were left unpublished.  They  just didn't show enough variation on the original idea.  The Quick Kit and its variants have arguably had a bigger impact to people, since it allowed for making a rugged Pi computer with a much simpler assembly process (and without any soldering).  The versions I released late last year only need a single 3D printed part and some hardware off Amazon, so making something simpler wasn't my direction here.</p><h3 id="challenges">Challenges</h3><p>This is going to be a pretty lengthy post, but if you just want the parts list, it's at the end of the article. For now though, I'd like to talk about some of the challenges with the original Recovery Kit.</p><p><strong>The battery.</strong>  Folks asking for a battery version almost outnumbers the people on Reddit asking, "<a href="https://www.reddit.com/r/DumbComments/?ref=doscher.com" rel="noreferrer">what do you use it for</a>?".  The battery one hit home for me though- the original kit had a battery that could handle the high amperage of a booting Raspberry Pi, but it disappeared off Amazon shortly before I published the post.  I was worried that it was a safety issue or recall, so I had to omit it.  I'm using a much larger battery this time, and I think I have a good approach for the new builds- more on that in a bit.</p><p><strong>The keyboard</strong>.  The original keyboard is the <a href="https://www.daskeyboard.com/blog/what-is-an-ortholinear-keyboard/?ref=doscher.com" rel="noreferrer">ortho</a> style for one major reason beyond looks- ANSI keyboards are just too wide to fit in the lid of a Pelican 1300 case- maybe a 40% might but that's asking too much of people.  The height of the lid was a real issue ergonomically too-typing on a keyboard surrounded by a lid's lip like that is not fun.  On top of that, the original Plaid keyboard wasn't programmed when it shipped, and the instructions for programming the chip were... not written well.  That made it really hard to recommend.  I have a newer pre-built keyboard that will lose me points with the handmade crowd, but hopefully this is easier for people to use.  Keyboards are still tough though- they don't stay in stock for long.</p><p><strong>The original Recovery Kit wiring</strong>.  Just look at it!</p><figure><img src="https://www.doscher.com/content/images/2024/04/JAY01401.jpg" alt="" loading="lazy" width="1000" height="667" srcset="https://www.doscher.com/content/images/size/w600/2024/04/JAY01401.jpg 600w, https://www.doscher.com/content/images/2024/04/JAY01401.jpg 1000w" sizes="(min-width: 720px) 720px"></figure><p>Seriously- I love the look of that harness, but between that and a through-hole keyboard kit, each deck took more than 100 hours to print.  That made the deck too hard for most people new to soldering, but also too expensive for most people to buy commissions.  I am open for commissions on the new build through the end of June 2024- email me if you're interested!</p><p><strong>The handle.</strong>  That handle and the clevis joints making the ends of the handle just above the monitor would catch on the Pelican case, rotate, and damage the screen.  Not good, and the handle is barely usable there.  I have a better option this time though.</p><p><strong>The GPIO.</strong>  I actually still love these, but these connectors are also pretty hard to solder.  The new one uses crimps. The idea is still the same though- I want an easy way to access GPIO for custom add-on boards and devices.</p><h3 id="compromises"><strong>Compromises</strong></h3><p>While I think I have good options for the stuff I have covered so far, there are still a few compromises I needed to make this time.  Maybe these will be in a larger deck sometime?</p><p><strong>Screen Size.</strong>  The Raspberry Pi 7" Touchscreen display is laughably bad for a desktop, but pretty decent for a terminal.  That's why you see the RKV2's here sporting terminals.  You can run a desktop, but why?  One good reason is to launch your favorite SDR app.  I don't actually intend on using this with a window manager though- this will be the workhorse the original was intended to be, enabling other devices off-grid.</p><p><strong>Power controls.</strong>  With all the changes I made, there's not the room I once had for individual power toggles.  This time there's a simple push button for the Raspberry Pi 5's power switch, or a GPIO input for the Raspberry Pi 4.  Internally there's simple plug connectors- less glamorous but also easy to mod and use as needed.  Again, reducing the amount of solder here as much as I can- and this is the only solder joint!  While I am appreciative that we have a couple through-hole connectors, I would have really loved the Pi Foundation to use a connector for this.</p><p><strong>No x86</strong>.  Not yet!</p><h3 id="build-overview"><strong>Build Overview</strong></h3><p>With all the background info covered, let's get into the build.  The general concept is still the same, but with modern printers like the Bambu or other more open CoreXY printers, this is a pretty easy parts list to print.</p><figure><img src="https://www.doscher.com/content/images/2024/04/800X600-Video-3.gif" alt="" loading="lazy" width="800" height="600" srcset="https://www.doscher.com/content/images/size/w600/2024/04/800X600-Video-3.gif 600w, https://www.doscher.com/content/images/2024/04/800X600-Video-3.gif 800w" sizes="(min-width: 720px) 720px"></figure><p>It goes together easy, but with a mix of M4, M3, and M2.5 screws.  No more M5 screws this time around, but an electric screwdriver is a must.  I ended up using a <a href="https://amzn.to/3VNQrry?ref=doscher.com" rel="noreferrer">Hoto</a> and the bits from my <a href="https://amzn.to/4au2DCn?ref=doscher.com" rel="noreferrer">iFixit Mako Driver Kit</a> - the tools are also linked at the end of the article.</p><figure><img src="https://www.doscher.com/content/images/2024/04/JAY00264-1.jpg" alt="" loading="lazy" width="1000" height="667" srcset="https://www.doscher.com/content/images/size/w600/2024/04/JAY00264-1.jpg 600w, https://www.doscher.com/content/images/2024/04/JAY00264-1.jpg 1000w" sizes="(min-width: 720px) 720px"></figure><p>I also used <a href="https://us.store.bambulab.com/collections/petg/products/petg-cf?ref=doscher.com" rel="noreferrer">Bambu's own Carbon Fiber PETG</a>.  Either Pelican or Orchestra changed their dye color since the original Recovery Kit, and it wasn't a good match this time.</p><p><strong>Network Switch</strong>- the same reliable 5 port gigabit switch is here, but has been press-fit into place this time- the accuracy of Fusion 360 and the repeatability of the Bambu X1C has let me really dial in this printer- so no ugly boot on the back holding the switch to the frame.</p><p><strong>Ethernet</strong> - Of course Ethernet is making an appearance!  This time we're using a slightly more expensive passthrough from McMaster Carr, but I'm hoping to skip the drama I ran into with the Quick Kit of parts shortages.  If you haven't used McMaster Carr you should check it out- their website is an amazing collection of industrial parts complete with visual references and specs for each part.</p><p><strong>GPIO</strong> is back again, but far more condensed this time to make room for the battery.  It took some squeezing but there are plenty of GPIO pins to re-use, and I am happy to say the connector is from McMaster Carr, but the crimpers are far cheaper off Amazon, and will be linked at the end of the article.  With crimpers on some extra long silicone jumper wires, they are easy to move around- I will probably add labels though, since I don't quite have the variety of cable colors that I did last time.</p><figure><img src="https://www.doscher.com/content/images/2024/04/JAY00253-2.jpg" alt="" loading="lazy" width="1000" height="667" srcset="https://www.doscher.com/content/images/size/w600/2024/04/JAY00253-2.jpg 600w, https://www.doscher.com/content/images/2024/04/JAY00253-2.jpg 1000w" sizes="(min-width: 720px) 720px"></figure><p>USB-C connectors are all over this build, and it could be cleaner- but with a few zip ties or fabric cable ties, this build actually comes apart pretty easy.  There's a couple key things to keep in mind:</p><ul>
<li>The USB-C power on the Power Bank is "smart" and will not see the Pi or network switch when the battery is first turned on, so you have to unplug and re-plug the USB-C connector on the battery.  A custom circuit may be better in the future, but considering we're using off the shelf parts, this is a compromise I had to make.</li>
<li>There are two USB-C ports that map back to the USB ports on the Pi.  You can easily "hotwire" one of them with a USB-C female-to-female adapter and make one a power port for the whole system.  There's a ton of options here, and it turned out to be really helpful building this system.</li>
<li>My setup uses right angle USB-A to USB-C connectors, and those might block the other ports- if you need all four Pi USB ports, you may need a few more adapters.</li>
</ul>
<figure><img src="https://www.doscher.com/content/images/2024/04/JAY00266.jpg" alt="" loading="lazy" width="1000" height="667" srcset="https://www.doscher.com/content/images/size/w600/2024/04/JAY00266.jpg 600w, https://www.doscher.com/content/images/2024/04/JAY00266.jpg 1000w" sizes="(min-width: 720px) 720px"></figure><p>Now for the good stuff- the NVME and the battery!  This system is able to run a full NVME drive as the boot drive AND do it all from battery.  I intentionally have space around the battery and vents at the top of the enclosure, but the system runs great at normal room temps.  With a black enclosure I don't recommend leaving this in the sun.  Jeff Geerling has a <a href="https://www.jeffgeerling.com/blog/2023/nvme-ssd-boot-raspberry-pi-5?ref=doscher.com" rel="noreferrer">great writeup</a> including the commands needed to boot from the NVME.  </p><figure><img src="https://www.doscher.com/content/images/2024/04/JAY00265.jpg" alt="" loading="lazy" width="1000" height="667" srcset="https://www.doscher.com/content/images/size/w600/2024/04/JAY00265.jpg 600w, https://www.doscher.com/content/images/2024/04/JAY00265.jpg 1000w" sizes="(min-width: 720px) 720px"></figure><p><strong>The DSI cable</strong> (in copper above).</p><p>This DSI cable has got to be one of the things I like least about the direction the Pi Foundation has gone.  Simply put, the new connector for the CSI (camera)/ DSI (display) ports is smaller, requiring a new cable.  Here's the catch though- it's physically the same size as a million of them on Amazon and Ali Express, but the only one I found to work was purchased from the Pi website/vendors... and there's a specific cable just for the displays, and another specific cable for the cameras.  This is not something that's easy to find at the time I write this.</p><figure><img src="https://www.doscher.com/content/images/2024/04/JAY00252.jpg" alt="" loading="lazy" width="1000" height="667" srcset="https://www.doscher.com/content/images/size/w600/2024/04/JAY00252.jpg 600w, https://www.doscher.com/content/images/2024/04/JAY00252.jpg 1000w" sizes="(min-width: 720px) 720px"></figure><p>The Power Button.  This is one of my favorites, it really adds a nice touch and helps for a quick shutdown, but really you can rewire it to GPIO if you want, especially if you plan to use a Pi4 since only the Pi5 has the solder points for the button.  In a pinch you could also print your own part and use this as a radio connector too if you're doing an internal SDR.  There are mounting posts and a frame for mounting an internal SDR if you want, but for now on this build it's a humble power button with a nice click to it.</p><figure><img src="https://www.doscher.com/content/images/2024/04/JAY00273.jpg" alt="" loading="lazy" width="1000" height="667" srcset="https://www.doscher.com/content/images/size/w600/2024/04/JAY00273.jpg 600w, https://www.doscher.com/content/images/2024/04/JAY00273.jpg 1000w" sizes="(min-width: 720px) 720px"></figure><p>Let's talk about the battery. I am using the <a href="https://amzn.to/3PXZuT4?ref=doscher.com" rel="noreferrer">Shargeek Storm 2</a> 100W, 25,600mAh power bank. Aside from doing the normal autosense that all smart banks do, this one has really held up well!  Even with the NVME it boots up fine and runs great- some builds of Raspberry Pi OS complained breifly about undervoltage, but I think that's an artifact of how the Pi5 tests- the screen showed that the Pi5 never came close to the battery's limits.  If you're going to keep this in log term storage, unplug the USB connector from the battery, or remove it altogether- the whole frame comes out with just a few screws.</p><p>The big downer is that we can't charge and use this battery at the same time- so that "hotwiring" of a USB-C port really helps here.</p><p>the Keyboard is our last stop before covering the parts list.  It's a <a href="https://drop.com/buy/planck-mechanical-keyboard/?ref=doscher.com" rel="noreferrer">Drop/OLKB Planck v7</a>.  This is a kit available as no-solder and I am using the white-on-black keys here.  While you can use the stock case, I made one a little bit wider that can wedge into the lid.  This leaves it tucked away for convenience and travel, but can pop out on a desk for daily usage.</p><figure><img src="https://www.doscher.com/content/images/2024/04/JAY00270.jpg" alt="" loading="lazy" width="1000" height="667" srcset="https://www.doscher.com/content/images/size/w600/2024/04/JAY00270.jpg 600w, https://www.doscher.com/content/images/2024/04/JAY00270.jpg 1000w" sizes="(min-width: 720px) 720px"></figure><p>The Recovery Kit Version Two took quite some time, and I'd like to thank my subscribers for supporting me with this build.  Paid members will have access to the post on my site that contains the STL files for this build, and the CAD members will have access to the Fusion 360 design files.  Monthly membership with access to the STL files for all my builds starts at just $5 a month, but the annual membership is heavily discounted and really helps me out.  Thanks for reading, and thanks for your support!</p><h3 id="parts-list">Parts List</h3><p><strong>Core Compute &amp; Display</strong><br>
Raspberry Pi 5 - <a href="https://amzn.to/3xADesc?ref=doscher.com">Amazon</a>, <a href="https://www.adafruit.com/product/5813?ref=doscher.com">Adafruit</a>, <a href="https://www.sparkfun.com/products/23551?ref=doscher.com">Sparkfun</a>, <a href="https://www.digikey.com/en/products/detail/raspberry-pi/SC1112/21658257?ref=doscher.com">Digikey</a> (Amazon may be faster but is almost always a scalper or reseller, so beware).<br>
Raspberry Pi 5 Heatsink - <a href="https://amzn.to/3Jeuvyf?ref=doscher.com">Amazon</a><br>
Pi5-Specific DSI Cable - <a href="https://www.sparkfun.com/products/23683?ref=doscher.com">Sparkfun</a> (DO NOT USE THE GENERIC VERSIONS OF THIS CABLE)<br>
Pi5 NVME adapter - <a href="https://amzn.to/4aKs6qK?ref=doscher.com">Amazon</a><br>
2242/42mm NVME - <a href="https://amzn.to/4aOQuYf?ref=doscher.com">Amazon</a><br>
Raspberry Pi 7" Touchscreen Display - <a href="https://amzn.to/4b804pB?ref=doscher.com">Amazon</a><br>
Ethernet Switch - <a href="https://amzn.to/3xwkEkZ?ref=doscher.com">Amazon</a></p>
<p><strong>Cables &amp; Adapters</strong><br>
Right Angle Ethernet Cable - <a href="https://amzn.to/4cPDDXI?ref=doscher.com">Amazon</a><br>
Zip Ties - <a href="https://amzn.to/3VW7IPu?ref=doscher.com">Amazon</a><br>
Hook and Loop Ties - <a href="https://amzn.to/3JckQbF?ref=doscher.com">Amazon</a><br>
USB-C Cable (for the Pi) - <a href="https://amzn.to/3TQRft7?ref=doscher.com">Amazon</a><br>
USB-C to Barrel Jack (for the network switch) - <a href="https://amzn.to/49uqOiB?ref=doscher.com">Amazon</a><br>
Right Angle Barrel Jack (for the network switch) - <a href="https://amzn.to/4cPxFWR?ref=doscher.com">Amazon</a><br>
Short Ethernet Patch Cable - <a href="https://amzn.to/43TtK7b?ref=doscher.com">Amazon</a><br>
30cm Jumper Wires - <a href="https://amzn.to/4cPxxXn?ref=doscher.com">Amazon</a><br>
Right Angle USB A Male to USB-C Female (2x kits): <a href="https://amzn.to/3TRvLfz?ref=doscher.com">Amazon</a></p>
<p><strong>Front Panel Connectors</strong><br>
DC Power jack - <a href="https://amzn.to/3xxacto?ref=doscher.com">Amazon</a><br>
GPIO Connector (Panel mount male) - <a href="https://www.mcmaster.com/6168T59/?ref=doscher.com">McMaster-Carr</a><br>
GPIO Connector (Female connector for any cable/IO you want to add) - <a href="https://www.mcmaster.com/6168T39/?ref=doscher.com">McMaster-Carr</a><br>
Panel Mount Ethernet - <a href="https://www.mcmaster.com/1422N13/?ref=doscher.com">McMaster-Carr</a><br>
Power Button - <a href="https://www.mcmaster.com/6886K111/?ref=doscher.com">McMaster-Carr</a><br>
Quick-Connect wires for the Power Button - <a href="https://www.adafruit.com/product/1152?ref=doscher.com">Adafruit</a><br>
Panel Mount USB-C (x2) - <a href="https://amzn.to/4cLJxJz?ref=doscher.com">Amazon</a></p>
<p><strong>Battery</strong><br>
Shargeek Storm 2 Battery - <a href="https://amzn.to/4aMEbvE?ref=doscher.com">Amazon</a><br>
Right Angle USB-C to 2x USB-C - <a href="https://amzn.to/3JfVRE6?ref=doscher.com">Amazon</a></p>
<p><strong>Keyboard Components</strong><br>
OLKB/Drop.com Planck v7 PCB - <a href="https://drop.com/buy/planck-mechanical-keyboard?ref=doscher.com">Drop.com</a><br>
Keycaps (MT3 White on Black) - <a href="https://drop.com/buy/drop-mt3-white-on-black-keycap-set?defaultSelectionIds=965854&amp;ref=doscher.com">Drop.com</a><br>
OLKB Switch Plate - <a href="https://drop.com/buy/drop-olkb-planck-switch-plate?defaultSelectionIds=976711&amp;ref=doscher.com">Drop.com</a><br>
Coiled USB-C Cord - <a href="https://amzn.to/43QfRH7?ref=doscher.com">Amazon</a><br>
Cherry MX Switches - (3x) <a href="https://amzn.to/3W10hpU?ref=doscher.com">Amazon</a></p>
<p><strong>Screws, Case, &amp; Misc Hardware</strong><br>
Pelican 1300 Case - <a href="https://amzn.to/3VTqarF?ref=doscher.com">Amazon</a><br>
M4x10mm Socket Head, Black Oxide - <a href="https://www.mcmaster.com/91290a144/?ref=doscher.com">McMaster-Carr</a><br>
M3x10mm Socket Head, Black Oxide - <a href="https://www.mcmaster.com/91290A115/?ref=doscher.com">McMaster-Carr</a><br>
M3x20mm Socket Head, Black Oxide - <a href="https://www.mcmaster.com/91290A123/?ref=doscher.com">McMaster-Carr</a><br>
M3x8mm Flathead, Black Oxide - <a href="https://www.mcmaster.com/91294A128/?ref=doscher.com">McMaster-Carr</a><br>
(All these screws can be ordered from Amazon as well, but the McMaster-Carr tend to be much higher quality, and in lots of 100 or 50, so you have some for the next project!</p>
<p><strong>Tools</strong><br>
HOTO Electric Screwdriver - <a href="https://amzn.to/3Jjmzvu?ref=doscher.com">Amazon</a><br>
iFixit Mako - <a href="https://amzn.to/3JezLln?ref=doscher.com">Amazon</a><br>
Connector Crimpers (required for the GPIO connector) - <a href="https://amzn.to/3VScSvC?ref=doscher.com">Amazon</a><br>
Pinecil Soldering Iron - <a href="https://amzn.to/4aKtfyy?ref=doscher.com">Amazon</a></p>
<p><strong>Digital Files</strong><br>
Digital files are for paid members only- signing up is very reasonable with membership granting you access to all my designs. CAD and Elite members can access the CAD and STL files <a href="https://www.doscher.com/cad-subscribers-only-recovery-kit-version-two-cad-files/">here</a>, STL members can access the STL zip file via <a href="https://www.doscher.com/paid-subscribers-only-recovery-kit-version-two-stl-files/">this post</a>.</p>

            </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: I'm Peter Roberts, immigration attorney who does work for YC and startups. AMA (131 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40014087</link>
            <guid>40014087</guid>
            <pubDate>Fri, 12 Apr 2024 15:34:07 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40014087">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="40016433"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016433" href="https://news.ycombinator.com/vote?id=40016433&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Getting a visa and staying on it can be emotionally difficult situation, do you know if there are therapists who specialize in helping with this area?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40016429"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016429" href="https://news.ycombinator.com/vote?id=40016429&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Hi Peter, which kind of visa would you recommend for a computer science master graduate from Germany.  Thank you very much.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40016371"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016371" href="https://news.ycombinator.com/vote?id=40016371&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi, Peter. Thank you for taking questions again.<p>Would you recommend a 3rd year(1.5 year to go) US PhD student with moderate research portfolio( + strong recommender )to look into EB2/NIW or O1 visa? The goals are to immigrate to the US and avoid H1B lottery.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40016319"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016319" href="https://news.ycombinator.com/vote?id=40016319&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Thanks for taking the time to do this today!<p>I'd like to ask how you found yourself practicing in this area of the law and any advice you may have for those who will be entering law school in a few months.</p><p>I will be heading into my first year of law school in a few months with some general ideas of what I'd like to do after graduation (compliance, regulatory, M&amp;A, contracts) with no concrete ideas either way.</p><p>- Would you have any specific advice on what you would have liked to have done in law school that would have helped you later on in your career that you'd recommend students to tackle early on?</p><p>- Are there any "wasted time/effort traps" that you might caution students to look out for as they navigate their program?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40016245"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016245" href="https://news.ycombinator.com/vote?id=40016245&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter - I understand that it's possible for recent STEM graduates to be self-employed, or a 1-man C-Corp, for the first year of their post-completion OPT. But in the 2 year STEM extension, it seems that you need a distinct supervisor to fill out I-983.<p>How do you suggest that a "solo entrepreneur" modify their business structure so that they can continue working on their business projects during the STEM OPT period? I've heard some people say that you need to have US Citizen co-owners of the company who own a majority share; others say that it suffices to have US Citizens on the Board, as this technically gives them the ability to remove you. Is there any such setup that is clearly within the bounds of the STEM OPT program?</p><p>Also - are there any other options aside from the STEM OPT extension? In particular, if the individual is from an E2 treaty country, is this the sort of business that could qualify for an E2 visa, or do those businesses have to have the potential to employ US Citizens?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40016399"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016399" href="https://news.ycombinator.com/vote?id=40016399&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter,<p>We have an employee that just won the H1B lotto. We don't actually pay employees any cash (all future payments). Will this be an issue?</p><p>Thanks!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40015740"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015740" href="https://news.ycombinator.com/vote?id=40015740&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>As an American living abroad who wants to help my international friends I’ve worked with get to the US and into tech jobs could I sponsor them or vouch for them in anyway? One is an Indian citizen the other Turkish but will soon have his German citizenship.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015800"><td></td></tr>
                  <tr id="40016263"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016263" href="https://news.ycombinator.com/vote?id=40016263&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Peter there's ongoing debate raging on whether showing immigrant intent at any point, precludes one from using a single intent visa such as TN anytime in the future. Is that true or is it the case if one can demonstrate close ties to your home country and abandonment of intent that you can resume use of these non-immigrant visas?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40014715"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014715" href="https://news.ycombinator.com/vote?id=40014715&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>My impression is that over the past 20 years or so, the visa process has become a lot more bureaucratic, involved, and time consuming.<p>Is that your impression as well? What has changed, and why? Are any of the changes for the better?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40014772"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40014772" href="https://news.ycombinator.com/vote?id=40014772&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>I agree. From an outcome standpoint, the changes are for the worse.  But some of them are understandable to the extent that in certain contexts USCIS is paying more attention to the letter of the law and not just accepting representations on face value but requiring evidence. From my standpoint, the biggest negative changes are the delays in the processing of green card applications (partly statutory and partly administrative) and the lack of access to USCIS officers.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40014825"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40014825" href="https://news.ycombinator.com/vote?id=40014825&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Thanks, that's sounds very reasonable.<p>Unfortunately, it also sounds like there is not much push at this point to reverse these changes and streamline the process.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40014869"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40014869" href="https://news.ycombinator.com/vote?id=40014869&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Not much although the Biden Administration is prodding the agencies involved in immigration to facilitate (within the bounds of the law) the attraction and retention of tech/AI talent and companies.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40016288"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40016288" href="https://news.ycombinator.com/vote?id=40016288&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>My PERM was certified yesterday after 12 months and 2 weeks. That used to take 6 months.<p>I'm now backlogged until October which never used to happen at all to ROW (rest of world excluding India and China) applicants.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="40016139"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016139" href="https://news.ycombinator.com/vote?id=40016139&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>What are you telling people who are either on a visa or looking at getting a visa with the upcoming elections coming up and a potential reversion to a previous administration with more strict visa rules?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40016155"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016155" href="https://news.ycombinator.com/vote?id=40016155&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter, thank you so much for doing this.<p>What are your recommendations for a successful EB2 NIW application for someone who holds an MS in Computer Science from EU? For example, is there a any particular salary minimum, an expected number of years of experience, or do I need to run a company? E.g. what can I do to improve my odds? Thanks.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40016204"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016204" href="https://news.ycombinator.com/vote?id=40016204&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Thank you for this, Peter.<p>As a non-US tech worker, I find many American companies not willing to hire (through my own pre-established company), is there something I can do to make that easier/smoother?</p><p>I have worked that way before but it seems a hard sell to many other. What do you recommend?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40016236"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016236" href="https://news.ycombinator.com/vote?id=40016236&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Hi Peter, thanks for taking this. How feasible is it for a Canadian Citizen who has a software consulting service to apply for E2 visa? Is it worth pursuing? What things are important to keep in mind for that?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40015161"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015161" href="https://news.ycombinator.com/vote?id=40015161&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>US citizen here. I have a special situation which I think will be only of interest to me...<p>My wife (a green card holder) and I have moved abroad and her travel document is expiring soon (in July). We have no returned to the US in the past 3 years, and were not planning to visit only for the purpose of resetting the counter..</p><p>We are planning to actually fly into the US for a vacation right after the expiry date... what if anything can be done to renew or extend the travel document (a.k.a re-entry permit)? Of course we are in good standing with respect to US taxation.</p><p>Else, can she apply for an ESTA (country of citizenship is part of the visa waiver program for tourists), which I believe be done up to 3 days before travel (is it ok to apply right at the deadline when the permit is about to expire but just so we travel 1 or 2 days immediately after expiry) ?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015194"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015194" href="https://news.ycombinator.com/vote?id=40015194&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Good questions and a not-uncommon situation. This is very specific to your situation so please email me.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40016093"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016093" href="https://news.ycombinator.com/vote?id=40016093&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter, thank you in advance for considering the question.<p>I'm a Canadian citizen, living in Canada. In the past I've considered moving to the US to work there.</p><p>I don't have a university degree. I attended university for ~3-4 years, did not complete my degree, and then enrolled in a 3-year college program which conferred an Ontario College Advanced Diploma. I've worked full-time as a software developer since then.</p><p>I think my expertise is comparable to my coworkers, many of whom have completed university degrees (albeit in a variety of disciplines, not specifically related to technology).</p><p>Could I move and work in the US under a TN visa? I'm concerned that, because I don't have a degree, I would not qualify. Though I do have specialized training (diploma + work experience).</p><p>Or am I looking at this backwards? i.e, if a US company were interested in hiring me, they would handle the visa requirements (I'm unsure if this would be TN or otherwise).</p><p>Thanks!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40015609"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015609" href="https://news.ycombinator.com/vote?id=40015609&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter,<p>Current Bio PhD student here. I would like to know more about ~ minimum requirements and costs for applying for an EB2 NIW. Bio papers take a long time to come out, so I would like to know whether I can still apply during the PhD while some of the papers are still in peer review or whether I would have to wait for a postdoc.</p><p>Thanks for doing this!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015820"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015820" href="https://news.ycombinator.com/vote?id=40015820&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Published papers are not a requirement for an EB2 NIW filing; they might help but they're absolutely not required.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40016135"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016135" href="https://news.ycombinator.com/vote?id=40016135&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Also, do you know of startups taking advantage of TEA status (e.g. 99% of Puerto Rico) to lower the EB-5 investment requirement back down to $800k?  (I didn't realize it had gotten up to $1.050mm; that's high!)</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40015895"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015895" href="https://news.ycombinator.com/vote?id=40015895&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter,<p>My wife is going to pursue her PhD in US in the coming months, and I'm working as part-time remote employee of a US startup, while owning a startup in India. 
What options do I have, which would allow me to visit her for 1-2 months, without affecting my work?</p><p>I've heard B1/B2 holders being questioned during entry after they stayed for a month during previous trips. 
As I'm not working full time for the US company, I'm not eligible for L1.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40016079"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40016079" href="https://news.ycombinator.com/vote?id=40016079&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>A B-1/B-2 visa would not allow you to work for the U.S. company while here or your Indian startup while here.  It might seem like overkill but possibly get an O-1 through the U.S. company.  There's no minimum amount of time that you must spend in the U.S.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40016286"><td></td></tr>
                        <tr id="40015919"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015919" href="https://news.ycombinator.com/vote?id=40015919&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter,<p>For F1 students, is it permissible to use the 3-year OPT period to operate a self-owned LLC?</p><p>Can one be employed by another company while managing their own LLC during this period?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40016020"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016020" href="https://news.ycombinator.com/vote?id=40016020&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>I'm an Australian living in the US on an E3 visa.  I would like to get a green card, but we never win the lottery to move to a H1B and start the process.  My youngest daughter was born in the US, and so is a citizen.  Is there any way to get a green card without having to win the H1B lottery?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40016048"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40016048" href="https://news.ycombinator.com/vote?id=40016048&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Yes.  You can be in E-3 status and pursue a green card; it just means that your international travel might be limited during the process.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40016089"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40016089" href="https://news.ycombinator.com/vote?id=40016089&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>I moved from e3 to green card. There is a no-travel period between petition and granting, but it’s possible</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40016100"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016100" href="https://news.ycombinator.com/vote?id=40016100&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Curious if you've seen any E-2 visas granted to people from Grenada, etc. to people not born in those countries (i.e. naturalized) after the 2021 (?) NDAA/Portugal friendship/etc. legal change. to E-2.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40016069"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016069" href="https://news.ycombinator.com/vote?id=40016069&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>I am actually curious if you see any indication of people leaving/emigrating from the USA? Either serial entrepreneurs who have already had some success or those who feel there are sufficient opportunities with less hassle elsewhere.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40014474"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014474" href="https://news.ycombinator.com/vote?id=40014474&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Can you comment on different models for international employees of a US startup who split time between their home country and USA?<p>It seems clear that they could come a few weeks a year to USA for training, meetings, etc. but coming for a year+ to do their work in USA clearly requires work authorization.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40014572"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40014572" href="https://news.ycombinator.com/vote?id=40014572&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>The frequency and duration of the trips can raise questions but in and of themselves aren't issues; what matters is the purpose of the trips, regardless of the duration; they must be limited to non-work/non-hands-on activities and the primary beneficiary of those activities must be entities or individuals outside the U.S. So it's possible to spend a lot of time in the U.S. as a visitor without crossing any line. But it's still important to be mindful of later repercussions even if no line is crossed; that is, if and when applying for a work visa, for example, an applicant might be questioned aggressively by a consular officer about the applicant's time in the U.S. as a visitor and might simply not believe that the applicant wasn't working if the applicant spent a lot of time in the U.S.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015669"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015669" href="https://news.ycombinator.com/vote?id=40015669&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hey Peter, I'm a foreigner residing abroad with an american spouse of 10 years and we have kids holding American Passports. I'm currently working as a contractor for a company in the US, but I'm thinking of moving to the US and applying for the IR1 visa due to family reasons. I am the primary earner of the household.<p>What are my limitations in this case? Can I keep the contract I'm currently in while the visa process chugs along? What about proof of income?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40016099"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40016099" href="https://news.ycombinator.com/vote?id=40016099&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>A friend of mine did the "fiancee visa" in Europe at the local embassy there. It took something like 3-5 months to get the visa approved for travel to the US. Once there, he could start working immediately.<p>But this was a long time ago, I have no idea what the processing times are today? Are they listed online?</p><p>And above all, since his spouse did not work in the US, so they needed a US based citizen with "not bad" finances to sponsor him.</p><p>If you're an independent contractor based in a different country, it'll be fun times figuring out the taxes for the year that you move to the US. Both your old country and the US will want a cut.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40015760"><td></td></tr>
            <tr id="40015873"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015873" href="https://news.ycombinator.com/vote?id=40015873&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>For you to apply for your green card while in the U.S., you need to be in the U.S. in a work visa status, not in a visitor status.  Alternatively, you can get your green card outside the U.S. through a U.S. Consulate (known as an "immigrant visa") but that process would take longer 6-18 months (versus 6 months or less if filed in the U.S.).  And your employment as a contractor (as well as your and wife's assets) can be used to meet  the financial support requirements.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40016083"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40016083" href="https://news.ycombinator.com/vote?id=40016083&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter,<p>I'm a pure mathematics PhD student (from SE Asia, studying in the US) graduating at the end of this year. I expect to work at the intersection of finance and tech.</p><p>I have heard that the standard for O1 visa is a bit lower these days. Do you know if ~5 publications with ~20 citations is enough to qualify for O1? This is low compared to CS people but is pretty respectable in pure math. If not what are my options beside H1B? Thanks!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40015610"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015610" href="https://news.ycombinator.com/vote?id=40015610&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Recently there was a thread about whether SaaS pricing information (enterprise plans behind Contact Us form) could be protected under NDA as a trade secret and whether a customer sharing how much they are paying for an enterprise plan would be in trouble or not.<p>Can you share your thoughts? <a href="https://news.ycombinator.com/item?id=39980222">https://news.ycombinator.com/item?id=39980222</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015879"><td></td></tr>
                  <tr id="40015359"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015359" href="https://news.ycombinator.com/vote?id=40015359&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter,<p>thanks so much for doing this! Here is an interesting situation: I live in the US on a J2 visa with my wife being the J1 holder (PhD student at an ivy league). I (German national) founded a startup in my home country and against all my efforts, it's going quite well so far. I'm currently holding the CEO/CTO title.</p><p>Assume that we'll raise the next funding round (seed) in the US (the market is just superior) and relocate/incorporate via a C-Corp here.</p><p>If my wife and I were to decide that we want to stay in the US for a longer time with a path to become naturalized down the road, what's the best option?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015686"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015686" href="https://news.ycombinator.com/vote?id=40015686&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>It looks like you and your wife will have multiple green card paths available to you either through your wife or through you, such as the EB2 national interest waiver path or the EB1 extraordinary ability or outstanding researcher path. Shorter (non green card) paths include the O-1 for both of you and the H-1B for her, among others.  The exact paths will require a discussion.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015743"><td></td></tr>
                        <tr id="40014552"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014552" href="https://news.ycombinator.com/vote?id=40014552&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>At what point should I make a legal entity for my business?<p>I’m working on a niche tool and have some interesting and I’m just starting to bring on the first users.</p><p>At what point should I incorporate?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40014719"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40014719" href="https://news.ycombinator.com/vote?id=40014719&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>That's not so much an immigration question but be careful about incorporating and creating products while currently employed because your current employer might have a claim over your IP. If the question is when can I/should I incorporate without crossing any immigration line and when should I get a work authorization through this company, then the act of incorporating isn't the issue, the question is whether your activities and the business's activities require work authorization and there are both legal and non-legal lines that shouldn't be crossed. The primary non-legal guideline is when a a gut level, the activities are no longer a sideline but a business, a focus of what you do. The primary legal guideline is when the company is starting to generate revenue.  In both instances, you should look to move to a work authorization status.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40014948"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40014948" href="https://news.ycombinator.com/vote?id=40014948&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>(Not a lawyer) I remember being told a rule of thumb that you should be a separate legal entity <i>before</i> "first contact" with a customer because that's generally where most of the risk of legal liability starts ramping up.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015182"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40015182" href="https://news.ycombinator.com/vote?id=40015182&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>That's one approach and a defensible one but I think that might hamstring efforts to build a company to the point where sponsorship is an option. Another approach is providing services or products for a fee.  But this is really a grey area and risks must be managed against the ability to be an entrepreneur and create valuable business and technology.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015602"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40015602" href="https://news.ycombinator.com/vote?id=40015602&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Oh woah, sorry I didn’t see the word “immigration” in the title.<p>Thank you for the answer nonetheless
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="40015581"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015581" href="https://news.ycombinator.com/vote?id=40015581&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hey Peter.. many thanks!  My husband is from Gibraltar and his spousal green card application has cleared USCIS and is with NVS.<p>Gibraltar has changed the name of the criminal record document it issues but NVS has not updated its system.  So they keep rejecting his documents even though we included a letter from the police commissioner explaining the situation.</p><p>Any suggestions?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015928"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015928" href="https://news.ycombinator.com/vote?id=40015928&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Call or email the NVC (there's an email address for questions) and if that doesn't work, get your Congressional representative involved (something that is done all the time and probably would be helpful here).</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40014685"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014685" href="https://news.ycombinator.com/vote?id=40014685&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>What's the easiest way for an H1b holder to work on their own startup and control their company? I've heard of VC like Unshackled, but taking VC seed is already shackling myself. I'm looking more for a solution where I'd pay some fee to a lawyer and turnkey solution for a company structure setup.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40014952"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40014952" href="https://news.ycombinator.com/vote?id=40014952&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>There are visas where the foreign national can control/own/run the company he or she works for, such as the E-1/E-2, O-1, and L-1 visas.  It's more complicated and tougher if you want to remain in H-1B status and work for your own company; in that situation, you have to give up some control to a cofounder or board. To be clear, it's possible to maintain your H-1B with your current employer and get a concurrent part-time H-1B through your own company (assuming the conditions above are met).</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015297"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015297" href="https://news.ycombinator.com/vote?id=40015297&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Hi Peter, what does a prospective founder on h1 need to know about starting up with YC? I’m really interested in becoming a founder however i currently work a full time job so not sure how much time I can legally dedicate based on h1. I also worry about the job safety aspect especially given my h1 (i do have an approved I140 if that matters). What would be the path to more flexibility that you have seen founders take?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015357"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015357" href="https://news.ycombinator.com/vote?id=40015357&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>That's more of a YC question because you have to go all in when you're in YC.  From an immigration standpoint, there are ways to find an immigration path participate in accelerators/incubators even if currently in H-1B status with another company.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015707"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015707" href="https://news.ycombinator.com/vote?id=40015707&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Is H-1B an option if I don't have a bachelor's degree (never went to university)? I'm currently doing remote contracting for a US company and I'm wondering what my options would be for working in the US</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015906"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015906" href="https://news.ycombinator.com/vote?id=40015906&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>The requirement is a bachelor's degree or higher or its equivalent and equivalent can be based on a combination of education and professional experience or even just professional experience.  So the H-1B might be an option for you.  Also, an O-1 doesn't require a degree but the standard is higher than (and different from) the H-1B.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40014795"><td></td></tr>
            <tr id="40015518"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015518" href="https://news.ycombinator.com/vote?id=40015518&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter, thanks for doing this. I’m on an H1B and my employer is preparing to apply for an EB2-NIW for me.<p>Assuming I get an I-140 approved through this, if I subsequently leave this employer to pursue my own startup, would it be easy to retain this I-140 and/or the associated P.D.?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40015646"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015646" href="https://news.ycombinator.com/vote?id=40015646&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hey Peter, appreciate the offer!<p>What advice would you give to a talented teenager who is looking to get to the U.S. under a B1/B2 visa for a short period (under 3 months) to engage as a volunteer in activities for a non-profit in the US (the organisation covers living expenses for the duration of their stay) and has failed a few times to get a visa approved? what would you advise them to do? Is the visa type incorrect?</p><p>For context the activities would be in the likes of content creation, helping organise events and perhaps open source software development.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015981"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015981" href="https://news.ycombinator.com/vote?id=40015981&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Broadly speaking, a B-1/B-2 would appear to be appropriate but U.S. Consulates oftentimes deny applications for such a visa even if it is the appropriate visa for other reasons (such as concerns about the applicant remaining in the U.S. or working without authorization) and the U.S. Consulates have a lot of discretion to deny such applications. You might want to look at the J-1 or even the H-2B or H-3 visa.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015314"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015314" href="https://news.ycombinator.com/vote?id=40015314&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>How can Mexican founders of a Delaware C Corporation establish an office, hire employees, and reside in Texas?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015376"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015376" href="https://news.ycombinator.com/vote?id=40015376&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>The TN visa might be the easiest quickest option.  The O-1 and E-2 also might be options although the timelines are longer and the costs are higher.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40014595"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014595" href="https://news.ycombinator.com/vote?id=40014595&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Do you recommend saas companies founded by non-us citizens to form llc and bank accounts in the us, or outside?
For the rest of the readers- what online service would you recommend for that as agent llc + accounting?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40014904"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40014904" href="https://news.ycombinator.com/vote?id=40014904&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>That really involves a case by case analysis particularly where work is being done in the U.S. for U.S. entities.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40014807"><td></td></tr>
                  <tr id="40015379"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015379" href="https://news.ycombinator.com/vote?id=40015379&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>If I am a Canadian citizen but I don’t have a computer science degree, is my only option to get H1-B?<p>And can I remotely work for a US company from Canada, given they don’t have a Canadian office, and I would be paid to a US bank account.  Would I still need a visa in this case?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40016003"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40016003" href="https://news.ycombinator.com/vote?id=40016003&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Question 1: No, there are a few TN occupations that don't require a degree and the O-1 doesn't require a degree.
Question 2: No, you would not need a visa; U.S. immigration doesn't reach beyond the borders of the U.S.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015386"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015386" href="https://news.ycombinator.com/vote?id=40015386&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Hello Peter, I am on F1 visa in US, working at Berkeley National Lab(DOE funded) as an intern for almost a year, I have received an offer to join them full time. Which visa should I consider: H1B or EB1/2 or O1?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40016034"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40016034" href="https://news.ycombinator.com/vote?id=40016034&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>The EB1/EB2 refers to a green card so that is probably not a good option because it takes too long.  Between the H-1B and the O-1; they each have their relative benefits: the H-1B would allow you to travel without interruption during the green card process and the O-1 is effectively easily transferrable to another employer (while the cap exempt H-1B is not).</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015631"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015631" href="https://news.ycombinator.com/vote?id=40015631&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>What would be the best manner to help a EU resident who works for a US co.?<p>Green card?
Visa?
Remote?</p><p>Any suggestions would be very helpful.  Specifically regarding taxes of income.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015990"><td></td></tr>
                <tr id="40016091"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40016091" href="https://news.ycombinator.com/vote?id=40016091&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>I'm maybe in a similar situation as OP so let me clarify what I would ask -- might be the same as them.<p>Citizen and resident of an EU (Schengen) country, de facto working for a US company, though the employment contract is a standard employment contract with the local (EU) entity.</p><p>Interested in moving to the US to become a permanent resident and eventual citizen, and I expect that the current company would be interested in sponsoring. What's the best path?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="40015283"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015283" href="https://news.ycombinator.com/vote?id=40015283&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>A friend of mine is on a TN status (Canadian) and has a lot of side-projects he’d love to monetize but feels like he can’t due to his status. What options does he have to make money off of his side projects barring returning to Canada?<p>Thanks in advance!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015303"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015303" href="https://news.ycombinator.com/vote?id=40015303&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>He could change his TN to a Management Consultant but that would mean entering into a consulting agreement with his current employer (assuming that he also wants to keep this employment).</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40016275"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40016275" href="https://news.ycombinator.com/vote?id=40016275&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Even if this change occurred would he be able to accept independent income generated outside of the sponsor of his TN?<p>In this case, wouldn't his new management consultant TN be tied to his (now former) employer, so would such revenues from his side projects need to flow through the TN sponsoring company in order not to step outside of the terms of his TN?</p><p>Side question: what are the risks of a TN holder operating what looks like commercial side projects whether they generate revenue or not (eg publishing a mobile game or running a successful website unrelated to the TN they hold - is it still commercial activity, is it still seen as removing a job from the US labor pool, etc?)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="40015235"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015235" href="https://news.ycombinator.com/vote?id=40015235&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>What are the odds of someone with a master's in Computer Science and 10+ years of experience (pre-degree) to get a EB-2 NIW visa? Does the master's itself increase the odds by a lot or does it even make any difference whatsoever?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40014579"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014579" href="https://news.ycombinator.com/vote?id=40014579&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Do you have any suggestions for finding firm or attorney that can help with immigration of health care workers, specifically nurses?<p>I have a friend searching for such a firm now and of the ~15 firms contacted none had any experience with health care immigration.</p><p>They have decades of experience with IT services and the path to H1-B for IT.</p><p>If no suggestions, can you share thoughts on why healthcare immigration attorney help seems to be difficult to find?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40014975"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40014975" href="https://news.ycombinator.com/vote?id=40014975&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>There used to be a visa specifically for nurses, the H-1C, and when that sunset, the options greatly were reduced but there still are solutions if the global organization is structured and funded properly. It's not appropriate for me to mention the names of firms here but I can give you some via email.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015004"><td></td></tr>
                  <tr id="40014669"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40014669" href="https://news.ycombinator.com/vote?id=40014669&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>My company (healthcare non-profit) filed my H1B on my behalf through Kramer Levin. Had to fill out a questionnaire, a little bit of back and forth, and they handled the rest.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015011"><td></td></tr>
                        <tr id="40015510"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015510" href="https://news.ycombinator.com/vote?id=40015510&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Why does the EAD/AP card exist? Let’s say one is on an L1 visa waiting for the GC. 
Why does the US even bother to issue the EAD?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015914"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015914" href="https://news.ycombinator.com/vote?id=40015914&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>If your L1 runs out before the GC is approved, you are allowed to stay in the US.<p>Without the EAD, you will NOT be able to work until the GC is approved.</p><p>Why is it not automatic? I assume USCIS doesn't want people to just get work authorization by filing frivolous i485 applications
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="40015792"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015792" href="https://news.ycombinator.com/vote?id=40015792&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>A very good question! There were historical reasons for it but now it just seems like a way to generate money for the USG.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015337"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015337" href="https://news.ycombinator.com/vote?id=40015337&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>A friend recently received greencard through eb3 a month ago. How long is it recommended to stay at the job before quitting/switching to a different employer</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015361"><td></td></tr>
                <tr id="40015459"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40015459" href="https://news.ycombinator.com/vote?id=40015459&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>This friend has already worked for the current employer for over 4 years under h1b visa with at will employment type. In this case, is it ok to say it is safe to quit?</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40014797"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014797" href="https://news.ycombinator.com/vote?id=40014797&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter,<p>What do you think about high skilled immigration in general post-covid era. A lot has happened since then...Ukraine war, recession, major macro economic shifts...How do these affect the talent pool in US ? Would you share your experiences ?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40014849"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40014849" href="https://news.ycombinator.com/vote?id=40014849&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>I don't see those as the main drivers of change for business/employment immigration. I think the biggest change is the explosion of AI technology and companies and the massive flow of money to AI technology/companies, which in turn is driving a frenzy of hiring of tech/AI talent.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40014642"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014642" href="https://news.ycombinator.com/vote?id=40014642&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Hey Peter!
How can a YC startup be founded in the US if none of the co-founders are US citizens? Does it mean after their batch they need to leave the country? Or are they allowed to stay in the US by the "$100k investment rule"?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015014"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015014" href="https://news.ycombinator.com/vote?id=40015014&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>If the activities of founders are limited, then it's possible to spend a significant amount of time in the U.S. as a business visitor but more often than not, a work status/visa is required and the timing of getting that must be managed so that that it's possible to remain and work in the U.S. But it's possible and done all the time.  (There's no $100k investment rule by the way unless you are thinking of the E-2 investor visa, which requires a substantial investment and is another type of work visa.)</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40014418"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014418" href="https://news.ycombinator.com/vote?id=40014418&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hello Peter, I have a friend coming from China and pursuing bachelor degree in America under F1 visa. What are some options available that can get green card as soon as possible other than the traditional route f1-&gt; h1b-&gt; eb3 ?<p>Is self-sponsorship an option?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015256"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015256" href="https://news.ycombinator.com/vote?id=40015256&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>You could always look at an E2 visa if you don't want to invest $800,000 in an EB-5 project; however, investment amounts vary from $150-400,000. The range is based on what USCIS would consider a "significant investment" based on your country of citizenship. E2s don't lead to a green card or US citizenship, but you may find a USC/PR spouse or another path while saving some capital.<p>Either E2 or EB-5 will have significant fees associated with them. Typical EB5 "administration/syndication" fees will run $80,000. You may be able to reduce this by going to the EB-5 Project directly and negotiating. Most of the time, 90% of that fee is used to pay a foreign agent(aka finder) for bringing the investor
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015270"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40015270" href="https://news.ycombinator.com/vote?id=40015270&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Oh wait, China doesn't have a E2 treaty. You may still be able to find a way via Grenada but I am not sure that tactic still works.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40014590"><td></td></tr>
            <tr id="40014464"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40014464" href="https://news.ycombinator.com/vote?id=40014464&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Is there any limitation or challenge in sponsoring greencard by very small business and the risk of getting rejected?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015255"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40015255" href="https://news.ycombinator.com/vote?id=40015255&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>As long as the company has the ability to pay the offered green card wage, green card sponsorship by small companies usually works.  The main issues are unrelated to a company's size.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40014526"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014526" href="https://news.ycombinator.com/vote?id=40014526&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Hey Peter, whats your checklist or just intuition for when a company is ready to support H1B visas? Is there some clear point between seed-round/current-YC-batch and public-mega-corp?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015059"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015059" href="https://news.ycombinator.com/vote?id=40015059&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>The bar is low. It's presumed that the sponsoring company will be able to pay the sponsored H-1B worker so the focus actually isn't on funding but the existence of basic corporate requirements - that is, the company must be incorporated, have an FEIN, have physical commercially zoned office space, and be authorized to do business where it operates. So, the long and short of it is that new small companies can sponsor H-1B workers.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015480"><td></td></tr>
            <tr id="40015477"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015477" href="https://news.ycombinator.com/vote?id=40015477&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Hi Peter, does the success of an H-1B application depend at all on the expertise level of an immigration lawyer representing me?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40014472"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014472" href="https://news.ycombinator.com/vote?id=40014472&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Hey Peter,
How can F1 students start their own startup and how does that affect their visa?
What needs to be done later as well?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015101"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015101" href="https://news.ycombinator.com/vote?id=40015101&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>While they are in school? That's tough and the student has to be very careful not to cross any lines. CPT and pre-completion OPT sometimes work but there are significant restrictions and conditions that must be met. Oftentimes, the solution is to take a leave of absence and switch to a work via (if possible and it oftentimes isn't).</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40014889"><td></td></tr>
            <tr id="40015370"><td></td></tr>
            <tr id="40016397"><td></td></tr>
            <tr id="40014908"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014908" href="https://news.ycombinator.com/vote?id=40014908&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Hi Peter, How easy is it to get an O1-A if someone meets 4 out of 8 criteria for extraordinary abilities but wants to do a single-member LLC and do some consulting to find the problem they would like to build a startup for?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015241"><td></td></tr>
                  <tr id="40014957"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014957" href="https://news.ycombinator.com/vote?id=40014957&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter,<p>I have a young friend in Russia who is a computer / programming prodigy. I'm trying to help him get out of Russia by getting a tech job in any other country. He's only 18 atm though.
Can you give me any advice? I'm based in Ireland so companies I've worked for + my friends work for would mainly be Irish. I have good contacts in USA and also Germany though.</p><p>I want to know...</p><p>1. If a company realizes he would be a great hire how feasible is it to hire a Russian?</p><p>2. Can the company help with Visa and getting him out of Russia (so as to not be conscripted)</p><p>3. What countries should he be applying for jobs in?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015287"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015287" href="https://news.ycombinator.com/vote?id=40015287&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>His age is not a bar.  Broadly speaking, there are visas available to really bright and talented young people (such as the O-1).  The issue is that Russian citizens are typically experiencing significant delays now when they take the final step of applying for a visa at a U.S. Consulate while background/security checks are done.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015578"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015578" href="https://news.ycombinator.com/vote?id=40015578&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Is it legal for a US employer to not promote US citizens that are currently part time to full time while hiring H1 Visa workers who do not do as good of a job?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015626"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015626" href="https://news.ycombinator.com/vote?id=40015626&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>Because the current I485 processing time is around 41 months or just shy of 4 years. That card gives you to opportunity to work and travel in/out of the country while waiting for your green card.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40014645"><td></td></tr>
                <tr id="40015108"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015108" href="https://news.ycombinator.com/vote?id=40015108&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>See my response above. After they graduate, it's much easier, OPT and under certain circumstances STEM OPT permit this.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015430"><td></td></tr>
                <tr id="40015473"><td></td></tr>
                              <tr id="40014855"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014855" href="https://news.ycombinator.com/vote?id=40014855&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Can illegal immigrants be hired?<p>There are so many educated illegal immigrants. Which is good for small startups i think
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40014883"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40014883" href="https://news.ycombinator.com/vote?id=40014883&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>If they have no work authorization, they can't be hired legally.  But even talented illegal immigrants, unless they are long-term overstays or have criminal records, can get sponsored for work visas.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40015318"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40015318" href="https://news.ycombinator.com/vote?id=40015318&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>What's current USCIS policy, in terms of months, for allowing out of status immigrants to be sponsored for say an H1B visa? 6 months, 1 year, longer?<p>Thank you in advance!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="40014639"><td></td></tr>
                <tr id="40015124"><td></td></tr>
                  <tr id="40015323"><td></td></tr>
            <tr id="40014742"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40014742" href="https://news.ycombinator.com/vote?id=40014742&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hi Peter, I've been curious regarding what sort of legal arrangements a certain family of content providers setup in order to display large sections of third party published materials.<p>Some examples</p><p>* Lexis Nexis news archives <a href="https://www.lexisnexis.com/en-gb/products/research-insights/news-archives" rel="nofollow">https://www.lexisnexis.com/en-gb/products/research-insights/...</a></p><p>* <a href="https://books.google.com/" rel="nofollow">https://books.google.com/</a></p><p>* <a href="https://www.perlego.com/" rel="nofollow">https://www.perlego.com/</a></p><p>Google books has restrictions to display a fraction of many texts but they obviously reproduced entire texts to filter upon. I'm curious if there are commentary apps out there that take whole literature and overlay new layers atop it and have published reproductions of their own somewhere in their infrastructure.</p><p>I'd like to know how websites like the above are made possible with most published stuff gated with permissions going like 
"""All rights reserved. No part of this publication may be reproduced, distributed, or transmitted in any form or by any means, including photocopying, recording, or other electronic or mechanical methods, without the prior written permission of the publisher""".</p><p>Thank you.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40014818"><td></td></tr>
                <tr id="40014860"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40014860" href="https://news.ycombinator.com/vote?id=40014860&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>whoops, my apologies, I somehow glossed over the immigration attorney detail and somehow my mind centered on IP attorney.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40015000"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015000" href="https://news.ycombinator.com/vote?id=40015000&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>Hello Mr Roberts,<p>How much do you agreed with the internet sentiment that HB-1 visas are exploitive? What rights or recourse do HB-1 holders have that they are not aware of?</p><p>I am an American citizen, but I do notice quite a few jobs filled by HB-1 that could have been filled by Americans but many Americans do not stay in because of low pay or poor career advancement.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40015149"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40015149" href="https://news.ycombinator.com/vote?id=40015149&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>The H-1B program is supposed to protect the wages and working conditions of U.S. workers (but it's debatable whether it does either) but there's no labor market test requirement to employ an H-1B workers.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40015394"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40015394" href="https://news.ycombinator.com/vote?id=40015394&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><p><span>I work for a large tech company, and have applied for EB3 green card in May 2023 (11 months ago). My priority date has been current since Jan 1st 2024. However, I have not heard any updates about my application. Is there anything concrete I can do to speed up the process?<p>I have read that reaching out to your congressperson/senator might help. Do you have any data on how successful this path is?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="40016106"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40016106" href="https://news.ycombinator.com/vote?id=40016106&amp;how=up&amp;goto=item%3Fid%3D40014087"></a></center>    </td><td><br><div>
                  <p><span>I am in a similar position with a priority date of June 2022 and have been current sin Jan 1. I would love to hear Peter's advice here. The endless checking of the application status is tough.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[96% of US hospital websites share visitor info with Meta, Google, data brokers (194 pts)]]></title>
            <link>https://www.theregister.com/2024/04/11/hospital_website_data_sharing/</link>
            <guid>40012466</guid>
            <pubDate>Fri, 12 Apr 2024 13:24:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/04/11/hospital_website_data_sharing/">https://www.theregister.com/2024/04/11/hospital_website_data_sharing/</a>, See on <a href="https://news.ycombinator.com/item?id=40012466">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Hospitals – despite being places where people implicitly expect to have their personal details kept private – frequently use tracking technologies on their websites to share user information with Google, Meta, data brokers, and other third parties, according to research published today.</p>
<p>Academics at the University of Pennsylvania analyzed a nationally representative sample of 100 non-federal acute care hospitals – essentially traditional hospitals with emergency departments – and <a target="_blank" href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2817444">their findings</a> were that 96 percent of their websites transmitted user data to third parties.</p>
<p>Additionally, not all of these websites even had a privacy policy. And of the 71 percent that did, 56 percent disclosed specific third-party companies that could receive user information.</p>

    

<p>"It's shocking, and really kind of incomprehensible," said Dr Ari Friedman, an assistant professor of emergency medicine at the University of Pennsylvania, who – along with Matthew McCoy, Angela Wu, Sam Burdyl, Yungjee Kim, Noell Kristen Smith, and Rachel Gonzales – authored the paper.</p>

        


        

<p>"People have cared about health privacy for a really, really, really long time," Friedman noted in an interview with <em>The Register</em>. "It's very fundamental to human nature. Even if it's information that you would have shared with people, there's still a loss, just an intrinsic loss, when you don't even have control over who you share that information with."</p>
<blockquote>

<p>There's an intrinsic loss when you don't even have control over who you share that information with&nbsp;</p>
</blockquote>
<p>The researchers' latest work builds on a study they <a target="_blank" rel="nofollow" href="https://www.healthaffairs.org/doi/abs/10.1377/hlthaff.2022.01205?journalCode=hlthaff&amp;journalCode=hlthaff">published</a> a year ago of 3,747 US non-federal hospital websites. That found 98.6 percent tracked and transferred visitors' data to large tech and social media companies, advertising firms, and data brokers.</p>
<p>To find the trackers on websites, the team checked out each hospitals' homepage on January 26 using <a target="_blank" rel="nofollow" href="https://github.com/agilemobiledev/webXray">webXray</a>, an open source tool that detects third-party HTTP requests and matches them to the organizations receiving the data. They also recorded the number of third-party cookies per page.&nbsp;</p>
<h3>Who is viewing your data?</h3>
<p>One name in particular stood out, in terms of who was receiving website visitors' information.</p>
<p>"In every study we've done, in any part of the health system, Google, whose parent company is Alphabet, is on nearly every page, including hospitals," Friedman observed.</p>
<p>"From there, it declines," he continued. "Meta was on a little over half of hospital webpages, and the Meta Pixel is notable because it seems to be one of the <a target="_blank" href="https://www.theregister.com/2024/01/18/facebook_tracking_data/">grabbier entities</a> out there in terms of tracking."</p>

        

<p>Both Meta and Google's tracking technologies have been the subject of <a target="_blank" href="https://www.theregister.com/2023/11/11/meta_youtube_criminal_charges/">criminal complaints</a> and <a target="_blank" href="https://www.theregister.com/2023/05/16/google_abortion_tracking_suit/">lawsuits</a> over the years – as have some <a target="_blank" href="https://www.theregister.com/2023/03/03/ftc_online_counseling_betterhelp/">healthcare companies</a> that shared data with these and other advertisers.&nbsp;</p>
<p>In addition, between 20 and 30 percent of the hospitals share data with Adobe, Friedman noted. "Everybody knows Adobe for PDFs. My understanding is they also have a <a target="_blank" rel="nofollow" href="https://experienceleague.adobe.com/en/docs/advertising/search-social-commerce/tracking/conversion-tracking/conversion-tracking-analytics">tracking division</a> within their ad division."</p>
<p>Others include telecom and digital marketing companies like The Trade Desk and Verizon, plus tech giants Oracle, Microsoft, and Amazon, according to Friedman. Then there's also analytics firms including Hotjar and data brokers such as Acxiom.</p>

        

<p>"And two thirds of hospital websites had some kind of data transfer to a third-party domain that we couldn't even identify," he added.</p>
<p>Of the 71 hospital website privacy policies that the team found, 69 addressed the types of user information that was collected. The most common were IP addresses (80 percent), web browser name and version (75 percent), pages visited on the website (73 percent), and the website from which the user arrived (73 percent).</p>
<p>Only 56 percent of these policies identified the third-party companies receiving user information.</p>
<p>While this puts hospital website visitors at risk of having their data collected and shared with others that they may not want, it also poses a risk to the hospitals themselves, the researchers noted.</p>
<p>Hospitals aren't legally required to publish website privacy policies that detail how they collect visitors' data and with whom they share it. But if they do have a privacy policy, they better make sure their processes on deleting personal information upon request, for example, follow the government polices – or they could face the <a target="_blank" href="https://www.theregister.com/2024/01/15/infosec_in_brief/">wrath</a> of regulators like the Federal Trade Commission.</p>
<ul>

<li><a href="https://www.theregister.com/2024/04/03/city_of_hope_data_theft/">Nearly 1M medical records feared stolen from City of Hope cancer centers</a></li>

<li><a href="https://www.theregister.com/2024/03/26/aixcc_healthcare/">Ransomware can mean life or death at hospitals. DEF CON hackers to the rescue?</a></li>

<li><a href="https://www.theregister.com/2024/03/14/change_healthcare_ransomware_investigation/">US to probe Change Healthcare's data protection standards as lawsuits mount</a></li>

<li><a href="https://www.theregister.com/2024/02/05/us_voluntary_cybersecurity_goals_hospitals/">Ignore Uncle Sam's 'voluntary' cybersecurity goals for hospitals at your peril</a></li>
</ul>
<p>"Websites that collect specific categories of information from certain users may also be subject to other federal and state-specific requirements in terms of data collection and notice," the paper warns.</p>
<p>"While the suit against Mass General Brigham and the Dana Farber Cancer Institute was brought under Massachusetts law, plaintiffs have brought similar class action lawsuits in multiple states."</p>
<p>Mass General Brigham ended up paying an <a target="_blank" rel="nofollow" href="https://www.hipaajournal.com/mass-general-brigham-settles-cookies-without-consent-lawsuit-for-18-4-million/">$18.4 million settlement</a> to resolve a class action lawsuit that alleged the institutions shared personally identifiable information about patients to Facebook, Google, and other companies.</p>
<h3>A fundamental rethink</h3>
<p>Of course, the data privacy threat extends beyond hospital websites, as&nbsp; Friedman is quick to point out.</p>
<p>"Why do hospitals have tracking on their webpages?" he wondered. "It's not that they're taking kickbacks from Google and Acxiom, data brokers and advertisers and social media companies that sell their patients' data in exchange for money.</p>
<p>"They're doing it because this stuff is ubiquitous across the whole web. They're doing it because there's an entire tens of billions of dollars ad economy."</p>
<p>While it presents a major challenge for healthcare providers in general and hospitals specifically, it's also an opportunity.</p>
<p>"Many hospitals are academic hospitals and have computer science departments that they could collaborate with, and design new tools and startups, which is something universities are good at doing," Friedman noted. "Build a new web that doesn't involve as much tracking.</p>
<p>But in the meantime, and in lieu of any <a target="_blank" href="https://www.theregister.com/2024/04/09/us_federal_privacy_law_apra/">federal data privacy law</a> in the US, protecting personal information falls to the individual. And for that,&nbsp; Friedman recommends browser-based tools Ghostery and Privacy Badger, which identify and block transfers to third-party domains.</p>
<p>"It impacts your browsing experience almost none," he explained. "It's free. And you will be shocked at how much tracking is actually happening, and how much data is actually flowing to third parties." ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Lost Faith in Kagi (484 pts)]]></title>
            <link>https://d-shoot.net/kagi.html</link>
            <guid>40011314</guid>
            <pubDate>Fri, 12 Apr 2024 11:17:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://d-shoot.net/kagi.html">https://d-shoot.net/kagi.html</a>, See on <a href="https://news.ycombinator.com/item?id=40011314">Hacker News</a></p>
<div id="readability-page-1" class="page">
		

		<p>I've posted many times before about paid search engine Kagi but I really wanted, like I have with other sites in the past, to make a post that just outlines why I don't have any faith in it.  Because the majority of my reasons come from being in their Discord, which tons of average users will never really look at, and I suspect a lot of Kagi supporters do not know a lot of this backstory.  This isn't a heavily researched deep dive or anything, I feel like I always need these kind of disclaimers when I'm just posting my thoughts about something backed up with my reasons for thinking it...it's just a quick rundown of why I personally went from a Kagi subscriber to a Kagi Naysayer, so that I can have my reasons in one place that I can link others to.  And I have seen a lot of people recommend Kagi that I personally know would be a lot less interested in doing so if they knew this information, but they don't because it's not readily apparent just from using the site.</p>

		<p>For those unfamiliar to start with, Kagi is a paid search engine that purportedly focuses on privacy.  It's meant to be a no-nonsense platform focused on being the best search it can be.  Frankly, I think at this point it's a very nonsense-focused platform that has an extremely disinterested view of privacy, and I think it's not a particularly sustainable project either.</p>

		<p>First of all, as a project, Kagi stretches itself way too thin.  "Kagi" isn't just Kagi Search, it's also a whole slew of AI tools, a Mac-only web browser called Orion, and right now they are planning on launching an email service as well.  None of these projects are particularly profitable, so it's not a case of one subsidizing the other, and when they announced Kagi Email even their most dedicated userbase (aka the types who hang around in a discord for a search engine) seemed largely disinterested.  For products this niche, I don't think you can afford to be running multiple niche products at once.  Your developers are going to be running far too ragged to keep them all going.  Last I could find Kagi employs about 16 employees, half full time.  Presumably not every single one of those 8 or so full timers are web devs.  That's not a lot of people to throw at this many things.</p>

		<p><a href="https://blog.kagi.com/celebrating-20k">Oh and they own a t-shirt factory.</a></p>

		<p>You see, when Kagi had a funding round and raised about $670k.  This funding came from an investment round including "42 accredited investors, most of whom are actual Kagi users".  Unfortunately unlike Cohost I can't give any kind of huge financial breakdown here, because Kagi's finances are even less transparent.  If there are Kagi financial reports, I haven't seen them, just the occasional Discord comment from Kagi employees.  A while back they raised their prices, which lost them a lot of subscribers, because they were losing money per search at the old prices.  They were actually still losing money per search on the new prices.  They eventually lowered the prices back down a bit (and maybe raised them again?  I've completely lost the plot on their pricing at this point) and have claimed that at 25,000 users they would be breaking even.  I am unsure what happened in all of this for this math to be true, but my point is that they did in fact reach at least 20,000 users, and to celebrate they set up a business entity in Germany (they are currently US based), in order to start a tiny little t-shirt printing company.  And their goal was to print 20,000 t-shirts to give out, FOR FREE, to their first 20,000 users (with users paying only shipping costs).  But I cannot stress enough, they did not just spend money on 20,000 tshirts to give out, they set up a whole new business entity in Germany to run their own t-shirt printing operation, with its own building and warehouse and employee(s? I get the sense it's one guy but I don't know).  And this cost them 1/3 of their $670k funding round.  One, fucking, third.  For t-shirts.  Did I mention that the t-shirts don't even have the Kagi name on them?  Just the Kagi dog mascot, who is at this point the only thing I like about Kagi, to the point where if I wasn't worried people would try to talk to me about Kagi I'd opt for one of the shirts myself.  Great artist, whoever did this.  Terrible financial choice to start a whole t-shirt business to make 20,000 free t-shirts that do not even get your name out there.</p>

		<img src="https://d-shoot.net/img/kagi/breakeven.png" alt="Discord convo, quoting @tkataja saying 'of course I don't know about the financial situation of the company now and then, but', and Vlad replies on 01/28/2024 'we are almost breakeven, at 25k subs we should be'"> <br>
		<img src="https://d-shoot.net/img/kagi/salaries.png" alt="Discord convo, quoting @HKayn saying 'Is Kagi actually operating at a loss atm?', dev Thomas replies on 09/29/2023 'So we price our plans for them to be slightly profitable on a revenue - cost per search basis, but for now there aren't enough of you for that to fully offset R&amp;D expenses (eg. salaries, etc...).  You can help by getting your friends on :)"> <br>
		<img src="https://d-shoot.net/img/kagi/employees.png" alt="Discord convo on 05/23/2023.  Vlad: 'we are not sustainable yet but we are able to pay search and infrastructure cost after revenue from subscriptions.  Since there is a surplus there it is a matter of scaling the number of users now to also pay for the salaries.  That should happen around 20k customers.'  Grooty: 'how many employees do you currently have?' Raymyn: 'Thanks for the response.  Definitely looking forward to the future of Kagi. You guys are definitely the first product I have used in awhile that I am more than willing to throw money at a subscription for.  Hoping you guys hit the 20k mark soon, and grow to 100k, 1 million, or more in the near future!'  Vlad, replying to Grooty: '~16, roughly half of them are full time'"> 

		<p>T-shirt companies aside, there's one other thing about Kagi's finances that was revealed recently in an update stream they did that caught my attention--Kagi was not paying sales tax for two years and they finally have to pay up.  They just...didn't do it.  Didn't think it was important?  I have no idea why.  Their reactions made it sound like they owed previous taxes, not that they just now had to pay them.  They genuinely made it sound like they only just now realized they needed to figure out sales tax.  It's a baffling thing to me and it meant a change in prices for users that some people were not thrilled with.</p>

		<img src="https://d-shoot.net/img/kagi/kagisalestax.png" alt="Kagi slide that says 'Also, sales tax/vat...' with a graph of the world, plus sign, and graph of an unreadable graph, with an equal sign saying 'we need to pay sales taxes'.  Then 'Implementing now - updates in the next few weeks, expect changes in January"> <br>
		<img src="https://d-shoot.net/img/kagi/taxes.png" alt="Discord screenshot from 03/19/2024, Vlad says 'To clarify, this means an end-price increase for affected members (sales tax/VAT will be automatically added on top of Kagi price, if applicable in their country/state) and this is mandated by Kagi becoming large enough to have legal sales tax/VAT obligation.  In addition, Kagi will have to retroactively pay for all sales tax/VAT that we did not collect in the last almost two years.  We have chosen to absorb this on behalf of our customers.'"> 

		<p>But let's say that you can live with their financial issues.  They'll either survive or they won't and you'll just enjoy the ride.  Yet, there's a lot of reasons that I can't just enjoy the ride either.  I think the ride is going to get worse and worse in ways the casual users haven't really noticed yet because of how much of it is in beta or just Ideas That Vlad (the founder of Kagi) Had.  And most of them are surrounding AI.  I have to assume a lot of people don't understand how deep the AI rabbithole for Kagi goes, because I have seen people recommend Kagi to people frustrated with Google's own AI bullshit.  If AI is the thing you are trying to get away from, moving to Kagi is a lateral move at best.</p>

		<p>As it turns out, Kagi was <a href="https://blog.kagi.com/kagi-ai-search">founded originally as an AI company</a>, who later pivoted to search.  And going by their comments in their Discord, AI tools seem to be what they spend most of their time on these days.  They're launching AI features left and right, and they have fully bought into AI being the future of search.  They believe that by embracing AI, that will be the thing that sets their product apart (kinda late now) and get them the kind of userbase that can keep them afloat financially.  They have <a href="https://kagi.com/fastgpt">"FastGPT"</a>, where their focus is having a ChatGPT style service that is focused on being fast, not accurate.  And boy, it sure isn't, I messed around with this for a while and it very confidently gave me a lot of extremely inaccurate information about old sitcoms.  But of course, it's all stuff where if you didn't already know the answer to the question you asked you wouldn't know it was wrong--like when I asked it about the All in the Family episode Cousin Liz, it kept identifying the woman Edith and Archie are talking to in the episode as Cousin Liz, but Cousin Liz is instead Edith's deceased cousin who they were attending a funeral for.  Following a theme I asked it more broadly about homosexuality in All in the Family and it spit out a bunch of text repeatedly saying that Lionel Jefferson was gay.  But I guess it did spit all of this wrong information at me faster than some of the alternatives!  I wrote more about this experience on <a href="https://hackers.town/@lori/110420844338899624">fedi</a>, even though the content warning says ChatGPT this was done with Kagi's FastGPT, I was just trying to keep the thread simple at the time.</p>

		<p>Like most search now Kagi has chosen to include Instant Answers that are AI generated, which means they're often wrong, as well as a <a href="https://kagi.com/summarizer/index.html">"Universal Summarizer"</a> tool, that again is more of the same old AI bullshit.  There's also a beta tool called Kagi Assitant, which I...don't know what's really different about it than the other AI stuff they're doing, I think it has a chatbot mode?  I believe you have to be a subscriber to see this.  It's getting increasingly hard to tell some of these services apart.  There's also another <a href="https://sidekick.kagi.com/">beta feature called Sidekick</a> that puts Kagi AI stuff as a sidebar on your own website.  There was some demo where you could put someone's Twitter handle in and it would give you a summary of who that person was (nightmare shit).  But the developers of Kagi fully believe that this is what search engines should be, a bunch of AI tools so that you don't even need to read primary sources anymore.  If AI is your problem with Google or Bing, Kagi is in no way a solution for you.  Kagi loves AI bullshit and they are going to find more and more ways to use it.  If you check out their Discord and listen to founder Vlad talk about AI tools, it's clear that he will not listen to anyone saying they might be bad in any way.  To the point where he truly believes that <a href="https://twitter.com/vladquant/status/1647017761569402882">AI should be used to remove bias from news articles</a>, and show you which articles are "constructive" or "good" to view.  This is something he legitimately wants to implement and he seems completely oblivious to the fact that this is not something AI can do, that AI spits out exactly as much bias as it is fed in its model.  He's 100% a true believer in AI as unbiased. (Note in the below screenshots: freediver is Vlad's HackerNews account)</p>

		<img src="https://d-shoot.net/img/kagi/personsummary.png" alt="Discord screenshot from 12/05/2022, Vlad says 'now something like this is super useful...we can make the AI read through 50 web pages in mere seconds, after doing 5 searches (that it decides on!), to at the end produce a summary of someone based on just a twitter handle.  Total cost for this is maybe 6-7 cents but it just saved me 10 minutes?' Vlad includes a screenshot of the search for a user named @scottleibrand, which shows a ChatGPT-style generated summary with links to the sites it used, the text is a bit too small for me to transcribe."> <br>

		<img src="https://d-shoot.net/img/kagi/unbiasedreviews.png" alt="Hackernews screenshot, malikNF says ''> Kagi shopping results have no affiliate links and feature the most helpful, unbiased reviews' Wonder how they determine if a review is unbiased or not.  Any attempt at automating this process means someone else will figure a way to game the system over time.' Vlad replies (as freediver) 'Which is why we cannot say how, but please check and let us know if the reviews sound unbiased :)'"> <br>

		<img src="https://d-shoot.net/img/kagi/constructivenews.png" alt="Discord convo from 12/05/23.  Vlad: 'Real balanced news would not discriminate between left and right, but between constructive and destructive.''  ram ram: 'it just so happens the left/right divide correlates strongly with true/false and constructive destructive' the last envoy quoting ram ram: 'I agree' Vlad: 'Interesting, that is implicitly saying that one political option is the source of truth' Ram ram: 'Not at all, it's explicitly saying that one political option is supported by truth.' Vlad: 'but news should not be only about politics?' Ram ram: 'All things are political in nature.' Vlad: 'A coffee shop opening in your town?' Ram ram: 'Business licensing, source of beans, employment practices, building regulation, city ordonances, taxes, food health and safety'"> <br>

		<img src="https://d-shoot.net/img/kagi/constructivenews2.png" alt="Discord convo from 12/05/23.  Vlad: 'line of thought basically comes from here https://www.theguardian.com/media/2013/apr/12/news-is-bad-rolf-dobelli, embed for the Guardian article 'News is bad for you - and giving up reading it will make you happier. News is bad for you, it leads to fear and aggression. It hinders your creativity and makes you sick.  We should stop consuming it, says Rolf Dobelli, who's abstained for years.'  Vlad continues: 'then you ask a question how does a product that makes news constructive to your life (vs irrelevant/destructive)'"> 

		<p>It's honestly impossible to write this without discussing Vlad a little bit.  Vlad is very "my way or the highway", but is the type that will try to appear very measured and calm while completely unwilling to budge.  And he is very, very much the type that believes "not everything is political" and "we don't get into politics".  I won't get into all of the Brave stuff because many people have written about this already, but the <a href="https://kagifeedback.org/d/2808-reconsider-your-partnership-with-brave">support thread</a> about it should give you some ideas.  His personal conception of bias is a guiding factor in a lot of Kagi's decisions but it's frankly ridiculous.  For example, he has stated before that he thinks 3 star reviews on products are "by definition" unbiased, because they must include good and bad points.  Nevermind that a lot of people's reviews of the recent Star Wars films were "good space war stuff but too many minorities in it".  At one point someone <a href="https://kagifeedback.org/d/865-suicide-results-should-probably-have-a-dont-do-that-widget-like-google">suggested the idea that searching for suicide-related terms should bring up a helpline</a>, and he rejected that idea because it would be "biased" (I guess towards not wanting people to kill themselves).  But at the same time, Kagi partners with a service called <a href="https://www.looria.com/">Looria</a> to provide "unbiased reviews" on products in Kagi's shopping page.  Nevermind that unbiased reviews do not exist (there is just a difference between a sincere review and a paid advertisement), but isn't promoting certain products in search at least as biased as telling people to not commit suicide?  You're letting a third party decide what reviews your users should see.</p>

		<p>And Vlad's attitude is also where Kagi's dedication to privacy falls apart for me.  Generally, if someone brings up a security or privacy concern, Vlad's response is either "trust me bro" or "that's not actually important".  He has repeatedly stated that he feels less than 100 people on earth need full anonymity in a search engine (he has never, that I could find, explained where he got this number or idea from).  He believes that email addresses don't count as personally identifiable information, because you can simply use a burner account.  If you say that you wouldn't want Kagi using information from your theoretical Kagi Email Address in your search results, and would rather have a Proton-style privacy focused email?  He says that there's nothing to worry about, Kagi wouldn't do anything bad with your data.  If you bring up "what if Kagi gets sold to someone else?"  He says well, if they sold to someone who did something bad with your data, they'd lose all of their privacy focused customers, so clearly they'd never do that.  Basically anything where you say "I don't want someone to have this data about what I'm doing in a search engine", his reply is "well, we wouldn't do anything with this information."  A lot of questions about what information Kagi collects on people is met with either saying nothing (which isn't true, they connect your account to an email address for payments, since it's a paid service), or saying he isn't sure, or saying it doesn't matter because they won't use it anyway.  Asking what data Stripe collects on them through Kagi, and more importantly what data Stripe sends back TO Kagi, also gets you a vague "I don't know" answer.  He doesn't entertain any discussions about GDPR because he thinks they have nothing that applies anyway.  Questions about what would happen if the government tried to force him to collect information about users are just brushed away with "well we'd simply close the company", although he also notes that he has no problem with criminals being caught through their searches and doesn't want criminals using the platform.</p>

		<img src="https://d-shoot.net/img/kagi/weregood1.png" alt="Discord convo from 02/15/2024, Zack: 'That's not actually the only thing I'm asking.  Was also curious about more of the intricacies.  I would use stripe, I wouldn't opt for bitcoin anyway.'  Vlad: 'the point is we do not really want, need or care about your data.  We just need $10/mo from you to run the service.  How you provide it to use is a technical detail as far as we're concerned.  Zack:  I know that you don't want/need/care.  I was more curious about the actual reality when using stripe, that's all.  I will delve more into the stripe policies another time though!"><br>
		<img src="https://d-shoot.net/img/kagi/weregood2.png" alt="Discord convo.  Heero, replying to Vlad asking 'what is missing?': 'GDPR Data download link. Where to get the data that is collected.' Heero replying to their own post saying 'Something you could do about this false time stamps?' with 'But this is more important for me, cry laughing emoji, it's just annoying when sites cheat like that' Vlad: Kagi does not collect any personal information' Kai: eh, you technically need to provide a download link to a CSV with the email address' Vlad: our payment processor does, and you can ask stripe for that'"><br>
		<img src="https://d-shoot.net/img/kagi/weregood3.png" alt="Discord convo, Heero replying to Vlad saying 'kagi does not collect any personal information: 'You have an account, with mail and billing. This is already personal information in the sense of GDPR, winking emoji.  Heero replying to vlad saying 'Well even email address is not personal information the way kagi does it as you can enter anything', Heero says 'That's actually I think not important' Heero replies to Kai saying 'GDPR is stupid that way' with 'Yes, I agree'.  Vlad replies 'personal information is what you can be identified with as an individual.  No information you submit to kagi is personal information except if you use your real email address to register. So a data download link would download a file that contains your email address.  Heero replies 'Yes, and then you need to provide the data associated with you.  That includes personal preference settings for search optimization I would think, no expert on this though.'"><br>
		<img src="https://d-shoot.net/img/kagi/weregood4.png" alt="Discord convo, Heero replying to Vlad asking 'can you clarify what is sensitive?': 'Sensitive data is private information that, if exposed without permission, can lead to harm or misuse.  Inputting websites that should be preferred or not preferred is highly sensitive.  That shows a bit of a disconnect between data protection and you.  Though I really don't want to blame you, could be more that I actually live in germany and we take such stuff far to serious :D' Vlad: We do not collect/extract this information but the user volunteers it.  As such it is not clear how is it sensitive in terms of privacy protection.' Heero: The thread model here would be something like that: I prefer websites that are pro trans or anti trans, you see the profile and the settings i have set, you see the mail too.'"><br>
		<img src="https://d-shoot.net/img/kagi/weregood5.png" alt="Discord convo, Heero: your not really following GDPR standards.  You don't understand what sensitive data means (to be fair, I don't know if you really do :) ). Even if I can input wrong data, you can't verify that, as such you need to always take it as sensitive, personal, private data.  Also, giving you data volunteerly isnt a thing in GDPR, the source of the data is not important.  So, my concern is, I don't know what you really track, as you don't give me a copy of my data.  I don't know if your obligated to, it's just things I as european citizen expect.' Vlad: (quoting 'your not really following DPR standards') You say these things but you do not really provide evidence for it :) So again, can you clearly articulate what your concern is?  We very well know what personal information is and what GDPR was made for. (quoting 'i don't know what you really track') We have very detailed privacy policy where this is literally written.  Have you read it and still do not know what we track?'  Heero: 'Did I make it clearer know? I really want to stress that I don't have anything against you or kagi :) just trying to be constructive.'"><br>

		<img src="https://d-shoot.net/img/kagi/readingemails.png" alt="Discord convo from 03/31/2024, Gohan: Just the thought of kagi search having the ability to go through my emails would keep me far away from using the email product'  Vlad: 'as I said, it would be opt in, we will need to be able to search email as a product feature anyway' Gohan: That's why I said even having the ability to do so is a massive turn off' Vlad: exposing it in search results is a benefit, not sure why you would think that?' Gohan: 'because if you give any company an inch, they'll take a mile', Vlad: 'not Kagi, I think you may be misunderstanding how would this work and what incentives in play'"><br>

		<img src="https://d-shoot.net/img/kagi/anonymity.png" alt="Discord convo from 07/15/22, Vlad: people who really need anonymity are very rare.  Probably less than a 100 in the entire world.  Definitely not typical Kagi users.  Unless they are criminals, in which case we don't care they don't have full anonymity (nor we want them as customers)'">

		<p><i>I want to note with the above, I'm not a GDPR expert either, I don't know what counts legally and what doesn't, but I strongly disagree with Vlad's handwaving of the issue and his insistence that email addresses aren't PII because you can use a fake one, because that's true for names and almost any other information as well, that logic doesn't hold up for this, and the logic is what I have the issue with.</i></p>		

		<p>Between the absolute blase attitude towards privacy, the 100% dedication to AI being the future of search, and the completely misguided use of the company's limited funds, I honestly can't see Kagi as something I could ever recommend to people.  Is the search good?  I mean...it's not really much better than any other search, it heavily leverages Bing like DDG and the other indie search platforms do, the only real killer feature it has to me is the ability to block domains from your results, which I can currently only do in other search engines via a user script that doesn't help me on mobile.  But what good is filtering out all of the AI generated spamblogs on a search platform that wants to spit more AI generated bullshit at me directly?  Sure I can turn it off, but who's to say that they won't start using my data to fuel their own LLM?  They already have an extremely skewed idea of what counts as PII or not.  They could easily see using people's searches as being "anonymized" and decide they're fine to use, because their primary business isn't search, it's AI.  They just don't want to admit to being an AI company anymore.  Frankly, it's not something I want to pay them to keep developing.  It's something I want less of out in the world.  Do you need to quit using Kagi?  That's up to you.  I'm not really trying to debate anyone into leaving Kagi.  My only interest is to explain why my opinion shifted on it, and to share information that may or may not shift your opinions.  If they don't, it's not something I want to debate people into.  But I think most of the info here is going to be news to a lot of people, and that's the thing.  Most people aren't going to dig into the discord for a project and read what the developers have said about it over time, that's a bonkers thing to do that I did, just do what you will with my findings.  Because I know for a fact, from talking to people, that a lot of Kagi users don't know any of this.</p>
		<hr>

		<a href="https://d-shoot.net/">=&gt; Return to Home</a>
	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why I recommend Renovate over any other dependency update tools (121 pts)]]></title>
            <link>https://www.jvt.me/posts/2024/04/12/use-renovate/</link>
            <guid>40011111</guid>
            <pubDate>Fri, 12 Apr 2024 10:35:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jvt.me/posts/2024/04/12/use-renovate/">https://www.jvt.me/posts/2024/04/12/use-renovate/</a>, See on <a href="https://news.ycombinator.com/item?id=40011111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>If you've read my blog before, or interacted with me at work or in the Open Source world, you're likely to know that I'm a huge fan of <a href="https://docs.renovatebot.com/">Renovate</a>.</p><p>For those that aren't aware, Renovate is one of the big players in dependency updating tooling, commonly seen in comparisons with Dependabot or Snyk.</p><p>I've been using Renovate for the last ~5 years, alongside a mix of Dependabot and Snyk to keep me grounded. I <em>absolutely love</em> Renovate, and make a point of making it so I can run Renovate where possible.</p><p>I've also had the experience operating Renovate in self-hosted mode as well as used <a href="https://developer.mend.io/">the hosted Renovate app by Mend</a> and the Mend Enterprise SAAS, and <a href="https://gitlab.com/tanna.dev/jvt.me/-/issues/1349">plan to</a> blog about the lessons learned self-hosting Renovate.</p><p>Renovate has some really key features that set it apart from the competition, and I'm sure in the time it's taken me to write this, they've shipped some new ones 😻</p><p>(Aside: I'm largely writing this blog post now, as I've recently been shouting the benefits of Renovate, and instead of writing another internal-only document at work (as I did at Deliveroo), I wanted to <a href="https://www.jvt.me/posts/2017/06/25/blogumentation/">write it as a form of blogumentation</a>).</p><h2 id="prior-art">Prior art</h2><p>A few years back, I wrote about <a href="https://www.jvt.me/posts/2021/09/26/whitesource-renovate-tips/">some tips for using Renovate</a> to make keeping your software up-to-date easier, which I still stand by.</p><p>Since then, I've worked across a number of different ecosystems, repository sizes, and levels of comfort merging dependency updates, and have learned a few more things about effectively using Renovate - but through it, I'm still very sure of Renovate being the best tool in the ecosystem.</p><h2 id="configurability">Configurability</h2><p>Renovate is <em>extremely</em> configurable, with <a href="https://docs.renovatebot.com/configuration-options/">dozens of configuration options</a> to tune your experience. But Renovate doesn't end up being "too" configurable, where you end up spending more time tweaking config than doing the changes, but configurable enough that it's very likely you can do what you need to with it.</p><p>This is something that really hit me when we were rolling out Dependabot at Deliveroo, where my team owned ~30 repositories, and so for each of these repos, we needed to hand-craft a <code>dependabot.yml</code>, as Dependabot needs to be told which directories contain which dependencies, and how often to update them.</p><p>After we'd had about a week of usage, we started needing to tweak the configuration in a few of them to reduce the noise, which then required us to raise PRs across all the repos and get them updated.</p><p>Because Dependabot requires a bit of a snowflake configuration per repository, this wasn't easily automatable, even with <a href="https://www.jvt.me/posts/2023/01/21/bulk-git-repo-changes/">great tools available to automate some of the bulk updates</a>, which made this a rather onerous and frustrating process.</p><p>Compare this to Renovate, where there's an excellent first-class support for <a href="https://docs.renovatebot.com/config-presets/">shareable config presets</a>, in which you can "extend" multiple configuration(s).</p><p>This allows a team that wants to have consistency (i.e. in how often they receive updates to the AWS and Google Cloud SDKs, or which labels they want on PRs) to create a shared preset for their team that defines this. Then, each of their repositories can "extend" this configuration, as well as defining their own configuration on top of it at a repo-specific level.</p><p>This also makes it possible to provide good guardrails for your organisation, providing a good set of defaults, such as a <code>base.json</code>:</p><div><pre tabindex="0"><code data-lang="json"><span><span><span>{</span>
</span></span><span><span>  <span>"$schema"</span><span>:</span> <span>"https://docs.renovatebot.com/renovate-schema.json"</span><span>,</span>
</span></span><span><span>  <span>"postUpdateOptions"</span><span>:</span> <span>[</span>
</span></span><span><span>    <span>"gomodTidy"</span><span>,</span>
</span></span><span><span>    <span>"gomodUpdateImportPaths"</span>
</span></span><span><span>  <span>],</span>
</span></span><span><span>  <span>"regexManagers"</span><span>:</span> <span>[</span>
</span></span><span><span>    <span>{</span>
</span></span><span><span>      <span>"fileMatch"</span><span>:</span> <span>[</span>
</span></span><span><span>        <span>"^Makefile$"</span>
</span></span><span><span>      <span>],</span>
</span></span><span><span>      <span>"matchStrings"</span><span>:</span> <span>[</span>
</span></span><span><span>        <span>"curl .*https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- .* (?&lt;currentValue&gt;.*?)\\n"</span>
</span></span><span><span>      <span>],</span>
</span></span><span><span>      <span>"depNameTemplate"</span><span>:</span> <span>"github.com/golangci/golangci-lint"</span><span>,</span>
</span></span><span><span>      <span>"datasourceTemplate"</span><span>:</span> <span>"go"</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>  <span>]</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>This then allows defining a somewhat opinionated good starting point for teams with a <code>default.json</code>:</p><div><pre tabindex="0"><code data-lang="json"><span><span><span>{</span>
</span></span><span><span>  <span>"$schema"</span><span>:</span> <span>"https://docs.renovatebot.com/renovate-schema.json"</span><span>,</span>
</span></span><span><span>  <span>"extends"</span><span>:</span> <span>[</span>
</span></span><span><span>    <span>"local&gt;your-org-here/renovate-config:base"</span><span>,</span>
</span></span><span><span>    <span>"config:best-practices"</span>
</span></span><span><span>  <span>]</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Then, teams only need to set the following in their repos' <code>renovate.json</code>, and they'll have the benefits of onboarding with a lot of the hard work done with starting:</p><div><pre tabindex="0"><code data-lang="json"><span><span><span>{</span>
</span></span><span><span>  <span>"$schema"</span><span>:</span> <span>"https://docs.renovatebot.com/renovate-schema.json"</span><span>,</span>
</span></span><span><span>  <span>"extends"</span><span>:</span> <span>[</span>
</span></span><span><span>    <span>"local&gt;your-org-here/renovate-config"</span>
</span></span><span><span>  <span>]</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>And that's it 👏</p><h2 id="good-defaults">Good defaults</h2><p>Linking back to the comment about having to hand-craft <code>dependabot.yml</code>s, the great thing about Renovate is that there's some great defaults, and that it can autodetect the ecosystems your repo uses, and appropriately raises PRs.</p><p>This ease of onboarding is truly excellent, and you can even have <a href="https://docs.renovatebot.com/getting-started/installing-onboarding/#repository-onboarding">an onboarding PR</a> raised to your repos to make it even simpler.</p><p>For instance, at Deliveroo, we made it so <a href="https://www.jvt.me/posts/2023/01/30/renovate-global-defaults/">there was a default set of configuration for all repos</a> to give us an easy means to update Docker images, but then if teams wanted to manage everything, they could add a <code>renovate.json</code>, and it'd be more fully featured.</p><p>Additionally, Renovate comes with some great inbuilt configuration in the form of presets, including <a href="https://docs.renovatebot.com/upgrade-best-practices/">a "best practices" guide</a> and associated preset, which makes it easier to keep on top of community best practices, without needing to bikeshed about what you think is best.</p><p>If you find that <code>config:best-practices</code> is a little too much, there's <code>config:recommended</code> as a starting point, and you can always downgrade or exclude rules you'd prefer not to follow. Or if you <em>really</em> want to control things <code>config:base</code> is the minimum you should pull in.</p><h2 id="grouping">Grouping</h2><p>As suggested above, it's possible to tune how different packages get updated, where you can <a href="https://docs.renovatebot.com/noise-reduction/#package-grouping">group multiple updates into a single PR</a>.</p><p>For instance, let's say that you use 9 different AWS services in your application, and instead of receiving 9 PRs every time there are updates across the SDKs, you want a single one. In this case, you could craft the following Renovate configuration:</p><div><pre tabindex="0"><code data-lang="json"><span><span><span>{</span>
</span></span><span><span>  <span>"$schema"</span><span>:</span> <span>"https://docs.renovatebot.com/renovate-schema.json"</span><span>,</span>
</span></span><span><span>  <span>"packageRules"</span><span>:</span> <span>[</span>
</span></span><span><span>    <span>{</span>
</span></span><span><span>      <span>"groupName"</span><span>:</span> <span>"aws-sdk-go"</span><span>,</span>
</span></span><span><span>      <span>"matchPackagePatterns"</span><span>:</span> <span>[</span>
</span></span><span><span>	<span>"^github.com/aws/aws-sdk-go-v2"</span>
</span></span><span><span>      <span>]</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>  <span>]</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>What's great about this is that you can also bundle things like all patch updates into a single PR:</p><div><pre tabindex="0"><code data-lang="json"><span><span><span>{</span>
</span></span><span><span>  <span>"packageRules"</span><span>:</span> <span>[</span>
</span></span><span><span>    <span>{</span>
</span></span><span><span>      <span>"matchPackagePatterns"</span><span>:</span> <span>[</span>
</span></span><span><span>        <span>"*"</span>
</span></span><span><span>      <span>],</span>
</span></span><span><span>      <span>"matchUpdateTypes"</span><span>:</span> <span>[</span>
</span></span><span><span>        <span>"patch"</span>
</span></span><span><span>      <span>],</span>
</span></span><span><span>      <span>"groupName"</span><span>:</span> <span>"all patch dependencies"</span><span>,</span>
</span></span><span><span>      <span>"groupSlug"</span><span>:</span> <span>"all-patch"</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>  <span>]</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>This is something that's <a href="https://github.blog/2023-08-24-a-faster-way-to-manage-version-updates-with-dependabot/">recently</a> been added to Dependabot, which is great, cause Renovate's had it for years 😝</p><h2 id="use-for-one-off-bumps">Use for one-off bumps</h2><p>Because Renovate is an Open Source tool you can run on the command-line, it means you can <em>also</em> get the ability to use <a href="https://www.jvt.me/posts/2022/12/12/renovate-one-off/">Renovate for one-off executions</a>, for instance to get everyone in your organisation to a minimum version of a given dependency, or just to do an infrequently performed set of updates.</p><p>Something I always refer back to is the fact that Renovate has a <em>tonne</em> of supported package managers, package ecosystems and versioning tools.</p><p>Renovate's <a href="https://docs.renovatebot.com/bot-comparison/">bot comparison guide docs</a> has a good example which links out to the differences between:</p><ul><li><a href="https://docs.renovatebot.com/modules/manager/">What platforms Renovate supports</a></li><li><a href="https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/about-dependabot-version-updates#supported-repositories-and-ecosystems">What platforms Dependabot supports</a></li></ul><p>As Dependabot is more focussed on being used on GitHub's platforms (citation needed?) support for things like CircleCI, GitLab CI or other competitors' tooling doesn't seem to be available.</p><h2 id="adding-support-for-additional-ecosystems">Adding support for additional ecosystems</h2><p>One of the things you'll be used to having from your dependency update tool of choice is your standard package manager support, where it'll update a <code>go.mod</code> or a <code>pom.xml</code> in your repositories.</p><p>But one thing that's quite important to understand is that there are <em>many more</em> dependencies in your project than those installed in your package manager. Something I find great about Renovate is that, as well as managing <code>Dockerfile</code>s, <code>build.gradle</code>s, <code>.gitlab-ci.yml</code>, etc, it will <em>also</em> manage things your <code>.ruby-version</code> or configuration for the ASDF version manager.</p><p>But what about some of the non-standard, or organisation-specific means for tracking versions?</p><p>For instance, installing the <code>golangci-lint</code> linting tool recommends using <code>curl | sh</code>:</p><div><pre tabindex="0"><code data-lang="makefile"><span><span><span>$(GOBIN)/golangci-lint</span><span>:</span>
</span></span><span><span>	curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh <span>|</span> sh -s -- -b <span>$(</span>GOBIN<span>)</span> v1.57.2
</span></span></code></pre></div><p>And it's common for <code>Dockerfile</code>s to have a definition of the version of dependencies:</p><div><pre tabindex="0"><code data-lang="docker"><span><span><span>ARG</span> <span>MONGODB_VERSION</span><span>=</span><span>6</span>.0.4<span>
</span></span></span></code></pre></div><p>Or what if you have a custom deployment configuration like:</p><div><pre tabindex="0"><code data-lang="yaml"><span><span><span># this is an (imaginary) company specific deployment configuration file</span><span>
</span></span></span><span><span><span></span><span>application</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>lb</span><span>:</span><span>
</span></span></span><span><span><span>    </span><span>image</span><span>:</span><span> </span><span>uk-tooling/load-balancer@v3.0.0</span><span>
</span></span></span></code></pre></div><p>It's very unlikely that you have these supported by other tools out-of-the-box, and Renovate is the same.</p><p>But Renovate <em>does</em> give you the ability to manage these versions yourself.</p><p>Using Renovate's <a href="https://docs.renovatebot.com/modules/manager/regex/">custom managers</a> we can craft a configuration file such as:</p><div><pre tabindex="0"><code data-lang="json"><span><span><span>{</span>
</span></span><span><span>  <span>"$schema"</span><span>:</span> <span>"https://docs.renovatebot.com/renovate-schema.json"</span><span>,</span>
</span></span><span><span>  <span>"regexManagers"</span><span>:</span> <span>[</span>
</span></span><span><span>    <span>{</span>
</span></span><span><span>      <span>"fileMatch"</span><span>:</span> <span>[</span>
</span></span><span><span>        <span>"^Makefile$"</span>
</span></span><span><span>      <span>],</span>
</span></span><span><span>      <span>"matchStrings"</span><span>:</span> <span>[</span>
</span></span><span><span>        <span>"curl .*https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- .* (?&lt;currentValue&gt;.*?)\\n"</span>
</span></span><span><span>      <span>],</span>
</span></span><span><span>      <span>"depNameTemplate"</span><span>:</span> <span>"github.com/golangci/golangci-lint"</span><span>,</span>
</span></span><span><span>      <span>"datasourceTemplate"</span><span>:</span> <span>"go"</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>  <span>]</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>And this will allow us to manage the <code>golangci-lint</code> installation.</p><p>Ideally, this sort of configuration may make it upstream so Renovate can do it out-of-the-box, but as we can see from the above example, there may be things that are organisation- or repo-specific, and so having it upstream'd doesn't make sense.</p><h2 id="because-you-can-do-more-with-it">Because you can do more with it</h2><p>And you can do even more with it - I've mentioned you can use it for one-off updates on the command-line, but for instance I've also written a tool <a href="https://gitlab.com/tanna.dev/renovate-graph">renovate-graph</a> which takes the detected dependency data from Renovate and gives you a JSON blob you can consume.</p><p>This underpins a large swathe of the power behind <a href="https://dmd.tanna.dev/">dependency-management-data</a> and can be used for other means, such as <a href="https://www.jvt.me/posts/2023/11/03/renovate-to-sbom/">converting Renovate data exports to a Software Bill of Materials (SBOM)</a>.</p><h2 id="dependency-dashboard">Dependency Dashboard</h2><p>One thing I love about Renovate is that you can enable the <a href="https://docs.renovatebot.com/key-concepts/dashboard/">Dependency Dashboard</a>, which gives you an overview of the detected dependencies, open PRs, as well s anything that may be waiting, or has failed to update.</p><p>This is a hugely useful insight into the at-a-glance how far behind are we on updates, giving a view of whether you maybe want to spend a bit more time focussing on updates, or looking at ways to cut through the noise.</p><p>When you merge a PR into your default branch, Renovate will rebase open PRs, so they're easier to review, and are guaranteed to run against the latest changes. However, if you're not getting to your updates as often as changes are going in, you may have a tonne of PRs constantly building, which is a waste of energy and CI minutes.</p><p>Instead, you can use the Dependency Dashboard for its ability to i.e. <a href="https://docs.renovatebot.com/key-concepts/dashboard/#require-approval-for-major-updates">require major bumps be gated behind a manual approval</a>, as it's likely you'll need some human interaction for that PR, and can then only raise it when you're actually ready to deal with it.</p><h2 id="open-source">Open Source</h2><p>A very important factor to me is that Renovate is Open Source, and open to community contributions.</p><p>Although it's probably best to be split into another article, I love the AGPL3, and it's a great way of making sure that anyone hosting Renovate as a platform makes sure that their users get the access to the source code.</p><p>Alternatively, Snyk is proprietary, and Dependabot is source available (or at a stretch "open source" not "Open Source") with <a href="https://github.com/dependabot/dependabot-core?tab=readme-ov-file#license">a license</a> that doesn't even appear on the SPDX license list.</p><p>Renovate is also brilliantly set up as a community project, where they're shipping hundreds of PRs a month, alongside managing the community really well. I've seen a few things change in the last few years I've been more actively contributing, and it's a really well run project and an indication of something I'd love to be able to replicate at some point!</p><p>It also helps that <a href="https://mend.io/">Mend</a>, the company behind Renovate, invests a fair bit of time and money into development of the project, as well as their commercial offerings on top of it, which continue to make the project sustainable and remain Free and Open.</p><h2 id="great-documentation">Great documentation</h2><p>Following on from the excellent community and maintainer contributions alike, there's also some really excellent work on a technical writing and documentation point of view, which makes a lot of tasks straightforward to solve.</p><p>If there's something a little more complex or custom than the docs can offer, it's usually something that can be answered by the community in a GitHub Discussion, and likely could turn into a docs improvement, if necessary.</p><p>There's even a great <a href="https://docs.renovatebot.com/bot-comparison/">comparison table between Renovate and other dependency update bots</a>, as well as <a href="https://docs.renovatebot.com/upgrade-best-practices/">tips on how best to update your projects</a>.</p><p>I've recently discovered a few pages and features that I wasn't aware, just by going through the docs.</p><h2 id="overall">Overall</h2><p>In summary, there's just so much that makes Renovate a greater choice than any of the alternatives, and I'm sure I could talk about more things that make it great.</p><p>Whether you self-host it for maximum control (such as being able to access internal artifact registries), or run the hosted app for ease of operations, or just run it from the command-line once in a while, it can be hugely useful to your experience as an engineer.</p><p>I hope you'll check it out!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fast, Declarative, Reproduble and Composable Developer Environments Using Nix (103 pts)]]></title>
            <link>https://devenv.sh/</link>
            <guid>40010991</guid>
            <pubDate>Fri, 12 Apr 2024 10:08:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devenv.sh/">https://devenv.sh/</a>, See on <a href="https://news.ycombinator.com/item?id=40010991">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Run services</p>
<p>
<a href="https://devenv.sh/services">Pick from a number of community maintained
services</a> like <strong>PostgreSQL, Redis, MySQL,
RabbitMQ, WireMock, MinIO, Caddy, ElasticSearch</strong>, and <a href="https://devenv.sh/services/#supported-services">more are being added</a> each day.
</p><dl>
<p>
<dt>
Pre-configured processes
</dt>
<dd>
Services define processes that are started when the service is enabled as part of <code>devenv up</code>.
</dd>
</p>
<p>
<dt>
Configuration
</dt>
<dd>
Each service provides a number of options how to configure it and a hook to pass extra configuration,
whatever the format.
</dd></p>
<p>
<dt>
Extendable
</dt>
<dd>
Define your development processes as a service, allowing reuse and simplicity of a few lines of
configuration.
</dd>
</p>
</dl>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tinygrad: Hacked 4090 driver to enable P2P (456 pts)]]></title>
            <link>https://github.com/tinygrad/open-gpu-kernel-modules</link>
            <guid>40010819</guid>
            <pubDate>Fri, 12 Apr 2024 09:27:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tinygrad/open-gpu-kernel-modules">https://github.com/tinygrad/open-gpu-kernel-modules</a>, See on <a href="https://news.ycombinator.com/item?id=40010819">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">NVIDIA Linux Open GPU with P2P support</h2><a id="user-content-nvidia-linux-open-gpu-with-p2p-support" aria-label="Permalink: NVIDIA Linux Open GPU with P2P support" href="#nvidia-linux-open-gpu-with-p2p-support"></a></p>
<p dir="auto">This is a fork of NVIDIA's driver with P2P support added for 4090's.</p>
<p dir="auto"><code>./install.sh</code> to install if that's all you want.</p>
<p dir="auto">You may need to uninstall the driver from DKMS. Your system needs large BAR support and IOMMU off.</p>
<p dir="auto">Not sure all the cache flushes are right, please file issues on here if you find any issues.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it works</h2><a id="user-content-how-it-works" aria-label="Permalink: How it works" href="#how-it-works"></a></p>
<p dir="auto">Normally, P2P on NVIDIA cards uses MAILBOXP2P. This is some hardware interface designed to allow GPUs to transfer memory back in the days of small BAR. It is not present or disabled in hardware on the 4090s, and that's why P2P doesn't work. There <a href="https://forums.developer.nvidia.com/t/standard-nvidia-cuda-tests-fail-with-dual-rtx-4090-linux-box/233202" rel="nofollow">was a bug in early versions</a> of the driver that reported that it did work, and it was actually sending stuff on the PCIe bus. However, because the mailbox hardware wasn't present, these copies wouldn't go to the right place. You could even crash the system by doing something like <code>torch.zeros(10000,10000).cuda().to("cuda:1")</code></p>
<p dir="auto">In some 3090s and all 4090s, NVIDIA added large BAR support.</p>
<div data-snippet-clipboard-copy-content="tiny@tiny14:~$ lspci -s 01:00.0 -v
01:00.0 VGA compatible controller: NVIDIA Corporation AD102 [GeForce RTX 4090] (rev a1) (prog-if 00 [VGA controller])
        Subsystem: Micro-Star International Co., Ltd. [MSI] Device 510b
        Physical Slot: 49
        Flags: bus master, fast devsel, latency 0, IRQ 377
        Memory at b2000000 (32-bit, non-prefetchable) [size=16M]
        Memory at 28800000000 (64-bit, prefetchable) [size=32G]
        Memory at 28400000000 (64-bit, prefetchable) [size=32M]
        I/O ports at 3000 [size=128]
        Expansion ROM at b3000000 [virtual] [disabled] [size=512K]
        Capabilities: <access denied>
        Kernel driver in use: nvidia
        Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia"><pre><code>tiny@tiny14:~$ lspci -s 01:00.0 -v
01:00.0 VGA compatible controller: NVIDIA Corporation AD102 [GeForce RTX 4090] (rev a1) (prog-if 00 [VGA controller])
        Subsystem: Micro-Star International Co., Ltd. [MSI] Device 510b
        Physical Slot: 49
        Flags: bus master, fast devsel, latency 0, IRQ 377
        Memory at b2000000 (32-bit, non-prefetchable) [size=16M]
        Memory at 28800000000 (64-bit, prefetchable) [size=32G]
        Memory at 28400000000 (64-bit, prefetchable) [size=32M]
        I/O ports at 3000 [size=128]
        Expansion ROM at b3000000 [virtual] [disabled] [size=512K]
        Capabilities: &lt;access denied&gt;
        Kernel driver in use: nvidia
        Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia
</code></pre></div>
<p dir="auto">Notice how BAR1 is size 32G. In H100, they also added support for a PCIe mode that uses the BAR directly instead of the mailboxes, called BAR1P2P. So, what happens if we try to enable that on a 4090?</p>
<p dir="auto">We do this by bypassing the HAL and calling a bunch of the GH100 methods directly. Methods like <code>kbusEnableStaticBar1Mapping_GH100</code>, which maps the entire VRAM into BAR1. This mostly just works, but we had to disable the use of that region in the <code>MapAperture</code> function for some reason. Shouldn't matter.</p>
<div data-snippet-clipboard-copy-content="[ 3491.654009] NVRM: kbusEnableStaticBar1Mapping_GH100: Static bar1 mapped offset 0x0 size 0x5e9200000
[ 3491.793389] NVRM: kbusEnableStaticBar1Mapping_GH100: Static bar1 mapped offset 0x0 size 0x5e9200000"><pre><code>[ 3491.654009] NVRM: kbusEnableStaticBar1Mapping_GH100: Static bar1 mapped offset 0x0 size 0x5e9200000
[ 3491.793389] NVRM: kbusEnableStaticBar1Mapping_GH100: Static bar1 mapped offset 0x0 size 0x5e9200000
</code></pre></div>
<p dir="auto">Perfect, we now have the VRAM mapped. However, it's not that easy to get P2P. When you run <code>./simpleP2P</code> from <code>cuda-samples</code>, you get this error.</p>
<div data-snippet-clipboard-copy-content="[ 3742.840689] NVRM: kbusCreateP2PMappingForBar1P2P_GH100: added PCIe BAR1 P2P mapping between GPU2 and GPU3
[ 3742.840762] NVRM: kbusCreateP2PMappingForBar1P2P_GH100: added PCIe BAR1 P2P mapping between GPU3 and GPU2
[ 3742.841089] NVRM: nvAssertFailed: Assertion failed: (shifted >> pField->shift) == value @ field_desc.h:272
[ 3742.841106] NVRM: nvAssertFailed: Assertion failed: (shifted &amp; pField->maskPos) == shifted @ field_desc.h:273
[ 3742.841281] NVRM: nvAssertFailed: Assertion failed: (shifted >> pField->shift) == value @ field_desc.h:272
[ 3742.841292] NVRM: nvAssertFailed: Assertion failed: (shifted &amp; pField->maskPos) == shifted @ field_desc.h:273
[ 3742.865948] NVRM: GPU at PCI:0000:01:00: GPU-49c7a6c9-e3a8-3b48-f0ba-171520d77dd1
[ 3742.865956] NVRM: Xid (PCI:0000:01:00): 31, pid=21804, name=simpleP2P, Ch 00000013, intr 00000000. MMU Fault: ENGINE CE3 HUBCLIENT_CE1 faulted @ 0x7f97_94000000. Fault is of type FAULT_INFO_TYPE_UNSUPPORTED_KIND ACCESS_TYPE_VIRT_WRITE"><pre><code>[ 3742.840689] NVRM: kbusCreateP2PMappingForBar1P2P_GH100: added PCIe BAR1 P2P mapping between GPU2 and GPU3
[ 3742.840762] NVRM: kbusCreateP2PMappingForBar1P2P_GH100: added PCIe BAR1 P2P mapping between GPU3 and GPU2
[ 3742.841089] NVRM: nvAssertFailed: Assertion failed: (shifted &gt;&gt; pField-&gt;shift) == value @ field_desc.h:272
[ 3742.841106] NVRM: nvAssertFailed: Assertion failed: (shifted &amp; pField-&gt;maskPos) == shifted @ field_desc.h:273
[ 3742.841281] NVRM: nvAssertFailed: Assertion failed: (shifted &gt;&gt; pField-&gt;shift) == value @ field_desc.h:272
[ 3742.841292] NVRM: nvAssertFailed: Assertion failed: (shifted &amp; pField-&gt;maskPos) == shifted @ field_desc.h:273
[ 3742.865948] NVRM: GPU at PCI:0000:01:00: GPU-49c7a6c9-e3a8-3b48-f0ba-171520d77dd1
[ 3742.865956] NVRM: Xid (PCI:0000:01:00): 31, pid=21804, name=simpleP2P, Ch 00000013, intr 00000000. MMU Fault: ENGINE CE3 HUBCLIENT_CE1 faulted @ 0x7f97_94000000. Fault is of type FAULT_INFO_TYPE_UNSUPPORTED_KIND ACCESS_TYPE_VIRT_WRITE
</code></pre></div>
<p dir="auto">Failing with an MMU fault. So you dive into this and find that it's using <code>GMMU_APERTURE_PEER</code> as the mapping type. That doesn't seem supported in the 4090. So let's see what types are supported, <code>GMMU_APERTURE_VIDEO</code>,<code>GMMU_APERTURE_SYS_NONCOH</code>, and <code>GMMU_APERTURE_SYS_COH</code>. We don't care about being coherent with the CPU's L2 cache, but it does have to go out the PCIe bus, so we rewrite <code>GMMU_APERTURE_PEER</code> to <code>GMMU_APERTURE_SYS_NONCOH</code>. We also no longer set the peer id that was corrupting the page table.</p>
<div data-snippet-clipboard-copy-content="cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 24.21GB/s
Preparing host buffer and memcpy to GPU0...
Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...
Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...
Copy data back to host from GPU0 and verify results...
Verification error @ element 1: val = 0.000000, ref = 4.000000
Verification error @ element 2: val = 0.000000, ref = 8.000000"><pre><code>cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 24.21GB/s
Preparing host buffer and memcpy to GPU0...
Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...
Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...
Copy data back to host from GPU0 and verify results...
Verification error @ element 1: val = 0.000000, ref = 4.000000
Verification error @ element 2: val = 0.000000, ref = 8.000000
</code></pre></div>
<p dir="auto">Progress! <code>./simpleP2P</code> appears to work, however the copy isn't happening. The address is likely wrong. It turns out they have a separate field for the peer address called <code>fldAddrPeer</code>, we change that to <code>fldAddrSysmem</code>. We also print out the addresses and note that the physical BAR address isn't being added properly, they provide a field <code>fabricBaseAddress</code> for <code>GMMU_APERTURE_PEER</code>, we reuse it and put the <code>BAR1</code> base address in there.</p>
<p dir="auto">That's it. Thanks to NVIDIA for writing such a stable driver. And with this, the tinybox green is even better.</p>
<p dir="auto">~ the tiny corp</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Functional</h2><a id="user-content-functional" aria-label="Permalink: Functional" href="#functional"></a></p>
<div data-snippet-clipboard-copy-content="Enabling peer access between GPU0 and GPU1...
Allocating buffers (64MB on GPU0, GPU1 and CPU Host)...
Creating event handles...
cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 24.44GB/s
Preparing host buffer and memcpy to GPU0...
Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...
Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...
Copy data back to host from GPU0 and verify results...
Disabling peer access...
Shutting down...
Test passed"><pre><code>Enabling peer access between GPU0 and GPU1...
Allocating buffers (64MB on GPU0, GPU1 and CPU Host)...
Creating event handles...
cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 24.44GB/s
Preparing host buffer and memcpy to GPU0...
Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...
Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...
Copy data back to host from GPU0 and verify results...
Disabling peer access...
Shutting down...
Test passed
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Fast</h2><a id="user-content-fast" aria-label="Permalink: Fast" href="#fast"></a></p>
<div data-snippet-clipboard-copy-content="Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3      4      5
     0 919.39  50.11  50.15  51.22  50.59  51.22
     1  50.19 921.29  50.31  51.21  50.62  51.22
     2  50.23  50.55 921.83  51.22  50.39  51.22
     3  50.33  50.65  51.20 920.20  50.43  51.22
     4  50.18  50.68  50.26  51.22 922.30  51.23
     5  50.12  50.09  50.44  51.22  51.21 921.29"><pre><code>Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3      4      5
     0 919.39  50.11  50.15  51.22  50.59  51.22
     1  50.19 921.29  50.31  51.21  50.62  51.22
     2  50.23  50.55 921.83  51.22  50.39  51.22
     3  50.33  50.65  51.20 920.20  50.43  51.22
     4  50.18  50.68  50.26  51.22 922.30  51.23
     5  50.12  50.09  50.44  51.22  51.21 921.29
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">And NCCL (aka torch) compatible!</h2><a id="user-content-and-nccl-aka-torch-compatible" aria-label="Permalink: And NCCL (aka torch) compatible!" href="#and-nccl-aka-torch-compatible"></a></p>
<div data-snippet-clipboard-copy-content="tiny@tiny14:~/build/nccl-tests/build$ ./all_reduce_perf -g 6
# nThread 1 nGpus 6 minBytes 33554432 maxBytes 33554432 step: 1048576(bytes) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0
#
# Using devices
#  Rank  0 Group  0 Pid  26230 on     tiny14 device  0 [0x01] NVIDIA GeForce RTX 4090
#  Rank  1 Group  0 Pid  26230 on     tiny14 device  1 [0x42] NVIDIA GeForce RTX 4090
#  Rank  2 Group  0 Pid  26230 on     tiny14 device  2 [0x81] NVIDIA GeForce RTX 4090
#  Rank  3 Group  0 Pid  26230 on     tiny14 device  3 [0x82] NVIDIA GeForce RTX 4090
#  Rank  4 Group  0 Pid  26230 on     tiny14 device  4 [0xc1] NVIDIA GeForce RTX 4090
#  Rank  5 Group  0 Pid  26230 on     tiny14 device  5 [0xc2] NVIDIA GeForce RTX 4090
#
#                                                              out-of-place                       in-place
#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong
#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)
    33554432       8388608     float     sum      -1   2275.1   14.75   24.58      0   2282.5   14.70   24.50      0
# Out of bounds values : 0 OK
# Avg bus bandwidth    : 24.5413
#"><pre><code>tiny@tiny14:~/build/nccl-tests/build$ ./all_reduce_perf -g 6
# nThread 1 nGpus 6 minBytes 33554432 maxBytes 33554432 step: 1048576(bytes) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0
#
# Using devices
#  Rank  0 Group  0 Pid  26230 on     tiny14 device  0 [0x01] NVIDIA GeForce RTX 4090
#  Rank  1 Group  0 Pid  26230 on     tiny14 device  1 [0x42] NVIDIA GeForce RTX 4090
#  Rank  2 Group  0 Pid  26230 on     tiny14 device  2 [0x81] NVIDIA GeForce RTX 4090
#  Rank  3 Group  0 Pid  26230 on     tiny14 device  3 [0x82] NVIDIA GeForce RTX 4090
#  Rank  4 Group  0 Pid  26230 on     tiny14 device  4 [0xc1] NVIDIA GeForce RTX 4090
#  Rank  5 Group  0 Pid  26230 on     tiny14 device  5 [0xc2] NVIDIA GeForce RTX 4090
#
#                                                              out-of-place                       in-place
#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong
#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)
    33554432       8388608     float     sum      -1   2275.1   14.75   24.58      0   2282.5   14.70   24.50      0
# Out of bounds values : 0 OK
# Avg bus bandwidth    : 24.5413
#
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An open source initiative to share and compare heat pump performance data (567 pts)]]></title>
            <link>https://heatpumpmonitor.org/</link>
            <guid>40010615</guid>
            <pubDate>Fri, 12 Apr 2024 08:53:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://heatpumpmonitor.org/">https://heatpumpmonitor.org/</a>, See on <a href="https://news.ycombinator.com/item?id=40010615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                    <h3 v-if="mode=='user'">My Systems</h3>
                    <h3 v-if="mode=='admin'">Admin Systems</h3>

                    <p v-if="mode=='user'">Add, edit and view systems associated with your account.</p>
                    <p v-if="mode=='admin'">Add, edit and view all systems.</p>
                    
                    <p v-if="mode=='public' &amp;&amp; showContent">Here you can see a variety of installations monitored with OpenEnergyMonitor, and compare detailed statistics to see how performance can vary.</p>
                    <p v-if="mode=='public' &amp;&amp; showContent">If you're monitoring a heat pump with <b>emoncms</b> and the My Heat Pump app, <a href="https://heatpumpmonitor.org//user/login">login</a> to add your details.</p>
                    <p v-if="mode=='public' &amp;&amp; showContent">To join in with discussion of the results, or for support please use the <a href="https://community.openenergymonitor.org/tag/heatpumpmonitor">OpenEnergyMonitor forums.</a></p> 
                    
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[H-1B visa fraud alive and well amid anti-abuse efforts (168 pts)]]></title>
            <link>https://www.theregister.com/2024/04/09/h1b_visa_fraud/</link>
            <guid>40010579</guid>
            <pubDate>Fri, 12 Apr 2024 08:47:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/04/09/h1b_visa_fraud/">https://www.theregister.com/2024/04/09/h1b_visa_fraud/</a>, See on <a href="https://news.ycombinator.com/item?id=40010579">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>In depth</span> The US Citizenship and Immigration Service (USCIS) last October proposed new rules to reform the H-1B visa program following its acknowledgement of widespread fraud in April last year.</p>
<p>At that time, USCIS, part of the Department of Homeland Security, announced it had "undertaken extensive fraud investigations" based on suspicions that certain employers – such as some IT outsourcers – were attempting to abuse the H-1B lottery process by submitting multiple visa applications for the same person.</p>
<p>The USCIS <a target="_blank" rel="nofollow" href="https://www.federalregister.gov/documents/2023/10/23/2023-23381/modernizing-h-1b-requirements-providing-flexibility-in-the-f-1-program-and-program-improvements">Notice of Proposed Rulemaking</a> (NPRM) issued in October includes measures to modernize the H-1B program, to make it more flexible, and to ensure program integrity through fraud mitigation. The final rule was <a target="_blank" rel="nofollow" href="https://www.uscis.gov/newsroom/news-releases/uscis-announces-strengthened-integrity-measures-for-h-1b-program">announced</a> in January.</p>

    

<p>The H-1B program is intended to allow folks with specialized knowledge to work in the US. The visa is a gold ticket to America for foreigners: among other things, it gives them a runway to apply for permanent residency, and greater freedom, which in turn provides a pathway to citizenship.</p>

        


        

<p>The tech industry – which has come to rely on H-1B workers – welcomed the rule revision, citing abuse of the application process.</p>
<p>Intel, in <a target="_blank" rel="nofollow" href="https://www.regulations.gov/comment/USCIS-2023-0005-0826">comments</a> filed with USCIS last November, argued that "As is true for other employers, the misuse of the H-1B registration process has undermined Intel's ability to retain critical talent. Since implementation of the electronic registration system, Intel's H-1B selection rates have steadily plummeted, hampering efforts to expand its semiconductor design and manufacturing efforts in the United States. A system meant to help administer the H-1B lottery process has instead become a source of frustration and disappointment."</p>

        

<p>The <a target="_blank" rel="nofollow" href="https://www.uscis.gov/newsroom/news-releases/dhs-issues-proposed-rule-to-modernize-the-h-1b-specialty-occupation-worker-program">changes</a> may not be enough, however.</p>
<p>"I'd say it's insufficient," opined Robert Law, director of the Center For Homeland Security And Immigration at the America First Policy Institute, who served as a senior policy advisor and chief of policy at USCIS during the Trump administration.</p>
<p>Law, in an interview with <em>The Register</em>, suggested <a target="_blank" rel="nofollow" href="https://trumpwhitehouse.archives.gov/briefings-statements/trump-administration-taking-action-tighten-foreign-worker-visa-requirements-protect-american-workers/">policies</a> adopted by the Trump administration that prioritized people by skill might have helped, but were abandoned by the Biden administration. "And really, I don't think the Biden administration has done anything in the H-1B space that puts integrity into the system and accountability."</p>
<blockquote>

<p>I don't think the Biden administration has done anything in the H-1B space that puts integrity into the system and accountability</p>
</blockquote>
<p>"The new rules are only intended to address one aspect of fraud, which is multiple submissions by the same person," noted John Miano, a fellow with the conservative Center for Immigration Studies, who has testified before Congress about foreign labor. "But that isn't going to do very much at all. There's no tracking of people through the process. The reality is too many people benefit from the fraud so it goes on."</p>
<p>Across the political aisle, the progressive Economic Policy Institute (EPI) expressed similar skepticism about the USCIS proposals.</p>

        

<p>Responding to the NPRM's solicitation for comments, EPI <a target="_blank" rel="nofollow" href="https://www.epi.org/publication/epi-comments-on-dhss-proposed-rule-on-modernizing-h-1b-requirements-providing-flexibility-in-the-f-1-program-and-program-improvements-affecting-other-nonimmigrant-workers/">declared</a>: "[T]he H-1B Modernization rule only tinkers at the edges of improving program integrity, but fails terribly to tackle any of the real problems that have been documented by investigative news reports, government audits, and researchers like ourselves –&nbsp;while simultaneously proposing to dramatically expand the size of an H-1B program that remains vulnerable to rampant fraud and abuse."</p>
<p>The H-1B program was created in 1990, and presently allocates 85,000 spots annually for temporary non-immigrant workers to come to the US – ostensibly to fill gaps in the American labor force. Counting other exemptions like those afforded academic institutions, the program awards about 130,000 visas per year to foreign workers, and renews about 300,000 previously awarded visas – which typically last for three years and can be extended for another three.</p>
<p>The process works as follows: Eligible H-1B applicants, or companies representing them, register to enter the <a target="_blank" rel="nofollow" href="https://www.uscis.gov/working-in-the-united-states/h-1b-specialty-occupations">H-1B cap lottery</a>. Some 20,000 advanced degree petitions and 65,000 general petitions get selected. For selected registrants, employers can submit H-1B petitions on behalf of prospective employees. USCIS then processes the selected petitions and those approved can then come and work in the US.</p>
<p>Previously, employers submitted completed H-1B petitions in March and USCIS conducted its H1-B cap lottery at the end of that month to determine which petitions would be processed for the 85,000 slots.</p>
<p>The new process is similar, but applicants must provide a valid passport or travel document during electronic registration – under the new rules, a beneficiary may not be registered under more than one travel document. Also, now all employers and their would-be employees have to have a shared "myUSCIS organizational account" for the registration process. Once registration is concluded, USCIS randomly selects 85,000 registrants from the pool and notifies registering employers that they can file an H-1B petition on behalf of the selected employee.</p>
<p>The goal here is ideally to prevent employers from gaming the lottery system by submitting multiple registrations for a single beneficiary.</p>
<p>As we <a target="_blank" href="https://www.theregister.com/2023/07/01/h1b_fraud/">reported</a> last year, demand for these visas far outstrips supply, with 780,884 applications for FY 2024 (received between March 1, 2023, and March 17). That's up from 483,927 for FY 2023. The FY 2025 lottery <a target="_blank" rel="nofollow" href="https://www.uscis.gov/newsroom/alerts/uscis-extends-initial-registration-period-for-fy-2025-h-1b-cap">concluded</a> on March 25, 2024, after being extended three days to make up for a system outage. The total number of applications has yet to be released – but as before, 85,000 H-1B visas are expected to be awarded.</p>
<h3>Exploited</h3>
<p>The H-1B program has been criticized for various reasons. EPI, for example, notes that American employers do not have to recruit US residents before hiring foreign H-1B workers, that it's legal for US employers to underpay H-1B workers, relatively speaking, and that H-1B workers are often exploited and lack job protection and mobility, among other issues.</p>
<p>Then there's the fraud. "For years, the H-1B program has been riddled with large-scale fraud yet the agency failed to take adequate fraud-prevention measures," the EPI argued. "The integrity efforts proposed in the NPRM are welcome, but even if fully implemented, would address only a fraction of the fraud."</p>
<p>"The system is entirely written to promote fraud," asserted the Center for Immigration Studies' Miano, noting that while the H-1B statute dictates workers are supposed to be paid a prevailing wage for their role and industry, employers get to determine that prevailing wage, and neither the US Department of Labor nor USCIS can really challenge the listed figure.</p>
<p>One form of H-1B fraud involves companies colluding to file multiple petitions on behalf of the same foreign worker. This serves to give the IT consultancies involved in trying to bring this worker to the US a greater chance of having their applicant selected, and of collecting a commission. As above, that's supposed to be more difficult under the new system.</p>
<p>A 2017 <a target="_blank" rel="nofollow" href="https://www.oig.dhs.gov/sites/default/files/assets/2017/OIG-18-03-Oct17.pdf">report</a> [PDF] from the DHS Office of the Inspector General noted that many technology consulting firms submitted petitions for workers with the intent of contracting them out to a third party, and that "in many cases, the projects provided within the petition are non-existent."</p>
<p>"When I was at USCIS," explained Law, "you would have registrations or petitions with very common names. The belief from some of the fraud folks was that some of the common names were inserted in there and then these businesses, after they won the lottery, would then go out and actually find somebody [with that name] as opposed to having a real candidate in place."</p>
<h3>Indentured</h3>
<p>The fraud extends beyond gaming the visa lottery.</p>
<p>A source familiar with the Indian IT community in the US – who asked not to be identified out of concern for retribution – described a culture of corruption through which technology consultancies, working with larger firms that turn a blind eye to abuse, exploit the lack of H-1B oversight for financial gain.</p>
<p>"It's not just that multiple visa applications are put in for one person to obtain a H-1B visa," our source explained. "Once the visa is obtained, that student then works off the cost of that visa (illegal but more common than not) – usually in a 70/30 split favoring the student at whatever tier they were hired in at. The higher tier you get recruited at, the better off you are with pay. They will also have indentured servant-like debt as the prize for getting to America."</p>
<blockquote>

<p>They will also have indentured servant-like debt as the prize for getting to America</p>
</blockquote>
<p>Foreign students studying in the US who didn't win the visa lottery may play a role in further fraud. We're told some consultancies tap up foreigners to work for clients illegally using the identities of legit citizens or green card holders.</p>
<p>"That's where the true scam is and the real money is at," our source explained. "These people, recent IT grads, will do almost anything to stay in the US and work an IT job. The visa scam is small potatoes compared to what happens to the losers.</p>
<p>"They are offered jobs paying fractions of a dollar, to live with six people in a one bedroom apartment to manage costs. These people number in the thousands each year, compounded over decades."</p>
<p>"Their job is to impersonate a green card holder or US citizen to receive 30 percent of the pay," with 70 percent going to the people organizing the fraud, our source continued. "Why? Because it’s still more money than their family back home could dream about.</p>
<p>"The winner is the consultancy … This scam is decades old and is becoming a massive problem in the IT market. And quite frankly, it's a major national security threat since it's a large exploitable population with access to sensitive American data."</p>
<p>These problems extend to other guest worker programs such as the <a target="_blank" rel="nofollow" href="https://www.uscis.gov/working-in-the-united-states/students-and-exchange-visitors/optional-practical-training-opt-for-f-1-students">Optional Practical Training</a> (OPT) scheme, which allows foreign students to work in the US for 36 months. In Senate budget committee <a target="_blank" rel="nofollow" href="https://www.budget.senate.gov/imo/media/doc/ronil_testimony_913.pdf">testimony</a> [PDF] last September, Ronil Hira, an EPI researcher and associate professor in the department of political science at Howard University, described OPT as an unauthorized guest worker program that isn't policed because Homeland Security insists that it's merely a training program.</p>
<p>"OPT workers directly compete with, and substitute for, US workers," argued Hira. "I know at least one worker who unwittingly trained his OPT replacement. When he filed formal complaints to ICE and the Department of Justice, the agencies told him tough luck."</p>
<ul>

<li><a href="https://www.theregister.com/2023/07/01/h1b_fraud/">H-1B fraud consultancies grow, with application abuse openly discussed online</a></li>

<li><a href="https://www.theregister.com/2024/03/19/us_department_of_security_talks/">Homeland Security will test out using genAI to train US immigration officers</a></li>

<li><a href="https://www.theregister.com/2023/10/03/supreme_court_visa/">Supreme Court doesn't want to hear union's beef about STEM grad work visas</a></li>

<li><a href="https://www.theregister.com/2023/05/04/register_kettle_h1b_visa/">Uncle Sam probes H-1B abuse surge: What do our vultures make of it?</a></li>
</ul>
<p>Then there's the fraud that happens after the visa selection process, in which IT consultancies for large clients create work for themselves, to boost billable hours. For example, our source recounted an incident from years past involving an individual who worked for a healthcare firm under another green card holder's name.</p>
<p>"Whenever he wanted overtime, he would just bring down the servers and cause a P0 [high-priority trouble ticket] to come through," we're told. "You know, he and his team would get paid. And that meant that they would maybe get like 10 to 12 hours of overtime, all the while knowing how to solve the problem. They created extra income for themselves."</p>
<p>Our source opined that while IT outsourcing of this sort is supposed to reduce costs, the consultancies providing these services are organized to do the opposite.</p>
<p>"They inflate the cost by doing these types of things, making things more difficult than they need to be. And it's not like any one person is ever doing it. But it is a culture that is causing it. They're not trying to actually make things easier, faster, more affordable, more reliable, and so on. They're just doing whatever will to cause enough roles to be created so the consultancy is warranted."</p>
<h3>Outsourcing</h3>
<p>During his testimony Hira described how the H-1B program evolved.</p>
<p>"More than twenty years ago, Cognizant, Infosys, and Tata Consultancy Services (TCS) pioneered the H-1B-outsourcing business model, which was so profitable it quickly came to dominate the IT services industry. They offer customers a way to cut costs by outsourcing their US IT work to a team of on-site (in the US) and offshore workers."</p>
<p>The ratio of on-site to offshore workers, said Hira, was commonly 30/70, because a portion of jobs could not be shipped overseas.</p>
<p>"Rather than hire US workers to perform the onshore work, these firms hire large numbers of H-1B and L-1 visa workers to fill the jobs in America," Hira explained to the Senate committee. "Exploiting the visa programs enables them to drive revenue growth and increase firm profitability. Hiring visa workers instead of US workers offers advantages. The H-1B guest workers are controllable, indentured, and are paid less than the US workers, and they facilitate the transfer of work to offshore teams in India."</p>
<p>USCIS last year <a target="_blank" rel="nofollow" href="https://www.uscis.gov/working-in-the-united-states/temporary-workers/h-1b-specialty-occupations-and-fashion-models/h-1b-electronic-registration-process">highlighted</a> its "extensive fraud investigations" and "referrals for criminal prosecution." Yet, a year later, it has added just two orgs to its <a target="_blank" rel="nofollow" href="https://www.dol.gov/agencies/whd/immigration/h1b/willful-violator-list">Willful Violator list</a>: Bonzer, LLC and BER-IT, Inc. If there is a fraud crackdown underway, it's not particularly visible.</p>
<blockquote>

<p>American workers are being sidelined, overlooked and their wages are being suppressed</p>
</blockquote>
<p>Law believes the primary concern is the impact the H-1B program has on the economy.</p>
<p>"American workers are being sidelined, overlooked and their wages are being suppressed because of H-1Bs," he argued. "Whether it's IT outsourcing firms or major companies like Microsoft, IBM, Facebook, Apple, and so on, the way the law was drafted and is currently being applied, all of these companies are legally allowed to pay less than market wages."</p>
<p>In 2021 rulemaking, the US Department of Labor <a target="_blank" rel="nofollow" href="https://www.regulations.gov/document/ETA-2020-0006-2342">estimated</a> that employers were underpaying H-1B workers to the tune of $15 billion per year.</p>
<p>Since December, 2023, according to the <a target="_blank" rel="nofollow" href="https://www.wsj.com/business/fired-americans-say-indian-firm-gave-their-jobs-to-h-1b-visa-holders-6da7cf26">Wall Street Journal</a>, at least 22 former employees of TCS, India's largest IT consultancy by revenue, have filed complaints with the US Equal Employment Opportunity Commission. They allege the firm illegally discriminated against them based on race and age by firing them and giving their jobs to lower paid Indian tech workers on temporary work visas.</p>
<p>The EEOC investigation is ongoing, and may or may not result in a formal complaint, but TCS has denied any wrongdoing.</p>
<p>While the US Justice Department has had <a target="_blank" rel="nofollow" href="https://www.justice.gov/usao-ndtx/pr/brothers-sentenced-visa-fraud-convictions">some</a> <a target="_blank" rel="nofollow" href="https://www.justice.gov/usao-sdtx/pr/houston-consulting-company-admits-h-1b-visa-fraud-conspiracy">success</a> going after visa fraud, it doesn't always prevail. In 2017, former TCS employee Anil Kini filed a whistleblower lawsuit against TCS alleging the company "violated the False Claims Act (FCA) by failing to obtain H-1B visas for, and pay the proper H-1B wage rate to, employees who were engaged in H-1B visa work, as well as retaliating against [him] for investigating the scheme."</p>
<p>In February the case, which had been taken over by US government prosecutors, was <a target="_blank" rel="nofollow" href="https://news.bloomberglaw.com/federal-contracting/tata-consultancy-beats-whistleblowers-h-1b-visa-fraud-lawsuit">dismissed</a> because the judge determined TCS "did not have an 'obligation' under the FCA to pay its employees higher wages" and because Kini's investigation was not a protected activity under the FCA. That decision has been appealed.</p>
<p>Law asserted that the biggest outrage is staffing companies, and pointed to Disney's <a target="_blank" rel="nofollow" href="https://www.nytimes.com/2016/01/26/us/lawsuit-claims-disney-colluded-to-replace-us-workers-with-immigrants.html">replacement</a> of US workers with H-1B workers several years ago.</p>
<p>"Many of these fired American tech workers are forced to sign various nondisclosure agreements or they can only get a severance package if they shut up, and things like that," he alleged. "Pretty much the entire tech industry has determined that H-1Bs are part of their business model."</p>
<p>Law doesn't expect the H-1B rule changes to have much impact on fraud.</p>
<p>"You're seeing a far more brazen attitude by those who have made it a business of their own to help people game the H-1B system," he lamented.</p>
<p>"The fact that a quick Google search can find you a chat room – or Discord server or Reddit thread or whatever the case may be – about how to do things that are not above board, the H-1B context just goes to show that there are not currently really any repercussions for doing those things. So until the Department of Homeland Security, Department of Labor, [and other agencies involved in this] get serious about it, you're going to continue to see [fraud] happen on a very large scale." ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Webb – Space Telescope Live. What Is Webb Observing Now? (287 pts)]]></title>
            <link>https://spacetelescopelive.org/webb?obsId=01HTJT20C0STKNZ01KQYGEKBQ1</link>
            <guid>40010221</guid>
            <pubDate>Fri, 12 Apr 2024 07:29:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spacetelescopelive.org/webb?obsId=01HTJT20C0STKNZ01KQYGEKBQ1">https://spacetelescopelive.org/webb?obsId=01HTJT20C0STKNZ01KQYGEKBQ1</a>, See on <a href="https://news.ycombinator.com/item?id=40010221">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main data-testid="telescope-view" id="main-content"><div><header><div><div><h2>What is Webb observing?</h2></div><div><p><img alt="Line drawing of the Webb Space Telescope" loading="lazy" width="85" height="68" decoding="async" data-nimg="1" src="https://spacetelescopelive.org/img/webb-vector.svg"></p><div><h2>Target: <span>Not applicable</span></h2><p><strong>Target Category: </strong>Not applicable</p><p><strong>Research Program: </strong>Not applicable</p></div><nav data-testid="prev-next--desktop"></nav></div></div><div><h2>What is Webb observing?</h2><p><strong>Target:  </strong><span>Not applicable</span></p><p><strong>Target Category: </strong>Not applicable</p><p><strong>Research Program: </strong>Not applicable</p></div><div><h3>Observation Status &amp; Details</h3></div><p><span><span>Planned Outage</span><span><span>On Friday, April 12th starting at noon through Sunday, April 14th, Space Telescope Live may be unavailable. We apologize for any inconvenience. </span></span></span></p></header><h3>Sky Map</h3><div data-testid="telescope-view__aladin"><h3>Sky Map Details</h3><p><strong>Background: </strong><span>Two Micron All Sky Survey</span></p><p><strong>Field of View: </strong><span data-testid="fov-label">30 arcminutes</span></p><p><strong>Coordinates:</strong><span data-testid="fov-label">+00 00 00.00 +00 00 00.0</span></p></div></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Here's a puzzle game. I call it Reverse the List of Integers." (130 pts)]]></title>
            <link>https://mathstodon.xyz/@two_star/112242224494626411</link>
            <guid>40010066</guid>
            <pubDate>Fri, 12 Apr 2024 06:59:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mathstodon.xyz/@two_star/112242224494626411">https://mathstodon.xyz/@two_star/112242224494626411</a>, See on <a href="https://news.ycombinator.com/item?id=40010066">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Running TempleOS in user space without virtualization (107 pts)]]></title>
            <link>https://github.com/1fishe2fishe/EXODUS</link>
            <guid>40010050</guid>
            <pubDate>Fri, 12 Apr 2024 06:57:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/1fishe2fishe/EXODUS">https://github.com/1fishe2fishe/EXODUS</a>, See on <a href="https://news.ycombinator.com/item?id=40010050">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">EXODUS: Executable Divine Operating System in Userspace</h2><a id="user-content-exodus-executable-divine-operating-system-in-userspace" aria-label="Permalink: EXODUS: Executable Divine Operating System in Userspace" href="#exodus-executable-divine-operating-system-in-userspace"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/1fishe2fishe/EXODUS/blob/master/img/Exodus.png"><img src="https://github.com/1fishe2fishe/EXODUS/raw/master/img/Exodus.png" alt="EXODUS"></a></p>
<p dir="auto">EXODUS is a port of the TempleOS kernel to user space for Linux, Windows and FreeBSD.
EXODUS makes HolyC a user-space general programming language that
isn't just an exclusivity of virtual machines. It contains a mostly
full-featured GUI from stock TempleOS, and a CLI mode which has found
uses for running the networked servers automatically and building
EXODUS' kernel itself.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building EXODUS</h2><a id="user-content-building-exodus" aria-label="Permalink: Building EXODUS" href="#building-exodus"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Linux/FreeBSD</h2><a id="user-content-linuxfreebsd" aria-label="Permalink: Linux/FreeBSD" href="#linuxfreebsd"></a></p>
<p dir="auto">Install the packages for CMake and the development headers for SDL2.
Navigate to the directory and run these commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="mkdir build
cd build
cmake ..
make -j$(nproc);
cd .."><pre>mkdir build
<span>cd</span> build
cmake ..
make -j<span><span>$(</span>nproc<span>)</span></span><span>;</span>
<span>cd</span> ..</pre></div>
<p dir="auto">You'll see the <code>exodus</code> binary. NOTE: <code>WARN</code>s during the build process aren't
a sign of failure.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Windows</h2><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<p dir="auto">Windows users tend to be less technically sophisticated,
so here is a more detailed instruction:</p>
<ol dir="auto">
<li>Install <a href="https://msys2.org/" rel="nofollow">msys2</a></li>
<li>Run "MSYS2 MINGW64" (NOTE: <em><strong>MUST</strong></em> BE MINGW64, NO SUPPORT FOR OTHER ENVIRONMENTS)</li>
<li>Run <code>pacman -Sy mingw-w64-x86_64-{gcc,SDL2,cmake}</code></li>
<li>Run the following after navigating to the directory(Ignore any <code>WARN</code>s):</li>
</ol>
<div data-snippet-clipboard-copy-content="mkdir build
cd build
cmake ..
ninja
cd .."><pre><code>mkdir build
cd build
cmake ..
ninja
cd ..
</code></pre></div>
<ol start="5" dir="auto">
<li>You will see the <code>exodus</code> binary in the directory</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running EXODUS</h2><a id="user-content-running-exodus" aria-label="Permalink: Running EXODUS" href="#running-exodus"></a></p>
<p dir="auto">Run EXODUS with <code>./exodus</code>.</p>
<p dir="auto">Consult <code>./exodus -h</code> for more command-line options such as CLI mode.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Host filesystem integration (T:/ is the root drive, and Z:/ is where EXODUS was launched)</li>
<li>Documentation in T:/Server (run <code>#include "run";</code> after <code>Cd("Server");</code>, on localhost:8080)</li>
<li>Mostly orthodox TempleOS GUI</li>
<li>CLI mode</li>
<li>Community software in T:/Community, such as Satania-buddy and games like BlazeItFgt2</li>
<li>Networking (Take a look at T:/Community/{gihon,HolyIRC} and T:/Server's code)</li>
</ul>
<p dir="auto">There have been some disputes on whether TempleOS can have networking or not.
To borrow the man's words, God did not ban networking.
Terry didn't implement networking in stock TempleOS because of
the line number restriction.</p>
<p dir="auto"><a href="https://old.reddit.com/r/TempleOS_Official/comments/2pzdy6/can_there_be_networking/cn1kshe/" rel="nofollow">"God did not ban networking"</a> <a href="https://archive.is/0epu3" rel="nofollow">(archived)</a></p>
<p dir="auto">Terry has also documented that the ideal TempleOS PC needs networking through a simple
serial interface.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/1fishe2fishe/EXODUS/blob/master/img/serialnetworking.png"><img src="https://github.com/1fishe2fishe/EXODUS/raw/master/img/serialnetworking.png" alt="Networking with serial"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Changes from stock TempleOS</h2><a id="user-content-changes-from-stock-templeos" aria-label="Permalink: Changes from stock TempleOS" href="#changes-from-stock-templeos"></a></p>
<ul dir="auto">
<li>The Windows key functionality has been replaced with the Alt key</li>
<li>Context switching is slower than a VM or native hardware because it runs in Ring 3, but it does not degrade general performance</li>
<li>Ring-0 opcodes (HLT, etc) and routines (InU8, etc) are not available (you can assemble the opcodes but they'll raise a fault)</li>
<li>Fs and Gs are pointers to CTask/CCPU instead of the whole structures at gs:0x28 and gs:0x50 respectively <a href="https://en.wikipedia.org/wiki/Win32_Thread_Information_Block#Contents_of_the_TIB_on_Windows" rel="nofollow">(blame microsoft)</a></li>
</ul>
<p dir="auto">A bit of trivia on Fs and Gs, gs:0 is not touched by 64 bit Windows but it's written to (not sure about read) on Wine, which doesn't guarantee consistency of gs:0. TEBs are allocated 0x2000 apart and sizeof(TEB) is 0x1838 so I could probably use 0x1900 or something but that seemed too hacky. Eventually I had to resort storing Fs at gs:0x28 and Gs at gs:0x50 (NtCurrentTeb()-&gt;ActiveRpcHandle (it isn't used anywhere)).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto"><a href="https://github.com/1fishe2fishe/EXODUS/blob/master/CONTRIBUTING.md">Refer to this document.</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Very quick HolyC reference</h2><a id="user-content-very-quick-holyc-reference" aria-label="Permalink: Very quick HolyC reference" href="#very-quick-holyc-reference"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="/* CTRL+SHIFT+F<num shown in autocomplete window>
 * after typing in a function name will show its src */
DirMk(&quot;folder&quot;);
Dir;
Dir(&quot;fold*&quot;);//supports wildcards
Cd(&quot;folder&quot;);
Man(&quot;Ed&quot;);
Ed(&quot;file.HC.Z&quot;);
Unzip(&quot;file.HC.Z&quot;);
Zip(&quot;file.HC&quot;);
Find(&quot;str&quot;,,&quot;-i&quot;);//&quot;-i&quot; is for case-sensitivity (grep -rn . -e str)
FF(&quot;file.*&quot;);//find .|grep file
RedSeaISO(&quot;Something.ISO.C&quot;,&quot;Community&quot;);//archive folder into an ISO
MountFile(&quot;Something.ISO.C&quot;);//defaults to M
Cd(&quot;M:/&quot;);
*0=0;//Raise the debugger
DbgHelp;//help on how to debug
Shutdown;"><pre><span>/* CTRL+SHIFT+F&lt;num shown in autocomplete window&gt;</span>
<span> * after typing in a function name will show its src */</span>
<span>DirMk</span>(<span>"folder"</span>);
<span>Dir</span>;
<span>Dir</span>(<span>"fold*"</span>);<span>//supports wildcards</span>
<span>Cd</span>(<span>"folder"</span>);
<span>Man</span>(<span>"Ed"</span>);
<span>Ed</span>(<span>"file.HC.Z"</span>);
<span>Unzip</span>(<span>"file.HC.Z"</span>);
<span>Zip</span>(<span>"file.HC"</span>);
<span>Find</span>(<span>"str"</span>,,<span>"-i"</span>);<span>//"-i" is for case-sensitivity (grep -rn . -e str)</span>
<span>FF</span>(<span>"file.*"</span>);<span>//find .|grep file</span>
<span>RedSeaISO</span>(<span>"Something.ISO.C"</span>,<span>"Community"</span>);<span>//archive folder into an ISO</span>
<span>MountFile</span>(<span>"Something.ISO.C"</span>);<span>//defaults to M</span>
<span>Cd</span>(<span>"M:/"</span>);
<span>*</span><span>0</span><span>=</span><span>0</span>;<span>//Raise the debugger</span>
<span>DbgHelp</span>;<span>//help on how to debug</span>
<span>Shutdown</span>;</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li><a href="https://github.com/argtable/argtable3">argtable3</a></li>
<li><a href="https://qt.gitorious.org/qt/qtbase/blobs/master/src/corelib/global/qprocessordetection.h" rel="nofollow">qprocessordetection.h</a></li>
<li><a href="https://github.com/rxi/dyad">dyad</a></li>
<li><a href="https://github.com/rxi/map">map</a></li>
<li><a href="https://github.com/rxi/vec">vec</a></li>
<li><a href="https://github.com/daanx/isocline">isocline</a></li>
<li><a href="https://github.com/minexew/templeos-loader">templeos-loader</a></li>
</ul>
<p dir="auto">People:</p>
<ul dir="auto">
<li><a href="https://github.com/nrootconauto">nrootconauto</a> for working on the Wiki</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Organizing OpenStreetMap Mapping Parties (162 pts)]]></title>
            <link>https://contrapunctus.codeberg.page/blog/mapping-party-tips.html</link>
            <guid>40009997</guid>
            <pubDate>Fri, 12 Apr 2024 06:46:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://contrapunctus.codeberg.page/blog/mapping-party-tips.html">https://contrapunctus.codeberg.page/blog/mapping-party-tips.html</a>, See on <a href="https://news.ycombinator.com/item?id=40009997">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="content">
<header>

</header><nav>
 <a href="https://contrapunctus.codeberg.page/blog/index.html">↩ blog</a> 
 </nav>


<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#work-experience">What I've done</a></li>
<li><a href="#guidance">What I've learned</a>
<ul>
<li><a href="#location">The location</a></li>
<li><a href="#announcing-inviting">Announcing and inviting</a></li>
<li><a href="#equipment">Equipment</a></li>
<li><a href="#meeting">Meeting</a></li>
<li><a href="#teaching">Teaching</a></li>
<li><a href="#mapping">Mapping</a></li>
</ul>
</li>
<li><a href="#what-apps">What apps should we introduce?</a>
<ul>
<li><a href="#old-advice">Old advice and new realizations</a></li>
<li><a href="#streetcomplete">StreetComplete</a></li>
<li><a href="#id,-vespucci,-go-map!">iD, Vespucci, and Go Map!!</a></li>
<li><a href="#vespucci">Vespucci? Really?</a></li>
<li><a href="#organic-maps">Organic Maps is still important</a></li>
<li><a href="#every-door">Every Door - our choice</a></li>
<li><a href="#osm-website">openstreetmap.org</a></li>
</ul>
</li>
<li><a href="#epilogue">Epilogue</a></li>
</ul>
</div>
</nav>

<p>
I've been asked a number of times to provide tips on organizing mapping parties. I don't claim to be an authority on the matter - I'm still figuring things out.
</p>

<section id="outline-container-work-experience">
<h2 id="work-experience">What I've done</h2>
<div id="text-work-experience">
<p>
I should probably mention my "work experience", in chronological order -
</p>

<ol>
<li>Volunteering to teach <a href="https://en.wikipedia.org/wiki/Accredited_Social_Health_Activist">ASHA</a> workers to map, at a Médecins Sans Frontières (Doctors Without Borders) clinic (2023). This was my first time teaching a group of people to map, and it was a <del>disaster</del> <i>vital learning experience</i>.<label id="fnr.1" for="fnr-in.1.7953239"><sup>1</sup></label><span><sup>1</sup>
It was here that I learned that you need to be at least a little literate in English and technology to even be able to register for OSM, let alone contribute. Additionally, their own areas were very sparsely mapped, and most of them didn't have laptops either.
</span></li>

<li>Conducting OSM workshops at <a href="https://indiafoss.net/2023/unconference-track/f22804299a">IndiaFOSS</a> (2023) and <a href="https://indiafoss.net/Delhi/2024/talk/da15132b17">DelhiFOSS</a> (2024)</li>

<li>Conducting six mapping parties in Delhi, with help from <a href="https://blog.sahilister.in/">Sahil</a>, <a href="https://ravidwivedi.in/">Ravi</a>, <a href="https://aaru_swartz.gitlab.io/">Aaru Swartz</a> from <a href="https://sflc.in/">SFLC.in</a>, and many other friends. We started in June 2023, and we'll have our seventh one this Saturday.</li>

<li>Conducting a 5-day workshop for students of Planning and Architecture at IIT Roorkee (2023), in which I was assisted by <a href="https://ravidwivedi.in/">Ravi</a>.</li>
</ol>
</div>
</section>

<section id="outline-container-guidance">
<h2 id="guidance">What I've learned</h2>
<p>
This should not be considered perfect advice, for I'm constantly observing, fine-tuning, and sometimes changing my approach wholesale, to give the best experience to participants and to attract as many people as I can to the magical world of OpenStreetMap contribution.
</p>

<div id="outline-container-location">
<h3 id="location">The location</h3>
<div id="text-location">
<p>
Select a place to map, one where participants would feel safe walking around.<label id="fnr.2" for="fnr-in.2.7849163"><sup>2</sup></label><span><sup>2</sup>
This often means that mapping parties are held in relatively posh areas. Which means that map data in economically backwards areas is worse. For this reason, I've been pondering about making an "advanced" variant of mapping parties, where we choose less-than-posh locations and only invite experienced and passionate mappers who are not afraid to venture in these areas.
</span>
</p>

<p>
Markets make good mapping party locations - shops and eateries are useful to everyone, and (unlike e.g. park features) are usually laid out linearly, which makes them easier to map.
</p>

<p>
Avoid indoor situations such as malls, as they can be challenging for beginners.
</p>

<p>
You can map buildings and streets remotely in advance, to give new mappers some points of reference.
</p>

<div>
   <div>

<p loading="lazy">
<img src="https://contrapunctus.codeberg.page/blog/img/mapping-party-tips/orig/2024-08-09%20Satya%20Niketan%20buildings%20before.svg" alt="2024-08-09 Satya Niketan buildings before.svg" loading="lazy">
   </p></div>
   <div>

<p loading="lazy">
<img src="https://contrapunctus.codeberg.page/blog/img/mapping-party-tips/orig/2024-08-09%20Satya%20Niketan%20buildings%20after.svg" alt="2024-08-09 Satya Niketan buildings after.svg" loading="lazy">
   </p></div>
 </div>
<figcaption>Before and after - preparing buildings in Satya Niketan for a mapping party (using JOSM and the Terracer plugin)<figcaption>

</figcaption></figcaption></div>
</div>

<div id="outline-container-announcing-inviting">
<h3 id="announcing-inviting">Announcing and inviting</h3>
<div id="text-announcing-inviting">
<p>
Announce the party at least two weeks in advance. Make an event on <a href="https://osmcal.org/">https://osmcal.org/</a> and share it around. Reach out in
</p>
<ul>
<li>the <a href="https://lists.openstreetmap.org/listinfo">OpenStreetMap mailing lists</a></li>
<li>the <a href="https://community.openstreetmap.org/">OpenStreetMap Discourse forums</a></li>
<li>local OSM communities (the <a href="https://openstreetmap.community/">OpenStreetMap Community Index</a> may help you find them)</li>
<li>FOSS circles</li>
<li>LUGs (Linux User Groups)</li>
<li>GIS circles</li>
<li>colleges (especially data science and planning and architecture departments)</li>
<li>social media, etc.</li>
</ul>
<p>
By announcing and conducting parties regularly, you'll build up a community.
</p>

<p>
Outside of OpenStreetMap communities, people don't know what OpenStreetMap is, nor what a "mapping party" entails. Describe what you'll be doing, e.g. like <a href="https://old.reddit.com/r/delhi/comments/1byzz2e/7th_openstreetmap_mapping_meetup/">our Reddit announcement</a>.
</p>

<p>
Make sure to get the phone number of each participant. Talking over phone is the most reliable way to announce the party, send reminders and confirmations, and to coordinate during the party.<label id="fnr.3" for="fnr-in.3.3277734"><sup>3</sup></label><span><sup>3</sup>
And I say that as someone who doesn't like talking on phone, unless it's over Jabber/XMPP. Cell calls are very likely to always be recorded by the telecom provider.
</span> That way, there's no question of "didn't check my messages/email".
</p>

<p>
As a mapping party organizer, you are in the powerful position of choosing the platform for the community you are building. It's easy to pick a proprietary network like Telegram or an unsustainable one like Matrix, but for sake of your community's privacy and freedom, I strongly recommend avoiding that temptation and using <a href="https://wiki.openstreetmap.org/wiki/XMPP">Jabber/XMPP</a> instead.
</p>

<p>
We frequently onboard participants to XMPP using projects like <a href="https://quicksy.im/">Quicksy</a> and <a href="https://prav.app/">Prav</a>, which make onboarding and contact discovery easier. We don't bridge to other networks either, so everyone can enjoy a modicum of privacy and freedom.
</p>

<p>
You should join me and others at the <a href="https://join.jabber.network/#openstreetmap@conference.macaw.me?join">OpenStreetMap XMPP room</a> and the <a href="https://join.jabber.network/#osm-in@conference.a3.pm?join">OSM India XMPP room</a>.
</p>
</div>
</div>

<div id="outline-container-equipment">
<h3 id="equipment">Equipment</h3>
<div id="text-equipment">
<p>
We always ask participants to get some optional equipment - a charging cable, a power bank, and a tape measure of some sort.
</p>

<p>
Still, we find it useful to bring some spare devices.
</p>

<p>
Someone's phone develops a problem? I can lend them my old spare Android phone.
</p>

<p>
I thought I wouldn't need my own laptop at an indoor iD mapping party, but it ended up being used by a number of people - some who happened to be there and were encouraged by us to join in, and some who missed the memo that a laptop was required.
</p>

<p>
Get a power bank and charging cables for common ports. Somebody else's phone might run low on battery and you could lend them your power bank and cable.
</p>

<p>
Bring water and snacks on hot days.
</p>

<p>
All surveyors should carry pepper spray for emergencies, whether they involve human or non-human animals.<label id="fnr.4" for="fnr-in.4.6293918"><sup>4</sup></label><span><sup>4</sup>
Usually, aggressive pet dogs who are outside without a leash.
</span> India is not for beginners, as the meme goes.
</p>
</div>

<div id="outline-container-tape-measure">
<h4 id="tape-measure">A tape measure?</h4>
<div id="text-tape-measure">
<p>
A tape measure can be used to map a number of things, like <a href="https://wiki.openstreetmap.org/wiki/Key:maxwidth:physical">maxwidth:physical</a> for barriers and <a href="https://wiki.openstreetmap.org/wiki/Key:width:carriageway">width:carriageway</a> for streets.<label id="fnr.5" for="fnr-in.5.9790334"><sup>5</sup></label><span><sup>5</sup>
Especially important when you realize that OSM has no concept of "can a bicycle/motorcycle/car/truck [physically] pass through this street?", only whether one is legally allowed or not. Which is perfectly sensible (since these vehicles don't have a universal standard width), but it means you have to survey street widths to answer that question.
</span>
</p>

<p>
A tailor's tape is good for mapping small lengths like <a href="https://wiki.openstreetmap.org/wiki/Tag:barrier%3Dkerb">kerb</a> heights, <a href="https://wiki.openstreetmap.org/wiki/Tag:barrier%3Dbollard">bollard</a> gaps, widths of <a href="https://wiki.openstreetmap.org/wiki/Tag:barrier%3Dwicket_gate">pedestrian gates</a>, etc. (These are all tagged with <code>maxwidth:physical</code>.)
</p>

<p>
A metal tape measure (used by carpenters and plumbers) can do all that, and also measure somewhat larger distances such as widths of narrow streets. High-quality ones are stiff enough to <a href="https://www.youtube.com/watch?v=6517aSbzlyE">extend a significant distance without folding</a>, allowing you to measure without needing someone to hold the other end.<label id="fnr.6" for="fnr-in.6.2856921"><sup>6</sup></label><span><sup>6</sup>
This distance is known as the "standout" of a tape measure.
</span>
</p>

<p>
Surveyor's tape (which can be 100m or more in length) can be used for all of the above, and also for mapping widths of larger streets, buildings footprints, indoor rooms, boundaries, etc.
</p>

<p>
A laser distance meter (like the <a href="https://www.bosch-pt.co.in/in/en/products/glm-50-23-g-0601072VK0">Bosch GLM 50-23G</a>, which I have) is a more expensive alternative to the above. It allows measurements in hard-to-reach places, e.g. measuring the <code>maxheight:physical</code> of tall height restrictors, gates, bridges, etc. Some have ingress protection (IP) ratings, allowing for use in dusty and/or wet conditions. Some of them also have angle sensors, useful for accurate distance measurements and for measuring inclines of roads, wheelchair ramps, steps, etc. (Inclines can also be measured on modern smartphones without any other equipment, using free software such as <a href="https://f-droid.org/packages/org.woheller69.level/">Bubble</a>.)
</p>
</div>
</div>
</div>

<div id="outline-container-meeting">
<h3 id="meeting">Meeting</h3>
<div id="text-meeting">
<p>
We usually meet in a park near the mapping location. Ideally, the park will have a gazebo or a cluster of benches to seat everyone comfortably - if not, make do with a good patch of grass. In order to find a good location to meet, I've started surveying parks in the weeks leading up to a mapping party.
</p>

<p>
The park shouldn't be too far from the area to be mapped - you don't want people getting tired from just walking to the place. If you're regrouping at the same spot, they also need to get back after the survey.
</p>

<p>
It's important to fix a time for every component of the party and to stick to the schedule. In particular, don't wait for latecomers - it amounts to punishing those who came on time, and gives latecomers further incentive to come late. Participants don't have infinite time and energy, so waste not a minute.
</p>
</div>
</div>

<div id="outline-container-teaching">
<h3 id="teaching">Teaching</h3>
<div id="text-teaching">
<p>
OSM is a big rabbithole, so avoid trying to teach everything at once. Despite our best intentions, experienced mappers can overload people with information. Make a lesson plan each time and stick to it.
</p>

<p>
You may have to talk about some common issues before they happen. Some of them depend on the editor you'll use.
</p>
<ol>
<li>Don't copy from other maps</li>
<li>Use Title Case in names</li>
<li>The name field can be left blank - not everything has a name</li>
<li>What the address fields mean (beginners often fill <code>addr:street</code> with the full address)</li>
<li>The format for phone numbers</li>
</ol>
</div>
</div>

<div id="outline-container-mapping">
<h3 id="mapping">Mapping</h3>
<div id="text-mapping">
<p>
Split people into groups of 2-3 people, assign each group an area to cover, and agree upon meeting again in (say) one hour. Review everyone's changes.
</p>

<p>
This motivates people to contribute while also preventing duplication and conflicts. You could add a competitive aspect by announcing a winning group…but we've never done that.
</p>
</div>
</div>
</section>

<section id="outline-container-what-apps">
<h2 id="what-apps">What apps should we introduce?</h2>
<div id="text-what-apps">
<p>
This is a question I agonize over before each party and workshop.
</p>

<p>
<a href="https://organicmaps.app/">Organic Maps</a>? <a href="https://osmand.net/">OsmAnd</a>? <a href="https://streetcomplete.app/">StreetComplete</a>? <a href="https://every-door.app/">Every Door</a>? <a href="https://wiki.openstreetmap.org/w/index.php?title=Go_Map!!">Go Map!!</a> and <a href="https://vespucci.io/">Vespucci</a>? One, or many? And how many is too many?
</p>

<p>
(Note that I don't have a whole lot of experience with Go Map!!, as I don't own an iOS device. I only got to see it in action once in the IIT Roorkee workshop, and all I know is that it's the closest thing to StreetComplete or Vespucci on iOS.)
</p>
</div>

<div id="outline-container-old-advice">
<h3 id="old-advice">Old advice and new realizations</h3>
<div id="text-old-advice">
<p>
Remember when I said I may change my approach radically? Just a week ago, I would have advised -
</p>

<blockquote>
<p>
New mappers should start editing with Organic Maps. This gives them one easy app for both using and editing OSM.
</p>

<p>
Those who outgrow Organic Maps can progress to OsmAnd, Every Door, StreetComplete, and (for the most experienced ones) Vespucci/Go Map!!
</p>

<p>
You can also host an indoor party to teach iD.
</p>
</blockquote>

<p>
Organic Maps also adds your contributed POIs to its local map, which is cool - you can use your own changes right away.
</p>

<p>
That said, user <a href="https://www.openstreetmap.org/user/uknown-gryphus">uknown-gryphus</a> also points out that
</p>
<blockquote>
<p>
[…] Organic Maps can be very confusing if you updated the buildings and street network just before the mapping party. In Bengaluru we've had great success with EveryDoor. <a href="https://fieldpapers.org/">Field Papers</a> were also a lot of fun but a little confusing for beginners.
</p>
</blockquote>

<p>
Recently, however, I realized that StreetComplete, iD, Vespucci, Go Map!! and Every Door may be much more fun to work with than Organic Maps and OsmAnd. Thus, by not introducing them to newcomers, <i>I may be giving them a bad impression of OSM editing, and preventing the entry of new regular contributors.</i>
</p>
</div>
</div>

<div id="outline-container-streetcomplete">
<h3 id="streetcomplete">StreetComplete</h3>
<div id="text-streetcomplete">
<p>
StreetComplete gamifies OpenStreetMap contribution with its quests, scores, and achievements. Its icons and pictorial answers add visual interest. The wide variety of quests prevent editing from becoming monotonous.
</p>

<p>
StreetComplete is not limited to just editing existing data, either. The Places and Things overlays allow users to add shops, amenities, etc, and the address overlay allows users to add addresses even when there aren't any buildings.
</p>

<p>
That said, StreetComplete may not be suitable for all kinds of mapping parties. When you use StreetComplete, <i>what you add is determined by what quests are selectable.</i> The app dictates what you map, not the other way round.
</p>

<p>
That's good if your mapping party has no specific mapping goals in mind. In such cases, SC also has a nice team mode which divides quests among mappers working in the same area.
</p>

<p>
But this aspect is not useful for the kind of mapping parties we hold, where we want to exhaustively cover a specific set of features within an hour of surveying. <label id="fnr.7" for="fnr-in.7.572549"><sup>7</sup></label><span><sup>7</sup>
In my parties, I ask people to map shops/eateries (and their details), building addresses, and building levels (for those cool 3D buildings).
</span> So while we may mention it, it's not our primary choice for a mapping party editor.
</p>
</div>
</div>

<div id="outline-container-id,-vespucci,-go-map!">
<h3 id="id,-vespucci,-go-map!">iD, Vespucci, and Go Map!!</h3>
<div id="text-id,-vespucci,-go-map!">
<p>
iD, Vespucci, and Go Map!! allow users to draw ways. This is a lot like drawing (as in illustrating), which most people enjoy - there's something very satisfying about tracing roads, buildings, feature boundaries, etc, getting them just right, and admiring your handiwork.
</p>

<p>
That could be much more fun than contributing with Organic Maps or OsmAnd, which have the simpler but less interesting process of "add nodes and fill forms".
</p>

<p>
iD/Vespucci sessions can take many forms -
</p>
<ul>
<li>indoors or outdoors</li>
<li>remote mapping - in participants' neighborhoods, or in unmapped areas</li>
<li>walking around and creating OSM notes (or writing in <a href="https://fieldpapers.org/">Field Papers</a>), then regrouping to edit from them (this lets you inspect everyone's edits and provide feedback faster)</li>
</ul>


<figure id="org7f6f53b">
<a href="https://contrapunctus.codeberg.page/blog/img/mapping-party-tips/small/2024-03-17_iD_mapping_party.jpg" loading="lazy"><img src="https://contrapunctus.codeberg.page/blog/img/mapping-party-tips/orig/2024-03-17_iD_mapping_party.jpg" alt="2024-03-17_iD_mapping_party.jpg" loading="lazy"></a>

<figcaption><span>Figure 1: </span>Sahil teaching iD at our recent indoor mapping party at SFLC.in</figcaption>
</figure>

<p>
And of course you can also do mobile mapping sessions with Vespucci, focusing on one feature type per session to keep complexity down, e.g. -
</p>
<ol>
<li>shops, eateries, and amenities as tagged nodes</li>
<li>buildings (possibly with building details and addresses) as tagged nodes</li>
<li>building as areas</li>
<li>roads</li>
</ol>
</div>
</div>

<div id="outline-container-vespucci">
<h3 id="vespucci">Vespucci? Really?</h3>
<div id="text-vespucci">
<p>
Even though I use Vespucci for almost all my editing, I had my doubts about teaching it to newcomers. Many regular contributors find it difficult to get started with. It took me a few false starts myself, and it certainly has its rough edges.
</p>

<p>
But we must remember is that most of us who struggled to learn it (and sometimes succeeded) did not have the help of a teacher. As teachers, it's our job to craft a good learning experience - we can flatten steep learning curves, soften rough edges, and move stumbling blocks out of the way.
</p>

<p>
Simon Poole (the current lead developer of Vespucci) provided another interesting perspective. In response to my question -
</p>
<blockquote>
<p>
Has anybody tried introducing Vespucci to complete OSM newbies? What was your experience like? Or is it a Very Bad Idea™?
</p>
</blockquote>

<p>
He said<label id="fnr.8" for="fnr-in.8.1716693"><sup>8</sup></label><span><sup>8</sup>
I've edited it a bit. The emphasis is mine.
</span> -
</p>
<blockquote>
<p>
That is something that needs a multi-page-essay as an answer. But essentially the question is whether you want largest possible numbers, or long-term engagement.
</p>

<p>
Vespucci isn't geared towards beginners in the sense that it assumes you know your way around OSM data at least a bit. […] So you would need to cover those basics in an introduction.
</p>

<p>
But in general, just because something isn't straightforward doesn't mean it affects long-term engagement. If anything, it is the other way around - that is, <b>learning a skill will drive engagement, and not having to learn anything will lead to disinterest.</b>
</p>
</blockquote>
</div>
</div>

<div id="outline-container-organic-maps">
<h3 id="organic-maps">Organic Maps is still important</h3>
<div id="text-organic-maps">
<p>
That said, I think the "using the map motivates you to contribute" factor is also important.
</p>

<p>
So I would definitely still introduce Organic Maps to newcomers, mentioning its benefits (works offline, no account needed, no ads, no tracking, free software, and you can improve the map directly <label id="fnr.9" for="fnr-in.9.5738824"><sup>9</sup></label><span><sup>9</sup>
Rather than submitting suggestions to Google Maps and waiting for days, weeks, or months for them to be approved by whatever opaque approval system they use.
</span>), and then focus on dedicated editors.
</p>

<p>
I don't "install and walk-through" OsmAnd these days - rather, I mention it as an option for power users, along with why they may want to try it, e.g. -
</p>
<ul>
<li>hourly incremental updates</li>
<li>GPS track recording</li>
<li>multi-modal PT routing, especially share taxi routing,</li>
<li>aerial imagery</li>
<li>rendering of details like road surface, road smoothness, road access restrictions, locked gates, street lighting, etc</li>
<li>showing photos from Wikimedia Commons, etc.</li>
</ul>

<p>
Power users can then explore OsmAnd by themselves at home. (Thanks to Sahil for this insight.)
</p>
</div>
</div>

<div id="outline-container-every-door">
<h3 id="every-door">Every Door - our choice</h3>
<div id="text-every-door">
<p>
I haven't discussed Every Door yet, which the Bangalore community often uses and recommends. The advantages I see are -
</p>

<ol>
<li>It works on both Android and iOS. <label id="fnr.10" for="fnr-in.10.9481302"><sup>10</sup></label><span><sup>10</sup>
StreetComplete is only available on Android, although an iOS version is in the works. The OSM wiki for SC mentions that Go Maps!! supports StreetComplete quests, which may work as a workaround.
</span></li>

<li>It's not a full-fledged editor like Vespucci/iD/Go Map!!, so the interface is simpler, it's easier to learn, and new editors can't mess up ways or geometries.</li>

<li>It's the only "simple" mobile editor that can add building nodes with building details. If you want a 3D map of a neighbourhood, teach Every Door to the participants, and ask them to map <code>building:levels</code>. <label id="fnr.11" for="fnr-in.11.2232248"><sup>11</sup></label><span><sup>11</sup>
You can achieve this with StreetComplete by enabling the building levels quest, but somebody has to trace the buildings in the area in advance.
</span></li>

<li>It's the only "simple" mobile editor which can use aerial imagery out of the box. <label id="fnr.12" for="fnr-in.12.8061131"><sup>12</sup></label><span><sup>12</sup>
Satellite imagery in OsmAnd needs some setup.



In Vespucci, it requires dealing with the powerful-but-complex layer system. (The need for setup has hopefully been reduced with Vespucci v20's new introduction dialogue, which offers to enable satellite imagery.)



StreetComplete and Organic Maps don't support aerial imagery at all.
</span></li>

<li>It has some unique interfaces that can make life easier -

<ol>
<li>When you add a certain feature, it auto-applies the key-values you used for the last instance of that feature. This is unique to Every Door (as far as I know) and often speeds up editing significantly.</li>

<li>It has an easy interface for adding directions to e.g. surveillance cameras, which I haven't seen in any other editor. (Which is why I switch to Every Door for adding surveillance cameras.)</li>
</ol></li>

<li>It generates changeset comments for you, so you can upload frequently without breaking your flow.</li>

<li>It may provide more long-term engagement than editing in Organic Maps and OsmAnd, because of the satellite imagery, different modes, more operations and feature types, and richer forms.</li>
</ol>

<p>
There are some downsides, though -
</p>

<ol>
<li>There's no compass, which makes it harder to know where to place new data.</li>

<li><p>
You can rotate the map, but there's no indication that the map has been rotated, nor any way to reset it.
</p>

<p>
There's a workaround for this, but it's clunky - switch back to OSM tiles and get them the right way up. It snaps into the north-side-up orientation and resists rotation there - that's all the indication you currently get.
</p></li>

<li><p>
It does not support <a href="https://github.com/Zverik/every_door/issues/455#issuecomment-1763369436">addr:block</a>, <a href="https://github.com/Zverik/every_door/issues/663">addr:suburb, and addr:neighbourhood</a>, which makes it problematic for Indian addresses - unless your first-time mappers have no issue with adding tags manually, but that brings the editing experience to OsmAnd levels of raw key-value drudgery.
</p>

<p>
The best workarounds I can think of are -
</p>
<ol>
<li>leave address mapping to somewhat experienced editors</li>
<li>let new editors add addresses as notes; someone can add the data later</li>
</ol></li>

<li>There's no dark mode.</li>
</ol>

<p>
If you are not careful of the first two, you can get thrown off and make mistakes in editing. I saw this happen when we used it at our IndiaFOSS workshop.
</p>

<p>
Despite those flaws, Every Door currently seems to be the best option for us, and has been our mainstay for the last few mobile mapping parties.
</p>
</div>
</div>

<div id="outline-container-osm-website">
<h3 id="osm-website">openstreetmap.org</h3>
<div id="text-osm-website">
<p>
The website is an important resource to mention, so users can see their edit history, find nearby mappers, see how their changeset comments are used, how to respond to changeset discussions, etc.
</p>

<p>
You could demonstrate it at the end of the survey by asking everyone to check out their changes on the website, while also reviewing their changes and discussing mistakes.
</p>
</div>
</div>
</section>

<section id="outline-container-epilogue">
<h2 id="epilogue">Epilogue</h2>
<div id="text-epilogue">
<p>
That's all I have for now. I hope your mapping parties succeed in growing OpenStreetMap's community and data coverage. Happy mapping [party organizing]!
</p>

<nav>
 <a href="https://contrapunctus.codeberg.page/blog/index.html">↩ blog</a> 
 </nav>

</div>
</section>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A 30-year old Rabbit Telepoint base station at Seven Sisters tube station (311 pts)]]></title>
            <link>https://www.ianvisits.co.uk/articles/theres-a-dead-rabbit-in-seven-sisters-tube-station-71502/</link>
            <guid>40009856</guid>
            <pubDate>Fri, 12 Apr 2024 06:14:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ianvisits.co.uk/articles/theres-a-dead-rabbit-in-seven-sisters-tube-station-71502/">https://www.ianvisits.co.uk/articles/theres-a-dead-rabbit-in-seven-sisters-tube-station-71502/</a>, See on <a href="https://news.ycombinator.com/item?id=40009856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<!-- Article Start -->
					
<p>For over thirty years, a dead rabbit has hung inside Seven Sisters tube station, and thousands of people walk past it every day without noticing.</p>
<p><a href="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg"><picture><source srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg.webp 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg.webp 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg.webp 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg.webp 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg.webp 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg.webp 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg.webp 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg.webp 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg.webp 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg.webp 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg.webp 1800w" sizes="(max-width: 605px) 100vw, 605px" type="image/webp"><img fetchpriority="high" decoding="async" src="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg" alt="" width="605" height="336" loading="none" srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg 1800w" sizes="(max-width: 605px) 100vw, 605px" data-eio="p"></picture></a></p>
<p>The dead rabbit is a legacy of an early form of mobile phone technology, albeit one that lasted less than two years before closing down.</p>
<p>We need to jump back to 1989, when the government awarded four licenses to operate Telepoint services, with the aim that their lower costs would offer competition to the country’s two mobile networks – Cellnet (now O2) and Vodafone.</p>
<p>At a time when the two mobile networks had 500,000 customers between them, it was <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;clickref=rabbit&amp;clickref2=article&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2Fviewer%2Fbl%2F0000540%2F19890127%2F238%2F0015">expected</a> these Telepoint phones would have as many as seven million customers by the middle of the 1990s.</p>
<p>Rabbit was created by the Hong-Kong based conglomerate, Hutchison, which didn’t have a license to operate a telepoint service, so they bought one of the four companies that did. However, by the time the company was ready to launch Rabbit to the world, the other three companies — Mercury Callpoint, Ferranti’s Zonephone and BT’s Phonepoint — had already <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;clickref=rabbit&amp;clickref2=article&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2Fviewer%2Fbl%2F0003740%2F19911003%2F011%2F0011">closed down</a>.</p>
<p>Rabbit pushed on and, with a huge marketing blitz, finally started appearing in the shops in May 1992.</p>
<p><a href="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02.jpg"><picture><source srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-611x1024.jpg.webp 611w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-358x600.jpg.webp 358w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-768x1288.jpg.webp 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-916x1536.jpg.webp 916w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-100x168.jpg.webp 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-150x252.jpg.webp 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-200x335.jpg.webp 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-300x503.jpg.webp 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-450x755.jpg.webp 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-600x1006.jpg.webp 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-900x1509.jpg.webp 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02.jpg.webp 1052w" sizes="(max-width: 605px) 100vw, 605px" type="image/webp"><img decoding="async" src="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-611x1024.jpg" alt="" width="605" height="1014" srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-611x1024.jpg 611w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-358x600.jpg 358w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-768x1288.jpg 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-916x1536.jpg 916w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-100x168.jpg 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-150x252.jpg 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-200x335.jpg 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-300x503.jpg 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-450x755.jpg 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-600x1006.jpg 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-900x1509.jpg 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02.jpg 1052w" sizes="(max-width: 605px) 100vw, 605px" data-eio="p"></picture></a></p>
<p>Despite the fact that it could only make calls within 100 yards of a base station and couldn’t receive calls (you could get a pager telling you to call someone) — in theory, it should have been a success.</p>


<p>At the time, the two mobile networks were for business people and <a href="https://en.wikipedia.org/wiki/Yuppie">Yuppies</a>, and were very expensive to use, so even the restrictive Rabbit phones would have an edge as they were much cheaper to make calls on – typically 50p a minute on a basic cellphone tariff vs 20p a minute on Rabbit. And cheaper monthly rentals.</p>
<p>The other factor is that the Rabbit phones could also be used at home.</p>
<p>Many homes had a <a href="https://www.britishtelephones.com/cordles3.htm">cordless phone</a>, but these domestic analogue radio-based cordless phones were large and frankly pretty poor quality, with calls often dropping out. The Rabbit phones came with a home base station and used early digital phone technologies, so the sound quality was considerably better. And the handset was much smaller than the home cordless phone.</p>
<p>So, when Rabbit hopped into the shops in May 1992, sales were expected to be brisk, and indeed, they were. Initially.</p>
<p>I worked in a shop selling them, and from memory the mobile calling function was only about half the appeal — it was the substantially better home phone function that really appealed to people when they heard about it from friends.</p>
<p>Rabbit launched with base stations on shop fronts and lamposts across Manchester and quickly spread, reaching nationwide coverage by late 1993.</p>
<p>Then it abruptly closed down.</p>
<p>On a day to remember, Friday 5th November, Hutchison Telecom suddenly <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;clickref=rabbit&amp;clickref2=article&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2Fviewer%2Fbl%2F0005278%2F19931107%2F714%2F0050">announced</a> that it was shutting down the Rabbit service. Hutchison Telecom had recently been granted a license to build a full mobile network and wasn’t interested in its old Telepoint based service anymore.</p>


<p>Customers who owned a Rabbit phone were given a refund, and a promise of a discount on Hutchison’s replacement, a GSM based mobile phone network — Orange (now EE).</p>
<p>The bunny had flopped. The future was Orange.</p>
<p>The announcement that Rabbit was closing down sparked something totally unexpected — a surge in sales.</p>
<p>Remember how I mentioned that the phones worked at home as a replacement for the clunky cordless phone — well, that wasn’t dependent on the Rabbit base stations, so the handsets would keep on working in the home after the mobile network was shut down.</p>
<p>And people really liked the Rabbit phone for their homes.</p>
<p>Awkwardly, customers offered a refund had to return the handsets, and many chose not to, while shops were told to return unsold stock to the warehouses, and many chose not to.</p>
<p>Hutchison’s attempt to kill the rabbit dead was struggling for life.</p>
<p>In the end, shops ran out of rabbit stock, and in December 1993, the Rabbit base stations switched off forever.</p>
<p>However, like many items of street furniture, what is installed in haste can take ages to remove at leisure. Unless the location is needed for something else, old signs and clutter can linger on for years, decades even.</p>
<p>So, inside <a href="https://www.ianvisits.co.uk/articles/tag/seven-sisters-station/">Seven Sisters tube station</a>, there’s still a Rabbit base station sitting on the wall, more than 30 years after it last broadcast a radio signal.</p>
<p>If you want to find the dead rabbit, just hop down to Seven Sisters tube station’s High Road entrance ticket hall. You can find it right next to the escalators to the Victoria line.</p>
<p><a href="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg"><picture><source srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg.webp 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg.webp 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg.webp 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg.webp 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg.webp 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg.webp 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg.webp 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg.webp 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg.webp 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg.webp 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg.webp 1800w" sizes="(max-width: 605px) 100vw, 605px" type="image/webp"><img decoding="async" src="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg" alt="" width="605" height="336" srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg 1800w" sizes="(max-width: 605px) 100vw, 605px" data-eio="p"></picture></a></p>
<p>There’s also another one, in <a href="https://twitter.com/ianvisits/status/1695383779659948112">rather better condition</a>, <span>in the waiting room on platform 7/8 at Watford Junction station.</span></p>




										


								
					



					

									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Threads on Mastodon and the bright future of the Fediverse (116 pts)]]></title>
            <link>https://www.augment.ink/threads-on-mastodon/</link>
            <guid>40009148</guid>
            <pubDate>Fri, 12 Apr 2024 03:32:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.augment.ink/threads-on-mastodon/">https://www.augment.ink/threads-on-mastodon/</a>, See on <a href="https://news.ycombinator.com/item?id=40009148">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <p>Last month, Meta's Threads took its <a href="https://engineering.fb.com/2024/03/21/networking-traffic/threads-has-entered-the-fediverse/?ref=augment.ink" rel="noreferrer">first step into the Fediverse</a>, a promise they made to users at launch. While I don't want to dive too far into the technology, which is something I will do in a future post, this basically means I can follow Threads users that opt-in to Fediverse integration through my Mastodon account. </p><p>So, I did what any "normal" person would do: I went to my Mastodon account and added anyone I follow on Threads who had their Fediverse switch turned on so I could see their posts on Mastodon. And then I started taking notes. </p><figure><img src="https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExYjg5bHEyc3Z6amJicDFjYTNwcTNxMjBsNjFyMDh0dGhtZ2M4ZGh2aSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/XyOZIlAvsNbIwEoMvW/giphy.gif" alt="totally normal" loading="lazy" width="480" height="270"></figure><p>This is my journey that started as an experiment to see how my Threads feed would look like on Mastodon and ended with me finding experiences that went above and beyond my expectations.</p><h2 id="nothing-is-perfect">Nothing is Perfect</h2><p>To start, this post isn't a long-winded way of me saying I hate the Threads experience. On the contrary, they've made some delightful choices. It's why I've been so active on the platform for this long. </p><p>My current need to look outward is more focused on what they <em>haven't</em> built and likely never will. Nothing is perfect for everyone and neither is Threads.</p><figure><img src="https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExaWpjMTc4aTM5azJnNGU3d3Fwd25sMnVxZXQ4eWZueG1ibGg3ZzMzcyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/l3mZo5YSJXPi9rtV6/giphy.gif" alt="Nobodys Perfect said Pobody's Nerfect" loading="lazy" width="500" height="250"></figure><p>Firstly, the app always defaults to the For You feed. I think this makes sense for most users, but it would be nice if power users were given the ability to default to a chronological feed.</p><p>Speaking of the chronological feed - frankly, it's not useful. It skips posts without explanation and there are no replies, which means no conversations in that feed. You only see replies serendipitously in the For You feed so only Threads can decide which conversations you take part in.</p><p>There are also no lists. From the way it sounds, there will never be lists. I can go on for hours about what makes lists useful - and boy, have I - but I'll save that for another day. The point is, I use lists to make sense of the chronological feed - almost like a hand-crafted For You feed that...no, we're not doing this today.  The point is, I need lists and Threads doesn't seem to be interested in catering to that need.</p><p>Finally, the main reason I'm considering my options is the limited developer API. To put it short: there will be no third-party clients or augmentations of your Threads feed. The feeds Threads gives you are the feeds you'll have to live with. And, as I mentioned above, those feeds are not doing what I need them to do. </p><p>And so, I looked to the next biggest thing that will let me interact with Threads while giving me more options for how I consume my content: Mastodon.</p><h2 id="known-limitations">Known Limitations</h2><p>In this first phase, however, there is an important issue that will be ironed out once integration is complete: while Mastodon users can interact with Threads users, Threads users cannot interact back. That means: </p><ol><li>Threads users can't reply to Mastodon comments or see them in Threads.</li><li>Threads users can be followed by Mastodon accounts but they will not be notified nor can they follow back.</li><li>Likes from Mastodon are not counted toward your Threads like count</li></ol><p>Moreover, as of writing this post, 189 out of 1524 of my Threads follow list is captured. While there may be more than what I currently have, that's about 12% of my follow list. Based on my conversations with various users, it seems like there are three popular reasons for why this may be the case:</p><ol><li>It isn't rolled out in their region yet but they plan on switching it on.</li><li>They don't like the one-way integration and are waiting for full interaction with Mastodon users.</li><li>They don't like that there's a chance their post will remain indexed on Mastodon even if they delete it.</li></ol><p>The first two will be solved, but the last one isn't going to be easy and it's not in Meta's hands. I hope this changes in the future but I would understand if certain users didn't want to be spread across the internet that way.</p><p>But, knowing all this, I pushed forward to customize my experience.</p><h2 id="organizing-the-chaos">Organizing the Chaos</h2><p>So here I was - a Mastodon user who has about half his following list sourced from Threads. I started by moving all my Threads follows <a href="https://www.threads.net/@quillmatiq/post/C5TtlzKRxEq?ref=augment.ink" rel="noreferrer">into a separate list</a> so that it wouldn't fill my main Mastodon feed. I used the handy <a href="https://www.mastodonlistmanager.org/?ref=augment.ink" rel="noreferrer">Mastodon List Manager</a>, segregated my Follows by domain, and added the whole threads.net section to the Threads list. Then I switched on the Advanced UI (think TweetDeck for Mastodon) to get my real-time multi-column feed. </p><p>The reason for separating Threads users was to see how a real-time chronological feed for my Threads would actually look like. I also had to interact with Threads posts differently since it's only one-way interaction for this phase - I had to pop out of the Mastodon experience by opening the post on Threads and interacting there. Luckily, most Mastodon clients - including the default one - make this pretty easy and by separating my Threads follows in an independent list, I could be consistent with that workflow.</p><p>As expected, the chronological feed became overwhelming to follow. But there was an easy solution - I used lists. This made my feed easier to consume and it enabled me to focus on specific topics based on my mood. When I wanted to chat with online friends in the Tech Threads community, I would stay in a feed dedicated to that. If I was catching up on tech news, I would stay in that feed. When I was ready for heavier news, I would head over to my politics feed.</p><figure><img src="https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExOGY5NHcxZDd2bjU0YTRoNm45cGg0a2hwazN3OTJpOXkxcXhtbDFpNSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Ph5ELYJov9n5oHzVHZ/giphy.gif" alt="He Man I have the power" loading="lazy" width="480" height="360"></figure><p>It was nice to able to decide what kind of content I wanted to consume rather than getting an algorithm's scrambled egg of the day that doesn't consider what mood I'm in. But not all was perfect in my Mastodon experience, even if I put aside the one-way Threads limitation. </p><p>The main downside was that its UI is utility-first, design second. And coming from the polished Threads experience it left me a little unsatisfied. For instance: if a user does a multi-post (a thread), they come in reverse chronological order; the same is for conversations. In my opinion, threads and conversations should be grouped together in the UI to keep context. This, along with some other nits, left me wanting more.</p><h2 id="the-client-search">The Client Search</h2><p>But there's a major benefit to Mastodon: it provides a full API so that third-party developers can augment the experience for specific needs, even niche ones like mine. The lists manager I spoke about earlier is one example of this. <a href="https://flipboard.helpshift.com/hc/en/1-flipboard/faq/1565-use-mastodon-inside-flipboard-ios-android/?ref=augment.ink" rel="noreferrer">Flipboard is another</a> for a links-focused view. And the many - <a href="https://joinmastodon.org/apps?ref=augment.ink" rel="noreferrer">there are SO many</a> - third-party clients are all examples of how a user's needs can be met even if the default options don't.</p><figure><img src="https://64.media.tumblr.com/5bd40d0fb0123d6d654cef1ab126e34f/tumblr_n2g1z1V1HR1tqvq6co1_250.gifv" alt="The wilderness must be explored!" loading="lazy" width="245" height="245"></figure><p>With an ample amount of choices, I set out to find a client that matched my needs. I knew I wanted a few things that the Threads and Mastodon core experiences weren't giving me: a prettier UI, a real-time feed, and better feed grouping.</p><p>And I had a final ask: an easy way to catch-up if I hadn't looked at my feed for, say, 8 hours (a boy's gotta sleep, right?). What I wanted was something that did what a For You feed does but doesn't leave me endlessly scrolling. I figured I would find or build a separate tool since most clients would likely not serve that and there was no reason to limit my search for something so niche.</p><p>There were numerous options that I enjoyed using quite a bit. To keep things short, here were my favorites:</p><ol><li><a href="https://elk.zone/?ref=augment.ink" rel="noreferrer">Elk</a> - a minimalist PWA that makes some fantastic choices for the UX</li><li><a href="https://play.google.com/store/apps/details?id=org.joinmastodon.android.moshinda&amp;hl=en_US&amp;gl=US&amp;ref=augment.ink" rel="noreferrer">Moshidon</a> - an Android app that makes it easy to swipe between lists</li><li><a href="https://play.google.com/store/apps/details?id=allen.town.focus.mastodon&amp;hl=en_US&amp;gl=US&amp;ref=augment.ink" rel="noreferrer">Focus</a> - a highly customizable Android app that includes a widget</li></ol><p>I don't have an iOS device, but I've heard <a href="https://getmammoth.app/?ref=augment.ink" rel="noreferrer">Mammoth</a>, <a href="https://apps.apple.com/us/app/ice-cubes-for-mastodon/id6444915884?ref=augment.ink" rel="noreferrer">Ice Cubes</a>, and <a href="https://apps.apple.com/us/app/ivory-for-mastodon-by-tapbots/id6444602274?ref=augment.ink" rel="noreferrer">Ivory</a> are all great options as well. But, as good as these experiences were, in one way or another they didn't scratch my itch.</p><p>And then, <a href="https://phanpy.social/?ref=augment.ink" rel="noreferrer">I met Phanpy</a>.</p><h2 id="hello-phanpy">Hello, Phanpy</h2><p>From the moment I saw Phanpy, I knew this checked far more boxes than I was expecting any client to.</p><figure><img src="https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExaDhoNWR2cXo0aGM4MTN3YWc1NWtscWltbzh4eGh1MTV1bmlqanhjYSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/NiOPyn6a7tV3q/giphy.gif" alt="crying It's so beautiful" loading="lazy" width="300" height="169"></figure><p>Phanpy can be as simple or a complex as you want it to be. Want a single feed that refreshes when you explicitly ask it to? Default. Want multiple-columns hidden behind a tab bar? Easy. Want a multi-column, real-time chronological feed? Go for you extremely-online feed addict (it me).</p><figure><img src="https://www.augment.ink/content/images/2024/04/image.png" alt="Phanpy's Column Options" loading="lazy" width="537" height="223"><figcaption><span>Phanpy's column options</span></figcaption></figure><p>It most certainly doesn't stop there. As it brings new posts in real-time, it also groups conversations and threads so you don't have to click around to get all the context you need to understand a single post. Here are two examples: the first is a thread with replies in between and the second is a three-post thread.</p><figure><img src="https://www.augment.ink/content/images/2024/04/MixCollage-09-Apr-2024-12-31-PM-5405-crop.jpg" alt="Conversations and threads in Phanpy" loading="lazy" width="2000" height="2074" srcset="https://www.augment.ink/content/images/size/w600/2024/04/MixCollage-09-Apr-2024-12-31-PM-5405-crop.jpg 600w, https://www.augment.ink/content/images/size/w1000/2024/04/MixCollage-09-Apr-2024-12-31-PM-5405-crop.jpg 1000w, https://www.augment.ink/content/images/size/w1600/2024/04/MixCollage-09-Apr-2024-12-31-PM-5405-crop.jpg 1600w, https://www.augment.ink/content/images/2024/04/MixCollage-09-Apr-2024-12-31-PM-5405-crop.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>Conversations and threads in Phanpy</span></figcaption></figure><p>And sometimes there's multiple conversations happening at once - here's an example of a lively discussion under a post:</p><figure><img src="https://www.augment.ink/content/images/2024/04/Media_240409_193841-1.gif" alt="Multiple conversations under a post on Phanpy" loading="lazy" width="370" height="800"><figcaption><span>Multiple conversations under a post on Phanpy</span></figcaption></figure><p>Clean and easy to understand. And when a reply pops up in the feed, it brings contextual groups with it. It makes the chronological feed so much easier to follow, especially when you're running a multi-column dashboard. To make things cleaner, original posts, replies, and reposts are all color coded to white, yellow, and purple respectively which helps break down the complex nature of microblogging feeds.</p><figure><img src="https://www.augment.ink/content/images/2024/04/image-4.png" alt="Post in white, reply in yellow, and boost in purple" loading="lazy" width="327" height="816"><figcaption><span>Post in white, reply in yellow, and boost in purple</span></figcaption></figure><p>So I'd found it: a pretty client that grouped posts in a real-time feed while making it easy for me to pop in and out of the Threads UI. And then Phanpy threw this in front of me:</p><figure><img src="https://www.augment.ink/content/images/2024/04/ezgif-7-ebd1781fdc.gif" alt="Phanpy's &quot;Catch Up&quot; Feature" loading="lazy" width="600" height="1300" srcset="https://www.augment.ink/content/images/2024/04/ezgif-7-ebd1781fdc.gif 600w"><figcaption><span>Phanpy's "Catch Up" Feature</span></figcaption></figure><p>What you're seeing in the GIF above is "Catch Up", a Beta feature of Phanpy that collects all the posts in your feed between 1-12 hours (configurable per catch-up) and organizes them in a more sane way. You can filter by user or by post type (single, repost, or reply), sort by various attributes, and it extracts out all the links so you can see what popular news stories were filling your timeline for that period of time.</p><p>It's doing what a For You feed attempts to do but gives you the power of how you consume it. It also saves these sessions so you can go back when you're ready to check different parts of it, kind of like how I use my Tech and Politics lists. I typically run this in the morning to catch up while having my morning coffee. I also use it whenever I'm away from my feed for a while - working, social events, family time etc.</p><figure><img src="https://memes.memedrop.io/production/Nm08LXeX8Loe/source.gif" alt="No mo FOMO" loading="lazy" width="500" height="281"></figure><p>Somehow, I found a client that even hit my stretch goals. Did I mention it's a PWA so you don't have to download any apps? It just keeps getting better.</p><p>Like everything, Phanpy isn't perfect. For one, Threads posts federate 5m after posting but Phanpy will re-organize the feed chronologically. So if there's a Mastodon post that's been around for 2m and a Threads post makes it to my feed, the Threads post gets inserted underneath the Mastodon one. It makes sense but it's made me keep my Mastodon and Threads real-time feeds separate for the most part.</p><p>Phanpy also doesn't sync across devices which means that my preferences needed to be set up on each device I use it on. There's an experimental feature that does this, but since it uses my Mastodon profile notes I decided to hold off until it's production-ready.</p><p>There are some other limitations but overall, Phanpy hits enough notes for me that it's hard to complain. The code is also <a href="https://github.com/cheeaun/phanpy?ref=augment.ink" rel="noreferrer">completely open source</a> so there's nothing stopping me from pulling it, making some changes, and hosting a bespoke version of the app just for me. Which I will likely do if I decide to move to a Mastodon instance once Threads completely federates.</p><h2 id="what-now">What now?</h2><figure><img src="https://media1.tenor.com/m/Bd0bQ_cYoY0AAAAC/ted-lasso-tv-show.gif" alt="Ted Lasso decisions decisions" loading="lazy" width="498" height="402"></figure><p>At this point, I haven't decided if I'll move my account off of Threads after federation is complete. I think there's a lot going for Threads, especially on the experience front. I also have a feeling that leaving Threads will mean leaving behind some people I follow since I'm pretty sure not everyone is going to turn the switch on, especially if Threads never adds the option to turn it on during the onboarding experience. I've made a lot of friends there and I would hate to cut the cord completely. But after experiencing Phanpy, it'll be very hard to go back.</p><p>Meta's goal was always to build something Twitter-like, but for a billion users. The reason Twitter never hit that peak is because normal users didn't understand it. From what I can tell, the design decisions Threads is making are for those consumers: a single For You algorithm, no real-time feed, and - say it with me - no lists. This basic experience will suffice for the 90% of users that mostly lurk, comment, and like and don't want all the hurdles of understanding how any of it works. They're the ones that open an app like Threads once every few hours, scroll until they're done with it, then move on.</p><p>If that's their goal, then - in my opinion - they're succeeding in making all the right decisions for that user.</p><p>Unfortunately, I am not that user. I know many people on Threads who are not that user. And I appreciate that Meta has opened itself up to let users like me interact with and send content to Threads from an experience that works for them without being limited to Meta's vertical experience. What a time to be alive, right?</p><p>After seeing the already-rich ecosystem of third-party clients - many of whom serve power-users and creators much better than Threads currently does - along with Meta's decision not to allow third-party clients at all, I'm heavily leaning toward Phanpy for Mastodon for the long-term once Threads federates. </p><p>Honestly, though - would lists <em>really</em> be that bad, Meta?</p><figure><img src="https://i.pinimg.com/originals/8a/81/23/8a812348b181e255cdaf984cf180a265.gif" alt="please?" loading="lazy" width="400" height="264"></figure>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a Linux Container Using Namespaces: Part – 1 (2020) (203 pts)]]></title>
            <link>https://www.polarsparc.com/xhtml/Containers-1.html</link>
            <guid>40008841</guid>
            <pubDate>Fri, 12 Apr 2024 02:22:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.polarsparc.com/xhtml/Containers-1.html">https://www.polarsparc.com/xhtml/Containers-1.html</a>, See on <a href="https://news.ycombinator.com/item?id=40008841">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <br>
    
    
    <p>Building a Linux Container using Namespaces :: Part - 1</p>
    <br>
    
    <hr> 
    <p>Overview</p>
    <div id="para-div">
      <p>Ever wondered how Linux <span>Container</span>s worked ???</p>
      <p>Currently, <a href="https://www.polarsparc.com/xhtml/Docker.html" target="_blank"><span>Docker</span></a>
        is one of the most popular and prevalent container implementations.</p>
      <p>Containers run on top of the same Operating System kernel, but isolate the application processes running inside them
        from one another. One of the secret sauces behind containers is <a href="http://man7.org/linux/man-pages/man7/namespaces.7.html" target="_blank"><span>Namespaces</span></a>.</p>
      <p>A <span>Namespace</span> abstracts global system resources, such as, host names, user IDs, group IDs,
        process IDs, network ports, etc., in a way that it appears to the processes (within the namespace) as though they have
        their own isolated instance of the global system resources. One of the primary goals of namespaces is to support the
        implementation of containers (lightweight virtualization).</p>
      <p>Currently, in Linux there are <span>6</span> types of namespaces - <span>IPC</span>,
        <span>Network</span>, <span>Mount</span>, <span>PID</span>,
        <span>User</span>, and <span>UTS</span>.</p>
    </div>
    <div id="para-div">
      <p>The following are brief descriptions for each of the namespaces:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p><span>IPC</span> :: This namespace isolates certain interprocess communication (IPC) resources,
            namely, Message Queues, Semaphores, and Shared Memory</p>
        </li>
        <li>
          <p><span>Network</span> :: This namespace provides isolation of the system resources associated with
            networking, such as, Network devices, IP addresses, IP routing tables, /proc/net directory, port numbers, and so on</p>
        </li>
        <li>
          <p><span>Mount</span> :: This namespace isolates the set of filesystem mount points seen by a group
            of processes. Processes in different mount namespaces can have different views of the filesystem hierarchy</p>
        </li>
        <li>
          <p><span>PID</span> :: This namespace isolates the process ID number space. This allows processes
            in different PID namespaces to have the same PID</p>
        </li>
        <li>
          <p><span>User</span> :: This namespace isolates the user and group ID number spaces, such that, a
            process's user and group IDs can be different inside and outside the user namespace</p>
        </li>
        <li>
          <p><span>UTS</span> :: This namespace isolates two system identifiers — the hostname and the
            domainname. For containers, the UTS namespaces allows each container to have its own hostname and NIS domain name</p>
        </li>
      </ul>
      <p>For the demonstration in this article, we will be using the <span>unshare</span> Linux command
          as well as implement, build, and execute a simple container using <span>golang</span>.</p>
    </div>
    <p>Installation and Setup</p>
    <p>The installation is on a <span>Ubuntu 18.04 LTS</span> based Linux desktop.</p>
    <div id="para-div">
      <p>We will need two commands <span>newuidmap</span> and <span>newgidmap</span>
        to demonstrate <span>User</span> namespace. For this, we need to install the package
        <span>uidmap</span>.</p>
      <p>To install the package <span>uidmap</span>, execute the following command:</p>
    </div>
    <p>$ sudo apt install -y uidmap</p>
    <div id="para-div">
      <p>Next, we will need the <span>brctl</span> command to create a <span>bridge</span>
        network interface. For this, we need to install the package <span>bridge-utils</span>.</p>
      <p>To install the package <span>bridge-utils</span>, execute the following command:</p>
    </div>
    <p>$ sudo apt install -y bridge-utils</p>
    <div id="para-div">
      <p>To develop, build, and execute the simple container in <span>go</span> programming language, we
        need to install the <span>golang</span> package.</p>
      <p>To check the version of <span>golang</span> available to install, execute the following command:</p>
    </div>
    <p>$ sudo apt-cache policy golang</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>golang:
  Installed: (none)
  Candidate: 2:1.13~1ubuntu1ppa1~bionic
  Version table:
 *** 2:1.13~1ubuntu1ppa1~bionic 500
        500 http://ppa.launchpad.net/hnakamur/golang-1.13/ubuntu bionic/main amd64 Packages
        500 http://ppa.launchpad.net/hnakamur/golang-1.13/ubuntu bionic/main i386 Packages
        100 /var/lib/dpkg/status
     2:1.10~4ubuntu1 500
        500 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages</pre>
    </div>
    <p>To install <span>golang</span>, execute the following command:</p>
    <p>$ sudo apt install -y golang</p>
    <p>The above installation procedure installs <span>golang</span> from the official Ubuntu repository.</p>
    <p>Create a directory for developing, building, and running <span>go</span> programs by executing the
        following commands:</p>
    <div id="cmd-div">
      <p>$ mkdir $HOME/projects/go</p>
      <p>$ export GOPATH=$HOME/projects/go</p>
    </div>
    
    <p>We will need one of the popular <span>go</span> packages on <span>netlink</span>
        for networking.</p>
    <p>To download the <span>go</span> package, execute the following command:</p>
    <p>$ go get github.com/vishvananda/netlink</p>
    <p>Open two <span>Terminal</span> windows - we will refer to them as <span>TA</span>
        and <span>TB</span> respectively. <span>TB</span> is where we will demonstrate the
        simple container.</p>
    <div id="para-div">
      <p>We need to download a minimal root filesystem (<span>rootfs</span>) that will be used as the base
        image for the simple container. For our demonstration, we will choose the latest <span>
        <a href="http://cdimage.ubuntu.com/ubuntu-base/releases/18.04.4/release/ubuntu-base-18.04.4-base-amd64.tar.gz" target="_blank"><span>Ubuntu Base 18.04.4 LTS</span></a></span> at the time of this article.</p>
      <p>We will assume the latest Ubuntu Base is downloaded to the directory <span>$HOME/Downloads</span>.</p>
    </div>
    <p>Hands-on with Namespaces</p>
    <p>UTS Namespace</p>
    <p>The <span>unshare</span> command executes the specified program with the indicated namespace(s)
        isolated from the parent process.</p>
    <p>To display the options for the <span>unshare</span> command, execute the following command in
        <span>TA</span>:</p>
    <p>$ unshare -h</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>Usage:
 unshare [options] [&lt;program&gt; [&lt;argument&gt;...]]

Run a program with some namespaces unshared from the parent.

Options:
 -m, --mount[=&lt;file&gt;]      unshare mounts namespace
 -u, --uts[=&lt;file&gt;]        unshare UTS namespace (hostname etc)
 -i, --ipc[=&lt;file&gt;]        unshare System V IPC namespace
 -n, --net[=&lt;file&gt;]        unshare network namespace
 -p, --pid[=&lt;file&gt;]        unshare pid namespace
 -U, --user[=&lt;file&gt;]       unshare user namespace
 -C, --cgroup[=&lt;file&gt;]     unshare cgroup namespace
 -f, --fork                fork before launching &lt;program&gt;
     --mount-proc[=&lt;dir&gt;]  mount proc filesystem first (implies --mount)
 -r, --map-root-user       map current user to root (implies --user)
     --propagation slave|shared|private|unchanged
                           modify mount propagation in mount namespace
 -s, --setgroups allow|deny  control the setgroups syscall in user namespaces

 -h, --help                display this help
 -V, --version             display version</pre>
    </div>
    <p>Each process (with [PID]) has associated with it a sub-directory <span>/proc/[PID]/ns</span> that
        contains one entry for each of the namespaces.</p>
    <p>To list all the namespaces associated with a process, execute the following command in <span>TA</span>
        :</p>
    <p>$ ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>total 0
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 alice alice 0 Mar  7 20:41 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 user -&gt; 'user:[4026531837]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 uts -&gt; 'uts:[4026531838]'</pre>
    </div>
    <p>To launch a simple container whose host name is isolated from the parent host name, execute the following command in
        <span>TB</span>:</p>
    <p>$ sudo unshare -u /bin/sh</p>
    <div id="para-div">
      <p>The <span>-u</span> option enables the <span>UTS</span> namespace.</p>
      <p>The command prompt will change to a <span>#</span>.</p>
    </div>
    <p>To check the <span>PID</span> of the simple container, execute the following command in <span>
        TB</span>:</p>
    <p># echo $$</p>
    <p>The following would be a typical output:</p>
    
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p># ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar  7 12:36 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 user -&gt; 'user:[4026531837]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 uts -&gt; 'uts:[4026533064]'</pre>
    </div>
    <p>Comparing Output.5 to Output.3, we see a change in the <span>uts</span> namespace, which is expected
        and correct.</p>
    <p>To change the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p># hostname leopard</p>
    <p>To display the host name of the parent host, execute the following command in <span>TA</span>:</p>
    <p>$ hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p># hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>This demonstrates to us that we have isolated the host name of the simple container from the parent host name.</p>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p>Next, we will mimic the above <span>UTS</span> namespace isolation using the following <span>
        go</span> program:</p>    
    <fieldset id="sc-fieldset">
      <legend>Listing.1</legend>
      <pre>package main

import (
    "log"
    "os"
    "os/exec"
    "syscall"
)

func execContainerShell() {
    log.Printf("Ready to exec container shell ...\n")

    if err := syscall.Sethostname([]byte("leopard")); err != nil {
       panic(err)
    }

    const sh = "/bin/sh"

    env := os.Environ()
    env = append(env, "PS1=-&gt; ")

    if err := syscall.Exec(sh, []string{""}, env); err != nil {
        panic(err)
    }
}

func main() {
    log.Printf("Starting process %s with args: %v\n", os.Args[0], os.Args)

    const clone = "CLONE"

    if len(os.Args) &gt; 1 &amp;&amp; os.Args[1] == clone {
        execContainerShell()
    }

    log.Printf("Ready to run command ...\n")

    cmd := exec.Command(os.Args[0], []string{clone}...)
    cmd.Stdin = os.Stdin
    cmd.Stdout = os.Stdout
    cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &amp;syscall.SysProcAttr{
        Cloneflags: syscall.CLONE_NEWUTS,
    }

    if err := cmd.Run(); err != nil {
        panic(err)
    }
}</pre>
    </fieldset>
    <div id="para-div">
      <p>The <span>Command</span> function from the <span>exec</span> package allows one
        to run the specified command (1st parameter) with the supplied arguments (2nd parameter). It returns an instance of
        the <span>Cmd</span> struct.</p>
      <p>One can set the standard input (<span>os.Stdin</span>), the standard output <span>
        os.Stdout</span>, the standard error <span>os.Stderr</span>, and some operating system specific
        attributes on the returned <span>Cmd</span> instance. In this case, we specify the <span>
        syscall.CLONE_NEWUTS</span> OS attribute to indicate the command be run in a new <span>UTS</span>
        namespace.</p>
      <p><span>IMPORTANT</span> : When the <span>main</span> process starts, it internally 
        spawns another <span>main</span> process (with the <span>CLONE</span> argument) in a new
        namespace. It is this spawned <span>main</span> process (running in the new namespace) that is overlayed
        (<span>syscall.Exec</span>) with the shell command by invoking the function <span>
        execContainerShell</span>.</p>
    </div>
    <p>Create and change to the directory <span>$GOPATH/uts</span> by executing the following commands in
        <span>TB</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p $GOPATH/uts</p>
      <p>$ cd $GOPATH/uts</p>
    </div>
    <p>Copy the above code into the program file <span>main.go</span> in the current directory.</p>
    <p>To compile the program file <span>main.go</span>, execute the following command in <span>
        TB</span>:</p>
    <p>$ go build main.go</p>
    <p>To run program <span>main</span>, execute the following command in <span>TB</span>:</p>
    <p>$ sudo ./main</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.8</h4>
      <pre>2020/03/07 12:49:11 Starting process ./main with args: [./main]
2020/03/07 12:49:11 Ready to run command ...
2020/03/07 12:49:11 Starting process ./main with args: [./main CLONE]
2020/03/07 12:49:11 Ready to exec container shell ...
-&gt;</pre>
    </div>
    <p>The command prompt will change to a <span>-&gt;</span>.</p>
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the <span>UTS</span> namespace using both
        the <span>unshare</span> command and a simple <span>go</span> program.</p>
    <p>User Namespace</p>
    <div id="para-div">
      <p>Let us layer the <span>User</span> namespace on top of the <span>UTS</span> namespace.</p>
      <p>To launch a simple container whose user/group IDs as well as the host name are isolated from the parent namespace,
        execute the following command in <span>TB</span>:</p>
    </div>
    <p>$ sudo unshare -uU /bin/sh</p>
    <div id="para-div">
      <p>The <span>-U</span> option enables the <span>User</span> namespace.</p>
      <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    </div>
    <p>$ id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.10</h4>
      <pre>uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup)</pre>
    </div>
    <p>When a <span>User</span> namespace is created, it starts without a mapping for the user/group IDs in
        the new namespace to the parent user/group IDs. The unmapped user/group ID is assigned the default value of the
        overflow user/group ID. The default value for the overflow user ID is read from
        <span>/proc/sys/kernel/overflowuid</span> (which is 65534). Similarly, the default value for the
        overflow group ID is read from <span>/proc/sys/kernel/overflowgid</span> (which is 65534).</p>
    <p>To fix the mapping for the user/group ID to the parent user/group ID, exit the simple container by executing the
        following command in <span>TB</span>:</p>
    <p>$ exit</p>
    <p>To re-launch the simple container with the current effective user/group ID mapped to the superuser user/group ID
        in the new namespace, execute the following command in <span>TB</span>:</p>
    <p>$ sudo unshare -uUr /bin/sh</p>
    <div id="para-div">
      <p>The <span>-r</span> option enables the mapping of the user/group IDs in the new namespace to the
        parent namespace user/group IDs.</p>
      <p>The command prompt will change to a <span>#</span>.</p>
    </div>
    <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    <p># id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.11</h4>
      <pre>uid=0(root) gid=0(root) groups=0(root)</pre>
    </div>
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p># ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.12</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar 7 13:09 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 user -&gt; 'user:[4026532892]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 uts -&gt; 'uts:[4026533401]'</pre>
    </div>
    <p>Comparing Output.12 to Output.3, we see a change in both the <span>uts</span> namespace as well as
        the <span>user</span> namespace, which is what is expected and correct.</p>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p>Next, we will mimic the above <span>UTS</span> and <span>User</span> namespace isolation
        using the following <span>go</span> program:</p>    
    <fieldset id="sc-fieldset">
      <legend>Listing.2</legend>
      <pre>package main

import (
    "log"
    "os"
    "os/exec"
    "syscall"
)

func execContainerShell() {
    log.Printf("Ready to exec container shell ...\n")

    if err := syscall.Sethostname([]byte("leopard")); err != nil {
        panic(err)
    }

    const sh = "/bin/sh"

    env := os.Environ()
    env = append(env, "PS1=-&gt; ")

    if err := syscall.Exec(sh, []string{""}, env); err != nil {
        panic(err)
    }
}

func main() {
    log.Printf("Starting process %s with args: %v\n", os.Args[0], os.Args)

    const clone = "CLONE"

    if len(os.Args) &gt; 1 &amp;&amp; os.Args[1] == clone {
        execContainerShell()
        os.Exit(0)
    }

    log.Printf("Ready to run command ...\n")

    cmd := exec.Command(os.Args[0], []string{clone}...)
    cmd.Stdin = os.Stdin
    cmd.Stdout = os.Stdout
    cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &amp;syscall.SysProcAttr{
        Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWUSER,
        UidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
        GidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
    }

    if err := cmd.Run(); err != nil {
        panic(err)
    }
}</pre>
    </fieldset>
    <div id="para-div">
      <p>As indicated previously, the <span>Command</span> function returns an instance of the
        <span>Cmd</span> struct.</p>
      <p>In this example, we specify the additional <span>syscall.CLONE_NEWUSER</span> OS attribute
        to indicate the command be run in a new <span>User</span> namespace.</p>
      <p>In addition, we set the user ID map <span>UidMappings</span> as an array of
        <span>syscall.SysProcIDMap</span> struct entries, each consisting of the user ID mapping in
        the container (<span>ContainerID</span>) to the user ID in the host namespace
        (<span>HostID</span>). In this case, we map the <span>root</span> user ID
        <span>0</span> in the container to the <span>root</span> user ID <span>
        0</span> of the host namespace. Similarly, we set the group ID map <span>GidMappings</span></p>
    </div>
    <p>Create and change to the directory <span>$GOPATH/user</span> by executing the following commands in
        <span>TB</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p $GOPATH/user</p>
      <p>$ cd $GOPATH/user</p>
    </div>
    <p>Copy the above code into the program file <span>main.go</span> in the current directory.</p>
    <p>To compile the program file <span>main.go</span>, execute the following command in <span>
        TB</span>:</p>
    <p>$ go build main.go</p>
    <p>To run program <span>main</span>, execute the following command in <span>TB</span>:</p>
    <p>$ sudo ./main</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.13</h4>
      <pre>2020/03/07 13:17:02 Starting process ./main with args: [./main]
2020/03/07 13:17:02 Ready to run command ...
2020/03/07 13:17:02 Starting process ./main with args: [./main CLONE]
2020/03/07 13:17:02 Ready to exec container shell ...
-&gt;</pre>
    </div>
    <p>The command prompt will change to a <span>-&gt;</span>.</p>
    <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    <p>-&gt; id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.14</h4>
      <pre>uid=0(root) gid=0(root) groups=0(root)</pre>
    </div>
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p>-&gt; ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.15</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar 13 21:17 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 user -&gt; 'user:[4026532666]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 uts -&gt; 'uts:[4026532723]'</pre>
    </div>
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the combined <span>UTS</span> and
        <span>User</span> namespaces using both the <span>unshare</span> command and a simple
        <span>go</span> program.</p>
    <p>PID Namespace</p>
    <div id="para-div">
      <p>Let us now layer the <span>PID</span> namespace on top of the <span>User</span> namespace
        and the <span>UTS</span> namespace.</p>
      <p>To launch a simple container whose process IDs as well as the user/group IDs and the host name are isolated from
        the parent namespace, execute the following command in <span>TB</span>:</p>
    </div>
    <p>$ sudo unshare -uUrpf --mount-proc /bin/sh</p>
    <div id="para-div">
      <p>The <span>-p</span> option enables the <span>PID</span> namespace.</p>
      <p>The <span>-f</span> option enables spawning (or forking) of new processes in the new namespace.</p>
      <p>The <span>--mount-proc</span> option mounts the <span>proc</span> filesystem as
        a private mount at <span>/proc</span> in the new namespace. This means the <span>
        /proc</span> pseudo directory only shows information only about processes within that <span>PID</span>
        namespace.</p>
    </div>
    <div id="error-div">
      <h4>ATTENTION</h4>
      <pre><span>Ensure</span> the option <span>-f</span> is *<span>SPECIFIED</span>*. Else will encounter the following error:<p><span>/bin/sh: 4: Cannot fork</span></p></pre>
    </div>
    <p>The command prompt will change to a <span>#</span>.</p>
    <p>To display all the processes in the new namespace, execute the following command in <span>TB</span>:</p>
    <p># ps -fu</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.17</h4>
      <pre>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   4628   880 pts/1    S    09:08   0:00 /bin/sh
root         6  0.0  0.0  37368  3340 pts/1    R+   09:12   0:00 ps -fu</pre>
    </div>
    <p>To display all the processes in the parent namespace, execute the following command in <span>TA</span>:</p>
    <p>$ ps -fu</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.18</h4>
      <pre>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
polarsparc  8695  0.0  0.0  22840  5424 pts/1    Ss   08:43   0:00 bash
polarsparc  8681  0.0  0.0  22708  5096 pts/0    Ss   08:43   0:00 bash
polarsparc  9635  0.0  0.0  37368  3364 pts/0    R+   09:12   0:00  \_ ps -fu</pre>
    </div>
    <p>Comparing Output.17 to Output.18, we see the isolation between the new namespace and the parent namespace, which is
        what is expected and correct.</p>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p>Next, we will mimic the above <span>UTS</span>, <span>User</span>, and <span>
        PID</span> namespace isolation using the following <span>go</span> program:</p>    
    <fieldset id="sc-fieldset">
      <legend>Listing.3</legend>
      <pre>package main

import (
    "log"
    "os"
    "os/exec"
    "syscall"
)

func execContainerShell() {
    log.Printf("Ready to exec container shell ...\n")

    if err := syscall.Sethostname([]byte("leopard")); err != nil {
        panic(err)
    }
    
    if err := syscall.Mount("proc", "/proc", "proc", 0, ""); err != nil {
        panic(err)
    }

    const sh = "/bin/sh"

    env := os.Environ()
    env = append(env, "PS1=-&gt; ")

    if err := syscall.Exec(sh, []string{""}, env); err != nil {
        panic(err)
    }
}

func main() {
    log.Printf("Starting process %s with args: %v\n", os.Args[0], os.Args)

    const clone = "CLONE"

    if len(os.Args) &gt; 1 &amp;&amp; os.Args[1] == clone {
        execContainerShell()
        os.Exit(0)
    }

    log.Printf("Ready to run command ...\n")

    cmd := exec.Command(os.Args[0], []string{clone}...)
    cmd.Stdin = os.Stdin
    cmd.Stdout = os.Stdout
    cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &amp;syscall.SysProcAttr{
        Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWUSER | syscall.CLONE_NEWNS | syscall.CLONE_NEWPID,
        UidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
        GidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
    }

    if err := cmd.Run(); err != nil {
        panic(err)
    }
}</pre>
    </fieldset>
    <div id="para-div">
      <p>As indicated previously, the <span>Command</span> function returns an instance of the
        <span>Cmd</span> struct.</p>
      <p>In this example, we specify the additional <span>syscall.CLONE_NEWNS</span> and
        <span>syscall.CLONE_NEWPID</span> OS attributes to indicate the command be run in a new
        <span>PID</span> namespace.</p>
    </div>
    <p>Create and change to the directory <span>$GOPATH/pid</span> by executing the following commands in
        <span>TB</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p $GOPATH/pid</p>
      <p>$ cd $GOPATH/pid</p>
    </div>
    <p>Copy the above code into the program file <span>main.go</span> in the current directory.</p>
    <p>To compile the program file <span>main.go</span>, execute the following command in <span>
        TB</span>:</p>
    <p>$ go build main.go</p>
    <p>To run program <span>main</span>, execute the following command in <span>TB</span>:</p>
    <p>$ sudo ./main</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.19</h4>
      <pre>2020/03/07 13:38:02 Starting process ./main with args: [./main]
2020/03/07 13:38:02 Ready to run command ...
2020/03/07 13:38:02 Starting process ./main with args: [./main CLONE]
2020/03/07 13:38:02 Ready to exec container shell ...
-&gt;</pre>
    </div>
    <p>The command prompt will change to a <span>-&gt;</span>.</p>
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    <p>-&gt; id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.21</h4>
      <pre>uid=0(root) gid=0(root) groups=0(root)</pre>
    </div>
    <p>To display all the processes in the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; ps -fu</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.22</h4>
      <pre>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   4628   776 pts/1    S    09:41   0:00 
root         6  0.0  0.0  37368  3400 pts/1    R+   09:41   0:00 ps -fu</pre>
    </div>
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p>-&gt; ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.23</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar 14 09:44 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 mnt -&gt; 'mnt:[4026532366]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 pid -&gt; 'pid:[4026532368]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 pid_for_children -&gt; 'pid:[4026532368]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 user -&gt; 'user:[4026532365]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 uts -&gt; 'uts:[4026532367]'</pre>
    </div>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the combined <span>UTS</span>,
        <span>User</span>, and <span>PID</span> namespaces using both the
        <span>unshare</span> command and a simple <span>go</span> program.</p>
    <p>Mount Namespace</p>
    <p>We will now setup the minimal Ubuntu Base image for use in the new namespace in the <span>/tmp</span>
        directory.</p>
    <p>To create and copy the base image to a directory in <span>/tmp</span>, execute the following commands
        in <span>TA</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p /tmp/rootfs/.old_root</p>
      <p>$ tar -xvf $HOME/Downloads/ubuntu-base-18.04.4-base-amd64.tar.gz --directory /tmp/rootfs</p>
      <p>cd /tmp</p>
    </div>
    <p>Now let us now layer the <span>Mount</span> namespace on top of the <span>User</span>,
        the <span>UTS</span>, and the <span>PID</span> namespaces.</p>
    <p>To launch a simple container whose mount points as well as the process IDs, the user/group IDs, and the host name
        are isolated from the parent namespace, execute the following command in <span>TB</span>:</p>
    <p>$ sudo unshare -uUrpfm --mount-proc /bin/sh</p>
    <p>The <span>-m</span> option enables the <span>Mount</span> namespace.</p>
    <p>The command prompt will change to a <span>#</span>.</p>
    <p>To list all the mount points in the parent namespace, execute the following command in <span>TA</span>:</p>
    <p>$ cat /proc/mounts | sort</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.24</h4>
      <pre>cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0
cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0
cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0
cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0
cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0
cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0
cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0
cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0
cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0
cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0
cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0
cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0
cgroup /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0
configfs /sys/kernel/config configfs rw,relatime 0 0
debugfs /sys/kernel/debug debugfs rw,relatime 0 0
devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0
/dev/sda1 / ext4 rw,relatime,errors=remount-ro,data=ordered 0 0
/dev/sdb1 /home ext4 rw,relatime,data=ordered 0 0
/dev/sdc1 /home/data ext4 rw,relatime,data=ordered 0 0
fusectl /sys/fs/fuse/connections fusectl rw,relatime 0 0
gvfsd-fuse /run/user/1000/gvfs fuse.gvfsd-fuse rw,nosuid,nodev,relatime,user_id=1000,group_id=1000 0 0
hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0
mqueue /dev/mqueue mqueue rw,relatime 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0
securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0
sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0
systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=25,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=28210 0 0
tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0
tmpfs /run/lock tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k 0 0
tmpfs /run tmpfs rw,nosuid,noexec,relatime,size=3293620k,mode=755 0 0
tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=3293616k,mode=700,uid=1000,gid=1000 0 0
tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0
udev /dev devtmpfs rw,nosuid,relatime,size=16402556k,nr_inodes=4100639,mode=755 0 0</pre>
    </div>
    <p>Now, let us list all the mount points in the new namespace by executing the following command in <span>
        TB</span>:</p>
    <p># cat /proc/mounts | sort</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.25</h4>
      <pre>cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0
cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0
cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0
cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0
cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0
cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0
cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0
cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0
cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0
cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0
cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0
cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0
cgroup /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0
configfs /sys/kernel/config configfs rw,relatime 0 0
debugfs /sys/kernel/debug debugfs rw,relatime 0 0
devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0
/dev/sda1 / ext4 rw,relatime,errors=remount-ro,data=ordered 0 0
/dev/sdb1 /home ext4 rw,relatime,data=ordered 0 0
/dev/sdc1 /home/data ext4 rw,relatime,data=ordered 0 0
fusectl /sys/fs/fuse/connections fusectl rw,relatime 0 0
gvfsd-fuse /run/user/1000/gvfs fuse.gvfsd-fuse rw,nosuid,nodev,relatime,user_id=1000,group_id=1000 0 0
hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0
mqueue /dev/mqueue mqueue rw,relatime 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0
securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0
sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0
systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=25,pgrp=0,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=28210 0 0
tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0
tmpfs /run/lock tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k 0 0
tmpfs /run tmpfs rw,nosuid,noexec,relatime,size=3293620k,mode=755 0 0
tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=3293616k,mode=700,uid=1000,gid=1000 0 0
tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0
udev /dev devtmpfs rw,nosuid,relatime,size=16402556k,nr_inodes=4100639,mode=755 0 0</pre>
    </div>
    <p>Comparing Output.25 and Output.24, we see the one difference for <span>proc</span>. When a new
        <span>Mount</span> namespace is created, the mount points of the new namespace is a copy of the
        mount points in the parent's namespace.</p>
    <p>We will now demonstrate any changes to the new namespace will not affect the parent namespace.</p>
    <p>To make the mount point <span>/</span> (and its children recursively) to be private to the new
        namespace, execute the following command in <span>TB</span>:</p>
    <p># mount --make-rprivate /</p>
    <p>To recursive bind the mount point <span>rootfs/</span> to <span>rootfs/</span> in the new
        namespace, execute the following command in <span>TB</span>:</p>
    <p># mount --rbind rootfs/ rootfs/</p>
    <p>We need the <span>proc</span> filesystem in the new namespace for making changes to mounts. To mount
        <span>/proc</span> as the proc filesystem <span>proc</span> in the new namespace,
        execute the following command in <span>TB</span>:</p>
    <p># mount -t proc proc rootfs/proc</p>
    <p>Next, we need to make <span>rootfs/</span> the root filesystem in the new namespace and move the parent
        root filesystem to <span>rootfs/.old_root</span> using the <span>pivot_root</span>
        command. To do that, execute the following commands in <span>TB</span>:</p>
    <div id="cmd-div">
      <p># pivot_root rootfs/ rootfs/.old_root</p>
      <p># cd /</p>
    </div>
    <p>To list all the file(s) under <span>/</span> in the parent namespace, execute the following command in
        <span>TA</span>:</p>
    <p>$ ls -l /</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.26</h4>
      <pre>total 96
drwxr-xr-x   2 root root  4096 Mar  1 10:58 bin
drwxr-xr-x   3 root root  4096 Mar 16 21:15 boot
drwxr-xr-x   2 root root  4096 Sep 13  2019 cdrom
drwxr-xr-x  22 root root  4560 Mar 21 06:59 dev
drwxr-xr-x 163 root root 12288 Mar 20 10:01 etc
drwxr-xr-x   5 root root  4096 Sep 13  2019 home
lrwxrwxrwx   1 root root    33 Mar 16 21:15 initrd.img -&gt; boot/initrd.img-4.15.0-91-generic
lrwxrwxrwx   1 root root    33 Feb 17 14:08 initrd.img.old -&gt; boot/initrd.img-4.15.0-88-generic
drwxr-xr-x  25 root root  4096 Mar 16 13:37 lib
drwxr-xr-x   2 root root  4096 Jul 29  2019 lib64
drwx------   2 root root 16384 Sep 13  2019 lost+found
drwxr-xr-x   3 root root  4096 Nov 10 13:00 media
drwxr-xr-x   2 root root  4096 Jul 29  2019 mnt
drwxr-xr-x   7 root root  4096 Mar 13 08:04 opt
dr-xr-xr-x 328 root root     0 Mar 21 06:59 proc
drwx------   9 root root  4096 Feb 23 13:25 root
drwxr-xr-x  36 root root  1140 Mar 21 07:04 run
drwxr-xr-x   2 root root 12288 Mar 16 13:37 sbin
drwxr-xr-x   2 root root  4096 Jul 29  2019 srv
dr-xr-xr-x  13 root root     0 Mar 21 06:59 sys
drwxrwxrwt  20 root root  4096 Mar 21 11:10 tmp
drwxr-xr-x  11 root root  4096 Jul 29  2019 usr
drwxr-xr-x  11 root root  4096 Jul 29  2019 var
lrwxrwxrwx   1 root root    30 Mar 16 21:15 vmlinuz -&gt; boot/vmlinuz-4.15.0-91-generic
lrwxrwxrwx   1 root root    30 Feb 17 14:08 vmlinuz.old -&gt; boot/vmlinuz-4.15.0-88-generic</pre>
    </div>
    <p>To list all the file(s) under <span>/</span> in the new namespace, execute the following command in
        <span>TB</span>:</p>
    <p># ls -l /</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.27</h4>
      <pre>total 72
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:24 bin
drwxr-xr-x   2 nobody nogroup 4096 Apr 24  2018 boot
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:24 dev
drwxr-xr-x  29 nobody nogroup 4096 Feb  3 20:24 etc
drwxr-xr-x   2 nobody nogroup 4096 Apr 24  2018 home
drwxr-xr-x   8 nobody nogroup 4096 May 23  2017 lib
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 lib64
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 media
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 mnt
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 opt
dr-xr-xr-x 328 root   root       0 Mar 21 14:10 proc
drwx------   2 nobody nogroup 4096 Feb  3 20:24 root
drwxr-xr-x   4 nobody nogroup 4096 Feb  3 20:23 run
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:24 sbin
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 srv
drwxr-xr-x   2 nobody nogroup 4096 Apr 24  2018 sys
drwxrwxr-x   2 nobody nogroup 4096 Feb  3 20:24 tmp
drwxr-xr-x  10 nobody nogroup 4096 Feb  3 20:23 usr
drwxr-xr-x  11 nobody nogroup 4096 Feb  3 20:24 var</pre>
    </div>
    <p>Comparing Output.26 and Output.27, we see the root filesystems are totally different.</p>
    <p>To mount <span>/tmp</span> as the temporary filesystem <span>tmpfs</span> in the
        new namespace, execute the following command in <span>TB</span>:</p>
    <p># mount -t tmpfs tmpfs /tmp</p>
    <p>To create a text file <span>/tmp/leopard.txt</span> in the directory <span>/tmp</span>
        of the new namespace, execute the following command in <span>TB</span>:</p>
    <p># echo 'leopard' &gt; /tmp/leopard.txt</p>
    <p>To list the properties of the file <span>/tmp/leopard.txt</span> in the new namespace, execute
        the following command in <span>TB</span>:</p>
    <p># ls -l /tmp/leopard.txt</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.28</h4>
      <pre>-rw-r--r-- 1 root root 7 Mar 14 22:05 /tmp/leopard.txt</pre>
    </div>
    <p>To list the properties of the file <span>/tmp/leopard.txt</span> in the parent namespace, execute
        the following command in <span>TA</span>:</p>
    <p>$ ls -l /tmp/leopard.txt</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.29</h4>
      <pre>ls: cannot access '/tmp/leopard.txt': No such file or directory</pre>
    </div>
    <p>Finally, to completely remove the parent root filesystem <span>rootfs/.old_root</span> from the new
        namespace, execute the following commands in <span>TB</span>:</p>
    <div id="cmd-div">
      <p># mount --make-rprivate /.old_root</p>
      <p># umount -l /.old_root</p>
    </div>
    <p>To list all the mount points in the new namespace by executing the following command in <span>TB</span>
        :</p>
    <p># cat /proc/mounts | sort</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.30</h4>
      <pre>/dev/sda1 / ext4 rw,relatime,errors=remount-ro,data=ordered 0 0
proc /proc proc rw,relatime 0 0
tmpfs /tmp tmpfs rw,relatime 0 0</pre>
    </div>
    <p>To exit the new namespace, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the combined <span>UTS</span>,
        <span>User</span>, <span>PID</span>, and <span>Mount</span> namespaces
        using the <span>unshare</span> command.</p>
    <p>References</p>
    
    <br>
    <hr>
    
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[DwarFS – Deduplicating Warp-Speed Advanced Read-Only File System (181 pts)]]></title>
            <link>https://github.com/mhx/dwarfs</link>
            <guid>40008755</guid>
            <pubDate>Fri, 12 Apr 2024 02:04:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mhx/dwarfs">https://github.com/mhx/dwarfs</a>, See on <a href="https://news.ycombinator.com/item?id=40008755">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://github.com/mhx/dwarfs/releases/latest"><img src="https://camo.githubusercontent.com/7c14c3f1b59f09fab877e8fd13d3682f78ce4b9318fbe3cc4572e937200d6e56/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6d68782f6477617266733f6c6162656c3d4c617465737425323052656c65617365" alt="Latest Release" data-canonical-src="https://img.shields.io/github/release/mhx/dwarfs?label=Latest%20Release"></a>
<a href="https://github.com/mhx/dwarfs/releases"><img src="https://camo.githubusercontent.com/2b6c35d2fbd0dac827465bda94342ea83a91f3effafab9c1d117eb790cc936e9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6d68782f6477617266732f746f74616c2e7376673f26636f6c6f723d453935343230266c6162656c3d546f74616c253230446f776e6c6f616473" alt="Total Downloads" data-canonical-src="https://img.shields.io/github/downloads/mhx/dwarfs/total.svg?&amp;color=E95420&amp;label=Total%20Downloads"></a>
<a href="https://github.com/mhx/dwarfs/actions/workflows/build.yml"><img src="https://github.com/mhx/dwarfs/actions/workflows/build.yml/badge.svg" alt="DwarFS CI Build"></a>
<a href="https://app.travis-ci.com/github/mhx/dwarfs" rel="nofollow"><img src="https://camo.githubusercontent.com/9d83f6eb2696807dc865c941a7b37c7fc27346db3cbb11595be061b977b8866b/68747470733a2f2f6170702e7472617669732d63692e636f6d2f6d68782f6477617266732e7376673f6272616e63683d6d61696e" alt="Build Status" data-canonical-src="https://app.travis-ci.com/mhx/dwarfs.svg?branch=main"></a>
<a href="https://app.codacy.com/gh/mhx/dwarfs/dashboard" rel="nofollow"><img src="https://camo.githubusercontent.com/6d5dac47834dc9a72470dd868d91391d3a808686e1457628e3022dfb61cf92fd/68747470733a2f2f6170702e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3533343839663737373535323438633939396533383035303032363765383839" alt="Codacy Badge" data-canonical-src="https://app.codacy.com/project/badge/Grade/53489f77755248c999e380500267e889"></a>
<a href="https://codecov.io/github/mhx/dwarfs" rel="nofollow"><img src="https://camo.githubusercontent.com/028b266bc1c1b17af19e188abadf5901e33b4b9972e56631c7a76a0dea82d2e5/68747470733a2f2f636f6465636f762e696f2f6769746875622f6d68782f6477617266732f67726170682f62616467652e7376673f746f6b656e3d424b52344133584b4139" alt="codecov" data-canonical-src="https://codecov.io/github/mhx/dwarfs/graph/badge.svg?token=BKR4A3XKA9"></a>
<a href="https://www.bestpractices.dev/projects/8663" rel="nofollow"><img src="https://camo.githubusercontent.com/9a7cc655c21144482581cbbfc96b3a1d20b4dbb174d99ca64ca27a682d2d9aa1/68747470733a2f2f7777772e626573747072616374696365732e6465762f70726f6a656374732f383636332f6261646765" alt="OpenSSF Best Practices" data-canonical-src="https://www.bestpractices.dev/projects/8663/badge"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">DwarFS</h2><a id="user-content-dwarfs" aria-label="Permalink: DwarFS" href="#dwarfs"></a></p>
<p dir="auto">The <strong>D</strong>eduplicating <strong>W</strong>arp-speed <strong>A</strong>dvanced <strong>R</strong>ead-only <strong>F</strong>ile <strong>S</strong>ystem.</p>
<p dir="auto">A fast high compression read-only file system for Linux and Windows.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#overview">Overview</a></li>
<li><a href="#history">History</a></li>
<li><a href="#building-and-installing">Building and Installing</a>
<ul dir="auto">
<li><a href="#prebuilt-binaries">Prebuilt Binaries</a></li>
<li><a href="#universal-binaries">Universal Binaries</a></li>
<li><a href="#dependencies">Dependencies</a></li>
<li><a href="#building">Building</a></li>
<li><a href="#installing">Installing</a></li>
<li><a href="#static-builds">Static Builds</a></li>
</ul>
</li>
<li><a href="#usage">Usage</a></li>
<li><a href="#windows-support">Windows Support</a>
<ul dir="auto">
<li><a href="#building-on-windows">Building on Windows</a></li>
</ul>
</li>
<li><a href="#macos-support">macOS Support</a>
<ul dir="auto">
<li><a href="#building-on-macos">Building on macOS</a></li>
</ul>
</li>
<li><a href="#use-cases">Use Cases</a>
<ul dir="auto">
<li><a href="#astrophotography">Astrophotography</a></li>
</ul>
</li>
<li><a href="#dealing-with-bit-rot">Dealing with Bit Rot</a></li>
<li><a href="#extended-attributes">Extended Attributes</a></li>
<li><a href="#comparison">Comparison</a>
<ul dir="auto">
<li><a href="#with-squashfs">With SquashFS</a></li>
<li><a href="#with-squashfs--xz">With SquashFS &amp; xz</a></li>
<li><a href="#with-lrzip">With lrzip</a></li>
<li><a href="#with-zpaq">With zpaq</a></li>
<li><a href="#with-zpaqfranz">With zpaqfranz</a></li>
<li><a href="#with-wimlib">With wimlib</a></li>
<li><a href="#with-cromfs">With Cromfs</a></li>
<li><a href="#with-erofs">With EROFS</a></li>
<li><a href="#with-fuse-archive">With fuse-archive</a></li>
</ul>
</li>
<li><a href="#performance-monitoring">Performance Monitoring</a></li>
<li><a href="#other-obscure-features">Other Obscure Features</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mhx/dwarfs/blob/main/doc/windows.gif?raw=true"><img src="https://github.com/mhx/dwarfs/raw/main/doc/windows.gif?raw=true" alt="Windows Screen Capture" title="DwarFS Windows" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mhx/dwarfs/blob/main/doc/screenshot.gif?raw=true"><img src="https://github.com/mhx/dwarfs/raw/main/doc/screenshot.gif?raw=true" alt="Linux Screen Capture" title="DwarFS Linux" data-animated-image=""></a></p>
<p dir="auto">DwarFS is a read-only file system with a focus on achieving <strong>very
high compression ratios</strong> in particular for very redundant data.</p>
<p dir="auto">This probably doesn't sound very exciting, because if it's redundant,
it <em>should</em> compress well. However, I found that other read-only,
compressed file systems don't do a very good job at making use of
this redundancy. See <a href="#comparison">here</a> for a comparison with other
compressed file systems.</p>
<p dir="auto">DwarFS also <strong>doesn't compromise on speed</strong> and for my use cases I've
found it to be on par with or perform better than SquashFS. For my
primary use case, <strong>DwarFS compression is an order of magnitude better
than SquashFS compression</strong>, it's <strong>6 times faster to build the file
system</strong>, it's typically faster to access files on DwarFS and it uses
less CPU resources.</p>
<p dir="auto">To give you an idea of what DwarFS is capable of, here's a quick comparison
of DwarFS and SquashFS on a set of video files with a total size of 39 GiB.
The twist is that each unique video file has two sibling files with a
different set of audio streams (I didn't make this up, this is
<a href="https://github.com/mhx/dwarfs/discussions/63" data-hovercard-type="discussion" data-hovercard-url="/mhx/dwarfs/discussions/63/hovercard">an actual use case</a>). So
there's redundancy in both the video and audio data, but as the streams
are interleaved and identical blocks are typically very far apart, it's
quite challenging to make use of that redundancy for compression. SquashFS
essentially fails to compress the source data at all, whereas DwarFS is
able to reduce the size by almost a factor of 3, which is close to the
theoretical maximum:</p>
<div data-snippet-clipboard-copy-content="$ du -hs dwarfs-video-test
39G     dwarfs-video-test
$ ls -lh dwarfs-video-test.*fs
-rw-r--r-- 1 mhx users 14G Jul  2 13:01 dwarfs-video-test.dwarfs
-rw-r--r-- 1 mhx users 39G Jul 12 09:41 dwarfs-video-test.squashfs"><pre><code>$ du -hs dwarfs-video-test
39G     dwarfs-video-test
$ ls -lh dwarfs-video-test.*fs
-rw-r--r-- 1 mhx users 14G Jul  2 13:01 dwarfs-video-test.dwarfs
-rw-r--r-- 1 mhx users 39G Jul 12 09:41 dwarfs-video-test.squashfs
</code></pre></div>
<p dir="auto">While this is already impressive, it gets even better. When mounting
the SquashFS image and performing a random-read throughput test using
<a href="https://github.com/axboe/fio/">fio</a>-3.34, both <code>squashfuse</code> and
<code>squashfuse_ll</code> top out at around 230 MiB/s:</p>
<div data-snippet-clipboard-copy-content="$ fio --readonly --rw=randread --name=randread --bs=64k --direct=1 \
      --opendir=mnt --numjobs=4 --ioengine=libaio --iodepth=32 \
      --group_reporting --runtime=60 --time_based
[...]
   READ: bw=230MiB/s (241MB/s), 230MiB/s-230MiB/s (241MB/s-241MB/s), io=13.5GiB (14.5GB), run=60004-60004msec"><pre><code>$ fio --readonly --rw=randread --name=randread --bs=64k --direct=1 \
      --opendir=mnt --numjobs=4 --ioengine=libaio --iodepth=32 \
      --group_reporting --runtime=60 --time_based
[...]
   READ: bw=230MiB/s (241MB/s), 230MiB/s-230MiB/s (241MB/s-241MB/s), io=13.5GiB (14.5GB), run=60004-60004msec
</code></pre></div>
<p dir="auto">DwarFS, however, manages to sustain <strong>random read rates of 20 GiB/s</strong>:</p>
<div data-snippet-clipboard-copy-content="  READ: bw=20.2GiB/s (21.7GB/s), 20.2GiB/s-20.2GiB/s (21.7GB/s-21.7GB/s), io=1212GiB (1301GB), run=60001-60001msec"><pre><code>  READ: bw=20.2GiB/s (21.7GB/s), 20.2GiB/s-20.2GiB/s (21.7GB/s-21.7GB/s), io=1212GiB (1301GB), run=60001-60001msec
</code></pre></div>
<p dir="auto">Distinct features of DwarFS are:</p>
<ul dir="auto">
<li>
<p dir="auto">Clustering of files by similarity using a similarity hash function.
This makes it easier to exploit the redundancy across file boundaries.</p>
</li>
<li>
<p dir="auto">Segmentation analysis across file system blocks in order to reduce
the size of the uncompressed file system. This saves memory when
using the compressed file system and thus potentially allows for
higher cache hit rates as more data can be kept in the cache.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs.md#categorizers">Categorization framework</a> to categorize
files or even fragments of files and then process individual categories
differently. For example, this allows you to not waste time trying to
compress incompressible files or to compress PCM audio data using FLAC
compression.</p>
</li>
<li>
<p dir="auto">Highly multi-threaded implementation. Both the
<a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs.md">file system creation tool</a> as well as the
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs.md">FUSE driver</a> are able to make good use of the
many cores of your system.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">History</h2><a id="user-content-history" aria-label="Permalink: History" href="#history"></a></p>
<p dir="auto">I started working on DwarFS in 2013 and my main use case and major
motivation was that I had several hundred different versions of Perl
that were taking up something around 30 gigabytes of disk space, and
I was unwilling to spend more than 10% of my hard drive keeping them
around for when I happened to need them.</p>
<p dir="auto">Up until then, I had been using <a href="https://bisqwit.iki.fi/source/cromfs.html" rel="nofollow">Cromfs</a>
for squeezing them into a manageable size. However, I was getting more
and more annoyed by the time it took to build the filesystem image
and, to make things worse, more often than not it was crashing after
about an hour or so.</p>
<p dir="auto">I had obviously also looked into <a href="https://en.wikipedia.org/wiki/SquashFS" rel="nofollow">SquashFS</a>,
but never got anywhere close to the compression rates of Cromfs.</p>
<p dir="auto">This alone wouldn't have been enough to get me into writing DwarFS,
but at around the same time, I was pretty obsessed with the recent
developments and features of newer C++ standards and really wanted
a C++ hobby project to work on. Also, I've wanted to do something
with <a href="https://en.wikipedia.org/wiki/Filesystem_in_Userspace" rel="nofollow">FUSE</a>
for quite some time. Last but not least, I had been thinking about
the problem of compressed file systems for a bit and had some ideas
that I definitely wanted to try.</p>
<p dir="auto">The majority of the code was written in 2013, then I did a couple
of cleanups, bugfixes and refactors every once in a while, but I
never really got it to a state where I would feel happy releasing
it. It was too awkward to build with its dependency on Facebook's
(quite awesome) <a href="https://github.com/facebook/folly">folly</a> library
and it didn't have any documentation.</p>
<p dir="auto">Digging out the project again this year, things didn't look as grim
as they used to. Folly now builds with CMake and so I just pulled
it in as a submodule. Most other dependencies can be satisfied
from packages that should be widely available. And I've written
some rudimentary docs as well.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building and Installing</h2><a id="user-content-building-and-installing" aria-label="Permalink: Building and Installing" href="#building-and-installing"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prebuilt Binaries</h3><a id="user-content-prebuilt-binaries" aria-label="Permalink: Prebuilt Binaries" href="#prebuilt-binaries"></a></p>
<p dir="auto"><a href="https://github.com/mhx/dwarfs/releases">Each release</a> has pre-built,
statically linked binaries for <code>Linux-x86_64</code>, <code>Linux-aarch64</code> and
<code>Windows-AMD64</code> available for download. These <em>should</em> run without
any dependencies and can be useful especially on older distributions
where you can't easily build the tools from source.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Universal Binaries</h3><a id="user-content-universal-binaries" aria-label="Permalink: Universal Binaries" href="#universal-binaries"></a></p>
<p dir="auto">In addition to the binary tarballs, there's a <strong>universal binary</strong>
available for each architecture. These universal binaries contain
<em>all</em> tools (<code>mkdwarfs</code>, <code>dwarfsck</code>, <code>dwarfsextract</code> and the <code>dwarfs</code>
FUSE driver) in a single executable. These executables are compressed
using <a href="https://github.com/upx/upx">upx</a>, so they are much smaller than
the individual tools combined. However, it also means the binaries need
to be decompressed each time they are run, which can have a signficant
overhead. If that is an issue, you can either stick to the "classic"
individual binaries or you can decompress the universal binary, e.g.:</p>
<div data-snippet-clipboard-copy-content="upx -d dwarfs-universal-0.7.0-Linux-aarch64"><pre><code>upx -d dwarfs-universal-0.7.0-Linux-aarch64
</code></pre></div>
<p dir="auto">The universal binaries can be run through symbolic links named after
the proper tool. e.g.:</p>
<div data-snippet-clipboard-copy-content="$ ln -s dwarfs-universal-0.7.0-Linux-aarch64 mkdwarfs
$ ./mkdwarfs --help"><pre><code>$ ln -s dwarfs-universal-0.7.0-Linux-aarch64 mkdwarfs
$ ./mkdwarfs --help
</code></pre></div>
<p dir="auto">This also works on Windows if the file system supports symbolic links:</p>
<div data-snippet-clipboard-copy-content="> mklink mkdwarfs.exe dwarfs-universal-0.7.0-Windows-AMD64.exe
> .\mkdwarfs.exe --help"><pre><code>&gt; mklink mkdwarfs.exe dwarfs-universal-0.7.0-Windows-AMD64.exe
&gt; .\mkdwarfs.exe --help
</code></pre></div>
<p dir="auto">Alternatively, you can select the tool by passing <code>--tool=&lt;name&gt;</code> as
the first argument on the command line:</p>
<div data-snippet-clipboard-copy-content="> .\dwarfs-universal-0.7.0-Windows-AMD64.exe --tool=mkdwarfs --help"><pre><code>&gt; .\dwarfs-universal-0.7.0-Windows-AMD64.exe --tool=mkdwarfs --help
</code></pre></div>
<p dir="auto">Note that just like the <code>dwarfs.exe</code> Windows binary, the universal
Windows binary depends on the <code>winfsp-x64.dll</code> from the
<a href="https://github.com/winfsp/winfsp">WinFsp</a> project. However, for the
universal binary, the DLL is loaded lazily, so you can still use all
other tools without the DLL.
See the <a href="#windows-support">Windows Support</a> section for more details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dependencies</h3><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<p dir="auto">DwarFS uses <a href="https://cmake.org/" rel="nofollow">CMake</a> as a build tool.</p>
<p dir="auto">It uses both <a href="https://www.boost.org/" rel="nofollow">Boost</a> and
<a href="https://github.com/facebook/folly">Folly</a>, though the latter is
included as a submodule since very few distributions actually
offer packages for it. Folly itself has a number of dependencies,
so please check <a href="https://github.com/facebook/folly#dependencies">here</a>
for an up-to-date list.</p>
<p dir="auto">It also uses <a href="https://github.com/facebook/fbthrift">Facebook Thrift</a>,
in particular the <code>frozen</code> library, for storing metadata in a highly
space-efficient, memory-mappable and well defined format. It's also
included as a submodule, and we only build the compiler and a very
reduced library that contains just enough for DwarFS to work.</p>
<p dir="auto">Other than that, DwarFS really only depends on FUSE3 and on a set
of compression libraries that Folly already depends on (namely
<a href="https://github.com/lz4/lz4">lz4</a>, <a href="https://github.com/facebook/zstd">zstd</a>
and <a href="https://github.com/kobolabs/liblzma">liblzma</a>).</p>
<p dir="auto">The dependency on <a href="https://github.com/google/googletest">googletest</a>
will be automatically resolved if you build with tests.</p>
<p dir="auto">A good starting point for apt-based systems is probably:</p>
<div data-snippet-clipboard-copy-content="$ apt install \
    gcc \
    g++ \
    clang \
    git \
    ccache \
    ninja-build \
    cmake \
    make \
    bison \
    flex \
    ronn \
    fuse3 \
    pkg-config \
    binutils-dev \
    libacl1-dev \
    libarchive-dev \
    libbenchmark-dev \
    libboost-chrono-dev \
    libboost-context-dev \
    libboost-filesystem-dev \
    libboost-iostreams-dev \
    libboost-program-options-dev \
    libboost-regex-dev \
    libboost-system-dev \
    libboost-thread-dev \
    libbrotli-dev \
    libevent-dev \
    libhowardhinnant-date-dev \
    libjemalloc-dev \
    libdouble-conversion-dev \
    libiberty-dev \
    liblz4-dev \
    liblzma-dev \
    libmagic-dev \
    librange-v3-dev \
    libssl-dev \
    libunwind-dev \
    libdwarf-dev \
    libelf-dev \
    libfmt-dev \
    libfuse3-dev \
    libgoogle-glog-dev \
    libutfcpp-dev \
    libflac++-dev \
    python3-mistletoe"><pre><code>$ apt install \
    gcc \
    g++ \
    clang \
    git \
    ccache \
    ninja-build \
    cmake \
    make \
    bison \
    flex \
    ronn \
    fuse3 \
    pkg-config \
    binutils-dev \
    libacl1-dev \
    libarchive-dev \
    libbenchmark-dev \
    libboost-chrono-dev \
    libboost-context-dev \
    libboost-filesystem-dev \
    libboost-iostreams-dev \
    libboost-program-options-dev \
    libboost-regex-dev \
    libboost-system-dev \
    libboost-thread-dev \
    libbrotli-dev \
    libevent-dev \
    libhowardhinnant-date-dev \
    libjemalloc-dev \
    libdouble-conversion-dev \
    libiberty-dev \
    liblz4-dev \
    liblzma-dev \
    libmagic-dev \
    librange-v3-dev \
    libssl-dev \
    libunwind-dev \
    libdwarf-dev \
    libelf-dev \
    libfmt-dev \
    libfuse3-dev \
    libgoogle-glog-dev \
    libutfcpp-dev \
    libflac++-dev \
    python3-mistletoe
</code></pre></div>
<p dir="auto">Note that when building with <code>gcc</code>, the optimization level will be
set to <code>-O2</code> instead of the CMake default of <code>-O3</code> for release
builds. At least with versions up to <code>gcc-10</code>, the <code>-O3</code> build is
<a href="https://github.com/mhx/dwarfs/issues/14" data-hovercard-type="issue" data-hovercard-url="/mhx/dwarfs/issues/14/hovercard">up to 70% slower</a> than a
build with <code>-O2</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building</h3><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Firstly, either clone the repository...</p>
<div data-snippet-clipboard-copy-content="$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs"><pre><code>$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs
</code></pre></div>
<p dir="auto">...or unpack the release archive:</p>
<div data-snippet-clipboard-copy-content="$ tar xvf dwarfs-x.y.z.tar.bz2
$ cd dwarfs-x.y.z"><pre><code>$ tar xvf dwarfs-x.y.z.tar.bz2
$ cd dwarfs-x.y.z
</code></pre></div>
<p dir="auto">Once all dependencies have been installed, you can build DwarFS
using:</p>
<div data-snippet-clipboard-copy-content="$ mkdir build
$ cd build
$ cmake .. -DWITH_TESTS=1
$ make -j$(nproc)"><pre><code>$ mkdir build
$ cd build
$ cmake .. -DWITH_TESTS=1
$ make -j$(nproc)
</code></pre></div>
<p dir="auto">You can then run tests with:</p>

<p dir="auto">All binaries use <a href="https://github.com/jemalloc/jemalloc">jemalloc</a>
as a memory allocator by default, as it is typically uses much less
system memory compared to the <code>glibc</code> or <code>tcmalloc</code> allocators.
To disable the use of <code>jemalloc</code>, pass <code>-DUSE_JEMALLOC=0</code> on the
<code>cmake</code> command line.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing</h3><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto">Installing is as easy as:</p>

<p dir="auto">Though you don't have to install the tools to play with them.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Static Builds</h3><a id="user-content-static-builds" aria-label="Permalink: Static Builds" href="#static-builds"></a></p>
<p dir="auto">Attempting to build statically linked binaries is highly discouraged
and not officially supported. That being said, here's how to set up
an environment where you <em>might</em> be able to build static binaries.</p>
<p dir="auto">This has been tested with <code>ubuntu-22.04-live-server-amd64.iso</code>. First,
install all the packages listed as dependencies above. Also install:</p>
<div data-snippet-clipboard-copy-content="$ apt install ccache ninja libacl1-dev"><pre><code>$ apt install ccache ninja libacl1-dev
</code></pre></div>
<p dir="auto"><code>ccache</code> and <code>ninja</code> are optional, but help with a speedy compile.</p>
<p dir="auto">Depending on your distibution, you'll need to build and install static
versions of some libraries, e.g. <code>libarchive</code> and <code>libmagic</code> for Ubuntu:</p>
<div data-snippet-clipboard-copy-content="$ wget https://github.com/libarchive/libarchive/releases/download/v3.6.2/libarchive-3.6.2.tar.xz
$ tar xf libarchive-3.6.2.tar.xz &amp;&amp; cd libarchive-3.6.2
$ ./configure --prefix=/opt/static-libs --without-iconv --without-xml2 --without-expat
$ make &amp;&amp; sudo make install"><pre><code>$ wget https://github.com/libarchive/libarchive/releases/download/v3.6.2/libarchive-3.6.2.tar.xz
$ tar xf libarchive-3.6.2.tar.xz &amp;&amp; cd libarchive-3.6.2
$ ./configure --prefix=/opt/static-libs --without-iconv --without-xml2 --without-expat
$ make &amp;&amp; sudo make install
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ wget ftp://ftp.astron.com/pub/file/file-5.44.tar.gz
$ tar xf file-5.44.tar.gz &amp;&amp; cd file-5.44
$ ./configure --prefix=/opt/static-libs --enable-static=yes --enable-shared=no
$ make &amp;&amp; make install"><pre><code>$ wget ftp://ftp.astron.com/pub/file/file-5.44.tar.gz
$ tar xf file-5.44.tar.gz &amp;&amp; cd file-5.44
$ ./configure --prefix=/opt/static-libs --enable-static=yes --enable-shared=no
$ make &amp;&amp; make install
</code></pre></div>
<p dir="auto">That's it! Now you can try building static binaries for DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs &amp;&amp; mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=1 -DSTATIC_BUILD_DO_NOT_USE=1 \
           -DSTATIC_BUILD_EXTRA_PREFIX=/opt/static-libs
$ ninja
$ ninja test"><pre><code>$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs &amp;&amp; mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=1 -DSTATIC_BUILD_DO_NOT_USE=1 \
           -DSTATIC_BUILD_EXTRA_PREFIX=/opt/static-libs
$ ninja
$ ninja test
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Please check out the manual pages for <a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs.md">mkdwarfs</a>,
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs.md">dwarfs</a>, <a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfsck.md">dwarfsck</a> and
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfsextract.md">dwarfsextract</a>. You can also access the manual
pages using the <code>--man</code> option to each binary, e.g.:</p>

<p dir="auto">The <a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs.md">dwarfs</a> manual page also shows an example for setting
up DwarFS with <a href="https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt" rel="nofollow">overlayfs</a>
in order to create a writable file system mount on top a read-only
DwarFS image.</p>
<p dir="auto">A description of the DwarFS filesystem format can be found in
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs-format.md">dwarfs-format</a>.</p>
<p dir="auto">A high-level overview of the internal operation of <code>mkdwarfs</code> is shown
in <a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs-sequence.svg">this sequence diagram</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Windows Support</h2><a id="user-content-windows-support" aria-label="Permalink: Windows Support" href="#windows-support"></a></p>
<p dir="auto">Support for the Windows operating system is currently experimental.
Having worked pretty much exclusively in a Unix world for the past two
decades, my experience with Windows development is rather limited and
I'd expect there to definitely be bugs and rough edges in the Windows
code.</p>
<p dir="auto">The Windows version of the DwarFS filesystem driver relies on the awesome
<a href="https://github.com/winfsp/winfsp">WinFsp</a> project and its <code>winfsp-x64.dll</code>
must be discoverable by the <code>dwarfs.exe</code> driver.</p>
<p dir="auto">The different tools should behave pretty much the same whether you're
using them on Linux or Windows. The file system images can be copied
between Linux and Windows and images created on one OS should work fine
on the other.</p>
<p dir="auto">There are a few things worth pointing out, though:</p>
<ul dir="auto">
<li>
<p dir="auto">DwarFS supports both hardlinks and symlinks on Windows, just as it
does on Linux. However, creating hardlinks and symlinks seems to
require admin privileges on Windows, so if you want to e.g. extract
a DwarFS image that contains links of some sort, you might run into
errors if you don't have the right privileges.</p>
</li>
<li>
<p dir="auto">Due to a <a href="https://github.com/winfsp/winfsp/issues/454" data-hovercard-type="issue" data-hovercard-url="/winfsp/winfsp/issues/454/hovercard">problem</a> in
WinFsp, symlinks cannot currently point outside of the mounted file
system.  Furthermore, due to another
<a href="https://github.com/winfsp/winfsp/issues/530" data-hovercard-type="issue" data-hovercard-url="/winfsp/winfsp/issues/530/hovercard">problem</a> in WinFsp,
symlinks with a drive letter will appear with a mangled target path.</p>
</li>
<li>
<p dir="auto">The DwarFS driver on Windows correctly reports hardlink counts via
its API, but currently these counts are not correctly propagated
to the Windows file system layer. This is presumably due to a
<a href="https://github.com/winfsp/winfsp/issues/511" data-hovercard-type="issue" data-hovercard-url="/winfsp/winfsp/issues/511/hovercard">problem</a> in WinFsp.</p>
</li>
<li>
<p dir="auto">When mounting a DwarFS image on Windows, the mount point must not
exist. This is different from Linux, where the mount point must
actually exist. Also, it's possible to mount a DwarFS image as a
drive letter, e.g.</p>
<p dir="auto">dwarfs.exe image.dwarfs Z:</p>
</li>
<li>
<p dir="auto">Filter rules for <code>mkdwarfs</code> always require Unix path separators,
regardless of whether it's running on Windows or Linux.</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building on Windows</h3><a id="user-content-building-on-windows" aria-label="Permalink: Building on Windows" href="#building-on-windows"></a></p>
<p dir="auto">Building on Windows is not too complicated thanks to <a href="https://vcpkg.io/" rel="nofollow">vcpkg</a>.
You'll need to install:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://visualstudio.microsoft.com/vs/features/cplusplus/" rel="nofollow">Visual Studio and the MSVC C/C++ compiler</a></p>
</li>
<li>
<p dir="auto"><a href="https://git-scm.com/download/win" rel="nofollow">Git</a></p>
</li>
<li>
<p dir="auto"><a href="https://cmake.org/download/" rel="nofollow">CMake</a></p>
</li>
<li>
<p dir="auto"><a href="https://github.com/ninja-build/ninja/releases">Ninja</a></p>
</li>
<li>
<p dir="auto"><a href="https://github.com/winfsp/winfsp/releases">WinFsp</a></p>
</li>
</ul>
<p dir="auto"><code>WinFsp</code> is expected to be installed in <code>C:\Program Files (x68)\WinFsp</code>;
if it's not, you'll need to set <code>WINFSP_PATH</code> when running CMake via
<code>cmake/win.bat</code>.</p>
<p dir="auto">Now you need to clone <code>vcpkg</code> and <code>dwarfs</code>:</p>
<div data-snippet-clipboard-copy-content="> cd %HOMEPATH%
> mkdir git
> cd git
> git clone https://github.com/Microsoft/vcpkg.git
> git clone https://github.com/mhx/dwarfs"><pre><code>&gt; cd %HOMEPATH%
&gt; mkdir git
&gt; cd git
&gt; git clone https://github.com/Microsoft/vcpkg.git
&gt; git clone https://github.com/mhx/dwarfs
</code></pre></div>
<p dir="auto">Then, bootstrap <code>vcpkg</code>:</p>
<div data-snippet-clipboard-copy-content="> .\vcpkg\bootstrap-vcpkg.bat"><pre><code>&gt; .\vcpkg\bootstrap-vcpkg.bat
</code></pre></div>
<p dir="auto">And build DwarFS:</p>
<div data-snippet-clipboard-copy-content="> cd dwarfs
> mkdir build
> cd build
> ..\cmake\win.bat
> ninja"><pre><code>&gt; cd dwarfs
&gt; mkdir build
&gt; cd build
&gt; ..\cmake\win.bat
&gt; ninja
</code></pre></div>
<p dir="auto">Once that's done, you should be able to run the tests.
Set <code>CTEST_PARALLEL_LEVEL</code> according to the number of CPU cores in
your machine.</p>
<div data-snippet-clipboard-copy-content="> set CTEST_PARALLEL_LEVEL=10
> ninja test"><pre><code>&gt; set CTEST_PARALLEL_LEVEL=10
&gt; ninja test
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">macOS Support</h2><a id="user-content-macos-support" aria-label="Permalink: macOS Support" href="#macos-support"></a></p>
<p dir="auto">Support for the macOS operating system is currently experimental.</p>
<p dir="auto">The macOS version of the DwarFS filesystem driver relies on the awesome
<a href="https://https//osxfuse.github.io/" rel="nofollow">macFUSE</a> project.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building on macOS</h3><a id="user-content-building-on-macos" aria-label="Permalink: Building on macOS" href="#building-on-macos"></a></p>
<p dir="auto">Building on macOS involves a few steps, but should be relatively
straightforward:</p>
<ul dir="auto">
<li>
<p dir="auto">Install <a href="https://brew.sh/" rel="nofollow">Homebrew</a></p>
</li>
<li>
<p dir="auto">Use Homebrew to install the necessary dependencies:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="$ brew install cmake ninja ronn macfuse python3 brotli howard-hinnant-date \
               double-conversion fmt glog libarchive libevent flac openssl \
               pkg-config range-v3 utf8cpp xxhash boost zstd jemalloc"><pre><code>$ brew install cmake ninja ronn macfuse python3 brotli howard-hinnant-date \
               double-conversion fmt glog libarchive libevent flac openssl \
               pkg-config range-v3 utf8cpp xxhash boost zstd jemalloc
</code></pre></div>
<ul dir="auto">
<li>
<p dir="auto">When installing macFUSE for the first time, you'll need to explicitly
allow the sofware in <em>System Preferences</em> / <em>Privacy &amp; Security</em>. It's
quite likely that you'll have to reboot after this.</p>
</li>
<li>
<p dir="auto">Clone the DwarFS repository:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="$ git clone --recurse-submodules https://github.com/mhx/dwarfs"><pre><code>$ git clone --recurse-submodules https://github.com/mhx/dwarfs
</code></pre></div>
<ul dir="auto">
<li>Prepare the build by installing the <code>mistletoe</code> python module
in a virtualenv:</li>
</ul>
<div data-snippet-clipboard-copy-content="$ cd dwarfs
$ python3 -m venv @buildenv
$ source ./@buildenv/bin/activate
$ pip3 install mistletoe"><pre><code>$ cd dwarfs
$ python3 -m venv @buildenv
$ source ./@buildenv/bin/activate
$ pip3 install mistletoe
</code></pre></div>
<ul dir="auto">
<li>Build DwarFS and run its tests:</li>
</ul>
<div data-snippet-clipboard-copy-content="$ git checkout v0.9.4
$ git submodule update
$ mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=ON
$ ninja
$ export CTEST_PARALLEL_LEVEL=$(sysctl -n hw.logicalcpu)
$ ninja test"><pre><code>$ git checkout v0.9.4
$ git submodule update
$ mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=ON
$ ninja
$ export CTEST_PARALLEL_LEVEL=$(sysctl -n hw.logicalcpu)
$ ninja test
</code></pre></div>
<ul dir="auto">
<li>Install DwarFS:</li>
</ul>

<p dir="auto">That's it!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Use Cases</h2><a id="user-content-use-cases" aria-label="Permalink: Use Cases" href="#use-cases"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Astrophotography</h3><a id="user-content-astrophotography" aria-label="Permalink: Astrophotography" href="#astrophotography"></a></p>
<p dir="auto">Astrophotography can generate huge amounts of raw image data. During a
single night, it's not unlikely to end up with a few dozens of gigabytes
of data. With most dedicated astrophotography cameras, this data ends up
in the form of FITS images. These are usually uncompressed, don't compress
very well with standard compression algorithms, and while there are certain
compressed FITS formats, these aren't widely supported.</p>
<p dir="auto">One of the compression formats (simply called "Rice") compresses reasonably
well and is really fast. However, its implementation for compressed FITS
has a few drawbacks. The most severe drawbacks are that compression isn't
quite as good as it could be for color sensors and sensors with a less than
16 bits of resolution.</p>
<p dir="auto">DwarFS supports the <code>ricepp</code> (Rice++) compression, which builds on the basic
idea of Rice compression, but makes a few enhancements: it compresses color
and low bit depth images significantly better and always searches for the
optimum solution during compression instead of relying on a heuristic.</p>
<p dir="auto">Let's look at an example using 129 images (darks, flats and lights) taken
with an ASI1600MM camera. Each image is 32 MiB, so a total of 4 GiB of data.
Compressing these with the standard <code>fpack</code> tool takes about 16.6 seconds
and yields a total output size of 2.2 GiB:</p>
<div data-snippet-clipboard-copy-content="$ time fpack */*.fit */*/*.fit

user	14.992
system	1.592
total	16.616

$ find . -name '*.fz' -print0 | xargs -0 cat | wc -c
2369943360"><pre><code>$ time fpack */*.fit */*/*.fit

user	14.992
system	1.592
total	16.616

$ find . -name '*.fz' -print0 | xargs -0 cat | wc -c
2369943360
</code></pre></div>
<p dir="auto">However, this leaves you with <code>*.fz</code> files that not every application can
actually read.</p>
<p dir="auto">Using DwarFS, here's what we get:</p>
<div data-snippet-clipboard-copy-content="$ mkdwarfs -i ASI1600 -o asi1600-20.dwarfs -S 20 --categorize
I 08:47:47.459077 scanning &quot;ASI1600&quot;
I 08:47:47.491492 assigning directory and link inodes...
I 08:47:47.491560 waiting for background scanners...
I 08:47:47.675241 scanning CPU time: 1.051s
I 08:47:47.675271 finalizing file inodes...
I 08:47:47.675330 saved 0 B / 3.941 GiB in 0/258 duplicate files
I 08:47:47.675360 assigning device inodes...
I 08:47:47.675371 assigning pipe/socket inodes...
I 08:47:47.675381 building metadata...
I 08:47:47.675393 building blocks...
I 08:47:47.675398 saving names and symlinks...
I 08:47:47.675514 updating name and link indices...
I 08:47:47.675796 waiting for segmenting/blockifying to finish...
I 08:47:50.274285 total ordering CPU time: 616.3us
I 08:47:50.274329 total segmenting CPU time: 1.132s
I 08:47:50.279476 saving chunks...
I 08:47:50.279622 saving directories...
I 08:47:50.279674 saving shared files table...
I 08:47:50.280745 saving names table... [1.047ms]
I 08:47:50.280768 saving symlinks table... [743ns]
I 08:47:50.282031 waiting for compression to finish...
I 08:47:50.823924 compressed 3.941 GiB to 1.201 GiB (ratio=0.304825)
I 08:47:50.824280 compression CPU time: 17.92s
I 08:47:50.824316 filesystem created without errors [3.366s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
5 dirs, 0/0 soft/hard links, 258/258 files, 0 other
original size: 3.941 GiB, hashed: 315.4 KiB (18 files, 0 B/s)
scanned: 3.941 GiB (258 files, 117.1 GiB/s), categorizing: 0 B/s
saved by deduplication: 0 B (0 files), saved by segmenting: 0 B
filesystem: 3.941 GiB in 4037 blocks (4550 chunks, 516/516 fragments, 258 inodes)
compressed filesystem: 4037 blocks/1.201 GiB written"><pre><code>$ mkdwarfs -i ASI1600 -o asi1600-20.dwarfs -S 20 --categorize
I 08:47:47.459077 scanning "ASI1600"
I 08:47:47.491492 assigning directory and link inodes...
I 08:47:47.491560 waiting for background scanners...
I 08:47:47.675241 scanning CPU time: 1.051s
I 08:47:47.675271 finalizing file inodes...
I 08:47:47.675330 saved 0 B / 3.941 GiB in 0/258 duplicate files
I 08:47:47.675360 assigning device inodes...
I 08:47:47.675371 assigning pipe/socket inodes...
I 08:47:47.675381 building metadata...
I 08:47:47.675393 building blocks...
I 08:47:47.675398 saving names and symlinks...
I 08:47:47.675514 updating name and link indices...
I 08:47:47.675796 waiting for segmenting/blockifying to finish...
I 08:47:50.274285 total ordering CPU time: 616.3us
I 08:47:50.274329 total segmenting CPU time: 1.132s
I 08:47:50.279476 saving chunks...
I 08:47:50.279622 saving directories...
I 08:47:50.279674 saving shared files table...
I 08:47:50.280745 saving names table... [1.047ms]
I 08:47:50.280768 saving symlinks table... [743ns]
I 08:47:50.282031 waiting for compression to finish...
I 08:47:50.823924 compressed 3.941 GiB to 1.201 GiB (ratio=0.304825)
I 08:47:50.824280 compression CPU time: 17.92s
I 08:47:50.824316 filesystem created without errors [3.366s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
5 dirs, 0/0 soft/hard links, 258/258 files, 0 other
original size: 3.941 GiB, hashed: 315.4 KiB (18 files, 0 B/s)
scanned: 3.941 GiB (258 files, 117.1 GiB/s), categorizing: 0 B/s
saved by deduplication: 0 B (0 files), saved by segmenting: 0 B
filesystem: 3.941 GiB in 4037 blocks (4550 chunks, 516/516 fragments, 258 inodes)
compressed filesystem: 4037 blocks/1.201 GiB written
</code></pre></div>
<p dir="auto">In less than 3.4 seconds, it compresses the data down to 1.2 GiB, almost
half the size of the <code>fpack</code> output.</p>
<p dir="auto">In addition to saving a lot of disk space, this can also be useful when your
data is stored on a NAS. Here's a comparison of the same set of data accessed
over a 1 Gb/s network connection, first using the uncompressed raw data:</p>
<div data-snippet-clipboard-copy-content="find /mnt/ASI1600 -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 36.0455 s, 117 MB/s"><pre><code>find /mnt/ASI1600 -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 36.0455 s, 117 MB/s
</code></pre></div>
<p dir="auto">And next using a DwarFS image on the same share:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs /mnt/asi1600-20.dwarfs mnt

$ find mnt -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 14.3681 s, 294 MB/s"><pre><code>$ dwarfs /mnt/asi1600-20.dwarfs mnt

$ find mnt -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 14.3681 s, 294 MB/s
</code></pre></div>
<p dir="auto">That's roughly 2.5 times faster. You can very likely see similar results
with slow external hard drives.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dealing with Bit Rot</h2><a id="user-content-dealing-with-bit-rot" aria-label="Permalink: Dealing with Bit Rot" href="#dealing-with-bit-rot"></a></p>
<p dir="auto">Currently, DwarFS has no built-in ability to add recovery information to a
file system image. However, for archival purposes, it's a good idea to have
such recovery infomation in order to be able to repair a damaged image.</p>
<p dir="auto">This is fortunately relatively straightforward using something like
<a href="https://github.com/Parchive/par2cmdline">par2cmdline</a>:</p>
<div data-snippet-clipboard-copy-content="$ par2create -n1 asi1600-20.dwarfs"><pre><code>$ par2create -n1 asi1600-20.dwarfs
</code></pre></div>
<p dir="auto">This will create two additional files that you can place alongside the image
(or on a different storage), as you'll only need them if DwarFS has detected
an issue with the file system image. If there's an issue, you can run</p>
<div data-snippet-clipboard-copy-content="$ par2repair asi1600-20.dwarfs"><pre><code>$ par2repair asi1600-20.dwarfs
</code></pre></div>
<p dir="auto">which will very likely be able to recover the image if less than 5% (that's
the default used by <code>par2create</code>) of the image are damaged.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extended Attributes</h2><a id="user-content-extended-attributes" aria-label="Permalink: Extended Attributes" href="#extended-attributes"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Preserving Extended Attributes in DwarFS Images</h3><a id="user-content-preserving-extended-attributes-in-dwarfs-images" aria-label="Permalink: Preserving Extended Attributes in DwarFS Images" href="#preserving-extended-attributes-in-dwarfs-images"></a></p>
<p dir="auto">Extended attributes are not currently supported. Any extended attributes
stored in the source file system will not currently be preserved when
building a DwarFS image using <code>mkdwarfs</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Extended Attributes exposed by the FUSE Driver</h3><a id="user-content-extended-attributes-exposed-by-the-fuse-driver" aria-label="Permalink: Extended Attributes exposed by the FUSE Driver" href="#extended-attributes-exposed-by-the-fuse-driver"></a></p>
<p dir="auto">That being said, the root inode of a mounted DwarFS image currently exposes
one or two extended attributes on Linux:</p>
<div data-snippet-clipboard-copy-content="$ attr -l mnt
Attribute &quot;dwarfs.driver.pid&quot; has a 4 byte value for mnt
Attribute &quot;dwarfs.driver.perfmon&quot; has a 4849 byte value for mnt"><pre><code>$ attr -l mnt
Attribute "dwarfs.driver.pid" has a 4 byte value for mnt
Attribute "dwarfs.driver.perfmon" has a 4849 byte value for mnt
</code></pre></div>
<p dir="auto">The <code>dwarfs.driver.pid</code> attribute simply contains the PID of the DwarFS
FUSE driver. The <code>dwarfs.driver.perfmon</code> attribute contains the current
results of the <a href="#performance-monitoring">performance monitor</a>.</p>
<p dir="auto">Furthermore, each regular file exposes an attribute <code>dwarfs.inodeinfo</code>
with information about the undelying inode:</p>
<div data-snippet-clipboard-copy-content="$ attr -l &quot;05 Disappear.caf&quot;
Attribute &quot;dwarfs.inodeinfo&quot; has a 448 byte value for 05 Disappear.caf"><pre><code>$ attr -l "05 Disappear.caf"
Attribute "dwarfs.inodeinfo" has a 448 byte value for 05 Disappear.caf
</code></pre></div>
<p dir="auto">The attribute contains a JSON object with information about the
underlying inode:</p>
<div data-snippet-clipboard-copy-content="$ attr -qg dwarfs.inodeinfo &quot;05 Disappear.caf&quot;
{
  &quot;chunks&quot;: [
    {
      &quot;block&quot;: 2,
      &quot;category&quot;: &quot;pcmaudio/metadata&quot;,
      &quot;offset&quot;: 270976,
      &quot;size&quot;: 4096
    },
    {
      &quot;block&quot;: 414,
      &quot;category&quot;: &quot;pcmaudio/waveform&quot;,
      &quot;offset&quot;: 37594368,
      &quot;size&quot;: 29514492
    },
    {
      &quot;block&quot;: 419,
      &quot;category&quot;: &quot;pcmaudio/waveform&quot;,
      &quot;offset&quot;: 0,
      &quot;size&quot;: 29385468
    }
  ],
  &quot;gid&quot;: 100,
  &quot;mode&quot;: 33188,
  &quot;modestring&quot;: &quot;----rw-r--r--&quot;,
  &quot;uid&quot;: 1000
}"><pre><code>$ attr -qg dwarfs.inodeinfo "05 Disappear.caf"
{
  "chunks": [
    {
      "block": 2,
      "category": "pcmaudio/metadata",
      "offset": 270976,
      "size": 4096
    },
    {
      "block": 414,
      "category": "pcmaudio/waveform",
      "offset": 37594368,
      "size": 29514492
    },
    {
      "block": 419,
      "category": "pcmaudio/waveform",
      "offset": 0,
      "size": 29385468
    }
  ],
  "gid": 100,
  "mode": 33188,
  "modestring": "----rw-r--r--",
  "uid": 1000
}
</code></pre></div>
<p dir="auto">This is useful, for example, to check how a particular file is spread
across multiple blocks or which categories have been assigned to the
file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comparison</h2><a id="user-content-comparison" aria-label="Permalink: Comparison" href="#comparison"></a></p>
<p dir="auto">The SquashFS, <code>xz</code>, <code>lrzip</code>, <code>zpaq</code> and <code>wimlib</code> tests were all done on
an 8 core Intel(R) Xeon(R) E-2286M CPU @ 2.40GHz with 64 GiB of RAM.</p>
<p dir="auto">The Cromfs and EROFS tests were done with an older version of DwarFS
on a 6 core Intel(R) Xeon(R) CPU D-1528 @ 1.90GHz with 64 GiB of RAM.</p>
<p dir="auto">The systems were mostly idle during all of the tests.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With SquashFS</h3><a id="user-content-with-squashfs" aria-label="Permalink: With SquashFS" href="#with-squashfs"></a></p>
<p dir="auto">The source directory contained <strong>1139 different Perl installations</strong>
from 284 distinct releases, a total of 47.65 GiB of data in 1,927,501
files and 330,733 directories. The source directory was freshly
unpacked from a tar archive to an XFS partition on a 970 EVO Plus 2TB
NVME drive, so most of its contents were likely cached.</p>
<p dir="auto">I'm using the same compression type and compression level for
SquashFS that is the default setting for DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ time mksquashfs install perl-install.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on perl-install-zstd.squashfs, block size 131072.
[=========================================================/] 2107401/2107401 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 4637597.63 Kbytes (4528.90 Mbytes)
        9.29% of uncompressed filesystem size (49922299.04 Kbytes)
Inode table size 19100802 bytes (18653.13 Kbytes)
        26.06% of uncompressed inode table size (73307702 bytes)
Directory table size 19128340 bytes (18680.02 Kbytes)
        46.28% of uncompressed directory table size (41335540 bytes)
Number of duplicate files found 1780387
Number of inodes 2255794
Number of files 1925061
Number of fragments 28713
Number of symbolic links  0
Number of device nodes 0
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 330733
Number of ids (unique uids + gids) 2
Number of uids 1
        mhx (1000)
Number of gids 1
        users (100)

real    32m54.713s
user    501m46.382s
sys     0m58.528s"><pre><code>$ time mksquashfs install perl-install.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on perl-install-zstd.squashfs, block size 131072.
[=========================================================/] 2107401/2107401 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 4637597.63 Kbytes (4528.90 Mbytes)
        9.29% of uncompressed filesystem size (49922299.04 Kbytes)
Inode table size 19100802 bytes (18653.13 Kbytes)
        26.06% of uncompressed inode table size (73307702 bytes)
Directory table size 19128340 bytes (18680.02 Kbytes)
        46.28% of uncompressed directory table size (41335540 bytes)
Number of duplicate files found 1780387
Number of inodes 2255794
Number of files 1925061
Number of fragments 28713
Number of symbolic links  0
Number of device nodes 0
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 330733
Number of ids (unique uids + gids) 2
Number of uids 1
        mhx (1000)
Number of gids 1
        users (100)

real    32m54.713s
user    501m46.382s
sys     0m58.528s
</code></pre></div>
<p dir="auto">For DwarFS, I'm sticking to the defaults:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install -o perl-install.dwarfs
I 11:33:33.310931 scanning install
I 11:33:39.026712 waiting for background scanners...
I 11:33:50.681305 assigning directory and link inodes...
I 11:33:50.888441 finding duplicate files...
I 11:34:01.120800 saved 28.2 GiB / 47.65 GiB in 1782826/1927501 duplicate files
I 11:34:01.122608 waiting for inode scanners...
I 11:34:12.839065 assigning device inodes...
I 11:34:12.875520 assigning pipe/socket inodes...
I 11:34:12.910431 building metadata...
I 11:34:12.910524 building blocks...
I 11:34:12.910594 saving names and links...
I 11:34:12.910691 bloom filter size: 32 KiB
I 11:34:12.910760 ordering 144675 inodes using nilsimsa similarity...
I 11:34:12.915555 nilsimsa: depth=20000 (1000), limit=255
I 11:34:13.052525 updating name and link indices...
I 11:34:13.276233 pre-sorted index (660176 name, 366179 path lookups) [360.6ms]
I 11:35:44.039375 144675 inodes ordered [91.13s]
I 11:35:44.041427 waiting for segmenting/blockifying to finish...
I 11:37:38.823902 bloom filter reject rate: 96.017% (TPR=0.244%, lookups=4740563665)
I 11:37:38.823963 segmentation matches: good=454708, bad=6819, total=464247
I 11:37:38.824005 segmentation collisions: L1=0.008%, L2=0.000% [2233254 hashes]
I 11:37:38.824038 saving chunks...
I 11:37:38.860939 saving directories...
I 11:37:41.318747 waiting for compression to finish...
I 11:38:56.046809 compressed 47.65 GiB to 430.9 MiB (ratio=0.00883101)
I 11:38:56.304922 filesystem created without errors [323s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
330733 dirs, 0/2440 soft/hard links, 1927501/1927501 files, 0 other
original size: 47.65 GiB, dedupe: 28.2 GiB (1782826 files), segment: 15.19 GiB
filesystem: 4.261 GiB in 273 blocks (319178 chunks, 144675/144675 inodes)
compressed filesystem: 273 blocks/430.9 MiB written [depth: 20000]
█████████████████████████████████████████████████████████████████████████████▏100% |

real    5m23.030s
user    78m7.554s
sys     1m47.968s"><pre><code>$ time mkdwarfs -i install -o perl-install.dwarfs
I 11:33:33.310931 scanning install
I 11:33:39.026712 waiting for background scanners...
I 11:33:50.681305 assigning directory and link inodes...
I 11:33:50.888441 finding duplicate files...
I 11:34:01.120800 saved 28.2 GiB / 47.65 GiB in 1782826/1927501 duplicate files
I 11:34:01.122608 waiting for inode scanners...
I 11:34:12.839065 assigning device inodes...
I 11:34:12.875520 assigning pipe/socket inodes...
I 11:34:12.910431 building metadata...
I 11:34:12.910524 building blocks...
I 11:34:12.910594 saving names and links...
I 11:34:12.910691 bloom filter size: 32 KiB
I 11:34:12.910760 ordering 144675 inodes using nilsimsa similarity...
I 11:34:12.915555 nilsimsa: depth=20000 (1000), limit=255
I 11:34:13.052525 updating name and link indices...
I 11:34:13.276233 pre-sorted index (660176 name, 366179 path lookups) [360.6ms]
I 11:35:44.039375 144675 inodes ordered [91.13s]
I 11:35:44.041427 waiting for segmenting/blockifying to finish...
I 11:37:38.823902 bloom filter reject rate: 96.017% (TPR=0.244%, lookups=4740563665)
I 11:37:38.823963 segmentation matches: good=454708, bad=6819, total=464247
I 11:37:38.824005 segmentation collisions: L1=0.008%, L2=0.000% [2233254 hashes]
I 11:37:38.824038 saving chunks...
I 11:37:38.860939 saving directories...
I 11:37:41.318747 waiting for compression to finish...
I 11:38:56.046809 compressed 47.65 GiB to 430.9 MiB (ratio=0.00883101)
I 11:38:56.304922 filesystem created without errors [323s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
330733 dirs, 0/2440 soft/hard links, 1927501/1927501 files, 0 other
original size: 47.65 GiB, dedupe: 28.2 GiB (1782826 files), segment: 15.19 GiB
filesystem: 4.261 GiB in 273 blocks (319178 chunks, 144675/144675 inodes)
compressed filesystem: 273 blocks/430.9 MiB written [depth: 20000]
█████████████████████████████████████████████████████████████████████████████▏100% |

real    5m23.030s
user    78m7.554s
sys     1m47.968s
</code></pre></div>
<p dir="auto">So in this comparison, <code>mkdwarfs</code> is <strong>more than 6 times faster</strong> than <code>mksquashfs</code>,
both in terms of CPU time and wall clock time.</p>
<div data-snippet-clipboard-copy-content="$ ll perl-install.*fs
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs"><pre><code>$ ll perl-install.*fs
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs
</code></pre></div>
<p dir="auto">In terms of compression ratio, the <strong>DwarFS file system is more than 10 times
smaller than the SquashFS file system</strong>. With DwarFS, the content has been
<strong>compressed down to less than 0.9% (!) of its original size</strong>. This compression
ratio only considers the data stored in the individual files, not the actual
disk space used. On the original XFS file system, according to <code>du</code>, the
source folder uses 52 GiB, so <strong>the DwarFS image actually only uses 0.8% of
the original space</strong>.</p>
<p dir="auto">Here's another comparison using <code>lzma</code> compression instead of <code>zstd</code>:</p>
<div data-snippet-clipboard-copy-content="$ time mksquashfs install perl-install-lzma.squashfs -comp lzma

real    13m42.825s
user    205m40.851s
sys     3m29.088s"><pre><code>$ time mksquashfs install perl-install-lzma.squashfs -comp lzma

real    13m42.825s
user    205m40.851s
sys     3m29.088s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install -o perl-install-lzma.dwarfs -l9

real    3m43.937s
user    49m45.295s
sys     1m44.550s"><pre><code>$ time mkdwarfs -i install -o perl-install-lzma.dwarfs -l9

real    3m43.937s
user    49m45.295s
sys     1m44.550s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install-lzma.*fs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-lzma.dwarfs
-rw-r--r-- 1 mhx users 3838406656 Mar  3 20:50 perl-install-lzma.squashfs"><pre><code>$ ll perl-install-lzma.*fs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-lzma.dwarfs
-rw-r--r-- 1 mhx users 3838406656 Mar  3 20:50 perl-install-lzma.squashfs
</code></pre></div>
<p dir="auto">It's immediately obvious that the runs are significantly faster and the
resulting images are significantly smaller. Still, <code>mkdwarfs</code> is about
<strong>4 times faster</strong> and produces and image that's <strong>12 times smaller</strong> than
the SquashFS image. The DwarFS image is only 0.6% of the original file size.</p>
<p dir="auto">So, why not use <code>lzma</code> instead of <code>zstd</code> by default? The reason is that <code>lzma</code>
is about an order of magnitude slower to decompress than <code>zstd</code>. If you're
only accessing data on your compressed filesystem occasionally, this might
not be a big deal, but if you use it extensively, <code>zstd</code> will result in
better performance.</p>
<p dir="auto">The comparisons above are not completely fair. <code>mksquashfs</code> by default
uses a block size of 128KiB, whereas <code>mkdwarfs</code> uses 16MiB blocks by default,
or even 64MiB blocks with <code>-l9</code>. When using identical block sizes for both
file systems, the difference, quite expectedly, becomes a lot less dramatic:</p>
<div data-snippet-clipboard-copy-content="$ time mksquashfs install perl-install-lzma-1M.squashfs -comp lzma -b 1M

real    15m43.319s
user    139m24.533s
sys     0m45.132s"><pre><code>$ time mksquashfs install perl-install-lzma-1M.squashfs -comp lzma -b 1M

real    15m43.319s
user    139m24.533s
sys     0m45.132s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install -o perl-install-lzma-1M.dwarfs -l9 -S20 -B3

real    4m25.973s
user    52m15.100s
sys     7m41.889s"><pre><code>$ time mkdwarfs -i install -o perl-install-lzma-1M.dwarfs -l9 -S20 -B3

real    4m25.973s
user    52m15.100s
sys     7m41.889s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install*.*fs
-rw-r--r-- 1 mhx users  935953866 Mar 13 12:12 perl-install-lzma-1M.dwarfs
-rw-r--r-- 1 mhx users 3407474688 Mar  3 21:54 perl-install-lzma-1M.squashfs"><pre><code>$ ll perl-install*.*fs
-rw-r--r-- 1 mhx users  935953866 Mar 13 12:12 perl-install-lzma-1M.dwarfs
-rw-r--r-- 1 mhx users 3407474688 Mar  3 21:54 perl-install-lzma-1M.squashfs
</code></pre></div>
<p dir="auto">Even this is <em>still</em> not entirely fair, as it uses a feature (<code>-B3</code>) that allows
DwarFS to reference file chunks from up to two previous filesystem blocks.</p>
<p dir="auto">But the point is that this is really where SquashFS tops out, as it doesn't
support larger block sizes or back-referencing. And as you'll see below, the
larger blocks that DwarFS is using by default don't necessarily negatively
impact performance.</p>
<p dir="auto">DwarFS also features an option to recompress an existing file system with
a different compression algorithm. This can be useful as it allows relatively
fast experimentation with different algorithms and options without requiring
a full rebuild of the file system. For example, recompressing the above file
system with the best possible compression (<code>-l 9</code>):</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs --recompress -i perl-install.dwarfs -o perl-lzma-re.dwarfs -l9
I 20:28:03.246534 filesystem rewrittenwithout errors [148.3s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
filesystem: 4.261 GiB in 273 blocks (0 chunks, 0 inodes)
compressed filesystem: 273/273 blocks/372.7 MiB written
████████████████████████████████████████████████████████████████████▏100% \

real    2m28.279s
user    37m8.825s
sys     0m43.256s"><pre><code>$ time mkdwarfs --recompress -i perl-install.dwarfs -o perl-lzma-re.dwarfs -l9
I 20:28:03.246534 filesystem rewrittenwithout errors [148.3s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
filesystem: 4.261 GiB in 273 blocks (0 chunks, 0 inodes)
compressed filesystem: 273/273 blocks/372.7 MiB written
████████████████████████████████████████████████████████████████████▏100% \

real    2m28.279s
user    37m8.825s
sys     0m43.256s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-*.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 390845518 Mar  4 20:28 perl-lzma-re.dwarfs
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-lzma.dwarfs"><pre><code>$ ll perl-*.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 390845518 Mar  4 20:28 perl-lzma-re.dwarfs
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-lzma.dwarfs
</code></pre></div>
<p dir="auto">Note that while the recompressed filesystem is smaller than the original image,
it is still a lot bigger than the filesystem we previously build with <code>-l9</code>.
The reason is that the recompressed image still uses the same block size, and
the block size cannot be changed by recompressing.</p>
<p dir="auto">In terms of how fast the file system is when using it, a quick test
I've done is to freshly mount the filesystem created above and run
each of the 1139 <code>perl</code> executables to print their version.</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c &quot;umount mnt&quot; -p &quot;umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1&quot; -P procs 5 20 -D 5 &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.810 s ±  0.013 s    [User: 1.847 s, System: 0.623 s]
  Range (min … max):    1.788 s …  1.825 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.333 s ±  0.009 s    [User: 1.993 s, System: 0.656 s]
  Range (min … max):    1.321 s …  1.354 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.181 s ±  0.018 s    [User: 2.086 s, System: 0.712 s]
  Range (min … max):    1.165 s …  1.214 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.149 s ±  0.015 s    [User: 2.128 s, System: 0.781 s]
  Range (min … max):    1.136 s …  1.186 s    10 runs"><pre><code>$ hyperfine -c "umount mnt" -p "umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1" -P procs 5 20 -D 5 "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.810 s ±  0.013 s    [User: 1.847 s, System: 0.623 s]
  Range (min … max):    1.788 s …  1.825 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.333 s ±  0.009 s    [User: 1.993 s, System: 0.656 s]
  Range (min … max):    1.321 s …  1.354 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.181 s ±  0.018 s    [User: 2.086 s, System: 0.712 s]
  Range (min … max):    1.165 s …  1.214 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.149 s ±  0.015 s    [User: 2.128 s, System: 0.781 s]
  Range (min … max):    1.136 s …  1.186 s    10 runs
</code></pre></div>
<p dir="auto">These timings are for <em>initial</em> runs on a freshly mounted file system,
running 5, 10, 15 and 20 processes in parallel. 1.1 seconds means that
it takes only about 1 millisecond per Perl binary.</p>
<p dir="auto">Following are timings for <em>subsequent</em> runs, both on DwarFS (at <code>mnt</code>)
and the original XFS (at <code>install</code>). DwarFS is around 15% slower here:</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -P procs 10 20 -D 10 -w1 &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot; &quot;ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     347.0 ms ±   7.2 ms    [User: 1.755 s, System: 0.452 s]
  Range (min … max):   341.3 ms … 365.2 ms    10 runs

Benchmark #2: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     302.5 ms ±   3.3 ms    [User: 1.656 s, System: 0.377 s]
  Range (min … max):   297.1 ms … 308.7 ms    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     342.2 ms ±   4.1 ms    [User: 1.766 s, System: 0.451 s]
  Range (min … max):   336.0 ms … 349.7 ms    10 runs

Benchmark #4: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     302.0 ms ±   3.0 ms    [User: 1.659 s, System: 0.374 s]
  Range (min … max):   297.0 ms … 305.4 ms    10 runs

Summary
  'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'' ran
    1.00 ± 0.01 times faster than 'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null''
    1.13 ± 0.02 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null''
    1.15 ± 0.03 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null''"><pre><code>$ hyperfine -P procs 10 20 -D 10 -w1 "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'" "ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     347.0 ms ±   7.2 ms    [User: 1.755 s, System: 0.452 s]
  Range (min … max):   341.3 ms … 365.2 ms    10 runs

Benchmark #2: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     302.5 ms ±   3.3 ms    [User: 1.656 s, System: 0.377 s]
  Range (min … max):   297.1 ms … 308.7 ms    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     342.2 ms ±   4.1 ms    [User: 1.766 s, System: 0.451 s]
  Range (min … max):   336.0 ms … 349.7 ms    10 runs

Benchmark #4: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     302.0 ms ±   3.0 ms    [User: 1.659 s, System: 0.374 s]
  Range (min … max):   297.0 ms … 305.4 ms    10 runs

Summary
  'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'' ran
    1.00 ± 0.01 times faster than 'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null''
    1.13 ± 0.02 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null''
    1.15 ± 0.03 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null''
</code></pre></div>
<p dir="auto">Using the lzma-compressed file system, the metrics for <em>initial</em> runs look
considerably worse (about an order of magnitude):</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c &quot;umount mnt&quot; -p &quot;umount mnt; dwarfs perl-install-lzma.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1&quot; -P procs 5 20 -D 5 &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     10.660 s ±  0.057 s    [User: 1.952 s, System: 0.729 s]
  Range (min … max):   10.615 s … 10.811 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      9.092 s ±  0.021 s    [User: 1.979 s, System: 0.680 s]
  Range (min … max):    9.059 s …  9.126 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      9.012 s ±  0.188 s    [User: 2.077 s, System: 0.702 s]
  Range (min … max):    8.839 s …  9.277 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      9.004 s ±  0.298 s    [User: 2.134 s, System: 0.736 s]
  Range (min … max):    8.611 s …  9.555 s    10 runs"><pre><code>$ hyperfine -c "umount mnt" -p "umount mnt; dwarfs perl-install-lzma.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1" -P procs 5 20 -D 5 "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     10.660 s ±  0.057 s    [User: 1.952 s, System: 0.729 s]
  Range (min … max):   10.615 s … 10.811 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      9.092 s ±  0.021 s    [User: 1.979 s, System: 0.680 s]
  Range (min … max):    9.059 s …  9.126 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      9.012 s ±  0.188 s    [User: 2.077 s, System: 0.702 s]
  Range (min … max):    8.839 s …  9.277 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      9.004 s ±  0.298 s    [User: 2.134 s, System: 0.736 s]
  Range (min … max):    8.611 s …  9.555 s    10 runs
</code></pre></div>
<p dir="auto">So you might want to consider using <code>zstd</code> instead of <code>lzma</code> if you'd
like to optimize for file system performance. It's also the default
compression used by <code>mkdwarfs</code>.</p>
<p dir="auto">Now here's a comparison with the SquashFS filesystem:</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c 'sudo umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs-zstd &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot; -p 'sudo umount mnt; sudo mount -t squashfs perl-install.squashfs mnt; sleep 1' -n squashfs-zstd &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: dwarfs-zstd
  Time (mean ± σ):      1.151 s ±  0.015 s    [User: 2.147 s, System: 0.769 s]
  Range (min … max):    1.118 s …  1.174 s    10 runs

Benchmark #2: squashfs-zstd
  Time (mean ± σ):      6.733 s ±  0.007 s    [User: 3.188 s, System: 17.015 s]
  Range (min … max):    6.721 s …  6.743 s    10 runs

Summary
  'dwarfs-zstd' ran
    5.85 ± 0.08 times faster than 'squashfs-zstd'"><pre><code>$ hyperfine -c 'sudo umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs-zstd "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'" -p 'sudo umount mnt; sudo mount -t squashfs perl-install.squashfs mnt; sleep 1' -n squashfs-zstd "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: dwarfs-zstd
  Time (mean ± σ):      1.151 s ±  0.015 s    [User: 2.147 s, System: 0.769 s]
  Range (min … max):    1.118 s …  1.174 s    10 runs

Benchmark #2: squashfs-zstd
  Time (mean ± σ):      6.733 s ±  0.007 s    [User: 3.188 s, System: 17.015 s]
  Range (min … max):    6.721 s …  6.743 s    10 runs

Summary
  'dwarfs-zstd' ran
    5.85 ± 0.08 times faster than 'squashfs-zstd'
</code></pre></div>
<p dir="auto">So, DwarFS is almost six times faster than SquashFS. But what's more,
SquashFS also uses significantly more CPU power. However, the numbers
shown above for DwarFS obviously don't include the time spent in the
<code>dwarfs</code> process, so I repeated the test outside of hyperfine:</p>
<div data-snippet-clipboard-copy-content="$ time dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4 -f

real    0m4.569s
user    0m2.154s
sys     0m1.846s"><pre><code>$ time dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4 -f

real    0m4.569s
user    0m2.154s
sys     0m1.846s
</code></pre></div>
<p dir="auto">So, in total, DwarFS was using 5.7 seconds of CPU time, whereas
SquashFS was using 20.2 seconds, almost four times as much. Ignore
the 'real' time, this is only how long it took me to unmount the
file system again after mounting it.</p>
<p dir="auto">Another real-life test was to build and test a Perl module with 624
different Perl versions in the compressed file system. The module I've
used, <a href="https://github.com/mhx/Tie-Hash-Indexed">Tie::Hash::Indexed</a>,
has an XS component that requires a C compiler to build. So this really
accesses a lot of different stuff in the file system:</p>
<ul dir="auto">
<li>
<p dir="auto">The <code>perl</code> executables and its shared libraries</p>
</li>
<li>
<p dir="auto">The Perl modules used for writing the Makefile</p>
</li>
<li>
<p dir="auto">Perl's C header files used for building the module</p>
</li>
<li>
<p dir="auto">More Perl modules used for running the tests</p>
</li>
</ul>
<p dir="auto">I wrote a little script to be able to run multiple builds in parallel:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#!/bin/bash
set -eu
perl=$1
dir=$(echo &quot;$perl&quot; | cut -d/ --output-delimiter=- -f5,6)
rsync -a Tie-Hash-Indexed/ $dir/
cd $dir
$1 Makefile.PL >/dev/null 2>&amp;1
make test >/dev/null 2>&amp;1
cd ..
rm -rf $dir
echo $perl"><pre><span><span>#!</span>/bin/bash</span>
<span>set</span> -eu
perl=<span>$1</span>
dir=<span><span>$(</span>echo <span><span>"</span><span>$perl</span><span>"</span></span> <span>|</span> cut -d/ --output-delimiter=- -f5,6<span>)</span></span>
rsync -a Tie-Hash-Indexed/ <span>$dir</span>/
<span>cd</span> <span>$dir</span>
<span>$1</span> Makefile.PL <span>&gt;</span>/dev/null <span>2&gt;&amp;1</span>
make <span>test</span> <span>&gt;</span>/dev/null <span>2&gt;&amp;1</span>
<span>cd</span> ..
rm -rf <span>$dir</span>
<span>echo</span> <span>$perl</span></pre></div>
<p dir="auto">The following command will run up to 16 builds in parallel on the 8 core
Xeon CPU, including debug, optimized and threaded versions of all Perl
releases between 5.10.0 and 5.33.3, a total of 624 <code>perl</code> installations:</p>
<div data-snippet-clipboard-copy-content="$ time ls -1 /tmp/perl/install/*/perl-5.??.?/bin/perl5* | sort -t / -k 8 | xargs -d $'\n' -P 16 -n 1 ./build.sh"><pre><code>$ time ls -1 /tmp/perl/install/*/perl-5.??.?/bin/perl5* | sort -t / -k 8 | xargs -d $'\n' -P 16 -n 1 ./build.sh
</code></pre></div>
<p dir="auto">Tests were done with a cleanly mounted file system to make sure the caches
were empty. <code>ccache</code> was primed to make sure all compiler runs could be
satisfied from the cache. With SquashFS, the timing was:</p>
<div data-snippet-clipboard-copy-content="real    0m52.385s
user    8m10.333s
sys     4m10.056s"><pre><code>real    0m52.385s
user    8m10.333s
sys     4m10.056s
</code></pre></div>
<p dir="auto">And with DwarFS:</p>
<div data-snippet-clipboard-copy-content="real    0m50.469s
user    9m22.597s
sys     1m18.469s"><pre><code>real    0m50.469s
user    9m22.597s
sys     1m18.469s
</code></pre></div>
<p dir="auto">So, frankly, not much of a difference, with DwarFS being just a bit faster.
The <code>dwarfs</code> process itself used:</p>
<div data-snippet-clipboard-copy-content="real    0m56.686s
user    0m18.857s
sys     0m21.058s"><pre><code>real    0m56.686s
user    0m18.857s
sys     0m21.058s
</code></pre></div>
<p dir="auto">So again, DwarFS used less raw CPU power overall, but in terms of wallclock
time, the difference is really marginal.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With SquashFS &amp; xz</h3><a id="user-content-with-squashfs--xz" aria-label="Permalink: With SquashFS &amp; xz" href="#with-squashfs--xz"></a></p>
<p dir="auto">This test uses slightly less pathological input data: the root filesystem of
a recent Raspberry Pi OS release. This file system also contains device inodes,
so in order to preserve those, we pass <code>--with-devices</code> to <code>mkdwarfs</code>:</p>
<div data-snippet-clipboard-copy-content="$ time sudo mkdwarfs -i raspbian -o raspbian.dwarfs --with-devices
I 21:30:29.812562 scanning raspbian
I 21:30:29.908984 waiting for background scanners...
I 21:30:30.217446 assigning directory and link inodes...
I 21:30:30.221941 finding duplicate files...
I 21:30:30.288099 saved 31.05 MiB / 1007 MiB in 1617/34582 duplicate files
I 21:30:30.288143 waiting for inode scanners...
I 21:30:31.393710 assigning device inodes...
I 21:30:31.394481 assigning pipe/socket inodes...
I 21:30:31.395196 building metadata...
I 21:30:31.395230 building blocks...
I 21:30:31.395291 saving names and links...
I 21:30:31.395374 ordering 32965 inodes using nilsimsa similarity...
I 21:30:31.396254 nilsimsa: depth=20000 (1000), limit=255
I 21:30:31.407967 pre-sorted index (46431 name, 2206 path lookups) [11.66ms]
I 21:30:31.410089 updating name and link indices...
I 21:30:38.178505 32965 inodes ordered [6.783s]
I 21:30:38.179417 waiting for segmenting/blockifying to finish...
I 21:31:06.248304 saving chunks...
I 21:31:06.251998 saving directories...
I 21:31:06.402559 waiting for compression to finish...
I 21:31:16.425563 compressed 1007 MiB to 287 MiB (ratio=0.285036)
I 21:31:16.464772 filesystem created without errors [46.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
4435 dirs, 5908/0 soft/hard links, 34582/34582 files, 7 other
original size: 1007 MiB, dedupe: 31.05 MiB (1617 files), segment: 47.23 MiB
filesystem: 928.4 MiB in 59 blocks (38890 chunks, 32965/32965 inodes)
compressed filesystem: 59 blocks/287 MiB written [depth: 20000]
████████████████████████████████████████████████████████████████████▏100% |

real    0m46.711s
user    10m39.038s
sys     0m8.123s"><pre><code>$ time sudo mkdwarfs -i raspbian -o raspbian.dwarfs --with-devices
I 21:30:29.812562 scanning raspbian
I 21:30:29.908984 waiting for background scanners...
I 21:30:30.217446 assigning directory and link inodes...
I 21:30:30.221941 finding duplicate files...
I 21:30:30.288099 saved 31.05 MiB / 1007 MiB in 1617/34582 duplicate files
I 21:30:30.288143 waiting for inode scanners...
I 21:30:31.393710 assigning device inodes...
I 21:30:31.394481 assigning pipe/socket inodes...
I 21:30:31.395196 building metadata...
I 21:30:31.395230 building blocks...
I 21:30:31.395291 saving names and links...
I 21:30:31.395374 ordering 32965 inodes using nilsimsa similarity...
I 21:30:31.396254 nilsimsa: depth=20000 (1000), limit=255
I 21:30:31.407967 pre-sorted index (46431 name, 2206 path lookups) [11.66ms]
I 21:30:31.410089 updating name and link indices...
I 21:30:38.178505 32965 inodes ordered [6.783s]
I 21:30:38.179417 waiting for segmenting/blockifying to finish...
I 21:31:06.248304 saving chunks...
I 21:31:06.251998 saving directories...
I 21:31:06.402559 waiting for compression to finish...
I 21:31:16.425563 compressed 1007 MiB to 287 MiB (ratio=0.285036)
I 21:31:16.464772 filesystem created without errors [46.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
4435 dirs, 5908/0 soft/hard links, 34582/34582 files, 7 other
original size: 1007 MiB, dedupe: 31.05 MiB (1617 files), segment: 47.23 MiB
filesystem: 928.4 MiB in 59 blocks (38890 chunks, 32965/32965 inodes)
compressed filesystem: 59 blocks/287 MiB written [depth: 20000]
████████████████████████████████████████████████████████████████████▏100% |

real    0m46.711s
user    10m39.038s
sys     0m8.123s
</code></pre></div>
<p dir="auto">Again, SquashFS uses the same compression options:</p>
<div data-snippet-clipboard-copy-content="$ time sudo mksquashfs raspbian raspbian.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on raspbian.squashfs, block size 131072.
[===============================================================\] 39232/39232 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 371934.50 Kbytes (363.22 Mbytes)
        35.98% of uncompressed filesystem size (1033650.60 Kbytes)
Inode table size 399913 bytes (390.54 Kbytes)
        26.53% of uncompressed inode table size (1507581 bytes)
Directory table size 408749 bytes (399.17 Kbytes)
        42.31% of uncompressed directory table size (966174 bytes)
Number of duplicate files found 1618
Number of inodes 44932
Number of files 34582
Number of fragments 3290
Number of symbolic links  5908
Number of device nodes 7
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 4435
Number of ids (unique uids + gids) 18
Number of uids 5
        root (0)
        mhx (1000)
        unknown (103)
        shutdown (6)
        unknown (106)
Number of gids 15
        root (0)
        unknown (109)
        unknown (42)
        unknown (1000)
        users (100)
        unknown (43)
        tty (5)
        unknown (108)
        unknown (111)
        unknown (110)
        unknown (50)
        mail (12)
        nobody (65534)
        adm (4)
        mem (8)

real    0m50.124s
user    9m41.708s
sys     0m1.727s"><pre><code>$ time sudo mksquashfs raspbian raspbian.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on raspbian.squashfs, block size 131072.
[===============================================================\] 39232/39232 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 371934.50 Kbytes (363.22 Mbytes)
        35.98% of uncompressed filesystem size (1033650.60 Kbytes)
Inode table size 399913 bytes (390.54 Kbytes)
        26.53% of uncompressed inode table size (1507581 bytes)
Directory table size 408749 bytes (399.17 Kbytes)
        42.31% of uncompressed directory table size (966174 bytes)
Number of duplicate files found 1618
Number of inodes 44932
Number of files 34582
Number of fragments 3290
Number of symbolic links  5908
Number of device nodes 7
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 4435
Number of ids (unique uids + gids) 18
Number of uids 5
        root (0)
        mhx (1000)
        unknown (103)
        shutdown (6)
        unknown (106)
Number of gids 15
        root (0)
        unknown (109)
        unknown (42)
        unknown (1000)
        users (100)
        unknown (43)
        tty (5)
        unknown (108)
        unknown (111)
        unknown (110)
        unknown (50)
        mail (12)
        nobody (65534)
        adm (4)
        mem (8)

real    0m50.124s
user    9m41.708s
sys     0m1.727s
</code></pre></div>
<p dir="auto">The difference in speed is almost negligible. SquashFS is just a bit
slower here. In terms of compression, the difference also isn't huge:</p>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian.* *.xz
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz
-rw-r--r-- 1 root root  287M Mar  4 21:31 raspbian.dwarfs
-rw-r--r-- 1 root root  364M Mar  4 21:33 raspbian.squashfs"><pre><code>$ ls -lh raspbian.* *.xz
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz
-rw-r--r-- 1 root root  287M Mar  4 21:31 raspbian.dwarfs
-rw-r--r-- 1 root root  364M Mar  4 21:33 raspbian.squashfs
</code></pre></div>
<p dir="auto">Interestingly, <code>xz</code> actually can't compress the whole original image
better than DwarFS.</p>
<p dir="auto">We can even again try to increase the DwarFS compression level:</p>
<div data-snippet-clipboard-copy-content="$ time sudo mkdwarfs -i raspbian -o raspbian-9.dwarfs --with-devices -l9

real    0m54.161s
user    8m40.109s
sys     0m7.101s"><pre><code>$ time sudo mkdwarfs -i raspbian -o raspbian-9.dwarfs --with-devices -l9

real    0m54.161s
user    8m40.109s
sys     0m7.101s
</code></pre></div>
<p dir="auto">Now that actually gets the DwarFS image size well below that of the
<code>xz</code> archive:</p>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian-9.dwarfs *.xz
-rw-r--r-- 1 root root  244M Mar  4 21:36 raspbian-9.dwarfs
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz"><pre><code>$ ls -lh raspbian-9.dwarfs *.xz
-rw-r--r-- 1 root root  244M Mar  4 21:36 raspbian-9.dwarfs
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz
</code></pre></div>
<p dir="auto">Even if you actually build a tarball and compress that (instead of
compressing the EXT4 file system itself), <code>xz</code> isn't quite able to
match the DwarFS image size:</p>
<div data-snippet-clipboard-copy-content="$ time sudo tar cf - raspbian | xz -9 -vT 0 >raspbian.tar.xz
  100 %     246.9 MiB / 1,037.2 MiB = 0.238    13 MiB/s       1:18

real    1m18.226s
user    6m35.381s
sys     0m2.205s"><pre><code>$ time sudo tar cf - raspbian | xz -9 -vT 0 &gt;raspbian.tar.xz
  100 %     246.9 MiB / 1,037.2 MiB = 0.238    13 MiB/s       1:18

real    1m18.226s
user    6m35.381s
sys     0m2.205s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz"><pre><code>$ ls -lh raspbian.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz
</code></pre></div>
<p dir="auto">DwarFS also comes with the <a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfsextract.md">dwarfsextract</a> tool
that allows extraction of a filesystem image without the FUSE driver.
So here's a comparison of the extraction speed:</p>
<div data-snippet-clipboard-copy-content="$ time sudo tar xf raspbian.tar.xz -C out1

real    0m12.846s
user    0m12.313s
sys     0m1.616s"><pre><code>$ time sudo tar xf raspbian.tar.xz -C out1

real    0m12.846s
user    0m12.313s
sys     0m1.616s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time sudo dwarfsextract -i raspbian-9.dwarfs -o out2

real    0m3.825s
user    0m13.234s
sys     0m1.382s"><pre><code>$ time sudo dwarfsextract -i raspbian-9.dwarfs -o out2

real    0m3.825s
user    0m13.234s
sys     0m1.382s
</code></pre></div>
<p dir="auto">So, <code>dwarfsextract</code> is almost 4 times faster thanks to using multiple
worker threads for decompression. It's writing about 300 MiB/s in this
example.</p>
<p dir="auto">Another nice feature of <code>dwarfsextract</code> is that it allows you to directly
output data in an archive format, so you could create a tarball from
your image without extracting the files to disk:</p>
<div data-snippet-clipboard-copy-content="$ dwarfsextract -i raspbian-9.dwarfs -f ustar | xz -9 -T0 >raspbian2.tar.xz"><pre><code>$ dwarfsextract -i raspbian-9.dwarfs -f ustar | xz -9 -T0 &gt;raspbian2.tar.xz
</code></pre></div>
<p dir="auto">This has the interesting side-effect that the resulting tarball will
likely be smaller than the one built straight from the directory:</p>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian*.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz
-rw-r--r-- 1 mhx users 240M Mar  4 23:52 raspbian2.tar.xz"><pre><code>$ ls -lh raspbian*.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz
-rw-r--r-- 1 mhx users 240M Mar  4 23:52 raspbian2.tar.xz
</code></pre></div>
<p dir="auto">That's because <code>dwarfsextract</code> writes files in inode-order, and by
default inodes are ordered by similarity for the best possible
compression.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With lrzip</h3><a id="user-content-with-lrzip" aria-label="Permalink: With lrzip" href="#with-lrzip"></a></p>
<p dir="auto"><a href="https://github.com/ckolivas/lrzip">lrzip</a> is a compression utility
targeted especially at compressing large files. From its description,
it looks like it does something very similar to DwarFS, i.e. it looks
for duplicate segments before passsing the de-duplicated data on to
an <code>lzma</code> compressor.</p>
<p dir="auto">When I first read about <code>lrzip</code>, I was pretty certain it would easily
beat DwarFS. So let's take a look. <code>lrzip</code> operates on a single file,
so it's necessary to first build a tarball:</p>
<div data-snippet-clipboard-copy-content="$ time tar cf perl-install.tar install

real    2m9.568s
user    0m3.757s
sys     0m26.623s"><pre><code>$ time tar cf perl-install.tar install

real    2m9.568s
user    0m3.757s
sys     0m26.623s
</code></pre></div>
<p dir="auto">Now we can run <code>lrzip</code>:</p>
<div data-snippet-clipboard-copy-content="$ time lrzip -vL9 -o perl-install.tar.lrzip perl-install.tar
The following options are in effect for this COMPRESSION.
Threading is ENABLED. Number of CPUs detected: 16
Detected 67106172928 bytes ram
Compression level 9
Nice Value: 19
Show Progress
Verbose
Output Filename Specified: perl-install.tar.lrzip
Temporary Directory set as: ./
Compression mode is: LZMA. LZO Compressibility testing enabled
Heuristically Computed Compression Window: 426 = 42600MB
File size: 52615639040
Will take 2 passes
Beginning rzip pre-processing phase
Beginning rzip pre-processing phase
perl-install.tar - Compression Ratio: 100.378. Average Compression Speed: 14.536MB/s.
Total time: 00:57:32.47

real    57m32.472s
user    81m44.104s
sys     4m50.221s"><pre><code>$ time lrzip -vL9 -o perl-install.tar.lrzip perl-install.tar
The following options are in effect for this COMPRESSION.
Threading is ENABLED. Number of CPUs detected: 16
Detected 67106172928 bytes ram
Compression level 9
Nice Value: 19
Show Progress
Verbose
Output Filename Specified: perl-install.tar.lrzip
Temporary Directory set as: ./
Compression mode is: LZMA. LZO Compressibility testing enabled
Heuristically Computed Compression Window: 426 = 42600MB
File size: 52615639040
Will take 2 passes
Beginning rzip pre-processing phase
Beginning rzip pre-processing phase
perl-install.tar - Compression Ratio: 100.378. Average Compression Speed: 14.536MB/s.
Total time: 00:57:32.47

real    57m32.472s
user    81m44.104s
sys     4m50.221s
</code></pre></div>
<p dir="auto">That definitely took a while. This is about an order of magnitude
slower than <code>mkdwarfs</code> and it barely makes use of the 8 cores.</p>
<div data-snippet-clipboard-copy-content="$ ll -h perl-install.tar.lrzip
-rw-r--r-- 1 mhx users 500M Mar  6 21:16 perl-install.tar.lrzip"><pre><code>$ ll -h perl-install.tar.lrzip
-rw-r--r-- 1 mhx users 500M Mar  6 21:16 perl-install.tar.lrzip
</code></pre></div>
<p dir="auto">This is a surprisingly disappointing result. The archive is 65% larger
than a DwarFS image at <code>-l9</code> that takes less than 4 minutes to build.
Also, you can't just access the files in the <code>.lrzip</code> without fully
unpacking the archive first.</p>
<p dir="auto">That being said, it <em>is</em> better than just using <code>xz</code> on the tarball:</p>
<div data-snippet-clipboard-copy-content="$ time xz -T0 -v9 -c perl-install.tar >perl-install.tar.xz
perl-install.tar (1/1)
  100 %      4,317.0 MiB / 49.0 GiB = 0.086    24 MiB/s      34:55

real    34m55.450s
user    543m50.810s
sys     0m26.533s"><pre><code>$ time xz -T0 -v9 -c perl-install.tar &gt;perl-install.tar.xz
perl-install.tar (1/1)
  100 %      4,317.0 MiB / 49.0 GiB = 0.086    24 MiB/s      34:55

real    34m55.450s
user    543m50.810s
sys     0m26.533s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install.tar.xz -h
-rw-r--r-- 1 mhx users 4.3G Mar  6 22:59 perl-install.tar.xz"><pre><code>$ ll perl-install.tar.xz -h
-rw-r--r-- 1 mhx users 4.3G Mar  6 22:59 perl-install.tar.xz
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">With zpaq</h3><a id="user-content-with-zpaq" aria-label="Permalink: With zpaq" href="#with-zpaq"></a></p>
<p dir="auto"><a href="http://mattmahoney.net/dc/zpaq.html" rel="nofollow">zpaq</a> is a journaling backup
utility and archiver. Again, it appears to share some of the ideas in
DwarFS, like segmentation analysis, but it also adds some features on
top that make it useful for incremental backups. However, it's also
not usable as a file system, so data needs to be extracted before it
can be used.</p>
<p dir="auto">Anyway, how does it fare in terms of speed and compression performance?</p>
<div data-snippet-clipboard-copy-content="$ time zpaq a perl-install.zpaq install -m5"><pre><code>$ time zpaq a perl-install.zpaq install -m5
</code></pre></div>
<p dir="auto">After a few million lines of output that (I think) cannot be turned off:</p>
<div data-snippet-clipboard-copy-content="2258234 +added, 0 -removed.

0.000000 + (51161.953159 -> 8932.000297 -> 490.227707) = 490.227707 MB
2828.082 seconds (all OK)

real    47m8.104s
user    714m44.286s
sys     3m6.751s"><pre><code>2258234 +added, 0 -removed.

0.000000 + (51161.953159 -&gt; 8932.000297 -&gt; 490.227707) = 490.227707 MB
2828.082 seconds (all OK)

real    47m8.104s
user    714m44.286s
sys     3m6.751s
</code></pre></div>
<p dir="auto">So, it's an order of magnitude slower than <code>mkdwarfs</code> and uses 14 times
as much CPU resources as <code>mkdwarfs -l9</code>. The resulting archive it pretty
close in size to the default configuration DwarFS image, but it's more
than 50% bigger than the image produced by <code>mkdwarfs -l9</code>.</p>
<div data-snippet-clipboard-copy-content="$ ll perl-install*.*
-rw-r--r-- 1 mhx users 490227707 Mar  7 01:38 perl-install.zpaq
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs"><pre><code>$ ll perl-install*.*
-rw-r--r-- 1 mhx users 490227707 Mar  7 01:38 perl-install.zpaq
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs
</code></pre></div>
<p dir="auto">What's <em>really</em> surprising is how slow it is to extract the <code>zpaq</code>
archive again:</p>
<div data-snippet-clipboard-copy-content="$ time zpaq x perl-install.zpaq
2798.097 seconds (all OK)

real    46m38.117s
user    711m18.734s
sys     3m47.876s"><pre><code>$ time zpaq x perl-install.zpaq
2798.097 seconds (all OK)

real    46m38.117s
user    711m18.734s
sys     3m47.876s
</code></pre></div>
<p dir="auto">That's 700 times slower than extracting the DwarFS image.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With zpaqfranz</h3><a id="user-content-with-zpaqfranz" aria-label="Permalink: With zpaqfranz" href="#with-zpaqfranz"></a></p>
<p dir="auto"><a href="https://github.com/fcorbelli/zpaqfranz">zpaqfranz</a> is a derivative of zpaq.
Much to my delight, it doesn't generate millions of lines of output.
It claims to be multi-threaded and de-duplicating, so definitely worth
taking a look. Like zpaq, it supports incremental backups.</p>
<p dir="auto">We'll use a different input to compare zpaqfranz and DwarFS: The source code
of 670 different releases of the "wine" emulator. That's 73 gigabytes of data
in total, spread across slightly more than 3 million files. It's obviously
highly redundant and should thus be a good data set to compare the tools.
For reference, a <code>.tar.xz</code> of the directory is still 7 GiB in size and a
SquashFS image of the data gets down to around 1.6 GiB. An "optimized"
<code>.tar.xz</code>, where the input files were ordered by similarity, compresses down
to 399 MiB, almost 20 times better than without ordering.</p>
<p dir="auto">Now it's time to try zpaqfranz. The input data is stored on a fast SSD and a
large fraction of it is already in the file system cache from previous runs,
so disk I/O is not a bottleneck.</p>
<div data-snippet-clipboard-copy-content="$ time ./zpaqfranz a winesrc.zpaq winesrc
zpaqfranz v58.8k-JIT-L(2023-08-05)
Creating winesrc.zpaq at offset 0 + 0
Add 2024-01-11 07:25:22 3.117.413     69.632.090.852 (  64.85 GB) 16T (362.904 dirs)
3.480.317 +added, 0 -removed.

0 + (69.632.090.852 -> 8.347.553.798 -> 617.600.892) = 617.600.892 @ 58.38 MB/s

1137.441 seconds (000:18:57) (all OK)

real    18m58.632s
user    11m51.052s
sys     1m3.389s"><pre><code>$ time ./zpaqfranz a winesrc.zpaq winesrc
zpaqfranz v58.8k-JIT-L(2023-08-05)
Creating winesrc.zpaq at offset 0 + 0
Add 2024-01-11 07:25:22 3.117.413     69.632.090.852 (  64.85 GB) 16T (362.904 dirs)
3.480.317 +added, 0 -removed.

0 + (69.632.090.852 -&gt; 8.347.553.798 -&gt; 617.600.892) = 617.600.892 @ 58.38 MB/s

1137.441 seconds (000:18:57) (all OK)

real    18m58.632s
user    11m51.052s
sys     1m3.389s
</code></pre></div>
<p dir="auto">That is considerably faster than the original zpaq, and uses about 60 times
less CPU resources. The output file is 589 MiB, so slightly larger than both
the "optimized" <code>.tar.gz</code> and the zpaq output.</p>
<p dir="auto">How does <code>mkdwarfs</code> do?</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i winesrc -o winesrc.dwarfs -l9
[...]
I 07:55:20.546636 compressed 64.85 GiB to 93.2 MiB (ratio=0.00140344)
I 07:55:20.826699 compression CPU time: 6.726m
I 07:55:20.827338 filesystem created without errors [2.283m]
[...]

real    2m17.100s
user    9m53.633s
sys     2m29.236s"><pre><code>$ time mkdwarfs -i winesrc -o winesrc.dwarfs -l9
[...]
I 07:55:20.546636 compressed 64.85 GiB to 93.2 MiB (ratio=0.00140344)
I 07:55:20.826699 compression CPU time: 6.726m
I 07:55:20.827338 filesystem created without errors [2.283m]
[...]

real    2m17.100s
user    9m53.633s
sys     2m29.236s
</code></pre></div>
<p dir="auto">It uses pretty much the same amount of CPU resources, but finishes more than
8 times faster. The DwarFS output file is more than 6 times smaller.</p>
<p dir="auto">You can actually squeeze a bit more redundancy out of the original data by
tweaking the similarity ordering and switching from lzma to brotli compression,
albeit at a somewhat slower compression speed:</p>
<div data-snippet-clipboard-copy-content="mkdwarfs -i winesrc -o winesrc.dwarfs -l9 -C brotli:quality=11:lgwin=26 --order=nilsimsa:max-cluster-size=200k
[...]
I 08:21:01.138075 compressed 64.85 GiB to 73.52 MiB (ratio=0.00110716)
I 08:21:01.485737 compression CPU time: 36.58m
I 08:21:01.486313 filesystem created without errors [5.501m]
[...]
real    5m30.178s
user    40m59.193s
sys     2m36.234s"><pre><code>mkdwarfs -i winesrc -o winesrc.dwarfs -l9 -C brotli:quality=11:lgwin=26 --order=nilsimsa:max-cluster-size=200k
[...]
I 08:21:01.138075 compressed 64.85 GiB to 73.52 MiB (ratio=0.00110716)
I 08:21:01.485737 compression CPU time: 36.58m
I 08:21:01.486313 filesystem created without errors [5.501m]
[...]
real    5m30.178s
user    40m59.193s
sys     2m36.234s
</code></pre></div>
<p dir="auto">That's almost a 1000x reduction in size.</p>
<p dir="auto">Let's also look at decompression speed:</p>
<div data-snippet-clipboard-copy-content="$ time zpaqfranz x winesrc.zpaq
zpaqfranz v58.8k-JIT-L(2023-08-05)
/home/mhx/winesrc.zpaq:
1 versions, 3.480.317 files, 617.600.892 bytes (588.99 MB)
Extract 69.632.090.852 bytes (64.85 GB) in 3.117.413 files (362.904 folders) / 16 T
        99.18% 00:00:00  (  64.32 GB)=>(  64.85 GB)  548.83 MB/sec

125.636 seconds (000:02:05) (all OK)

real    2m6.968s
user    1m36.177s
sys     1m10.980s"><pre><code>$ time zpaqfranz x winesrc.zpaq
zpaqfranz v58.8k-JIT-L(2023-08-05)
/home/mhx/winesrc.zpaq:
1 versions, 3.480.317 files, 617.600.892 bytes (588.99 MB)
Extract 69.632.090.852 bytes (64.85 GB) in 3.117.413 files (362.904 folders) / 16 T
        99.18% 00:00:00  (  64.32 GB)=&gt;(  64.85 GB)  548.83 MB/sec

125.636 seconds (000:02:05) (all OK)

real    2m6.968s
user    1m36.177s
sys     1m10.980s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time dwarfsextract -i winesrc.dwarfs

real    1m49.182s
user    0m34.667s
sys     1m28.733s"><pre><code>$ time dwarfsextract -i winesrc.dwarfs

real    1m49.182s
user    0m34.667s
sys     1m28.733s
</code></pre></div>
<p dir="auto">Decompression time is pretty much in the same ballpark, with just slightly
shorter times for the DwarFS image.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With wimlib</h3><a id="user-content-with-wimlib" aria-label="Permalink: With wimlib" href="#with-wimlib"></a></p>
<p dir="auto"><a href="https://wimlib.net/" rel="nofollow">wimlib</a> is a really interesting project that is
a lot more mature than DwarFS. While DwarFS at its core has a library
component that could potentially be ported to other operating systems,
wimlib already is available on many platforms. It also seems to have
quite a rich set of features, so it's definitely worth taking a look at.</p>
<p dir="auto">I first tried <code>wimcapture</code> on the perl dataset:</p>
<div data-snippet-clipboard-copy-content="$ time wimcapture --unix-data --solid --solid-chunk-size=16M install perl-install.wim
Scanning &quot;install&quot;
47 GiB scanned (1927501 files, 330733 directories)
Using LZMS compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    15m23.310s
user    174m29.274s
sys     0m42.921s"><pre><code>$ time wimcapture --unix-data --solid --solid-chunk-size=16M install perl-install.wim
Scanning "install"
47 GiB scanned (1927501 files, 330733 directories)
Using LZMS compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    15m23.310s
user    174m29.274s
sys     0m42.921s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install.*
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs
-rw-r--r-- 1 mhx users 1016981520 Mar  6 21:12 perl-install.wim"><pre><code>$ ll perl-install.*
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs
-rw-r--r-- 1 mhx users 1016981520 Mar  6 21:12 perl-install.wim
</code></pre></div>
<p dir="auto">So, wimlib is definitely much better than squashfs, in terms of both
compression ratio and speed. DwarFS is however about 3 times faster to
create the file system and the DwarFS file system less than half the size.
When switching to LZMA compression, the DwarFS file system is more than
3 times smaller (wimlib uses LZMS compression by default).</p>
<p dir="auto">What's a bit surprising is that mounting a <em>wim</em> file takes quite a bit
of time:</p>
<div data-snippet-clipboard-copy-content="$ time wimmount perl-install.wim mnt
[WARNING] Mounting a WIM file containing solid-compressed data; file access may be slow.

real    0m2.038s
user    0m1.764s
sys     0m0.242s"><pre><code>$ time wimmount perl-install.wim mnt
[WARNING] Mounting a WIM file containing solid-compressed data; file access may be slow.

real    0m2.038s
user    0m1.764s
sys     0m0.242s
</code></pre></div>
<p dir="auto">Mounting the DwarFS image takes almost no time in comparison:</p>
<div data-snippet-clipboard-copy-content="$ time git/github/dwarfs/build-clang-11/dwarfs perl-install-default.dwarfs mnt
I 00:23:39.238182 dwarfs (v0.4.0, fuse version 35)

real    0m0.003s
user    0m0.003s
sys     0m0.000s"><pre><code>$ time git/github/dwarfs/build-clang-11/dwarfs perl-install-default.dwarfs mnt
I 00:23:39.238182 dwarfs (v0.4.0, fuse version 35)

real    0m0.003s
user    0m0.003s
sys     0m0.000s
</code></pre></div>
<p dir="auto">That's just because it immediately forks into background by default and
initializes the file system in the background. However, even when
running it in the foreground, initializing the file system takes only
about 60 milliseconds:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs perl-install.dwarfs mnt -f
I 00:25:03.186005 dwarfs (v0.4.0, fuse version 35)
I 00:25:03.248061 file system initialized [60.95ms]"><pre><code>$ dwarfs perl-install.dwarfs mnt -f
I 00:25:03.186005 dwarfs (v0.4.0, fuse version 35)
I 00:25:03.248061 file system initialized [60.95ms]
</code></pre></div>
<p dir="auto">If you actually build the DwarFS file system with uncompressed metadata,
mounting is basically instantaneous:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs perl-install-meta.dwarfs mnt -f
I 00:27:52.667026 dwarfs (v0.4.0, fuse version 35)
I 00:27:52.671066 file system initialized [2.879ms]"><pre><code>$ dwarfs perl-install-meta.dwarfs mnt -f
I 00:27:52.667026 dwarfs (v0.4.0, fuse version 35)
I 00:27:52.671066 file system initialized [2.879ms]
</code></pre></div>
<p dir="auto">I've tried running the benchmark where all 1139 <code>perl</code> executables
print their version with the wimlib image, but after about 10 minutes,
it still hadn't finished the first run (with the DwarFS image, one run
took slightly more than 2 seconds). I then tried the following instead:</p>
<div data-snippet-clipboard-copy-content="$ ls -1 /tmp/perl/install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P1 sh -c 'time $0 -v >/dev/null' 2>&amp;1 | grep ^real
real    0m0.802s
real    0m0.652s
real    0m1.677s
real    0m1.973s
real    0m1.435s
real    0m1.879s
real    0m2.003s
real    0m1.695s
real    0m2.343s
real    0m1.899s
real    0m1.809s
real    0m1.790s
real    0m2.115s"><pre><code>$ ls -1 /tmp/perl/install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P1 sh -c 'time $0 -v &gt;/dev/null' 2&gt;&amp;1 | grep ^real
real    0m0.802s
real    0m0.652s
real    0m1.677s
real    0m1.973s
real    0m1.435s
real    0m1.879s
real    0m2.003s
real    0m1.695s
real    0m2.343s
real    0m1.899s
real    0m1.809s
real    0m1.790s
real    0m2.115s
</code></pre></div>
<p dir="auto">Judging from that, it would have probably taken about half an hour
for a single run, which makes at least the <code>--solid</code> wim image pretty
much unusable for actually working with the file system.</p>
<p dir="auto">The <code>--solid</code> option was suggested to me because it resembles the way
that DwarFS actually organizes data internally. However, judging by the
warning when mounting a solid image, it's probably not ideal when using
the image as a mounted file system. So I tried again without <code>--solid</code>:</p>
<div data-snippet-clipboard-copy-content="$ time wimcapture --unix-data install perl-install-nonsolid.wim
Scanning &quot;install&quot;
47 GiB scanned (1927501 files, 330733 directories)
Using LZX compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    8m39.034s
user    64m58.575s
sys     0m32.003s"><pre><code>$ time wimcapture --unix-data install perl-install-nonsolid.wim
Scanning "install"
47 GiB scanned (1927501 files, 330733 directories)
Using LZX compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    8m39.034s
user    64m58.575s
sys     0m32.003s
</code></pre></div>
<p dir="auto">This is still more than 3 minutes slower than <code>mkdwarfs</code>. However, it
yields an image that's almost 10 times the size of the DwarFS image
and comparable in size to the SquashFS image:</p>
<div data-snippet-clipboard-copy-content="$ ll perl-install-nonsolid.wim -h
-rw-r--r-- 1 mhx users 4.6G Mar  6 23:24 perl-install-nonsolid.wim"><pre><code>$ ll perl-install-nonsolid.wim -h
-rw-r--r-- 1 mhx users 4.6G Mar  6 23:24 perl-install-nonsolid.wim
</code></pre></div>
<p dir="auto">This <em>still</em> takes surprisingly long to mount:</p>
<div data-snippet-clipboard-copy-content="$ time wimmount perl-install-nonsolid.wim mnt

real    0m1.603s
user    0m1.327s
sys     0m0.275s"><pre><code>$ time wimmount perl-install-nonsolid.wim mnt

real    0m1.603s
user    0m1.327s
sys     0m0.275s
</code></pre></div>
<p dir="auto">However, it's really usable as a file system, even though it's about
4-5 times slower than the DwarFS image:</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c 'umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot; -p 'umount mnt; wimmount perl-install-nonsolid.wim mnt; sleep 1' -n wimlib &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: dwarfs
  Time (mean ± σ):      1.149 s ±  0.019 s    [User: 2.147 s, System: 0.739 s]
  Range (min … max):    1.122 s …  1.187 s    10 runs

Benchmark #2: wimlib
  Time (mean ± σ):      7.542 s ±  0.069 s    [User: 2.787 s, System: 0.694 s]
  Range (min … max):    7.490 s …  7.732 s    10 runs

Summary
  'dwarfs' ran
    6.56 ± 0.12 times faster than 'wimlib'"><pre><code>$ hyperfine -c 'umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'" -p 'umount mnt; wimmount perl-install-nonsolid.wim mnt; sleep 1' -n wimlib "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: dwarfs
  Time (mean ± σ):      1.149 s ±  0.019 s    [User: 2.147 s, System: 0.739 s]
  Range (min … max):    1.122 s …  1.187 s    10 runs

Benchmark #2: wimlib
  Time (mean ± σ):      7.542 s ±  0.069 s    [User: 2.787 s, System: 0.694 s]
  Range (min … max):    7.490 s …  7.732 s    10 runs

Summary
  'dwarfs' ran
    6.56 ± 0.12 times faster than 'wimlib'
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">With Cromfs</h3><a id="user-content-with-cromfs" aria-label="Permalink: With Cromfs" href="#with-cromfs"></a></p>
<p dir="auto">I used <a href="https://bisqwit.iki.fi/source/cromfs.html" rel="nofollow">Cromfs</a> in the past
for compressed file systems and remember that it did a pretty good job
in terms of compression ratio. But it was never fast. However, I didn't
quite remember just <em>how</em> slow it was until I tried to set up a test.</p>
<p dir="auto">Here's a run on the Perl dataset, with the block size set to 16 MiB to
match the default of DwarFS, and with additional options suggested to
speed up compression:</p>
<div data-snippet-clipboard-copy-content="$ time mkcromfs -f 16777216 -qq -e -r100000 install perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --24bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f3a18259000..0x7f3a1bfffb9c
Root inode spans 0x7f3a205d2948..0x7f3a205d294c
Beginning task for Files and directories: Finding identical blocks
2163608 reuse opportunities found. 561362 unique blocks. Block table will be 79.4% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Blockifying:  0.04% (140017/2724970) idx(siz=80423,del=0) rawin(20.97 MB)rawout(20.97 MB)diff(1956 bytes)
Termination signalled, cleaning up temporaries

real    29m9.634s
user    201m37.816s
sys     2m15.005s"><pre><code>$ time mkcromfs -f 16777216 -qq -e -r100000 install perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --24bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f3a18259000..0x7f3a1bfffb9c
Root inode spans 0x7f3a205d2948..0x7f3a205d294c
Beginning task for Files and directories: Finding identical blocks
2163608 reuse opportunities found. 561362 unique blocks. Block table will be 79.4% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Blockifying:  0.04% (140017/2724970) idx(siz=80423,del=0) rawin(20.97 MB)rawout(20.97 MB)diff(1956 bytes)
Termination signalled, cleaning up temporaries

real    29m9.634s
user    201m37.816s
sys     2m15.005s
</code></pre></div>
<p dir="auto">So, it processed 21 MiB out of 48 GiB in half an hour, using almost
twice as much CPU resources as DwarFS for the <em>whole</em> file system.
At this point I decided it's likely not worth waiting (presumably)
another month (!) for <code>mkcromfs</code> to finish. I double checked that
I didn't accidentally build a debugging version, <code>mkcromfs</code> was
definitely built with <code>-O3</code>.</p>
<p dir="auto">I then tried once more with a smaller version of the Perl dataset.
This only has 20 versions (instead of 1139) of Perl, and obviously
a lot less redundancy:</p>
<div data-snippet-clipboard-copy-content="$ time mkcromfs -f 16777216 -qq -e -r100000 install-small perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --16bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f00e0774000..0x7f00e08410a8
Root inode spans 0x7f00b40048f8..0x7f00b40048fc
Beginning task for Files and directories: Finding identical blocks
25362 reuse opportunities found. 9815 unique blocks. Block table will be 72.1% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Compressing raw rootdir inode (28 bytes)z=982370,del=2) rawin(641.56 MB)rawout(252.72 MB)diff(388.84 MB)
 compressed into 35 bytes
INOTAB pseudo file is 839.85 kB
Inotab inode spans 0x7f00bc036ed8..0x7f00bc036ef4
Beginning task for INOTAB: Finding identical blocks
0 reuse opportunities found. 13 unique blocks. Block table will be 0.0% smaller than without the index search.
Beginning task for INOTAB: Blockifying
mkcromfs: Automatically enabling --packedblocks because it is possible for this filesystem.
Compressing raw inotab inode (52 bytes)
 compressed into 58 bytes
Compressing 9828 block records (4 bytes each, total 39312 bytes)
 compressed into 15890 bytes
Compressing and writing 16 fblocks...

16 fblocks were written: 35.31 MB = 13.90 % of 254.01 MB
Filesystem size: 35.33 MB = 5.50 % of original 642.22 MB
End

real    27m38.833s
user    277m36.208s
sys     11m36.945s"><pre><code>$ time mkcromfs -f 16777216 -qq -e -r100000 install-small perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --16bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f00e0774000..0x7f00e08410a8
Root inode spans 0x7f00b40048f8..0x7f00b40048fc
Beginning task for Files and directories: Finding identical blocks
25362 reuse opportunities found. 9815 unique blocks. Block table will be 72.1% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Compressing raw rootdir inode (28 bytes)z=982370,del=2) rawin(641.56 MB)rawout(252.72 MB)diff(388.84 MB)
 compressed into 35 bytes
INOTAB pseudo file is 839.85 kB
Inotab inode spans 0x7f00bc036ed8..0x7f00bc036ef4
Beginning task for INOTAB: Finding identical blocks
0 reuse opportunities found. 13 unique blocks. Block table will be 0.0% smaller than without the index search.
Beginning task for INOTAB: Blockifying
mkcromfs: Automatically enabling --packedblocks because it is possible for this filesystem.
Compressing raw inotab inode (52 bytes)
 compressed into 58 bytes
Compressing 9828 block records (4 bytes each, total 39312 bytes)
 compressed into 15890 bytes
Compressing and writing 16 fblocks...

16 fblocks were written: 35.31 MB = 13.90 % of 254.01 MB
Filesystem size: 35.33 MB = 5.50 % of original 642.22 MB
End

real    27m38.833s
user    277m36.208s
sys     11m36.945s
</code></pre></div>
<p dir="auto">And repeating the same task with <code>mkdwarfs</code>:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install-small -o perl-install-small.dwarfs
21:13:38.131724 scanning install-small
21:13:38.320139 waiting for background scanners...
21:13:38.727024 assigning directory and link inodes...
21:13:38.731807 finding duplicate files...
21:13:38.832524 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:13:38.832598 waiting for inode scanners...
21:13:39.619963 assigning device inodes...
21:13:39.620855 assigning pipe/socket inodes...
21:13:39.621356 building metadata...
21:13:39.621453 building blocks...
21:13:39.621472 saving names and links...
21:13:39.621655 ordering 3559 inodes using nilsimsa similarity...
21:13:39.622031 nilsimsa: depth=20000, limit=255
21:13:39.629206 updating name and link indices...
21:13:39.630142 pre-sorted index (3360 name, 2127 path lookups) [8.014ms]
21:13:39.752051 3559 inodes ordered [130.3ms]
21:13:39.752101 waiting for segmenting/blockifying to finish...
21:13:53.250951 saving chunks...
21:13:53.251581 saving directories...
21:13:53.303862 waiting for compression to finish...
21:14:11.073273 compressed 611.8 MiB to 24.01 MiB (ratio=0.0392411)
21:14:11.091099 filesystem created without errors [32.96s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 121.5 MiB
filesystem: 222.5 MiB in 14 blocks (7177 chunks, 3559/3559 inodes)
compressed filesystem: 14 blocks/24.01 MiB written
██████████████████████████████████████████████████████████████████████▏100% \

real    0m33.007s
user    3m43.324s
sys     0m4.015s"><pre><code>$ time mkdwarfs -i install-small -o perl-install-small.dwarfs
21:13:38.131724 scanning install-small
21:13:38.320139 waiting for background scanners...
21:13:38.727024 assigning directory and link inodes...
21:13:38.731807 finding duplicate files...
21:13:38.832524 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:13:38.832598 waiting for inode scanners...
21:13:39.619963 assigning device inodes...
21:13:39.620855 assigning pipe/socket inodes...
21:13:39.621356 building metadata...
21:13:39.621453 building blocks...
21:13:39.621472 saving names and links...
21:13:39.621655 ordering 3559 inodes using nilsimsa similarity...
21:13:39.622031 nilsimsa: depth=20000, limit=255
21:13:39.629206 updating name and link indices...
21:13:39.630142 pre-sorted index (3360 name, 2127 path lookups) [8.014ms]
21:13:39.752051 3559 inodes ordered [130.3ms]
21:13:39.752101 waiting for segmenting/blockifying to finish...
21:13:53.250951 saving chunks...
21:13:53.251581 saving directories...
21:13:53.303862 waiting for compression to finish...
21:14:11.073273 compressed 611.8 MiB to 24.01 MiB (ratio=0.0392411)
21:14:11.091099 filesystem created without errors [32.96s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 121.5 MiB
filesystem: 222.5 MiB in 14 blocks (7177 chunks, 3559/3559 inodes)
compressed filesystem: 14 blocks/24.01 MiB written
██████████████████████████████████████████████████████████████████████▏100% \

real    0m33.007s
user    3m43.324s
sys     0m4.015s
</code></pre></div>
<p dir="auto">So, <code>mkdwarfs</code> is about 50 times faster than <code>mkcromfs</code> and uses 75 times
less CPU resources. At the same time, the DwarFS file system is 30% smaller:</p>
<div data-snippet-clipboard-copy-content="$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs"><pre><code>$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs
</code></pre></div>
<p dir="auto">I noticed that the <code>blockifying</code> step that took ages for the full dataset
with <code>mkcromfs</code> ran substantially faster (in terms of MiB/second) on the
smaller dataset, which makes me wonder if there's some quadratic complexity
behaviour that's slowing down <code>mkcromfs</code>.</p>
<p dir="auto">In order to be completely fair, I also ran <code>mkdwarfs</code> with <code>-l 9</code> to enable
LZMA compression (which is what <code>mkcromfs</code> uses by default):</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install-small -o perl-install-small-l9.dwarfs -l 9
21:16:21.874975 scanning install-small
21:16:22.092201 waiting for background scanners...
21:16:22.489470 assigning directory and link inodes...
21:16:22.495216 finding duplicate files...
21:16:22.611221 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:16:22.611314 waiting for inode scanners...
21:16:23.394332 assigning device inodes...
21:16:23.395184 assigning pipe/socket inodes...
21:16:23.395616 building metadata...
21:16:23.395676 building blocks...
21:16:23.395685 saving names and links...
21:16:23.395830 ordering 3559 inodes using nilsimsa similarity...
21:16:23.396097 nilsimsa: depth=50000, limit=255
21:16:23.401042 updating name and link indices...
21:16:23.403127 pre-sorted index (3360 name, 2127 path lookups) [6.936ms]
21:16:23.524914 3559 inodes ordered [129ms]
21:16:23.525006 waiting for segmenting/blockifying to finish...
21:16:33.865023 saving chunks...
21:16:33.865883 saving directories...
21:16:33.900140 waiting for compression to finish...
21:17:10.505779 compressed 611.8 MiB to 17.44 MiB (ratio=0.0284969)
21:17:10.526171 filesystem created without errors [48.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 122.2 MiB
filesystem: 221.8 MiB in 4 blocks (7304 chunks, 3559/3559 inodes)
compressed filesystem: 4 blocks/17.44 MiB written
██████████████████████████████████████████████████████████████████████▏100% /

real    0m48.683s
user    2m24.905s
sys     0m3.292s"><pre><code>$ time mkdwarfs -i install-small -o perl-install-small-l9.dwarfs -l 9
21:16:21.874975 scanning install-small
21:16:22.092201 waiting for background scanners...
21:16:22.489470 assigning directory and link inodes...
21:16:22.495216 finding duplicate files...
21:16:22.611221 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:16:22.611314 waiting for inode scanners...
21:16:23.394332 assigning device inodes...
21:16:23.395184 assigning pipe/socket inodes...
21:16:23.395616 building metadata...
21:16:23.395676 building blocks...
21:16:23.395685 saving names and links...
21:16:23.395830 ordering 3559 inodes using nilsimsa similarity...
21:16:23.396097 nilsimsa: depth=50000, limit=255
21:16:23.401042 updating name and link indices...
21:16:23.403127 pre-sorted index (3360 name, 2127 path lookups) [6.936ms]
21:16:23.524914 3559 inodes ordered [129ms]
21:16:23.525006 waiting for segmenting/blockifying to finish...
21:16:33.865023 saving chunks...
21:16:33.865883 saving directories...
21:16:33.900140 waiting for compression to finish...
21:17:10.505779 compressed 611.8 MiB to 17.44 MiB (ratio=0.0284969)
21:17:10.526171 filesystem created without errors [48.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 122.2 MiB
filesystem: 221.8 MiB in 4 blocks (7304 chunks, 3559/3559 inodes)
compressed filesystem: 4 blocks/17.44 MiB written
██████████████████████████████████████████████████████████████████████▏100% /

real    0m48.683s
user    2m24.905s
sys     0m3.292s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ls -l perl-install-small*.*fs
-rw-r--r-- 1 mhx users 18282075 Dec 10 21:17 perl-install-small-l9.dwarfs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs"><pre><code>$ ls -l perl-install-small*.*fs
-rw-r--r-- 1 mhx users 18282075 Dec 10 21:17 perl-install-small-l9.dwarfs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs
</code></pre></div>
<p dir="auto">It takes about 15 seconds longer to build the DwarFS file system with LZMA
compression (this is still 35 times faster than Cromfs), but reduces the
size even further to make it almost half the size of the Cromfs file system.</p>
<p dir="auto">I would have added some benchmarks with the Cromfs FUSE driver, but sadly
it crashed right upon trying to list the directory after mounting.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With EROFS</h3><a id="user-content-with-erofs" aria-label="Permalink: With EROFS" href="#with-erofs"></a></p>
<p dir="auto"><a href="https://github.com/hsiangkao/erofs-utils">EROFS</a> is a new read-only
compressed file system that has recently been added to the Linux kernel.
Its goals are quite different from those of DwarFS, though. It is
designed to be lightweight (which DwarFS is definitely not) and to run
on constrained hardware like embedded devices or smartphones. It only
supports LZ4 compression.</p>
<p dir="auto">I was feeling lucky and decided to run it on the full Perl dataset:</p>
<div data-snippet-clipboard-copy-content="$ time mkfs.erofs perl-install.erofs install -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]
^C

real    912m42.601s
user    903m2.777s
sys     1m52.812s"><pre><code>$ time mkfs.erofs perl-install.erofs install -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]
^C

real    912m42.601s
user    903m2.777s
sys     1m52.812s
</code></pre></div>
<p dir="auto">As you can tell, after more than 15 hours I just gave up. In those
15 hours, <code>mkfs.erofs</code> had produced a 13 GiB output file:</p>
<div data-snippet-clipboard-copy-content="$ ll -h perl-install.erofs
-rw-r--r-- 1 mhx users 13G Dec  9 14:42 perl-install.erofs"><pre><code>$ ll -h perl-install.erofs
-rw-r--r-- 1 mhx users 13G Dec  9 14:42 perl-install.erofs
</code></pre></div>
<p dir="auto">I don't think this would have been very useful to compare with DwarFS.</p>
<p dir="auto">Just as for Cromfs, I re-ran with the smaller Perl dataset:</p>
<div data-snippet-clipboard-copy-content="$ time mkfs.erofs perl-install-small.erofs install-small -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]

real    0m27.844s
user    0m20.570s
sys     0m1.848s"><pre><code>$ time mkfs.erofs perl-install-small.erofs install-small -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]

real    0m27.844s
user    0m20.570s
sys     0m1.848s
</code></pre></div>
<p dir="auto">That was surprisingly quick, which makes me think that, again, there
might be some accidentally quadratic complexity hiding in <code>mkfs.erofs</code>.
The output file it produced is an order of magnitude larger than the
DwarFS image:</p>
<div data-snippet-clipboard-copy-content="$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users  26928161 Dec  8 15:05 perl-install-small.dwarfs
-rw-r--r-- 1 mhx users 296488960 Dec  9 14:45 perl-install-small.erofs"><pre><code>$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users  26928161 Dec  8 15:05 perl-install-small.dwarfs
-rw-r--r-- 1 mhx users 296488960 Dec  9 14:45 perl-install-small.erofs
</code></pre></div>
<p dir="auto">Admittedly, this isn't a fair comparison. EROFS has a fixed block size
of 4 KiB, and it uses LZ4 compression. If we tweak DwarFS to the same
parameters, we get:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install-small -o perl-install-small-lz4.dwarfs -C lz4hc:level=9 -S 12
21:21:18.136796 scanning install-small
21:21:18.376998 waiting for background scanners...
21:21:18.770703 assigning directory and link inodes...
21:21:18.776422 finding duplicate files...
21:21:18.903505 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:21:18.903621 waiting for inode scanners...
21:21:19.676420 assigning device inodes...
21:21:19.677400 assigning pipe/socket inodes...
21:21:19.678014 building metadata...
21:21:19.678101 building blocks...
21:21:19.678116 saving names and links...
21:21:19.678306 ordering 3559 inodes using nilsimsa similarity...
21:21:19.678566 nilsimsa: depth=20000, limit=255
21:21:19.684227 pre-sorted index (3360 name, 2127 path lookups) [5.592ms]
21:21:19.685550 updating name and link indices...
21:21:19.810401 3559 inodes ordered [132ms]
21:21:19.810519 waiting for segmenting/blockifying to finish...
21:21:26.773913 saving chunks...
21:21:26.776832 saving directories...
21:21:26.821085 waiting for compression to finish...
21:21:27.020929 compressed 611.8 MiB to 140.7 MiB (ratio=0.230025)
21:21:27.036202 filesystem created without errors [8.899s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 0 B
filesystem: 344 MiB in 88073 blocks (91628 chunks, 3559/3559 inodes)
compressed filesystem: 88073 blocks/140.7 MiB written
████████████████████████████████████████████████████████████████▏100% |

real    0m9.075s
user    0m37.718s
sys     0m2.427s"><pre><code>$ time mkdwarfs -i install-small -o perl-install-small-lz4.dwarfs -C lz4hc:level=9 -S 12
21:21:18.136796 scanning install-small
21:21:18.376998 waiting for background scanners...
21:21:18.770703 assigning directory and link inodes...
21:21:18.776422 finding duplicate files...
21:21:18.903505 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:21:18.903621 waiting for inode scanners...
21:21:19.676420 assigning device inodes...
21:21:19.677400 assigning pipe/socket inodes...
21:21:19.678014 building metadata...
21:21:19.678101 building blocks...
21:21:19.678116 saving names and links...
21:21:19.678306 ordering 3559 inodes using nilsimsa similarity...
21:21:19.678566 nilsimsa: depth=20000, limit=255
21:21:19.684227 pre-sorted index (3360 name, 2127 path lookups) [5.592ms]
21:21:19.685550 updating name and link indices...
21:21:19.810401 3559 inodes ordered [132ms]
21:21:19.810519 waiting for segmenting/blockifying to finish...
21:21:26.773913 saving chunks...
21:21:26.776832 saving directories...
21:21:26.821085 waiting for compression to finish...
21:21:27.020929 compressed 611.8 MiB to 140.7 MiB (ratio=0.230025)
21:21:27.036202 filesystem created without errors [8.899s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 0 B
filesystem: 344 MiB in 88073 blocks (91628 chunks, 3559/3559 inodes)
compressed filesystem: 88073 blocks/140.7 MiB written
████████████████████████████████████████████████████████████████▏100% |

real    0m9.075s
user    0m37.718s
sys     0m2.427s
</code></pre></div>
<p dir="auto">It finishes in less than half the time and produces an output image
that's half the size of the EROFS image.</p>
<p dir="auto">I'm going to stop the comparison here, as it's pretty obvious that the
domains in which EROFS and DwarFS are being used have extremely little
overlap. DwarFS will likely never be able to run on embedded devices
and EROFS will likely never be able to achieve the compression ratios
of DwarFS.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With fuse-archive</h3><a id="user-content-with-fuse-archive" aria-label="Permalink: With fuse-archive" href="#with-fuse-archive"></a></p>
<p dir="auto">I came across <a href="https://github.com/google/fuse-archive">fuse-archive</a>
while looking for FUSE drivers to mount archives and it seems to be
the most versatile of the alternatives (and the one that actually
compiles out of the box).</p>
<p dir="auto">An interesting test case straight from fuse-archive's README is in
the <a href="https://github.com/google/fuse-archive#performance">Performance</a>
section: an archive with a single huge file full of zeroes. Let's
make the example a bit more extreme and use a 1 GiB file instead of
just 256 MiB:</p>
<div data-snippet-clipboard-copy-content="$ mkdir zerotest
$ truncate --size=1G zerotest/zeroes"><pre><code>$ mkdir zerotest
$ truncate --size=1G zerotest/zeroes
</code></pre></div>
<p dir="auto">Now, we build several different archives and a DwarFS image:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i zerotest -o zerotest.dwarfs -W16 --log-level=warn --progress=none

real    0m7.604s
user    0m7.521s
sys     0m0.083s

$ time zip -9 zerotest.zip zerotest/zeroes
  adding: zerotest/zeroes (deflated 100%)

real    0m4.923s
user    0m4.840s
sys     0m0.080s

$ time 7z a -bb0 -bd zerotest.7z zerotest/zeroes

7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,16 CPUs Intel(R) Xeon(R) E-2286M  CPU @ 2.40GHz (906ED),ASM,AES-NI)

Scanning the drive:
1 file, 1073741824 bytes (1024 MiB)

Creating archive: zerotest.7z

Items to compress: 1

Files read from disk: 1
Archive size: 157819 bytes (155 KiB)
Everything is Ok

real    0m5.535s
user    0m48.281s
sys     0m1.116s

$ time tar --zstd -cf zerotest.tar.zstd zerotest/zeroes

real    0m0.449s
user    0m0.510s
sys     0m0.610s"><pre><code>$ time mkdwarfs -i zerotest -o zerotest.dwarfs -W16 --log-level=warn --progress=none

real    0m7.604s
user    0m7.521s
sys     0m0.083s

$ time zip -9 zerotest.zip zerotest/zeroes
  adding: zerotest/zeroes (deflated 100%)

real    0m4.923s
user    0m4.840s
sys     0m0.080s

$ time 7z a -bb0 -bd zerotest.7z zerotest/zeroes

7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,16 CPUs Intel(R) Xeon(R) E-2286M  CPU @ 2.40GHz (906ED),ASM,AES-NI)

Scanning the drive:
1 file, 1073741824 bytes (1024 MiB)

Creating archive: zerotest.7z

Items to compress: 1

Files read from disk: 1
Archive size: 157819 bytes (155 KiB)
Everything is Ok

real    0m5.535s
user    0m48.281s
sys     0m1.116s

$ time tar --zstd -cf zerotest.tar.zstd zerotest/zeroes

real    0m0.449s
user    0m0.510s
sys     0m0.610s
</code></pre></div>
<p dir="auto">Turns out that <code>tar --zstd</code> is easily winning the compression speed
test. Looking at the file sizes did actually blow my mind just a bit:</p>
<div data-snippet-clipboard-copy-content="$ ll zerotest.* --sort=size
-rw-r--r-- 1 mhx users 1042231 Jul  1 15:24 zerotest.zip
-rw-r--r-- 1 mhx users  157819 Jul  1 15:26 zerotest.7z
-rw-r--r-- 1 mhx users   33762 Jul  1 15:28 zerotest.tar.zstd
-rw-r--r-- 1 mhx users     848 Jul  1 15:23 zerotest.dwarfs"><pre><code>$ ll zerotest.* --sort=size
-rw-r--r-- 1 mhx users 1042231 Jul  1 15:24 zerotest.zip
-rw-r--r-- 1 mhx users  157819 Jul  1 15:26 zerotest.7z
-rw-r--r-- 1 mhx users   33762 Jul  1 15:28 zerotest.tar.zstd
-rw-r--r-- 1 mhx users     848 Jul  1 15:23 zerotest.dwarfs
</code></pre></div>
<p dir="auto">I definitely didn't expect the DwarFS image to be <em>that</em> small.
Dropping the section index would actually save another 100 bytes.
So, if you want to archive lots of zeroes, DwarFS is your friend.</p>
<p dir="auto">Anyway, let's look at how fast and efficiently the zeroes can
be read from the different archives. First, the <code>zip</code> archive:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
1020117504 bytes (1.0 GB, 973 MiB) copied, 2 s, 510 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.10309 s, 511 MB/s

real    0m2.104s
user    0m0.264s
sys     0m0.486s"><pre><code>$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
1020117504 bytes (1.0 GB, 973 MiB) copied, 2 s, 510 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.10309 s, 511 MB/s

real    0m2.104s
user    0m0.264s
sys     0m0.486s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 1.8 seconds and mount time
was in the milliseconds.</p>
<p dir="auto">Now, the <code>7z</code> archive:</p>
<div data-snippet-clipboard-copy-content=" $ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
594759168 bytes (595 MB, 567 MiB) copied, 1 s, 595 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.76904 s, 607 MB/s

real    0m1.772s
user    0m0.229s
sys     0m0.572s"><pre><code> $ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
594759168 bytes (595 MB, 567 MiB) copied, 1 s, 595 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.76904 s, 607 MB/s

real    0m1.772s
user    0m0.229s
sys     0m0.572s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 2.9 seconds and mount time
was just over 1.0 seconds.</p>
<p dir="auto">Now, the <code>.tar.zstd</code> archive:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.799409 s, 1.3 GB/s

real    0m0.801s
user    0m0.262s
sys     0m0.537s"><pre><code>$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.799409 s, 1.3 GB/s

real    0m0.801s
user    0m0.262s
sys     0m0.537s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 0.53 seconds and mount time
was 0.13 seconds.</p>
<p dir="auto">Last but not least, let's look at DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.753 s, 1.4 GB/s

real    0m0.757s
user    0m0.220s
sys     0m0.534s"><pre><code>$ time dd if=mnt/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.753 s, 1.4 GB/s

real    0m0.757s
user    0m0.220s
sys     0m0.534s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 0.17 seconds and mount time
was less than a millisecond.</p>
<p dir="auto">If we increase the block size for the <code>dd</code> command, we can get
even higher throughput. For fuse-archive with the <code>.tar.zstd</code>:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.318682 s, 3.4 GB/s

real    0m0.323s
user    0m0.005s
sys     0m0.154s"><pre><code>$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.318682 s, 3.4 GB/s

real    0m0.323s
user    0m0.005s
sys     0m0.154s
</code></pre></div>
<p dir="auto">And for DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.172226 s, 6.2 GB/s

real    0m0.176s
user    0m0.020s
sys     0m0.141s"><pre><code>$ time dd if=mnt/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.172226 s, 6.2 GB/s

real    0m0.176s
user    0m0.020s
sys     0m0.141s
</code></pre></div>
<p dir="auto">This is all nice, but what about a more real-life use case?
Let's take the 1.82.0 boost release archives:</p>
<div data-snippet-clipboard-copy-content="$ ll --sort=size boost_1_82_0.*
-rw-r--r-- 1 mhx users 208188085 Apr 10 14:25 boost_1_82_0.zip
-rw-r--r-- 1 mhx users 142580547 Apr 10 14:23 boost_1_82_0.tar.gz
-rw-r--r-- 1 mhx users 121325129 Apr 10 14:23 boost_1_82_0.tar.bz2
-rw-r--r-- 1 mhx users 105901369 Jun 28 12:47 boost_1_82_0.dwarfs
-rw-r--r-- 1 mhx users 103710551 Apr 10 14:25 boost_1_82_0.7z"><pre><code>$ ll --sort=size boost_1_82_0.*
-rw-r--r-- 1 mhx users 208188085 Apr 10 14:25 boost_1_82_0.zip
-rw-r--r-- 1 mhx users 142580547 Apr 10 14:23 boost_1_82_0.tar.gz
-rw-r--r-- 1 mhx users 121325129 Apr 10 14:23 boost_1_82_0.tar.bz2
-rw-r--r-- 1 mhx users 105901369 Jun 28 12:47 boost_1_82_0.dwarfs
-rw-r--r-- 1 mhx users 103710551 Apr 10 14:25 boost_1_82_0.7z
</code></pre></div>
<p dir="auto">Here are the timings for mounting each archive and then using
<code>tar</code> to build another archive from the mountpoint and just counting
the number of bytes in that archive, e.g.:</p>
<div data-snippet-clipboard-copy-content="$ time tar cf - mnt | wc -c
803614720

real    0m4.602s
user    0m0.156s
sys     0m1.123s"><pre><code>$ time tar cf - mnt | wc -c
803614720

real    0m4.602s
user    0m0.156s
sys     0m1.123s
</code></pre></div>
<p dir="auto">Here are the results in terms of wallclock time and FUSE driver
CPU time:</p>
<table>
<thead>
<tr>
<th>Archive</th>
<th>Mount Time</th>
<th><code>tar</code> Wallclock Time</th>
<th>FUSE Driver CPU Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>.zip</code></td>
<td>0.458s</td>
<td>5.073s</td>
<td>4.418s</td>
</tr>
<tr>
<td><code>.tar.gz</code></td>
<td>1.391s</td>
<td>3.483s</td>
<td>3.943s</td>
</tr>
<tr>
<td><code>.tar.bz2</code></td>
<td>15.663s</td>
<td>17.942s</td>
<td>32.040s</td>
</tr>
<tr>
<td><code>.7z</code></td>
<td>0.321s</td>
<td>32.554s</td>
<td>31.625s</td>
</tr>
<tr>
<td><code>.dwarfs</code></td>
<td>0.013s</td>
<td>2.974s</td>
<td>1.984s</td>
</tr>
</tbody>
</table>
<p dir="auto">DwarFS easily wins all categories while still compressing the data
almost as well as <code>7z</code>.</p>
<p dir="auto">What about accessing files more randomly?</p>
<div data-snippet-clipboard-copy-content="$ find mnt -type f -print0 | xargs -0 -P32 -n32 cat | dd of=/dev/null status=progress"><pre><code>$ find mnt -type f -print0 | xargs -0 -P32 -n32 cat | dd of=/dev/null status=progress
</code></pre></div>
<p dir="auto">It turns out that fuse-archive grinds to a halt in this case, so I had
to run the test on a subset (the <code>boost</code> subdirectory) of the data.
The <code>.tar.bz2</code> and <code>.7z</code> archives were so slow to read that I stopped
them after a few minutes.</p>
<table>
<thead>
<tr>
<th>Archive</th>
<th>Throughput</th>
<th>Wallclock Time</th>
<th>FUSE Driver CPU Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>.zip</code></td>
<td>1.8 MB/s</td>
<td>83.245s</td>
<td>83.669s</td>
</tr>
<tr>
<td><code>.tar.gz</code></td>
<td>1.2 MB/s</td>
<td>121.377s</td>
<td>122.711s</td>
</tr>
<tr>
<td><code>.tar.bz2</code></td>
<td>0.2 MB/s</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>.7z</code></td>
<td>0.3 MB/s</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>.dwarfs</code></td>
<td>598.0 MB/s</td>
<td>0.249s</td>
<td>1.099s</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance Monitoring</h2><a id="user-content-performance-monitoring" aria-label="Permalink: Performance Monitoring" href="#performance-monitoring"></a></p>
<p dir="auto">Both the FUSE driver and <code>dwarfsextract</code> by default have support for
simple performance monitoring. You can build binaries without this
feature (<code>-DENABLE_PERFMON=OFF</code>), but impact should be negligible even
if performance monitoring is enabled at run-time.</p>
<p dir="auto">To enable the performance monitor, you pass a list of components for which
you want to collect latency metrics, e.g.:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs test.dwarfs mnt -f -operfmon=fuse"><pre><code>$ dwarfs test.dwarfs mnt -f -operfmon=fuse
</code></pre></div>
<p dir="auto">When the driver exits, you will see output like this:</p>
<div data-snippet-clipboard-copy-content="[fuse.op_read]
      samples: 45145
      overall: 3.214s
  avg latency: 71.2us
  p50 latency: 131.1us
  p90 latency: 131.1us
  p99 latency: 262.1us

[fuse.op_readdir]
      samples: 2
      overall: 51.31ms
  avg latency: 25.65ms
  p50 latency: 32.77us
  p90 latency: 67.11ms
  p99 latency: 67.11ms

[fuse.op_lookup]
      samples: 16
      overall: 19.98ms
  avg latency: 1.249ms
  p50 latency: 2.097ms
  p90 latency: 4.194ms
  p99 latency: 4.194ms

[fuse.op_init]
      samples: 1
      overall: 199.4us
  avg latency: 199.4us
  p50 latency: 262.1us
  p90 latency: 262.1us
  p99 latency: 262.1us

[fuse.op_open]
      samples: 16
      overall: 122.2us
  avg latency: 7.641us
  p50 latency: 4.096us
  p90 latency: 32.77us
  p99 latency: 32.77us

[fuse.op_getattr]
      samples: 1
      overall: 5.786us
  avg latency: 5.786us
  p50 latency: 8.192us
  p90 latency: 8.192us
  p99 latency: 8.192us"><pre><code>[fuse.op_read]
      samples: 45145
      overall: 3.214s
  avg latency: 71.2us
  p50 latency: 131.1us
  p90 latency: 131.1us
  p99 latency: 262.1us

[fuse.op_readdir]
      samples: 2
      overall: 51.31ms
  avg latency: 25.65ms
  p50 latency: 32.77us
  p90 latency: 67.11ms
  p99 latency: 67.11ms

[fuse.op_lookup]
      samples: 16
      overall: 19.98ms
  avg latency: 1.249ms
  p50 latency: 2.097ms
  p90 latency: 4.194ms
  p99 latency: 4.194ms

[fuse.op_init]
      samples: 1
      overall: 199.4us
  avg latency: 199.4us
  p50 latency: 262.1us
  p90 latency: 262.1us
  p99 latency: 262.1us

[fuse.op_open]
      samples: 16
      overall: 122.2us
  avg latency: 7.641us
  p50 latency: 4.096us
  p90 latency: 32.77us
  p99 latency: 32.77us

[fuse.op_getattr]
      samples: 1
      overall: 5.786us
  avg latency: 5.786us
  p50 latency: 8.192us
  p90 latency: 8.192us
  p99 latency: 8.192us
</code></pre></div>
<p dir="auto">The metrics should be self-explanatory. However, note that the
percentile metrics are logarithmically quantized in order to use
as little resources as possible. As a result, you will only see
values that look an awful lot like powers of two.</p>
<p dir="auto">Currently, the supported components are <code>fuse</code> for the FUSE
operations, <code>filesystem_v2</code> for the DwarFS file system component
and <code>inode_reader_v2</code> for the component that handles all <code>read()</code>
system calls.</p>
<p dir="auto">The FUSE driver also exposes the performance monitor metrics via
an <a href="#extended-attributes">extended attribute</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other Obscure Features</h2><a id="user-content-other-obscure-features" aria-label="Permalink: Other Obscure Features" href="#other-obscure-features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setting Worker Thread CPU Affinity</h3><a id="user-content-setting-worker-thread-cpu-affinity" aria-label="Permalink: Setting Worker Thread CPU Affinity" href="#setting-worker-thread-cpu-affinity"></a></p>
<p dir="auto">This only works on Linux and usually only makes sense if you have CPUs
with different types of cores (e.g. "performance" vs "efficiency" cores)
and are <em>really</em> trying to squeeze the last ounce of speed out of DwarFS.</p>
<p dir="auto">By setting the environment variable <code>DWARFS_WORKER_GROUP_AFFINITY</code>, you
can set the CPU affinity of different worker thread groups, e.g.:</p>
<div data-snippet-clipboard-copy-content="export DWARFS_WORKER_GROUP_AFFINITY=blockify=3:compress=6,7"><pre><code>export DWARFS_WORKER_GROUP_AFFINITY=blockify=3:compress=6,7
</code></pre></div>
<p dir="auto">This will set the affinity of the <code>blockify</code> worker group to CPU 3 and
the affinity of the <code>compress</code> worker group to CPUs 6 and 7.</p>
<p dir="auto">You can use this feature for all tools that use one or more worker thread
groups. For example, the FUSE driver <code>dwarfs</code> and <code>dwarfsextract</code> use a
worker group <code>blkcache</code> that the block cache (i.e. block decompression and
lookup) runs on. <code>mkdwarfs</code> uses a whole array of different worker groups,
namely <code>compress</code> for compression, <code>scanner</code> for scanning, <code>ordering</code> for
input ordering, and <code>blockify</code> for segmenting. <code>blockify</code> is what you would
typically want to run on your "performance" cores.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DNS over Wikipedia (266 pts)]]></title>
            <link>https://github.com/aaronjanse/dns-over-wikipedia</link>
            <guid>40008383</guid>
            <pubDate>Fri, 12 Apr 2024 00:52:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/aaronjanse/dns-over-wikipedia">https://github.com/aaronjanse/dns-over-wikipedia</a>, See on <a href="https://news.ycombinator.com/item?id=40008383">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DNS over Wikipedia</h2><a id="user-content-dns-over-wikipedia" aria-label="Permalink: DNS over Wikipedia" href="#dns-over-wikipedia"></a></p>
<p dir="auto">Wikipedia keeps track of official URLs for popular websites. With DNS over Wikipedia installed, domains ending with <code>.idk</code> are redirected by searching Wikipedia and extracting the relevant URL from the infobox.</p>
<p dir="auto">Example:</p>
<ol dir="auto">
<li>Type <code>scihub.idk/</code> in browser address bar</li>
<li>Observe redirect to <code>https://sci-hub.tw</code> (at the time of writing)</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/aaronjanse/dns-over-wikipedia/blob/master/demo.gif"><img src="https://github.com/aaronjanse/dns-over-wikipedia/raw/master/demo.gif" width="600" data-animated-image=""></a></p>
<blockquote>
<p dir="auto">Instead of googling for the site, I google for the site's Wikipedia article ("schihub wiki") which usually has an up-to-date link to the site in the sidebar, whereas Google is forced to censor their results.</p>
<p dir="auto">If you Google "Piratebay", the first search result is a fake "thepirate-bay.org" (with a dash) but the Wikipedia article lists the right one.
— <a href="https://news.ycombinator.com/item?id=22414031" rel="nofollow">shpx</a></p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation Options</h2><a id="user-content-installation-options" aria-label="Permalink: Installation Options" href="#installation-options"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://chrome.google.com/webstore/detail/mjmjpfncapfopnommmngnmjalkopljji/" rel="nofollow">Chrome Extension</a></h4><a id="user-content-chrome-extension" aria-label="Permalink: Chrome Extension" href="#chrome-extension"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://addons.mozilla.org/en-US/firefox/addon/dns-over-wikipedia/" rel="nofollow">Firefox Extension</a></h4><a id="user-content-firefox-extension" aria-label="Permalink: Firefox Extension" href="#firefox-extension"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://github.com/aaronjanse/dns-over-wikipedia/blob/master/hosts-file">(optional) Rust Redirect Script</a></h4><a id="user-content-optional-rust-redirect-script" aria-label="Permalink: (optional) Rust Redirect Script" href="#optional-rust-redirect-script"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon virtually kills efforts to develop Alexa Skills (155 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2024/04/amazon-virtually-kills-efforts-to-develop-alexa-skills-disappointing-dozens/</link>
            <guid>40008170</guid>
            <pubDate>Fri, 12 Apr 2024 00:19:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2024/04/amazon-virtually-kills-efforts-to-develop-alexa-skills-disappointing-dozens/">https://arstechnica.com/gadgets/2024/04/amazon-virtually-kills-efforts-to-develop-alexa-skills-disappointing-dozens/</a>, See on <a href="https://news.ycombinator.com/item?id=40008170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      disincentives    —
</h4>
            
            <h2 itemprop="description">Most devs would need to pay out of pocket to host Alexa apps after June. </h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2020/12/amazon-echo-dot-4-hero-800x450.jpg" alt="amazon echo dot gen 4">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2020/12/amazon-echo-dot-4-hero.jpg" data-height="1080" data-width="1920">Enlarge</a> <span>/</span> The 4th-gen Amazon Echo Dot smart speaker.</p><p>Amazon</p></figcaption>  </figure>

  




<!-- cache hit 723:single/related:5d634d4fdba58a69bfdeca1f6e197a57 --><!-- empty -->
<p>Alexa hasn't worked out the way Amazon originally planned.</p>
<p>There was a time when it thought that Alexa would yield a robust ecosystem of apps, or Alexa Skills, that would make the voice assistant an integral part of users' lives. Amazon envisioned tens of thousands of software developers building valued abilities for Alexa that would grow the voice assistant's popularity—and help Amazon make some money.</p>
<p>But about seven years after launching a rewards program to encourage developers to build Skills, Alexa's most preferred abilities are the basic ones, like checking the weather. And on June 30, Amazon will stop giving out the monthly Amazon Web Services credits that have made it free for third-party developers to build and host Alexa Skills. The company also recently told devs that its <a href="https://techcrunch.com/2017/08/16/amazon-expands-program-that-pays-alexa-developers-for-top-performing-voice-apps/">Alexa Developer Rewards</a> program was ending, virtually disincentivizing third-party devs to build for Alexa.</p>
<h2>Death knell for third-party Alexa apps</h2>
<p>The news has left dozens of Alexa Skills developers wondering if they have a future with Alexa, especially as Amazon preps a generative AI and <a href="https://arstechnica.com/gadgets/2024/01/alexa-is-in-trouble-paid-for-alexa-gives-inaccurate-answers-in-early-demos/">subscription-based version of Alexa</a>. <a href="https://www.youtube.com/watch?v=lKie-vgUGdI">"Dozens" may sound like a dig</a> at Alexa's ecosystem, but it's an estimation based on a podcast from Skills developers <a href="https://developer.amazon.com/en-US/alexa/champions/mark-tucker">Mark Tucker</a> and Allen Firstenberg, who, in a recent <a href="https://www.youtube.com/watch?v=kIVFnP2Z8ZQ">podcast,</a> agreed that "dozens" of third-party devs were contemplating if it's still worthwhile to develop Alexa skills. The casual summary wasn't stated as a hard fact or confirmed by Amazon but, rather, seemed like a rough and quick estimation based on the developers' familiarity with the Skills community. But with such minimal interest and money associated with Skills, dozens isn't an implausible figure either.</p>
<p>Amazon admitted that there's little interest in its Skills incentives programs. Bloomberg reported that "fewer than 1 percent of developers were using the soon-to-end programs," per Amazon spokesperson Lauren Raemhild.</p>                                            
                                                        
<p>"Today, with over 160,000 skills available for customers and a well-established Alexa developer community, these programs have run their course, and we decided to sunset them," she told the publication.</p>
<p>The writing on the wall, though, is that Amazon doesn't have the incentive or money to grow the Alexa app ecosystem it once imagined. Voice assistants largely became <a href="https://arstechnica.com/gadgets/2022/11/amazon-alexa-is-a-colossal-failure-on-pace-to-lose-10-billion-this-year/">money pits</a>, and the Alexa division has endured recent <a href="https://arstechnica.com/gadgets/2023/11/amazon-lays-off-alexa-employees-as-2010s-voice-assistant-boom-gives-way-to-ai/">layoffs</a> as it fights for survival and relevance. Meanwhile, Google Assistant stopped using third-party apps <a href="https://9to5google.com/2022/06/13/google-assistant-voice-apps/">in 2022</a>.</p>
<p>"Many developers are now going to need to make some tough decisions about maintaining existing or creating future experiences on Alexa," Tucker said via a <a href="https://www.linkedin.com/feed/update/urn:li:activity:7182139278531915776/">LinkedIn post</a>.</p>
<h2>Alexa Skills criticized as “useless”</h2>
<p>As of this writing, the top Alexa skills, in order, are: <em>Jeopardy</em>, <em>Are You Smarter Than a 5th Grader?</em>, <em>Who Wants to Be a Millionaire?</em>, and Calm. That's not exactly a futuristic list of must-have technological feats. For years, people have wondered when the "<a href="https://www.gearbrain.com/amazon-alexa-lack-of-skills-2638790700.html">killer app</a>" would come to catapult Alexa's popularity. But now it seems like Alexa's only hope at that killer use case is generative AI (a gamble filled with its <a href="https://arstechnica.com/gadgets/2023/09/amazons-generative-ai-powered-alexa-is-as-big-a-privacy-red-flag-as-old-alexa/">own obstacles</a>).</p>
<p>But like Amazon, third-party developers found it hard to make money off Skills, with a rare few pointing to making <a href="https://www.cnet.com/home/smart-home/amazon-alexa-economy-echo-speaker-google-assistant-siri/">thousands of dollars at most</a> and the vast majority not making anything.</p>
<p>"If you can't make money off it, no one's going to seriously engage," Joseph "Jo" Jaquinta, a developer who had made over 12 Skills, told <a href="https://www.cnet.com/home/smart-home/amazon-alexa-economy-echo-speaker-google-assistant-siri/">CNET</a> in 2017.</p>
<p>By 2018, Amazon had paid developers <a href="https://developer.amazon.com/en-US/blogs/alexa/post/373f6769-9ee7-41bc-9104-25cd1b393e93/alexa-developer-rewards-program-expands-to-skills-for-kid.html">millions</a> to grow Alexa Skills. But by 2020, Amazon reduced the amount of money it paid out to third-party developers, an anonymous source told Bloomberg, The source noted that the apps made by paid developers weren't making the company much money. Come 2024, the most desirable things you can make Alexa do remain basic tasks, like playing a song and apparently trivia games.</p>
<p>Amazon hasn't said it's ending Skills. That would seem premature considering that its Alexa chatbot isn't expected until June. Developers can still make money off Skills with in-app purchases, but the incentive is minimal.</p>
<p>"Developers like you have and will play a critical role in the success of Alexa, and we appreciate your continued engagement," Amazon's notice to devs said, per Bloomberg.</p>
<p>We'll see how "critical" Amazon treats those remaining developers once its generative AI chatbot is ready.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Descent: A classic 3-D first-person shooter (2012) (134 pts)]]></title>
            <link>http://insectoid.budwin.net/dos/descent/descent.html</link>
            <guid>40006697</guid>
            <pubDate>Thu, 11 Apr 2024 20:52:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://insectoid.budwin.net/dos/descent/descent.html">http://insectoid.budwin.net/dos/descent/descent.html</a>, See on <a href="https://news.ycombinator.com/item?id=40006697">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="rdescm">
<p>DESCENT is a classic 3-D first-person shooter for DOS that was 
released in 1995, developed by Parallax Software and published by Interplay 
Productions.&nbsp; DESCENT is a contemporary of the other very popular FPS of 
the time, DOOM by id Software, released just over a year before.&nbsp; However, 
it is <em>not</em> based on DOOM; the DESCENT engine works quite differently, 
using 3-D models (rather than sprites) to render enemy robots, and has 
"six degrees of freedom".&nbsp; DESCENT also has very little of the 
blood or gore of DOOM.</p>

<p>There were two games made for the DOS platform, <b>DESCENT</b> (1995) and 
<b>DESCENT II</b> (1996, which was also made for Windows 95), as well a sequel 
for Windows 9x, <b>DESCENT<sup>3</sup></b> (1999).&nbsp; There have been many 
expansions for and re-releases of each; the Game Info pages detail as many of 
those that I know about.</p>

<p>For the moment, the rest of the pages will be concentrating on the first 
mission, <span>DESCENT: First Strike</span>.&nbsp; In the future I 
may make <span>DESCENT II</span> pages; but that remains to be 
seen.</p>

<p><img src="http://insectoid.budwin.net/Images/email.png" alt="Email">Questions or comments about the pages, a specific level, or Descent 
in general may be directed to <b>insectoid</b> <i>(at)</i> <b>budwin</b> 
<i>(dot)</i> <b>net</b>; please put <b>IWP:</b> in the subject line.&nbsp; I do 
appreciate the feedback, folks, both good and bad.&nbsp; However, <em>any spam 
mail will be subject to immediate de-resolution!</em>&nbsp; Requests for copies 
of the full-version games are firmly discouraged and will be treated in like 
manner.</p>

<h2><span><img alt="DESCENT" src="http://insectoid.budwin.net/Images/DOS/Descent/d1-logo.png"></span></h2>

<ul>
	<li><a href="http://insectoid.budwin.net/dos/descent/d1gminfo.html">Game Info</a> – Release data, game 
	features, add-ons, and ports to other systems.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1basics.html">Basics</a> – A mini-guide to 
	DESCENT.&nbsp; Under construction.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1levels.html">Walkthrough</a> – A walkthrough of 
	each level.&nbsp; Currently has only a few levels completed.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1robots.html">Robots</a> – Every robot in D1, 
	including the rarely-seen Red Triangle.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1weapons.html">Weapons and Items</a> – All of 
	the power-ups and weapons in D1.</li>
</ul>

<h2><span><img alt="DESCENT II" src="http://insectoid.budwin.net/Images/DOS/Descent/d2-logo.png"></span></h2>

<ul>
	<li><a href="http://insectoid.budwin.net/dos/descent/d2gminfo.html">Game Info</a> – Release data, game 
	features, add-ons, and ports to other systems.</li>
</ul>

<h2><span><img alt="DESCENT 3" src="http://insectoid.budwin.net/Images/DOS/Descent/d3-logo.png"></span></h2>

<ul>
	<li><a href="http://insectoid.budwin.net/dos/descent/d3gminfo.html">Game Info</a> – Release data, game 
	features, add-ons, and mods.</li>
</ul>

<hr>

<h4>LEGAL STUFF:</h4>

<p>The beveled panels, buttons, and page design are ©2011 
Thomas Keith / Insectoid.<br>

The typeface for most of the banners and the dark grey sidebar buttons are 
TrueType versions of the fonts from DESCENT II; ©1997 Harald Koenigsperger 
(Wild Style GraphX).<br>

The typeface for the game logos is a reproduction of the font used on the discs 
and packaging for all three games; I'm attributing this to Interplay 
Productions.<br>

DESCENT is a registered trademark of Interplay Productions.<br> 

DESCENT, DESCENT II, and all related content and images are ©1995, 1996 
Parallax Software Corporation.<br>

DXX-Rebirth, D1X-Rebirth and D2X-Rebirth are ©Christian Beckhaeuser.<br>

DESCENT<sup>3</sup> and all related content and images are ©1999 Outrage 
Entertainment.<br>

All other copyrights and trademarks are property of their respective owners.</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>