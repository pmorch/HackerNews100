<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 21 Feb 2025 01:30:25 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Treasury agrees to block DOGE's access to personal taxpayer data at IRS (162 pts)]]></title>
            <link>https://www.washingtonpost.com/business/2025/02/20/doge-irs-taxpayer-data-privacy/</link>
            <guid>43121306</guid>
            <pubDate>Thu, 20 Feb 2025 22:39:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/business/2025/02/20/doge-irs-taxpayer-data-privacy/">https://www.washingtonpost.com/business/2025/02/20/doge-irs-taxpayer-data-privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=43121306">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/business/2025/02/20/doge-irs-taxpayer-data-privacy/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Immersive Gaussian Splat experience of Sutro Tower, San Francisco (258 pts)]]></title>
            <link>https://vincentwoo.com/3d/sutro_tower/</link>
            <guid>43120582</guid>
            <pubDate>Thu, 20 Feb 2025 21:39:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vincentwoo.com/3d/sutro_tower/">https://vincentwoo.com/3d/sutro_tower/</a>, See on <a href="https://news.ycombinator.com/item?id=43120582">Hacker News</a></p>
Couldn't get https://vincentwoo.com/3d/sutro_tower/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DOGE puts $1 spending limit on government employee credit cards (258 pts)]]></title>
            <link>https://www.wired.com/story/doge-government-credit-cards/</link>
            <guid>43120231</guid>
            <pubDate>Thu, 20 Feb 2025 21:10:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/doge-government-credit-cards/">https://www.wired.com/story/doge-government-credit-cards/</a>, See on <a href="https://news.ycombinator.com/item?id=43120231">Hacker News</a></p>
Couldn't get https://www.wired.com/story/doge-government-credit-cards/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Running Pong in 240 browser tabs (118 pts)]]></title>
            <link>https://eieio.games/blog/running-pong-in-240-browser-tabs/</link>
            <guid>43119086</guid>
            <pubDate>Thu, 20 Feb 2025 19:33:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eieio.games/blog/running-pong-in-240-browser-tabs/">https://eieio.games/blog/running-pong-in-240-browser-tabs/</a>, See on <a href="https://news.ycombinator.com/item?id=43119086">Hacker News</a></p>
Couldn't get https://eieio.games/blog/running-pong-in-240-browser-tabs/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[New horizons for Julia (147 pts)]]></title>
            <link>https://lwn.net/Articles/1006117/</link>
            <guid>43118962</guid>
            <pubDate>Thu, 20 Feb 2025 19:23:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/1006117/">https://lwn.net/Articles/1006117/</a>, See on <a href="https://news.ycombinator.com/item?id=43118962">Hacker News</a></p>
Couldn't get https://lwn.net/Articles/1006117/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[OpenEuroLLM (290 pts)]]></title>
            <link>https://openeurollm.eu/</link>
            <guid>43118634</guid>
            <pubDate>Thu, 20 Feb 2025 18:57:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openeurollm.eu/">https://openeurollm.eu/</a>, See on <a href="https://news.ycombinator.com/item?id=43118634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><div><hr><div><h2>A series of foundation models for transparent AI in Europe</h2><div><div><h3>truly open</h3><p>including data, documentation, training and testing code, and evaluation metrics; including community involvement</p></div><div><h3>compliant</h3><p>under EU regulations, OpenEuroLLM will provide a series of transparent and performant LLMs</p></div><div><h3>diverse</h3><p>for European languages and other socially and economically interesting ones, preserving linguistic and cultural diversity</p></div></div></div></div><div><div><h2>GOALS TOWARDS PERFORMANT AND TRANSPARENT AI</h2><div><div><svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect width="7" height="7" x="14" y="3" rx="1"></rect><path d="M10 21V8a1 1 0 0 0-1-1H4a1 1 0 0 0-1 1v12a1 1 0 0 0 1 1h12a1 1 0 0 0 1-1v-5a1 1 0 0 0-1-1H3"></path></svg><p>Extend the multilingual capabilities of existing models for EU official languages and beyond.</p></div> <div><svg xmlns="http://www.w3.org/2000/svg" width="38" height="38" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 16V8a2 2 0 0 0-1-1.73l-7-4a2 2 0 0 0-2 0l-7 4A2 2 0 0 0 3 8v8a2 2 0 0 0 1 1.73l7 4a2 2 0 0 0 2 0l7-4A2 2 0 0 0 21 16z"></path><circle cx="12" cy="12" r="4"></circle></svg><p>Ensure easy and sustainable access to foundational models ready to be fine-tuned to a wide range of applications.</p></div> <div><svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 13c0 5-3.5 7.5-7.66 8.95a1 1 0 0 1-.67-.01C7.5 20.5 4 18 4 13V6a1 1 0 0 1 1-1c2 0 4.5-1.2 6.24-2.72a1.17 1.17 0 0 1 1.52 0C14.51 3.81 17 5 19 5a1 1 0 0 1 1 1z"></path><path d="M9 12h6"></path><path d="M12 9v6"></path></svg><p>Extend the number of evaluation results in all EU official languages and beyond, including AI safety and alignment with the AI Act and European AI standards</p></div> <div><svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="5" r="1"></circle><path d="m9 20 3-6 3 6"></path><path d="m6 8 6 2 6-2"></path><path d="M12 10v4"></path></svg><p>Extend the number of available training datasets and benchmarks for these languages, and make them easily accessible.</p></div> <div><svg xmlns="http://www.w3.org/2000/svg" width="38" height="38" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14.7 6.3a1 1 0 0 0 0 1.4l1.6 1.6a1 1 0 0 0 1.4 0l3.77-3.77a6 6 0 0 1-7.94 7.94l-6.91 6.91a2.12 2.12 0 0 1-3-3l6.91-6.91a6 6 0 0 1 7.94-7.94l-3.76 3.76z"></path></svg><p>Share transparently the tools, recipes and intermediate results of the training processes.</p></div> <div><svg xmlns="http://www.w3.org/2000/svg" width="38" height="38" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="6" width="20" height="8" rx="1"></rect><path d="M17 14v7"></path><path d="M7 14v7"></path><path d="M17 3v3"></path><path d="M7 3v3"></path><path d="M10 14 2.3 6.3"></path><path d="m14 6 7.7 7.7"></path><path d="m8 6 8 8"></path></svg><p>Share the dataset enrichment and anonymization pipelines to enable further data sourcing for future needs.</p></div> <div><svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 21v-2a4 4 0 0 0-4-4H6a4 4 0 0 0-4 4v2"></path><circle cx="9" cy="7" r="4"></circle><path d="M22 21v-2a4 4 0 0 0-3-3.87"></path><path d="M16 3.13a4 4 0 0 1 0 7.75"></path></svg><p>Create an active and engaged community of developers and stakeholders among the public and private sector.</p></div></div></div><div><h2>PARTNERS IN OPENEUROLLM</h2><p>Europe's leading AI companies and research institutions combine their forces and expertise to develop next-generation open-source language models in an unprecedented collaboration to advance European AI capabilities, the OpenEuroLLM project</p><hr><hr><hr></div></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS S3 SDK breaks its compatible services (171 pts)]]></title>
            <link>https://xuanwo.io/links/2025/02/aws_s3_sdk_breaks_its_compatible_services/</link>
            <guid>43118592</guid>
            <pubDate>Thu, 20 Feb 2025 18:54:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xuanwo.io/links/2025/02/aws_s3_sdk_breaks_its_compatible_services/">https://xuanwo.io/links/2025/02/aws_s3_sdk_breaks_its_compatible_services/</a>, See on <a href="https://news.ycombinator.com/item?id=43118592">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Apache Iceberg community has raised a PR <a href="https://github.com/apache/iceberg/pull/12264">S3: Disable strong integrity checksums</a> to disable the newly introduced integrity checksums in AWS S3 SDKs.</p><p>AWS has <a href="https://aws.amazon.com/blogs/aws/introducing-default-data-integrity-protections-for-new-objects-in-amazon-s3/">introduced default data integrity protections for new objects in Amazon S3</a>, which is a positive development. However, they have also chosen to update the default settings in all SDKs, breaking compatibility with nearly all S3-compatible services.</p><hr><p>First of all, this is a good thing for me because checksums like <code>crc64-nvme</code> looks great—it's fast, secure, and has excellent SIMD support. As a user and developer, I'm excited to use it and integrate it into my projects. However, the S3 API is more than that. Many S3-compatible services are recommending that their users use the S3 SDK directly, and changing the default settings in this way can have a direct impact on their users.</p><p>For examples:</p><ul><li>Cloudlare R2: <a href="https://community.cloudflare.com/t/an-error-occurred-internalerror-when-calling-the-putobject-operation/764905/1">An error occurred (InternalError) when calling the PutObject operation</a></li><li>Tigris: <a href="https://www.tigrisdata.com/blog/downgrade-py-js/">If you’ve upgraded boto3 or the JavaScript S3 client in the last week, uploading files won’t work. Here’s how to fix it.</a></li></ul><blockquote><p>The recent AWS SDK bump introduced strong integrity checksums, and broke compatibility with many S3-compatible object storages (pre-2025 Minio, Vast, Dell EC etc).</p><p>In Trino project, we received the error report (Missing required header for this request: Content-Md5) from several users and had to disable the check temporarily. We recommend disabling it in Iceberg as well. I faced this issue when I tried upgrading Iceberg library to 1.8.0 in Trino.</p></blockquote><p>Although this feature is good, the AWS team has implemented it poorly by enforcing it, causing issues for many users of related services. This reminds me of the position where <a href="https://github.com/apache/opendal">Apache OpenDAL</a> should stand.</p><p>OpenDAL integrates all services by directly communicating with APIs instead of relying on SDKs, protecting users from potential disruptions like this one. OpenDAL's community also takes checksum support into deep consideration and is working to find a solution that benefits users while ensuring compatibility with unsupported services.</p><p>Maybe it's time to move away from using S3 SDKs and switch to OpenDAL if you just want to access compatible services for data:</p><ul><li>OpenDAL has a wide range of integrations tests for s3 compatible services from minio to ceph.</li><li>OpenDAL is governed by <a href="https://people.apache.org/phonebook.html?project=opendal">the Apache OpenDAL PMC</a> under <a href="https://www.apache.org/theapacheway/">the Apache Way</a> and not controlled by any business entity.</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You can’t build a moat with AI (redux) (123 pts)]]></title>
            <link>https://frontierai.substack.com/p/you-cant-build-a-moat-with-ai-redux</link>
            <guid>43118512</guid>
            <pubDate>Thu, 20 Feb 2025 18:49:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frontierai.substack.com/p/you-cant-build-a-moat-with-ai-redux">https://frontierai.substack.com/p/you-cant-build-a-moat-with-ai-redux</a>, See on <a href="https://news.ycombinator.com/item?id=43118512">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Last spring, we wrote an article called </span><em><a href="https://frontierai.substack.com/p/you-cant-build-a-moat-with-ai" rel="">You can’t build a moat with AI</a></em><span>. That post argued that prompt engineering, while important, would be difficult to defend over time given how easy it is to experiment with LLMs. As a result, you have to focus on the quality of data your application has access to and your use of that data to differentiate yourself.</span></p><p>Everything we said in that post has held true, but the release of models like DeepSeek R1 and o3-mini have brought the concerns about AI applications’ moats back to the forefront. In the last few weeks, we’ve heard many forms of the question, “If LLMs keep getting cheaper and better, won’t everyone’s application get better? If so, how do you differentiate?” It’s worth it for every application builder to think through this in the more detail.</p><p>In short, unless you’re building or hosting foundation models, saying that AI is a differentiating factor is sort of like saying that your choice of database is what differentiated your SaaS product: No one cares.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp" width="1456" height="832" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:832,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:633206,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://frontierai.substack.com/i/157554740?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9e64e92-3d60-4b43-bb64-ddf89f442ef5_1792x1024.webp 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Source: DALL-E 3.</figcaption></figure></div><p><span>As a baseline assumption, better models are a rising tide that lifts all boats. It’s generally a good thing for AI applications </span><a href="https://frontierai.substack.com/p/deepseek-o3-and-ai-applications" rel="">when more powerful models come out</a><span> because it gives you a more powerful engine to build on, but this isn’t the basis on which you’re going to build a moat. In the immediate frenzy after a new model is released, one team or another might figure out a better way to use that model, but that advantage is going to evaporate extremely quickly. There’s far too much useful content out on the internet for those advantages to last. This is basically the same as the prompt engineering moat we discussed above. In other words, if you’re relying on that as your differentiation, you’re going to get quickly steamrolled by the competition.</span></p><p>Of course, the existence of a good tool isn’t sufficient — how you use the tool matters just as much. Again, you can’t credibly claim that your application is “smarter” than the competition’s, so you’re going to have to look elsewhere for your differentiation. We’ll refer you back to last year’s post for a walkthrough of how you can build a technical moat, but in light of recent model advancements, we’re thinking about this from a product and market perspective.</p><p>We think defensibility comes down to three things.</p><p><strong>User experience.</strong><span> Sprinkling AI on your application for the sake having AI is something that we’ve railed against many times, so we’ll spare you the sanctimony. As we’ve talked about a couple times recently, we believe that most user experiences should be rethought for AI applications. There are many scenarios in which we’ve had to rely on user input previously that we can now infer. In many ways, the whole point of AI applications is that it should feel like magic because something that you previously had to do by hand is now fully automated with believable intelligence. If you’re thinking about taking traditional forms of UX and adding AI to them, that’s an okay starting point but not a defensible moat. On the other hand, if you’re thinking about how you can meet the user where they are, integrate into their workflow, and get tedious busy work out of their hair seamlessly, you’ll find a way to change the UX. Good UX, of course can be copied, but the teams that are capable of innovating once will do it again (see: Snap). In the context of enterprise products, </span><a href="https://frontierai.substack.com/p/the-rise-of-ai-work" rel="">the focus should be on delivering useful work</a><span>.</span></p><p><strong>Integration and workflows.</strong><span> We’re in the business of selling enterprise products, and many of you probably are too. We like to joke that every enterprise company either becomes a database company or an integrations company, and it’s clear that most AI applications companies are going to be integrations-heavy. In the previous paragraph, we touched on the importance of delivering work and snapping into the user’s works — that means that you’ll need to integrate with the customer’s messaging tool, document system, task tracker, and so on. This might sound tedious (and it can be!), but it’s also critical. Products that are able to integrate deeply into a customer’s workflow are incredibly sticky. You’re providing them so much value that they want you to be a part of everything they do — if they’re ever considering replacing you with something else, that product will have to be at least as valuable and justify the cost of re-integrating with everything you work with.</span></p><p><strong>Data in, data out.</strong><span> Everything in AI always comes back to data. We talked about the use of data extensively in our previous post. To summarize briefly, simply having data is not enough. The hard part is using the right data at the right time (hint: vector search is not enough). From a business perspective, data </span><em>out</em><span> is just as important as data in. At RunLLM, we’re now answering over 100K questions per-month. We can use that data to help our customers in many ways: understanding what features their customers are using, tracking the popularity of different workloads, identifying bugs, and so on. With some labeling, we can also use that content to help ourselves understand what kinds of questions we could do a better job of answering. Not every application is going to do all of those things on day one, but gathering the data over time builds an asset that you will benefit from tremendously in the long run. How you gather that data and how you use it over time will make a huge difference.</span></p><p>Ultimately, our opinion is that AI is no longer an interesting thing to talk about in terms of differentiation. At best, it’s a diminishing advantage, and its worst, it means you’re not planning for the future. Every application should probably be using AI in some form or the other, and the question is how effectively you understand your user’s needs and how effectively you’re able to shape your application to meet those needs. AI has so quickly become mainstream that it should now be considered a standard tool — a carpenter doesn’t brag about their hammer, and you shouldn’t brag about your use of GPT.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Obsidian is now free for work (237 pts)]]></title>
            <link>https://obsidian.md/blog/free-for-work/</link>
            <guid>43117020</guid>
            <pubDate>Thu, 20 Feb 2025 16:50:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://obsidian.md/blog/free-for-work/">https://obsidian.md/blog/free-for-work/</a>, See on <a href="https://news.ycombinator.com/item?id=43117020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://obsidian.md/enterprise">
	<img src="https://obsidian.md/images/blog/free-for-work.png" alt="Obsidian is now free for work">
</a></p><p>People in over 10,000 organizations use Obsidian for work.</p>
<p>Starting today, the Obsidian Commercial license is optional. Anyone can use Obsidian for work, for free. If Obsidian benefits your organization, you can still <a href="https://obsidian.md/buy/license">purchase</a> Commercial licenses to support<span>&nbsp;</span>development.</p>
<p>Nothing else is changing. No account required, no ads, no tracking, no strings attached. Your data remains fully in your control, stored locally in plain text Markdown files. All features are available to you for free without<span>&nbsp;</span>limits.</p>
<p>Why make this change? Simplicity. The Commercial license terms were confusing and added unnecessary complexity to our pricing. Furthermore, as the <a href="https://obsidian.md/about">Obsidian Manifesto</a> states: "<em>we believe that everyone should have the tools to think clearly and organize ideas effectively</em>". This change brings us closer to that<span>&nbsp;</span>principle.</p>
<p>People in over <a href="https://obsidian.md/enterprise">10,000 organizations</a> use Obsidian. Many work in high-security environments, like government, cybersecurity, healthcare, and finance. Some of the largest organizations in the world, including Amazon and Google, have thousands of employees using Obsidian every day. These teams rely on Obsidian to think more effectively and keep total ownership over private<span>&nbsp;</span>data.</p>
<p>Previously, people at companies with two or more employees were required to purchase a Commercial license to use Obsidian for work. Going forward, the Commercial license is no longer required, but remains an optional way for organizations to support Obsidian, similar to the <a href="https://help.obsidian.md/Licenses+and+payment/Catalyst+license">Catalyst license</a> for<span>&nbsp;</span>individuals.</p>
<p>Organizations that support Obsidian are now featured on the <a href="https://obsidian.md/enterprise">Obsidian Enterprise</a> page. Your organization can be showcased by purchasing 25 licenses or<span>&nbsp;</span>more.</p>
<p>Along with Commercial and Catalyst support, our add-on services, <a href="https://obsidian.md/sync">Sync</a> and <a href="https://obsidian.md/publish">Publish</a> help Obsidian remain <a href="https://stephango.com/vcware">100% user-supported</a>. In the future, we hope to offer more services designed for teams. As always, these will be<span>&nbsp;</span>optional.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A cryptocurrency scam that turned a small town against itself (151 pts)]]></title>
            <link>https://www.nytimes.com/2025/02/19/magazine/cryptocurrency-scam-kansas-heartland-bank.html</link>
            <guid>43116410</guid>
            <pubDate>Thu, 20 Feb 2025 16:09:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/02/19/magazine/cryptocurrency-scam-kansas-heartland-bank.html">https://www.nytimes.com/2025/02/19/magazine/cryptocurrency-scam-kansas-heartland-bank.html</a>, See on <a href="https://news.ycombinator.com/item?id=43116410">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/02/19/magazine/cryptocurrency-scam-kansas-heartland-bank.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Spice86 – A PC emulator for real mode reverse engineering (136 pts)]]></title>
            <link>https://github.com/OpenRakis/Spice86</link>
            <guid>43116112</guid>
            <pubDate>Thu, 20 Feb 2025 15:47:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/OpenRakis/Spice86">https://github.com/OpenRakis/Spice86</a>, See on <a href="https://news.ycombinator.com/item?id=43116112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Spice86 - A PC emulator for real mode reverse engineering</h2><a id="user-content-spice86---a-pc-emulator-for-real-mode-reverse-engineering" aria-label="Permalink: Spice86 - A PC emulator for real mode reverse engineering" href="#spice86---a-pc-emulator-for-real-mode-reverse-engineering"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e6d28433c0c1041770537fc7f5af3110f9d9cb0b8e8aded756769aebdba81135/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4c696e75782d677265793f6c6f676f3d6c696e7578"><img src="https://camo.githubusercontent.com/e6d28433c0c1041770537fc7f5af3110f9d9cb0b8e8aded756769aebdba81135/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4c696e75782d677265793f6c6f676f3d6c696e7578" alt="Linux" data-canonical-src="https://img.shields.io/badge/-Linux-grey?logo=linux"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e3f5cc40287954bd7669e0fa45d81273536e73fc77f349d757d8532bb2d4e029/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4f53582d626c61636b3f6c6f676f3d6170706c65"><img src="https://camo.githubusercontent.com/e3f5cc40287954bd7669e0fa45d81273536e73fc77f349d757d8532bb2d4e029/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4f53582d626c61636b3f6c6f676f3d6170706c65" alt="macOS" data-canonical-src="https://img.shields.io/badge/-OSX-black?logo=apple"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5f4d965162d37a8002e568d5a317cf9cb456891091f6797bcb69726243dec71f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d57696e646f77732d7265643f6c6f676f3d77696e646f7773"><img src="https://camo.githubusercontent.com/5f4d965162d37a8002e568d5a317cf9cb456891091f6797bcb69726243dec71f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d57696e646f77732d7265643f6c6f676f3d77696e646f7773" alt="Windows" data-canonical-src="https://img.shields.io/badge/-Windows-red?logo=windows"></a>
<a href="https://github.com/OpenRakis/Spice86/actions/workflows/dotnet.yml"><img src="https://github.com/OpenRakis/Spice86/actions/workflows/prerelease.yml/badge.svg" alt=".NET Build"></a>
<a href="https://www.nuget.org/packages/Spice86" rel="nofollow"><img src="https://camo.githubusercontent.com/6c0cd8a991b665c5092216a443c174c886dd6211ac04a87abc9a3c70fbba3ae9/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f537069636538362e737667" alt="NuGet" data-canonical-src="https://img.shields.io/nuget/v/Spice86.svg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b24b4c09827e3771dd0f71e4e2a091f8fded829f81ca1b58d2c36cf1b12d0e8a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d3434636331312e737667"><img src="https://camo.githubusercontent.com/b24b4c09827e3771dd0f71e4e2a091f8fded829f81ca1b58d2c36cf1b12d0e8a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d3434636331312e737667" alt="Licence" data-canonical-src="https://img.shields.io/badge/License-Apache_2.0-44cc11.svg"></a>
<a href="https://www.nuget.org/packages/Spice86" rel="nofollow"><img src="https://camo.githubusercontent.com/fbc35d558c742f016223fa929affddaf79feb9c51329bb2ba438db1b78037fde/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f64742f53706963653836" alt="downloads" data-canonical-src="https://img.shields.io/nuget/dt/Spice86"></a></p>
<p dir="auto">Spice86 is a tool to execute, reverse engineer and rewrite real mode DOS programs for which source code is not available.</p>
<p dir="auto">Release are available <a href="https://www.nuget.org/packages/Spice86/" rel="nofollow">on Nuget</a>.</p>
<p dir="auto">Pre-releases are also available <a href="https://github.com/OpenRakis/Spice86/releases/tag/latest">on the Release page</a></p>
<p dir="auto">NOTE: This is a port, and a continuation from the <a href="https://github.com/kevinferrare/spice86">original Java Spice86</a>.</p>
<p dir="auto">It requires <a href="https://dotnet.microsoft.com/en-us/download/dotnet/8.0" rel="nofollow">.NET 8</a> and runs on Windows, macOS, and Linux.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Approach</h2><a id="user-content-approach" aria-label="Permalink: Approach" href="#approach"></a></p>
<p dir="auto">Rewriting a program from only the binary is a hard task.</p>
<p dir="auto">Spice86 is a tool that helps you do so with a methodic divide and conquer approach.</p>
<p dir="auto">General process:</p>
<ul dir="auto">
<li>You start by emulating the program in the Spice86 emulator.</li>
<li>At the end of each run, the emulator dumps some runtime data (memory dump and execution flow)</li>
<li>You load those data into <a href="https://github.com/NationalSecurityAgency/ghidra">ghidra</a> via the <a href="https://github.com/OpenRakis/spice86-ghidra-plugin">spice86-ghidra-plugin</a></li>
<li>The plugin converts the assembly instructions in the memory dump to C# that can be loaded into spice86 and used either partially or completely instead of the assembly code.</li>
<li>This allows you to gradually reimplement the assembly code with your C# methods</li>
<li>This is helpful because:
<ul dir="auto">
<li>Small sequences of assembly can be statically analyzed and are generally easy to translate to a higher level language.</li>
<li>You work all the time with a fully working version of the program so it is relatively easy to catch mistakes early.</li>
<li>Rewriting code function by function allows you to discover the intent of the author.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running your exe</h2><a id="user-content-running-your-exe" aria-label="Permalink: Running your exe" href="#running-your-exe"></a></p>
<p dir="auto">This is a .NET program, you run it with the regular command line or dotnet run. Example with running a program called file.exe:</p>

<p dir="auto">COM files and BIOS files are also supported.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dumping data</h2><a id="user-content-dumping-data" aria-label="Permalink: Dumping data" href="#dumping-data"></a></p>
<p dir="auto">It is recommended to set SPICE86_DUMPS_FOLDER environment variable pointing to where the emulator should dump the runtime data.
If the variable is set or if --RecordedDataDirectory parameter is passed, the emulator will dump a bunch of information about the run there. If nothing is set, data will be dumped in the current directory.
If there is already data there the emulator will load it first and complete it, you don't need to start from zero each time!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">More command line options</h2><a id="user-content-more-command-line-options" aria-label="Permalink: More command line options" href="#more-command-line-options"></a></p>
<div data-snippet-clipboard-copy-content="  --Debug                            (Default: false) Starts the program paused.
  --Ems                              (Default: false) Enables EMS memory. EMS adds 8 MB of memory accessible to DOS programs through the EMM Page Frame.
  --A20Gate                          (Default: false) Disables the 20th address line to support programs relying on the rollover of memory addresses above the HMA (slightly above 1 MB).
  -m, --Mt32RomsPath                 Zip file or directory containing the MT-32 ROM files
  -c, --CDrive                       Path to C drive, default is exe parent
  -r, --RecordedDataDirectory        Directory to dump data to when not specified otherwise. Working directory if blank
  -e, --Exe                          Required. Path to executable
  -a, --ExeArgs                      List of parameters to give to the emulated program
  -x, --ExpectedChecksum             Hexadecimal string representing the expected SHA256 checksum of the emulated program
  -f, --FailOnUnhandledPort          (Default: false) If true, will fail when encountering an unhandled IO port. Useful to check for unimplemented hardware. false by default.
  -g, --GdbPort                      gdb port, if empty gdb server will not be created. If not empty, application will pause until gdb connects
  -o, --OverrideSupplierClassName    Name of a class that will generate the initial function information. See documentation for more information.
  -p, --ProgramEntryPointSegment     (Default: 4096) Segment where to load the program. DOS PSP and MCB will be created before it.
  -u, --UseCodeOverride              (Default: true) <true or false> if false it will use the names provided by overrideSupplierClassName but not the code
  -i, --InstructionsPerSecond        <number of instructions that have to be executed by the emulator to consider a second passed> if blank will use time based timer.
  -t, --TimeMultiplier               (Default: 1) <time multiplier> if >1 will go faster, if <1 will go slower.
  -d, --DumpDataOnExit               (Default: true) When true, records data at runtime and dumps them at exit time
  -h, --HeadlessMode                 (Default: false) Headless mode. If true, no GUI is shown.
  -l, --VerboseLogs                  (Default: false) Enable verbose level logs
  -w, --WarningLogs                  (Default: false) Enable warning level logs
  -s, --SilencedLogs                 (Default: false) Disable all logs
  -i, --InitializeDOS                (Default: true) Install DOS interrupt vectors or not.
  --StructureFile                    Path to a C header file that describes the structures in the application. Works best with exports from IDA or Ghidra
  --help                             Display this help screen.
  --version                          Display version information."><pre><code>  --Debug                            (Default: false) Starts the program paused.
  --Ems                              (Default: false) Enables EMS memory. EMS adds 8 MB of memory accessible to DOS programs through the EMM Page Frame.
  --A20Gate                          (Default: false) Disables the 20th address line to support programs relying on the rollover of memory addresses above the HMA (slightly above 1 MB).
  -m, --Mt32RomsPath                 Zip file or directory containing the MT-32 ROM files
  -c, --CDrive                       Path to C drive, default is exe parent
  -r, --RecordedDataDirectory        Directory to dump data to when not specified otherwise. Working directory if blank
  -e, --Exe                          Required. Path to executable
  -a, --ExeArgs                      List of parameters to give to the emulated program
  -x, --ExpectedChecksum             Hexadecimal string representing the expected SHA256 checksum of the emulated program
  -f, --FailOnUnhandledPort          (Default: false) If true, will fail when encountering an unhandled IO port. Useful to check for unimplemented hardware. false by default.
  -g, --GdbPort                      gdb port, if empty gdb server will not be created. If not empty, application will pause until gdb connects
  -o, --OverrideSupplierClassName    Name of a class that will generate the initial function information. See documentation for more information.
  -p, --ProgramEntryPointSegment     (Default: 4096) Segment where to load the program. DOS PSP and MCB will be created before it.
  -u, --UseCodeOverride              (Default: true) &lt;true or false&gt; if false it will use the names provided by overrideSupplierClassName but not the code
  -i, --InstructionsPerSecond        &lt;number of instructions that have to be executed by the emulator to consider a second passed&gt; if blank will use time based timer.
  -t, --TimeMultiplier               (Default: 1) &lt;time multiplier&gt; if &gt;1 will go faster, if &lt;1 will go slower.
  -d, --DumpDataOnExit               (Default: true) When true, records data at runtime and dumps them at exit time
  -h, --HeadlessMode                 (Default: false) Headless mode. If true, no GUI is shown.
  -l, --VerboseLogs                  (Default: false) Enable verbose level logs
  -w, --WarningLogs                  (Default: false) Enable warning level logs
  -s, --SilencedLogs                 (Default: false) Disable all logs
  -i, --InitializeDOS                (Default: true) Install DOS interrupt vectors or not.
  --StructureFile                    Path to a C header file that describes the structures in the application. Works best with exports from IDA or Ghidra
  --help                             Display this help screen.
  --version                          Display version information.
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dynamic analysis</h2><a id="user-content-dynamic-analysis" aria-label="Permalink: Dynamic analysis" href="#dynamic-analysis"></a></p>
<p dir="auto">Spice86 speaks the <a href="https://www.gnu.org/software/gdb/" rel="nofollow">GDB</a> remote protocol:</p>
<ul dir="auto">
<li>it supports most of the commands you need to debug.</li>
<li>it also provides custom GDB commands to do dynamic analysis.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Alternative to GDB</h3><a id="user-content-alternative-to-gdb" aria-label="Permalink: Alternative to GDB" href="#alternative-to-gdb"></a></p>
<p dir="auto">Alternatively, Spice86 has a <a href="https://github.com/OpenRakis/Spice86/wiki/Spice86-internal-debugger">home-grown debugger</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Connecting to GDB</h3><a id="user-content-connecting-to-gdb" aria-label="Permalink: Connecting to GDB" href="#connecting-to-gdb"></a></p>
<p dir="auto">You need to specify a port for the GDB server to start when launching Spice86:</p>

<p dir="auto">Spice86 will wait for GDB to connect before starting execution so that you can setup breakpoints and so on.</p>
<p dir="auto">Here is how to connect from GDB command line client and how to set the architecture:</p>
<div data-snippet-clipboard-copy-content="(gdb) target remote localhost:10000
(gdb) set architecture i8086"><pre><code>(gdb) target remote localhost:10000
(gdb) set architecture i8086
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Vanilla GDB</h3><a id="user-content-vanilla-gdb" aria-label="Permalink: Vanilla GDB" href="#vanilla-gdb"></a></p>
<p dir="auto">You can add breakpoints, step, view memory and so on.</p>
<p dir="auto">Example with a breakpoint on VGA VRAM writes:</p>

<p dir="auto">Viewing assembly:</p>

<p dir="auto">Removing a breakpoint:</p>

<p dir="auto">Searching for a sequence of bytes in memory (start address 0, length F0000, ascii bytes of 'Spice86' string):</p>
<div data-snippet-clipboard-copy-content="(gdb) find /b 0x0, 0xF0000, 0x53, 0x70, 0x69, 0x63, 0x65, 0x38, 0x36"><pre><code>(gdb) find /b 0x0, 0xF0000, 0x53, 0x70, 0x69, 0x63, 0x65, 0x38, 0x36
</code></pre></div>
<p dir="auto">GDB does not support x86 real mode segmented addressing, so pointers need to refer to the actual physical address in memory. VRAM at address A000:0000 would be 0xA0000 in GDB.</p>
<p dir="auto">Similarly, The $pc variable in GDB will be exposed by Spice86 as the physical address pointed by CS:IP.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom GDB commands (where the magic happens)</h3><a id="user-content-custom-gdb-commands-where-the-magic-happens" aria-label="Permalink: Custom GDB commands (where the magic happens)" href="#custom-gdb-commands-where-the-magic-happens"></a></p>
<p dir="auto">The list of custom commands can be displayed like this:</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Dump information about current run</h4><a id="user-content-dump-information-about-current-run" aria-label="Permalink: Dump information about current run" href="#dump-information-about-current-run"></a></p>

<p dir="auto">Dumps everything described below in one shot. Files are created in the dump folder as explained <a href="https://github.com/OpenRakis/Spice86#dumping-data">here</a>
Several files are produced:</p>
<ul dir="auto">
<li>spice86dumpMemoryDump.bin: Snapshot of the real mode address space. Contains the instructions that are actually loaded and executed. They may differ from the exe you are running because DOS programs can rewrite some of their instructions / load additional modules in memory.</li>
<li>spice86dumpExecutionFlow.json: Contains information used by the <a href="https://github.com/OpenRakis/spice86-ghidra-plugin">spice86-ghidra-plugin</a> to make sense of the memory dump, like addresses of the functions, the labels, the instructions that have been executed ...</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Special breakpoints</h4><a id="user-content-special-breakpoints" aria-label="Permalink: Special breakpoints" href="#special-breakpoints"></a></p>
<p dir="auto">Break after x emulated CPU Cycles:</p>
<div data-snippet-clipboard-copy-content="(gdb) monitor breakCycles 1000"><pre><code>(gdb) monitor breakCycles 1000
</code></pre></div>
<p dir="auto">Break at the end of the emulated program:</p>

<p dir="auto">#Refreshing screen or buffers while debugging</p>
<div data-snippet-clipboard-copy-content="(gdb) monitor vbuffer refresh"><pre><code>(gdb) monitor vbuffer refresh
</code></pre></div>
<p dir="auto">For a pleasing and productive experience with GDB, the <a href="https://github.com/epasveer/seer">seerGDB</a> client is highly recommended.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reverse engineering process</h2><a id="user-content-reverse-engineering-process" aria-label="Permalink: Reverse engineering process" href="#reverse-engineering-process"></a></p>
<p dir="auto">Concrete example with Cryo Dune <a href="https://github.com/OpenRakis/Cryogenic">here</a>.</p>
<p dir="auto">First run your program and make sure everything works fine in Spice86. If you encounter issues it could be due to unimplemented hardware / DOS / BIOS features.</p>
<p dir="auto">When Spice86 exits, it should dump data in current folder or in folder specified by env variable</p>
<p dir="auto">Open the data in <a href="https://github.com/NationalSecurityAgency/ghidra">ghidra</a> with the <a href="https://github.com/OpenRakis/spice86-ghidra-plugin">spice86-ghidra-plugin</a> and generate code. You can import the generated files in a template project you generate via the <a href="https://github.com/OpenRakis/spice86-dotnet-templates">spice86-dotnet-templates</a>:</p>
<div data-snippet-clipboard-copy-content="dotnet new spice86.project"><pre><code>dotnet new spice86.project
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overriding emulated code with C# code</h2><a id="user-content-overriding-emulated-code-with-c-code" aria-label="Permalink: Overriding emulated code with C# code" href="#overriding-emulated-code-with-c-code"></a></p>
<p dir="auto">You can provide your own C# code to override the program original assembly code.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Defining overrides</h3><a id="user-content-defining-overrides" aria-label="Permalink: Defining overrides" href="#defining-overrides"></a></p>
<p dir="auto">Spice86 can take in input an instance of Spice86.Core.Emulator.Function.IOverrideSupplier that builds a mapping between the memory address of functions and their C# overrides.</p>
<p dir="auto">For a complete example you can check the source code of <a href="https://github.com/OpenRakis/Cryogenic">Cryogenic</a>.</p>
<p dir="auto">Here is a simple example of how it would look like:</p>
<div dir="auto" data-snippet-clipboard-copy-content="namespace My.Program;

// This class is responsible for providing the overrides to spice86.
// There is only one per program you reimplement.
public class MyProgramOverrideSupplier : IOverrideSupplier {
  public IDictionary<SegmentedAddress, FunctionInformation> GenerateFunctionInformations(int programStartSegment,
                                                                                 Machine machine) {
    Dictionary<SegmentedAddress, FunctionInformation> res = new();
    // In more complex examples, overrides may call each other
    new MyOverrides(res, programStartSegment, machine);
    return res;
  }

  public override string ToString() {
    return &quot;Overrides My program exe. class is &quot; + GetType().FullName;
  }
}

// This class contains the actual overrides. As the project grows, you will probably need to split the reverse engineered code in several classes.
public class MyOverrides : CSharpOverrideHelper {
  private MyOverridesGlobalsOnDs globalsOnDs;

  public MyOverrides(IDictionary<SegmentedAddress, FunctionInformation> functionInformations, ushort entrySegment, Machine machine, ILoggerService loggerService, Configuration configuration) {
    // &quot;myOverides&quot; is a prefix that will be appended to all the function names defined in this class
    base(functionInformations, machine,  loggerService, configuration);
    globalsOnDs = new MyOverridesGlobalsOnDs(machine);
    // incUnknown47A8_0x1ED_0xA1E8_0xC0B8 will get executed instead of the assembly code when a call to 1ED:A1E8 is performed.
    // Also when dumping functions, the name myOverides.incUnknown47A8 or instead of unknown
    // Note: the segment is provided in parameter as spice86 can load executables in different places depending on the configuration
    DefineFunction(segment, 0xA1E8, &quot;incDialogueCount47A8&quot;, IncDialogueCount47A8_0x1ED_0xA1E8_0xC0B8);
    DefineFunction(segment, 0x0100, &quot;addOneToAX&quot;, AddOneToAX_0x1ED_0x100_0x1FD0);
  }

  public Action IncDialogueCount47A8_0x1ED_0xA1E8_0xC0B8() {
    // Accessing the memory via accessors
    globalsOnDs.SetDialogueCount47A8(globalsOnDs.GetDialogueCount47A8() + 1);
    // Depends on the actual return instruction performed by the function, needed to be called from the emulated code as
    // some programs like to mess with the stack ...
    return NearRet();
  }

  private Action AddOneToAX_0x1ED_0x100_0x1FD0() {
    // Assembly for this would be
    // INC AX
    // RETF
    // Note that you can access the whole emulator to change the state in the overrides.
    state.AX++;
    return NearRet();
  }
}

// Memory accesses can be encapsulated into classes like this to give names to addresses and make the code shorter.
public class GlobalsOnDs : MemoryBasedDataStructureWithDsBaseAddress {
    public GlobalsOnDs(IByteReaderWriter memory, SegmentRegisters segmentRegisters) : base(memory, segmentRegisters) {
    }

    // Getters and Setters for address 0x1DD:0x2/0x1DD2.
    // Was accessed via the following registers: DS
    public int Get01DD_0002_Word16() {
        return UInt16[0x2];
    }

    // Operation not registered by running code
    public void Set01DD_0002_Word16(byte value) {
        UInt16[0x2] = value;
    }

    // Getters and Setters for address 0x1138:0x0/0x11380.
    // Operation not registered by running code
    public int Get1138_0000_Word16() {
        return UInt16[0x0];
    }
}"><pre><span>namespace</span> <span>My</span><span>.</span><span>Program</span><span>;</span>

<span>// This class is responsible for providing the overrides to spice86.</span>
<span>// There is only one per program you reimplement.</span>
<span>public</span> <span>class</span> <span>MyProgramOverrideSupplier</span> <span>:</span> <span>IOverrideSupplier</span> <span>{</span>
  <span>public</span> <span>IDictionary</span><span>&lt;</span><span>SegmentedAddress</span><span>,</span> <span>FunctionInformation</span><span>&gt;</span> <span>GenerateFunctionInformations</span><span>(</span><span>int</span> <span>programStartSegment</span><span>,</span>
                                                                                 <span>Machine</span> <span>machine</span><span>)</span> <span>{</span>
    <span>Dictionary</span><span>&lt;</span><span>SegmentedAddress</span><span>,</span> <span>FunctionInformation</span><span>&gt;</span> <span>res</span> <span>=</span> <span>new</span><span>(</span><span>)</span><span>;</span>
    <span>// In more complex examples, overrides may call each other</span>
    <span>new</span> <span>MyOverrides</span><span>(</span><span>res</span><span>,</span> <span>programStartSegment</span><span>,</span> <span>machine</span><span>)</span><span>;</span>
    <span>return</span> <span>res</span><span>;</span>
  <span>}</span>

  <span>public</span> <span>override</span> <span>string</span> <span>ToString</span><span>(</span><span>)</span> <span>{</span>
    <span>return</span> <span>"Overrides My program exe. class is "</span> <span>+</span> <span>GetType</span><span>(</span><span>)</span><span>.</span><span>FullName</span><span>;</span>
  <span>}</span>
<span>}</span>

<span>// This class contains the actual overrides. As the project grows, you will probably need to split the reverse engineered code in several classes.</span>
<span>public</span> <span>class</span> <span>MyOverrides</span> <span>:</span> <span>CSharpOverrideHelper</span> <span>{</span>
  <span>private</span> <span>MyOverridesGlobalsOnDs</span> <span>globalsOnDs</span><span>;</span>

  <span>public</span> <span>MyOverrides</span><span>(</span><span>IDictionary</span><span>&lt;</span><span>SegmentedAddress</span><span>,</span> <span>FunctionInformation</span><span>&gt;</span> <span>functionInformations</span><span>,</span> <span>ushort</span> <span>entrySegment</span><span>,</span> <span>Machine</span> <span>machine</span><span>,</span> <span>ILoggerService</span> <span>loggerService</span><span>,</span> <span>Configuration</span> <span>configuration</span><span>)</span> <span>{</span>
    <span>// "myOverides" is a prefix that will be appended to all the function names defined in this class</span>
    <span>base</span><span>(</span><span>functionInformations</span><span>,</span> <span>machine</span><span>,</span>  <span>loggerService</span><span>,</span> <span>configuration</span><span>)</span><span>;</span>
    <span>globalsOnDs</span> <span>=</span> <span>new</span> <span>MyOverridesGlobalsOnDs</span><span>(</span><span>machine</span><span>)</span><span>;</span>
    <span>// incUnknown47A8_0x1ED_0xA1E8_0xC0B8 will get executed instead of the assembly code when a call to 1ED:A1E8 is performed.</span>
    <span>// Also when dumping functions, the name myOverides.incUnknown47A8 or instead of unknown</span>
    <span>// Note: the segment is provided in parameter as spice86 can load executables in different places depending on the configuration</span>
    <span>DefineFunction</span><span>(</span><span>segment</span><span>,</span> <span>0xA1E8</span><span>,</span> <span>"incDialogueCount47A8"</span><span>,</span> <span>IncDialogueCount47A8_0x1ED_0xA1E8_0xC0B8</span><span>)</span><span>;</span>
    <span>DefineFunction</span><span>(</span><span>segment</span><span>,</span> <span>0x0100</span><span>,</span> <span>"addOneToAX"</span><span>,</span> <span>AddOneToAX_0x1ED_0x100_0x1FD0</span><span>)</span><span>;</span>
  <span>}</span>

  <span>public</span> <span>Action</span> <span>IncDialogueCount47A8_0x1ED_0xA1E8_0xC0B8</span><span>(</span><span>)</span> <span>{</span>
    <span>// Accessing the memory via accessors</span>
    <span>globalsOnDs</span><span>.</span><span>SetDialogueCount47A8</span><span>(</span><span>globalsOnDs</span><span>.</span><span>GetDialogueCount47A8</span><span>(</span><span>)</span> <span>+</span> <span>1</span><span>)</span><span>;</span>
    <span>// Depends on the actual return instruction performed by the function, needed to be called from the emulated code as</span>
    <span>// some programs like to mess with the stack ...</span>
    <span>return</span> <span>NearRet</span><span>(</span><span>)</span><span>;</span>
  <span>}</span>

  <span>private</span> <span>Action</span> <span>AddOneToAX_0x1ED_0x100_0x1FD0</span><span>(</span><span>)</span> <span>{</span>
    <span>// Assembly for this would be</span>
    <span>// INC AX</span>
    <span>// RETF</span>
    <span>// Note that you can access the whole emulator to change the state in the overrides.</span>
    <span>state</span><span>.</span><span>AX</span><span>++</span><span>;</span>
    <span>return</span> <span>NearRet</span><span>(</span><span>)</span><span>;</span>
  <span>}</span>
<span>}</span>

<span>// Memory accesses can be encapsulated into classes like this to give names to addresses and make the code shorter.</span>
<span>public</span> <span>class</span> <span>GlobalsOnDs</span> <span>:</span> <span>MemoryBasedDataStructureWithDsBaseAddress</span> <span>{</span>
    <span>public</span> <span>GlobalsOnDs</span><span>(</span><span>IByteReaderWriter</span> <span>memory</span><span>,</span> <span>SegmentRegisters</span> <span>segmentRegisters</span><span>)</span> <span>:</span> <span>base</span><span>(</span><span>memory</span><span>,</span> <span>segmentRegisters</span><span>)</span> <span>{</span>
    <span>}</span>

    <span>// Getters and Setters for address 0x1DD:0x2/0x1DD2.</span>
    <span>// Was accessed via the following registers: DS</span>
    <span>public</span> <span>int</span> <span>Get01DD_0002_Word16</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>UInt16</span><span>[</span><span>0x2</span><span>]</span><span>;</span>
    <span>}</span>

    <span>// Operation not registered by running code</span>
    <span>public</span> <span>void</span> <span>Set01DD_0002_Word16</span><span>(</span><span>byte</span> <span>value</span><span>)</span> <span>{</span>
        <span>UInt16</span><span>[</span><span>0x2</span><span>]</span> <span>=</span> <span>value</span><span>;</span>
    <span>}</span>

    <span>// Getters and Setters for address 0x1138:0x0/0x11380.</span>
    <span>// Operation not registered by running code</span>
    <span>public</span> <span>int</span> <span>Get1138_0000_Word16</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>UInt16</span><span>[</span><span>0x0</span><span>]</span><span>;</span>
    <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><em>Remember</em>: You must tell Spice86 to use your assembly code overrides with the command line argument "--UseCodeOverride true" when debugging your project.</p>
<p dir="auto">Along with the mandatory path to your DOS program, passed with the --ExePath argument.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Debugger</h2><a id="user-content-debugger" aria-label="Permalink: Debugger" href="#debugger"></a></p>
<p dir="auto">Spice86 comes with a built-in debugger that can be used to debug the emulated program. It is a simple debugger that allows you to inspect the memory, the disassembly, the registers, and the stack.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Structure viewer</h3><a id="user-content-structure-viewer" aria-label="Permalink: Structure viewer" href="#structure-viewer"></a></p>
<p dir="auto">The structure viewer allows you to inspect the memory in a structured way. It is useful to inspect the memory as a structure, like the DOS PSP, the DOS MCB, the VGA registers, etc.</p>
<p dir="auto">First you need a C header file that describes the structures in the application. You can generate one with Ghidra or IDA. Then you can load it with the <code>--StructureFile</code> commandline argument.
This will enable the "Structure view" button in the Memory tab of the debugger.</p>
<p dir="auto">There you enter a segment:offset address and choose the structure you want to view. The structure will be displayed in a tree view and the memory in a hex view.</p>
<p dir="auto">The display updates whenever the application is paused, so you can step through the program and see how the structure changes.
Exporting a new C header file from Ghidra or IDA will also update the structure viewer with the new information real-time.</p>
<p dir="auto">You can also enter the Structure view by selecting a range of bytes in the Memory tab and right-clicking on it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Misc</h2><a id="user-content-misc" aria-label="Permalink: Misc" href="#misc"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">C Drive</h3><a id="user-content-c-drive" aria-label="Permalink: C Drive" href="#c-drive"></a></p>
<p dir="auto">It is possible to provide a C: Drive for emulated DOS functions with the option <strong>--CDrive</strong>. Default is current folder. For some games you may need to set the C drive to the game folder.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Emulated program arguments</h3><a id="user-content-emulated-program-arguments" aria-label="Permalink: Emulated program arguments" href="#emulated-program-arguments"></a></p>
<p dir="auto">You can pass arguments (max 127 chars!) to the emulated program with the option <strong>--ExeArgs</strong>. Default is empty.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Time</h3><a id="user-content-time" aria-label="Permalink: Time" href="#time"></a></p>
<p dir="auto">The emulated Timer hardware of the PC (Intel 8259) supports measuring time from either:</p>
<ul dir="auto">
<li>The real elapsed time. Speed can be altered with parameter <strong>--TimeMultiplier</strong>.</li>
<li>The number of instructions the emulated CPU executed. This is the behaviour that is activated with parameter <strong>--InstructionsPerSecond</strong> and is forced when in GDB mode so that you can debug with peace of mind without the timer triggering.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Screen refresh</h3><a id="user-content-screen-refresh" aria-label="Permalink: Screen refresh" href="#screen-refresh"></a></p>
<p dir="auto">Screen is refreshed 30 times per second and each time a VGA retrace wait is detected (see Renderer.cs).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Emulator features</h3><a id="user-content-emulator-features" aria-label="Permalink: Emulator features" href="#emulator-features"></a></p>
<p dir="auto">CPU:</p>
<ul dir="auto">
<li>Only 16 bits instructions are fully supported, memory size is 1MB</li>
<li>Most 32 bits instructions are implemented, but not validated via integration tests for now.</li>
<li>The only supported addressing mode is real mode. 286/386 Protected mode and the related instructions are not implemented.</li>
<li>Instruction set is (hopefully!) fully implemented for 8086, and validated via automated tests.</li>
<li>For 80186, BOUND instruction is missing.</li>
<li>For 80286, instructions related to protected mode are not implemented</li>
<li>For 80386, protected mode is not implemented.</li>
<li>No FPU instruction implemented apart those used for FPU detection.</li>
</ul>
<p dir="auto">Memory:</p>
<ul dir="auto">
<li>Segmented addressing is implemented.</li>
<li>The A20 Gate is supported.</li>
<li>Helpers are available in order to convert a segmented address into a physical address, and vice-versa.</li>
<li>EMS (Expanded Memory) 3.2 is partially implemented.</li>
<li>XMS (Extended Memory) is not implemented.</li>
<li>X86 Paging (virtual memory) is not implemented.</li>
</ul>
<p dir="auto">Graphics:</p>
<ul dir="auto">
<li>Text modes, VGA, EGA, and CGA are implemented.</li>
</ul>
<p dir="auto">DOS:</p>
<ul dir="auto">
<li>Part of int 21 is implemented. Identifies itself as dos 5.0 for now.</li>
</ul>
<p dir="auto">Input:</p>
<ul dir="auto">
<li>Keyboard</li>
<li>Mouse</li>
<li>No joystick for now</li>
</ul>
<p dir="auto">CD-ROM:</p>
<ul dir="auto">
<li>No MSCDEX support for now. Some games, like DUNE, can be copied entirely from the CD and run from the hard drive.</li>
</ul>
<p dir="auto">Sound:</p>
<p dir="auto">On *nix systems, you'll need to have libportaudio installed.
Without it, there will be no sound.</p>
<ul dir="auto">
<li>PC Speaker is implemented.</li>
<li>Adlib/SoundBlaster MIDI OPL is supported.</li>
<li>SoundBlaster PCM is supported.</li>
<li>MT-32 is supported. However, not on macOS, as a static build of MUNT is missing for that platform. (PRs welcome !)</li>
<li>General MIDI is supported.</li>
</ul>
<p dir="auto">Compatibility list available <a href="https://github.com/OpenRakis/Spice86/blob/master/COMPATIBILITY.md">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to build on your machine</h3><a id="user-content-how-to-build-on-your-machine" aria-label="Permalink: How to build on your machine" href="#how-to-build-on-your-machine"></a></p>
<ul dir="auto">
<li>Install the <a href="https://dotnet.microsoft.com/en-us/download/dotnet/8.0" rel="nofollow">.NET 8 SDK</a> (once)</li>
<li>clone the repo</li>
<li>run this where Spice86.sln is located:</li>
</ul>

<p dir="auto"><h3 tabindex="-1" dir="auto">How to run</h3><a id="user-content-how-to-run" aria-label="Permalink: How to run" href="#how-to-run"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="   Spice86 -e <path to executable>"><pre>   Spice86 -e <span>&lt;</span>path to executable<span>&gt;</span></pre></div>
<p dir="auto">or use this where Spice86.csproj is located:</p>
<div dir="auto" data-snippet-clipboard-copy-content="   dotnet run -e <path to executable>"><pre>   dotnet run -e <span>&lt;</span>path to executable<span>&gt;</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Ghidra plugin</h3><a id="user-content-ghidra-plugin" aria-label="Permalink: Ghidra plugin" href="#ghidra-plugin"></a></p>
<p dir="auto">This uses Ghidra and Java 17.</p>
<p dir="auto">Before using it, define an environnement variable named SPICE86_DUMPS_FOLDER pointing to a folder where the Spice86 dumps are located.
They are generated on exit.</p>
<p dir="auto">General procedure, in order:</p>
<p dir="auto">1.Ghidra's own script 'ImportSymbolScript.py' (input used is "spice86dumpGhidraSymbols.txt")</p>
<p dir="auto">2.Ghidra's Auto-Analyze (only enable 'Dissasemble Entry Points')</p>
<p dir="auto">3.Now, you can use the plugin.</p>
<p dir="auto">Remember: if Ghidra displays SUBROUTINES, use the 'f' key to convert them into functions. The code generator only works with functions.</p>
<p dir="auto">Also, if you have any weird behaviour, make sure you have Java 17 and ONLY Java 17. That's how Ghidra likes it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Some screenshots</h3><a id="user-content-some-screenshots" aria-label="Permalink: Some screenshots" href="#some-screenshots"></a></p>
<p dir="auto">Cryo dune:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenRakis/Spice86/blob/master/doc/cryodune_worm.png"><img src="https://github.com/OpenRakis/Spice86/raw/master/doc/cryodune_worm.png" alt=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenRakis/Spice86/blob/master/doc/cryodune_orni.png"><img src="https://github.com/OpenRakis/Spice86/raw/master/doc/cryodune_orni.png" alt=""></a></p>
<p dir="auto">Prince of persia:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenRakis/Spice86/blob/master/doc/prince_of_persia.PNG"><img src="https://github.com/OpenRakis/Spice86/raw/master/doc/prince_of_persia.PNG" alt=""></a></p>
<p dir="auto">Stunts:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenRakis/Spice86/blob/master/doc/stunts_menu.PNG"><img src="https://github.com/OpenRakis/Spice86/raw/master/doc/stunts_menu.PNG" alt=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenRakis/Spice86/blob/master/doc/stunts_loop.png"><img src="https://github.com/OpenRakis/Spice86/raw/master/doc/stunts_loop.png" alt=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenRakis/Spice86/blob/master/doc/stunts_crash.PNG"><img src="https://github.com/OpenRakis/Spice86/raw/master/doc/stunts_crash.PNG" alt=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenRakis/Spice86/blob/master/doc/stunts_skid.PNG"><img src="https://github.com/OpenRakis/Spice86/raw/master/doc/stunts_skid.PNG" alt=""></a></p>
<p dir="auto">Betrayal at Krondor:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenRakis/Spice86/blob/master/doc/BaK.png"><img src="https://github.com/OpenRakis/Spice86/raw/master/doc/BaK.png" alt=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">Some emulation code was adapted from the <a href="https://github.com/gregdivis/Aeon">Aeon emulator</a> by @gregdivis. Those are: The DMA Controller, the PC Speaker, the SoundBlaster, the MT-32, and General MIDI.</p>
<p dir="auto">This project uses JetBrains Rider licenses, thanks to JetBrains' <a href="https://www.jetbrains.com/community/opensource/#support" rel="nofollow">Open Source Community Support</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenRakis/Spice86/blob/master/jetbrainsrider.svg"><img src="https://github.com/OpenRakis/Spice86/raw/master/jetbrainsrider.svg" alt=""></a></p>
<p dir="auto">The UI is powered by <a href="https://avaloniaui.net/" rel="nofollow">Avalonia UI</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OpenRakis/Spice86/blob/master/avalonia.png"><img src="https://github.com/OpenRakis/Spice86/raw/master/avalonia.png" alt=""></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Obsidian is now free for work (123 pts)]]></title>
            <link>https://obsidian.md/pricing</link>
            <guid>43115767</guid>
            <pubDate>Thu, 20 Feb 2025 15:23:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://obsidian.md/pricing">https://obsidian.md/pricing</a>, See on <a href="https://news.ycombinator.com/item?id=43115767">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="faq">

			<div>
				<h2>FAQ</h2>
				<p>Frequently asked questions about pricing</p>
			</div>

			<div>
				<h3>Do I have to pay for commercial&nbsp;use? <i icon-name="chevron-down"></i></h3>
				<p>No. You are not <em>required</em> to pay for a commercial license, however if you are using Obsidian for work in an organization we encourage you to purchase a commercial license to keep Obsidian independent and 100% user-supported. <a href="https://obsidian.md/commercial">Learn&nbsp;more.</a></p>
			</div>
			<div>
				<h3>Can I buy Obsidian add-ons or licenses for someone&nbsp;else? <i icon-name="chevron-down"></i></h3>
				<p>Yes! You can purchase Obsidian Credit for friends, family, and coworkers. This credit can be used to pay for any Obsidian license or service. To get started, <a href="https://obsidian.md/account">create an account</a> and buy credit. You can then send it to the recipient via email or a shareable link. <a href="https://help.obsidian.md/Licenses+and+payment/Gifting" target="_blank">Learn&nbsp;more.</a>
					</p>
			</div>
			<div>
				<h3>What is your refund&nbsp;policy? <i icon-name="chevron-down"></i></h3>
				<p>Obsidian Publish, and Obsidian Sync — no questions asked. However, please note that Catalyst licenses, Commercial licenses, and Obsidian Credit are non-refundable. <a href="https://help.obsidian.md/Licenses+and+payment/Refund+policy" target="_blank">Learn&nbsp;more.</a>
					</p>
			</div>
			<div>
				<h3>Do you store, access, or process user&nbsp;data? <i icon-name="chevron-down"></i></h3>
				<div>
					<p>Obsidian is a private and secure space for your thoughts. You can use our apps without sharing any personal information, and your data is stored locally on your device, making it inaccessible to us. Additionally, our apps do not collect telemetry data, and we never sell user data. </p>
					<p>If you choose to use <a href="https://obsidian.md/sync">Obsidian Sync</a>, your data is secured with AES‑256 end-to-end encryption, preventing us from reading it. <a href="https://obsidian.md/security">Learn more.</a></p>
				</div>
			</div>

			<div>
				<h3>Do you offer discounts for education, nonprofits, or other&nbsp;groups? <i icon-name="chevron-down"></i></h3>
				<p>Yes, we do! Students, faculty members, and nonprofit employees are eligible for a 40% discount on Obsidian Sync and Publish. We do not offer any other discounts or promotions. <a href="https://help.obsidian.md/Licenses+and+payment/Education+and+non-profit+discount" target="_blank">Learn&nbsp;more.</a>
					</p>
			</div>
			<div>
				<h3>What benefits does the Catalyst license&nbsp;grant? <i icon-name="chevron-down"></i></h3>
				<p>By becoming a <a href="https://help.obsidian.md/Licenses+and+payment/Catalyst+license" target="_blank">Catalyst member</a>, you'll receive early access to beta versions of Obsidian, as well as special badges within the community. Catalyst plays a vital role in helping Obsidian remain 100% user-supported, free from investor influence that could compromise <a href="https://obsidian.md/about">our&nbsp;values</a>.</p>
			</div>

			<div><p>
				More questions? Visit our <a href="https://help.obsidian.md/" target="_blank">Help&nbsp;site&nbsp;→</a>
			</p></div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Helix: A Vision-Language-Action Model for Generalist Humanoid Control (236 pts)]]></title>
            <link>https://www.figure.ai/news/helix</link>
            <guid>43115079</guid>
            <pubDate>Thu, 20 Feb 2025 14:30:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.figure.ai/news/helix">https://www.figure.ai/news/helix</a>, See on <a href="https://news.ycombinator.com/item?id=43115079">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2><span id="introducing-helix"></span>Introducing Helix</h2><p>We're introducing Helix, a generalist Vision-Language-Action (VLA) model that unifies perception, language understanding, and learned control to overcome multiple longstanding challenges in robotics. Helix is a series of firsts:</p><ul><li><p><b>Full-upper-body control</b>: Helix is the first VLA to output high-rate continuous control of the entire humanoid upper body, including wrists, torso, head, and individual fingers.</p></li><li><p><b>Multi-robot collaboration</b>: Helix is the first VLA to operate simultaneously on two robots, enabling them to solve a shared, long-horizon manipulation task with items they have never seen before.</p></li><li><p><b>Pick up anything:</b> Figure robots equipped with Helix can now pick up virtually any small household object, including thousands of items they have never encountered before, simply by following natural language prompts.</p></li><li><p><b>One neural network</b>: Unlike prior approaches, Helix uses a single set of neural network weights to learn all behaviors—picking and placing items, using drawers and refrigerators, and cross-robot interaction—without any task-specific fine-tuning.</p></li><li><p><b>Commercial-ready</b>: Helix is the first VLA that runs entirely onboard embedded low-power-consumption GPUs, making it immediately ready for commercial deployment.</p></li></ul><figure><figcaption>Video 1: Collaborative grocery storage. A single set of Helix neural network weights runs simultaneously on two robots as they work together to put away groceries neither robot has ever seen before.</figcaption></figure><p><b>New Scaling for Humanoid Robotics</b></p><p>The home presents robotics' greatest challenge. Unlike controlled industrial settings, homes are filled with countless objects–delicate glassware, crumpled clothing, scattered toys–each with unpredictable shapes, sizes, colors, and textures. For robots to be useful in households, they will need to be capable of generating intelligent new behaviors on-demand, especially for objects they've never seen before.</p><p>The current state of robotics will not scale to the home without a step change. Teaching robots even a single new behavior currently requires substantial human effort: either hours of PhD-level expert manual programming or thousands of demonstrations. Both are prohibitively expensive when we consider how vast the problem of the home truly is.</p><figure><img alt="" sizes="(min-width: 1024px) 1280px, 100vw" srcset="https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&amp;w=640&amp;q=70 640w, https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&amp;w=750&amp;q=70 750w, https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&amp;w=828&amp;q=70 828w, https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&amp;w=1080&amp;q=70 1080w, https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&amp;w=1200&amp;q=70 1200w, https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&amp;w=1920&amp;q=70 1920w, https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&amp;w=2048&amp;q=70 2048w, https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&amp;w=3840&amp;q=70 3840w" src="https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&amp;w=3840&amp;q=70" width="3840" height="2161" decoding="async" data-nimg="1" loading="lazy"><figcaption>Figure 1: Scaling curves for different approaches to acquiring new robot skills. In conventional heuristic manipulation, skills grow with PhDs who manually script them. In conventional robot imitation learning, skills scale with data collected. With Helix, new skills can be specified on the fly with language.</figcaption></figure><p>But other domains of AI have mastered this kind of instant generalization. What if we could simply translate the rich semantic knowledge captured in Vision Language Models (VLMs) directly into robot actions? This new capability would fundamentally alter robotics' scaling trajectory (Figure 1). Suddenly, new skills that once took hundreds of demonstrations could be obtained instantly just by talking to robots in natural language. The key problem becomes: how do we extract all this common-sense knowledge from VLMs and translate it into generalizable robot control? We built Helix to bridge this gap.</p><h2><span id="helix-a-system-1-system-2-vla-for-whole-upper-body-control"></span>Helix: A "System 1, System 2" VLA for Whole Upper Body Control</h2><p>Helix is a first-of-its-kind "System 1, System 2" VLA model for high-rate, dexterous control of the entire humanoid upper body.&nbsp;</p><p>Prior approaches face a fundamental tradeoff: VLM backbones are general, but not fast, and robot visuomotor policies are fast but not general. Helix resolves this tradeoff through two complementary systems, trained end-to-end to communicate:</p><ul><li><p>System 2 (S2): An onboard internet-pretrained VLM operating at 7-9 Hz for scene understanding and language comprehension, enabling broad generalization across objects and contexts.</p></li><li><p>System 1 (S1): A fast reactive visuomotor policy that translates the latent semantic representations produced by S2 into precise continuous robot actions at 200 Hz.</p></li></ul><p>This decoupled architecture allows each system to operate at its optimal timescale. S2 can "think slow" about high-level goals, while S1 can "think fast" to execute and adjust actions in real-time. For example, during collaborative behavior (see Video 2), S1 quickly adapts to the changing motions of a partner robot while maintaining S2's semantic objectives.</p><figure><figcaption>Video 2: Helix allows for fast fine grained motor adjustments, necessary when reacting to a collaborative partner, while carrying out novel semantic goals.</figcaption></figure><p>Helix's design offers several key advantages over existing approaches:</p><ul><li><p><b>Speed and Generalization</b>: Helix matches the speed of specialized single-task behavioral cloning policies while generalizing zero-shot to thousands of novel test objects.</p></li><li><p><b>Scalability</b>: Helix directly outputs continuous control for high-dimensional action spaces, avoiding complex action tokenization schemes used in prior VLA approaches, which have shown some success in low-dimensional control setups (e.g. binarized parallel grippers) but face scaling challenges with high-dimensional humanoid control.</p></li><li><p><b>Architectural Simplicity</b>: Helix uses standard architectures - an open source, open weight VLM for System 2 and a simple transformer-based visuomotor policy for S1.</p></li><li><p><b>Separation of concerns</b>: Decoupling S1 and S2 allows us to iterate on each system separately, without constraints of finding unified observation space or action representations.</p></li></ul><h2><span id="model-and-training-details"></span>Model and Training Details</h2><p><b>Data</b></p><p>We collect a high quality, multi-robot, multi-operator dataset of diverse teleoperated behaviors, ~500 hours in total. To generate natural language-conditioned training pairs, we use an auto-labeling VLM to generate hindsight instructions. The VLM processes segmented video clips from the onboard robot cameras, prompted with: "What instruction would you have given the robot to get the action seen in this video?" All items handled during training are excluded from evaluations to prevent contamination.</p><p><b>Architecture</b></p><p>Our system comprises two main components: S2, a VLM backbone, and S1, a latent-conditional visuomotor transformer. S2 is built on a 7B-parameter open-source, open-weight VLM pretrained on internet-scale data. It processes monocular robot images and robot state information (consisting of wrist pose and finger positions) after projecting them into vision-language embedding space. Combined with natural language commands specifying desired behaviors, S2 distills all semantic task-relevant information into a single continuous latent vector, passed to S1 to condition its low-level actions.</p><p>S1, an 80M parameter cross-attention encoder-decoder transformer, handles low-level control. It relies on a fully convolutional, multi-scale vision backbone for visual processing, initialized from pretraining done entirely in simulation. While S1 receives the same image and state inputs as S2, it processes them at a higher frequency to enable more responsive closed-loop control. The latent vector from S2 is projected into S1's token space and concatenated with visual features from S1's vision backbone along the sequence dimension, providing task conditioning.</p><p>S1 outputs full upper body humanoid control at 200hz, including desired wrist poses, finger flexion and abduction control, and torso and head orientation targets. We append to the action space a synthetic "percentage task completion" action, allowing Helix to predict its own termination condition, which makes it easier to sequence multiple learned behaviors.</p><p><b>Training</b></p><p>Helix is trained fully end-to-end, mapping from raw pixels and text commands to continuous actions with a standard regression loss. Gradients are backpropagated from S1 into S2 via the latent communication vector used to condition S1's behavior, allowing joint optimization of both components. Helix requires no task-specific adaptation; it maintains a single training stage and single set of neural network weights without separate action heads or per-task fine-tuning stages.</p><p>During training, we add a temporal offset between S1 and S2 inputs. This offset is calibrated to match the gap between S1 and S2's deployed inference latency, ensuring that the real-time control requirements during deployment are accurately reflected in training.</p><p><b>Optimized Streaming Inference</b></p><p>Helix's training design enables efficient model parallel deployment on Figure robots, each equipped with dual low-power-consumption embedded GPUs. The inference pipeline splits across S2 (high-level latent planning) and S1 (low-level control) models, each running on dedicated GPUs. S2 operates as an asynchronous background process, consuming the latest observation (onboard camera and robot state) and natural language commands. It continuously updates a shared memory latent vector that encodes the high-level behavioral intent.</p><p>S1 executes as a separate real-time process, maintaining the critical 200Hz control loop required for smooth whole upper body action. It takes both the latest observation and the most recent S2 latent vector. The inherent speed difference between S2 and S1 inference naturally results in S1 operating with higher temporal resolution on robot observations, creating a tighter feedback loop for reactive control.&nbsp;</p><p>This deployment strategy deliberately mirrors the temporal offset introduced in training, minimizing the train-inference distribution gap. The asynchronous execution model allows both processes to run at their optimal frequencies, allowing us to run Helix as fast as our fastest single task imitation learning policies.</p><h2><span id="results"></span>Results</h2><figure><figcaption>Video 3: Helix's VLA controls the full humanoid upper body, a first in robot learning.</figcaption></figure><p><b>Fine-grained VLA whole upper body control</b></p><p>Helix coordinates a 35-DoF action space at 200Hz, controlling everything from individual finger movements to end-effector trajectories, head gaze, and torso posture. Head and torso control pose unique challenges—as they move, they change both what the robot can reach and what it can see, creating feedback loops that have historically caused instability. Video 3 demonstrates this coordination in action: the robot smoothly tracks its hands with its head while adjusting its torso for optimal reach, all while maintaining precise finger control for grasping. Historically, achieving this level of precision with such a high-dimensional action space has been considered extremely challenging, even for a single known task. To our knowledge, no prior VLA system has demonstrated this degree of real-time coordination while maintaining the ability to generalize across tasks and objects.</p><figure><figcaption>Video 4: Helix coordinates precise multi-robot manipulation.</figcaption></figure><p><b>Zero-shot multi-robot coordination</b></p><p>We push Helix to the limit in a challenging multi-agent manipulation scenario: collaborative zero-shot grocery storage between two Figure robots. Video 1 showcases two fundamental advances: The robots successfully manipulate entirely novel groceries—items never encountered during training—demonstrating robust generalization across diverse shapes, sizes, and materials. Additionally, both robots operate using identical Helix model weights, eliminating the need for robot-specific training or explicit role assignments. They achieve coordination through natural language prompts like "Hand the bag of cookies to the robot on your right" or "Receive the bag of cookies from the robot on your left and place it in the open drawer" (see Video 4). This marks the first demonstration of flexible, extended collaborative manipulation between multiple robots using a VLA, particularly significant given their successful handling of completely novel objects.</p><p><b>Emergent "Pick up anything"</b></p><p>We find that Figure robots equipped with Helix can pick up virtually any small household object with a simple "Pick up the [X]" command. In systematic testing, the robots successfully handled thousands of novel items in clutter—from glassware and toys to tools and clothing—without any prior demonstrations or custom programming.</p><p>Particularly notable is how Helix bridges the gap between internet-scale language understanding and precise robot control. When prompted to "Pick up the desert item", for instance, Helix not only recognizes that a toy cactus matches this abstract concept, but also selects the closest hand and executes the precise motor commands needed to grasp it securely.</p><p>This general-purpose "language-to-action" grasping capability opens new exciting new possibilities for humanoid deployment in unstructured environments.</p><figure><figcaption>Video 5: Helix translates high level conceptual commands like "Pick up the desert item" to low-level action.</figcaption></figure><h2><span id="discussion"></span>Discussion</h2><p><b>Helix's training is highly efficient</b></p><p>Helix achieves strong object generalization with remarkably few resources. We train Helix with ~500 hours of high quality supervised data in total, a small fraction of the size of previously collected VLA datasets (&lt;5%), and without any dependencies around multi-robot-embodiment collect or multiple stages of training. We note that this is a scale of collect more comparable to modern <i>single</i>-task imitation learning datasets. Despite this comparatively small data requirement, Helix scales to the significantly more challenging action space of full upper body humanoid control, with high-rate, high-dimensional outputs.</p><p><b>A single set of weights</b></p><p>Existing VLA systems often require specialized fine-tuning or dedicated action heads to optimize performance across different high-level behaviors. Remarkably, Helix achieves strong performance across diverse tasks with a single unified model. Using just one set of neural network weights (7B for System 2, 80M for System 1), Helix picks and places items in various containers, operates drawers and refrigerators, coordinates dexterous multi-robot handovers, and manipulates thousands of novel objects.</p><figure><figcaption>Video 6: "Pick up the helix"</figcaption></figure><h2><span id="conclusion"></span>Conclusion</h2><p>We have presented Helix, the first Vision-Language-Action model to directly control an entire humanoid upper body from natural language. Unlike earlier robot systems, Helix is capable of generating long-horizon, collaborative, dexterous manipulation on the fly without any task-specific demonstrations or extensive manual programming. Helix displays strong object generalization, being able to pick up thousands of novel household items with varying shapes, sizes, colors, and material properties never encountered before in training, simply by asking in natural language. This represents a transformative step forward in how Figure scales humanoid robot behaviors—one that we believe will be pivotal as our robots increasingly assist in everyday home environments.</p><p>While these early results are truly exciting, we think they only scratch the surface of what is possible. We are eager to see what happens when we scale Helix by 1,000x and beyond. If you’re as fascinated by the possibilities of Helix—and the future of dexterous humanoid robotics, we invite you to join us on this journey.</p><p>Consider joining our Helix team to help scale Embodied AI to millions of robots. <a target="_blank" href="https://www.figure.ai/careers">Check out our open roles here. </a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Texas Banned Abortion. Then Sepsis Rates Soared (144 pts)]]></title>
            <link>https://www.propublica.org/article/texas-abortion-ban-sepsis-maternal-mortality-analysis</link>
            <guid>43114990</guid>
            <pubDate>Thu, 20 Feb 2025 14:24:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.propublica.org/article/texas-abortion-ban-sepsis-maternal-mortality-analysis">https://www.propublica.org/article/texas-abortion-ban-sepsis-maternal-mortality-analysis</a>, See on <a href="https://news.ycombinator.com/item?id=43114990">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pp-location="article body">

                    


        
                    <div data-pp-location="top-note">
                

                                                
            <p>ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive <a href="https://www.propublica.org/newsletters/the-big-story?source=www.propublica.org&amp;placement=top-note&amp;region=national">our biggest stories</a> as soon as they’re published.</p>

                

            </div><!-- end .article-body__top-notes -->
        
        
        




                    

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="3.0">Pregnancy became far more dangerous in Texas after the state banned abortion in 2021, ProPublica found in a first-of-its-kind data analysis. </p>

<p data-pp-blocktype="copy" data-pp-id="3.1">The rate of sepsis shot up more than 50% for women hospitalized when they lost their pregnancies in the second trimester, ProPublica found.</p>

<p data-pp-blocktype="copy" data-pp-id="3.2">The surge in this life-threatening condition, caused by infection, was most pronounced for patients whose fetus may still have had a heartbeat when they arrived at the hospital. </p>

<p data-pp-blocktype="copy" data-pp-id="3.3">ProPublica previously reported on two such cases in which miscarrying women in Texas died of sepsis after doctors delayed evacuating their uteruses. Doing so would have been considered an abortion.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="4.0">The new reporting shows that, after the state banned abortion, dozens more pregnant and postpartum women died in Texas hospitals than had in pre-pandemic years, which ProPublica used as a baseline to avoid COVID-19-related distortions. As the maternal mortality rate dropped nationally, ProPublica found, it rose substantially in Texas.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="5.0">ProPublica’s analysis is the most detailed look yet at a rise in life-threatening complications for women losing a pregnancy after Texas banned abortion. It raises concerns that the same pattern may be occurring in more than a dozen other states with similar bans.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="6.0">To chart the scope of pregnancy-related infections, ProPublica purchased and analyzed seven years of Texas’ hospital discharge data.</p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        





    


            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="8.0">“This is exactly what we predicted would happen and exactly what we were afraid would happen,” said Dr. Lorie Harper, a maternal-fetal medicine specialist in Austin.</p>
        
    
                    
            
    
                    
<p data-pp-blocktype="copy" data-pp-id="10.0">She and a dozen other maternal health experts who reviewed ProPublica’s findings say they add to the  evidence that the state’s abortion ban is leading to dangerous delays in care. Texas law threatens up to 99 years in prison for providing an abortion. Though the ban includes an exception for a “medical emergency,” the definition of what constitutes an emergency has been subject to confusion and debate.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="11.0">Many said the ban is the only explanation they could see for the sudden jump in sepsis cases.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="12.0">The new analysis comes as Texas legislators consider amending the abortion ban in the wake of ProPublica’s previous reporting, and as doctors, federal lawmakers and the state’s largest newspaper have urged Texas officials to review pregnancy-related deaths from the first full years after the ban was enacted; the state maternal mortality review committee has, thus far, opted not to examine the death data for 2022 and 2023.   </p>

<p data-pp-blocktype="copy" data-pp-id="12.1">The standard of care for miscarrying patients in the second trimester is to offer to empty the uterus, according to leading medical organizations, which can lower the risk of contracting an infection and developing sepsis. If a patient’s water breaks or her cervix opens, that risk rises with every passing hour.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="13.0">Sepsis can lead to permanent kidney failure, brain damage and dangerous blood clotting. Nationally, it is one of the leading causes of deaths in hospitals.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="14.0">While some Texas doctors have told ProPublica they regularly offer to empty the uterus in these cases, others say their hospitals don’t allow them to do so until the fetal heartbeat stops or they can document a life-threatening complication.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="15.0">Last year, ProPublica reported on the repercussions of these kinds of delays.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="16.0">Forced to wait 40 hours as her dying fetus pressed against her cervix, <a href="https://www.propublica.org/article/josseli-barnica-death-miscarriage-texas-abortion-ban">Josseli Barnica risked a dangerous infection</a>. Doctors didn’t induce labor until her fetus no longer had a heartbeat.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="17.0">Physicians waited, too, as <a href="https://www.propublica.org/article/nevaeh-crain-death-texas-abortion-ban-emtala">Nevaeh Crain’s organs failed</a>. Before rushing the pregnant teenager to the operating room, they ran an extra test to confirm her fetus had expired.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="18.0">Both women had hoped to carry their pregnancies to term, both suffered miscarriages and both died.</p>
        
    
                    
        <figure data-pp-id="20" data-pp-blocktype="intro-module">
    <h4>
        What We’re Watching
    </h4>
    <p>During Donald Trump’s second presidency, ProPublica will focus on the areas most in need of scrutiny. Here are some of the issues our reporters will be watching — and how to get in touch with them securely.</p>
    <div>

        
<div>
            
    


    <p><img alt="" width="400" height="400" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/ProPublica-Reporters-Trump_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=800&amp;s=4ded2e639bc7a5ceafbc0e26fede28fb" srcset="https://img.assets-d.propublica.org/v5/images/ProPublica-Reporters-Trump_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=400&amp;q=75&amp;w=400&amp;s=41ee031e0b19bd7a9b593023cb32ec74 400w, https://img.assets-d.propublica.org/v5/images/ProPublica-Reporters-Trump_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=800&amp;s=4ded2e639bc7a5ceafbc0e26fede28fb 800w, https://img.assets-d.propublica.org/v5/images/ProPublica-Reporters-Trump_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1200&amp;q=75&amp;w=1200&amp;s=6813e35a3f61a4d65aeace96046d9d5c 1200w, https://img.assets-d.propublica.org/v5/images/ProPublica-Reporters-Trump_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1300&amp;q=75&amp;w=1300&amp;s=c156d8b6c175da431c7838bd97a86dbe 1300w, https://img.assets-d.propublica.org/v5/images/ProPublica-Reporters-Trump_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1450&amp;q=75&amp;w=1450&amp;s=0c8bab7c2608527a42a03775bed40d7e 1450w, https://img.assets-d.propublica.org/v5/images/ProPublica-Reporters-Trump_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1600&amp;q=75&amp;w=1600&amp;s=68b4f93ce5534506098073b8b35df69b 1600w, https://img.assets-d.propublica.org/v5/images/ProPublica-Reporters-Trump_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2000&amp;q=75&amp;w=2000&amp;s=3df23fd68e3b45483b727b2c381e6a98 2000w"></p>
</div>



                
                    
<div>
            <p><img alt="Portrait of Mark Olalde" src="https://assets-c3.propublica.org/images/bio/_oneOne75w/mark-olalde.jpg" width="75" height="75" srcset="https://assets-c3.propublica.org/images/bio/_oneOne75w/mark-olalde.jpg 75w, https://assets-c3.propublica.org/images/bio/_oneOne150w/mark-olalde.jpg 150w" loading="lazy"></p><div>
                    <h5>Mark Olalde</h5>
<p>I’m interested in Trump’s and his allies’ promises to dismantle the federal bureaucracy and laws that protect the environment.</p>

                <ul>
                            <li><a href="https://www.propublica.org/cdn-cgi/l/email-protection#97faf6e5fcb9f8fbf6fbf3f2d7e7e5f8e7e2f5fbfef4f6b9f8e5f0">Email</a></li>
                            <li><a href="https://signal.me/#p/+16306054470">Signal</a></li>
                    </ul>
    </div>
</div>
                    
<div>
            <p><img alt="Photo of Maryam Jameel" width="400" height="400" src="https://img.assets-c3.propublica.org/images/bio/maryam-jameel-sized.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=800&amp;s=f76107a3ce87b74ecea00f3151342016" srcset="https://img.assets-c3.propublica.org/images/bio/maryam-jameel-sized.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=400&amp;q=75&amp;w=400&amp;s=0799731aedffff4b6f9e9e5543561d5f 400w, https://img.assets-c3.propublica.org/images/bio/maryam-jameel-sized.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=800&amp;s=f76107a3ce87b74ecea00f3151342016 800w, https://img.assets-c3.propublica.org/images/bio/maryam-jameel-sized.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1200&amp;q=75&amp;w=1200&amp;s=3f0b20c9c263cbd501f8266d3d2dc80e 1200w, https://img.assets-c3.propublica.org/images/bio/maryam-jameel-sized.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1300&amp;q=75&amp;w=1300&amp;s=31e273e5b9d3d5bfdb0bd6b173e0ae50 1300w, https://img.assets-c3.propublica.org/images/bio/maryam-jameel-sized.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1450&amp;q=75&amp;w=1450&amp;s=4dc206dffedfd6773ef2357841158031 1450w, https://img.assets-c3.propublica.org/images/bio/maryam-jameel-sized.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1600&amp;q=75&amp;w=1600&amp;s=ea238008565d2cdcc2e1a5eb5202e617 1600w, https://img.assets-c3.propublica.org/images/bio/maryam-jameel-sized.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2000&amp;q=75&amp;w=2000&amp;s=25e432eca8ec6ce956e791bd8edbdc81 2000w"></p><div>
                    <h5>Maryam Jameel</h5>
<p>I’m an engagement reporter interested in immigration, labor and the federal workforce.</p>

                <ul>
                            <li><a href="tel:12028869548">Phone</a></li>
                            <li><a href="https://signal.me/#p/+12028869548">Signal</a></li>
                            <li><a href="https://www.propublica.org/cdn-cgi/l/email-protection#f49995868d9599da9e9599919198b484869b848196989d9795da9b8693">Email</a></li>
                    </ul>
    </div>
</div>
                    
<div>
            <p><img alt="Portrait of Robert Faturechi" src="https://assets-c3.propublica.org/images/bio/_oneOne75w/20171213_portraits00280-robert-faturechi-sized.jpg" width="75" height="75" srcset="https://assets-c3.propublica.org/images/bio/_oneOne75w/20171213_portraits00280-robert-faturechi-sized.jpg 75w, https://assets-c3.propublica.org/images/bio/_oneOne150w/20171213_portraits00280-robert-faturechi-sized.jpg 150w" loading="lazy"></p><div>
                    <h5>Robert Faturechi</h5>
<p>I have been reporting on Trump Media, the parent company of Truth Social. I’m also reporting on the Trump administration’s trade policies, including tariffs.</p>

                <ul>
                            <li><a href="https://signal.me/#p/+12132717217">Signal</a></li>
                            <li><a href="https://wa.me/12132717217">WhatsApp</a></li>
                            <li><a href="https://www.propublica.org/cdn-cgi/l/email-protection#3b4954595e494f155d5a4f4e495e5853527b4b49544b4e595752585a1554495c">Email</a></li>
                    </ul>
    </div>
</div>
                    
<div>
            <p><img alt="Photo of Jesse Coburn" width="400" height="400" src="https://img.assets-c3.propublica.org/images/bio/Jesse-Coburn-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=800&amp;s=10f6ed75833d0c1965211df2b79660e7" srcset="https://img.assets-c3.propublica.org/images/bio/Jesse-Coburn-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=400&amp;q=75&amp;w=400&amp;s=60beeb4d6f81e4c09ac4c15512adbd97 400w, https://img.assets-c3.propublica.org/images/bio/Jesse-Coburn-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=800&amp;s=10f6ed75833d0c1965211df2b79660e7 800w, https://img.assets-c3.propublica.org/images/bio/Jesse-Coburn-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1200&amp;q=75&amp;w=1200&amp;s=099f2b4e7684e137bb19100524a961c0 1200w, https://img.assets-c3.propublica.org/images/bio/Jesse-Coburn-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1300&amp;q=75&amp;w=1300&amp;s=f9c56c686d6881eb42dc122f55459f33 1300w, https://img.assets-c3.propublica.org/images/bio/Jesse-Coburn-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1450&amp;q=75&amp;w=1450&amp;s=63b2a160afcbbe699afb2bbf30d2fbd6 1450w, https://img.assets-c3.propublica.org/images/bio/Jesse-Coburn-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1600&amp;q=75&amp;w=1600&amp;s=e9d7e6cc061cda74eaf398ee03a64c07 1600w, https://img.assets-c3.propublica.org/images/bio/Jesse-Coburn-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2000&amp;q=75&amp;w=2000&amp;s=7a3afc11e47aafbee2095d35a7a6beaf 2000w"></p><div>
                    <h5>Jesse Coburn</h5>
<p>I’m tracking how the Trump administration reshapes policy at the Department of Housing and Urban Development and the Department of Transportation.</p>

                <ul>
                            <li><a href="https://www.propublica.org/cdn-cgi/l/email-protection#95fff0e6e6f0bbf6faf7e0e7fbd5e5e7fae5e0f7f9fcf6f4bbfae7f2">Email</a></li>
                            <li><a href="https://signal.me/#p/+19172396642">Signal</a></li>
                            <li><a href="https://wa.me/19172396642">WhatsApp</a></li>
                            <li><a href="tel:19172396642">Phone</a></li>
                    </ul>
    </div>
</div>
        
        
<div>
            
    


    <p><img alt="" width="400" height="400" loading="lazy" js-autosizes="" src="https://img.assets-c3.propublica.org/images/externals/20191029-federal-govt-callout-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=800&amp;s=d53f4f2e61169b796ae4d7a314ae618d" srcset="https://img.assets-c3.propublica.org/images/externals/20191029-federal-govt-callout-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=400&amp;q=75&amp;w=400&amp;s=be2589eb833e6e7b11c9bf797a166799 400w, https://img.assets-c3.propublica.org/images/externals/20191029-federal-govt-callout-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=800&amp;s=d53f4f2e61169b796ae4d7a314ae618d 800w, https://img.assets-c3.propublica.org/images/externals/20191029-federal-govt-callout-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1200&amp;q=75&amp;w=1200&amp;s=0a9cdcc42005c4310a5b181a4bfa3188 1200w, https://img.assets-c3.propublica.org/images/externals/20191029-federal-govt-callout-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1300&amp;q=75&amp;w=1300&amp;s=5f97bad407ac7a14f1e3ba65fb0bd577 1300w, https://img.assets-c3.propublica.org/images/externals/20191029-federal-govt-callout-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1450&amp;q=75&amp;w=1450&amp;s=36bce00446cce8166beb952e5d24ba7b 1450w, https://img.assets-c3.propublica.org/images/externals/20191029-federal-govt-callout-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1600&amp;q=75&amp;w=1600&amp;s=949b62d97e21f05e775e9b41780df347 1600w, https://img.assets-c3.propublica.org/images/externals/20191029-federal-govt-callout-1x1.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2000&amp;q=75&amp;w=2000&amp;s=d7655549311bc7c97ce7e5d98f70ff20 2000w"></p><p>If you don’t have a specific tip or story in mind, we could still use your help. Sign up to be a member of our <a href="https://www.propublica.org/tips/federal-workers/">federal worker source network</a> to stay in touch.</p>
</div>

            </div>
    <nav>
        <a href="#"><span></span>&nbsp;</a>
        <a href="#"><span></span>&nbsp;</a>
        <a href="#"><span></span>&nbsp;</a>
        <a href="#"><span></span>&nbsp;</a>
        <a href="#"><span></span>&nbsp;</a>
        <a href="#"><span></span>&nbsp;</a>
            </nav>
    <p>We’re trying something new. <a href="https://iteratehq.com/propublica/670eabefffb484af7dc7db86">Was it helpful?</a></p>
</figure>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="21.0">In response to their stories, <a href="https://www.documentcloud.org/documents/25345654-letter-from-111-texas-obgyns/">111 doctors wrote a letter to the Legislature</a> saying the abortion ban kept them from providing lifesaving care and demanding a change.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="22.0">“It’s black and white in the law, but it’s very vague when you’re in the moment,” said Dr. Tony Ogburn, an OB-GYN in San Antonio. When the fetus has a heartbeat, doctors can’t simply follow the usual evidence-based guidelines, he said. Instead, there is a legal obligation to assess whether a woman’s condition is dire enough to merit an abortion under a prosecutor’s interpretation of the law.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="23.0">Some prominent Texas Republicans who helped write and pass Texas’ strict abortion bans have recently said that the law should be changed to protect women’s lives — though it’s unclear if proposed amendments will receive a public hearing during the current legislative session.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="24.0">ProPublica’s findings indicate that the law is getting in the way of providing abortions that can protect against life-threatening infections, said Dr. Sarah Prager, a professor of obstetrics and gynecology at the University of Washington.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="25.0">“We have the ability to intervene before these patients get sick,” she said. “This is evidence that we aren’t doing that.”</p>

<h3>A New View</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="26.0">Health experts, specially equipped to study maternal deaths, sit on federal agencies and state-appointed review panels. But, as ProPublica previously reported, <a href="https://www.propublica.org/article/abortion-ban-deaths-cdc-maternal-health-care">none of these bodies have systematically assessed</a> the <a href="https://www.propublica.org/article/abortion-bans-deaths-state-maternal-mortality-committees">consequences of abortion bans</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="27.0">So ProPublica set out to do so, <a href="https://www.propublica.org/article/georgia-abortion-ban-amber-thurman-death">first</a> by <a href="https://www.propublica.org/article/candi-miller-abortion-ban-death-georgia">investigating</a> preventable <a href="https://www.propublica.org/article/porsha-ngumezi-miscarriage-death-texas-abortion-ban">deaths</a>, and now by using data to take a broader view, looking at what happened in Texas hospitals after the state banned abortion, in particular as women faced miscarriages.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="28.0">“It is kind of mindblowing that even before the bans researchers barely looked into complications of pregnancy loss in hospitals,” said perinatal epidemiologist Alison Gemmill, an expert on miscarriage at Johns Hopkins Bloomberg School of Public Health.</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="30.0">In consultation with Gemmill and more than a dozen other maternal health researchers and obstetricians, ProPublica built a framework for <a href="https://www.propublica.org/article/texas-maternal-mortality-analysis-methodology">analyzing Texas hospital discharge data</a> from 2017 to 2023, the most recent full year available. This billing data, kept by hospitals and collected by the state, catalogues what happens in every hospitalization. It is anonymized but remarkable in its granularity, including details such as gestational age, complications and procedures.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="31.0">To study infections during pregnancy loss, ProPublica identified all hospitalizations that included miscarriages, terminations and births from the beginning of the second trimester up to 22 weeks’ gestation, before fetal viability. Since first-trimester miscarriage is often managed in an outpatient setting, ProPublica did not include those cases in this analysis.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="32.0">When looking at stays for second-trimester pregnancy loss, ProPublica found a relatively steady rate of sepsis before Texas made abortion a crime. In late 2021, the state made it a civil offense to end a pregnancy after a fetus developed cardiac activity, and in the summer of 2022, the state made it a felony to terminate any pregnancy, with few exceptions.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="33.0">In 2021, 67 patients who lost a pregnancy in the second trimester were diagnosed with sepsis — as in the previous years, they accounted for about 3% of the hospitalizations.</p>

<p data-pp-blocktype="copy" data-pp-id="33.1">In 2022, that number jumped to 90. </p>

<p data-pp-blocktype="copy" data-pp-id="33.2">The following year, it climbed to 99.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="34.0">ProPublica’s analysis was conservative and likely missed some cases. It doesn’t capture what happened to miscarrying patients who were turned away from emergency rooms or those like Barnica who were made to wait, then discharged home before they returned with sepsis.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="35.0">Our analysis showed that patients who were admitted while their fetus was still believed to have a heartbeat were far more likely to develop sepsis.</p>
        
            
    
    
    
                            

<figure data-pp-id="2" data-pp-blocktype="embed">

    
<div>
            <p id="sepsis-rates-spiked-for-patients-whose-initial-diagnosis-didnt-include-fetal-death">Sepsis Rates Spiked for Patients Whose Initial Diagnosis Didn’t Include Fetal Death</p>
                <p>For patients in Texas hospitals who lost a pregnancy, about half were not diagnosed with fetal demise when they were admitted, meaning that their fetus may still have had a heartbeat at that time. Those patients saw a dramatic increase in sepsis after the state banned abortion.</p>
    </div>


                        

<div>
    <div>
        <p>Years abortion was banned</p>
        <p>Sepsis rate for hospitalizations without fetal death</p>
        <p>With fetal death</p>
        <p>6.9%</p>
        <p>3.7%</p>
        <p>3.1%</p>
        <p>2.1%</p>
    </div>

    <svg id="iufdChartSvg"></svg>
</div>



            
    
<figcaption>
        <span>Note: For hospitalizations involving a pregnancy loss between 13 weeks’ gestation and the end of the 21st week. We identified patients whose fetus had no heartbeat when they were admitted by looking for a diagnosis of “intrauterine death” or “missed abortion.” Rates are annual.</span>
    
        <span>
        <span>Credit: </span>
        Lucas Waldron/ProPublica
    </span>
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="37.0">“What this says to me is that once a fetal death is diagnosed, doctors can appropriately take care of someone to prevent sepsis, but if the fetus still has a heartbeat, then they aren’t able to act and the risk for maternal sepsis goes way up,” said Dr. Kristina Adams Waldorf, professor of obstetrics and gynecology at UW Medicine and an expert in pregnancy complications. “This is needlessly putting a woman’s life in danger.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="38.0">Studies indicate that waiting to evacuate the uterus <a href="https://www.thieme-connect.com/products/ejournals/abstract/10.1055/a-2302-8657">increases rates of sepsis for patients</a> whose water breaks before the fetus can survive outside the womb, a condition called previable premature rupture of membranes or PPROM. Because of the risk of infection, major medical organizations like the Society for Maternal-Fetal Medicine and the American College of Obstetricians and Gynecologists <a href="https://www.ajog.org/article/S0002-9378(24)00759-2/fulltext">advise doctors</a> to <a href="http://unmfm.pbworks.com/w/file/fetch/140666496/Prelabor%20Rupture%20of%20Membranes_ACOG%20Practice%20Bulletin,%20Number%20217.pdf">always offer abortions</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="39.0">Researchers in Dallas and Houston examined cases of previable pregnancy complications at their local hospitals after the state ban. Both studies found that when women weren’t able to end their pregnancies right away, they were <a href="https://www.ajog.org/article/S0002-9378(22)00536-1/abstract">significantly more likely to develop dangerous conditions</a> than before the ban. The study of the University of Texas Health Science Center in Houston, not yet published, found that the <a href="https://www.smfm.org/news/new-study-finds-increased-maternal-complications-with-previable-premature-rupture-of-membranes">rate of sepsis tripled</a> after the ban.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="40.0">Dr. Emily Fahl, a co-author of that study, recently urged professional societies and state medical boards to “explicitly clarify” that doctors need to recommend evacuating the uterus for patients with a PPROM diagnosis, even with no sign of infection, <a href="https://www.medpagetoday.com/meetingcoverage/smfm/114026">according to MedPage Today</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="41.0">UTHealth Houston did not respond to several requests for comment.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="42.0">ProPublica zoomed out beyond the second trimester to look at deaths of all women hospitalized in Texas while pregnant or up to six weeks postpartum. Deaths peaked amid the COVID-19 pandemic, and most patients who died then were diagnosed with the virus. But looking at the two years before the pandemic, 2018 and 2019, and the two most recent years of data, 2022 and 2023, there is a clear shift:</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="43.0">In the two earlier years, there were 79 maternal hospital deaths.</p>

<p data-pp-blocktype="copy" data-pp-id="43.1">In the two most recent, there were 120.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="44.0">Caitlin Myers, an economist at Middlebury College, said it’s crucial to examine these deaths from different angles, as ProPublica has done. Data analyses help illuminate trends but can’t reveal a patient’s history or wishes, as a detailed medical chart might. Diving deep into individual cases can reveal the timeline of treatment and how doctors behave. “When you see them together, it tells a really compelling story that people are dying as a result of the abortion restrictions.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="45.0">Texas has no plans to scrutinize those deaths. The chair of the maternal mortality review committee said the group is skipping data from 2022 and 2023 and picking up  its analysis with 2024 <a href="https://www.propublica.org/article/abortion-bans-deaths-state-maternal-mortality-committees">to get a more “contemporary” view of deaths</a>. She added that the decision had “absolutely no nefarious intent.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="46.0">“The fact that Texas is not reviewing those years does a disservice to the 120 individuals you identified who died inpatient and were pregnant,” said Dr. Jonas Swartz, an assistant professor of obstetrics and gynecology at Duke University. “And that is an underestimation of the number of people who died.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="47.0">The committee is also prohibited by law from reviewing cases that include an abortion medication or procedure, which can also be used during miscarriages. In response to ProPublica’s reporting, a Democratic state representative filed a bill to overturn that prohibition and order those cases to be examined.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="48.0">Because not all maternal deaths take place in hospitals and the Texas hospital data did not include cause of death, ProPublica also looked at data compiled from death certificates by the Centers for Disease Control and Prevention.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="49.0">It shows that the rate of maternal deaths in Texas rose 33% between 2019 and 2023 even as the national rate fell by 7.5%.</p>

<h3>A New Imperative</h3>
<p data-pp-blocktype="copy" data-pp-id="49.1">Texas’ abortion law is under review this legislative session. Even the party that championed it and the senator who authored it say they would consider a change.  </p>

<p data-pp-blocktype="copy" data-pp-id="49.2">On a local television program last month, Republican Lt. Gov. Dan Patrick <a href="https://www.texastribune.org/2025/01/20/dan-patrick-texas-abortion-law-mothers-risk/">said the law should be amended</a>. </p>

<p data-pp-blocktype="copy" data-pp-id="49.3">“I do think we need to clarify any language,” Patrick said, “so that doctors are not in fear of being penalized if they think the life of the mother is at risk.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="50.0">State Sen. Bryan Hughes, who <a href="https://www.documentcloud.org/documents/25345668-i-wrote-texas-abortion-law-its-clear-about-medical-emergencies/">once argued that the abortion ban he wrote was “plenty clear,”</a> has since <a href="https://www.statesman.com/story/news/politics/state/2025/01/20/lt-gov-dan-patrick-says-texas-should-amend-abortion-ban/77839131007/">reversed course</a>, saying he is working to propose language to amend the ban. Texas Gov. Greg Abbott told ProPublica, through a spokesperson, that he would “look forward to seeing any clarifying language in any proposed legislation from the Legislature.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="51.0">Patrick, Hughes and Attorney General Ken Paxton did not respond to ProPublica’s questions about what changes they would like to see made this session and did not comment on findings ProPublica shared.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="52.0">In response to ProPublica’s analysis, Abbott’s office said in a statement that Texas law is clear and pointed to Texas health department data that shows 135 abortions have been performed since Roe was overturned without resulting in prosecution. The vast majority of the abortions were categorized as responses to an emergency but the data did not specify what kind. Only five were solely to “preserve [the] health of [the] woman.”  </p>

<p data-pp-blocktype="copy" data-pp-id="52.1">At least seven bills related to repealing or creating new exceptions to the abortion laws have been introduced in Texas.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="53.0">Doctors told ProPublica they would most like to see the bans overturned so all patients could receive standard care, including the option to terminate pregnancies for health considerations, regardless of whether it’s an emergency. No list of exceptions can encompass every situation and risk a patient might face, obstetricians said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="54.0">“A list of exceptions is always going to exclude people,” said Dallas OB-GYN Dr. Allison Gilbert.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="55.0">It seems unlikely a Republican-controlled Legislature would overturn the ban. Gilbert and others are advocating to at least end criminal and civil penalties for doctors. Though no doctor has been prosecuted for violating the ban, the mere threat of criminal charges continues to obstruct care, she said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="56.0">In 2023, an amendment was passed that permitted physicians to intervene when patients are diagnosed with PPROM. But it is written in such a way that still exposes physicians to prosecution; it allows them to offer an “affirmative defense,” like arguing self-defense when charged with murder.</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="58.0">“Anything that can reduce those severe penalties that have really chilled physicians in Texas would be helpful,” Gilbert said. “I think it will mean that we save patients’ lives.” </p>

<p data-pp-blocktype="copy" data-pp-id="58.1">Rep. Mihaela Plesa, a Democrat from outside Dallas who filed a bill to create new health exceptions, said that ProPublica’s latest findings were “infuriating.” </p>

<p data-pp-blocktype="copy" data-pp-id="58.2">She is urging Republicans to bring the bills to a hearing for debate and discussion.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="59.0">Last session, there were no public hearings, even as women have sued the state after being denied treatment for their pregnancy complications. This year, though some Republicans appeared open to change, others have gone a different direction.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="60.0">One recently filed a bill that would allow the state to charge women who get an abortion with homicide, for which they could face the death penalty.</p>
        
    
                    <div data-pp-location="bottom-note">
                                    

                                    
        <div data-pp-location="bottom-note" data-pp-view="" data-pp-category="get involved">
                        <p>Do you live in a state that has passed laws affecting abortion in the last few years? In the time since, have you or a loved one experienced delayed health care while pregnant or experiencing a miscarriage?</p>
<p>ProPublica would like to hear from you to better understand the unintended impact of abortion bans across the country. Email our reporters at <a href="https://www.propublica.org/cdn-cgi/l/email-protection#dcaeb9acaeb3b8a9bfa8b5aab9b4b9bdb0a8b49cacaeb3aca9beb0b5bfbdf2b3aebb"><span data-cfemail="6e1c0b1e1c010a1b0d1a07180b060b0f021a062e1e1c011e1b0c02070d0f40011c09">[email&nbsp;protected]</span></a> to share your story.</p>
<p>We understand this may be difficult to talk about, and we have detailed <a href="https://www.propublica.org/article/help-propublica-report-on-maternal-health-abortion-bans">how we report on maternal health</a> to let you know what you can expect from us.</p>

        </div>

    


                                    
        <div data-pp-location="bottom-note">
                        <p><a href="https://www.propublica.org/people/lucas-waldron">Lucas Waldron</a> contributed graphics. <a href="https://www.propublica.org/people/mariam-elba">Mariam Elba</a> contributed research.</p>

        </div>

    
            </div><!-- end .article-body__bottom-notes -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RT64: N64 graphics renderer in emulators and native ports (102 pts)]]></title>
            <link>https://github.com/rt64/rt64</link>
            <guid>43114362</guid>
            <pubDate>Thu, 20 Feb 2025 13:26:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/rt64/rt64">https://github.com/rt64/rt64</a>, See on <a href="https://news.ycombinator.com/item?id=43114362">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">RT64</h2><a id="user-content-rt64" aria-label="Permalink: RT64" href="#rt64"></a></p>
<p dir="auto">RT64 is an N64 graphics renderer for playing games with enhancements in emulators and native ports.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Work in Progress</h2><a id="user-content-work-in-progress" aria-label="Permalink: Work in Progress" href="#work-in-progress"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><strong>Emulator Support (Plugin) and Ray Tracing (RT) are not available in this repository yet.</strong></h3><a id="user-content-emulator-support-plugin-and-ray-tracing-rt-are-not-available-in-this-repository-yet" aria-label="Permalink: Emulator Support (Plugin) and Ray Tracing (RT) are not available in this repository yet." href="#emulator-support-plugin-and-ray-tracing-rt-are-not-available-in-this-repository-yet"></a></p>
<p dir="auto">This repository has been made public to provide a working implementation to native ports that wish to use RT64 as their renderer.</p>
<p dir="auto"><strong>Development of these features is still ongoing and will be added to this repository when they're ready.</strong> Thank you for your patience!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features available</h2><a id="user-content-features-available" aria-label="Permalink: Features available" href="#features-available"></a></p>
<ul dir="auto">
<li>Modern N64 renderer built on the latest APIs (D3D12 and Vulkan).</li>
<li>Uses ubershaders to guarantee no stutters due to pipeline compilation.</li>
<li>Brand new architecture designed around offering novel enhancements.</li>
<li>High level of accuracy and no tolerance for game-specific workarounds to achieve correct rendering.</li>
<li>Input latency reduction options to either skip the game's native buffering or draw as early as possible.</li>
<li>Render with a higher resolution and downsample to a resolution closer to the original game.</li>
<li>Support for Widescreen with arbitrary aspect ratios, including Ultrawide support (limited game support).</li>
<li>Interpolate the game's visuals to 60 FPS or above (HFR) by generating new frames and modifying them in 3D space (limited game support).</li>
<li>Extended command set for better integration of widescreen, interpolation and path tracing features (for use with rom patches, rom hacks, and ports).</li>
<li>Texture packs with DDS and asynchronous streaming support. Compatible with Rice filenames. <a href="https://github.com/rt64/rt64/blob/main/TEXTURE-PACKS.md">Read about how to make texture packs for RT64 here</a>.</li>
<li>Supports Windows 10, Windows 11 and Linux.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features in development (in priority order)</h2><a id="user-content-features-in-development-in-priority-order" aria-label="Permalink: Features in development (in priority order)" href="#features-in-development-in-priority-order"></a></p>
<ul dir="auto">
<li>Game script interpreter.
<ul dir="auto">
<li>Support a runtime language for configuring the path traced renderer based on the contents of the game's memory.</li>
<li>Support patching the game's memory to provide various enhancements automatically integrated with the game script.</li>
</ul>
</li>
<li>Fully path traced renderer (RT).
<ul dir="auto">
<li>Calculate all lighting in real time and replace the contents of the drawn scene entirely with a path traced version.</li>
<li>Provide support for extra modifications for altering the material properties of the surfaces in the game.</li>
<li>Game support will be limited to a very small selection of games initially.</li>
</ul>
</li>
<li>Emulator integration.
<ul dir="auto">
<li>Game compatibility database and feature whitelist.</li>
<li>Configuration screen.</li>
<li>List of supported emulators to be determined.</li>
</ul>
</li>
<li>Model replacements.
<ul dir="auto">
<li>Details to be determined.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<ul dir="auto">
<li>CMake 3.20 or above.</li>
<li>C++17 compiler.
<ul dir="auto">
<li>Known to work with Microsoft Visual C++, Clang or GCC.</li>
</ul>
</li>
<li>Windows 10 or 11 SDK (Windows only).</li>
<li>Make sure to clone submodules correctly when checking out the repository.
<ul dir="auto">
<li>Use <code>git submodule update --init --recursive</code> if you cloned without using <code>--recursive</code>.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deferred frames</h2><a id="user-content-deferred-frames" aria-label="Permalink: Deferred frames" href="#deferred-frames"></a></p>
<p dir="auto">Draw calls are never sent right away to the GPU but instead stored on an auxiliary structure that logs the history of an entire frame. A frame's contents can be examined in great detail with the in-game debugger included with RT64. The renderer requires this to be able to optimize the rendering process, perform enhancements, replace assets and even generate new frames entirely.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deferred RDP</h2><a id="user-content-deferred-rdp" aria-label="Permalink: Deferred RDP" href="#deferred-rdp"></a></p>
<p dir="auto">Any operation related to reading memory and using it as texture information during rendering is completely deferred until the game requires a full synchronization of the RDP. All requests are stored in a list of operations and flushed before it is time to render. This allows RT64 to perform extra optimizations and detect when it is necessary to synchronize the output of the RDP back to memory. It also opens up the ability to detect when direct copies can be made on GPU memory and even detect patterns that can be replaced with equivalent but faster operations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deferred RSP (Compute)</h2><a id="user-content-deferred-rsp-compute" aria-label="Permalink: Deferred RSP (Compute)" href="#deferred-rsp-compute"></a></p>
<p dir="auto">All vertex transformations by the RSP (e.g. position, lighting, texturing, etc.) are performed by a highly parallel compute shader in the GPU. This saves a lot of CPU processing time and allows for much higher vertex and polygon counts. Since all transformations are deferred to this step, it's very easy for RT64 to patch transformations of the objects in the scene and the camera and produce a new frame very quickly. This effectively removes CPU bottlenecks that are critical for reaching very high target framerates.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Texture Decoder (Compute)</h2><a id="user-content-texture-decoder-compute" aria-label="Permalink: Texture Decoder (Compute)" href="#texture-decoder-compute"></a></p>
<p dir="auto">RT64 does not decode textures on the CPU and instead opts for uploading TMEM (4 KB) directly to the GPU. If possible, a RGBA32 version of the texture will be decoded and cached using a compute shader. If the sampling parameters prove to be too troublesome for that (e.g. giant texture masks with no clamp due to bad configuration), then RT64 can sample TMEM directly just like the console with a small performance sacrifice.</p>
<p dir="auto">RT64 features one of the most accurate TMEM loaders to date so far which has been directly reverse engineered by observing console behavior with the aid of homebrew test ROMs developed by <a href="https://github.com/Mr-Wiseguy">Wiseguy</a>. All the color conversion formulas for decoding have also been sourced from <a href="https://github.com/Thar0">Tharo</a>'s excellent RDP research.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dual renderers</h2><a id="user-content-dual-renderers" aria-label="Permalink: Dual renderers" href="#dual-renderers"></a></p>
<p dir="auto">RT64 takes advantage of the multi-threaded capabilities of modern APIs to run two renderers at the same time with highly different goals. One renderer draws at native resolution (e.g. 240p) and synchronizes back immediately with the game running at its original rate. The other renderer replays all the draw calls detected by the main renderer at higher resolution and even at a different rate when using interpolation. This design guarantees that the game will see the correct data in RAM, which is very important for games that read from these memory regions for gameplay reasons. If it's not required by the game, this native resolution renderer can be turned off completely to save performance.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Framebuffer detection</h2><a id="user-content-framebuffer-detection" aria-label="Permalink: Framebuffer detection" href="#framebuffer-detection"></a></p>
<p dir="auto">RT64 will keep track of any memory addresses used as framebuffers, their dimensions and their matching contents that live in GPU memory. If other operations in the frame load and sample a part of memory owned by a framebuffer, then a direct copy of one of its regions is performed and stored in memory.</p>
<p dir="auto">The framebuffer copy mechanic is only used when the load and sample parameters line up correctly and the final result would look no different than if it was sampled from RAM (except for preserving the high precision of the image). As framebuffer detection works at the RDP level, that means all related performance and visual enhancements can apply even in LLE mode.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Framebuffer upscaling</h2><a id="user-content-framebuffer-upscaling" aria-label="Permalink: Framebuffer upscaling" href="#framebuffer-upscaling"></a></p>
<p dir="auto">When framebuffer operations are detected, the renderer is able to upscale these effects by increasing the resolution of all framebuffers and any associated region copies by a scaling factor. The resolution increase is only performed in the high resolution renderer. All texture coordinates are adjusted automatically to account for this change.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Framebuffer reinterpretation</h2><a id="user-content-framebuffer-reinterpretation" aria-label="Permalink: Framebuffer reinterpretation" href="#framebuffer-reinterpretation"></a></p>
<p dir="auto">Games will sometimes draw to a framebuffer using one format and then read it as another format to perform post-processing on the screen. RT64 supports this operation natively at high resolution using per-pixel reinterpretation inside a dedicated compute shader. While this requires adding specific paths encountered on a case-by-case basis (to preserve high precision), the framework can be easily extended to support as many cases as possible. This feature is essential for emulating some of the more infamous effects in higher resolutions such as greyscale filters.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Draw call matching</h2><a id="user-content-draw-call-matching" aria-label="Permalink: Draw call matching" href="#draw-call-matching"></a></p>
<p dir="auto">Once all information for a particular game frame has been recorded, RT64 will attempt to perform automatic matching of draw calls between frames based on their rendering parameters, the textures in use, their position, orientation, their tracked velocities and much more. Matching is essential to generating interpolated frames and providing motion vector information to the path traced renderer. Draw call matching is still a highly experimental area that is prone to errors and requires a lot of processing to get right. The algorithms behind this feature will be likely to change as more research is done to accommodate as many games as possible.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Frame interpolation</h2><a id="user-content-frame-interpolation" aria-label="Permalink: Frame interpolation" href="#frame-interpolation"></a></p>
<p dir="auto">New frames are generated using the information from the previous draw call matching step. For any calls that match, new 3D transformations will be created via interpolation and uploaded to the GPU for the deferred RSP to do its work again. When possible, transformations are decomposed into their constituent components. Those components are interpolated separately and recomposed into the resultant transform to prevent interpolation artifacts caused by naive transformation interpolation. This interpolation can also be applied to texture scrolling coordinates and even per vertex if vertex velocities are computed. More parameters are planned to be supported in the future as long as they're part of the recorded frame data.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extended command set</h2><a id="user-content-extended-command-set" aria-label="Permalink: Extended command set" href="#extended-command-set"></a></p>
<p dir="auto">RT64 offers an extended command set that can be used by native ports, ROM patches, and ROM hacks to provide extra information that the renderer can use to enhance the final result. Using this feature is vital to providing accurate frame interpolation and widescreen enhancements for 2D elements. If you're interested in using this feature, check out the <a href="https://github.com/rt64/rt64/blob/main/include/rt64_extended_gbi.h">included header in the repository</a>. The main activation method is encoded into a NOOP command so it should be safe to run it in real hardware. RT64 will recognize this command and enable the extended command set feature, allowing for an extended repertoire of functions aimed at fixing some common grievances that show up at high framerates or wide aspect ratios.</p>
<p dir="auto">For example, objects that rely on pixel depth checks to draw can instead use the <code>gEXVertexZTest()</code> command to specify any triangles drawn after the command should only be drawn if the vertex is not covered by something else. This means that effects such as light halos can be replaced with this command and it'll automatically work on widescreen ratios and no longer be delayed by one frame. Instead, RT64 will just flush the depth buffer at the correct step and run a small compute shader that will invalidate the draw call's triangles if the depth test doesn't pass. Though the original effect already works as-is thanks to the native renderer, the extended GBI method can be leveraged to make the experience feel even better.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future texture and asset replacement</h2><a id="user-content-future-texture-and-asset-replacement" aria-label="Permalink: Future texture and asset replacement" href="#future-texture-and-asset-replacement"></a></p>
<p dir="auto">Texture replacements are easily performed at runtime by using two capabilities that RT64 already supports: bindless textures and texture coordinate scaling. The entire renderer uses only one contiguous array of texture resources and replacing one of these by an HD texture replacement is straightforward. By detecting the difference in sizes between the original texture and the replacement, the texture coordinates can be scaled easily with the same logic that is used for framebuffer tiles.</p>
<p dir="auto">Asset replacement, while not currently implemented and more of a long term feature, will leverage the fact that only one contiguous vertex and index array is used by RT64. This allows for highly efficient rendering as it minimizes the amount of times vertex and index buffers must be bound, but it also means the RSP can process the vertices placed in this buffer during rendering time as if they came from the game itself. Therefore, the entire process of model replacement will just consist of allocating chunks of the buffer directly for replacements in a native format, assigning it the correct transforms used by the draw call and letting the RSP compute shader do its job. This will allow the renderer to draw models that have orders of magnitude higher triangle counts with very little additional cost to the CPU.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future path tracing</h2><a id="user-content-future-path-tracing" aria-label="Permalink: Future path tracing" href="#future-path-tracing"></a></p>
<p dir="auto">RT64 will attempt to perform scene detection by merging as many draw calls as it can as long as they're inside of the same compatible perspective view point. Since all data remains untransformed, bottom-level acceleration structures (BLAS) can be built by using a compute shader to transform vertices to world space instead of screen space. Once these BLAS are built, a top-level acceleration structure (TLAS) is constructed with all the BLAS that were detected.</p>
<p dir="auto">These structures are used directly for path tracing the scene and generating all lighting in real time. Lighting information is derived from various sources (the game itself, scripts, integrated light editor) and will be detailed more in the future as this feature is closer to a final state.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When your last name is Null, nothing works (119 pts)]]></title>
            <link>https://www.wsj.com/lifestyle/null-last-name-computer-scientists-forms-f0a43b08</link>
            <guid>43113997</guid>
            <pubDate>Thu, 20 Feb 2025 12:39:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/lifestyle/null-last-name-computer-scientists-forms-f0a43b08">https://www.wsj.com/lifestyle/null-last-name-computer-scientists-forms-f0a43b08</a>, See on <a href="https://news.ycombinator.com/item?id=43113997">Hacker News</a></p>
Couldn't get https://www.wsj.com/lifestyle/null-last-name-computer-scientists-forms-f0a43b08: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Customizable HTML Select (148 pts)]]></title>
            <link>https://developer.chrome.com/blog/rfc-customizable-select</link>
            <guid>43113790</guid>
            <pubDate>Thu, 20 Feb 2025 12:13:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.chrome.com/blog/rfc-customizable-select">https://developer.chrome.com/blog/rfc-customizable-select</a>, See on <a href="https://news.ycombinator.com/item?id=43113790">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
    




<div translate="no">
        
          <p><img alt="Una Kravets" src="https://web.dev/images/authors/unakravets.jpg" decoding="async" height="64" loading="lazy" width="64"></p>
      </div>

<p>Styling form controls like the <code translate="no" dir="ltr">&lt;select&gt;</code> element has been reported as a top developer pain point for years, and we've been working on a solution. While this work is complex and has taken a long time to get right, we're getting very close to landing this feature. A customizable version of the select element is officially in <a href="https://whatwg.org/stages">Stage 2</a> in the <a href="https://github.com/whatwg/html/issues/9799">WHATWG</a>, with strong cross-browser interest and a prototype for you to test out from Chrome Canary 130.</p>

<h2 id="try_it_out_and_give_us_your_feedback" data-text="Try it out and give us your feedback" tabindex="-1">Try it out and give us your feedback</h2>

<p>Check that your install of <a href="https://www.google.com/chrome/canary/">Chrome Canary</a> is updated to version 130, and that you have the experimental web platform features flag on. You can turn this flag on by going to chrome://flags in your address bar and turning on <em>#experimental-web-platform-features</em>. Then, you should be able to see the Codepen demos in this post. Alternatively, you can check out this <a href="https://codepen.io/collection/BNZjPe">Codepen collection</a> to view them all in one place.</p>

<p>Use <a href="https://forms.gle/h5i83K85YfmBvtgp9">this form</a> to provide feedback on the feature. It will only take three minutes to complete!</p>

<p>Let's dive into the features of the customizable select API, which builds on the existing HTML select tag.</p>

<h2 id="opting-in_to_the_new_select" data-text="Opting-in to the new <select>" tabindex="-1">Opting-in to the new <code translate="no" dir="ltr">&lt;select&gt;</code></h2>

<p>To opt-in to the new behavior use the CSS <code translate="no" dir="ltr">appearance</code> property on both the in-page select button and also on the select picker. To opt-in, set <code translate="no" dir="ltr">appearance: base-select</code> on your <code translate="no" dir="ltr">&lt;select&gt;</code> element and on the <code translate="no" dir="ltr">::picker(select)</code>.</p>

<p><code translate="no" dir="ltr">::picker(select)</code> is a new user-agent provided pseudo-element that only applies to <code translate="no" dir="ltr">&lt;select&gt;</code> elements which have been opted into the new behavior using <code translate="no" dir="ltr">appearance: base-select</code>. This picker pseudo-element is the popover that is triggered by the base select button. You can opt-in both as shown in the following code:</p>
<devsite-code><pre translate="no" dir="ltr" is-upgraded="" syntax="CSS"><code translate="no" dir="ltr"><span>select</span><span>,</span>
<span>::</span><span>picker</span><span>(</span><span>select</span><span>)</span><span> </span><span>{</span>
<span>  </span><span>appearance</span><span>:</span><span> </span><span>base-select</span><span>;</span>
<span>}</span>
</code></pre></devsite-code>
<p>You can choose to only opt-in the in-page button, but you can't opt-in only the picker popover without opting in the in-page button. <code translate="no" dir="ltr">::picker(select)</code> is only created once <code translate="no" dir="ltr">appearance: base-select</code> is applied to the <code translate="no" dir="ltr">&lt;select&gt;</code>.</p>

<p>Now you're ready to customize your select element. The new customizable select comes with some default styles that look the same across browsers and operating systems. Here's what the default customized select looks like against the existing select in Chrome on macOS:</p>

<figure>
  <img src="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on.png" alt="" srcset="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_36.png 36w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_48.png 48w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_72.png 72w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_96.png 96w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_480.png 480w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_720.png 720w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_856.png 856w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_960.png 960w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_1440.png 1440w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_1920.png 1920w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-on_2880.png 2880w" sizes="(max-width: 840px) 100vw, 856px">
  <figcaption>Default user agent style for customizable select on the right. This is subject to change, and something we'd love your feedback on.</figcaption>
</figure>

<figure>
  
  <figcaption>Demo of a basic select versus customizable select.</figcaption>
</figure>

<h2 id="breaking_down_the_parts" data-text="Breaking down the parts" tabindex="-1">Breaking down the parts</h2>

<figure>
  <img src="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts.png" alt="" srcset="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_36.png 36w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_48.png 48w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_72.png 72w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_96.png 96w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_480.png 480w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_720.png 720w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_856.png 856w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_960.png 960w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_1440.png 1440w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_1920.png 1920w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-parts_2880.png 2880w" sizes="(max-width: 840px) 100vw, 856px">
  <figcaption>Diagram showing parts of a select.</figcaption>
</figure>

<p>Once you're in the new customizable select mode, the new elements you now have access to include:
  - <code translate="no" dir="ltr">selectedoption</code>: reflects the inner HTML of the option which is currently selected.
  - <code translate="no" dir="ltr">option::before</code>: contains a checkmark to indicate the currently selected option as a default accessibility affordance (this is subject to change).
  - <code translate="no" dir="ltr">::picker(select)</code>: popover that contains all of the content outside of the <code translate="no" dir="ltr">button</code> inside of a customizable select.</p>

<p>You can style any part of the select. For example, you can add arbitrary non-interactive content within the <code translate="no" dir="ltr">&lt;option&gt;</code> elements, style the in-page button which opens the select drop-down, and style the drop-down list of options (the <code translate="no" dir="ltr">::picker(select)</code>).</p>

<p>You can also style the <code translate="no" dir="ltr">button</code>, bring-your-own arrow indicator, and add arbitrary content within and surrounding any of the elements. In addition to adding content, you can hide any of these new elements and default styles. For example, if you don't want an indicator checkmark in the ::before pseudo element of the option, use the following CSS.</p>
<devsite-code><pre translate="no" dir="ltr" is-upgraded="" syntax="CSS"><code translate="no" dir="ltr"><span>/* Remove the default checkmark from the selected option */</span>
<span>option</span><span>::</span><span>before</span><span> </span><span>{</span>
<span> </span><span>display</span><span>:</span><span> </span><span>none</span><span>;</span>
<span>}</span>
</code></pre></devsite-code>
<p>While there can be an unlimited number of elements inside of your select, the browser will bucket anything outside of a <code translate="no" dir="ltr">&lt;button&gt;</code> element into the <code translate="no" dir="ltr">::picker(select)</code> pseudo-element, which behaves as a popover anchored to the button. This <code translate="no" dir="ltr">&lt;button&gt;</code> toggles the <code translate="no" dir="ltr">::picker(select)</code>. Options and other elements directly inside of the select will be hoisted into the <code translate="no" dir="ltr">::picker(select)</code>, or you can bring your own wrapper for styling purposes. This wrapper, too, will be placed inside of the <code translate="no" dir="ltr">::picker(select)</code> pseudo-element.</p>
<devsite-code><pre translate="no" dir="ltr" is-upgraded="" syntax="HTML"><code translate="no" dir="ltr">&lt;select&gt;
  &lt;button&gt;
    &lt;selectedoption&gt;&lt;/selectedoption&gt;
  &lt;/button&gt;
  // Everything else that will go into the ::picker(select) popover
&lt;/select&gt;
</code></pre></devsite-code>
<p>The new customizable <code translate="no" dir="ltr">&lt;select&gt;</code> uses functionality from <a href="https://developer.chrome.com/blog/introducing-popover-api">popover</a> and <a href="https://developer.chrome.com/blog/anchor-positioning-api">anchor positioning</a>. It's built with these two underlying technologies. This means that the drop-down option list within a select acts as a popover which is anchored to the trigger button that opens the select.</p>

<p>You can use anchor positioning to style this <code translate="no" dir="ltr">::picker(select)</code> popover (including making it anchored to other elements). This content model also means that <a href="https://developer.chrome.com/blog/entry-exit-animations">top layer animation</a> styles work with the option list to animate entry and exit effects.</p>

<h2 id="enhance_the_existing_select_element" data-text="Enhance the existing <select> element" tabindex="-1">Enhance the existing <code translate="no" dir="ltr">&lt;select&gt;</code> element</h2>

<p>Previously, the Chrome team was working on the idea of a <code translate="no" dir="ltr">&lt;selectlist&gt;</code> element. What's described in this post is that feature redesigned to reuse the existing <code translate="no" dir="ltr">&lt;select&gt;</code> element instead.</p>

<p>One of the key benefits of reusing the existing <code translate="no" dir="ltr">&lt;select&gt;</code> element is the ability to progressively enhance the basic HTML element. In comparison to a brand new element, reusing <code translate="no" dir="ltr">&lt;select&gt;</code> will still render meaningful content on your page. The following example shows the customized select versus what a user in a non-supported browser would see:</p>

<figure>
  <img src="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback.png" alt="" srcset="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_36.png 36w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_48.png 48w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_72.png 72w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_96.png 96w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_480.png 480w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_720.png 720w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_856.png 856w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_960.png 960w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_1440.png 1440w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_1920.png 1920w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-fallback_2880.png 2880w" sizes="(max-width: 840px) 100vw, 856px">
  <figcaption>All of the text content within the <code translate="no" dir="ltr">option</code> is rendered in the fallback version of the select element.</figcaption>
</figure>

<h3 id="basic_styling" data-text="Basic styling" tabindex="-1">Basic styling</h3>

<p>Changes may be as simple as visual styling of the select element. For example, to update the button styles, hover and focus styles, or the background of the select options. After opting-in with <code translate="no" dir="ltr">appearance: base-select</code>, apply any CSS you want to the parts of your select.</p>

<figure>
  <img src="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic.png" alt="" srcset="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_36.png 36w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_48.png 48w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_72.png 72w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_96.png 96w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_480.png 480w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_720.png 720w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_856.png 856w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_960.png 960w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_1440.png 1440w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_1920.png 1920w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-basic_2880.png 2880w" sizes="(max-width: 840px) 100vw, 856px">
  <figcaption>Changing the styles of various parts of the select, with the default button.</figcaption>
</figure>



<p>To customize the arrow indicator, add your own button and arrow inside of the select.</p>
<devsite-code><pre translate="no" dir="ltr" is-upgraded="" syntax="HTML"><code translate="no" dir="ltr">&lt;select&gt;
  &lt;button&gt;
    &lt;selectedoption&gt;&lt;/selectedoption&gt;
    &lt;span&gt;
      // Arrow here
    &lt;/span&gt;
  &lt;/button&gt;
  // Everything else that will go into the ::picker(select) popover
&lt;/select&gt;
</code></pre></devsite-code>
<p>Then, style the arrow:</p>
<devsite-code><pre translate="no" dir="ltr" is-upgraded="" syntax="CSS"><code translate="no" dir="ltr"><span>/* style the arrow */</span>
<span>button</span><span> </span><span>span</span><span> </span><span>{</span>
<span>  </span><span>/* arrow styles */</span>
<span>  </span><span>transition</span><span>:</span><span> </span><span>rotate</span><span> </span><span>0.2</span><span>s</span><span>;</span>
<span>}</span>

<span>/* adjust arrow styles when the picker is open */</span>
<span>select</span><span>:</span><span>open</span><span> </span><span>button</span><span> </span><span>span</span><span> </span><span>{</span>
<span>  </span><span>rotate</span><span>:</span><span> </span><span>-180</span><span>deg</span><span>;</span>
<span>}</span>
</code></pre></devsite-code>


<h3 id="complex_content_within_options" data-text="Complex content within options" tabindex="-1">Complex content within options</h3>

<p>Take things further with the ability to add and style content beyond strings within the <code translate="no" dir="ltr">&lt;option&gt;</code> elements inside <code translate="no" dir="ltr">&lt;select&gt;</code>. A basic example is adding flag images next to country names in a drop-down menu. To achieve this, add an image element next to the option text.</p>
<devsite-code><pre translate="no" dir="ltr" is-upgraded="" syntax="HTML"><code translate="no" dir="ltr">&lt;option value="france"&gt;
  &lt;img src="img/flag_of_france.svg" alt="" /&gt;
  &lt;span&gt;France&lt;/span&gt;
&lt;/option&gt;
</code></pre></devsite-code>
<figure>
  <img src="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country.png" alt="" srcset="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_36.png 36w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_48.png 48w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_72.png 72w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_96.png 96w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_480.png 480w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_720.png 720w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_856.png 856w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_960.png 960w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_1440.png 1440w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_1920.png 1920w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-country_2880.png 2880w" sizes="(max-width: 840px) 100vw, 856px">
  <figcaption>Country picker with flags.</figcaption>
</figure>



<p>A more complex example could include profile photos, names, and alternative information to help you make decisions about which item to select in the drop-down.</p>
<devsite-code><pre translate="no" dir="ltr" is-upgraded="" syntax="HTML"><code translate="no" dir="ltr">&lt;option value="eur"&gt;
    &lt;img src="euro-flag.png" alt="" /&gt;
    &lt;div class="currency"&gt;
      &lt;div class="currency-short"&gt;EUR&lt;/div&gt;
      &lt;div class="currency-long"&gt;Euro&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class="symbol" aria-hidden="true"&gt;€&lt;/div&gt;
&lt;/option&gt;
</code></pre></devsite-code>
<figure>
  <img src="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency.png" alt="" srcset="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_36.png 36w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_48.png 48w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_72.png 72w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_96.png 96w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_480.png 480w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_720.png 720w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_856.png 856w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_960.png 960w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_1440.png 1440w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_1920.png 1920w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-currency_2880.png 2880w" sizes="(max-width: 840px) 100vw, 856px">
  <figcaption>Screenshot of currency picker.</figcaption>
</figure>



<h3 id="style_the_selected_option" data-text="Style the selected option" tabindex="-1">Style the selected option</h3>

<p>You might want the selected option to be displayed differently in the selected state than it is in the drop-down. An example of this is the gmail UI, where, to save space, the label is removed once the option is selected. Do this by hooking into the <code translate="no" dir="ltr">&lt;selectedoption&gt;</code> element for styling. The <code translate="no" dir="ltr">&lt;option&gt;</code> contains all of the following markup:</p>
<devsite-code><pre translate="no" dir="ltr" is-upgraded="" syntax="HTML"><code translate="no" dir="ltr"> &lt;option value="reply-all"&gt;
    &lt;img class="material-symbol"  src="material-symbol-reply.png"&gt;
    &lt;span class="text"&gt;Reply all&lt;/span&gt;
  &lt;/option&gt;
</code></pre></devsite-code>
<p>Now apply <code translate="no" dir="ltr">display: none</code> on <code translate="no" dir="ltr">.text</code> inside <code translate="no" dir="ltr">&lt;selectedoption&gt;</code>' to hide the text content and only show the icon:</p>
<devsite-code><pre translate="no" dir="ltr" is-upgraded="" syntax="CSS"><code translate="no" dir="ltr"><span>selectedoption</span><span> </span><span>.</span><span>text</span><span> </span><span>{</span>
<span>  </span><span>display</span><span>:</span><span> </span><span>none</span><span>;</span>
<span>}</span>
</code></pre></devsite-code>
<figure>
  <img src="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail.png" alt="" srcset="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_36.png 36w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_48.png 48w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_72.png 72w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_96.png 96w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_480.png 480w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_720.png 720w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_856.png 856w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_960.png 960w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_1440.png 1440w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_1920.png 1920w,https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail_2880.png 2880w" sizes="(max-width: 840px) 100vw, 856px">
  <figcaption>Gmail-style select with an icon representing the selected option.</figcaption>
</figure>



<h3 id="interactive_options" data-text="Interactive options" tabindex="-1">Interactive options</h3>

<p>With full control over the styling of the <code translate="no" dir="ltr">::picker(select)</code>, build on the previous demo to make it interactive on hover and focus. In this demo, the new <a href="https://chromestatus.com/feature/5196713071738880">calc-size()</a> function is used to animate the picker's width from showing the icons to showing the full width of the options on hover or if the select has an option with focus-visible.</p>
<devsite-code><pre translate="no" dir="ltr" is-upgraded="" syntax="CSS"><code translate="no" dir="ltr"><span>/* base styles when picker is open but not interacted with */</span>
<span>::</span><span>picker</span><span>(</span><span>select</span><span>)</span><span> </span><span>{</span>
<span>  </span><span>width</span><span>:</span><span> </span><span>var</span><span>(</span><span>--icon-width</span><span>);</span>
<span>  </span><span>transition</span><span>:</span><span> </span><span>width</span><span> </span><span>0.5</span><span>s</span><span>;</span>
<span>}</span>

<span>/* animate the text in on hover &amp; focus */</span>
<span>::</span><span>picker</span><span>(</span><span>select</span><span>)</span><span>:</span><span>hover</span><span>,</span>
<span>select</span><span>:</span><span>has</span><span>(</span><span>option</span><span>:</span><span>focus-visible</span><span>)</span><span>::</span><span>picker</span><span>(</span><span>select</span><span>)</span><span> </span><span>{</span>
<span>  </span><span>/*  auto width!  */</span>
<span>  </span><span>width</span><span>:</span><span> </span><span>calc-size</span><span>(</span><span>auto</span><span>,</span><span> </span><span>size</span><span> </span><span>+</span><span> </span><span>0.5</span><span>rem</span><span>);</span>
<span>}</span>
</code></pre></devsite-code>
<figure>
  <video width="738" height="544" controls="" muted="" playsinline="" autoplay="" loop="">
    <source src="https://developer.chrome.com/static/blog/rfc-customizable-select/image/select-gmail-interactive.mp4" type="video/mp4">
  </video>
  <figcaption>Interactive Gmail-style select with progressively shown content on hover or focus.</figcaption>
</figure>



<h2 id="limitations_and_accessibility_notes" data-text="Limitations and accessibility notes" tabindex="-1">Limitations and accessibility notes</h2>

<p>With great power comes great responsibility. To keep things accessible there are some limitations to the feature.</p>

<ul>
<li>Other than <code translate="no" dir="ltr">&lt;option&gt;</code> elements, no interactive (focusable) elements are yet allowed inside of the <code translate="no" dir="ltr">&lt;select&gt;</code>, such as buttons or other elements. For now, the proposed content model only allows for <code translate="no" dir="ltr">&lt;div&gt;</code>, <code translate="no" dir="ltr">&lt;span&gt;</code>, <code translate="no" dir="ltr">&lt;option&gt;</code>, <code translate="no" dir="ltr">&lt;optgroup&gt;</code>, <code translate="no" dir="ltr">&lt;img&gt;</code>, <code translate="no" dir="ltr">&lt;svg&gt;</code>, and <code translate="no" dir="ltr">&lt;hr&gt;</code> elements.</li>
<li><a href="https://codepen.io/web-dot-dev/pen/ExBrdOa">Split buttons</a> are currently in the experimentation phase as we work out an accessible solution.</li>
</ul>

<p>In the future, the content model is expected to expand to be more flexible, as the accessibility story for these experiences is fleshed out.</p>

<h2 id="conclusion" data-text="Conclusion" tabindex="-1">Conclusion</h2>

<p>We're excited to see this feature progress through working groups and standards bodies, and share our progress as we actively build the prototype and evaluate the shape of this feature. If you encounter something that doesn't work as you expect, let us know!</p>

<p>And while this feature is still in development, we'd love to hear your feedback through this <a href="https://forms.gle/h5i83K85YfmBvtgp9">short feedback form</a>.</p>

<p>Thank you for being a part of making sure we get this right and make it easier to build accessible, customizable form controls on the web!</p>

  

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Amazon Appstore for Android devices will be discontinued on August 20, 2025 (203 pts)]]></title>
            <link>https://www.amazon.com/gp/mas/appstore/android/faq</link>
            <guid>43113397</guid>
            <pubDate>Thu, 20 Feb 2025 11:13:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amazon.com/gp/mas/appstore/android/faq">https://www.amazon.com/gp/mas/appstore/android/faq</a>, See on <a href="https://news.ycombinator.com/item?id=43113397">Hacker News</a></p>
Couldn't get https://www.amazon.com/gp/mas/appstore/android/faq: Error: Request failed with status code 404]]></description>
        </item>
        <item>
            <title><![CDATA[After 20 Years, Math Couple Solves Major Group Theory Problem (384 pts)]]></title>
            <link>https://www.quantamagazine.org/after-20-years-math-couple-solves-major-group-theory-problem-20250219/</link>
            <guid>43113024</guid>
            <pubDate>Thu, 20 Feb 2025 10:09:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/after-20-years-math-couple-solves-major-group-theory-problem-20250219/">https://www.quantamagazine.org/after-20-years-math-couple-solves-major-group-theory-problem-20250219/</a>, See on <a href="https://news.ycombinator.com/item?id=43113024">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h2>Introduction</h2>
            <div data-role="selectable">
    <p>In 2003, a German graduate student named <a href="https://www.darstellungstheorie.uni-wuppertal.de/de/prof-dr-britta-spaeth/">Britta Späth</a> encountered the McKay conjecture, one of the biggest open problems in the mathematical realm known as group theory. At first her goals were relatively modest: She hoped to prove a theorem or two that would make incremental progress on the problem, as many other mathematicians had done before her. But over the years, she was drawn back to it, again and again. Whenever she tried to focus on something else, she said, “it didn’t connect.”</p>
<p>There was a risk that such a single-minded pursuit of so difficult a problem could hurt her academic career, but Späth dedicated all her time to it anyway. It brought her to the office of <a href="http://perso.imj-prg.fr/marc-cabanes/">Marc Cabanes</a>, a mathematician now at the Institute of Mathematics of Jussieu in Paris who, inspired by her efforts, became consumed by the conjecture, too. While working together, the pair fell in love and eventually started a family.</p>
<p>The problem that absorbed them takes a key theme in mathematics and turns it into a concrete tool for group theorists. Math is full of enormously complicated abstract objects that are impossible to study in their entirety. But often, mathematicians have discovered, it’s enough to look at a small fragment of such an object to understand its broader properties. In the third century BCE, for instance, the ancient Greek mathematician Eratosthenes estimated the circumference of the Earth — roughly 25,000 miles — by measuring shadows cast by the sun in just two cities about 500 miles apart. Similarly, when mathematicians want to understand an impossibly convoluted function, they might only need to look at how it behaves for a small subset of possible inputs. That can be enough to tell them what the function does for all possible inputs.</p>
<p>The McKay conjecture is another example of this principle. It says that if you want to formulate a thorough description of a group — <a href="https://www.quantamagazine.org/groups-underpin-modern-math-heres-how-they-work-20240906/">an important mathematical entity</a> that can get prohibitively difficult to study — you only need to look at a tiny piece of it.</p>
</div>
    </div><div data-role="selectable">
    <p>After the conjecture was posed in the 1970s, dozens of mathematicians tried their hand at proving it. They made partial progress — and in the process they learned a great deal about groups, which are abstract objects that describe the various symmetries of a mathematical system. But a full proof seemed out of reach.</p>
<p>Then Späth came along. Now, 20 years after she first learned about the problem and more than a decade after she met Cabanes, the two mathematicians have <a href="https://arxiv.org/abs/2410.20392">finally completed the proof</a>.</p>
<p>When the couple announced their result, their colleagues were in awe. “I wanted there to be parades,” said <a href="https://statistics.stanford.edu/people/persi-diaconis">Persi Diaconis</a> of Stanford University. “Years of hard, hard, hard work, and she did it, they did it.”</p>
<h2><strong>The Power of Primes</strong></h2>
<p>The McKay conjecture began with the observation of a strange coincidence.</p>
<p>John McKay — <a href="https://arxiv.org/abs/2305.00850">described by one friend</a> as “brilliant, soft-spoken, and charmingly disorganized” — was known for his ability to spot numerical patterns in unexpected places. The Concordia University mathematician is perhaps most famous for his “monstrous moonshine” conjecture, which was proved in 1992 and established a deep connection between <a href="https://www.quantamagazine.org/mathematicians-chase-moonshine-string-theory-connections-20150312/">the so-called monster group</a> and a special function from number theory.</p>
<p>Before his death a few years ago, McKay unearthed lots of other important connections, too, many involving groups. A group is a set of elements combined with a rule for how those elements relate to one another. It can be thought of as a collection of symmetries — transformations that leave a shape, a function or some other mathematical object unchanged in specific ways. For all their abstraction, groups are immensely useful, and they play a central role in mathematics.</p>

<p>In 1972, McKay was <a href="https://www.sciencedirect.com/science/article/pii/002186937290066X?">focused on finite groups</a> — groups that have a finite number of elements. He observed that in many cases, you can deduce important information about a finite group by looking at a very small subset of its elements. In particular, McKay looked at elements that form a special, smaller group — called a Sylow normalizer — inside the original group.</p>
<p>Imagine you have a group with 72 elements. This alone doesn’t tell you much: There are <a href="https://people.maths.bris.ac.uk/~matyd/GroupNames/index120.html">50 different groups</a> of that size. But 72 can also be written as a product of prime numbers, 2 $latex \times$ 2 $latex \times$ 2 $latex \times$ 3 $latex \times$ 3 — that is, as 2<sup>3</sup> $latex \times$ 3<sup>2</sup>. (Generally, the more distinct primes you need to describe the size of your group, the more complicated your group is.) You can decompose your group into smaller subgroups based on these primes. In this case, for instance, you could look at subgroups with eight (2<sup>3</sup>) elements and subgroups with nine (3<sup>2</sup>) elements. By studying those subgroups, you can learn more about the structure of your overall group — what other building blocks the group is composed of, for instance.</p>
<p>Now take one of those subgroups and add a few particular elements to it to create a special subgroup, the Sylow normalizer. In your 72-element group, you can build a different Sylow normalizer for each eight-element and nine-element subgroup — these are the 2-Sylow normalizers and 3-Sylow normalizers, respectively.</p>
<p>Sylow normalizers, like the subgroups they’re built out of, can tell mathematicians a lot about the original group. But McKay hypothesized that this connection was far stronger than anyone had imagined. It wasn’t just that a Sylow normalizer could give insights into a finite group’s overall structure. He asserted that if mathematicians wanted to compute a crucial quantity that would help them characterize their group, they’d just have to look at one of a particular set of Sylow normalizers: The Sylow normalizer would be characterized by the exact same number.</p>
<p>This quantity counts the number of “representations” of a certain type — ways you can rewrite elements of the group using arrays of numbers called matrices. Such a tally might seem arbitrary, but it gives mathematicians a sense of how the group’s elements relate to each other, and it is involved in calculations of other important properties.</p>

<p>There seemed to be no good reason why McKay’s quantity should always be the same for a finite group and its Sylow normalizers. A Sylow normalizer might contain just a fraction of a fraction of a percent of the number of elements in the larger group. Moreover, the Sylow normalizer often has a very different structure.</p>
<p>It would be as if “in every U.S. election, you count the votes in general, and in this little town in Montana, they are exactly the same proportionally,” said <a href="https://www.uv.es/gabriel/">Gabriel Navarro</a> of the University of Valencia. “Not similar, not more or less. Exactly the same.”</p>
<p>But that’s what McKay conjectured — for all finite groups. If true, it would make mathematicians’ lives much easier: Sylow normalizers are much easier to work with than their parent groups. It would also hint at the presence of a deeper mathematical truth, one that mathematicians don’t yet have a handle on.</p>
<p>A year after McKay first observed the coincidence, a mathematician named Marty Isaacs proved that it held for <a href="https://www.jstor.org/stable/2373731">a large class of groups</a>. But then mathematicians got stuck. They were able to show that it held up for one specific group or another, but there were still infinitely many groups left to tackle.</p>
<p>Proving the full conjecture seemed prohibitively difficult. As it turned out, the next major advance on the problem would require the completion of one of the most herculean mathematical projects in history.</p>
<h2><strong>One Giant Leap for Group Theory, One Small Step for McKay</strong></h2>
<p>The project — an effort to classify all the building blocks of finite groups — ultimately required thousands of proofs and took more than 100 years to complete. But in 2004, mathematicians finally succeeded in showing that all the building blocks must fall into one of three categories, or else belong to a list of 26 outliers.</p>

<p>Mathematicians had long suspected that, once complete, this classification would help simplify problems such as the McKay conjecture. Maybe they didn’t have to prove the conjecture for all finite groups. Maybe they only had to prove an alternative statement covering the 29 types of building blocks — or for some related set of straightforward groups — that would automatically imply the full McKay conjecture.</p>
<p>But first, someone had to show that this strategy would actually work. The very year that the classification was officially completed, Isaacs, Navarro and Gunter Malle <a href="https://link.springer.com/article/10.1007/s00222-007-0057-y">figured out</a> the right way to reframe the McKay conjecture so that they only had to focus on a narrow set of groups.</p>
<p>For each group in this new set, they’d have to show something a bit stronger than what McKay had proposed: Not only would the number of representations have to be the same for both the group and the Sylow normalizer, but those representations would have to relate to each other according to certain rules. Isaacs, Navarro and Malle showed that if this stronger statement held for these particular groups, then the McKay conjecture had to be true for every finite group. (“This was during the Euro 2004,” Navarro recalled. His co-authors “didn’t know that I was sneaking off sometimes to see some games. But important things are important things.”)</p>
<figure>
    <p><img width="2640" height="2934" src="https://www.quantamagazine.org/wp-content/uploads/2025/02/GabrielNavarro_crJavierNavarro-edited.png" alt="Man standing with his hands in his pockets." decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/02/GabrielNavarro_crJavierNavarro-edited.png 2640w, https://www.quantamagazine.org/wp-content/uploads/2025/02/GabrielNavarro_crJavierNavarro-edited-1548x1720.png 1548w, https://www.quantamagazine.org/wp-content/uploads/2025/02/GabrielNavarro_crJavierNavarro-edited-468x520.png 468w, https://www.quantamagazine.org/wp-content/uploads/2025/02/GabrielNavarro_crJavierNavarro-edited-768x854.png 768w, https://www.quantamagazine.org/wp-content/uploads/2025/02/GabrielNavarro_crJavierNavarro-edited-1382x1536.png 1382w, https://www.quantamagazine.org/wp-content/uploads/2025/02/GabrielNavarro_crJavierNavarro-edited-1843x2048.png 1843w" sizes="(max-width: 2640px) 100vw, 2640px">    </p>
            <figcaption>
                            <p>Gabriel Navarro and two colleagues turned one of group theory’s biggest open conjectures into a tractable problem.</p>
            <p>Javier Navarro</p>
        </figcaption>
    </figure>

<p>The trio’s reformulation of the problem was a major breakthrough. Within a few years, mathematicians had used it to resolve most cases of the McKay conjecture. Moreover, it helped them simplify related questions that also involved using one part of an object to study the whole. “Tons and tons of conjectures have now been reduced using this as a blueprint,” said <a href="https://sites.google.com/view/mandisf">Mandi Schaeffer Fry</a>, a mathematician at the University of Denver.</p>
<p>But there was one class of groups — “groups of Lie type” — for which the new version of the McKay conjecture remained open. The representations of these groups were particularly difficult to study, and it was challenging to prove that the relationships among them satisfied the conditions that Isaacs, Navarro and Malle had outlined.</p>
<p>But one of Malle’s graduate students was on the case. Britta Späth.</p>
<h2><strong>‘Our Obsession’</strong></h2>
<p>In 2003, Späth arrived at the University of Kassel to start her doctorate with Malle. She was almost perfectly suited for working on the McKay conjecture: Even in high school, she could spend days or weeks on a single problem. She particularly reveled in ones that tested her endurance, and she fondly recalls long hours spent searching for “tricks that are, in a way, not even so deep.”</p>
<p>Späth spent her time studying group representations as deeply as she could. After she completed her graduate degree, she decided to use that expertise to continue chipping away at the McKay conjecture. “She has this crazy, really good intuition,” said Schaeffer Fry, her friend and collaborator. “She’s able to see it’s going to be like this.”</p>

<p>A few years later, in 2010, Späth started working at Paris Cité University, where she met Cabanes. He was an expert in the narrower set of groups at the center of the reformulated version of the McKay conjecture, and Späth often went to his office to ask him questions. Cabanes was “always protesting, ‘Those groups are complicated, my God,’” he recalled. Despite his initial hesitancy, he too eventually grew enamored with the problem. It became “our obsession,” he said.</p>
<p>There are four categories of Lie-type groups. Together, Späth and Cabanes started proving the conjecture for each of those categories, and they <a href="https://www.ams.org/journals/ert/2017-21-05/S1088-4165-2017-00497-3/">reported several</a> <a href="https://www.sciencedirect.com/science/article/pii/S0001870819304372">major results</a> over the next decade.</p>
<p>Their work led them to develop a deep understanding of groups of Lie type. Although these groups are the most common building blocks of other groups, and therefore of great mathematical interest, their representations are incredibly difficult to study. Cabanes and Späth often had to rely on opaque theories from disparate areas of math. But in digging those theories up, they provided some of the best characterizations yet of these important groups.</p>
<p>As they did so, they started dating and went on to have two children. (They eventually settled down together in Germany, where they enjoy working together at one of the three whiteboards in their home.)</p>
<p>By 2018, they had just one category of Lie-type groups left. Once that was done, they would have proved the McKay conjecture.</p>
<p>That final case took them six more years.</p>
<h2><strong>A ‘Spectacular Achievement’</strong></h2>
<p>The fourth kind of Lie group “had so many difficulties, so many bad surprises,” Späth said. (It didn’t help that in 2020, the pandemic kept their two young children home from school, making it difficult for them to work.) But gradually, she and Cabanes managed to show that the number of representations for these groups matched those of their Sylow normalizers — and that the way the representations matched up satisfied the necessary rules. The last case was done. It followed automatically that the McKay conjecture was true.</p>
<p>In October 2023, they finally felt confident enough in their proof to announce it to a room of more than 100 mathematicians. A year later, they <a href="https://arxiv.org/abs/2410.20392">posted it online</a> for the rest of the community to digest. “It’s an absolutely spectacular achievement,” said <a href="https://research.manchester.ac.uk/en/persons/radha.kessar">Radha Kessar</a> of the University of Manchester.</p>
<p>Mathematicians can now confidently study important properties of groups by looking at their Sylow normalizers alone — a much easier approach to making sense of these abstract entities, and one that might have practical applications. And in the process of establishing this connection, Navarro said, the researchers developed “beautiful, wonderful, deep mathematics.”</p>
        
        
<p>Other mathematicians now hope to explore the even deeper conceptual reason why the strange coincidence McKay uncovered is true. Although Späth and Cabanes have proved it, mathematicians still don’t understand why a comparatively tiny set is enough to tell you so much about its larger parent group.</p>
<p>“There has to be some structural reason why these numbers are the same,” Kessar said.</p>
<p>Some mathematicians have done preliminary work to try to understand this connection, but so far it remains a mystery.</p>
<p>Späth and Cabanes are moving on, each searching for their next obsession. So far, according to Späth, nothing has consumed her like the McKay conjecture. “If you have done one big thing, then it’s difficult to find the courage, the excitement for the next,” she said. “It was such a fight sometimes. It also gave you, every day, a purpose.”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DOGE has 'god mode' access to government data (784 pts)]]></title>
            <link>https://www.theatlantic.com/technology/archive/2025/02/doge-god-mode-access/681719/</link>
            <guid>43112084</guid>
            <pubDate>Thu, 20 Feb 2025 07:36:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/technology/archive/2025/02/doge-god-mode-access/681719/">https://www.theatlantic.com/technology/archive/2025/02/doge-god-mode-access/681719/</a>, See on <a href="https://news.ycombinator.com/item?id=43112084">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header data-event-module="hero"><div><div><figure><div data-flatplan-lead_figure_media="true"><picture><source media="(prefers-reduced-motion)" srcset="https://cdn.theatlantic.com/thumbor/7nAql81FNLE1GxDEA2dhHOMVRVU=/0x0:2000x1125/640x360/filters:still()/media/img/2025/02/18/DogeData/original.gif 640w, https://cdn.theatlantic.com/thumbor/5cychT7bw2azzXgZHeMsbKuTZMY=/0x0:2000x1125/750x422/filters:still()/media/img/2025/02/18/DogeData/original.gif 750w, https://cdn.theatlantic.com/thumbor/onJP3-elTNee27SeG-OP6ZgVC-s=/0x0:2000x1125/850x478/filters:still()/media/img/2025/02/18/DogeData/original.gif 850w, https://cdn.theatlantic.com/thumbor/xn52k2Uxapd_w_z-n-EmtPJUdIA=/0x0:2000x1125/1536x864/filters:still()/media/img/2025/02/18/DogeData/original.gif 1536w, https://cdn.theatlantic.com/thumbor/XMClMJSNBRBgd8vWbspO-pEcvME=/0x0:2000x1125/1920x1080/filters:still()/media/img/2025/02/18/DogeData/original.gif 1920w" sizes="(min-width: 1920px) 1920px, 100vw"><img alt="Stairs lead to computer code in the clouds." sizes="(min-width: 1920px) 1920px, 100vw" srcset="https://cdn.theatlantic.com/thumbor/9wScbKwtm02nGR_tjJx0qoyqkME=/0x0:2000x1125/640x360/media/img/2025/02/18/DogeData/original.gif 640w, https://cdn.theatlantic.com/thumbor/N53GR_2--fyk9nM3z8PWM06dPpw=/0x0:2000x1125/750x422/media/img/2025/02/18/DogeData/original.gif 750w, https://cdn.theatlantic.com/thumbor/1cuaUViCPRY_tzGO56ETzu9p1xc=/0x0:2000x1125/850x478/media/img/2025/02/18/DogeData/original.gif 850w, https://cdn.theatlantic.com/thumbor/JT_m8miWKdf5xcEbPS4ywGcdKuY=/0x0:2000x1125/1536x864/media/img/2025/02/18/DogeData/original.gif 1536w, https://cdn.theatlantic.com/thumbor/eq3VovWRXwxKy41TzEQFCk9nWv4=/0x0:2000x1125/1920x1080/media/img/2025/02/18/DogeData/original.gif 1920w" src="https://cdn.theatlantic.com/thumbor/1swtYZseehdHHxLaaIbWD6OAVUM=/0x0:2000x1125/1440x810/media/img/2025/02/18/DogeData/original.gif" id="article-lead-image" width="1440" height="810"></picture></div><figcaption data-flatplan-lead_figure_caption="true">Illustration by Paul Spella / The Atlantic. Source: Getty.</figcaption></figure></div><div><p>The president’s special commission now has an unprecedented ability to view and manipulate information at many federal agencies.</p></div></div><div><p><time datetime="2025-02-19T20:59:19Z" data-flatplan-timestamp="true">February 19, 2025, 3:59 PM ET</time> </p></div><gpt-ad format="injector" sizes-at-0="mobile-wide" targeting-pos="injector-article-start" sizes-at-976="desktop-wide"></gpt-ad></header><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true"><small><em>If you have tips about the remaking of the federal government, you can contact Charlie, Ian, and Matteo on Signal at @cwarzel.92, @ibogost.47, and @matteowong.52.</em></small></p><hr><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">D<span>OGE has achieved “God mode.”</span> That’s according to an employee in senior leadership at USAID, who told us that Elon Musk’s Department of Government Efficiency now has full, unrestricted access to the agency’s digital infrastructure—including total control over systems that Americans working in conflict zones rely on, the ability to see and manipulate financial systems that have historically awarded tens of billions of dollars, and perhaps much more.</p><p data-flatplan-paragraph="true">The employee’s account, along with the accounts of several others across federal agencies, offers the clearest portrait yet of just how deep DOGE has burrowed into the systems of the federal government—and the sensitive information of countless Americans.</p><p data-flatplan-paragraph="true">In the coming weeks, the team is expected to enter IT systems at the CDC and Federal Aviation Administration, and it already has done so at NASA, according to sources we’ve spoken with at each of those agencies. At least one DOGE ally appears to be working to open back doors into systems used throughout the federal government. Thomas Shedd, a former Tesla engineer who was recently appointed director of the Technology Transformation Services, requested privileged access to 19 different IT systems administered by teams within TTS, according to two federal workers we spoke with who are familiar with his request. With this level of control, Shedd would be able to not only view and modify federal data, but also grant and revoke access to other people. (In a written statement, Will Powell, the acting press secretary for the General Services Administration, of which TTS is a part, said Shedd needs this level of access to rapidly identify “areas for optimization and efficiencies” and insisted that he is working with “appropriate GSA officials” to follow established protocols.)</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/technology/archive/2025/02/elon-musk-doge-security/681600/">Read: The government’s computing experts say they are terrified</a></p><p data-flatplan-paragraph="true">Over the past few days, we’ve talked with civil servants working for numerous agencies, all of whom requested anonymity because they fear what will happen if they lose their job—not just to themselves, but to the functioning of the federal government. Their observations reveal the abnormal degree of power that DOGE has already achieved. Federal agencies are subject to various forms of administrative and legal oversight, but they operate separately from one another for good reasons: to support a specialized purpose and to insulate them from undue outside influence. Now they effectively roll up to Elon Musk. (Neither the White House nor DOGE responded to requests for comment for this story. Earlier this week, a White House official <a data-event-element="inline link" href="https://www.nytimes.com/2025/02/18/us/politics/elon-musk-doge-leader.html">claimed</a> that Musk is not the head of DOGE. He is clearly the group’s functional leader.)</p><p data-flatplan-paragraph="true">Among the federal agencies we reported on, USAID is the only one where we could confirm that DOGE has acquired God-mode access across the entire digital system. (The Trump administration has sought to effectively shut down USAID since the inauguration.) But as Musk and his acolytes enter a growing number of federal databases and IT systems, their unfettered access at USAID offers a sense of what they might be able to do elsewhere. At NASA, for example, it could mean access to knowledge about sensitive government technologies used for defense. At the CDC, such ability could expose millions of Americans’ health data and allow DOGE to access labs that store deadly pathogens. At Treasury, such access would allow Musk’s employees to view Americans’ names, Social Security numbers, and financial information. “It is not ridiculous to think they’d have bank-account and routing numbers for every single person in the United States,” the senior USAID source said. “What do you do with this information? I had to ask myself, <em>Do I file my taxes this year or not?</em> I had to sit and debate that.”</p><p data-flatplan-paragraph="true">The federal government does not typically grant such wide-ranging access to a single entity, let alone one that is effectively under the control of an unelected, erratic, and politically extreme actor such as Musk. The group is working on behalf of Donald Trump, but sources we spoke with emphasized that the level of access DOGE possesses means that the organization may already be able to siphon data that Musk or his agents could hold on to forever, long after his time as a government liaison, or even after a potential falling-out with the president.</p><p data-flatplan-paragraph="true">One experienced government information-security contractor offered a blunt response to the God-mode situation at USAID: “That sounds like our worst fears come true.” The purpose of DOGE’s incursions remains unclear to employees at these agencies. Musk was supposed to help improve the workings of the government—that is DOGE’s stated purpose. But in the offices where the team is reaching internal IT systems, some are beginning to worry that he might prefer to destroy it, to take it over, or just to loot its vaults for himself.</p><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">“O<span>nce they’re in, they’re in</span>,” the USAID employee told us. And this is a big part of the problem in a nutshell: Access is everything, and in many cases, DOGE has it.</p><p data-flatplan-paragraph="true">At USAID and other agencies whose employees we spoke with, leaders explained that Musk’s team could copy and remove information from government servers without anybody knowing. The team could then feed this classified information into AI tools, either for training purposes or to mine the data for insights. (Members of DOGE already reportedly <a data-event-element="inline link" href="https://www.washingtonpost.com/nation/2025/02/06/elon-musk-doge-ai-department-education/">have put sensitive data</a> from the Education Department into AI software.) Within USAID, DOGE has full access to human-resources information—Social Security numbers, addresses, reputational data such as performance reviews, plus classified information and disciplinary information. The USAID source noted that DOGE can also control USAID systems that help with disbursement of funds, building-access tools, and payroll: “If they wanted to change how much a person is making, they could modify that, given their access in the system.” According to the employee, DOGE is also inside of an internal system for managing contracts and grants, which functions like a high-security online marketplace where USAID plans and approves billions in government spending.</p><p data-flatplan-paragraph="true">Inside NASA, according to one agency employee we spoke with, DOGE workers already have access to contracts, partnerships, performance reviews, classified national-security information, and satellite data, among other materials. The NASA worker told us that such knowledge could erase generations of advantage in aerospace and defense capabilities if it falls into the wrong hands. Agency technologies such as propulsion systems, novel materials, and satellites overlap with Department of Defense projects. Someone with information about NASA’s thermal-protection or encryption technologies could take advantage of vulnerabilities in aerospace vehicles, for example.</p><p id="injected-recirculation-link-1" data-view-action="view link - injected link - item 2" data-event-element="injected link" data-event-position="2"><a href="https://www.theatlantic.com/ideas/archive/2025/02/musk-terror-reign/681731/">Derek Thompson: DOGE’s reign of ineptitude </a></p><p data-flatplan-paragraph="true">USAID employees have felt more acute effects of DOGE’s operations. Employees there say they have been rattled by the demands of DOGE engineers: “They have walked in and said to senior staff,<em> You have 15 minutes to do this or you’re fired</em>,” the USAID senior leader told us. Now USAID staff are “operating in a zero-trust environment.” With its God-mode IT control at the agency, DOGE can read emails and chats, plus see who’s attending which meetings. The source described employees in a recent meeting growing alarmed when transcription services seemed to turn on without warning. An employee at NASA reported similar concerns, after unfamiliar messages appeared on workstations. “We’re operating believing our systems are completely bugged,” one person told us.</p><p data-flatplan-paragraph="true">The senior USAID official fears that DOGE could terminate somebody working in a conflict zone like Ukraine, Sudan, or Ethiopia from an agency system. “If they lose access to their USAID laptop, phone, and accounts, for a lot of them that’s their only means of communication. We are putting their lives on the line,” one said.</p><p data-flatplan-paragraph="true">For those who have watched DOGE storm into their workplace, what is perhaps most terrifying is its attempts to scale. If DOGE were to acquire God-mode administrative access across many systems, several sources told us, that level of control could affect every citizen at home, and many American interests abroad: personal financial data, defense secrets, and more, all in the palm of Musk’s hand.</p><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">T<span>here’s reason to believe</span> that health information may be next. The Trump administration fired roughly <a data-event-element="inline link" href="https://www.washingtonpost.com/health/2025/02/18/trump-health-firings-fda-cdc/">700</a> people at the CDC last weekend. As in other agencies, the firings will hollow out expertise but also remove obstacles to further changes. A CDC employee told us that the agency’s Office of the Chief Information Officer is expecting DOGE, but “no one has seen anyone yet.”</p><p data-flatplan-paragraph="true">The individual, who has knowledge of how CDC information systems work, fears that DOGE could gain access to an abundant store of sensitive information about health and disease. This year, the CDC is supposed to roll out a <a data-event-element="inline link" href="https://www.cdc.gov/public-health-data-strategy/php/about/index.html">central data platform</a> for public-health surveillance and emergency response to better address new threats such as H5N1 bird flu and old ones such as <a data-event-element="inline link" href="https://apnews.com/article/measles-texas-mmr-vaccine-homeschooling-b29d08d53cf26704968e8c00dfa712ba">measles</a>. The new system, called the One CDC Data Platform, promises to aggregate all of the CDC’s public-health data, including hundreds of thousands of daily anonymized lab tests, data from emergency-room visits, and measurements from wastewater disease-reporting sites.</p><p data-flatplan-paragraph="true">The design and rollout of this system were already controversial inside the agency, our source said, even before Trump and Musk came on the scene. Putting everybody’s health data in one place carries risks. Although the health data the CDC houses are usually de-identified or aggregated, “people with very stigmatizing illnesses could be identified by certain characteristics” if the data are exposed or misused, the CDC worker said. What’s more, plenty of health data contain information that, when correlated with other data outside the system, could pinpoint specific individuals. Given all of the data that DOGE appears to be capable of siphoning from all over the government, such identification could become much easier. The CDC collects electronic health-record details from all over the country, meaning that this could affect just about everyone—including us, and you too.</p><p data-flatplan-paragraph="true">CDC systems control more than mere information about disease. At the agency’s facility in Atlanta, the CDC stores the microbes that cause disease and can hold secrets to treatment. Some are relatively benign, such as strains of <em>E. coli</em>. Others are intrinsically dangerous, including the Ebola and Marburg viruses, and bacterium that causes tuberculosis. These materials are housed in labs with associated biosafety levels. The highest level, BSL-4, applies to only a small number of labs around the world containing “dangerous and exotic” microbes, as the CDC <a data-event-element="inline link" href="https://www.cdc.gov/training/quicklearns/biosafety/#:~:text=%C2%AB%20Prev%20Page-,BSL%2D4,include%20Ebola%20and%20Marburg%20viruses">describes</a> them, that pose a high risk of spread.</p><p data-flatplan-paragraph="true">Access to such labs is managed by computers, and management of those computers is local to the CDC. If DOGE got the same kind of access to CDC IT systems as it has elsewhere, would that give the group direct access to CDC facilities? “Yes, those are all out of CDC level,” our source at the agency told us. Does that mean that DOGE could gain direct access to BSL-4 labs? we asked. “It’s definitely possible,” the employee said.</p><p id="injected-recirculation-link-2" data-view-action="view link - injected link - item 3" data-event-element="injected link" data-event-position="3"><a href="https://www.theatlantic.com/technology/archive/2025/02/elon-musk-doge-nuclear-weapons/681581/">Read: If DOGE goes nuclear</a></p><p data-flatplan-paragraph="true">Our source hoped that such a prospect would be averted. (To repeat, DOGE hasn’t taken control of CDC IT systems yet, at least as far as our source knew.) But the employee also explained that the recent layoffs will reduce the agency’s ability to defend against IT or security errors, on top of diminishing morale. The CDC did not respond to requests for comment about whether someone with full, local IT control could indeed grant entry to, or control over, BSL-4 labs and their contents.</p><p data-flatplan-paragraph="true" data-flatplan-dropcap="true">T<span>he risk of harm, abuse, or political revenge</span> is clear. But simple, brazen corruption is also a concern among the federal workers we spoke with. The CDC staffer wondered if DOGE’s unelected and seemingly unaccountable leadership, including Musk, might simply want to sell the public-health data the CDC collects. Democratic leaders have also expressed the worry that Musk’s interest in SpaceX, which has received billions of dollars in contracts from NASA over the years, creates an untenable conflict of interest. The NASA employee worried that Musk would end up “reaping all of the profits of the investment that the American public put into NASA’s research, which was being shared with the country.” NASA holds technical specs and research data for SpaceX competitors, and insiders fear that such information will soon be compromised, too. They also worry that classified NASA R&amp;D in areas such as quantum, biotech, and astrobiology could be stolen for private gain.</p><p data-flatplan-paragraph="true">A number of lawsuits have been filed seeking to limit DOGE’s access, with <a data-event-element="inline link" href="https://www.reuters.com/legal/judges-consider-barring-musks-doge-team-government-systems-2025-02-14/">mixed results</a>. Meanwhile, Trump and Musk have both <a data-event-element="inline link" href="https://www.reuters.com/world/us/trump-allies-ramp-up-attacks-judges-musk-calls-impeachments-2025-02-12/">attacked</a> judges who have ruled against their interests; Musk has said they should be impeached. Trump has also indicated that he might just <a data-event-element="inline link" href="https://www.theatlantic.com/politics/archive/2025/02/trump-vance-courts/681632/">ignore the courts</a>—an act that would be <a data-event-element="inline link" href="https://www.nytimes.com/2025/02/16/opinion/what-if-trump-defies-courts.html?smid=nytcore-ios-share&amp;referringSource=articleShare">challenging to counter, providing plenty of opportunity for the administration to get its way.</a> Across agencies, leaders have started to step aside voluntarily. Jim Jones, head of the Food and Drug Administration’s food division, resigned this week. Michelle King, who ran the Social Security Administration, stepped down too rather than carry out DOGE’s wishes. The resignations may be principled, but they open the door for more compliant replacements.</p><p data-flatplan-paragraph="true">The request from Shedd, the former Tesla engineer, in particular, illustrates the variety of avenues and back doors that DOGE and Musk’s allies may have available to enter the government’s systems. According to the two federal workers we spoke with about Shedd’s efforts, such access typically is not granted to TTS leadership and requires a specific reason and the permission of each system’s owner. Shedd initially issued a blanket request, the sources told us, and is now attempting to bypass the individual system owners by seeking permission from other officials, circumventing standard security procedures. He also had not completed a background check, which is usually required for such access, at least as of when he first made the request, according to our sources. How much access Shedd has been granted remains uncertain.</p><p data-flatplan-paragraph="true">This is the DOGE playbook: There are no norms to be respected, and everything is up for grabs. Once the damage is done, it will be difficult to remedy, especially if DOGE staffers can themselves grant or remove access to others at their discretion.</p><p data-flatplan-paragraph="true">Musk and DOGE’s first month has been so chaotic, their incursions so haphazard, that assessing what has even happened is difficult. DOGE claims to be improving the government, but the agency workers we spoke with feel that they are being hacked instead. So it is worth stepping back to note the most basic fact: No good reason or case can be made for one person or entity to have this scope of access to this many government agencies containing this much sensitive information. Even in one government office, full administrative access to all systems is the rarest privilege. In the aggregate, across the whole of the government, it would be unfathomable.</p></section><div data-event-module="footer"><p><h3>About the Authors</h3></p><div><address id="article-writer-0" data-event-element="author" data-event-position="1" data-flatplan-bio="true"><div><p><a href="https://www.theatlantic.com/author/charlie-warzel/" data-event-element="image"><img alt="" loading="lazy" src="https://cdn.theatlantic.com/thumbor/lyW9wA_jPYje_sXRDGnpW8Gl4Kk=/54x0:1756x1702/120x120/media/img/authors/2022/11/IMG_0264/original.jpg" width="60" height="60"></a></p></div></address></div><div><address id="article-writer-1" data-event-element="author" data-event-position="2" data-flatplan-bio="true"><div><p><a href="https://www.theatlantic.com/author/ian-bogost/" data-event-element="image"><img alt="" loading="lazy" src="https://cdn.theatlantic.com/thumbor/dMbJwBoshVoOt2SGSV-7Y1D29xI=/38x38:1890x1890/120x120/media/None/ibogost_atlantic_new/original.jpg" width="60" height="60"></a></p></div></address></div><div><address id="article-writer-2" data-event-element="author" data-event-position="3" data-flatplan-bio="true"><div><p><a href="https://www.theatlantic.com/author/matteo-wong/" data-event-element="image"><img alt="" loading="lazy" src="https://cdn.theatlantic.com/thumbor/cvqkiGqr5oeTvJi706qtcaOkivk=/815x349:3584x3118/120x120/media/img/authors/2023/06/Matteo_Wong/original.jpg" width="60" height="60"></a></p></div></address></div></div><gpt-ad format="injector" sizes-at-0="mobile-wide,native,house" targeting-pos="injector-most-popular" sizes-at-976="desktop-wide,native,house"></gpt-ad></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FAQ on Microsoft's topological qubit thing (297 pts)]]></title>
            <link>https://scottaaronson.blog/?p=8669</link>
            <guid>43112021</guid>
            <pubDate>Thu, 20 Feb 2025 07:25:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scottaaronson.blog/?p=8669">https://scottaaronson.blog/?p=8669</a>, See on <a href="https://news.ycombinator.com/item?id=43112021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-8669">
				
<p><strong>Q1.</strong> Did you <a href="https://azure.microsoft.com/en-us/blog/quantum/2025/02/19/microsoft-unveils-majorana-1-the-worlds-first-quantum-processor-powered-by-topological-qubits/">see</a> <a href="https://www.nature.com/articles/d41586-025-00527-z">Microsoft’s</a> <a href="https://www.nytimes.com/2025/02/19/technology/microsoft-quantum-computing-topological-qubit.html">announcement</a>?<br>A. Yes, thanks, you can stop emailing to ask!  Microsoft’s Chetan Nayak was even kind enough to give me a personal briefing a few weeks ago.  Yesterday I did a <a href="https://www.bbc.co.uk/sounds/play/w3ct60h5">brief interview</a> on this for the BBC’s World Business Report, and I also <a href="https://www.technologyreview.com/2025/02/19/1112072/a-new-microsoft-chip-could-lead-to-more-stable-quantum-computers/">commented</a> for MIT Technology Review.</p>



<p><strong>Q2.</strong> What <em>is</em> a topological qubit?<br><strong>A.</strong> It’s a special kind of qubit built using nonabelian <a href="https://en.wikipedia.org/wiki/Anyon">anyons</a>, which are excitations that can exist in a two-dimensional medium, behaving neither as fermions nor as bosons.  The idea grew out of seminal work by Alexei Kitaev, Michael Freedman, and others starting in the late 1990s.  Topological qubits have proved harder to create and control than ordinary qubits.</p>



<p><strong>Q3.</strong> Then why do people care about topological qubits?<br><strong>A.</strong> The dream is that they could <em>eventually</em> be more resilient to decoherence than regular qubits, since an error, in order to matter, needs to change the <em>topology</em> of how the nonabelian anyons are braided around each other.  So you’d have some robustness built in to the physics of your system, rather than having to engineer it laboriously at the software level (via <a href="https://en.wikipedia.org/wiki/Threshold_theorem">quantum fault-tolerance</a>).</p>



<p><strong>Q4.</strong> Did Microsoft create the first topological qubit?<br><strong>A.</strong> Well, they say they did!</p>



<p><strong>Q5.</strong> Didn’t Microsoft claim the experimental creation of Majorana zero modes—a building block of topological qubits—back in 2018, and didn’t they then need to <a href="https://spectrum.ieee.org/majorana-microsoft-backed-quantum-computer-research-retracted">retract</a> that claim?<br><strong>A.</strong> Yep.  Certainly that history is making some experts cautious about the new claim.  When I asked Chetan Nayak how confident I should be, his response was basically “look, we now have a topological qubit that’s behaving fully as a qubit; how much more do people want?”</p>



<p><strong>Q6.</strong> Is this a big deal?<br><strong>A.</strong> <em>If</em> the claim stands, I’d say it’s a scientific milestone for the field of topological quantum computing and physics beyond.  The number of topological qubits manipulated in a single experiment has then finally increased from 0 to 1, and depending on how you define things, arguably a “new state of matter” has even been created, one that doesn’t appear in nature (but only in <em>Nature</em>).</p>



<p><strong>Q7.</strong> Is this useful?<br><strong>A.</strong> Not yet!  If anyone claims that a single qubit, or even 30 qubits, are already <em>useful</em> for speeding up computation, you can ignore anything else that person says.  (Certainly Microsoft makes no such claim.)  On the question of what we believe quantum computers will or won’t <em>eventually</em> be useful for, see like half the archives of this blog over the past twenty years.</p>



<p><strong>Q8.</strong> Does this announcement vindicate topological qubits as the way forward for quantum computing?<br><strong>A.</strong> Think of it this way.  <em>If</em> Microsoft’s claim stands, then topological qubits have finally reached some sort of parity with where more traditional qubits were 20-30 years ago.  I.e., the non-topological approaches like superconducting, trapped-ion, and neutral-atom have an absolutely <em>massive</em> head start: there, Google, IBM, Quantinuum, QuEra, and other companies now routinely do experiments with dozens or even hundreds of entangled qubits, and thousands of two-qubit gates.  Topological qubits can win if, and only if, they turn out to be <em>so much</em> more reliable that they leapfrog the earlier approaches—sort of like the transistor did to the vacuum tube and electromechanical relay.  Whether that will happen is still an open question, to put it extremely mildly.</p>



<p><strong>Q9.</strong> Are there other major experimental efforts to build topological qubits?<br><strong>A.</strong> No, it’s pretty much just Microsoft.  Purely as a scientist who likes to see things tried, I’m grateful that one player stuck with the topological approach even when it ended up being a long, painful slog.</p>



<p><strong>Q10.</strong> Is Microsoft now on track to scale to a million topological qubits in the next few years?<br><strong>A.</strong> In the world of corporate PR and pop-science headlines, sure, why not?  As Bender from <em>Futurama</em> <a href="https://www.youtube.com/watch?v=DGZ10kZ4lmE">says</a>, “I can guarantee anything you want!” In the world of reality, a “few years” certainly feels overly aggressive to me, but good luck to Microsoft and good luck to its competitors!  I foresee exciting times ahead, provided we still have a functioning civilization in which to enjoy them.</p>

		
				
				<p>
					<small>
						This entry was posted
												on Thursday, February 20th, 2025 at 12:34 am						and is filed under <a href="https://scottaaronson.blog/?cat=4" rel="category">Quantum</a>.
						You can follow any responses to this entry through the <a href="https://scottaaronson.blog/?feed=rss2&amp;p=8669">RSS 2.0</a> feed.

													You can <a href="#respond">leave a response</a>, or <a href="https://scottaaronson.blog/wp-trackback.php?p=8669" rel="trackback">trackback</a> from your own site.

						
					</small>
				</p>

			</div><p>You can use rich HTML in comments!  You can also use basic TeX, by enclosing it within <span>$$ $$</span> for displayed equations or <span>\( \)</span> for inline equations.</p><p>
	After two decades of mostly-open comments, in July 2024 <i>Shtetl-Optimized</i> transitioned to the following policy:
	
</p><p>All comments are treated, by default, as personal missives to me, Scott Aaronson---with no expectation either that they'll appear on the blog or that I'll reply to them.

</p><p>At my leisure and discretion, and in consultation with the <a href="https://scottaaronson.blog/?p=6576"><i>Shtetl-Optimized</i> Committee of Guardians</a>, I'll put on the blog a curated selection of comments that I judge to be particularly interesting or to move the topic forward, and I'll do my best to answer those.  But it will be more like Letters to the Editor.  Anyone who feels unjustly censored is welcome to the rest of the Internet.

</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grok 3: Another win for the bitter lesson (123 pts)]]></title>
            <link>https://www.thealgorithmicbridge.com/p/grok-3-another-win-for-the-bitter</link>
            <guid>43111963</guid>
            <pubDate>Thu, 20 Feb 2025 07:15:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thealgorithmicbridge.com/p/grok-3-another-win-for-the-bitter">https://www.thealgorithmicbridge.com/p/grok-3-another-win-for-the-bitter</a>, See on <a href="https://news.ycombinator.com/item?id=43111963">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>For once, it seems Elon Musk wasn’t exaggerating when he called Grok 3 the “</span><a href="https://x.com/elonmusk/status/1890958798841389499" rel="">smartest AI on Earth</a><span>.” Grok 3 is a </span><em>massive leap</em><span> </span><em>forward </em><span>compared to Grok 2. (You can watch the full presentation </span><a href="https://x.com/xai/status/1891699715298730482" rel="">here</a><span>.)</span></p><p><span>Grok 3 performs at a level comparable to, and in some cases even exceeding, models from more mature labs like OpenAI, Google DeepMind, and Anthropic. It </span><a href="https://x.com/lmarena_ai/status/1891706264800936307" rel="">tops all categories in the LMSys arena</a><span> and the reasoning version shows strong results—o3-level—in math, coding, and science questions (according to benchmarks). By most measures, it is, at the very least, co-state-of-the-art (not </span><a href="https://x.com/theo/status/1891736803796832298" rel="">at every task</a><span>, though).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg" width="1283" height="871" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:871,&quot;width&quot;:1283,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Imagen&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Imagen" title="Imagen" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e32ee1-f5ca-4639-910f-74b82d1cdc32_1283x871.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg" width="1283" height="903" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:903,&quot;width&quot;:1283,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Imagen&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Imagen" title="Imagen" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bcb2f00-4ee8-40be-95e5-482a06b58d61_1283x903.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg" width="1283" height="866" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:866,&quot;width&quot;:1283,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Imagen&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Imagen" title="Imagen" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a0cf710-e0f4-42b8-8305-cc64494e0e8c_1283x866.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>In one word, Grok 3 is </span><em>great</em><span>.</span></p><p><span>But more than just a win for xAI, Grok 3 represents yet another victory for </span><a href="https://www.thealgorithmicbridge.com/p/gpt-4-the-bitterer-lesson" rel="">the Bitter Lesson</a><span>. Perhaps the clearest one so far. Contrary to what the press and critics keep saying, the scaling laws still govern AI progress—more than ever before.</span></p><p>This article is not about DeepSeek, but I want to revisit a valuable insight to support my argument.</p><p><span>DeepSeek managed to </span><a href="https://www.thealgorithmicbridge.com/p/deepseek-is-chinese-but-its-ai-models" rel="">compete with top players</a><span> despite having a moderate compute disadvantage—50K Nvidia Hoppers vs 100K+ Nvidia H100s for US labs). To compensate, they had to optimize across the entire stack, showing impressive engineering prowess. They succeeded, turning the startup into an exception that could prove misplaced the community’s trust in both the bitter lesson and the scaling paradigm.</span></p><p><span>That is, at least, what skeptics argued: DeepSeek, with its “tiny” GPU cluster and cheap training runs, built a model at the level of OpenAI o1. To them, this was strong evidence that hand-crafted features, human ingenuity, and algorithmic improvements were more important than using giant clusters to train AI models. It even meant that you should sell all your Nvidia stock because GPUs were </span><em>not that important</em><span>.</span></p><p>However, this conclusion emerges from a mistaken understanding of the Bitter Lesson (which isn’t a law of nature but an empirical observation): It doesn’t say that algorithmic improvements don’t work. It very much praises them. What it says is that, if you can choose, scaling learning and search algorithms with more computing power is the better choice over applying heuristic solutions, however clever.</p><p><span>DeepSeek achieved good results doing the latter because they</span><em> had no option</em><span>. Had they trained on a 100K GPU cluster like xAI did for Grok 3, they would have obtained much better results. DeepSeek proved that further optimization is possible—not that scaling is useless. Those two realities are uncorrelated.</span></p><p><span>Indeed, DeepSeek’s CEO, Liang Wenfeng, acknowledged that </span><a href="https://archive.is/JnE4j" rel="">US export controls are their main bottleneck</a><span> to make better models. The fact that he said this despite having access to ~50K Hoppers reveals the opposite of “GPUs don’t matter.” While DeepSeek relied on clever optimizations, they still needed </span><em>scale</em><span>. DeepSeek CEO said it—why should anyone outside the company think they know better?</span></p><p>DeepSeek’s unprecedented success supports the Bitter Lesson and the scaling paradigm, even if to some degree as an exception to the rule.</p><p>Back to Grok 3.</p><p><span>I wonder if xAI’s achievement will make skeptics rethink their criticisms of scale. We don’t know whether they changed the architecture or how much they optimized the infrastructure to make Grok 3, that’s true. What we do know is they trained the model in the </span><a href="https://www.tomshardware.com/desktops/servers/first-in-depth-look-at-elon-musks-100-000-gpu-ai-cluster-xai-colossus-reveals-its-secrets" rel="">100K H100 Colossus supercomputer</a><span> that xAI built in Memphis, Tennessee. That’s a lot of GPUs. That’s much more GPUs than DeepSeek has.</span></p><p>(Probably both used just a fraction of the total compute available, but it’s a reasonable assumption that having 2x more GPUs allows you to train your models with ~2x more GPUs, whether you’re using all of them or not.)</p><p><span>Unlike DeepSeek, they </span><em>didn’t need</em><span> to optimize the infrastructure beyond reasonable standards (so, not touching the CUDA kernels or applying unproven algorithmic tricks). They surely did some optimization, but I highly doubt they went nearly that far. The Bitter Lesson says that if you have the compute it’s better to use it </span><em>before </em><span>you fiddle like a scrappy GPU-poor; scaling a bit further eventually brings more benefits than doing some hand-crafting (</span><a href="https://x.com/elonmusk/status/1891767926975283425" rel="">this cryptic tweet by Musk</a><span> can mean many things). And remember, xAI is scale-pilled.</span></p><p>So that’s probably what they did: They threw more compute at Grok 3 than even OpenAI could, and the result is a state-of-the-art model.</p><p><span>I want to make a quick digression here because I’m tired of referencing the Bitter Lesson as if it were an AI-exclusive insight. No—it is a fundamental truth. If you have more of the primary resource, you don’t waste time squeezing out another drop out of a secondary resource; you take a full glass from the cascade. Would you rather be like </span><em>Dune</em><span>’s Fremen, recycling sweat and wringing moisture from the dead with clever but desperate devices, or live on a planet where it rains?</span></p><p><span>Improving algorithms and increasing the compute you run them on </span><em>are both valuable approaches</em><span> but if the marginal gains you get suddenly shrink as a function of the effort you’re putting in, you’re better off </span><em>changing your variable of focus </em><span>rather than being stubborn. And because computing power is mostly available as long as you have money (for now) whereas valid algorithmic tricks require rare Eureka moments that may work today but not tomorrow—and perhaps never scale—you’re almost always better off focusing on growing the power of your computer.</span></p><p>If you “find a wall,” then you just change the thing you scale, but you don’t. stop. scaling.</p><p>Constraints do fuel innovation—and struggle builds character; I’m sure DeepSeek’s team is absolutely cracked—but in the end, having more beats doing the same with less. This is an unfair world and I’m sorry.</p><p>I wonder if DeepSeek likes to be in the position they are or if they would gladly change places with xAI or OpenAI. Or, conversely, do you think OpenAI and xAI would ditch all their GPUs so they can innovate through their constraints like DeepSeek did?</p><p><span>In a way, being both latecomers, xAI and DeepSeek represent opposite approaches to the same challenge. Brute-force scale vs squeezing out limited resources. Both </span><em>did great</em><span> with what they had. But, undoubtedly, xAI is better off and will be in the coming months (as long as DeepSeek remains compute-bottlenecked). That’s thanks to their adherence to a law that albeit empirical—and heavily contested by the Academia—has proven valuable in the arena for more than a decade.</span></p><p><span>For a while, a late start seemed like an insurmountable burden in the AI race. When I first assessed their chances, I wasn’t sure xAI could catch up to OpenAI and Anthropic (said that much in the very last sentence of </span><a href="https://www.thealgorithmicbridge.com/p/8-insights-to-make-sense-of-openai" rel="">this article</a><span>).</span></p><p>But between Grok 2 (Aug. 2024) and Grok 3 (Feb. 2025), something—besides the Colossus GPU cluster—helped xAI’s chances: the dominant scaling paradigm shifted.</p><ul><li><p>The pre-training era (2019-2024): Initially, scaling meant building ever-larger models trained on massive datasets and massive computers: GPT-2 (Feb. 2019) had 1.5 billion parameters whereas GPT-4 (Mar. 2023) is estimated at 1.76 trillion. That’s three orders of magnitude larger. This approach naturally favored early movers like OpenAI, who had a multi-year head start in collecting training data, growing models, and buying GPUs. Even without that advantage, if each new model took ~half a year to train—their size slowed down iteration speed between generations—OpenAI would always be at least that far ahead of xAI.</p></li><li><p><span>The post-training era (2024-???): The game changed when companies realized that simply making models larger was yielding diminishing returns (the press was quick to misreport this as “scale is over” so I urge you to </span><a href="https://youtu.be/1yvBqasHLZs" rel="">watch this talk by Ilya Sutskever</a><span> at NeurIPS 2024 in December). Instead, the focus became scaling test-time compute (i.e., allowing models to use compute to think answers through), </span><a href="https://www.thealgorithmicbridge.com/p/openai-o1-a-new-paradigm-for-ai" rel="">spearheaded by OpenAI with o1-preview</a><span>. Reinforcement learning combined with supervised fine-tuning </span><a href="https://www.thealgorithmicbridge.com/p/deepseek-is-chinese-but-its-ai-models" rel="">proved to be highly effective</a><span>—especially in structured domains like math and coding, where well-defined, verifiable reward functions exist.</span></p></li></ul><p>This paradigm shift meant that scaling post-training became as important—if not more—than scaling pre-training. AI companies “stopped” making larger models and began to make them better thinkers. This happened last year. Right when DeepSeek and xAI were building their brand-new models. A happy coincidence.</p><p><span>Importantly, post-training is still in its early days, rapid improvements </span><a href="https://darioamodei.com/on-deepseek-and-export-controls#three-dynamics-of-ai-development" rel="">can be achieved cheaply compared to pre-training</a><span>. That’s how OpenAI </span><a href="https://x.com/_jasonwei/status/1870184982007644614" rel="">jumped from o1 to o3 in just three months</a><span>. It’s how DeepSeek caught up with </span><a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf" rel="">R1</a><span> despite owning fewer and worse GPUs. It’s how Grok has reached a top-tier level in just two years.</span></p><p><span>OpenAI still has a moderate advantage, but it is no longer the kind of lead that made catching up impossible. And while Sam Altman has to balance cutting-edge research with the demands of running ChatGPT—a product used by </span><a href="https://www.theverge.com/2024/12/4/24313097/chatgpt-300-million-weekly-users" rel="">300 million people weekly</a><span>—xAI and DeepSeek have more flexibility to focus on breakthroughs (DeepSeek’s app spiked in popularity but </span><a href="https://substack.com/@thealgorithmicbridge/note/c-94091734" rel="">went back down</a><span> because the company doesn’t have enough compute to serve inference to so many users).</span></p><p>A new paradigm; a new rivalry.</p><p>Acknowledging neither the Bitter Lesson nor this scaling paradigm shift takes away from either company’s achievements. They had it easier but they had to do it anyway. Others tried and failed (e.g. Mistral, Character, Inflection). As I said, Grok 3 is primarily a win for the Bitter Lesson and DeepSeek is primarily an exception that proves the rule, but let’s not reduce them to just that.</p><p><span>Compute alone—or the lack thereof—isn’t everything. Just like the Bitter Lesson doesn’t deny the value of improving algorithms and infrastructure, we shouldn’t ignore the fact that xAI has an outstanding team, now around 1,000 employees—on par with OpenAI (~2,000) and Anthropic (~700). There’s also Elon Musk’s deep connections in tech and finance, which give xAI a massive fundraising advantage. The same goes for DeepSeek, deserving as much praise as it got for the way it navigated its constraints and emerged victorious in a local ecosystem lacking ambition and experienced talent, as well as government support (</span><a href="https://www.theinformation.com/briefings/chinese-president-xi-jinping-meets-tech-leaders" rel="">that might change soon</a><span>).</span></p><p>But recognizing the victory is just as important as putting it into context.</p><p><span>OpenAI, Google DeepMind, and Anthropic built their models when scaling was harder, slower, and more expensive (pre-training era). No one knew whether something like ChatGPT would work as well as it did (OpenAI </span><a href="https://www.forbes.com/sites/alexkonrad/2023/02/03/exclusive-openai-sam-altman-chatgpt-agi-google-search/" rel="">almost didn’t launch it</a><span> and when it did, it was touted as a “</span><a href="https://www.theverge.com/2023/12/5/23989871/openai-execs-dubbed-chatgpt-a-low-key-research-preview" rel="">low-key research preview</a><span>”). These startups acted as bold trailblazers moved by an </span><a href="https://www.thealgorithmicbridge.com/p/what-no-one-outside-openai-can-really" rel="">unwavering conviction</a><span>. Their role, although now slightly overshadowed by the opposition in press headlines, will be recorded in history books.</span></p><p>DeepSeek and xAI, in contrast, stood on the shoulders of those giants, leveraging the lessons painstakingly learned from their early efforts, and benefiting from the sheer luck of building their models at a moment when a paradigm shift made faster, more cost-effective progress possible (post-training era). They didn’t have to navigate as many missteps or endure a massive upfront investment with uncertain returns.</p><p>So let’s not downplay their wins but let’s also not forget how we got here.</p><p>There’s a final key takeaway from Grok 3 and xAI.</p><p><span>Once companies figure out how to scale post-training to the same investment levels as pre-training—it will happen, they’re stockpiling hundreds of thousands of GPUs and building giant clusters, to the bewildered lamentation of the “GPUs don’t matter” crowd—only those with the money and compute to keep up will remain competitive. (That’s why Dario Amodei and others have written extensively </span><a href="https://darioamodei.com/on-deepseek-and-export-controls" rel="">on the value of export controls</a><span>.)</span></p><p><span>This is where xAI has positioned itself remarkably well. Better than DeepSeek and, arguably, even better than OpenAI and Anthropic. (Having Elon Musk as CEO helps.) With a 100K H100 cluster—soon expanding to 200K—xAI has secured a major advantage in the next phase of AI development. Meta is following the same playbook, preparing to launch Llama 4 in the coming months, having trained it in a </span><a href="https://analyticsindiamag.com/ai-news-updates/mark-zuckerberg-confirms-llama-4-release-in-early-2025/" rel="">100K+ H100 cluster</a><span>.</span></p><p><span>For DeepSeek, engineering ingenuity alone won’t be enough this time, no matter how good they are at tinkering across the full stack (perhaps </span><a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/huawei-adds-deepseek-inference-support-for-its-ascend-ai-gpus" rel="">Huawei can help them</a><span>). There comes a point where no amount of optimizations can make up for a 150K-GPU gap. Don’t get me wrong—DeepSeek would have done the same as xAI did (they’re scale-pilled as well) but export restrictions are truly limiting their ability to grow.</span></p><p>Not even OpenAI and Anthropic have their clusters as locked in as xAI—Nvidia’s favoritism means Musk’s company is getting priority access to next-gen hardware.</p><p><span>Despite all of this, OpenAI, Google DeepMind, and Anthropic still have a small head start. OpenAI will release </span><a href="https://x.com/sama/status/1889755723078443244" rel="">GPT-4.5/GPT-5</a><span>, then o4, and Anthropic is </span><a href="https://www.theinformation.com/articles/anthropic-strikes-back?rc=j0xnsg" rel="">about to launch Claude 4</a><span>. Google DeepMind keeps improving the Thinking-model versions of </span><a href="https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/" rel="">Gemini 2.0</a><span> and working hard to reduce costs and augment the context window.</span></p><p><span>My prediction was that </span><a href="https://www.thealgorithmicbridge.com/p/20-predictions-for-ai-in-2025" rel="">Google would be ahead by year’s end</a><span> but I’m not sure anymore. The landscape has never been more competitive, and the AGI race has no clear winner. The new paradigm favors late entrants while demanding agility—a skill I’m not sure Google has mastered yet. It’s also possible they’re simply terrible at marketing their advances, making their impending successes feel far less tangible than those of their competitors.</span></p><p><span>My conclusion for this post, however, is not going to be about the AI race. It’s about a lesson that keeps resurfacing, unsettling those who want to believe that human ingenuity will always triumph over simply </span><em>letting go</em><span>. I hate to break it to you, dear friends, but some things are simply beyond us.</span></p><p>Grok 3 is impressive. But more than anything, it’s yet another reminder that when it comes to building intelligence, scaling wins over raw cleverness every time.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Magma: A foundation model for multimodal AI agents (292 pts)]]></title>
            <link>https://microsoft.github.io/Magma/</link>
            <guid>43110265</guid>
            <pubDate>Thu, 20 Feb 2025 02:11:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://microsoft.github.io/Magma/">https://microsoft.github.io/Magma/</a>, See on <a href="https://news.ycombinator.com/item?id=43110265">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div>
                <p><img src="https://microsoft.github.io/Magma/static/images/logo.png" alt="Magma Logo"></p>
              </div>
  



  <div>
        <p><img src="https://microsoft.github.io/Magma/static/images/magma_teaser.png" alt="Image description" width="100%">
        </p>  
        <br> 
        <!-- <h2 class="subtitle has-text-centered"> -->
        <h2>
          <strong>Magma</strong> is the <span><em>first</em></span> foundation model that is capable of interpreting and grounding multimodal inputs within its environment. Given a described goal, <strong>Magma</strong> is able to formulate plans and execute actions to achieve it. By effectively transferring knowledge from freely available visual and language data, <strong>Magma</strong> bridges verbal, spatial and temporal intelligence to navigate complex tasks and settings.
        </h2>
        
      </div>

  <div>
        <div>
          <p>
            <h2>Abstract</h2>
          </p>
      </div>
    
      <div>
          <p>
            We present <strong>Magma</strong>, a foundation model serving multimodal AI agentic tasks in both the digital and physical worlds.  <strong>Magma</strong> is a significant extension of vision-language (VL) models in that the former not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial intelligence) and to complete agentic tasks ranging from UI navigation to robot manipulation.
            <strong>Magma</strong> is pretrained on large amounts of heterogeneous VL datasets including images, videos and robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) and the object movements (e.g., the trace of a robotic arm) in videos are labeled by Trace-of-Mark (ToM). 
            Extensive experiments show that SoM and ToM facilitate the acquisition of spatial intelligence from large-scale training data.
            <strong>Magma</strong> creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are tailored specifically to these tasks. On VL tasks, Magma also compares favorably to popular VL models that are trained on much larger datasets.
          </p>
        </div>
      </div>

  <div>
          <h2>Magma Pretraining</h2>
          <div>
              <div>
                <p><img src="https://microsoft.github.io/Magma/static/images/magma_pretrain.png">
                </p>
              </div>
              <p><strong>Magma pretraining pipeline. </strong>For all training data, texts are tokenized into tokens, while images and videos from different domains
                are encoded by a shared vision encoder. The resulted discrete and continuous tokens are then fed into a LLM to generate the outputs in
                verbal, spatial and action types.
              </p>
            </div>
        </div>

  <div>
          <h2>Magma Technical Details</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <div>

            
              
                <div>
                  <p><img src="https://microsoft.github.io/Magma/static/images/som_fig.jpg" alt="Image description" width="100%" ;="">
                  </p>
                </div>
                <div>
                  <p><strong>Set-of-Mark (SoM) for Action Grounding. </strong>Set-of-Mark prompting enables effective action grounding in images for both UI screenshot (left), robot manipulation (middle) and human video (right) by having the model predict numeric marks for clickable buttons or robot arms in image space.</p>
              </div>
              
            

            
              
                <div>
                  <p><img src="https://microsoft.github.io/Magma/static/images/tom_fig.png" alt="Image description" width="100%" ;="">
                  </p>
                </div>
                <div>
                  <p><strong>Trace-of-Mark (ToM) for Action Planning. </strong>Trace-of-Mark supervisions for robot manipulation (left) and human action (right). It compels the model to comprehend temporal video dynamics and anticipate future states before acting, while using fewer tokens than next-frame prediction to capture longer temporal horizons and action-related dynamics without ambient distractions.</p>
              </div>
              
            

            
              
                <div>
                  <p><img src="https://microsoft.github.io/Magma/static/images/magma_data_sunburst.png">
                    <img src="https://microsoft.github.io/Magma/static/images/magma_data_sample.png">
                </p>
                </div>
                <div>
                  <p>
                      <strong>Overview of Pretraining Data Sources</strong>: 
                      <span>instructional videos</span>, 
                      <span>robotics manipulation</span>, 
                      <span>UI navigation</span>, and 
                      <span>multimodal understanding.</span>
                      We apply SoM and ToM for different data types, with SoM enabling unified action grounding across all modalities while ToM is specifically applied to video and robotics data.
                  </p>
              </div>
              
            
          </div>
        </div>


  <div>
          <h2>Agentic Intelligence</h2>
          <div>

            
              
                <div>
                  <p><img src="https://microsoft.github.io/Magma/static/images/magma_agentic.png">
                </p>
                </div>
                <div>
                  <p><strong>Zero-shot evaluation on agentic intelligence.</strong> We report the results for pretrained Magma without any domain-specific finetuning.
                        Magma is the <strong>only model</strong> that can conduct the <strong>full task spectrum</strong>.</p>
              </div>
              
            
          </div>
        </div>

  <div>
      <h2>UI Navigation</h2>
      <div>
            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" height="80%">
                <source src="https://microsoft.github.io/Magma/static/videos/ui_weather_and_flight_mode.mp4" type="video/mp4">
              </video>
              <p>What's weather in Seattle? &amp; turn on flight mode</p>
            </div>
            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" height="80%">
                <source src="https://microsoft.github.io/Magma/static/videos/ui_wordle.mp4" type="video/mp4">
              </video>
              <p>Share and message this to Bob Steve, click send button to complete</p>
            </div>
          </div>
      <div>
          <div>
            <p><img src="https://microsoft.github.io/Magma/static/images/ui_efficient_mind2web.png">
            </p>
          </div>
          <p><strong>Efficient finetuning on Mind2Web for web UI navigation.</strong></p>
        </div>
      <div>
          <div>
            <p><img src="https://microsoft.github.io/Magma/static/images/ui_efficient_aitw.png">
            </p>
          </div>
          <p><strong>Efficient finetuning on AITW for mobile UI navigation.</strong></p>
        </div>

    </div>
  
  <div id="real">
      <h2>Robot Manipulation</h2>
      <div>
          <p>
              WidowX videos show real robot rollouts with <strong>Magma</strong> model. (Videos are sped up by 2.5x.)<br>
            </p>
        </div>

      

      

      <div>
            <p>Pick Place Hotdog Sausage</p>
            <p>Put Mushroom Place Pot</p>
            <p>Push Cloth Left to Right (Out-of-Dist.)</p>
          </div>

      <div>
          <p><strong>Few-shot finetuning on WidowX robot. </strong><strong>Magma</strong> consistently outperforms OpenVLA (finetuning) across soft object manipulation and pick-and-place operations, demonstrating reliable performance in both in-distribution and out-of-distribution generalization tasks on real robots.</p>
          <div>
            <div>
                <p><img src="https://microsoft.github.io/Magma/static/images/magma_real.png"></p><p>Magma on Real Robot</p>
              </div>
            <div>
                <p><img src="https://microsoft.github.io/Magma/static/images/magma_libero.png"></p><p>Magma on LIBERO</p>
              </div>
          </div>
          <p><strong>Few-shot finetuning on LIBERO. </strong> <strong>Magma</strong> achieves a significantly higher average success rate
            in all task suites. Additionally, removing SoM and ToM during pretraining has a negative impact on model performance.
          </p>
          <p><img src="https://microsoft.github.io/Magma/static/images/magma_simpler.png" alt="Image description" width="100%">
          </p>
          <p><strong>Zero-shot SimplerEnv evaluation on Google Robots and Bridge. </strong> <strong>Magma</strong> shows strong zero-shot cross-domain robustness and demonstrates impressive results in cross-embodiment manipulation simulation tasks.
          </p>
        </div>

    </div>

  <!--section class="section body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" style="text-align: center; padding-bottom: 10px; margin-top: -40px;">Simulation Benchmark</h2>
          <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">
          
            <tr>
              <td>
                <div class="columns is-centered ">
                  <div class="column">
                      <p style="text-align: left;font-size: 18px; margin-top: 10px;"><strong>Few-shot finetuning on LIBERO. </strong> <strong>Magma</strong> achieves a significantly higher average success rate
                        in all task suites. Additionally, removing SoM and ToM during pretraining has a negative impact on model performance.</p>
                  </div>
                </div>
                <div class="row">
                  <div class="col">
                    <img src="./static/images/magma_simpler.png" alt="Image description" width="100%">
                  </div>
                </div>
                <div class="columns is-centered ">
                  <div class="column">
                      <p style="text-align: left;font-size: 18px; margin-top: 10px;"><strong>Zero-shot SimplerEnv evaluation on Google Robots and Bridge. </strong> <strong>Magma</strong> shows strong zero-shot cross-domain robustness and demonstrates impressive results in cross-embodiment manipulation simulation tasks.</p>
                </div>
              </div>
              </td>
            </tr>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section-->

  <div>
          <h2>Spatial Reasoning</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <!--div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">

            <tr>
              <td>
                <div class="row">
                  <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; width: 100%;">
                    <img src="./static/images/magma_spatial_table.png" style="max-width: 750px; margin-bottom: 20px;">
                    <img src="./static/images/magma_spatial_visualizations.png" style="max-width: 750px;">
                  </div>
                </div>
                <div class="columns is-centered">
                  <div class="column">
                      <p style="text-align: left;font-size: 18px; margin-top: 20px;"><strong>Spatial reasoning evaluations.</strong> Spatial reasoning questions are challenging even for GPT-4o but Magma can answer relatively well despite relying on much fewer pretraining data</p>
                </div>
              </div>
              </td>
            </tr>
          </div>
          </div>
          </div-->
          <div>
              <div>
                <p><img src="https://microsoft.github.io/Magma/static/images/magma_spatial_table.png">
                </p>
                <p><img src="https://microsoft.github.io/Magma/static/images/magma_spatial_visualizations.png">
                </p>
              </div>
              <p><strong>Spatial reasoning evaluations.</strong> Spatial reasoning questions are challenging even for GPT-4o but Magma can answer relatively well despite relying on much fewer pretraining data.</p>
            </div>
        </div>

  <div>
          <h2>Multimodal Understanding</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <div>

            
              
                <div>
                  <p><img src="https://microsoft.github.io/Magma/static/images/magma_video_qa.png">
                </p>
                </div>
                <div>
                  <p><strong>Zero-shot Video QA benchmarks.</strong> <strong>Magma</strong> performs competitively and even outperforms some state-of-the-art approaches such as Video-Llama2 and ShareGPT4Video on most benchmarks, despite using much fewer video instruction tuning data.</p>
              </div>
              
            
              
            </div>
          </div>

  <div id="real">
      <!-- <hr width="100%" size="2"> -->
      <h2>Video conversation examples</h2>
      

      <div>
            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" height="80%">
                <source src="https://microsoft.github.io/Magma/static/videos/magma_chess.mp4" type="video/mp4">
              </video>
              <p><strong><u>User</u></strong>: I really want to win this game. Can you suggest the next move that I should make to maximize my chances of winning?</p>
              <p><strong><u>Magma</u></strong>: I suggest moving the black pawn to d4 to maximize your chances of winning.</p>
            </div>
            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" height="80%">
                <source src="https://microsoft.github.io/Magma/static/videos/magma_relaxation.mp4" type="video/mp4">
              </video>
              <p><strong><u>User</u></strong>: I want to relax for a few hours in here. Can you suggest some things for me to do?</p>
              <p><strong><u>Magma</u></strong>: Watch TV, read a book, and enjoy the view of the autumn forest.</p>
            </div>
          </div>

      <div>
            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" height="80%">
                <source src="https://microsoft.github.io/Magma/static/videos/magma_counter_checkout.mp4" type="video/mp4">
              </video>
              <p><strong><u>User</u></strong>: Please provide a detailed description of the video, focusing on the main subject and the actions taking place as well as predict what is going to happen next.</p>
              <p><strong><u>Magma</u></strong>: The video takes place in a convenience store, focusing on the checkout counter. It begins with a view of the counter displaying a sign that reads 'PICK 5 FOR $8.00' along with images of various snacks. The counter is filled with various items, including snacks and a payment terminal. An employee, wearing a black shirt with white sleeves, is seen handling the payment terminal. A customer, identifiable by their white sleeve and colorful bracelet, places a red shopping basket filled with snacks and a drink on the counter. The employee continues to operate the payment terminal while the customer adjusts the items in the basket. The interaction between the customer and the employee is highlighted, with the employee assisting the customer with the items.                 The scene continues at the same checkout counter, with the 'PICK 5 FOR $8.00' sign still prominently displayed. The customer's red shopping basket, now filled with snacks including a drink cup labeled 'Fruit Swoosh' and a blue package of cookies, is placed on the counter. The employee is seen handling the payment terminal. The customer's hand, adorned with a colorful bracelet, interacts with the items in the basket. The employee continues to assist the customer, adjusting the items on the counter. The interaction between the customer and the employee remains the focal point, with the employee assisting the customer with the items. The person behind the counter is likely to process the payment and possibly hand over the items.</p>
            </div>

            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" width="100%" height="80%">
                <source src="https://microsoft.github.io/Magma/static/videos/magma_potted_plant.mp4" type="video/mp4">
              </video>
              <p><strong><u>User</u></strong>: Please describe in detail what is happening in the video by focusing on the main actions.</p>
              <p><strong><u>Magma</u></strong>: The video begins with a close-up of a hand reaching for a red potted plant placed on a wooden shelf. The plant has green and yellow-striped leaves and is accompanied by a book titled 'YOUNG' and a wooden bowl stacked on top of each other. The hand carefully adjusts the position of the plant on the shelf, ensuring it is properly placed. The background includes a beige watering can hanging on a stand and a radiator with white pipes. The hand then adjusts the plant once more, making sure it is centered on the shelf. The scene develops as the hand, now wearing a yellow sweater and a ring on the left hand, reaches out to adjust the plant's position once more. The hand then moves away from the plant, and the camera shifts to show more of the surrounding area, including a red high chair and a wooden countertop with various items, such as a smartphone and a small red cup. The video wraps up with a wider view of the room, revealing a checkered floor and several other plants in the background, adding a homely touch to the setting.</p>
            </div>
          </div>
    </div>

<div id="real">
        <div>
          <video autoplay="" muted="" loop="" controls="" playsinline="" width="100%" height="80%">
            <source src="https://microsoft.github.io/Magma/static/videos/rayban-video-1.mp4" type="video/mp4">
          </video>
          <p><strong><u>User</u></strong>: Please describe in detail what is happening in the video by focusing on the main actions.</p>
          <p><strong><u>Magma</u></strong>: The video begins with a close-up view of a kitchen sink area. The sink has a shiny silver faucet with water running, and a person is holding a black mug under the running water. On the left side of the sink, there is a bottle of dish soap and a blue toothbrush. The person then picks up a blue sponge and starts scrubbing the inside of the mug. The scene continues with the person methodically scrubbing the mug, ensuring it is clean. The video wraps up with the person placing the scrubbed mug in the sink and the blue sponge beside it, indicating that the cleaning process is complete.</p>
        </div>
        <div>
          <video autoplay="" muted="" loop="" controls="" playsinline="" width="100%" height="80%">
            <source src="https://microsoft.github.io/Magma/static/videos/rayban-video-2.MOV" type="video/mp4">
          </video>
          <p><strong><u>User</u></strong>: What drink did the person choose in the video?</p>
          <p><strong><u>Magma</u></strong>: The person chose a soy beverage labeled as 'SAVE SOY'.</p> 
        </div>
      </div>

<div id="real">
      <div>
        <video autoplay="" muted="" loop="" controls="" playsinline="" width="100%" height="80%">
          <source src="https://microsoft.github.io/Magma/static/videos/rayban-video-3.MOV" type="video/mp4">
        </video>
        <p><strong><u>User</u></strong>: Can you predict the next actions that the person is going to carry out beyond the video?</p>
        <p><strong><u>Magma</u></strong>: The person is likely about to pour hot water into the cup and proceed with the tea preparation process.</p>
      </div>
      <div>
        <video autoplay="" muted="" loop="" controls="" playsinline="" width="100%" height="80%">
          <source src="https://microsoft.github.io/Magma/static/videos/rayban-video-5.MOV" type="video/mp4">
        </video>
        <p><strong><u>User</u></strong>: Can you describe what the person is doing in the video?</p>
        <p><strong><u>Magma</u></strong>: The person is interacting with the coffee machine by pressing buttons, likely to make a coffee choice.</p>
      </div>
    </div>

  <div id="BibTeX">
      <h2>BibTeX</h2>
      <pre><code>@misc{yang2025magmafoundationmodelmultimodal,
        title={Magma: A Foundation Model for Multimodal AI Agents}, 
        author={Jianwei Yang and Reuben Tan and Qianhui Wu and Ruijie Zheng and Baolin Peng and Yongyuan Liang and Yu Gu and Mu Cai and Seonghyeon Ye and Joel Jang and Yuquan Deng and Lars Liden and Jianfeng Gao},
        year={2025},
        eprint={2502.13130},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2502.13130},
      }</code></pre>
    </div>
     

  





</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mexico issues legal threat to Google (106 pts)]]></title>
            <link>https://thecomeback.com/politics/mexico-legal-action-donald-trump-executive-order.html</link>
            <guid>43110226</guid>
            <pubDate>Thu, 20 Feb 2025 02:03:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thecomeback.com/politics/mexico-legal-action-donald-trump-executive-order.html">https://thecomeback.com/politics/mexico-legal-action-donald-trump-executive-order.html</a>, See on <a href="https://news.ycombinator.com/item?id=43110226">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

		<article id="main">
			
			<!-- .entry-header -->

			<div>
				<main role="main">

				
	<img width="832" height="447" src="https://cdn1.thecomeback.com/wp-content/uploads/2023/04/USATSI_19306971-scaled-e1686251008738-832x447.jpg" alt="Oct 27, 2022; Miami, Florida, USA; Former President Donald Trump walks onto the 18th green during the Pro-Am tournament before the LIV Golf series at Trump National Doral. Mandatory Credit: John David Mercer-USA TODAY Sports" decoding="async" fetchpriority="high" srcset="https://thecomeback.com/wp-content/uploads/2023/04/USATSI_19306971-scaled-e1686251008738-832x447.jpg 832w, https://thecomeback.com/wp-content/uploads/2023/04/USATSI_19306971-scaled-e1686251008738-300x161.jpg 300w, https://thecomeback.com/wp-content/uploads/2023/04/USATSI_19306971-scaled-e1686251008738-1024x551.jpg 1024w, https://thecomeback.com/wp-content/uploads/2023/04/USATSI_19306971-scaled-e1686251008738-768x413.jpg 768w, https://thecomeback.com/wp-content/uploads/2023/04/USATSI_19306971-scaled-e1686251008738.jpg 1430w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px">    <span>John David Mercer-USA TODAY Sports</span>

	<!-- .entry-meta -->

	<div>
		<!-- Arbitrary hidden heading for SumoMe to hook into -->
		
<p>On Jan. 20, United States President Donald Trump signed an executive order renaming the “Gulf of Mexico” to “Gulf of America.” Last week, Google Maps officially changed the name of the gulf on its maps, sparking a flurry of memes and conversation on social media, and Mexico is now threatening legal action if they don’t change it back.</p>
<p>In a&nbsp;press conference on Monday, Mexican President Claudia Sheinbaum said that Mexico will sue Google if the company does not change the “Gulf of America” label to cover only the portion of the body of water under U.S. jurisdiction, which is just a small portion of the gulf.</p>

<p>Sheinbaum argues that Donald Trump and the United States do not have the authority to rename the entire gulf and that the executive order applies only to the United States continental shelf.</p>
<p>“We’re talking about 22 nautical miles, not the entire gulf,” Sheinbaum said <a href="https://www.businessinsider.com/mexico-lawsuit-threat-google-maps-gulf-of-america-2025-2">according to Business Insider</a>.</p>

<p>Sheinbaum pointed out that Google has also changed the name of the continental shelves of Mexico and Cuba when those two countries have not changed the name themselves.</p>

<p>In a letter to Google, Sheinbaum made it clear that Mexico will not&nbsp;accept the renaming of any geographical area that includes its national territory or that falls under its jurisdiction.</p>
<p>“Any reference to the ‘Gulf of America’ initiative on your Google Maps platform must be exclusively limited to the marine area under US jurisdiction,” the letter read. “Any extension beyond that zone exceeds the authority of any national government or private entity. Should that be the case, the Government of Mexico will take the appropriate legal actions as deemed necessary.”</p>

<p>Sheinbaum said that it’s possible that the nation could sue Google in civil court over the matter, but they will await Google’s response to their most recent letter before proceeding with legal action.</p>
<!-- CONTENT END 1 -->
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				

				
					
					

					

				
				
	<div id="author-box">
							<p><img src="https://secure.gravatar.com/avatar/daa5cebd0847565a04e3f18468700f23?s=96&amp;r=pg" alt=""></p><h2>About Kevin Harrish</h2>
			
			
		</div>

	
									
				
				

				
				</main><!-- #main -->
			</div><!-- .primary -->

						
<!-- .secondary -->

			

		</article>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Run structured extraction on documents/images locally with Ollama and Pydantic (159 pts)]]></title>
            <link>https://github.com/vlm-run/vlmrun-hub</link>
            <guid>43110173</guid>
            <pubDate>Thu, 20 Feb 2025 01:54:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/vlm-run/vlmrun-hub">https://github.com/vlm-run/vlmrun-hub</a>, See on <a href="https://news.ycombinator.com/item?id=43110173">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Welcome to <strong>VLM Run Hub</strong>, a comprehensive repository of pre-defined <a href="https://docs.pydantic.dev/latest/" rel="nofollow">Pydantic</a> schemas for extracting structured data from unstructured visual domains such as images, videos, and documents. Designed for <a href="https://huggingface.co/blog/vlms" rel="nofollow">Vision Language Models (VLMs)</a> and optimized for real-world use cases, VLM Run Hub simplifies the integration of visual ETL into your workflows.</p>
<markdown-accessiblity-table><table>
<tbody><tr>
<td> <b>Image</b> </td>
<td> <b>JSON</b> </td>
</tr>
<tr>
<td>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a09edb5ef4682626db4db41658d0917100b57c8bbcb82e79c6fa1c67acd55a19/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f766c6d2d646174612d7075626c69632d70726f642f6875622f6578616d706c65732f646f63756d656e742e75732d647269766572732d6c6963656e73652f646c332e6a7067"><img src="https://camo.githubusercontent.com/a09edb5ef4682626db4db41658d0917100b57c8bbcb82e79c6fa1c67acd55a19/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f766c6d2d646174612d7075626c69632d70726f642f6875622f6578616d706c65732f646f63756d656e742e75732d647269766572732d6c6963656e73652f646c332e6a7067" data-canonical-src="https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.us-drivers-license/dl3.jpg"></a>
</td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;issuing_state&quot;: &quot;MT&quot;,
  &quot;license_number&quot;: &quot;0812319684104&quot;,
  &quot;first_name&quot;: &quot;Brenda&quot;,
  &quot;middle_name&quot;: &quot;Lynn&quot;,
  &quot;last_name&quot;: &quot;Sample&quot;,
  &quot;address&quot;: {
    &quot;street&quot;: &quot;123 MAIN STREET&quot;,
    &quot;city&quot;: &quot;HELENA&quot;,
    &quot;state&quot;: &quot;MT&quot;,
    &quot;zip_code&quot;: &quot;59601&quot;
  },
  &quot;date_of_birth&quot;: &quot;1968-08-04&quot;,
  &quot;gender&quot;: &quot;F&quot;,
  &quot;height&quot;: &quot;5'06\&quot;&quot;,
  &quot;weight&quot;: 150.0,
  &quot;eye_color&quot;: &quot;BRO&quot;,
  &quot;issue_date&quot;: &quot;2015-02-15&quot;,
  &quot;expiration_date&quot;: &quot;2023-08-04&quot;,
  &quot;license_class&quot;: &quot;D&quot;
}"><pre>{
  <span>"issuing_state"</span>: <span><span>"</span>MT<span>"</span></span>,
  <span>"license_number"</span>: <span><span>"</span>0812319684104<span>"</span></span>,
  <span>"first_name"</span>: <span><span>"</span>Brenda<span>"</span></span>,
  <span>"middle_name"</span>: <span><span>"</span>Lynn<span>"</span></span>,
  <span>"last_name"</span>: <span><span>"</span>Sample<span>"</span></span>,
  <span>"address"</span>: {
    <span>"street"</span>: <span><span>"</span>123 MAIN STREET<span>"</span></span>,
    <span>"city"</span>: <span><span>"</span>HELENA<span>"</span></span>,
    <span>"state"</span>: <span><span>"</span>MT<span>"</span></span>,
    <span>"zip_code"</span>: <span><span>"</span>59601<span>"</span></span>
  },
  <span>"date_of_birth"</span>: <span><span>"</span>1968-08-04<span>"</span></span>,
  <span>"gender"</span>: <span><span>"</span>F<span>"</span></span>,
  <span>"height"</span>: <span><span>"</span>5'06<span>\"</span><span>"</span></span>,
  <span>"weight"</span>: <span>150.0</span>,
  <span>"eye_color"</span>: <span><span>"</span>BRO<span>"</span></span>,
  <span>"issue_date"</span>: <span><span>"</span>2015-02-15<span>"</span></span>,
  <span>"expiration_date"</span>: <span><span>"</span>2023-08-04<span>"</span></span>,
  <span>"license_class"</span>: <span><span>"</span>D<span>"</span></span>
}</pre></div>
</td>
</tr>
</tbody></table></markdown-accessiblity-table>

<p dir="auto"><h3 tabindex="-1" dir="auto">💡 Motivation</h3><a id="user-content--motivation" aria-label="Permalink: 💡 Motivation" href="#-motivation"></a></p>
<p dir="auto">While vision models like OpenAI’s <a href="https://openai.com/index/hello-gpt-4o/" rel="nofollow">GPT-4o</a> and Anthropic’s <a href="https://www.anthropic.com/claude" rel="nofollow">Claude Vision</a> excel in exploratory tasks like "chat with images," they often lack practicality for automation and integration, where <strong>strongly-typed</strong>, <strong>validated outputs</strong> are crucial.</p>
<p dir="auto">The <strong>Structured Outputs API</strong> (popularized by <a href="https://openai.com/index/introducing-structured-outputs-in-the-api/" rel="nofollow">GPT-4o</a>, <a href="https://ai.google.dev/gemini-api/docs/structured-output" rel="nofollow">Gemini</a>) addresses this by constraining LLMs to return data in precise, strongly-typed formats such as <a href="https://docs.pydantic.dev/latest/" rel="nofollow">Pydantic</a> models. This eliminates complex parsing and validation, ensuring outputs conform to expected types and structures. These schemas can be nested and include complex types like lists and dictionaries, enabling seamless integration with existing systems while leveraging the full capabilities of the model.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">🧰 Why use this hub of pre-defined Pydantic schemas?</h3><a id="user-content--why-use-this-hub-of-pre-defined-pydantic-schemas" aria-label="Permalink: 🧰 Why use this hub of pre-defined Pydantic schemas?" href="#-why-use-this-hub-of-pre-defined-pydantic-schemas"></a></p>
<ul dir="auto">
<li>📚 <strong>Easy to use:</strong> <a href="https://docs.pydantic.dev/latest/" rel="nofollow">Pydantic</a> is a well-understood and battle-tested data model for structured data.</li>
<li>🔋 <strong>Batteries included:</strong>  Each schema in this repo has been validated across real-world industry use cases—from healthcare to finance to media—saving you weeks of development effort.</li>
<li>🔍 <strong>Automatic Data-validation:</strong> Built-in <a href="https://docs.pydantic.dev/latest/concepts/validators/" rel="nofollow">Pydantic validation</a> ensures your extracted data is clean, accurate, and reliable, reducing errors and simplifying downstream workflows.</li>
<li>🔌 <strong>Type-safety:</strong> With <a href="https://docs.pydantic.dev/latest/concepts/types/" rel="nofollow">Pydantic’s type-safety</a> and compatibility with tools like <code>mypy</code> and <code>pyright</code>, you can build composable, modular systems that are robust and maintainable.</li>
<li>🧰 <strong>Model-agnostic:</strong> Use the same schema with multiple VLM providers, no need to rewrite prompts for different VLMs.</li>
<li>🚀 <strong>Optimized for Visual ETL:</strong> Purpose-built for extracting structured data from images, videos, and documents, this repo bridges the gap between unstructured data and actionable insights.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">📖 Schema Catalog</h3><a id="user-content--schema-catalog" aria-label="Permalink: 📖 Schema Catalog" href="#-schema-catalog"></a></p>
<p dir="auto">The VLM Run Hub maintains a comprehensive catalog of all available schemas in the <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/catalog.yaml"><code>vlmrun/hub/catalog.yaml</code></a> file. The catalog is automatically validated to ensure consistency and completeness of schema documentation. We refer the developer to the <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/docs/catalog-spec.yaml">catalog-spec.yaml</a> for the full YAML specification. Here are some featured schemas:</p>
<ul dir="auto">
<li>Documents: <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/document/bank_statement.py">document.bank-statement</a>, <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/document/invoice.py">document.invoice</a>, <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/document/receipt.py">document.receipt</a>, <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/document/resume.py">document.resume</a>, <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/document/us_drivers_license.py">document.us-drivers-license</a>, <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/document/utility_bill.py">document.utility-bill</a>, <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/document/w2_form.py">document.w2-form</a></li>
<li>Other industry-specific schemas: <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/healthcare/medical_insurance_card.py">healthcare.medical-insurance-card</a>, <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/retail/ecommerce_product_caption.py">retail.ecommerce-product-caption</a>, <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/media/tv_news.py">media.tv-news</a>, <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/aerospace/remote_sensing.py">aerospace.remote-sensing</a></li>
</ul>
<p dir="auto">If you have a new schema you want to add to the catalog, please refer to the <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/docs/SCHEMA-GUIDELINES.md">SCHEMA-GUIDELINES.md</a> for the full guidelines.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">🚀 Getting Started</h3><a id="user-content--getting-started" aria-label="Permalink: 🚀 Getting Started" href="#-getting-started"></a></p>
<p dir="auto">Let's say we want to extract invoice metadata from an <a href="https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg" rel="nofollow">invoice image</a>. You can readily use our <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/schemas/document/invoice.py"><code>Invoice</code></a> schema we have defined under <code>vlmrun.hub.schemas.document.invoice</code> and use it with any VLM of your choosing.</p>
<p dir="auto">For a comprehensive walkthrough of available schemas and their usage, check out our <a href="https://github.com/vlm-run/vlmrun-cookbook/blob/main/notebooks/01_schema_showcase.ipynb">Schema Showcase Notebook</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">💾 Installation</h3><a id="user-content--installation" aria-label="Permalink: 💾 Installation" href="#-installation"></a></p>

<p dir="auto"><h4 tabindex="-1" dir="auto">With <a href="https://github.com/jxnl/instructor">Instructor</a> / OpenAI</h4><a id="user-content-with-instructor--openai" aria-label="Permalink: With Instructor / OpenAI" href="#with-instructor--openai"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import instructor
from openai import OpenAI

from vlmrun.hub.schemas.document.invoice import Invoice

IMAGE_URL = &quot;https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg&quot;

client = instructor.from_openai(
    OpenAI(), mode=instructor.Mode.MD_JSON
)
response = client.chat.completions.create(
    model=&quot;gpt-4o-mini&quot;,
    messages=[
        { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Extract the invoice in JSON.&quot;},
            {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: IMAGE_URL}, &quot;detail&quot;: &quot;auto&quot;}
        ]}
    ],
    response_model=Invoice,
    temperature=0,
)"><pre><span>import</span> <span>instructor</span>
<span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span>

<span>from</span> <span>vlmrun</span>.<span>hub</span>.<span>schemas</span>.<span>document</span>.<span>invoice</span> <span>import</span> <span>Invoice</span>

<span>IMAGE_URL</span> <span>=</span> <span>"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg"</span>

<span>client</span> <span>=</span> <span>instructor</span>.<span>from_openai</span>(
    <span>OpenAI</span>(), <span>mode</span><span>=</span><span>instructor</span>.<span>Mode</span>.<span>MD_JSON</span>
)
<span>response</span> <span>=</span> <span>client</span>.<span>chat</span>.<span>completions</span>.<span>create</span>(
    <span>model</span><span>=</span><span>"gpt-4o-mini"</span>,
    <span>messages</span><span>=</span>[
        { <span>"role"</span>: <span>"user"</span>, <span>"content"</span>: [
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"Extract the invoice in JSON."</span>},
            {<span>"type"</span>: <span>"image_url"</span>, <span>"image_url"</span>: {<span>"url"</span>: <span>IMAGE_URL</span>}, <span>"detail"</span>: <span>"auto"</span>}
        ]}
    ],
    <span>response_model</span><span>=</span><span>Invoice</span>,
    <span>temperature</span><span>=</span><span>0</span>,
)</pre></div>
<details>
<summary>JSON Response:</summary>
<markdown-accessiblity-table><table>
<tbody><tr>
<td> Image </td>
<td> JSON Output 🔐 </td>
</tr>
<tr>
<td>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/fce88f797bfcda936d3acfc0a6627de1708b95130c42c87513d0b97d43976e2b/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f766c6d2d646174612d7075626c69632d70726f642f6875622f6578616d706c65732f646f63756d656e742e696e766f6963652f696e766f6963655f312e6a7067"><img src="https://camo.githubusercontent.com/fce88f797bfcda936d3acfc0a6627de1708b95130c42c87513d0b97d43976e2b/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f766c6d2d646174612d7075626c69632d70726f642f6875622f6578616d706c65732f646f63756d656e742e696e766f6963652f696e766f6963655f312e6a7067" data-canonical-src="https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg"></a>
</td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;invoice_id&quot;: &quot;9999999&quot;,
  &quot;period_start&quot;: null,
  &quot;period_end&quot;: null,
  &quot;invoice_issue_date&quot;: &quot;2023-11-11&quot;,
  &quot;invoice_due_date&quot;: null,
  &quot;order_id&quot;: null,
  &quot;customer_id&quot;: null,
  &quot;issuer&quot;: &quot;Anytown, USA&quot;,
  &quot;issuer_address&quot;: {
    &quot;street&quot;: &quot;123 Main Street&quot;,
    &quot;city&quot;: &quot;Anytown&quot;,
    &quot;state&quot;: &quot;USA&quot;,
    &quot;postal_code&quot;: &quot;01234&quot;,
    &quot;country&quot;: null
  },
  &quot;customer&quot;: &quot;Fred Davis&quot;,
  &quot;customer_email&quot;: &quot;email@invoice.com&quot;,
  &quot;customer_phone&quot;: &quot;(800) 123-4567&quot;,
  &quot;customer_billing_address&quot;: {
    &quot;street&quot;: &quot;1335 Martin Luther King Jr Ave&quot;,
    &quot;city&quot;: &quot;Dunedin&quot;,
    &quot;state&quot;: &quot;FL&quot;,
    &quot;postal_code&quot;: &quot;34698&quot;,
    &quot;country&quot;: null
  },
  &quot;customer_shipping_address&quot;: {
    &quot;street&quot;: &quot;249 Windward Passage&quot;,
    &quot;city&quot;: &quot;Clearwater&quot;,
    &quot;state&quot;: &quot;FL&quot;,
    &quot;postal_code&quot;: &quot;33767&quot;,
    &quot;country&quot;: null
  },
  &quot;items&quot;: [
    {
      &quot;description&quot;: &quot;Service&quot;,
      &quot;quantity&quot;: 1,
      &quot;currency&quot;: null,
      &quot;unit_price&quot;: 200.0,
      &quot;total_price&quot;: 200.0
    },
    {
      &quot;description&quot;: &quot;Parts AAA&quot;,
      &quot;quantity&quot;: 1,
      &quot;currency&quot;: null,
      &quot;unit_price&quot;: 100.0,
      &quot;total_price&quot;: 100.0
    },
    {
      &quot;description&quot;: &quot;Parts BBB&quot;,
      &quot;quantity&quot;: 2,
      &quot;currency&quot;: null,
      &quot;unit_price&quot;: 50.0,
      &quot;total_price&quot;: 100.0
    }
  ],
  &quot;subtotal&quot;: 400.0,
  &quot;tax&quot;: null,
  &quot;total&quot;: 400.0,
  &quot;currency&quot;: null,
  &quot;notes&quot;: &quot;&quot;,
  &quot;others&quot;: null
}"><pre>{
  <span>"invoice_id"</span>: <span><span>"</span>9999999<span>"</span></span>,
  <span>"period_start"</span>: <span>null</span>,
  <span>"period_end"</span>: <span>null</span>,
  <span>"invoice_issue_date"</span>: <span><span>"</span>2023-11-11<span>"</span></span>,
  <span>"invoice_due_date"</span>: <span>null</span>,
  <span>"order_id"</span>: <span>null</span>,
  <span>"customer_id"</span>: <span>null</span>,
  <span>"issuer"</span>: <span><span>"</span>Anytown, USA<span>"</span></span>,
  <span>"issuer_address"</span>: {
    <span>"street"</span>: <span><span>"</span>123 Main Street<span>"</span></span>,
    <span>"city"</span>: <span><span>"</span>Anytown<span>"</span></span>,
    <span>"state"</span>: <span><span>"</span>USA<span>"</span></span>,
    <span>"postal_code"</span>: <span><span>"</span>01234<span>"</span></span>,
    <span>"country"</span>: <span>null</span>
  },
  <span>"customer"</span>: <span><span>"</span>Fred Davis<span>"</span></span>,
  <span>"customer_email"</span>: <span><span>"</span>email@invoice.com<span>"</span></span>,
  <span>"customer_phone"</span>: <span><span>"</span>(800) 123-4567<span>"</span></span>,
  <span>"customer_billing_address"</span>: {
    <span>"street"</span>: <span><span>"</span>1335 Martin Luther King Jr Ave<span>"</span></span>,
    <span>"city"</span>: <span><span>"</span>Dunedin<span>"</span></span>,
    <span>"state"</span>: <span><span>"</span>FL<span>"</span></span>,
    <span>"postal_code"</span>: <span><span>"</span>34698<span>"</span></span>,
    <span>"country"</span>: <span>null</span>
  },
  <span>"customer_shipping_address"</span>: {
    <span>"street"</span>: <span><span>"</span>249 Windward Passage<span>"</span></span>,
    <span>"city"</span>: <span><span>"</span>Clearwater<span>"</span></span>,
    <span>"state"</span>: <span><span>"</span>FL<span>"</span></span>,
    <span>"postal_code"</span>: <span><span>"</span>33767<span>"</span></span>,
    <span>"country"</span>: <span>null</span>
  },
  <span>"items"</span>: [
    {
      <span>"description"</span>: <span><span>"</span>Service<span>"</span></span>,
      <span>"quantity"</span>: <span>1</span>,
      <span>"currency"</span>: <span>null</span>,
      <span>"unit_price"</span>: <span>200.0</span>,
      <span>"total_price"</span>: <span>200.0</span>
    },
    {
      <span>"description"</span>: <span><span>"</span>Parts AAA<span>"</span></span>,
      <span>"quantity"</span>: <span>1</span>,
      <span>"currency"</span>: <span>null</span>,
      <span>"unit_price"</span>: <span>100.0</span>,
      <span>"total_price"</span>: <span>100.0</span>
    },
    {
      <span>"description"</span>: <span><span>"</span>Parts BBB<span>"</span></span>,
      <span>"quantity"</span>: <span>2</span>,
      <span>"currency"</span>: <span>null</span>,
      <span>"unit_price"</span>: <span>50.0</span>,
      <span>"total_price"</span>: <span>100.0</span>
    }
  ],
  <span>"subtotal"</span>: <span>400.0</span>,
  <span>"tax"</span>: <span>null</span>,
  <span>"total"</span>: <span>400.0</span>,
  <span>"currency"</span>: <span>null</span>,
  <span>"notes"</span>: <span><span>"</span><span>"</span></span>,
  <span>"others"</span>: <span>null</span>
}</pre></div>
</td>
</tr>
</tbody></table></markdown-accessiblity-table>
</details>
<p dir="auto"><h4 tabindex="-1" dir="auto">With <a href="https://vlm.run/" rel="nofollow">VLM Run</a></h4><a id="user-content-with-vlm-run" aria-label="Permalink: With VLM Run" href="#with-vlm-run"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import requests

from vlmrun.hub.schemas.document.invoice import Invoice


IMAGE_URL = &quot;https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg&quot;

json_data = {
    &quot;image&quot;: IMAGE_URL,
    &quot;model&quot;: &quot;vlm-1&quot;,
    &quot;domain&quot;: &quot;document.invoice&quot;,
    &quot;json_schema&quot;: Invoice.model_json_schema(),
}
response = requests.post(
    f&quot;https://api.vlm.run/v1/image/generate&quot;,
    headers={&quot;Authorization&quot;: f&quot;Bearer <your-api-key>&quot;},
    json=json_data,
)"><pre><span>import</span> <span>requests</span>

<span>from</span> <span>vlmrun</span>.<span>hub</span>.<span>schemas</span>.<span>document</span>.<span>invoice</span> <span>import</span> <span>Invoice</span>


<span>IMAGE_URL</span> <span>=</span> <span>"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg"</span>

<span>json_data</span> <span>=</span> {
    <span>"image"</span>: <span>IMAGE_URL</span>,
    <span>"model"</span>: <span>"vlm-1"</span>,
    <span>"domain"</span>: <span>"document.invoice"</span>,
    <span>"json_schema"</span>: <span>Invoice</span>.<span>model_json_schema</span>(),
}
<span>response</span> <span>=</span> <span>requests</span>.<span>post</span>(
    <span>f"https://api.vlm.run/v1/image/generate"</span>,
    <span>headers</span><span>=</span>{<span>"Authorization"</span>: <span>f"Bearer &lt;your-api-key&gt;"</span>},
    <span>json</span><span>=</span><span>json_data</span>,
)</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">With <a href="https://platform.openai.com/docs/guides/structured-outputs" rel="nofollow">OpenAI Structured Outputs API</a></h4><a id="user-content-with-openai-structured-outputs-api" aria-label="Permalink: With OpenAI Structured Outputs API" href="#with-openai-structured-outputs-api"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import instructor
from openai import OpenAI

from vlmrun.hub.schemas.document.invoice import Invoice

IMAGE_URL = &quot;https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg&quot;

client = OpenAI()
completion = client.beta.chat.completions.parse(
    model=&quot;gpt-4o-mini&quot;,
    messages=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Extract the invoice in JSON.&quot;},
            {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: IMAGE_URL}, &quot;detail&quot;: &quot;auto&quot;}
        ]},
    ],
    response_format=Invoice,
    temperature=0,
)"><pre><span>import</span> <span>instructor</span>
<span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span>

<span>from</span> <span>vlmrun</span>.<span>hub</span>.<span>schemas</span>.<span>document</span>.<span>invoice</span> <span>import</span> <span>Invoice</span>

<span>IMAGE_URL</span> <span>=</span> <span>"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg"</span>

<span>client</span> <span>=</span> <span>OpenAI</span>()
<span>completion</span> <span>=</span> <span>client</span>.<span>beta</span>.<span>chat</span>.<span>completions</span>.<span>parse</span>(
    <span>model</span><span>=</span><span>"gpt-4o-mini"</span>,
    <span>messages</span><span>=</span>[
        {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: [
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"Extract the invoice in JSON."</span>},
            {<span>"type"</span>: <span>"image_url"</span>, <span>"image_url"</span>: {<span>"url"</span>: <span>IMAGE_URL</span>}, <span>"detail"</span>: <span>"auto"</span>}
        ]},
    ],
    <span>response_format</span><span>=</span><span>Invoice</span>,
    <span>temperature</span><span>=</span><span>0</span>,
)</pre></div>
<blockquote>
<p dir="auto">When working with the OpenAI Structured Outputs API, you need to ensure that the <code>response_format</code> is a valid Pydantic model with the <a href="https://platform.openai.com/docs/guides/structured-outputs#supported-schemas" rel="nofollow">supported types</a>.</p>
</blockquote>
<p dir="auto"><h4 tabindex="-1" dir="auto">Locally with <a href="https://ollama.com/" rel="nofollow">Ollama</a></h4><a id="user-content-locally-with-ollama" aria-label="Permalink: Locally with Ollama" href="#locally-with-ollama"></a></p>
<p dir="auto">Note: For certain <code>vlmrun.common</code> utilities, you will need to install our main <a href="https://github.com/vlm-run/vlmrun-python-sdk">Python SDK</a>
via <code>pip install vlmrun</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from ollama import chat

from vlmrun.common.image import encode_image
from vlmrun.common.utils import remote_image
from vlmrun.hub.schemas.document.invoice import Invoice


IMAGE_URL = &quot;https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg&quot;

img = remote_image(IMAGE_URL)
chat_response = chat(
    model=&quot;llama3.2-vision:11b&quot;,
    format=Invoice.model_json_schema(),
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Extract the invoice in JSON.&quot;,
            &quot;images&quot;: [encode_image(img, format=&quot;JPEG&quot;).split(&quot;,&quot;)[1]],
        },
    ],
    options={
        &quot;temperature&quot;: 0
    },
)
response = Invoice.model_validate_json(
    chat_response.message.content
)"><pre><span>from</span> <span>ollama</span> <span>import</span> <span>chat</span>

<span>from</span> <span>vlmrun</span>.<span>common</span>.<span>image</span> <span>import</span> <span>encode_image</span>
<span>from</span> <span>vlmrun</span>.<span>common</span>.<span>utils</span> <span>import</span> <span>remote_image</span>
<span>from</span> <span>vlmrun</span>.<span>hub</span>.<span>schemas</span>.<span>document</span>.<span>invoice</span> <span>import</span> <span>Invoice</span>


<span>IMAGE_URL</span> <span>=</span> <span>"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg"</span>

<span>img</span> <span>=</span> <span>remote_image</span>(<span>IMAGE_URL</span>)
<span>chat_response</span> <span>=</span> <span>chat</span>(
    <span>model</span><span>=</span><span>"llama3.2-vision:11b"</span>,
    <span>format</span><span>=</span><span>Invoice</span>.<span>model_json_schema</span>(),
    <span>messages</span><span>=</span>[
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: <span>"Extract the invoice in JSON."</span>,
            <span>"images"</span>: [<span>encode_image</span>(<span>img</span>, <span>format</span><span>=</span><span>"JPEG"</span>).<span>split</span>(<span>","</span>)[<span>1</span>]],
        },
    ],
    <span>options</span><span>=</span>{
        <span>"temperature"</span>: <span>0</span>
    },
)
<span>response</span> <span>=</span> <span>Invoice</span>.<span>model_validate_json</span>(
    <span>chat_response</span>.<span>message</span>.<span>content</span>
)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">📖 Qualitative Results</h3><a id="user-content--qualitative-results" aria-label="Permalink: 📖 Qualitative Results" href="#-qualitative-results"></a></p>
<p dir="auto">We periodically run popular VLMs on each of the examples &amp; schemas in the <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/vlmrun/hub/catalog.yaml">catalog.yaml</a> file and publish the results in the <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/tests/benchmarks">benchmarks</a> directory.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Provider</th>
<th>Model</th>
<th>Date</th>
<th>Results</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenAI</td>
<td>gpt-4o-2024-11-20</td>
<td>2025-01-09</td>
<td><a href="https://github.com/vlm-run/vlmrun-hub/blob/main/tests/benchmarks/2025-01-09-gpt-4o-2024-11-20-instructor-results.md">link</a></td>
</tr>
<tr>
<td>OpenAI</td>
<td>gpt-4o-mini-2024-07-18</td>
<td>2025-01-09</td>
<td><a href="https://github.com/vlm-run/vlmrun-hub/blob/main/tests/benchmarks/2025-01-09-gpt-4o-mini-2024-07-18-instructor-results.md">link</a></td>
</tr>
<tr>
<td>Gemini</td>
<td>gemini-2.0-flash-exp</td>
<td>2025-01-10</td>
<td><a href="https://github.com/vlm-run/vlmrun-hub/blob/main/tests/benchmarks/2025-01-10-gemini-2.0-flash-exp-instructor-results.md">link</a></td>
</tr>
<tr>
<td>Llama 3.2 11B</td>
<td>llama3.2-vision:11b</td>
<td>2025-01-10</td>
<td><a href="https://github.com/vlm-run/vlmrun-hub/blob/main/tests/benchmarks/2025-01-10-llama3.2-vision:11b-instructor-results.md">link</a></td>
</tr>
<tr>
<td>Microsoft</td>
<td>phi-4</td>
<td>2025-01-10</td>
<td><a href="https://github.com/vlm-run/vlmrun-hub/blob/main/tests/benchmarks/2025-01-11-phi4-instructor-results.md">link</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">📂 Directory Structure</h3><a id="user-content--directory-structure" aria-label="Permalink: 📂 Directory Structure" href="#-directory-structure"></a></p>
<p dir="auto">Schemas are organized by industry for easy navigation:</p>
<div data-snippet-clipboard-copy-content="vlmrun
└── hub
    ├── schemas
    |   ├── <industry>
    |   |   ├── <use-case-1>.py
    |   |   ├── <use-case-2>.py
    |   |   └── ...
    │&nbsp;&nbsp; ├── aerospace
    │&nbsp;&nbsp; │&nbsp;&nbsp; └── remote_sensing.py
    │&nbsp;&nbsp; ├── document  # all document schemas are here
    |   |   ├── invoice.py
    |   |   ├── us_drivers_license.py
    |   |   └── ...
    │&nbsp;&nbsp; ├── healthcare
    │&nbsp;&nbsp; │&nbsp;&nbsp; └── medical_insurance_card.py
    │&nbsp;&nbsp; └── retail
    │&nbsp;&nbsp; │&nbsp;&nbsp; └── ecommerce_product_caption.py
    │&nbsp;&nbsp; └── contrib  # all contributions are welcome here!
    │&nbsp;&nbsp;     └── <schema-name>.py
    └── version.py"><pre><code>vlmrun
└── hub
    ├── schemas
    |   ├── &lt;industry&gt;
    |   |   ├── &lt;use-case-1&gt;.py
    |   |   ├── &lt;use-case-2&gt;.py
    |   |   └── ...
    │&nbsp;&nbsp; ├── aerospace
    │&nbsp;&nbsp; │&nbsp;&nbsp; └── remote_sensing.py
    │&nbsp;&nbsp; ├── document  # all document schemas are here
    |   |   ├── invoice.py
    |   |   ├── us_drivers_license.py
    |   |   └── ...
    │&nbsp;&nbsp; ├── healthcare
    │&nbsp;&nbsp; │&nbsp;&nbsp; └── medical_insurance_card.py
    │&nbsp;&nbsp; └── retail
    │&nbsp;&nbsp; │&nbsp;&nbsp; └── ecommerce_product_caption.py
    │&nbsp;&nbsp; └── contrib  # all contributions are welcome here!
    │&nbsp;&nbsp;     └── &lt;schema-name&gt;.py
    └── version.py
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">✨ How to Contribute</h3><a id="user-content--how-to-contribute" aria-label="Permalink: ✨ How to Contribute" href="#-how-to-contribute"></a></p>
<p dir="auto">We're building this hub for the community, and contributions are always welcome! Follow the <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/docs/CONTRIBUTING.md">CONTRIBUTING</a> and <a href="https://github.com/vlm-run/vlmrun-hub/blob/main/docs/SCHEMA-GUIDELINES.md">SCHEMA-GUIDELINES.md</a> to get started.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">🔗  Quick Links</h3><a id="user-content---quick-links" aria-label="Permalink: 🔗  Quick Links" href="#--quick-links"></a></p>
<ul dir="auto">
<li>💬 Send us an email at <a href="mailto:support@vlm.run">support@vlm.run</a> or join our <a href="https://discord.gg/4jgyECY4rq" rel="nofollow">Discord</a> for help.</li>
<li>📣 Follow us on <a href="https://x.com/vlmrun" rel="nofollow">Twitter</a>, and <a href="https://www.linkedin.com/company/vlm-run" rel="nofollow">LinkedIn</a> to keep up-to-date on our products.</li>
</ul>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>