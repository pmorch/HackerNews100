<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 03 Jul 2024 22:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A practical introduction to constraint programming using CP-SAT and Python (130 pts)]]></title>
            <link>https://pganalyze.com/blog/a-practical-introduction-to-constraint-programming-using-cp-sat</link>
            <guid>40867746</guid>
            <pubDate>Wed, 03 Jul 2024 16:48:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pganalyze.com/blog/a-practical-introduction-to-constraint-programming-using-cp-sat">https://pganalyze.com/blog/a-practical-introduction-to-constraint-programming-using-cp-sat</a>, See on <a href="https://news.ycombinator.com/item?id=40867746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Imagine you're an e-commerce giant that would like to build a new warehouse to improve service to your customers, but you need to know what is the best location for it. Or you're a global shipping company that assigns packages to their delivery trucks and has to choose the best routes in order to save gas and reduce driver overtime. Or an airline that is looking to offer service to a new location, and needs to know which types of planes they should use and on what schedule, to maximize the resulting revenue.</p>
<p>These kinds of problems mentioned above are known as <strong>discrete optimization problems</strong>. There exist several methods that can be used to tackle such problems. In this article, we will discuss the theory and practice for one of them, called <strong><a href="https://en.wikipedia.org/wiki/Constraint_programming">constraint programming</a></strong>.</p>
<p>This is the first part in a two part series on constraint programming, used inside <a href="https://pganalyze.com/docs/indexing-engine/cp-model">pganalyze Indexing Engine</a> and <a href="https://pganalyze.com/docs/index-advisor/getting-started">pganalyze Index Advisor</a>. We're sharing this knowledge to help the wider programming community become familiar with how to utilize solvers like <a href="https://developers.google.com/optimization/cp/cp_solver">CP-SAT</a> in practice.</p>
<div>
<ul>
<li>
<p><a href="#a-declarative-paradigm">A declarative paradigm</a></p>
</li>
<li>
<p><a href="#the-basics-of-constraint-programming-cp">The basics of constraint programming (CP)</a></p>
</li>
<li>
<p><a href="#a-practical-example-with-python-and-cp-sat">A practical example with Python and CP-SAT</a></p>
<ul>
<li><a href="#an-empty-model">An empty model</a></li>
<li><a href="#the-data">The data</a></li>
<li><a href="#the-variables">The variables</a></li>
<li><a href="#the-constraints">The constraints</a></li>
<li><a href="#solving-the-model">Solving the model</a></li>
<li><a href="#adding-more-constraints">Adding more constraints</a></li>
<li><a href="#interlude-the-solver-status">Interlude: The solver status</a></li>
<li><a href="#sorry-emma">"Sorry, Emma"</a></li>
<li><a href="#objective-distributing-the-shifts-more-evenly">Objective: Distributing the shifts more evenly</a></li>
</ul>
</li>
<li>
<p><a href="#concluding-remarks">Concluding remarks</a></p>
</li>
</ul>
</div>
<h2 id="a-declarative-paradigm"><a href="#a-declarative-paradigm" aria-label="a declarative paradigm permalink"></a>A declarative paradigm</h2>
<p>Constraint programming (CP) is a declarative paradigm used to solve discrete optimization problems. This contrasts with the imperative paradigm that we are generally used to. When programming imperatively, we describe the steps necessary to reach a result. For example, suppose that we want to know who the adults are from a given list of people:</p>








<table><colgroup>
<col></colgroup><colgroup><col>
</colgroup><thead><tr><th scope="col">Name</th><th scope="col">Age</th></tr></thead><tbody><tr><td>Phil</td><td>20</td></tr><tr><td>Emma</td><td>17</td></tr><tr><td>David</td><td>11</td></tr><tr><td>Thomas</td><td>51</td></tr><tr><td>Sarah</td><td>45</td></tr><tr><td>Rebecca</td><td>6</td></tr></tbody></table>
<p>A typical imperative approach would explain the sequence of operations required to get the desired result:</p>
<div data-language="python"><pre><code>adult_people <span>=</span> <span>[</span><span>]</span>
<span>for</span> person <span>in</span> people<span>:</span>
    <span>if</span> person<span>.</span>Age <span>&gt;=</span> <span>18</span><span>:</span>
        adult_people <span>+=</span> person<span>.</span>Name</code></pre></div>

<p>Meanwhile, the same result can be described declaratively as:</p>
<div data-language="sql"><pre><code><span>SELECT</span> person_name <span>FROM</span> people <span>WHERE</span> age <span>&gt;=</span> <span>18</span><span>;</span></code></pre></div>

<p>Both approaches are equivalent in their outcome, but the two processes are different. In the imperative case, the program follows each step in sequence in order to reach the result. In the declarative case, the program is given a description of the desired result (using the available constructs of the language) and gets there by itself.</p>
<h2 id="the-basics-of-constraint-programming-cp"><a href="#the-basics-of-constraint-programming-cp" aria-label="the basics of constraint programming cp permalink"></a>The basics of constraint programming (CP)</h2>
<p>Similarly to the declarative example mentioned above, with CP we describe the desired result to a problem. This description is called a <strong>model</strong>. The main components of a model are variables and constraints. Variables represent <strong>what</strong> we are looking for, and each variable has an associated <strong>domain</strong> which is the set of values that this variable is allowed to take. Constraints describe <strong>relationships</strong> between variables.</p>
<p>A solution is an assignment of values to the variables (from their domains) such that the constraints are satisfied. Let's jump straight in with a simple example:</p>
<blockquote>
<div><p>Alice, Bob, and Carol each have $20, and they want to pool their money to purchase a candy bar worth $50 (yes, inflation is running wild). Alice has stated that she will put in at least as much money as Bob. Carol only has $5 bills, so her contribution will be a multiple of that. None of them want to contribute the exact same amount as any other.
</p><p>
How much should each of them chip in?</p></div>
</blockquote>
<p>Here, we are looking for the amount of money each person should contribute to the purchase of the candy bar. This means that we need one variable per person, indicating the amount of money that this person should contribute toward the purchase (<code>a</code> for Alice, <code>b</code> for Bob, and <code>c</code> for Carol). We start by setting the domains of these variables (the symbol <code>∈</code> means "in"):</p>
<div data-language="text"><pre><code>a ∈ {0, ..., 20}
b ∈ {0, ..., 20}
c ∈ {0, ..., 20}</code></pre></div>
<p>These domains ensure that the final values of the variables represent amounts of money that the contributors actually have in their pockets (that is, a maximum of $20). Next, we want to make sure that the combined contributions are enough to cover the price of the candy bar, so we add the constraint:</p>

<p>Alice will contribute at least as much as Bob, so we translate this into:</p>

<p>Carol's contribution must be a multiple of 5:</p>

<p>Finally, the amount of each of their contributions must be unique. We could model this using the following constraints:</p>

<p>We only have a handful of people in this example so these disequalities would work well. But what if they were hundreds, or thousands? It turns out that CP has a rich catalogue of expressive constraints that can encapsulate complex concepts, called <strong>global constraints</strong>. An alternative to the above would be to use the so-called <code>alldifferent</code> constraint, which ensures that a set of variables are all assigned different values:</p>

<p>This completes the model of this problem. You will note that we have not assigned any values to <code>a</code>, <code>b</code>, or <code>c</code> ourselves. We have simply defined three variables and their domains, and described the properties of the problem using constraints on those variables. Our job is done.</p>
<p>The piece of software that interprets this model and returns a solution is called a <strong>solver</strong>. The inner workings of a solver are outside the scope of this article, so for our purposes we will consider the solver as a black box that takes a model as input, and returns a valid solution:</p>

<p>The solution returned is valid as each variable takes a value from its domain and all the constraints are satisfied. However, we see that Carol contributes almost twice Bob's amount. Perhaps there exists another valid solution where all parties contribute more equally to the purchase?</p>
<p>We can add an <strong>objective</strong> to our CP model to try to reach this goal. Adding an objective to a model allows us to minimize or maximize an expression, without compromising the validity of the resulting solution (with respect to the constraints). If we can somehow minimize the amount of money spent by the largest contributor, this should push the three contributions closer together. Our objective will then be to find a valid solution where this value is minimal:</p>
<div data-language="text"><pre><code>x ∈ {0, ..., 20}
maximum(x, [a, b, c])
minimize: x</code></pre></div>
<p>To achieve this, we create a new variable <code>x</code> representing the amount of the largest contribution. The <code>maximum</code> constraint takes care of assigning to <code>x</code> the largest value from <code>[a, b, c]</code>. The objective is then to minimize <code>x</code>. The solution returned by the solver is now:</p>
<div data-language="text"><pre><code>a = 18
b = 17
c = 15
x = 18</code></pre></div>
<p>Previously there was a $9 difference between the largest and smallest contributions. With the objective we introduced this has now been reduced to $3, and this is as fair as this is going to get. Now that the basic concepts are cleared up, let's move on to a more challenging problem.</p>
<h2 id="a-practical-example-with-python-and-cp-sat"><a href="#a-practical-example-with-python-and-cp-sat" aria-label="a practical example with python and cp sat permalink"></a>A practical example with Python and CP-SAT</h2>
<p>Let's use this new CP knowledge to solve a more complex real-world example: the scheduling of employees for a small business.</p>
<blockquote>
<div><p>A store owner wishes to create the weekly work schedule for its employees. The store is open from 8AM to 8PM every day, and each day is divided into three shifts of 4 hours: morning, afternoon, and evening. There are two roles in the store: cashier and restocker.
</p></div>
<ul>
<li>
<p>Some employees are qualified to do either role, but others can only be a cashier, or a restocker.</p>
</li>
<li>
<p>There has to be a cashier scheduled at all times, but restocking only takes about 4 hours every day. Hence, for the restocking task we only need to schedule an employee for a single shift every day. This can be any shift, but two restocking shifts cannot be scheduled one after the other. If a restocking is scheduled on the evening shift on Tuesday, for example, we cannot schedule the Wednesday restocking on the morning shift.</p>
</li>
<li>
<p>An employee that is qualified in both roles can still only be assigned to one role per shift.</p>
</li>
<li>
<p>Employees cannot work more than 8 hours per day, which is 2 shifts. If they do work 2 shifts in a day, we must ensure that there is no idle time between these shifts—for example, we can't schedule them on both the morning and the evening shifts of the same day, as they would be idle for 4 hours during the afternoon shift.</p>
</li>
</ul>
</blockquote>
<p>This is the basic premise of the problem. Let's break this down into manageable parts.</p>
<h3 id="an-empty-model"><a href="#an-empty-model" aria-label="an empty model permalink"></a>An empty model</h3>
<p>We first start by creating an empty model using <a href="https://developers.google.com/optimization/cp/cp_solver">CP-SAT</a>, an open-source CP solver developed by Google as part of it's <a href="https://developers.google.com/optimization">OR-Tools</a> project.</p>
<div data-language="python"><pre><code><span>from</span> ortools<span>.</span>sat<span>.</span>python <span>import</span> cp_model

model <span>=</span> cp_model<span>.</span>CpModel<span>(</span><span>)</span></code></pre></div>
<h3 id="the-data"><a href="#the-data" aria-label="the data permalink"></a>The data</h3>
<blockquote>
<p>A store owner wishes to create the <span>weekly work schedule</span> for its <span>employees</span>. The store is open from 8AM to 8PM every day, and each day is divided into <span>three shifts</span> of 4 hours: <span>morning, afternoon, and evening</span>. There are <span>two roles</span> in the store: <span>cashier and restocker</span>. Some employees are <span>qualified</span> to do either role, but others can only be a cashier, or a restocker.</p>
</blockquote>
<p>Let's create a list of employees and the roles they are qualified for:</p>
<div data-language="python"><pre><code>employees <span>=</span> <span>{</span><span>"Phil"</span><span>:</span> <span>[</span><span>"Restocker"</span><span>]</span><span>,</span>
             <span>"Emma"</span><span>:</span> <span>[</span><span>"Cashier"</span><span>,</span> <span>"Restocker"</span><span>]</span><span>,</span>
             <span>"David"</span><span>:</span> <span>[</span><span>"Cashier"</span><span>,</span> <span>"Restocker"</span><span>]</span><span>,</span>
             <span>"Rebecca"</span><span>:</span> <span>[</span><span>"Cashier"</span><span>]</span><span>}</span></code></pre></div>
<p>The schedule is said to span a week, and we are told that there are three types of shifts, and two types of roles:</p>
<div data-language="python"><pre><code>days <span>=</span> <span>[</span><span>"Monday"</span><span>,</span>
        <span>"Tuesday"</span><span>,</span>
        <span>"Wednesday"</span><span>,</span>
        <span>"Thursday"</span><span>,</span>
        <span>"Friday"</span><span>,</span>
        <span>"Saturday"</span><span>,</span>
        <span>"Sunday"</span><span>]</span>

shifts <span>=</span> <span>[</span><span>"Morning"</span><span>,</span>
          <span>"Afternoon"</span><span>,</span>
          <span>"Evening"</span><span>]</span>

roles <span>=</span> <span>[</span><span>"Cashier"</span><span>,</span>
         <span>"Restocker"</span><span>]</span></code></pre></div>
<h3 id="the-variables"><a href="#the-variables" aria-label="the variables permalink"></a>The variables</h3>
<p>Now, let's define <strong>what</strong> we are looking for. To describe the schedule, we need to refer to employees, roles, days, and shifts: <strong>Does Emma work as a restocker on the Monday evening shift?</strong> This can be achieved using boolean variables. A boolean variable is a variable with a domain of <code>{0, 1}</code>.</p>
<div data-language="python"><pre><code>schedule <span>=</span> <span>{</span>e<span>:</span>
             <span>{</span>r<span>:</span>
               <span>{</span>d<span>:</span>
                 <span>{</span>s<span>:</span> model<span>.</span>new_bool_var<span>(</span><span><span>f"schedule_</span><span><span>{</span>e<span>}</span></span><span>_</span><span><span>{</span>r<span>}</span></span><span>_</span><span><span>{</span>d<span>}</span></span><span>_</span><span><span>{</span>s<span>}</span></span><span>"</span></span><span>)</span>
                   <span>for</span> s <span>in</span> shifts<span>}</span>
                 <span>for</span> d <span>in</span> days<span>}</span>
               <span>for</span> r <span>in</span> roles<span>}</span>
             <span>for</span> e <span>in</span> employees<span>}</span></code></pre></div>
<p>The function <code>model.new_bool_var()</code> creates and then returns a boolean variable, which we store in <code>schedule</code>. Within this structure, <code>schedule["Emma"]["Restocker"]["Monday"]["Evening"]</code> refers to one of those boolean variables. This variable is equal to <code>1</code> if Emma <strong>does</strong> work as a restocker on the Monday evening shift, or to <code>0</code> if she <strong>doesn't</strong>.</p>
<p>Our <code>schedule</code> variables are currently unconstrained. By following the problem description presented earlier and constraining these variables accordingly, we should be able to get a schedule that satisfies the requirements of the store owner.</p>
<h3 id="the-constraints"><a href="#the-constraints" aria-label="the constraints permalink"></a>The constraints</h3>
<p>Let's go through the rest of the problem description to correctly constrain the <code>schedule</code> variables. To add a new constraint to the model, we simply use <code>model.add(...)</code>.</p>
<blockquote>
<p>There has to be a <span>cashier</span> scheduled at <span>all times</span>.</p>
</blockquote>
<p>Since the schedule is comprised of boolean variables, it's easy to see how we can put limits on subsets of these variables by summing them and enforcing constraints on these sums:</p>
<div data-language="python"><pre><code><span>for</span> d <span>in</span> days<span>:</span>
    <span>for</span> s <span>in</span> shifts<span>:</span>
        model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span><span>"Cashier"</span><span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> e <span>in</span> employees<span>)</span> <span>==</span> <span>1</span><span>)</span></code></pre></div>
<p>If we need a cashier at all times, this means that for every day-shift pair, the sum of employees assigned the <code>"Cashier"</code> role has to be equal to <code>1</code>.</p>
<blockquote>
<p>For the <span>restocking</span> task we only need to schedule an employee for <span>a single shift every day</span>.</p>
</blockquote>
<p>Similarly to the previous constraint, we now want the sum of all employees assigned the <code>"Restocker"</code> role for all shifts of a given day to be equal to <code>1</code>:</p>
<div data-language="python"><pre><code><span>for</span> d <span>in</span> days<span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span><span>"Restocker"</span><span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> e <span>in</span> employees <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>1</span><span>)</span></code></pre></div>
<blockquote>
<p>This [restocking] can be any shift, but <span>two restocking shifts cannot be scheduled one after the other</span>.</p>
</blockquote>
<p>Because of the previous constraint, we already know that two restocking shifts cannot take place on the same day. The only way two restocking shifts could be scheduled one after the other would be to assign one on the evening shift of a day, and another one on the morning shift of the next day. By enforcing that the sum of restocking shifts for each evening-morning pair is not greater than <code>1</code>, we ensure that <strong>at most one</strong> restocking shift is scheduled for those pairs:</p>
<div data-language="python"><pre><code><span>for</span> i <span>in</span> <span>range</span><span>(</span><span>len</span><span>(</span>days<span>)</span><span>-</span><span>1</span><span>)</span><span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span><span>"Restocker"</span><span>]</span><span>[</span>days<span>[</span>i<span>]</span><span>]</span><span>[</span><span>"Evening"</span><span>]</span> <span>+</span> schedule<span>[</span>e<span>]</span><span>[</span><span>"Restocker"</span><span>]</span><span>[</span>days<span>[</span>i<span>+</span><span>1</span><span>]</span><span>]</span><span>[</span><span>"Morning"</span><span>]</span> <span>for</span> e <span>in</span> employees<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<blockquote>
<p>An <span>employee</span> that is qualified in both roles can still <span>only be assigned to one role per shift</span>.</p>
</blockquote>
<p>For every employee, the sum of all assigned roles for all day-shift pairs is either going to be <code>1</code> (they work a <em>single</em> role on this day-shift slot), or <code>0</code> (they don't work on that day-shift slot):</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    <span>for</span> d <span>in</span> days<span>:</span>
        <span>for</span> s <span>in</span> shifts<span>:</span>
            model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<blockquote>
<p>Some employees are qualified to do either role, but <span>others can only be a cashier, or a restocker</span>.</p>
</blockquote>
<p>To prevent an employee from being assigned a role that they are not qualified for, we simply match the value of that role to 0 (or put differently, we're adding a constraint asserting that it's zero) everywhere for that employee:</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    <span>for</span> r <span>in</span> roles<span>:</span>
        <span>for</span> d <span>in</span> days<span>:</span>
            <span>for</span> s <span>in</span> shifts<span>:</span>
                <span>if</span> r <span>not</span> <span>in</span> employees<span>[</span>e<span>]</span><span>:</span>
                    model<span>.</span>add<span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>
<blockquote>
<p><span>Employees cannot work more</span> than 8 hours per day, which is <span>2 shifts</span>. If they do work 2 shifts in a day, we must ensure that there is <span>no idle time between these shifts</span>—in other words, we can't schedule them on both the morning and the evening shifts of the same day, as they would be idle for 4 hours during the afternoon shift.</p>
</blockquote>
<p>It turns out that a single constraint can take care of both of these requirements: An employee can work <strong>either</strong> the morning shift, <strong>or</strong> the evening shift, <strong>or</strong> neither of these shifts:</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    <span>for</span> d <span>in</span> days<span>:</span>
        model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span><span>"Morning"</span><span>]</span> <span>+</span> schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span><span>"Evening"</span><span>]</span> <span>for</span> r <span>in</span> roles<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<p>Note that the above constraint does not need to specify anything about the afternoon shift. If the employee works in the morning, they can't work in the evening, and vice-versa. This both ensures that the employee works a maximum of two shifts per day, and also that there is no idle time, since there can only be idle time if the employee works both the morning <strong>and</strong> the evening shift.</p>
<p>The modeling of the problem is now complete. Let's see what the results are.</p>
<h3 id="solving-the-model"><a href="#solving-the-model" aria-label="solving the model permalink"></a>Solving the model</h3>
<p>To solve the model, we simply call a solver, with the model as an argument:</p>
<div data-language="python"><pre><code>solver <span>=</span> cp_model<span>.</span>CpSolver<span>(</span><span>)</span>

solver<span>.</span>solve<span>(</span>model<span>)</span></code></pre></div>
<p>After the solving process, we can get the resulting values of the <code>schedule</code> variables with <code>solver.value(...)</code>. We have organized these values in the schedule shown below:</p>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   7   |
Emma       |   |   |   | C |   |   | C |   |   | C |   |   | C |   |   | C |   |   | C |   |   |   6   |
David      | C |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   1   |
Rebecca    |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |  14   |</code></pre></div>
<p>The resulting schedule satisfies the constraints of the problem. Now let's think of additional real-world constraints that would make this more interesting.</p>
<h3 id="adding-more-constraints"><a href="#adding-more-constraints" aria-label="adding more constraints permalink"></a>Adding more constraints</h3>
<p>We see that Rebecca works 14 shifts for that week. The store owner is not keen on paying overtime wages, so they wish to cap each employee's schedule to a maximum of 40 hours per week (10 work shifts):</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>for</span> s <span>in</span> shifts<span>)</span> <span>&lt;=</span> <span>10</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   7   |
Emma       |   | C | C |   | C | C | C |   |   | C |   |   | C |   |   | C |   |   | C |   |   |   9   |
David      | C |   |   | C |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   2   |
Rebecca    |   |   |   |   |   |   |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |  10   |</code></pre></div>
<p>Phil is a full-time student and would like to work exactly 4 shifts per week. He also cannot work the morning and afternoon shifts during the week, in order to attend his classes.</p>
<div data-language="python"><pre><code>model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Phil"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>4</span><span>)</span>

model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Phil"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>if</span> d <span>not</span> <span>in</span> <span>[</span><span>"Saturday"</span><span>,</span> <span>"Sunday"</span><span>]</span> <span>for</span> s <span>in</span> shifts <span>if</span> s <span>in</span> <span>[</span><span>"Morning"</span><span>,</span> <span>"Afternoon"</span><span>]</span><span>)</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   |   |   |   | R |   |   | R |   |   | R |   |   |   |   | R |   |   |   |   |   4   |
Emma       | C | R |   |   |   | C | C |   |   | C |   |   | C | R |   | C |   |   | C | R |   |  10   |
David      |   | C | C | C | C |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   4   |
Rebecca    |   |   |   |   |   |   |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |  10   |</code></pre></div>
<p>Phil and Emma do not get along very well, and as such we don't want them working the same shifts.</p>
<div data-language="python"><pre><code><span>for</span> d <span>in</span> days<span>:</span>
    <span>for</span> s <span>in</span> shifts<span>:</span>
        model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> e <span>in</span> <span>[</span><span>"Phil"</span><span>,</span> <span>"Emma"</span><span>]</span> <span>for</span> r <span>in</span> roles<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   | R |   |   | R |   |   | R |   |   | R |   |   |   |   |   |   |   |   |   |   4   |
Emma       | C |   |   | C | C |   | C | C |   | C | C |   | C | C |   |   | C |   |   |   |   |  10   |
David      |   | C | C |   |   | C |   |   | C |   |   | C |   | R |   |   | R | C | C | R |   |  10   |
Rebecca    |   |   |   |   |   |   |   |   |   |   |   |   |   |   | C | C |   |   |   | C | C |   4   |</code></pre></div>
<p>No employee really likes to work on the weekend, so we would like to distribute these shifts equally between all employees. There are 8 such shifts (3 cashier shifts and 1 restocker shift on each day), and since we have 4 employees, we can give them all 2 shifts each for the weekend.</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> <span>[</span><span>"Saturday"</span><span>,</span> <span>"Sunday"</span><span>]</span> <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>2</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   |   |   |   | R |   |   | R |   |   |   |   |   |   | R |   |   | R |   |   |   4   |
Emma       | R | C |   | C | C |   | C |   |   |   |   | C |   | C | C |   |   |   |   | C | C |  10   |
David      | C |   |   |   |   | C |   | C | C | C | R |   | C | R |   |   |   | C | C |   |   |  10   |
Rebecca    |   |   | C |   |   |   |   |   |   |   | C |   |   |   |   | C | C |   |   |   |   |   4   |</code></pre></div>
<p>Emma would like to take Monday to Friday off, and asks us if this is possible. Let's see:</p>
<div data-language="python"><pre><code>model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Emma"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> <span>[</span><span>"Monday"</span><span>,</span> <span>"Tuesday"</span><span>,</span> <span>"Wednesday"</span><span>,</span> <span>"Thursday"</span><span>,</span> <span>"Friday"</span><span>]</span> <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>

<p>The solver returns a status of "infeasible". What does this mean?</p>
<h3 id="interlude-the-solver-status"><a href="#interlude-the-solver-status" aria-label="interlude the solver status permalink"></a>Interlude: The solver status</h3>
<p>The solver takes as input a model, and returns a status and a solution. Consider this simple case:</p>
<div data-language="text"><pre><code>x ∈ {0, ..., 10}
y ∈ {0, ..., 10}
x + y &gt;= 5
minimize: x + y

Status: OPTIMAL
x = 5
y = 0</code></pre></div>
<p>The variables <code>x</code> and <code>y</code> both have the domain <code>{0, ..., 10}</code>. The sum of the values assigned to them must be 5 or greater, and the objective is to minimize this sum. The solution <code>(x, y) = (5, 0)</code> is called <strong>optimal</strong>. A solution is optimal when there does not exist another solution that is <strong>strictly better</strong> than it. For instance, the solution <code>(x, y) = (3, 2)</code> would also be optimal.</p>
<p>Now consider the case:</p>
<div data-language="text"><pre><code>x ∈ {0, ..., 10}
x &gt;= 15

Status: INFEASIBLE</code></pre></div>
<p>An <strong>infeasible</strong> status means that there exists no assignment of values to the variables such that the constraints are satisfied. In the above example, it is not possible to assign a value to <code>x</code> that is at least as large as 15, since the largest value in its domain is 10.</p>
<p>There are also other possibilities. Let's take a look at this last example:</p>
<div data-language="text"><pre><code>x ∈ {0, ..., 10}
y ∈ {0, ..., 10}
...  // Many more variables
x + y &lt;= 12
x &gt;= 5
... // Many more constraints
minimize: x - 3*y + ...  // Very complex objective</code></pre></div>
<p>Sometimes, a problem is so large and complex, and takes so much time to solve, that we must interrupt the solver due to time constraints. In such a case, the solver will return one of two statuses:</p>
<ul>
<li>If a solution has been found, the status returned is <strong>feasible</strong>. A feasible solution means that the solution satisfies the constraints, but the solver does not know if that solution is optimal. In other words, we have a solution that works, but there may still exist a better one.</li>
<li>If no solution has been found, the status returned is <strong>unknown</strong>. This means that while the solver has not found a solution, it does not know whether one exists (feasible) or if the problem has no solution (infeasible).</li>
</ul>
<p>Let's get back to our schedule.</p>
<h3 id="sorry-emma"><a href="#sorry-emma" aria-label="sorry emma permalink"></a>"Sorry, Emma"</h3>
<p>With the infeasible status returned to us previously, we are forced to inform Emma that she can't take the whole week off, otherwise it would be impossible to fill the schedule without violating some of the other constraints. She decides to only take Monday to Wednesday off instead:</p>
<div data-language="python"><pre><code>model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Emma"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> <span>[</span><span>"Monday"</span><span>,</span> <span>"Tuesday"</span><span>,</span> <span>"Wednesday"</span><span>]</span> <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   | R |   |   |   |   |   | R |   |   |   |   |   |   | R |   |   | R |   |   |   4   |
Emma       |   |   |   |   |   |   |   |   |   |   | R | C |   | R | C |   |   |   |   | C | C |   6   |
David      | C |   |   | C | R |   | C |   |   | C | C |   | C | C |   |   |   | C | C |   |   |  10   |
Rebecca    |   | C | C |   | C | C |   | C | C |   |   |   |   |   |   | C | C |   |   |   |   |   8   |</code></pre></div>
<p>Phil works exactly 4 shifts as he wants, but the other shifts are not very well distributed: Emma gets 6, David gets 10, and Rebecca gets 8. Let's see if we can improve this by adding an objective.</p>
<h3 id="objective-distributing-the-shifts-more-evenly"><a href="#objective-distributing-the-shifts-more-evenly" aria-label="objective distributing the shifts more evenly permalink"></a>Objective: Distributing the shifts more evenly</h3>
<p>We would like to distribute the shifts as fairly as possible between Emma, David, and Rebecca. We will recall that an objective allows us to minimize the value of an expression. We could, for example, minimize the shift difference between the employee who is assigned the fewest shifts, and the one who is assigned the most. Currently, Emma is assigned 6 shifts to David's 10, so this difference stands at 4 shifts.</p>
<p>We start by creating integer variables to track the number of shifts assigned to each employee, and enforcing the values of these variables:</p>
<div data-language="python"><pre><code><span># total_shifts[e] indicates the number of shifts worked by employee `e`</span>
total_shifts <span>=</span> <span>{</span>e<span>:</span> model<span>.</span>new_int_var<span>(</span><span>0</span><span>,</span> <span>10</span><span>,</span> <span><span>f"total_shifts_</span><span><span>{</span>e<span>}</span></span><span>"</span></span><span>)</span>
                <span>for</span> e <span>in</span> employees<span>}</span>

<span>for</span> e <span>in</span> employees<span>:</span>
    model<span>.</span>add<span>(</span>total_shifts<span>[</span>e<span>]</span> <span>==</span> <span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>for</span> s <span>in</span> shifts<span>)</span><span>)</span></code></pre></div>
<p>Integer variables are created with <code>model.new_int_var(...)</code>, and the parameters allow us to specify the lower and upper bounds of the domain (in this case, the domain is <code>{0, ..., 10}</code>). We then create variables to track the highest and lowest number of shifts assigned to any employee (with the exception of Phil, who works part-time):</p>
<div data-language="python"><pre><code>min_shifts <span>=</span> model<span>.</span>new_int_var<span>(</span><span>0</span><span>,</span> <span>10</span><span>,</span> <span>"min_shifts"</span><span>)</span>
model<span>.</span>add_min_equality<span>(</span>min_shifts<span>,</span> <span>[</span>total_shifts<span>[</span>e<span>]</span> <span>for</span> e <span>in</span> employees <span>if</span> e <span>!=</span> <span>"Phil"</span><span>]</span><span>)</span>

max_shifts <span>=</span> model<span>.</span>new_int_var<span>(</span><span>0</span><span>,</span> <span>10</span><span>,</span> <span>"max_shifts"</span><span>)</span>
model<span>.</span>add_max_equality<span>(</span>max_shifts<span>,</span> <span>[</span>total_shifts<span>[</span>e<span>]</span> <span>for</span> e <span>in</span> employees <span>if</span> e <span>!=</span> <span>"Phil"</span><span>]</span><span>)</span></code></pre></div>
<p>The constraints <code>model.add_min_equality(...)</code> and <code>model.add_max_equality(...)</code> work in the same fashion as the <code>maximum(...)</code> constraint presented in the candy bar example (with Alice, Bob, and Carol). For example, <code>model.add_min_equality(var, list)</code> assigns to integer variable <code>var</code> the smallest value among the integer variables in <code>list</code>.</p>
<p>Finally, we minimize the difference between <code>max_shifts</code> and <code>min_shifts</code>:</p>
<div data-language="python"><pre><code>model<span>.</span>minimize<span>(</span>max_shifts <span>-</span> min_shifts<span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   | R |   |   |   |   |   | R |   |   |   |   |   |   | R |   |   | R |   |   |   4   |
Emma       |   |   |   |   |   |   |   |   |   |   | R | C |   | R | C |   |   |   |   | C | C |   6   |
David      | C |   |   | C | R |   | C |   |   |   | C |   | C | C |   |   |   | C | C |   |   |   9   |
Rebecca    |   | C | C |   | C | C |   | C | C | C |   |   |   |   |   | C | C |   |   |   |   |   9   |</code></pre></div>
<p>This looks pretty good. David and Rebecca both get 9 shifts. Emma only gets 6, but that is understandable as she is taking 3 days off that week. Phil gets exactly 4 shifts, as he is supposed to. Hopefully, everyone will be happy with this schedule.</p>

<p>We have introduced the basics of constraint programming and discussed its main components. We have built a model to generate work schedules that can take into account the types of constraints that one could reasonably expect to be faced with in the real world.</p>
<p>Thanks to this model, we could easily see that it would not be possible for Emma to take five days off as she initially wanted, and we were able to distribute the work shifts as fairly as possible between the employees. In the end, we created a schedule that satisfied both the requiremends of the store owner, as well as the needs of the employees.</p>
<p>In the next article, we will discuss how to use constraint programming for index selection in Postgres.</p>
<p><strong>The code presented in this article can be found on the <a href="https://github.com/pganalyze/cp-sat-python-example">pganalyze GitHub</a>.</strong></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Cheapest NAS (116 pts)]]></title>
            <link>https://sigwait.org/~alex/blog/2024/07/01/the-cheapest-nas.html</link>
            <guid>40867709</guid>
            <pubDate>Wed, 03 Jul 2024 16:45:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sigwait.org/~alex/blog/2024/07/01/the-cheapest-nas.html">https://sigwait.org/~alex/blog/2024/07/01/the-cheapest-nas.html</a>, See on <a href="https://news.ycombinator.com/item?id=40867709">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<section>
<article data-file="2024/07/01/the-cheapest-nas.md">

  <nav>
    <span id="nbe_post--next"></span>
    <span id="nbe_post--prev"></span>
  </nav>

  <h2>The cheapest NAS</h2>

  <p>Latest update: <time datetime="2024-07-03T16:09:11.072Z">2024-07-03 19:09:11</time></p>

  

  <p>I wanted to replace my old trusty 'router' (with an attached
HDD)--that was not working as a router, but as a network drive after
flashing OpenWRT onto it--I wanter to replace it with an SBC+HDD
combo.</p>
<p>This new device should not only preserve all the services the old one
provided (samba, git, rsyncd, a dnf repo), but also perform faster,
for having a potato instead of a CPU, the ex-router struggled with
rsync over ssh &amp;, being gravely RAM limited, choked when I did 'git
push' commits containing binaries &gt; 15MB.</p>
<p>Searching for a suitable SBC led me to libre.computer, a company I had
never heard of before. At first glance, they had the el cheapo
<a href="https://libre.computer/products/aml-s805x-ac/">AML-S805X-AC</a> board I needed:</p>
<ul>
<li>LAN port (but 100 Mb only);</li>
<li>2 USB-A (but 2.0 only);</li>
<li>4-core ARM Cortex-A53;</li>
<li>1 GB RAM;</li>
<li>booting from an USB;</li>
<li>up-to-date Debian;</li>
<li>easy to buy without hunting it down.</li>
</ul>
<p>100Mb may seem like a joke nowadays, but the main purpose of such a
toy NAS for me is to keep a copy of a directory with ~200K small
files. Having 1Gb would only marginally improve the syncing speed even
if the SBC supported USB 3.0.</p>
<p>But this is just a board. I also needed an hdd enclosure with an
external power supply (for the device provides up to 900mA for
each USB-A), at least 3A power supply &amp; a micro-USB cable that can
handle 3A.</p>
<table id="tcnagcb-1">
<thead>
<tr><th>Item</th><th>Price, €</th><th>Comment</th></tr>
</thead>
<tbody>
<tr><td>SBC</td><td>20</td><td></td></tr>
<tr><td>HDD enclosure</td><td>12</td><td></td></tr>
<tr><td>3A Power Supply</td><td>5</td><td></td></tr>
<tr><td>Micro-USB cable</td><td>3</td><td></td></tr>
<tr><td>4 bolts, 12 nuts</td><td>0</td><td>I think the ones I found are older than me</td></tr>
<tr><td>TTL to USB dongle</td><td>3</td><td>Optional, the board has an HDMI output</td></tr>
<tr><th>Total</th><td>43</td><td></td></tr>
</tbody>
</table>



<p>(I didn't include an HDD in the table, for I presume everyone has a
couple of them lying around.)</p>
<p>When I bought the HDD enclosure, I didn't read the description
carefully &amp; thought it was going to be a small plastic container for
2.5-inch drives, but when the package arrived, it turned out to be a
box for 3.5-inch ones. Hence, I decided to shove the SBC into it too.</p>
<img alt="" src="https://sigwait.org/~alex/blog/2024/07/01/innards.avif">
<img alt="" src="https://sigwait.org/~alex/blog/2024/07/01/rear.avif">

<p>After connecting the TTL-to-USB dongle to the board's GPIO</p>
<img alt="" src="https://sigwait.org/~alex/blog/2024/07/01/gpio.avif">

<p>&amp; typing</p>
<pre><code>$ sudo screen /dev/ttyUSB0 115200
</code></pre>
<p>one of the 1st readouts appeared as:</p>
<pre><code>U-Boot 2023.07+ (Nov 03 2023 - 15:10:36 -0400) Libre Computer AML-S805X-AC

Model: Libre Computer AML-S805X-AC
SoC:   Amlogic Meson GXL (S805X) Revision 21:d (34:2)
DRAM:  512 MiB (effective 1 GiB)
</code></pre>
<p>What does the last line mean exactly? After I dd'ed Debian-12 onto a
flash drive, free(1) said it saw 1GB. Anyway, libre.computer has an
official OS image, based on stock Debian:</p>
<pre><code>$ fdisk debian-12-base-arm64+arm64.img -l
Disk debian-12-base-arm64+arm64.img: 2.25 GiB, 2415919104 bytes, 4718592 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x71f3f7cf

Device                          Boot  Start     End Sectors  Size Id Type
debian-12-base-arm64+arm64.img1 *      2048  524287  522240  255M ef EFI (FAT-12/16/32)
debian-12-base-arm64+arm64.img2      524288 4718591 4194304    2G 83 Linux
</code></pre>
<p>Yes, it has an EFI partition with the MBR layout! The 2nd partition is
btrfs (supposedly it's faster &amp; more gentle to flash storage than
ext4; no idea if both claims are true). You can examine its contents via:</p>
<pre><code>$ sudo mount -o loop,offset=$((524288*512)) debian-12-base-arm64+arm64.img ~/mnt/misc
</code></pre>
<p>This partition gets auto-resized on the 1st boot to fill the rest of
the free space available on the drive. Doing this on USB dongles
proved to be a miserable experience: of the 3 I had available, one
permanently got stuck on resizing, and another, despite finishing the
operation, became so sluggish afterwards that a 20-year-old PC would've
felt snappier.</p>
<p>This is I didn't like at all. There is no repo with from which the OS
image gets generated. The explanation is <a href="https://hub.libre.computer/t/source-code-git-repository-for-bootloaders-and-firmwares/2743/6">bizarre</a>:</p>
<blockquote>
<p>"The distribution builder is a proprietary commercial offering as it
involves a lot of customer IP and integrations so it cannot be
public."</p>
</blockquote>
<p>but with an consolation advice:</p>
<blockquote>
<p>"If you want to study them [images], bootstrap and do a diff. We
don't make any changes to the standard distros outside of setting a
few configs since we're not distro maintainers."</p>
</blockquote>
<p>Make of it what you will.</p>
<p>Then I connected the HDD enclosure to the board. This time, the
process went much, much faster (though there were still some
unexpected delays in random places). Right after logging in, I started
getting <code>uas_eh_abort_handler</code> errors from the kernel. It turns out I
got one of the worst HDD enclosure innards possible, if you believe
reviews from the interwebs:</p>
<pre><code>$ lsusb
Bus 001 Device 002: ID 152d:0578 JMicron Technology Corp. / JMicron USA Technology Corp. JMS578 SATA 6Gb/s
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
</code></pre>
<p>The remedy is to turn UAS off via adding
<code>usb-storage.quirks=152d:0578:u</code> to the kernel cmdline. It did help,
the delays went away, although 'benchmarks' became hardly thrilling:</p>
<pre><code>$ lsusb -t
/:  Bus 01.Port 1: Dev 1, Class=root_hub, Driver=xhci-hcd/2p, 480M
    |__ Port 2: Dev 2, If 0, Class=Mass Storage, Driver=usb-storage, 480M
$ sync; time sh -c "dd if=/dev/urandom of=1 bs=500k count=1k &amp;&amp; sync"; rm 1
1024+0 records in
1024+0 records out
524288000 bytes (524 MB, 500 MiB) copied, 15.1014 s, 34.7 MB/s

real    0m21.876s
user    0m0.001s
sys     0m7.976s
</code></pre>
<p>which means <math alttext="524/21.876 = 23.95">
<mfrac><mn>524</mn><mn>21.876</mn></mfrac><mo>=</mo><mn>23.95</mn>
</math> MB/s on an ext4 partition.</p>
<p>Would I recommend this setup? I wouldn't. One of the reasons I've
chosen the path with an SBC instead of a common micro-ITX route is to
save on <a href="https://sigwait.org/~alex/blog/2024/07/01/imf.html">power consumption</a>. If you don't have similar
problems, I see 0 reasons to struggle with such a finicky Chinese
device.</p>


  <br>Tags: <a href="https://sigwait.org/~alex/blog/t/da44800a6e351c6e94a3d3b6060d2be1.html">ойті</a>
  <br>Authors: <a href="https://sigwait.org/~alex/blog/a/4e42f7dd43ecbfe104de58610557c5ba.html">ag</a>

</article>
</section>

  <nav>
    

    
  </nav>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making a Linux-managed network switch (111 pts)]]></title>
            <link>https://blog.brixit.nl/making-a-linux-managed-network-switch/</link>
            <guid>40866442</guid>
            <pubDate>Wed, 03 Jul 2024 14:47:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.brixit.nl/making-a-linux-managed-network-switch/">https://blog.brixit.nl/making-a-linux-managed-network-switch/</a>, See on <a href="https://news.ycombinator.com/item?id=40866442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Network switches are simple devices, packets go in, packets go out. Luckily people have figured out how to make it complicated instead and invented managed switches.</p>

<p>Usually this is done by adding a web-interface for configuring the settings and see things like port status. If you have more expensive switches then you'd even get access to some alternate interfaces like telnet and serial console ports.</p>

<p>There is a whole second category of managed switches though that people don't initially think of. These are the network switches that are inside consumer routers. These routers are little Linux devices that have a switch chip inside of them, one or more ports are internally connected to the CPU and the rest are on the outside as physical ports.</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1719959978/RB2011UiAS-160620170256_160656.png"><figcaption>Mikrotik RB2011 block diagram from mikrotik.com</figcaption></figure>

<p>Here is an example of such a device that actually has this documented. I always thought that the configuration of these switch connected ports was just a nice abstraction by the webinterface but I was suprised to learn that with the DSA and switchdev subsystem in Linux these ports are actually fully functioning "local" network ports. Due to this practically only being available inside integrated routers It's pretty hard to play around with unfortunately.</p>

<p>What is shown as a single line on this diagram is actually the connection of the SoC of the router and the switch over the SGMII bus (or maybe RGMII in this case) and a management bus that's either SMI or MDIO. Network switches have a lot of these fun acronyms that even with the full name written out make little sense unless you know how all of this  fits together.</p>

<p>Controlling your standard off-the-shelf switch using this system simply isn't possible because the required connections of  the switch chip aren't exposed for this. So there's only one option left...</p>

<h2>Making my own gigabit network switch</h2>

<p>Making my own network switch can't be <i>that</i> hard right? Those things are available for the price of a cup of coffee and are most likely highly integrated to reach that price point. Since I don't see any homemade switches around on the internet I guess the chips for those must be pretty hard to get...</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1719960715/image.png"></figure>

<p>Nope, very easy to get. There's even a datasheet available for these. So I created a new KiCad project and started creating some footprints and symbols.</p>

<p>I'm glad there's any amount of datasheet available for this chip since that's not usually the case for Realtek devices, but it's still pretty minimal. I resorted to finding any devices that has schematics available for similar Realtek chips to find out how to integrate it and looking at a lot of documentation for how to implement ethernet in a design at all.</p>

<p>The implementation for the chip initially looked very complicated, there's about 7 different power nets it requires and there are several pretty badly documented communication interfaces. After going through other implementations it seem like the easiest way to power it is just connect all the nets with overlapping voltage ranges together and you're left with only needing a 3.3V and 1.1V regulator.</p>

<p>The extra communication busses are for all the extra ports I don't seem to need. The switch chip I selected is the RTL8367S which is a very widely used 5-port gigabit switch chip, but it's actually not a 5-port chip. It's a 7 port switch chip where 5 ports have an integrated PHY and two are for CPU connections.</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1719961532/image.png"><figcaption>CPU connection block diagram from the RTL8367S datasheet</figcaption></figure>

<p>My plan is different though, while there are these CPU ports available there is actually nothing in the Linux switchdev subsystem that requires the CPU connection to be to those ports. Instead I'll be connecting to port 0 on the switch with a network cable and as far  as the switchdev driver knows there's no ethernet PHY in between.</p>

<p>The next hurdle is the configuration of the switch chip, there's several configuration systems available and the datasheet does not really describe what is the minimum required setup to actually get it to function as a regular dumb switch. To sum up the configuration options of the chip:</p>

<ul><li>There's 8 pins on the chip that are read when it's starting up. These pins are shared with the led pins for the ports so that makes designing pretty annoying. Switching the setting from pull-up to pull-down also requires the led to be connected in the different orientation.</li>
<li>There's an i2c bus that can be connected to an eeprom chip. The pins for this are shared with the SMI bus that I require to make this chip talk to Linux though. There is pin configuration to select from one of two eeprom size ranges but does not further specify what this setting actually changes.</li>
<li>There's a SPI bus that supports connecting a NOR flash chip to it. This can store either configuration registers or firmware for the embedded 8051 core depending on the configuration of the bootup pins. The SPI bus pins are also shared with one of the CPU network ports.</li>
<li>There is a serial port available but from what I guess it probably does nothing at all unless there's firmware loaded in the 8051.</li>
</ul>

<p>My solution to figuring out is to just order a board and solder connections differently until it works. I've added a footprint for a flash chip that I ended up not needing and for all the configuration pins I added solder jumpers. I left out all the leds since making that configurable would be pretty hard.</p>

<p>The next step is figuring out how to do ethernet properly. There has been a lot of documentation written about this and they all make it sound like gigabit ethernet requires perfect precision engineering, impedance managed boards and a blessing from the ethernet gods themselves to work. This does not seem to match up with the reality that these switches are very very cheaply constructed and seem to work just fine. So I decided to open up a switch to check how many of these coupling capacitors and impedance matching planes are actually used in a real design. The answer seems to be that it doesn't matter that much.</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1719962591/image.png"></figure>

<p>This is the design I have ended up with now but it is not what is on my test PCB. I got it almost right the first time though :D</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1719962813/image.png"></figure>

<p>The important parts seem to be matching the pair skew but matching the length of the 4 network pairs is completely useless, this is mainly because network cables don't have the same twisting rate for the 4 pairs and so the length of these are already significantly different inside the cable.</p>

<p>The pairs between the transformer and the RJ45 jack has it's own ground plane that's coupled to the main ground through a capacitor. The pairs after the transformer are  just on the main board ground fill.</p>

<p>What I did wrong on my initial board revision was forgetting the capacitor that connects the center taps of the transformer on the switch side to ground making the signals on that side referenced to board ground. This makes ethernet very much not work anymore so I had to manually cut tiny traces on the board to disconnect that short to ground. In my test setup the capacitor just doesn't exist and all the center taps float. This seems to work just fine but the final design does have that capacitor added.</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1720003020/fixed.JPG"><figcaption>Cut ground traces on the ethernet transformer</figcaption></figure>

<p>The end result is this slightly weird gigabit switch. It has 4 ports facing one direction and one facing backwards and it is powered over a 2.54mm pinheader. I have also added a footprint for a USB Type-C connector to have an easy way to power it without bringing out the DuPont wires.</p>

<figure><img src="https://blog.brixit.nl/image/w1000//static/files/blog.brixit.nl/1720007603/IMG_20240626_221246.jpg"></figure>

<h2>Connecting it to Linux</h2>

<p>For my test setup I've picked the PINE64 A64-lts board since it has the connectors roughly in the spots where I want them. It not being an x86 platform is also pretty important because configuration requires a device tree change, can't do that on a platform that doesn't use device trees.</p>

<p>The first required thing was rebuilding the kernel for the board since most kernels simply don't have these kernel modules enabled. For this I enabled these options:</p>

<ul><li><code>CONFIG_NET_DSA</code> for the Distributed Switch Architecture system</li>
<li><code>CONFIG_NET_DSA_TAG_RTL8_4</code> for having port tagging for this Realtek switch chip</li>
<li><code>CONFIG_NET_SWITCHDEV</code> the driver system for network switches</li>
<li><code>CONFIG_NET_DSA_REALTEK</code>, <code>CONFIG_NET_DSA_REALTEK_SMI</code>, <code>CONFIG_NET_DSA_REALTEK_RTL8365MB</code> for the actual switch chip driver</li>
</ul>

<p>Then the more complicated part was figuring out how to actually get this all loaded. In theory it is possible to create a device tree overlay for this and get it loaded by U-Boot. I decided to not do that and patch the device tree for the A64-lts board instead since I'm rebuilding the kernel anyway. The device tree change I ended up with is this:</p>

<pre><code>diff --git a/arch/arm64/boot/dts/allwinner/sun50i-a64-pine64-lts.dts b/arch/arm64/boot/dts/allwinner/sun50i-a64-pine64-lts.dts
index 596a25907..10c1a5187 100644
--- a/arch/arm64/boot/dts/allwinner/sun50i-a64-pine64-lts.dts
+++ b/arch/arm64/boot/dts/allwinner/sun50i-a64-pine64-lts.dts
@@ -18,8 +18,78 @@ led {
 			gpios = &lt;&amp;r_pio 0 7 GPIO_ACTIVE_LOW&gt;; /* PL7 */
 		};
 	};
+
+switch {
+	compatible = "realtek,rtl8365rb";
+	mdc-gpios = &lt;&amp;pio 2 5 GPIO_ACTIVE_HIGH&gt;; // PC5
+	mdio-gpios = &lt;&amp;pio 2 7 GPIO_ACTIVE_HIGH&gt;; // PC7
+	reset-gpios = &lt;&amp;pio 8 5 GPIO_ACTIVE_LOW&gt;; // PH5
+	realtek,disable-leds;
+
+	mdio {
+		compatible = "realtek,smi-mdio";
+		#address-cells = &lt;1&gt;;
+		#size-cells = &lt;0&gt;;
+
+		ethphy0: ethernet-phy@0 {
+			reg = &lt;0&gt;;
+		};
+
+		ethphy1: ethernet-phy@1 {
+			reg = &lt;1&gt;;
+		};
+
+		ethphy2: ethernet-phy@2 {
+			reg = &lt;2&gt;;
+		};
+
+		ethphy3: ethernet-phy@3 {
+			reg = &lt;3&gt;;
+		};
+
+		ethphy4: ethernet-phy@4 {
+			reg = &lt;4&gt;;
+		};
+	};
+
+	ports {
+		#address-cells = &lt;1&gt;;
+		#size-cells = &lt;0&gt;;
+
+		port@0 {
+			reg = &lt;0&gt;;
+			label = "cpu";
+			ethernet = &lt;&amp;emac&gt;;
+		};
+
+		port@1 {
+			reg = &lt;1&gt;;
+			label = "lan1";
+			phy-handle = &lt;&amp;ethphy1&gt;;
+		};
+
+		port@2 {
+			reg = &lt;2&gt;;
+			label = "lan2";
+			phy-handle = &lt;&amp;ethphy2&gt;;
+		};
+
+		port@3 {
+			reg = &lt;3&gt;;
+			label = "lan3";
+			phy-handle = &lt;&amp;ethphy3&gt;;
+		};
+
+		port@4 {
+			reg = &lt;4&gt;;
+			label = "lan4";
+			phy-handle = &lt;&amp;ethphy4&gt;;
+		};
+	};
+};
 };
 </code></pre>

<p>It loads the driver for the switch with the <code>realtek,rtl8365rb</code>, this driver supports a whole range of Realtek switch chips including the RTL8367S I've used in this design. I've removed the CPU ports from the documentation example and just added the definitions of the 5 regular switch ports.</p>

<p>The important part is in <code>port@0</code>, this is the port that is facing backwards on my switch and is connected to the A64-lts, I've linked it up to <code>&amp;emac</code> which is a reference to the ethernet port of the computer.  The rest of the ports are  linked up to their respective PHYs in the switch chip. </p>

<p>In the top of the code there's also 3 GPIOs defined, these link up to SDA/SCL and Reset on the switch PCB to make the communication work. After booting up the system the result is this:</p>

<pre><code>1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &lt;BROADCAST,MULTICAST&gt; mtu 1508 qdisc noop state DOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
3 lan1@eth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
4 lan2@eth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
5 lan3@eth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
6 lan4@eth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff</code></pre>

<p>I have the <code>eth0</code> device here like normal and then I have the 4 interfaces for the ports on the switch I defined in the device tree. To make it actually do something the interfaces actually need to be brought online first:</p>

<pre><code>$ ip link set eth0 up
$ ip link set lan1 up
$ ip link set lan2 up
$ ip link set lan3 up
$ ip link set lan4 up
$ ip link
1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1508 qdisc mq state UP qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
3: lan1@eth0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
4: lan2@eth0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
5: lan3@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff
6: lan4@eth0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN qlen 1000
    link/ether 02:ba:6f:0c:21:c4 brd ff:ff:ff:ff:ff:ff</code></pre>

<p>Now the switch is up you can see I have a cable plugged into the third port. This system hooks into a lot of the Linux networking so it Just Works(tm) with a lot of tooling. Some examples:</p>

<ul><li>Add a few of the lan ports into a standard Linux bridge and the switchdev system will bridge those ports together in the switch chip so Linux doesn't have to forward that traffic.</li>
<li>Thinks like <code>ethtool lan3</code> just work to get information about the link. and with <code>ethtool -S lan3</code> all the standard status return info which includes packets that have been fully handled by the switch.</li>
</ul>

<h2>Limitations</h2>

<p>There's a few things that makes this not very nice to work with. First of all the requirement of either building a custom network switch or tearing open an existing one and finding the right connections. </p>

<p>It's not really possible  to use this system on regular computers/servers since you need device trees to configure the kernel for this and most computers don't have kernel-controlled GPIO pins available to hook up a switch.</p>

<p>As far as I can find there's also no way to use this with a network port on the computer side that's not fixed, USB network interfaces don't have a device tree node handle to refer to to set the conduit port.</p>

<p>There is a chance some of these limitations are possible to work around, maybe there's some weird USB device that exposes pins on the GPIO subsystem, maybe there's a way to load switchdev without being on an ARM device but that would certainly take a bit more documentation...</p>


                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do not taunt happy fun branch predictor (2023) (200 pts)]]></title>
            <link>https://www.mattkeeter.com/blog/2023-01-25-branch/</link>
            <guid>40866374</guid>
            <pubDate>Wed, 03 Jul 2024 14:42:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mattkeeter.com/blog/2023-01-25-branch/">https://www.mattkeeter.com/blog/2023-01-25-branch/</a>, See on <a href="https://news.ycombinator.com/item?id=40866374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<!-- End header -->






<p>I've been writing a lot of AArch64 assembly, for <em>reasons</em>.</p>
<p>I recently came up with a "clever" idea to eliminate one jump from an inner
loop, and was surprised to find that it slowed things down.  Allow me to explain
my terrible error, so that you don't fall victim in the future.</p>
<p>A toy model of the relevant code looks something like this:</p>
<pre><code>float run(const float* data, size_t n) {
    float g = 0.0;
    while (n) {
        n--;
        const float f = *data++;
        foo(f, &amp;g);
    }
    return g;
}

static void foo(float f, float* g) {
    // do some stuff, modifying g
}
</code></pre>
<p>(eliding headers and the forward declaration of <code>foo</code> for space)</p>
<p>A simple translation into AArch64 assembly gives something like this:</p>
<pre><code>// x0: const float* data
// x1: size_t n
// Returns a single float in s0

// Prelude: store frame and link registers
stp   x29, x30, [sp, #-16]!

// Initialize g = 0.0
fmov s0, #0.0

loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    bl foo   // call the function
    b loop   // keep looping

foo:
    // Do some work, reading from s1 and accumulating into s0
    // ...
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>Here, <code>foo</code> is kinda like a <a href="https://github.com/rust-lang/rfcs/blob/master/text/1201-naked-fns.md">naked
function</a>:
it uses the same stack frame and registers as the parent function, reads from
<code>s1</code>, and writes to <code>s0</code>.</p>
<p>The call to <code>foo</code> uses the the <code>bl</code> instruction, which is "branch and link":
it jumps to the given label, and stores the <strong>next</strong> instruction address in the
link register (<code>lr</code> or <code>x30</code>).</p>
<p>When <code>foo</code> is done, the <code>ret</code> instruction jumps to the address in the link
register, which is the instruction following the original <code>bl</code>.</p>
<p>Looking at this code, I was struck by the fact that it does two branches,
one after the other.  Surely, it would be more efficient to only branch once.</p>
<p>I had the clever idea to do so <strong>without changing <code>foo</code></strong>:</p>
<pre><code>stp   x29, x30, [sp, #-16]!
fmov s0, #0.0

bl loop // Set up x30 to point to the loop entrance
loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

foo:
    // Do some work, accumulating into `s0`
    // ...
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This is a little subtle:</p>
<ul>
<li>The first call to <code>bl loop</code> stores the beginning of the <code>loop</code> block in <code>x30</code></li>
<li>After checking for loop termination, we fall through into the <code>foo</code> function
(without a branch!)</li>
<li><code>foo</code> still ends with <code>ret</code>, which returns to the <code>loop</code> block (because
that's what's in <code>x30</code>).</li>
</ul>
<p>Within the body of the loop, we never change <code>x30</code>, so the repeated <code>ret</code>
instructions always return to the same place.</p>
<p>I set up a benchmark using a very simple <code>foo</code>:</p>
<pre><code>foo:
    fadd s0, s0, s1
    ret
</code></pre>
<p>With this <code>foo</code>, the function as a whole sums the incoming array of <code>float</code>
values.</p>
<p>Benchmarking with <a href="https://docs.rs/criterion/latest/criterion/"><code>criterion</code></a>
(on an M1 Max CPU),
with a 1024-element array:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Original </td><td>969 ns
</td></tr><tr><td>"Optimized"</td><td>3.85 µs
</td></tr></tbody></table>
<p>The "optimized" code with one jump per loop is about <strong>4x slower</strong>
than the original version with two jumps per loop!</p>
<p>I found this surprising, so I asked a few colleagues about it.</p>
<p>Between <a href="https://hachyderm.io/@cliffle">Cliff</a> and
<a href="https://discuss.systems/@cross">Dan</a>,
the consensus was that mismatched <code>bl</code> / <code>ret</code>
pairs were confusing the
<a href="https://en.wikipedia.org/wiki/Branch_predictor">branch predictor</a>.</p>
<p>The <a href="https://developer.arm.com/documentation/102374/0101/Function-calls">ARM documentation</a> agrees:</p>
<blockquote>
<p>Why do we need a special function return instruction? Functionally, BR LR
would do the same job as RET. Using RET tells the processor that this is a
function return. Most modern processors, and all Cortex-A processors, support
branch prediction. Knowing that this is a function return allows processors to
more accurately predict the branch.</p>
<p>Branch predictors guess the direction the program flow will take across
branches. The guess is used to decide what to load into a pipeline with
instructions waiting to be processed. If the branch predictor guesses
correctly, the pipeline has the correct instructions and the processor does
not have to wait for instructions to be loaded from memory.</p>
</blockquote>
<p>More specifically, the branch predictor probably keeps an internal stack of
function return addresses, which is pushed to whenever a <code>bl</code> is executed. When
the branch predictor sees a <code>ret</code> coming down the pipeline, it assumes that
you're returning to the address associated with the most recent <code>bl</code> (and begins
prefetching / speculative execution / whatever), then pops that top address from
its internal stack.</p>
<p>This works if you've got matched <code>bl</code> / <code>ret</code> pairs, but the prediction will
fail if the same address is used by multiple <code>ret</code> instructions; you'll end up
with (<em>vague handwaving</em>) useless prefetching, incorrect speculative execution,
and pipeline stalls / flushes</p>
<p>Dan made the great suggestion of replacing <code>ret</code> with <code>br x30</code> to test this
theory.  Sure enough, this fixes the performance regression:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Matched <code>bl</code> / <code>ret</code> </td><td>969 ns
</td></tr><tr><td>One <code>bl</code>, many <code>ret</code></td><td>3.85 µs
</td></tr><tr><td>One <code>bl</code>, many <code>br x30</code></td><td>913 ns
</td></tr></tbody></table>
<p>In fact, it's slightly faster, probably because it's only doing one branch
per loop instead of two!</p>
<p>To further test the "branch predictor" theory, I opened up Instruments and
examined performance counters for the first two programs. Picking out the worst
offenders, the results seem conclusive:</p>
<table>
<tbody><tr><th>Counter</th><th>Matched <code>bl</code> / <code>ret</code></th><th>One <code>bl</code>, many <code>ret</code>
</th></tr><tr><td><code>BRANCH_RET_INDIR_MISPRED_NONSPECIFIC</code></td><td>92</td><td>928,644,975
</td></tr><tr><td><code>FETCH_RESTART</code></td><td>61,121</td><td>987,765,276
</td></tr><tr><td><code>MAP_DISPATCH_BUBBLE</code></td><td>1,155,632</td><td>7,350,085,139
</td></tr><tr><td><code>MAP_REWIND</code></td><td>6,412,734</td><td>2,789,499,545
</td></tr></tbody></table>
<p>These measurements are captured while summing an array of 1B elements.  We see
that with mismatched <code>bl</code> / <code>ret</code> pairs, the return branch predictor fails about
93% of the time!</p>
<p>Apple doesn't fully document these counters, but I'm guessing that the other
counters are downstream effects of bad branch prediction:</p>
<ul>
<li><code>FETCH_RESTART</code> is presumably bad prefetching</li>
<li><code>MAP_DISPATCH_BUBBLE</code> probably refers to <a href="https://en.wikipedia.org/wiki/Pipeline_stall">pipeline stalls</a></li>
<li><code>MAP_REWIND</code> might be bad speculative execution that needs to be rewound</li>
</ul>
<p>In conclusion,
<a href="https://www.youtube.com/watch?v=GmqeZl8OI2M">do not taunt happy fun branch predictor</a>
with asymmetric usage of <code>bl</code> and <code>ret</code> instructions.</p>
<hr>
<h2>Appendix: Going Fast</h2>
<p>Take a second look at this program:</p>
<pre><code>stp   x29, x30, [sp, #-16]!
fmov s0, #0.0

loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    bl foo   // call the function
    b loop   // keep looping

foo:
    fadd s0, s0, s1
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>Upon seeing this program, it's a common reaction to ask "why is <code>foo</code> a
subroutine at all?"</p>
<p>The answer is "because this is a didactic example, not code that's trying
to go as fast as possible".</p>
<p>Still, it's a fair question.  You wanna go fast?  Let's go fast.</p>
<p>If we know the contents of <code>foo</code> when building this
function (and it's shorter than the maximum jump distance), we can remove the
<code>bl</code> and <code>ret</code> entirely:</p>
<pre><code>loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    // foo is completely inlined here
    fadd s0, s0, s1

    b loop

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This is a roughly 6% speedup: from 969 ns to 911 ns.</p>
<p>We can get faster still by trusting the compiler:</p>
<pre><code>pub fn sum_slice(f: &amp;[f32]) -&gt; f32 {
    f.iter().sum()
}
</code></pre>
<p>This brings us down to 833 ns, a significant improvement!</p>
<p><a href="https://godbolt.org/z/Kv77abW6c">Looking at the assembly</a>,
it's doing some loop unrolling.
However, even when compiled with <code>-C target-cpu=native</code>, it's not generating
<a href="https://developer.arm.com/Architectures/Neon">NEON SIMD instructions</a>.
Can we beat it?</p>
<p><strong>We sure can!</strong></p>
<pre><code>stp   x29, x30, [sp, #-16]!

fmov s0, #0.0
dup v1.4s, v0.s[0]
dup v2.4s, v0.s[0]

loop:  // 1x per loop
    ands xzr, x1, #3
    b.eq simd

    sub x1, x1, #1
    ldr s3, [x0], #4

    fadd s0, s0, s3
    b loop

simd:  // 4x SIMD per loop
    ands xzr, x1, #7
    b.eq simd2

    sub x1, x1, #4
    ldp d3, d4, [x0], #16
    mov v3.d[1], v4.d[0]

    fadd v1.4s, v1.4s, v3.4s

    b simd

simd2:  // 2 x 4x SIMD per loop
    cmp x1, #0
    b.eq exit

    sub x1, x1, #8

    ldp d3, d4, [x0], #16
    mov v3.d[1], v4.d[0]
    fadd v1.4s, v1.4s, v3.4s

    ldp d5, d6, [x0], #16
    mov v5.d[1], v6.d[0]
    fadd v2.4s, v2.4s, v5.4s

    b simd2

exit: // function exit
    fadd v2.4s, v2.4s, v1.4s
    mov s1, v2.s[0]
    fadd s0, s0, s1
    mov s1, v2.s[1]
    fadd s0, s0, s1
    mov s1, v2.s[2]
    fadd s0, s0, s1
    mov s1, v2.s[3]
    fadd s0, s0, s1

    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This code includes three different loops:</p>
<ul>
<li>The first loop (<code>loop</code>) sums individual values
into <code>s0</code> until we have a multiple of four values remaining</li>
<li>The second loop (<code>simd</code>) uses SIMD instructions to sum 4 values at a time
into the vector register <code>v1</code>, until we have a multiple of 8 values remaining</li>
<li>The last loop (<code>simd2</code>) is the same as <code>simd</code>, but is unrolled 2x so it
handles 8 values per loop iteration, summing into <code>v1</code> and <code>v2</code></li>
</ul>
<p>At the function exit, we accumulate the values in the vector registers <code>v1</code>/<code>v2</code>
into <code>s0</code>, which is returned.</p>
<p>The type punning here is particularly cute:</p>
<pre><code>ldp d3, d4, [x0], #16
mov v3.d[1], v4.d[0]
fadd v1.4s, v1.4s, v3.4s
</code></pre>
<p>Remember, <code>x0</code> holds a <code>float*</code>.  We pretend that it's a <code>double*</code> to load 128
bits (i.e. 4x <code>float</code> values) into <code>d3</code> and <code>d4</code>.  Then, we move the "double" in <code>d4</code>
to occupy the top 64 bits of the <code>v3</code> vector register (of which <code>d3</code> is the
<em>lower</em> 64 bits).</p>
<p>Of course, each "double" is two floats, but that doesn't matter when shuffling
them around.  When summing with <code>fadd</code>, we tell the processor to treat them as
four floats (the <code>.4s</code> suffix), and everything works out fine.</p>
<p>How fast are we now?</p>
<p>This runs in 94 ns, or about <strong>8.8x faster</strong> than our previous best.</p>
<p>Here's a summary of performance:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Matched <code>bl</code> / <code>ret</code> </td><td>969 ns
</td></tr><tr><td>One <code>bl</code>, many <code>ret</code></td><td>3.85 µs
</td></tr><tr><td>One <code>bl</code>, many <code>br x30</code></td><td>913 ns
</td></tr><tr><td>Plain loop with <code>b</code></td><td>911 ns
</td></tr><tr><td>Rewrite it in Rust</td><td>833 ns
</td></tr><tr><td>SIMD + manual loop unrolling</td><td>94 ns
</td></tr></tbody></table>
<p>Could we get even faster?  I'm sure it's possible; I make no claims to being
the <a href="https://www.agner.org/optimize/">Agner Fog</a> of AArch64 assembly.</p>
<p>Still, this is a reasonable point to wrap up: we've demystified the initial
performance regression, and had some fun hand-writing assembly to go very
fast indeed.</p>
<p>The SIMD code does come with one asterisk, though: because floating-point
addition is not associative, and it performs the summation in a different
order, it <strong>may not get the same result</strong> as straight-line code.  In retrospect,
this is likely why the compiler doesn't generate SIMD instructions to compute
the sum!</p>
<p>Does this matter for your use case?  Only you can know!</p>
<hr>
<p>All of the code from this post is
<a href="https://github.com/mkeeter/arm64-test">published to GitHub</a>.</p>
<p>You can reproduce benchmarks by running <code>cargo bench</code> on an ARM64 machine.</p>

<!-- Begin footer -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Has anyone successfully pivoted from web dev to AI/ML development? (116 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40866311</link>
            <guid>40866311</guid>
            <pubDate>Wed, 03 Jul 2024 14:35:13 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40866311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="40866311">
      <td><span></span></td>      <td><center><a id="up_40866311" href="https://news.ycombinator.com/vote?id=40866311&amp;how=up&amp;goto=item%3Fid%3D40866311"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=40866311">Ask HN: Has anyone successfully pivoted from web dev to AI/ML development?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_40866311">106 points</span> by <a href="https://news.ycombinator.com/user?id=ent_superpos">ent_superpos</a> <span title="2024-07-03T14:35:13"><a href="https://news.ycombinator.com/item?id=40866311">4 hours ago</a></span> <span id="unv_40866311"></span> | <a href="https://news.ycombinator.com/hide?id=40866311&amp;goto=item%3Fid%3D40866311">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Has%20anyone%20successfully%20pivoted%20from%20web%20dev%20to%20AI%2FML%20development%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=40866311&amp;auth=1344ad73a11754902765b72c2daad9533241ef0b">favorite</a> | <a href="https://news.ycombinator.com/item?id=40866311">65&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>I am currently working as a senior full-stack web software engineer. I have a BSc in Computer Science, and on my own, I've been learning more about AI/ML/deep learning. I really enjoy working with it, and I'd love to find a way to work on AI stuff professionally. The problem is that I've been working as a web developer professionally for about 10 years now, and I have no idea how I would pivot to more of a AI/data science role.</p><p>Does anyone have an experience of making this transition? As a web dev, I am senior level, but I'm sure I'd have to start from scratch on some things in the AI space. At least I have a good foundation of programming in general, math, and computer science.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Living in a Lucid Dream (102 pts)]]></title>
            <link>https://www.noemamag.com/living-in-a-lucid-dream/</link>
            <guid>40866155</guid>
            <pubDate>Wed, 03 Jul 2024 14:18:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.noemamag.com/living-in-a-lucid-dream/">https://www.noemamag.com/living-in-a-lucid-dream/</a>, See on <a href="https://news.ycombinator.com/item?id=40866155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="article">

        <div role="group">
    <p>Credits</p>
    <p>Claire L. Evans is a writer and musician exploring ecology, technology and culture.</p>
</div>

<p>The first time it happened, it was an accident. But every dream is.</p>



<p>It would have been my last REM cycle of the night, had I been able to sleep. Instead, for the previous six hours, I had counted sheep, had dressed for imaginary occasions in my mind, had tried the Army sleep techniques, alternately imagined myself in a black velvet hammock and in a canoe on a calm, still lake. I’d meditated. I’d thought of my mother, a lifelong insomniac who has rarely slept more than four hours a night in her life. I’d tried everything and given up. All I could do was wait for morning.</p>







<p>The dream grabbed my ankles first, pulled at me like someone dislodging a drain. Out it tossed me through my sliding glass window, over the garden, over my quiet street, over the dark sleeping skyline, a ragdoll flung into the Santa Anas. I soared high enough to see Los Angeles’s motherboard of electric light. I could see the city in perfect detail below. I was asleep — no, I was awake! I <em>felt</em> the cold whipping through my hair as I tumbled like a deflating balloon through the sky. I felt the miles beneath me. I felt the warm pillow beneath my cheek. I dove, flew, dipped, conscious of it all.</p>



<p>So <em>this</em> is a lucid dream, I thought.</p>



<p>More than half of adults will have this <a href="https://pubmed.ncbi.nlm.nih.gov/27337287/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">experience</a> or one like it at some point in their lives. They’ll go to sleep, and as their REM cycles accumulate, as night shades into morning, as their sports car turns into a banana, they will suddenly realize, as I did: This is not real. This is a dream.</p>



<p>The flash of lucidity can come as quite a shock, enough to startle a novice into waking. But if you can hang on and stay conscious, you’ll be in for the ride of your life.</p>



<p>After the first one, my curiosity was piqued. I started lurking in online forums, reading accounts of similar experiences from communities of “<a href="https://www.reddit.com/r/Oneironauts/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">oneironauts</a>,” or dream explorers. From them I learned that lucid dreams need not be a fluke experience: I could cultivate and, with some practice, control them.</p>



<p>The first thing I needed to do was establish a baseline awareness of the difference between waking and dreaming using a technique called “critical state testing.” This is how it works: Several times a day, ask yourself if what you are experiencing is a dream. To make sure you’re awake, count your fingers. Plug your nose. Look at your watch, then look again to see if the numbers have moved.</p>



<p>If you do this often enough, the habit will spill over into your dreams. And when it does, you’ll find that your fingers are jelly. That you can breathe with your nose plugged. That your watch is unreadable. “dream standard time,” the psychophysiologist Stephen LaBerge called it in his 1990 manual, “Exploring the World of Lucid Dreaming”:asleep and awake at once.</p>



<h2 id="h-glimpses-from-the-dreamworld"><strong>Glimpses From The Dreamworld</strong></h2>



<p>Lucid dreams are as ancient as the mind. They’ve long been central to the Vajrayana Buddhist tradition, which teaches the cultivation of conscious awareness even in deep sleep. In the West, the philosophical literature of lucidity reaches back to <a href="https://classics.mit.edu/Aristotle/dreams.html" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Aristotle</a>. René Descartes, in his first “<a href="https://yale.learningu.org/download/041e9642-df02-4eed-a895-70e472df2ca4/H2665_Descartes'%2520Meditations.pdf" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Meditation</a>,” famously proposed that it’s impossible to prove the difference between dreaming and wakefulness on empirical grounds alone. Friedrich Nietzsche <a href="https://www.themarginalian.org/2016/04/21/nietzsche-on-dreams/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">wrote</a> accounts of his own lucid dreams.</p>



<p>But in the modern sciences, the subject remained unexplored well into the 20th century. Even by the late 1970s, most scientists and psychologists believed that lucid dreams were the product of fleeting awakenings during sleep, misremembered in the morning. The very idea of conscious awareness during sleep was considered impossible to measure. After all, subjective accounts of dreamers couldn’t be quantified — how could a dreamer really know the difference between <em>dreaming </em>that they were conscious and actually <em>being </em>conscious?</p>



<p>Anyone who has ever had a lucid dream knows this difference intuitively, but without a measurable physiological sign, lucidity was relegated to the anecdotal, subjective experience of dreamers. That is, until someone thought to dig deeper, and look — as though lifting a veil — behind their eyelids.</p>



<p>In the early 1950s, a graduate student named <a href="https://www.smithsonianmag.com/science-nature/the-stubborn-scientist-who-unraveled-a-mystery-of-the-night-91514538/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Eugene Aserinsky</a> hooked his 8-year-old son Armond up to a rudimentary brain-wave machine he had dragged up from the basement of a University of Chicago building. Aserinksy did not relish sleep research, at the time on the fringes of science, and the prospect of spending his nights awake, observing sleeping subjects, seemed “as exciting as warm milk.”</p>



<p>In the bleary morning, looking over a half-mile of polygraph paper, he despaired. The ink pens had drawn jagged lines that looked suspiciously like the <span data-note="The movement of the eye between fixation points.">saccades</span> his son’s eyes made when they’d calibrated the machine. They showed the eye movements of a waking person. But his son had been out cold on the lab’s army cot all night. “The research project was blowing up before me,” he later recalled.</p>


<!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      “The dream grabbed my ankles first, pulled at me like someone dislodging a drain. Out it tossed me through my sliding glass window, over the garden, over my quiet street, over the dark sleeping skyline, a ragdoll flung into the Santa Anas.”    </p>

    
    
  </blockquote>
</figure>




<p>As it turned out, there was nothing wrong with the machine. Following further study, Aserinsky discovered that his son’s sleeping brain was not — as believed by practically everyone, particularly his thesis advisor, the venerable sleep scientist Nathaniel Kleitman — simply <em>off. </em>Rather, brains roar to life in the darkness, in periods of active, “paradoxical” sleep that seemed to coincide with dreaming. Aserinsky considered naming the phenomenon “jerky eye movements” but, hoping to avoid being called a jerk himself, opted instead for “<a href="https://www.science.org/doi/10.1126/science.118.3062.273" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">rapid eye movements</a>,” or REM.</p>



<p>When Aserinsky left dream science behind in 1953, he passed the baton to a medical student, William C. Dement, who’d go on to establish the world’s first sleep disorders clinic at Stanford University. Years later, during a Stanford sleep study, Dement woke a subject during a period of unusually consistent back-and-forth eye movements. As LaBerge, once a student of Dement’s, recounted it, the subject had been dreaming about a ping-pong match. His eye movements during REM sleep had corresponded to his dream-gaze: left, right, left, right, following the phantom ball.</p>



<p>This anecdote sparked an idea in LaBerge. The study of lucid dreaming had always been stymied by its dependence on subjective self-reports. But eye movements are measurable. If a subject could <em>deliberately</em> move their eyes in a dream, LaBerge reasoned, then they could signal to outside observers, in real-time, that they were <em>awake</em> in there.</p>



<p>He designed an experiment around this premise, tucking a group of experienced lucid dreamers into bed at the Stanford sleep lab and asking them to make a series of prearranged eye movements the moment they became lucid. The eye measurements, unmistakable ping-pong volleys on the polygraph paper, corresponded precisely with the dreamers’ waking reports — all in the depths of REM sleep.</p>



<p>These “signal-verified” lucid dreams changed sleep research forever. Modern lucid dream studies rely on left-right-left-right eye signals to time-stamp experimental tasks and receive messages from the dreamworld. Lucid dreamers have been asked to signal before and after counting to 10, to measure if time unspools in dreams at the same speed as in waking life. (<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2013.01013/full" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">It does</a>.) They’ve been tasked to signal, with their eyes alone, the answers to simple math problems piped into speakers in the sleep room to establish if two-way communication is possible between a dreamer and a waking experimenter. (<a href="https://www.cell.com/current-biology/fulltext/S0960-9822(21)00059-2?_returnURL=https://linkinghub.elsevier.com/retrieve/pii/S0960982221000592?showall=true" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">It is</a>.) Eyes, windows of the soul, open the door to dreams too.</p>



<p>But what is <em>behind</em> that door, exactly? And what is it the lucid dreamers see in there?</p>



<h2 id="h-alive-and-mystically-beautiful"><strong>Alive And Mystically Beautiful</strong></h2>



<p>I began my critical state testing without much expectation of what it might bring. At first, I felt weird counting my fingers. Would my friends think I’d had a stroke? Had I been Reddit-brained? Was I no better than one of those people who believes life is a simulation? But the ritual was a good reminder to put down my phone. I began enjoying the blips of mindfulness it brought to my otherwise scatterbrained existence. My own hands took on the trippy quality that I remembered from being on mushrooms in college. It only took a few days for the habit to show up in my dreams.</p>



<p>Dreams are almost always phenomenally embodied. We look out onto the dreamscape and experience its uncanny forms and flavors largely from within an immersed, first-person perspective. Philosophers like to say that a dream is a “self-in-a-world” experience. Even as its fantastical physics support impossible actions like flying, it can never quite succeed in tearing mind from limb. You can never witness your own death in a dream, for example, because if you did, there would be no <em>you</em> left to dream it. As the philosopher <a href="https://press.princeton.edu/books/hardcover/9780691220093/when-animals-dream" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">David M. Peña-Guzmán</a> has written, “There is no dream without a dream ego, no dream ego without a dream body, and no dream body without a dream body-image.”</p>



<p>But our dream bodies are only loose sketches of the real thing. When I’m awake, my hands have ink stains and nagging splinters. I have 10 fingers, and the signet ring I wear on my right hand reads “JB,” my husband’s initials. The first time I thought to examine my hands in a dream, though, they looked like a bouquet of wilting fingers. The signet ring was etched with unreadable glyphs. It’s difficult to put into words the feeling this gave me. One of the key attributes of dreams is that they feel real in the moment. When I looked at my hands and saw the mutant flippers of a Midjourney hallucination, I felt the walls drop. My whole body flushed. Nothing was real here — least of all <em>me</em>.</p>



<p>The imprecise texture of the dream-body is a marked contrast to the dream landscape, which can be extraordinarily detailed, even growing in complexity under close examination. One of the earliest written accounts of a lucid dream in medical literature, <a href="https://dreamscience.ca/en/documents/New%2520content/lucid%2520dreaming%2520pdfs/vanEeden_PSPR_26_1-12_1913.pdf" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">recorded</a> by the Dutch psychiatrist Frederik van Eeden in the late 19th century, highlights this phenomenon:</p>



<div>
<p>I was floating through a landscape with bare trees, knowing that it was April, and I remarked that the perspective of the branches and twigs changed quite naturally. Then I made the reflection, during sleep, that my fancy would never be able to invent or to make an image as intricate as the perspective movement of little twigs seen in floating by.
          </p>
        


<!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      “Lucid dreams are as ancient as the mind.”    </p>

    
    
  </blockquote>
</figure>

</div>



<p>In his dream, van Eeden experienced a moment of reflection as he observed the remarkable naturalness of each little branch and twig passing beneath his flying body. The trees didn’t appear to him as imprecise, or “dreamlike” — quite the opposite. He even thought to himself that such a landscape, although fantastic, <em>must</em> be real because there was no way his own mind could have rendered those spare branches in such detail. Such reflections are considered “pre-lucid” because they signal the dreamer’s stirring awareness — their nagging feeling that something is <em>off.</em></p>



<p>In another classic dream <a href="https://www.biblio.com/book/astral-projection-record-out-body-experiences/d/1459779131" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">report</a>, the early 20th-century occultist and researcher Hugh Callaway recounted observing that the paving stones outside his home had changed orientation, turning parallel to the curb. This act of noticing brought the true nature of his dream into focus:</p>



<div><p>Then the solution flashed upon me: though this glorious summer morning seemed as real as it could be, I was dreaming! … Instantly, the vividness of life increased a hundred fold. Never had sea and sky and trees shone with such glamorous beauty; even the commonplace houses seemed alive and mystically beautiful.</p></div>



<p>In dreams as in waking life, attention makes the world alive. As many oneironauts have by now realized, critical state testing is not very different from mindfulness, meditation or similar practices of closely examining, sensing and being present with the world. These practices don’t only help us distinguish between waking and dreaming; they enrich our experience of both states.</p>



<p>In my own lucid dreams, a ripe, summer stone-fruit from the market explodes with flavor. The sensation of someone licking my belly feels wet. Pain and pleasure unfold in their phenomenal fullness. But this is always the case. The sea and sky always shine with their glamorous beauty. The world is detailed and rich. How often do I really <em>taste </em>a peach? How often do I take the time to examine the paving stones? Of course it’s trippy to examine my own hands: I never do it. I rarely look at <em>anything</em> so closely.</p>



<h2 id="h-higher-consciousness"><strong>Higher Consciousness</strong></h2>



<p>As the philosophers <a href="https://psycnet.apa.org/record/2007-09897-009" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Jennifer Windt and Thomas Metzinger</a> have observed, we travel through our lives as “naive realists,” assuming that as we perceive, touch and taste the world, we’re interfacing directly with some external, stable reality. But in truth, all experience, including our conscious experience of selfhood, relies on input from a chaotic world and is mediated by our limited sensory organs and shaped by our subjective internal landscape. Our model of reality is, in philosophical terms, “transparent,” so convincing as to be invisible. In this sense, dreams aren’t so different from waking experience: They’re both illusions we take, falsely, to be real.</p>



<p>“In normal experience, we give ourselves over to the world without really questioning whether it holds at the seams or not,” explained Peña-Guzmán, whose work explores the dreaming lives of animals. “What happens in lucidity is that you begin to attend to the seams and to the cracks — you begin attending to the fact that this is a constructed space, a model, a simulation.”</p>



<p>The process of attending to the seams is what cognitive scientists call <em>metacognition</em>: the ability to think about thinking. Many philosophers believe that metacognition in dreams requires the capacity for language, since one cannot make the judgement “I am in a dream” without a subject and a predicate. But Peña-Guzmán argues that animals may be able to recognize the oddness of their dreams in a more affective and embodied way — to feel, without forming a linguistic judgement, that their world’s gone weird. As with lucidity in people, this is above all a matter of <em>noticing.</em></p>



<p>That’s why techniques to cultivate lucid dreams, like counting fingers or examining the numbers on a clock, ask us to scrutinize the world. In people&nbsp;— and I sincerely hope in animals too — close attention sparks sudden clarity. This may take many forms. As Peña-Guzmán said, a dog may dream in smell, a bird in song. “Think about an electric eel that senses and produces electricity,” he said. “It’s conceivable that eel will have electric dreams, and there’s no way for us to even imagine what that is, because we don’t know what it means to experience electricity as meaningful. But they do.”</p>



<p>For humans, this phenomenal feast has another, more specifically cognitive flavor, too: the pleasure of awareness itself. Dreaming and waking perception are both illusory; they’re models constructed by our brains that turn sensory stimulus, or its absence, into meaning. In waking life, short of a heavy psychedelic experience, that illusion is all-encompassing; there’s no other level of consciousness to “wake up” into.</p>


<!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      “In dreams as in waking life, attention makes the world alive.”    </p>

    
    
  </blockquote>
</figure>




<p>But in lucid dreams, we can examine the construction closely. Does this make a lucid dream <em>more </em>conscious than waking? Is it something, perhaps, just shy of enlightenment?</p>



<p>I asked the philosopher Jennifer Windt these questions when I reached her across the sleep-wake divide; as Los Angeles shaded into night, it was already the next morning in Melbourne, where Windt is senior lecturer of philosophy at Monash University. She winced. “I would struggle with that description,” she said. “I think that really assumes that consciousness can be neatly ordered in levels.” Windt is part of a school of philosophers painting a far less hierarchical, more multidimensional portrait of consciousness, one informed by close study of edge cases like out-of-body experiences, lucid dreams, mind-wanderings and hyper-realistic false awakenings.</p>



<p>“Traditionally, it’s been thought that not only are sleep and waking opposites, but that they impose a kind of rift, a sharp distinction between conscious states,” she explained. But recent empirical and theoretical work supports the idea that consciousness takes many forms along a spectrum between sleep and waking. Daydreams and mind-wanderings, for example, may be caused by spells of “local sleep” in the waking brain, and lucid dreams, which are linked to the reactivation of the dorsolateral prefrontal cortex — the seat of executive ego function, which is turned off during normal REM sleep — can be thought of as shades of waking consciousness in the gradient of sleep.</p>



<p>According to Windt’s work, a dream is an immersive, spatiotemporal hallucination, an experience of being present in a world where thoughts constitute reality. But perhaps such experiences can happen when we’re awake, too. “I think we should remain open to the possibility that if we define these states phenomenologically,” Windt said, “we might find them occurring in different behavioral states as well.”</p>



<h2 id="h-what-a-good-hologram"><strong>What A Good Hologram</strong></h2>



<p>Every time I go to sleep, I think a new world into being and put myself at its center. I used to take this nightly miracle for granted because in dreams we all think with an immediacy and directness that leaves no room for distanced self-awareness. When the lucidity comes on, however, autobiographical memories come rushing back. I can compare the dream to waking life or to previous dreams. I can see clearly that the dream phone in my dream hand is made of Jell-O because I have access to “<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2737577/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">secondary consciousness</a>,” or the meta-awareness of my own state.</p>



<p>I can’t say I’m putting that secondary consciousness to much use yet; mostly, I look for the seams. I find myself at a party in an unfamiliar house and I touch the walls, raid the fridge. What’s in there? Do the vegetables vanish when I close the door or do they have a little hoedown, like in an old cartoon? I wander into the den and turn on the TV. On the news, there’s a map of a country called “Orovno.” Did I name this country, draw this map? <em>Borges would love this shit</em>, I think. I ask a dream-character his name, trying to ascertain if he is, in some sense, me. He says: “Jeremy Allen White.” Ok, maybe not.</p>



<p>Lucidity advocates like LaBerge, who parlayed his early Stanford lucidity research into a long career as a dream guru, advise their acolytes that dream-control is a learned skill. With time, they promise, I can become a master of my dream domain, commanding characters to my will and redecorating the landscape to my tastes. I can stock my own fridge, draw my own maps, use the dream as a rehearsal space, perfect my tennis backhand, conquer my fear of public speaking. This emphasis on control and optimization doesn’t entice me much. It seems to diminish some of what makes dreams interesting to begin with — their weirdness and mystery, their associative logic.</p>



<p>“The thing that makes a dream exciting, at least for me, is that it’s <em>not</em> the waking world,” said Adam Haar Horowitz, a dream researcher and cognitive scientist who, when I reached him over Zoom at his home in rural Alaska, is fittingly curled up on his bed. Horowitz is the co-inventor of the <a href="https://www.media.mit.edu/projects/sleep-creativity/overview/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Dormio</a>, a dream engineering device that makes it possible to intentionally incubate specific dreams. The Dormio takes advantage of the moments between being awake and falling asleep known as the hypnagogic state; just as visions begin to form in the darkness and you sense yourself falling, the device whispers prompts into your ear, like “dream of a tree.”</p>



<p>Hypnagogia is when the full-fledged visions of the dreamworld begin to appear against the background of perception. As they coalesce into immersive hallucinations, they are remarkably suggestible. Horowitz’s technique involves waking sleepers at precise intervals, helping narrate the dream as it takes shape; he does so to foster creativity, treat recurrent nightmares in veterans and incubate bereavement dreams for people who have lost loved ones.</p>



<p>In these, he explained, lucidity would spoil the experience. Imagine sitting across the kitchen table from your deceased parent. “You don’t know it’s a dream,” Horowitz said. “That’s the beautiful thing. You’re sitting with them. Why would I want to be in a dream and <em>know</em> it’s a dream? I want to be in the room and want to have the conversation with the person. I don’t want to poke them and say, ‘Wow, what a good hologram.’”</p>


<!-- Quote Block Template -->

<figure>

  <blockquote>

    <p>
      “No wonder so many cultures have rituals of group dreaming.”    </p>

    
    
  </blockquote>
</figure>




<p>Horowitz finds the culture of lucid dreaming, with its emphasis on individuality and self-optimization, to be somewhat suspect. He asks me, rhetorically: What do people <em>do</em> in lucid dreams? They fulfill fantasies of control, seek personal thrills like flying, have sex with Jeremy Allen White.</p>



<p>The Dormio system is the only dream incubation tool of its type in experimental use, but there are plenty of consciousness-hacking <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6517539/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">devices</a> on the market intended to induce lucid dreams on demand. LaBerge has developed several over the decades with names like the DreamLight and the NovaDreamer. A well-funded AI startup,<a href="https://www.prophetic.com/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer"> Prophetic</a>, is currently training machine learning models on EEG and fMRI lucid dream data, hoping to beam their findings via transcranial-focused ultrasound directly into willing brains; their Halo device, “the most advanced consumer neurotechnology wearable ever created,” will soon be available.</p>



<p>As the media scholar Aleena Chia has <a href="https://openpublishing.library.umass.edu/cpo/article/id/53/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">written</a>, such technologies promise a “youtopia of automatized dream entertainment.” In certain sectors of Silicon Valley, lucid dreaming is talked about like virtual reality without a headset. “What a lame way to relate to your own consciousness,” said Horowitz.</p>



<p>Dream incubation, on the other hand, he said, “is a really holy, really humble, really hands-off tradition” with ancient roots. In classical Greek antiquity, dream-seekers traveled to dedicated temples in order to seek specific dreams of healing or guidance; after rituals of purification and supplication, they slept, communally, in sanctuaries dedicated to this purpose. As the religious scholar Kimberley C. Patton has proposed in her work, these temples were “god-haunted” places where the gods used dreams to speak between worlds. In the tradition of incubation, she has written, “Gods and human beings are the co-creators of dreams in the darkness of our mutual sleep.”</p>



<p>In the morning, the temple dreamers interpreted their visions with the guidance of priests. Nothing about the dream was private; it was something to be plumbed in public alongside others upon waking with the morning. The dream’s prescriptions, a cure proffered by the gods, were enacted in waking life, with the full support of the dreamers’ communities. It is from this tradition, far more ancient than Freud, that we get dream interpretation. Similar rituals of public dream incubation have existed throughout history and all over the world, in medieval Japanese Buddhism, in Shia and Sufi Islamic traditions, in Bengal, among the ancient and modern highland Maya, and in many North American Indigenous cultures.</p>



<p>I’ve had a few flying dreams since my first lucid dream. I’ve taken swan dives off fire escapes, soared over fields of wildflowers, been pulled into the sun itself. It can feel exhilarating to fly, to feel velocity amid total stillness. To linger in the light of an imagined sky. But it’s cold up there, too, trapped inside the dream. I can see it all, but I have nobody to share it with. No wonder so many cultures have rituals of group dreaming. No wonder they traveled far and wide to dream together, to seek succor and interpretation from oneiromantic priests. It makes the night less lonely.</p>

          
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Proton launches its own version of Google Docs (327 pts)]]></title>
            <link>https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html</link>
            <guid>40864914</guid>
            <pubDate>Wed, 03 Jul 2024 11:25:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html">https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html</a>, See on <a href="https://news.ycombinator.com/item?id=40864914">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/protons-windows-and-macos-mail-app-is-out-of-beta-and-available-now-110010822.html" data-ylk="slk:Proton;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas">Proton</a> now has its own version of Google Docs in its Drive cloud storage service, and like the company's other products, it comes with end-to-end encryption. The company says its flavor of Docs "offers a unique solution in a market where most popular products neglect privacy" and recommends it for use in the healthcare, media, finance and legal industries. <a data-i13n="elm:affiliate_link;sellerN:;elmt:;cpos:2;pos:1" href="https://shopping.yahoo.com/rdlw?siteId=us-engadget&amp;pageId=1p-autolink&amp;featureId=text-link&amp;custData=eyJzb3VyY2VOYW1lIjoiV2ViLURlc2t0b3AtVmVyaXpvbiIsImxhbmRpbmdVcmwiOiJodHRwczovL3Byb3Rvbi5tZS9ibG9nL2RvY3MtcHJvdG9uLWRyaXZlIiwiY29udGVudFV1aWQiOiJjMmYyMGEyYS05ZGEzLTQyZmMtYjhmMC03MGI3ZDRlMGFhNmYifQ&amp;signature=AQAAAWZaQ6nJO7zP4rH3JS2Y44qB7q_HbbOpt8RRVvU6nYbn&amp;gcReferrer=https%3A%2F%2Fproton.me%2Fblog%2Fdocs-proton-drive" rel="nofollow noopener" target="_blank" data-ylk="slk:Proton Docs;elm:affiliate_link;sellerN:;elmt:;cpos:2;pos:1;itc:0;sec:content-canvas">Proton Docs</a> has advanced formatting and image embed options like Google Docs has and can create, open and edit documents in multiple formats, including Microsoft .docx.</p><p>It has collaboration tools similar to Google Docs', as well. Users can invite anyone to view and edit their documents, though those without a Proton account will be prompted to create one first. The free tier of Proton Drive includes essential document features so people don't have to pay for the service if they don't want to. Participants will be able to add comments to the document, reply to them and resolve them. And users will see other participants' presence and their cursor placements in real time, so that they know who's working on which part of the document and so that their edits don't clash.</p><p>Proton didn't say whether the launch of Docs means it's going to roll out analogues of Google's other Workspace apps in the future, but the company did <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/proton-encrypted-email-vpn-calendar-rebrand-103024950.html" data-ylk="slk:expand its offerings;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas">expand its offerings</a> with several different products over the last few years. In addition to Drive cloud storage — and, of course, its email service — the company has a VPN, an encrypted calendar and even a <a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/proton-launches-its-own-password-manager-115039870.html" data-ylk="slk:password manager;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas">password manager</a>. Docs will make its way to Proton users over the coming days.</p><p>This article contains affiliate links; if you click such a link and make a purchase, we may earn a commission.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Jb / json.bash – Command-line tool (and bash library) that creates JSON (116 pts)]]></title>
            <link>https://github.com/h4l/json.bash</link>
            <guid>40864541</guid>
            <pubDate>Wed, 03 Jul 2024 10:18:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/h4l/json.bash">https://github.com/h4l/json.bash</a>, See on <a href="https://news.ycombinator.com/item?id=40864541">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto"><code>$ jb name=json.bash creates=JSON</code></h2><a id="user-content--jb-namejsonbash-createsjson" aria-label="Permalink: $ jb name=json.bash creates=JSON" href="#-jb-namejsonbash-createsjson"></a></p>
<p dir="auto"><code>json.bash</code> is a command-line tool and bash library that creates JSON.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb name=json.bash creates=JSON dependencies:[,]=Bash,Grep
{&quot;name&quot;:&quot;json.bash&quot;,&quot;creates&quot;:&quot;JSON&quot;,&quot;dependencies&quot;:[&quot;Bash&quot;,&quot;Grep&quot;]}

$ # Values are strings unless explicitly typed
$ jb id=42 size:number=42 surname=null data:null
{&quot;id&quot;:&quot;42&quot;,&quot;size&quot;:42,&quot;surname&quot;:&quot;null&quot;,&quot;data&quot;:null}

$ # Reference variables with @name
$ id=42 date=2023-06-23 jb @id created@date modified@date
{&quot;id&quot;:&quot;42&quot;,&quot;created&quot;:&quot;2023-06-23&quot;,&quot;modified&quot;:&quot;2023-06-23&quot;}

$ # Pull data from files
$ printf hunter2 > /tmp/password; jb @/tmp/password
{&quot;password&quot;:&quot;hunter2&quot;}

$ # Pull data from shell pipelines
$ jb sizes:number[]@<(seq 1 4)
{&quot;sizes&quot;:[1,2,3,4]}

$ # Nest jb calls
$ jb type=club members:json[]@<(jb name=Bob; jb name=Alice)
{&quot;type&quot;:&quot;club&quot;,&quot;members&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}

$ # The Bash API can reference arrays and create JSON efficiently — without forking
$ source json.bash
$ out=people json name=Bob; out=people json name=Alice; sizes=(42 91 2)
$ id=&quot;abc.123&quot; json @id @sizes:number[] @people:json[]
{&quot;id&quot;:&quot;abc.123&quot;,&quot;sizes&quot;:[42,91,2],&quot;people&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}"><pre>$ <span>jb name=json.bash creates=JSON dependencies:[,]=Bash,Grep</span>
<span>{"name":"json.bash","creates":"JSON","dependencies":["Bash","Grep"]}</span>

$ <span><span><span>#</span> Values are strings unless explicitly typed</span></span>
$ <span>jb id=42 size:number=42 surname=null data:null</span>
<span>{"id":"42","size":42,"surname":"null","data":null}</span>

$ <span><span><span>#</span> Reference variables with @name</span></span>
$ <span>id=42 date=2023-06-23 jb @id created@date modified@date</span>
<span>{"id":"42","created":"2023-06-23","modified":"2023-06-23"}</span>

$ <span><span><span>#</span> Pull data from files</span></span>
$ <span><span>printf</span> hunter2 <span>&gt;</span> /tmp/password<span>;</span> jb @/tmp/password</span>
<span>{"password":"hunter2"}</span>

$ <span><span><span>#</span> Pull data from shell pipelines</span></span>
$ <span>jb sizes:number[]@<span><span>&lt;(</span>seq 1 4<span>)</span></span></span>
<span>{"sizes":[1,2,3,4]}</span>

$ <span><span><span>#</span> Nest jb calls</span></span>
$ <span>jb type=club members:json[]@<span><span>&lt;(</span>jb name=Bob<span>;</span> jb name=Alice<span>)</span></span></span>
<span>{"type":"club","members":[{"name":"Bob"},{"name":"Alice"}]}</span>

$ <span><span><span>#</span> The Bash API can reference arrays and create JSON efficiently — without forking</span></span>
$ <span><span>source</span> json.bash</span>
$ <span>out=people json name=Bob<span>;</span> out=people json name=Alice<span>;</span> sizes=(42 91 2)</span>
$ <span>id=<span><span>"</span>abc.123<span>"</span></span> json @id @sizes:number[] @people:json[]</span>
<span>{"id":"abc.123","sizes":[42,91,2],"people":[{"name":"Bob"},{"name":"Alice"}]}</span></pre></div>
<p dir="auto"><code>json.bash</code>'s <em><a href="https://en.wikipedia.org/wiki/Unix_philosophy" rel="nofollow">one thing</a></em> is to get shell-native data (environment variables,
files, program output) to somewhere else, using JSON encapsulate it robustly.</p>
<p dir="auto">Creating JSON from the command line or a shell script can be useful when:</p>
<ul dir="auto">
<li>You need some ad-hoc JSON to interact with a JSON-consuming application</li>
<li>You need to bundle up some data to share or move elsewhere. JSON can a good
alternative to base64-encoding or a file archive.</li>
</ul>
<p dir="auto">It does no transformation or filtering itself, instead it pulls data from things
you <strong>already know how to use</strong>, like files, command-line arguments, environment
variables, shell pipelines and shell scripts. It glues together data from these
sources, giving it enough structure to make the data easy to consume reliably in
downstream programs.</p>
<p dir="auto">It's something like a reverse <code>tee</code> — it pulls together data sources, using JSON
to represent the aggregation. It's not an alternative to to data-processing
tools like <code>jq</code>, rather it helps assemble JSON to send into JSON-consuming tools
like <code>jq</code>.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents</h2><a id="user-content-contents" aria-label="Permalink: Contents" href="#contents"></a></p>
<ol dir="auto">
<li><a href="#install">Install</a></li>
<li><a href="#how-to-guides">How-to guides</a></li>
<li><a href="#background--performance-notes">Background &amp; performance notes</a></li>
<li><a href="#credits">Credits</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Container image</h3><a id="user-content-container-image" aria-label="Permalink: Container image" href="#container-image"></a></p>
<p dir="auto">We publish the container image
<a href="https://github.com/h4l/json.bash/pkgs/container/json.bash%2Fjb"><code>ghcr.io/h4l/json.bash/jb</code></a>
with <code>jb-*</code> and <code>json.bash</code>, perhaps useful to try without installing.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ docker container run --rm ghcr.io/h4l/json.bash/jb msg=Hi
{&quot;msg&quot;:&quot;Hi&quot;}

$ # Get a bash shell to try things interactively
$ docker container run --rm -it ghcr.io/h4l/json.bash/jb
bash-5.2# jb os-release:{}@<(xargs < /etc/os-release env -i)
{&quot;os-release&quot;:{&quot;NAME&quot;:&quot;Alpine Linux&quot;,&quot;ID&quot;:&quot;alpine&quot;,&quot;VERSION_ID&quot;:&quot;3.18.2&quot;,&quot;PRETTY_NAME&quot;:&quot;Alpine Linux v3.18&quot;,&quot;HOME_URL&quot;:&quot;https://alpinelinux.org/&quot;,&quot;BUG_REPORT_URL&quot;:&quot;https://gitlab.alpinelinux.org/alpine/aports/-/issues&quot;}}"><pre>$ <span>docker container run --rm ghcr.io/h4l/json.bash/jb msg=Hi</span>
<span>{"msg":"Hi"}</span>

$ <span><span><span>#</span> Get a bash shell to try things interactively</span></span>
$ <span>docker container run --rm -it ghcr.io/h4l/json.bash/jb</span>
<span>bash-5.2# jb os-release:{}@&lt;(xargs &lt; /etc/os-release env -i)</span>
<span>{"os-release":{"NAME":"Alpine Linux","ID":"alpine","VERSION_ID":"3.18.2","PRETTY_NAME":"Alpine Linux v3.18","HOME_URL":"https://alpinelinux.org/","BUG_REPORT_URL":"https://gitlab.alpinelinux.org/alpine/aports/-/issues"}}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">OS Packages</h3><a id="user-content-os-packages" aria-label="Permalink: OS Packages" href="#os-packages"></a></p>
<p dir="auto">Package-manager files are available for any package manager supported by
<a href="https://fpm.readthedocs.io/" rel="nofollow"><code>fpm</code></a> (at least apk, deb, freebsd, rpm, sh (self extracting), tar,
possibly more).</p>
<p dir="auto">We publish the container image
<a href="https://github.com/h4l/json.bash/pkgs/container/json.bash%2Fpkg"><code>ghcr.io/h4l/json.bash/pkg</code></a>
that can generate a package file in whichever format you like:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ docker container run --rm -v &quot;$(pwd):/pkg&quot; ghcr.io/h4l/json.bash/pkg deb
Generating: /pkg/json.bash_0.2.2-dev.deb

$ ls
json.bash_0.2.2-dev.deb
$ dpkg -i /pkg/json.bash_0.2.2-dev.deb"><pre>$ <span>docker container run --rm -v <span><span>"</span><span><span>$(</span>pwd<span>)</span></span>:/pkg<span>"</span></span> ghcr.io/h4l/json.bash/pkg deb</span>
<span>Generating: /pkg/json.bash_0.2.2-dev.deb</span>

$ <span>ls</span>
<span>json.bash_0.2.2-dev.deb</span>
$ <span>dpkg -i /pkg/json.bash_0.2.2-dev.deb</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual install</h3><a id="user-content-manual-install" aria-label="Permalink: Manual install" href="#manual-install"></a></p>
<p dir="auto">Installing manually is quite straightforward.</p>
<details>
  <summary>Expand this for instructions</summary>
<div dir="auto" data-snippet-clipboard-copy-content="# Alternatively, use /usr/local/bin to install system-wide
cd ~/.local/bin
curl -fsSL -O &quot;https://raw.githubusercontent.com/h4l/json.bash/HEAD/json.bash&quot;
chmod +x json.bash
ln -s json.bash jb
ln -s json.bash jb-array

# If your shell is bash, you can alias jb and jb-array to the bash functions for
# better performance. You should add this line to your ~/.bashrc
source json.bash; alias jb=json jb-array=json.array

# Optional: if you'd also like jb-echo, jb-cat, jb-stream
for name in jb-echo jb-cat jb-stream; do
  curl -fsSL -O &quot;https://raw.githubusercontent.com/h4l/json.bash/HEAD/bin/${name:?}&quot;
  chmod +x &quot;${name:?}&quot;
done"><pre><span><span>#</span> Alternatively, use /usr/local/bin to install system-wide</span>
<span>cd</span> <span>~</span>/.local/bin
curl -fsSL -O <span><span>"</span>https://raw.githubusercontent.com/h4l/json.bash/HEAD/json.bash<span>"</span></span>
chmod +x json.bash
ln -s json.bash jb
ln -s json.bash jb-array

<span><span>#</span> If your shell is bash, you can alias jb and jb-array to the bash functions for</span>
<span><span>#</span> better performance. You should add this line to your ~/.bashrc</span>
<span>source</span> json.bash<span>;</span> <span>alias</span> jb=json jb-array=json.array

<span><span>#</span> Optional: if you'd also like jb-echo, jb-cat, jb-stream</span>
<span>for</span> <span>name</span> <span>in</span> jb-echo jb-cat jb-stream<span>;</span> <span>do</span>
  curl -fsSL -O <span><span>"</span>https://raw.githubusercontent.com/h4l/json.bash/HEAD/bin/<span>${name<span>:?</span>}</span><span>"</span></span>
  chmod +x <span><span>"</span><span>${name<span>:?</span>}</span><span>"</span></span>
<span>done</span></pre></div>
<p dir="auto">To uninstall, remove <code>json.bash</code>, <code>jb</code>, <code>jb-array</code>, <code>jb-echo</code>, <code>jb-cat</code> and
<code>jb-stream</code> from the directory you installed them to (run <code>which -a json.bash</code>
to find where it is).</p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">How-to guides</h2><a id="user-content-how-to-guides" aria-label="Permalink: How-to guides" href="#how-to-guides"></a></p>
<ol dir="auto">
<li><a href="#the-jsonbash-commands">The <code>json.bash</code> commands</a></li>
<li><a href="#object-keys">Object keys</a></li>
<li><a href="#object-values">Object values</a></li>
<li><a href="#arrays-mixed-types-fixed-length">Arrays (mixed types, fixed length)</a></li>
<li><a href="#argument-types">Argument types</a></li>
<li><a href="#array-values-uniform-types-variable-length">Array values (uniform types, variable length)</a></li>
<li><a href="#object-values-uniform-types-variable-length">Object values (uniform types, variable length)</a></li>
<li><a href="#-arguments-merge-entries-into-the-host-objectarray"><code>...</code> arguments (merge entries into the host object/array)</a></li>
<li><a href="#missing--empty-values">Missing / empty values</a></li>
<li><a href="#nested-json-with-json-and-raw-types">Nested JSON with <code>:json</code> and <code>:raw</code> types</a></li>
<li><a href="#file-references">File references</a></li>
<li><a href="#argument-structure">Argument structure</a></li>
<li><a href="#error-handling">Error handling</a></li>
<li><a href="#security-and-correctness">Security and correctness</a></li>
<li><a href="#jb-cat-jb-echo-jb-stream-utility-programs"><code>jb-cat</code>, <code>jb-echo</code>, <code>jb-stream</code> utility programs</a></li>
<li><a href="#streaming-output">Streaming output</a></li>
</ol>
<p dir="auto">These examples mostly use <code>jb</code>, which is the <code>json.bash</code> library run as a
stand-alone program. From within a bash script you get better performance by
running <code>source json.bash</code> and using the <code>json</code> bash function, which is a
superset of stand-alone <code>jb</code> and much faster because it doesn't execute new
child processes when called. See the
<a href="#background--performance-notes">Background &amp; performance notes</a> section for
more.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The <code>json.bash</code> commands</h3><a id="user-content-the-jsonbash-commands" aria-label="Permalink: The json.bash commands" href="#the-jsonbash-commands"></a></p>
<p dir="auto"><code>jb</code> / <code>jb-array</code> / <code>json</code> / <code>json.array</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # The jb program creates JSON objects
$ jb
{}

$ # The jb-array creates arrays, but otherwise works like jb.
$ jb-array :number=4
[4]

$ # From a bash shell or bash script, use the json and json.array functions
$ source json.bash  # no path is needed if json.bash is on $PATH
$ json
{}

$ # json.array creates arrays, but otherwise works like json
$ json.array
[]"><pre>$ <span><span><span>#</span> The jb program creates JSON objects</span></span>
$ <span>jb</span>
<span>{}</span>

$ <span><span><span>#</span> The jb-array creates arrays, but otherwise works like jb.</span></span>
$ <span>jb-array :number=4</span>
<span>[4]</span>

$ <span><span><span>#</span> From a bash shell or bash script, use the json and json.array functions</span></span>
$ <span><span>source</span> json.bash  <span><span>#</span> no path is needed if json.bash is on $PATH</span></span>
$ <span>json</span>
<span>{}</span>

$ <span><span><span>#</span> json.array creates arrays, but otherwise works like json</span></span>
$ <span>json.array</span>
<span>[]</span></pre></div>
<p dir="auto">Each argument defines an entry in the object or array. Arguments can contain a
key, type and value in this structure:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/h4l/json.bash/blob/main/docs/syntax-diagrams/minimal-argument.svg"><img width="100%" src="https://github.com/h4l/json.bash/raw/main/docs/syntax-diagrams/minimal-argument.svg" alt="A railroad syntax diagram showing a high-level summary of the key, type and value structure of an argument." title="Minimal Argument Structure Diagram"></a></p>
<p dir="auto">The <a href="#argument-structure">Argument structure</a> section has more details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Object keys</h3><a id="user-content-object-keys" aria-label="Permalink: Object keys" href="#object-keys"></a></p>
<p dir="auto">Each argument creates an entry in the JSON object. The first part of each
argument defines the key.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb msg=hi
{&quot;msg&quot;:&quot;hi&quot;}

$ # Keys can contain most characters (except @:=, unless escaped)
$ jb &quot;🐚&quot;=JSON
{&quot;🐚&quot;:&quot;JSON&quot;}

$ # Key values can come from variables
$ key=&quot;The Message&quot; jb @key=hi
{&quot;The Message&quot;:&quot;hi&quot;}

$ # Key variables can contain any characters
$ key=&quot;@key:with=reserved-chars&quot; jb @key=hi
{&quot;@key:with=reserved-chars&quot;:&quot;hi&quot;}

$ # Each argument defines a key
$ var=c jb a=X b=Y @var=Z
{&quot;a&quot;:&quot;X&quot;,&quot;b&quot;:&quot;Y&quot;,&quot;c&quot;:&quot;Z&quot;}

$ # Keys may be reused, but should not be, because JSON parser behaviour for
$ # duplicate keys is undefined.
$ jb a=A a=B a=C
{&quot;a&quot;:&quot;A&quot;,&quot;a&quot;:&quot;B&quot;,&quot;a&quot;:&quot;C&quot;}

$ # The reserved characters can be escaped by doubling them
$ jb =@@handle=ok a::z=ok 1+1==2=ok
{&quot;@handle&quot;:&quot;ok&quot;,&quot;a:z&quot;:&quot;ok&quot;,&quot;1+1=2&quot;:&quot;ok&quot;}"><pre>$ <span>jb msg=hi</span>
<span>{"msg":"hi"}</span>

$ <span><span><span>#</span> Keys can contain most characters (except @:=, unless escaped)</span></span>
$ <span>jb <span><span>"</span>🐚<span>"</span></span>=JSON</span>
<span>{"🐚":"JSON"}</span>

$ <span><span><span>#</span> Key values can come from variables</span></span>
$ <span>key=<span><span>"</span>The Message<span>"</span></span> jb @key=hi</span>
<span>{"The Message":"hi"}</span>

$ <span><span><span>#</span> Key variables can contain any characters</span></span>
$ <span>key=<span><span>"</span>@key:with=reserved-chars<span>"</span></span> jb @key=hi</span>
<span>{"@key:with=reserved-chars":"hi"}</span>

$ <span><span><span>#</span> Each argument defines a key</span></span>
$ <span>var=c jb a=X b=Y @var=Z</span>
<span>{"a":"X","b":"Y","c":"Z"}</span>

$ <span><span><span>#</span> Keys may be reused, but should not be, because JSON parser behaviour for</span></span>
$ <span><span><span>#</span> duplicate keys is undefined.</span></span>
$ <span>jb a=A a=B a=C</span>
<span>{"a":"A","a":"B","a":"C"}</span>

$ <span><span><span>#</span> The reserved characters can be escaped by doubling them</span></span>
$ <span>jb =@@handle=ok a::z=ok 1+1==2=ok</span>
<span>{"@handle":"ok","a:z":"ok","1+1=2":"ok"}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Object values</h3><a id="user-content-object-values" aria-label="Permalink: Object values" href="#object-values"></a></p>
<p dir="auto">The last part of each argument after a <code>=</code> or <code>@</code> defines the value. Values can
contain their value directly, or reference a variable or file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb message=&quot;Hello World&quot;
{&quot;message&quot;:&quot;Hello World&quot;}

$ greeting=&quot;Hi there&quot; jb message@greeting
{&quot;message&quot;:&quot;Hi there&quot;}

$ # Variable references without a value define the key and value in one go.
$ greeting=&quot;Hi&quot; name=Bob jb @greeting @name
{&quot;greeting&quot;:&quot;Hi&quot;,&quot;name&quot;:&quot;Bob&quot;}

$ # This also applies (less usefully) to inline entries.
$ jb message
{&quot;message&quot;:&quot;message&quot;}

$ # Inline values following a `=` have no content restrictions.
$ jb message=@value:with=reserved-chars
{&quot;message&quot;:&quot;@value:with=reserved-chars&quot;}

$ # @ values that begin with / or ./ are references to files
$ printf hunter2 > /tmp/password; jb secret@/tmp/password
{&quot;secret&quot;:&quot;hunter2&quot;}

$ # File references without a value define the key and value in one go.
$ jb @/tmp/password
{&quot;password&quot;:&quot;hunter2&quot;}"><pre>$ <span>jb message=<span><span>"</span>Hello World<span>"</span></span></span>
<span>{"message":"Hello World"}</span>

$ <span>greeting=<span><span>"</span>Hi there<span>"</span></span> jb message@greeting</span>
<span>{"message":"Hi there"}</span>

$ <span><span><span>#</span> Variable references without a value define the key and value in one go.</span></span>
$ <span>greeting=<span><span>"</span>Hi<span>"</span></span> name=Bob jb @greeting @name</span>
<span>{"greeting":"Hi","name":"Bob"}</span>

$ <span><span><span>#</span> This also applies (less usefully) to inline entries.</span></span>
$ <span>jb message</span>
<span>{"message":"message"}</span>

$ <span><span><span>#</span> Inline values following a `=` have no content restrictions.</span></span>
$ <span>jb message=@value:with=reserved-chars</span>
<span>{"message":"@value:with=reserved-chars"}</span>

$ <span><span><span>#</span> @ values that begin with / or ./ are references to files</span></span>
$ <span><span>printf</span> hunter2 <span>&gt;</span> /tmp/password<span>;</span> jb secret@/tmp/password</span>
<span>{"secret":"hunter2"}</span>

$ <span><span><span>#</span> File references without a value define the key and value in one go.</span></span>
$ <span>jb @/tmp/password</span>
<span>{"password":"hunter2"}</span></pre></div>
<p dir="auto">File references are more powerful than they might first appear, as they enable
all sorts of dynamic content to be pulled into JSON data, including nested <code>jb</code>
calls. See <a href="#file-references">File references</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Arrays (mixed types, fixed length)</h3><a id="user-content-arrays-mixed-types-fixed-length" aria-label="Permalink: Arrays (mixed types, fixed length)" href="#arrays-mixed-types-fixed-length"></a></p>
<p dir="auto">Creating arrays is much like creating objects — arguments hold values, either
directly, or referencing variables or files.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb-array Hi &quot;Bob Bobson&quot;
[&quot;Hi&quot;,&quot;Bob Bobson&quot;]

$ message=Hi name=&quot;Bob Bobson&quot; jb-array @message @name
[&quot;Hi&quot;,&quot;Bob Bobson&quot;]

$ printf 'Bob Bobson' > /tmp/name
$ jb-array Hi @/tmp/name
[&quot;Hi&quot;,&quot;Bob Bobson&quot;]

$ # Array values in arguments cannot contain @:= characters (unless escaped by
$ # doubling them), because they would clash with @variable and :type syntax.
$ # However, values following a = can contain anything, so long as they follow a
$ # key or type section.
$ jb-array :='@foo:bar=baz' :='{&quot;not&quot;:&quot;parsed&quot;}' =@@es::cap==ed
[&quot;@foo:bar=baz&quot;,&quot;{\&quot;not\&quot;:\&quot;parsed\&quot;}&quot;,&quot;@es:cap=ed&quot;]

$ # Values from variables have no restrictions. Arrays use the same argument
$ # syntax as objects, so values in the key or value position work the same.
$ s1='@foo:bar=baz' s2='{&quot;not&quot;:&quot;parsed&quot;}' jb-array @s1: :@s2
[&quot;@foo:bar=baz&quot;,&quot;{\&quot;not\&quot;:\&quot;parsed\&quot;}&quot;]

$ # It's possible to set a key as well as value for array entries, but the key
$ # is ignored.
$ a=A b=B jb-array @a@a @b=B c=C
[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]"><pre>$ <span>jb-array Hi <span><span>"</span>Bob Bobson<span>"</span></span></span>
<span>["Hi","Bob Bobson"]</span>

$ <span>message=Hi name=<span><span>"</span>Bob Bobson<span>"</span></span> jb-array @message @name</span>
<span>["Hi","Bob Bobson"]</span>

$ <span><span>printf</span> <span><span>'</span>Bob Bobson<span>'</span></span> <span>&gt;</span> /tmp/name</span>
$ <span>jb-array Hi @/tmp/name</span>
<span>["Hi","Bob Bobson"]</span>

$ <span><span><span>#</span> Array values in arguments cannot contain @:= characters (unless escaped by</span></span>
$ <span><span><span>#</span> doubling them), because they would clash with @variable and :type syntax.</span></span>
$ <span><span><span>#</span> However, values following a = can contain anything, so long as they follow a</span></span>
$ <span><span><span>#</span> key or type section.</span></span>
$ <span>jb-array :=<span><span>'</span>@foo:bar=baz<span>'</span></span> :=<span><span>'</span>{"not":"parsed"}<span>'</span></span> =@@es::cap==ed</span>
<span>["@foo:bar=baz","{\"not\":\"parsed\"}","@es:cap=ed"]</span>

$ <span><span><span>#</span> Values from variables have no restrictions. Arrays use the same argument</span></span>
$ <span><span><span>#</span> syntax as objects, so values in the key or value position work the same.</span></span>
$ <span>s1=<span><span>'</span>@foo:bar=baz<span>'</span></span> s2=<span><span>'</span>{"not":"parsed"}<span>'</span></span> jb-array @s1: :@s2</span>
<span>["@foo:bar=baz","{\"not\":\"parsed\"}"]</span>

$ <span><span><span>#</span> It's possible to set a key as well as value for array entries, but the key</span></span>
$ <span><span><span>#</span> is ignored.</span></span>
$ <span>a=A b=B jb-array @a@a @b=B c=C</span>
<span>["A","B","C"]</span></pre></div>
<p dir="auto"><code>jb-array</code> is best for creating tuple-like arrays with a fixed number of entries
with a mix of types. Use
<a href="#value-arrays-uniform-types-variable-length">value arrays</a> to create
variable-length arrays containing the same type.</p>
<p dir="auto"><code>json.array</code> is the Bash API equivalent of <code>jb-array</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Argument types</h3><a id="user-content-argument-types" aria-label="Permalink: Argument types" href="#argument-types"></a></p>
<p dir="auto">Values are strings unless explicitly typed.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # These arguments are strings because they don't use a type
$ jb data=42 surname=null favourite_word=true
{&quot;data&quot;:&quot;42&quot;,&quot;surname&quot;:&quot;null&quot;,&quot;favourite_word&quot;:&quot;true&quot;}

$ # Non-string values need explicit types
$ jb size:number=42
{&quot;size&quot;:42}

$ # true/false/null have types which don't require redundant values
$ jb active:true enabled:false data:null
{&quot;active&quot;:true,&quot;enabled&quot;:false,&quot;data&quot;:null}

$ # Regardless, they can be given values if desired
$ jb active:true=true enabled:false=false data:null=null
{&quot;active&quot;:true,&quot;enabled&quot;:false,&quot;data&quot;:null}

$ # The bool type allows either true or false values.
$ active=true jb @active:bool enabled:bool=false
{&quot;active&quot;:true,&quot;enabled&quot;:false}

$ # The auto type outputs true/false/null and number values.
$ jb a:auto=42 b:auto=Hi c:auto=true d:auto=false e:auto=null f:auto=[] g:auto={}
{&quot;a&quot;:42,&quot;b&quot;:&quot;Hi&quot;,&quot;c&quot;:true,&quot;d&quot;:false,&quot;e&quot;:null,&quot;f&quot;:&quot;[]&quot;,&quot;g&quot;:&quot;{}&quot;}

$ # auto can be used selectively like other types
$ data=42 jb a=42 b:auto=42 c:auto@data
{&quot;a&quot;:&quot;42&quot;,&quot;b&quot;:42,&quot;c&quot;:42}

$ # In the Bash API (but not yet the jb CLI), the default type can be changed
$ # using the json_defaults option. First you create a named defaults set:
$ source json.bash
$ json.define_defaults num :number

$ # Then use the name with json_defaults when calling json to use your defaults
$ json_defaults=num json data=42
{&quot;data&quot;:42}

$ # In which case strings need to be explicitly typed
$ json_defaults=num json data=42 msg=Hi
json.encode_number(): not all inputs are numbers: 'Hi'
json(): Could not encode the value of argument 'msg=Hi' as a 'number' value. Read from inline value.
�␘

$ json_defaults=num json data=42 msg:string=Hi
{&quot;data&quot;:42,&quot;msg&quot;:&quot;Hi&quot;}"><pre>$ <span><span><span>#</span> These arguments are strings because they don't use a type</span></span>
$ <span>jb data=42 surname=null favourite_word=true</span>
<span>{"data":"42","surname":"null","favourite_word":"true"}</span>

$ <span><span><span>#</span> Non-string values need explicit types</span></span>
$ <span>jb size:number=42</span>
<span>{"size":42}</span>

$ <span><span><span>#</span> true/false/null have types which don't require redundant values</span></span>
$ <span>jb active:true enabled:false data:null</span>
<span>{"active":true,"enabled":false,"data":null}</span>

$ <span><span><span>#</span> Regardless, they can be given values if desired</span></span>
$ <span>jb active:true=true enabled:false=false data:null=null</span>
<span>{"active":true,"enabled":false,"data":null}</span>

$ <span><span><span>#</span> The bool type allows either true or false values.</span></span>
$ <span>active=true jb @active:bool enabled:bool=false</span>
<span>{"active":true,"enabled":false}</span>

$ <span><span><span>#</span> The auto type outputs true/false/null and number values.</span></span>
$ <span>jb a:auto=42 b:auto=Hi c:auto=true d:auto=false e:auto=null f:auto=[] g:auto={}</span>
<span>{"a":42,"b":"Hi","c":true,"d":false,"e":null,"f":"[]","g":"{}"}</span>

$ <span><span><span>#</span> auto can be used selectively like other types</span></span>
$ <span>data=42 jb a=42 b:auto=42 c:auto@data</span>
<span>{"a":"42","b":42,"c":42}</span>

$ <span><span><span>#</span> In the Bash API (but not yet the jb CLI), the default type can be changed</span></span>
$ <span><span><span>#</span> using the json_defaults option. First you create a named defaults set:</span></span>
$ <span><span>source</span> json.bash</span>
$ <span>json.define_defaults num :number</span>

$ <span><span><span>#</span> Then use the name with json_defaults when calling json to use your defaults</span></span>
$ <span>json_defaults=num json data=42</span>
<span>{"data":42}</span>

$ <span><span><span>#</span> In which case strings need to be explicitly typed</span></span>
$ <span>json_defaults=num json data=42 msg=Hi</span>
<span>json.encode_number(): not all inputs are numbers: 'Hi'</span>
<span>json(): Could not encode the value of argument 'msg=Hi' as a 'number' value. Read from inline value.</span>
<span>�␘</span>

$ <span>json_defaults=num json data=42 msg:string=Hi</span>
<span>{"data":42,"msg":"Hi"}</span></pre></div>
<details>
  <summary>Why does <code>json.bash</code> require explicit types?</summary>
  <p dir="auto"><h4 tabindex="-1" dir="auto">Why does <code>json.bash</code> require explicit types?</h4><a id="user-content-why-does-jsonbash-require-explicit-types" aria-label="Permalink: Why does json.bash require explicit types?" href="#why-does-jsonbash-require-explicit-types"></a></p>
  <p dir="auto">Type coercion can look good in demos, but my opinion is that in practice,
  fields are more commonly of a specific type than a union of several options,
  so coercing types by default makes it harder to achieve correct behaviour in
  the common case. The <a href="https://hitchdev.com/strictyaml/why/implicit-typing-removed/" rel="nofollow">Norway Problem</a>
  is worth reading about if you're not familiar with it.</p>
  <p dir="auto">Regardless, you can make the <code>:auto</code> type the default by using <code>json_defaults</code> when calling <code>json</code> from the
  Bash API (as demonstrated above). (This isn't yet exposed through the
  <code>jb</code> CLI.)</p>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Array values (uniform types, variable length)</h3><a id="user-content-array-values-uniform-types-variable-length" aria-label="Permalink: Array values (uniform types, variable length)" href="#array-values-uniform-types-variable-length"></a></p>
<p dir="auto">Arrays of values can be created using <code>[]</code> after the <code>:</code> type marker.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb sizes:number[]=42
{&quot;sizes&quot;:[42]}

$ # The value is split on the character inside the []
$ jb names:[,]=&quot;Alice,Bob,Dr Chris&quot;
{&quot;names&quot;:[&quot;Alice&quot;,&quot;Bob&quot;,&quot;Dr Chris&quot;]}

$ # Using a newline \n as the split character makes each line an array
$ # element. This integrates with line-oriented command-line tools:
$ jb sizes:number[$'\n']=&quot;$(seq 3)&quot;
{&quot;sizes&quot;:[1,2,3]}"><pre>$ <span>jb sizes:number[]=42</span>
<span>{"sizes":[42]}</span>

$ <span><span><span>#</span> The value is split on the character inside the []</span></span>
$ <span>jb names:[,]=<span><span>"</span>Alice,Bob,Dr Chris<span>"</span></span></span>
<span>{"names":["Alice","Bob","Dr Chris"]}</span>

$ <span><span><span>#</span> Using a newline \n as the split character makes each line an array</span></span>
$ <span><span><span>#</span> element. This integrates with line-oriented command-line tools:</span></span>
$ <span>jb sizes:number[<span><span>$'</span><span>\n</span><span>'</span></span>]=<span><span>"</span><span><span>$(</span>seq 3<span>)</span></span><span>"</span></span></span>
<span>{"sizes":[1,2,3]}</span></pre></div>
<p dir="auto"><a href="#file-references">File references</a> with process substitution are the a better
way to get the output of other programs into JSON though.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # The default type is used if the type name is left out
$ jb sizes:[,]=&quot;1,2,3&quot;
{&quot;sizes&quot;:[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;]}

$ # [:] is shorthand for /collection=array,split=:/
$ jb names:/collection=array,split=:/=&quot;Alice:Bob:Dr Chris&quot;
{&quot;names&quot;:[&quot;Alice&quot;,&quot;Bob&quot;,&quot;Dr Chris&quot;]}

$ # To split on null bytes, use split= (empty string). When used with inline and
$ # bash values this effectively inhibits splitting, because bash variables
$ # can't contain null bytes.
$ printf 'AB\nCD\x00EF\nGH\n\x00' | jb nullterm:[]/split=/@/dev/stdin
{&quot;nullterm&quot;:[&quot;AB\nCD&quot;,&quot;EF\nGH\n&quot;]}

$ # When using the Bash API, @var references can be bash arrays
$ source json.bash
$ names=(&quot;Bob Bobson&quot; &quot;Alice Alison&quot;) sizes=(42 55)
$ json @names:string[] @sizes:number[]
{&quot;names&quot;:[&quot;Bob Bobson&quot;,&quot;Alice Alison&quot;],&quot;sizes&quot;:[42,55]}

$ # json.array values can be arrays too
$ json.array @names:string[] @sizes:number[] :null[] :bool[]=true
[[&quot;Bob Bobson&quot;,&quot;Alice Alison&quot;],[42,55],[null],[true]]

$ # And jb-array values can be arrays as well
$ jb-array :[,]=&quot;Bob Bobson,Alice Alison&quot; :number[,]=42,55 :null[] :bool[]=true
[[&quot;Bob Bobson&quot;,&quot;Alice Alison&quot;],[42,55],[null],[true]]"><pre>$ <span><span><span>#</span> The default type is used if the type name is left out</span></span>
$ <span>jb sizes:[,]=<span><span>"</span>1,2,3<span>"</span></span></span>
<span>{"sizes":["1","2","3"]}</span>

$ <span><span><span>#</span> [:] is shorthand for /collection=array,split=:/</span></span>
$ <span>jb names:/collection=array,split=:/=<span><span>"</span>Alice:Bob:Dr Chris<span>"</span></span></span>
<span>{"names":["Alice","Bob","Dr Chris"]}</span>

$ <span><span><span>#</span> To split on null bytes, use split= (empty string). When used with inline and</span></span>
$ <span><span><span>#</span> bash values this effectively inhibits splitting, because bash variables</span></span>
$ <span><span><span>#</span> can't contain null bytes.</span></span>
$ <span><span>printf</span> <span><span>'</span>AB\nCD\x00EF\nGH\n\x00<span>'</span></span> <span>|</span> jb nullterm:[]/split=/@/dev/stdin</span>
<span>{"nullterm":["AB\nCD","EF\nGH\n"]}</span>

$ <span><span><span>#</span> When using the Bash API, @var references can be bash arrays</span></span>
$ <span><span>source</span> json.bash</span>
$ <span>names=(<span><span>"</span>Bob Bobson<span>"</span></span> <span><span>"</span>Alice Alison<span>"</span></span>) sizes=(42 55)</span>
$ <span>json @names:string[] @sizes:number[]</span>
<span>{"names":["Bob Bobson","Alice Alison"],"sizes":[42,55]}</span>

$ <span><span><span>#</span> json.array values can be arrays too</span></span>
$ <span>json.array @names:string[] @sizes:number[] :null[] :bool[]=true</span>
<span>[["Bob Bobson","Alice Alison"],[42,55],[null],[true]]</span>

$ <span><span><span>#</span> And jb-array values can be arrays as well</span></span>
$ <span>jb-array :[,]=<span><span>"</span>Bob Bobson,Alice Alison<span>"</span></span> :number[,]=42,55 :null[] :bool[]=true</span>
<span>[["Bob Bobson","Alice Alison"],[42,55],[null],[true]]</span></pre></div>
<p dir="auto">Arrays can be created from existing JSON arrays using the <code>[:json]</code> array
format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb tags:[:json]=&quot;$(jb-array foo bar baz)&quot;
{&quot;tags&quot;:[&quot;foo&quot;,&quot;bar&quot;,&quot;baz&quot;]}

$ # The type of values in the argument's array must match the argument type
$ jb measures:number[:json]='[1,2,3,4]'
{&quot;measures&quot;:[1,2,3,4]}

$ # Otherwise an error occurs
$ jb measures:number[:json]='[1,2,&quot;oops&quot;]'
json.encode_array_entries_from_json(): provided entries are not all valid JSON arrays with 'number' values — '[1,2,&quot;oops&quot;]'
json(): Could not encode the value of argument 'measures:number[:json]=[1,2,&quot;oops&quot;]' as an array with 'number' values. Read from inline value, without splitting (one chunk), interpreted chunks with 'json' format.
�␘"><pre>$ <span>jb tags:[:json]=<span><span>"</span><span><span>$(</span>jb-array foo bar baz<span>)</span></span><span>"</span></span></span>
<span>{"tags":["foo","bar","baz"]}</span>

$ <span><span><span>#</span> The type of values in the argument's array must match the argument type</span></span>
$ <span>jb measures:number[:json]=<span><span>'</span>[1,2,3,4]<span>'</span></span></span>
<span>{"measures":[1,2,3,4]}</span>

$ <span><span><span>#</span> Otherwise an error occurs</span></span>
$ <span>jb measures:number[:json]=<span><span>'</span>[1,2,"oops"]<span>'</span></span></span>
<span>json.encode_array_entries_from_json(): provided entries are not all valid JSON arrays with 'number' values — '[1,2,"oops"]'</span>
<span>json(): Could not encode the value of argument 'measures:number[:json]=[1,2,"oops"]' as an array with 'number' values. Read from inline value, without splitting (one chunk), interpreted chunks with 'json' format.</span>
<span>�␘</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Object values (uniform types, variable length)</h3><a id="user-content-object-values-uniform-types-variable-length" aria-label="Permalink: Object values (uniform types, variable length)" href="#object-values-uniform-types-variable-length"></a></p>
<p dir="auto">Variable-length JSON objects can be created using <code>{}</code> after the <code>:</code> type
marker. Object values use the same <code>key=value</code> syntax used in arguments'
attributes section (<code>:/a=b,c=d/</code>).</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # The default type is used if the type name is left out
$ jb sizes:{}=small=s,medium=m,large=l
{&quot;sizes&quot;:{&quot;small&quot;:&quot;s&quot;,&quot;medium&quot;:&quot;m&quot;,&quot;large&quot;:&quot;l&quot;}}

$ jb measurements:number{}=small=5,medium=10,large=15
{&quot;measurements&quot;:{&quot;small&quot;:5,&quot;medium&quot;:10,&quot;large&quot;:15}}"><pre>$ <span><span><span>#</span> The default type is used if the type name is left out</span></span>
$ <span>jb sizes:{}=small=s,medium=m,large=l</span>
<span>{"sizes":{"small":"s","medium":"m","large":"l"}}</span>

$ <span>jb measurements:number{}=small=5,medium=10,large=15</span>
<span>{"measurements":{"small":5,"medium":10,"large":15}}</span></pre></div>
<p dir="auto">Like array values (<code>[]</code>), object values consume multiple lines of input when
reading files</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # env is a command-line tool that prints environment variables
$ env -i small=s medium=m large=l
small=s
medium=m
large=l

$ # We can encode variables from env as a JSON object
$ env -i small=s medium=m large=l | jb sizes:{}@/dev/stdin
{&quot;sizes&quot;:{&quot;small&quot;:&quot;s&quot;,&quot;medium&quot;:&quot;m&quot;,&quot;large&quot;:&quot;l&quot;}}"><pre>$ <span><span><span>#</span> env is a command-line tool that prints environment variables</span></span>
$ <span>env -i small=s medium=m large=l</span>
<span>small=s</span>
<span>medium=m</span>
<span>large=l</span>

$ <span><span><span>#</span> We can encode variables from env as a JSON object</span></span>
$ <span>env -i small=s medium=m large=l <span>|</span> jb sizes:{}@/dev/stdin</span>
<span>{"sizes":{"small":"s","medium":"m","large":"l"}}</span></pre></div>
<p dir="auto">As with array values, JSON data can be used as values:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb user=h4l repo=json.bash >> info
$ jb @./info:{:json}
{&quot;info&quot;:{&quot;user&quot;:&quot;h4l&quot;,&quot;repo&quot;:&quot;json.bash&quot;}}

$ jb file_types:string[,]=bash,md,hcl year_created:number=2023 >> info
$ # The values of the JSON objects are validated to match the argument's type,
$ # so the :json type must be used to consume arbitrary JSON
$ jb @./info:json{:json}
{&quot;info&quot;:{&quot;user&quot;:&quot;h4l&quot;,&quot;repo&quot;:&quot;json.bash&quot;,&quot;file_types&quot;:[&quot;bash&quot;,&quot;md&quot;,&quot;hcl&quot;],&quot;year_created&quot;:2023}}"><pre>$ <span>jb user=h4l repo=json.bash <span>&gt;&gt;</span> info</span>
$ <span>jb @./info:{:json}</span>
<span>{"info":{"user":"h4l","repo":"json.bash"}}</span>

$ <span>jb file_types:string[,]=bash,md,hcl year_created:number=2023 <span>&gt;&gt;</span> info</span>
$ <span><span><span>#</span> The values of the JSON objects are validated to match the argument's type,</span></span>
$ <span><span><span>#</span> so the :json type must be used to consume arbitrary JSON</span></span>
$ <span>jb @./info:json{:json}</span>
<span>{"info":{"user":"h4l","repo":"json.bash","file_types":["bash","md","hcl"],"year_created":2023}}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>...</code> arguments (merge entries into the host object/array)</h3><a id="user-content--arguments-merge-entries-into-the-host-objectarray" aria-label="Permalink: ... arguments (merge entries into the host object/array)" href="#-arguments-merge-entries-into-the-host-objectarray"></a></p>
<p dir="auto">An argument prefixed with <code>...</code> (commonly called splat, spread or unpacking in
programming languages) results in the argument's entries being merged directly
into the object or array being created.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb id=ab12 ...:=user=h4l,repo=json.bash ...:number=year=2023,min_radish_count=3
{&quot;id&quot;:&quot;ab12&quot;,&quot;user&quot;:&quot;h4l&quot;,&quot;repo&quot;:&quot;json.bash&quot;,&quot;year&quot;:2023,&quot;min_radish_count&quot;:3}

$ seq 5 8 | jb-array :number=0 ...:number[,]=1,2,3,4 ...:number@/dev/stdin
[0,1,2,3,4,5,6,7,8]"><pre>$ <span>jb id=ab12 ...:=user=h4l,repo=json.bash ...:number=year=2023,min_radish_count=3</span>
<span>{"id":"ab12","user":"h4l","repo":"json.bash","year":2023,"min_radish_count":3}</span>

$ <span>seq 5 8 <span>|</span> jb-array :number=0 ...:number[,]=1,2,3,4 ...:number@/dev/stdin</span>
<span>[0,1,2,3,4,5,6,7,8]</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Missing / empty values</h3><a id="user-content-missing--empty-values" aria-label="Permalink: Missing / empty values" href="#missing--empty-values"></a></p>
<p dir="auto">References to undefined variables, missing files or unreadable files are missing
values. Empty array variables, empty string variables, empty files and empty
argument values are empty values.</p>
<p dir="auto">Missing or empty keys or values are errors by default, apart from empty argument
values, like <code>foo=</code>.</p>
<p dir="auto">The flags <code>+</code> <code>~</code> <code>?</code> and <code>??</code> alter how missing/empty values behave.</p>
<table>
<thead>
<tr>
<th>Flag</th>
<th>Name</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>+</code></td>
<td>strict</td>
<td>All missing/empty values are errors.</td>
</tr>
<tr>
<td><code>~</code></td>
<td>optional</td>
<td>Missing files/variables are treated as empty.</td>
</tr>
<tr>
<td><code>?</code></td>
<td>substitute empty</td>
<td>Empty values are substituted with a default.</td>
</tr>
<tr>
<td><code>??</code></td>
<td>omit empty</td>
<td>Entries with an empty key or value are omitted.</td>
</tr>
</tbody>
</table>
<div dir="auto" data-snippet-clipboard-copy-content="$ # empty argument values are substituted by default
$ jb str= num:number= bool:bool= arr:[]= obj:{}=
{&quot;str&quot;:&quot;&quot;,&quot;num&quot;:0,&quot;bool&quot;:false,&quot;arr&quot;:[],&quot;obj&quot;:{}}

$ # Using ? substitutes the empty var for the default string, which is &quot;&quot;
$ empty= jb @empty?
{&quot;empty&quot;:&quot;&quot;}

$ # The empty attribute controls the default value. It's interpreted as JSON.
$ CI=true jb ci:bool/empty=false/?@CI
{&quot;ci&quot;:true}

$ CI= jb ci:true/empty=false/?@CI
{&quot;ci&quot;:false}

$ # empty_key controls the default value for empty keys
$ PROP= jb ?@PROP:true/empty_key='&quot;🤷&quot;'/
{&quot;🤷&quot;:true}

$ # The type= can be used to encode a raw value as JSON for empty attributes
$ PROP=👌 jb ?@PROP:true/empty_key=string=🤷/
{&quot;👌&quot;:true}

$ # ?? causes an empty value to be omitted entirely
$ CI= jb ci:bool??@CI
{}

$ # ~ causes a missing value to be empty. A ? is needed to prevent the empty
$ # value being an error.
$ jb github_actions:bool~?@GITHUB_ACTION
{&quot;github_actions&quot;:false}

$ # Empty variables are errors if ? isn't used.
$ empty= jb @empty
json.apply_empty_action(): The value of argument '@empty' must be non-empty but is empty.
json(): Could not encode the value of argument '@empty' as a 'string' value. Read from variable $empty. (Use the '?' flag after the :type to substitute the entry's empty value with a default, or the '??' flag to omit the entry when it has an empty value.)
�␘

$ # (Only the json Bash function, not the jb executable can access bash array variables.)
$ . json.bash
$ empty_array=()

$ # Using ? substitutes the empty array for the default, which is []
$ json @empty_array:[]?
{&quot;empty_array&quot;:[]}

$ # Empty arrays are errors without ?.
$ json @empty_array:[]
json.apply_empty_action(): The value of argument '@empty_array:[]' must be non-empty but is empty.
json(): Could not encode the value of argument '@empty_array:[]' as an array with 'string' values. Read from array-variable $empty_array. (Use the '?' flag after the :type to substitute the entry's empty value with a default, or the '??' flag to omit the entry when it has an empty value.)
�␘

$ # Missing / empty files work like variables
$ jb @./config:/empty=null/~?
{&quot;config&quot;:null}"><pre>$ <span><span><span>#</span> empty argument values are substituted by default</span></span>
$ <span>jb str= num:number= bool:bool= arr:[]= obj:{}=</span>
<span>{"str":"","num":0,"bool":false,"arr":[],"obj":{}}</span>

$ <span><span><span>#</span> Using ? substitutes the empty var for the default string, which is ""</span></span>
$ <span>empty= jb @empty<span>?</span></span>
<span>{"empty":""}</span>

$ <span><span><span>#</span> The empty attribute controls the default value. It's interpreted as JSON.</span></span>
$ <span>CI=true jb ci:bool/empty=false/<span>?</span>@CI</span>
<span>{"ci":true}</span>

$ <span>CI= jb ci:true/empty=false/<span>?</span>@CI</span>
<span>{"ci":false}</span>

$ <span><span><span>#</span> empty_key controls the default value for empty keys</span></span>
$ <span>PROP= jb <span>?</span>@PROP:true/empty_key=<span><span>'</span>"🤷"<span>'</span></span>/</span>
<span>{"🤷":true}</span>

$ <span><span><span>#</span> The type= can be used to encode a raw value as JSON for empty attributes</span></span>
$ <span>PROP=👌 jb <span>?</span>@PROP:true/empty_key=string=🤷/</span>
<span>{"👌":true}</span>

$ <span><span><span>#</span> ?? causes an empty value to be omitted entirely</span></span>
$ <span>CI= jb ci:bool<span>??</span>@CI</span>
<span>{}</span>

$ <span><span><span>#</span> ~ causes a missing value to be empty. A ? is needed to prevent the empty</span></span>
$ <span><span><span>#</span> value being an error.</span></span>
$ <span>jb github_actions:bool~<span>?</span>@GITHUB_ACTION</span>
<span>{"github_actions":false}</span>

$ <span><span><span>#</span> Empty variables are errors if ? isn't used.</span></span>
$ <span>empty= jb @empty</span>
<span>json.apply_empty_action(): The value of argument '@empty' must be non-empty but is empty.</span>
<span>json(): Could not encode the value of argument '@empty' as a 'string' value. Read from variable $empty. (Use the '?' flag after the :type to substitute the entry's empty value with a default, or the '??' flag to omit the entry when it has an empty value.)</span>
<span>�␘</span>

$ <span><span><span>#</span> (Only the json Bash function, not the jb executable can access bash array variables.)</span></span>
$ <span><span>.</span> json.bash</span>
$ <span>empty_array=()</span>

$ <span><span><span>#</span> Using ? substitutes the empty array for the default, which is []</span></span>
$ <span>json @empty_array:[]<span>?</span></span>
<span>{"empty_array":[]}</span>

$ <span><span><span>#</span> Empty arrays are errors without ?.</span></span>
$ <span>json @empty_array:[]</span>
<span>json.apply_empty_action(): The value of argument '@empty_array:[]' must be non-empty but is empty.</span>
<span>json(): Could not encode the value of argument '@empty_array:[]' as an array with 'string' values. Read from array-variable $empty_array. (Use the '?' flag after the :type to substitute the entry's empty value with a default, or the '??' flag to omit the entry when it has an empty value.)</span>
<span>�␘</span>

$ <span><span><span>#</span> Missing / empty files work like variables</span></span>
$ <span>jb @./config:/empty=null/~<span>?</span></span>
<span>{"config":null}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Nested JSON with <code>:json</code> and <code>:raw</code> types</h3><a id="user-content-nested-json-with-json-and-raw-types" aria-label="Permalink: Nested JSON with :json and :raw types" href="#nested-json-with-json-and-raw-types"></a></p>
<p dir="auto">Nested objects and arrays are created using the <code>:json</code> or <code>:raw</code> types. The
<code>:json</code> type validates the provided value(s) and fails if they're not actually
JSON, whereas the <code>:raw</code> type allow <em>any</em> value to be inserted (even invalid
JSON).</p>
<p dir="auto">The reason for both is that <code>:json</code> depends on grep (with PCRE) being present,
so <code>:raw</code> can be used in situations where only bash is available, and validation
isn't necessary (e.g. when passing the output of one <code>json.bash</code> call into
another). <code>:raw</code> also supports <a href="#streaming-output">streaming output</a>, which
<code>:json</code> does not.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # Like other types, :json and :raw values can be directly embedded in arguments
$ jb user:json='{&quot;name&quot;:&quot;Bob Bobson&quot;}'
{&quot;user&quot;:{&quot;name&quot;:&quot;Bob Bobson&quot;}}

$ # Or come from variable references
$ user='{&quot;name&quot;:&quot;Bob Bobson&quot;}' jb @user:json
{&quot;user&quot;:{&quot;name&quot;:&quot;Bob Bobson&quot;}}

$ # Or files
$ jb name=&quot;Bob Bobson&quot; > /tmp/user; jb @/tmp/user:json
{&quot;user&quot;:{&quot;name&quot;:&quot;Bob Bobson&quot;}}

$ # Arrays of JSON work the same way as other types.
$ jb users:json[$'\n']=&quot;$(jb name=Bob; jb name=Alice)&quot;
{&quot;users&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}

$ # :json and :raw values are not formatted — whitespace in them is preserved
$ jb user:json=$'{\n  &quot;name&quot;: &quot;Bob Bobson&quot;\n}'
{&quot;user&quot;:{
  &quot;name&quot;: &quot;Bob Bobson&quot;
}}

$ # :json detects invalid JSON and fails with an error
$ jb oops:json='{&quot;truncated&quot;:'
json.encode_json(): not all inputs are valid JSON: '{&quot;truncated&quot;:'
json(): Could not encode the value of argument 'oops:json={&quot;truncated&quot;:' as a 'json' value. Read from inline value.
�␘

$ # However :raw performs no validation, so it must only be used with great care
$ # 🚨 This emits invalid JSON without failing! 🚨
$ jb broken:raw='{&quot;truncated&quot;:'
{&quot;broken&quot;:{&quot;truncated&quot;:}"><pre>$ <span><span><span>#</span> Like other types, :json and :raw values can be directly embedded in arguments</span></span>
$ <span>jb user:json=<span><span>'</span>{"name":"Bob Bobson"}<span>'</span></span></span>
<span>{"user":{"name":"Bob Bobson"}}</span>

$ <span><span><span>#</span> Or come from variable references</span></span>
$ <span>user=<span><span>'</span>{"name":"Bob Bobson"}<span>'</span></span> jb @user:json</span>
<span>{"user":{"name":"Bob Bobson"}}</span>

$ <span><span><span>#</span> Or files</span></span>
$ <span>jb name=<span><span>"</span>Bob Bobson<span>"</span></span> <span>&gt;</span> /tmp/user<span>;</span> jb @/tmp/user:json</span>
<span>{"user":{"name":"Bob Bobson"}}</span>

$ <span><span><span>#</span> Arrays of JSON work the same way as other types.</span></span>
$ <span>jb users:json[<span><span>$'</span><span>\n</span><span>'</span></span>]=<span><span>"</span><span><span>$(</span>jb name=Bob<span>;</span> jb name=Alice<span>)</span></span><span>"</span></span></span>
<span>{"users":[{"name":"Bob"},{"name":"Alice"}]}</span>

$ <span><span><span>#</span> :json and :raw values are not formatted — whitespace in them is preserved</span></span>
$ <span>jb user:json=<span><span>$'</span>{<span>\n</span>  "name": "Bob Bobson"<span>\n</span>}<span>'</span></span></span>
<span>{"user":{</span>
<span>  "name": "Bob Bobson"</span>
<span>}}</span>

$ <span><span><span>#</span> :json detects invalid JSON and fails with an error</span></span>
$ <span>jb oops:json=<span><span>'</span>{"truncated":<span>'</span></span></span>
<span>json.encode_json(): not all inputs are valid JSON: '{"truncated":'</span>
<span>json(): Could not encode the value of argument 'oops:json={"truncated":' as a 'json' value. Read from inline value.</span>
<span>�␘</span>

$ <span><span><span>#</span> However :raw performs no validation, so it must only be used with great care</span></span>
$ <span><span><span>#</span> 🚨 This emits invalid JSON without failing! 🚨</span></span>
$ <span>jb broken:raw=<span><span>'</span>{"truncated":<span>'</span></span></span>
<span>{"broken":{"truncated":}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">File references</h3><a id="user-content-file-references" aria-label="Permalink: File references" href="#file-references"></a></p>
<p dir="auto">The <code>@ref</code> syntax can be used to reference the content of files. If an <code> @ref</code>
starts with <code>/</code> or <code>./</code> it's taken to be a file (rather than a shell variable).</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ printf 'orange #3\nblue #5\n' > colours

$ jb my_colours@./colours
{&quot;my_colours&quot;:&quot;orange #3\nblue #5\n&quot;}

$ # The final path segment is used as the key if a key isn't set.
$ jb @./colours
{&quot;colours&quot;:&quot;orange #3\nblue #5\n&quot;}

$ # Array values split on newlines
$ jb @./colours:[]
{&quot;colours&quot;:[&quot;orange #3&quot;,&quot;blue #5&quot;]}

$ printf 'apple:pear:grape' > fruit

$ # The file can be split on a different character by naming it in the []
$ jb @./fruit:[:]
{&quot;fruit&quot;:[&quot;apple&quot;,&quot;pear&quot;,&quot;grape&quot;]}

$ # Which is shorthand for
$ jb @./fruit:/collection=array,split=:/
{&quot;fruit&quot;:[&quot;apple&quot;,&quot;pear&quot;,&quot;grape&quot;]}

$ # Split on null by setting split to the empty string
$ printf 'foo\nbar\n\x00bar baz\n\x00' > nullterminated
$ jb @./nullterminated:[]/split=/
{&quot;nullterminated&quot;:[&quot;foo\nbar\n&quot;,&quot;bar baz\n&quot;]}

$ # Read from stdin using the special /dev/stdin file
$ seq 3 | jb counts:number[]@/dev/stdin
{&quot;counts&quot;:[1,2,3]}"><pre>$ <span><span>printf</span> <span><span>'</span>orange #3\nblue #5\n<span>'</span></span> <span>&gt;</span> colours</span>

$ <span>jb my_colours@./colours</span>
<span>{"my_colours":"orange #3\nblue #5\n"}</span>

$ <span><span><span>#</span> The final path segment is used as the key if a key isn't set.</span></span>
$ <span>jb @./colours</span>
<span>{"colours":"orange #3\nblue #5\n"}</span>

$ <span><span><span>#</span> Array values split on newlines</span></span>
$ <span>jb @./colours:[]</span>
<span>{"colours":["orange #3","blue #5"]}</span>

$ <span><span>printf</span> <span><span>'</span>apple:pear:grape<span>'</span></span> <span>&gt;</span> fruit</span>

$ <span><span><span>#</span> The file can be split on a different character by naming it in the []</span></span>
$ <span>jb @./fruit:[:]</span>
<span>{"fruit":["apple","pear","grape"]}</span>

$ <span><span><span>#</span> Which is shorthand for</span></span>
$ <span>jb @./fruit:/collection=array,split=:/</span>
<span>{"fruit":["apple","pear","grape"]}</span>

$ <span><span><span>#</span> Split on null by setting split to the empty string</span></span>
$ <span><span>printf</span> <span><span>'</span>foo\nbar\n\x00bar baz\n\x00<span>'</span></span> <span>&gt;</span> nullterminated</span>
$ <span>jb @./nullterminated:[]/split=/</span>
<span>{"nullterminated":["foo\nbar\n","bar baz\n"]}</span>

$ <span><span><span>#</span> Read from stdin using the special /dev/stdin file</span></span>
$ <span>seq 3 <span>|</span> jb counts:number[]@/dev/stdin</span>
<span>{"counts":[1,2,3]}</span></pre></div>
<p dir="auto">File references become especially powerful when combined with process
substitution — a shell feature that provides a dynamic, temporary file
containing the output of another program.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # Use process substitution to nest jb calls and pull multiple shell pipelines
$ # into one JSON output.
$ jb counts:number[]@<(seq 3) \
>    people:json[]@<(jb name=Bob; jb name=Alice)
{&quot;counts&quot;:[1,2,3],&quot;people&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}"><pre>$ <span><span><span>#</span> Use process substitution to nest jb calls and pull multiple shell pipelines</span></span>
$ <span><span><span>#</span> into one JSON output.</span></span>
$ <span>jb counts:number[]@<span><span>&lt;(</span>seq 3<span>)</span></span> \</span>
&gt;    <span>people:json[]@<span><span>&lt;(</span>jb name=Bob<span>;</span> jb name=Alice<span>)</span></span></span>
<span>{"counts":[1,2,3],"people":[{"name":"Bob"},{"name":"Alice"}]}</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Aside: Process substitution 101</h4><a id="user-content-aside-process-substitution-101" aria-label="Permalink: Aside: Process substitution 101" href="#aside-process-substitution-101"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # What's going on when we use process substitution? The <(...) syntax.
$ jb msg@<(printf &quot;Hi!&quot;)
{&quot;msg&quot;:&quot;Hi!&quot;}

$ # The shell replaces <(...) with a file path. That file contains the output of
$ # the command inside the <(...) when read. (But the catch is, the file only
$ # exists while the command runs, and it's not a normal file, so the contents
$ # isn't stored on disk.)

$ # We can see this if we echo the the substitution:
$ echo This is the substitution result: <(printf &quot;Hi!&quot;)
This is the substitution result: /dev/fd/...

$ # If we cat the substitution instead of echoing it, we read the file contents:
$ cat <(printf &quot;Hi!&quot;)
Hi!

$ # So when we use this with jb, it's as if we ran:  jb msg@/dev/fd/...

$ # We can see this in action by enabling tracing in Bash:
$ set -o xtrace;  jb msg@<(printf &quot;Hi!&quot;);  set +o xtrace
+ jb msg@/dev/fd/...
++ printf 'Hi!'
{&quot;msg&quot;:&quot;Hi!&quot;}
+ set +o xtrace"><pre>$ <span><span><span>#</span> What's going on when we use process substitution? The &lt;(...) syntax.</span></span>
$ <span>jb msg@<span><span>&lt;(</span>printf <span><span>"</span>Hi!<span>"</span></span><span>)</span></span></span>
<span>{"msg":"Hi!"}</span>

$ <span><span><span>#</span> The shell replaces &lt;(...) with a file path. That file contains the output of</span></span>
$ <span><span><span>#</span> the command inside the &lt;(...) when read. (But the catch is, the file only</span></span>
$ <span><span><span>#</span> exists while the command runs, and it's not a normal file, so the contents</span></span>
$ <span><span><span>#</span> isn't stored on disk.)</span></span>

$ <span><span><span>#</span> We can see this if we echo the the substitution:</span></span>
$ <span><span>echo</span> This is the substitution result: <span><span>&lt;(</span>printf <span><span>"</span>Hi!<span>"</span></span><span>)</span></span></span>
<span>This is the substitution result: /dev/fd/...</span>

$ <span><span><span>#</span> If we cat the substitution instead of echoing it, we read the file contents:</span></span>
$ <span>cat <span><span>&lt;(</span>printf <span><span>"</span>Hi!<span>"</span></span><span>)</span></span></span>
<span>Hi!</span>

$ <span><span><span>#</span> So when we use this with jb, it's as if we ran:  jb msg@/dev/fd/...</span></span>

$ <span><span><span>#</span> We can see this in action by enabling tracing in Bash:</span></span>
$ <span><span>set</span> -o xtrace<span>;</span>  jb msg@<span><span>&lt;(</span>printf <span><span>"</span>Hi!<span>"</span></span><span>)</span></span><span>;</span>  <span>set</span> +o xtrace</span>
<span>+ jb msg@/dev/fd/...</span>
<span>++ printf 'Hi!'</span>
<span>{"msg":"Hi!"}</span>
<span>+ set +o xtrace</span></pre></div>
<p dir="auto">Because <code>&lt;(...)</code> becomes a path, you don't <em>have</em> to quote it, which makes
forming commands a bit easier than using <em>command substitution</em> to do the same
thing (<code>echo "$(printf like this)"</code>). And you only pass a short file path as an
argument, not a potentially huge string.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Back to file references</h4><a id="user-content-back-to-file-references" aria-label="Permalink: Back to file references" href="#back-to-file-references"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # Process substitution can nest multiple times
$ jb owners:json@<(
>   jb people:json[]@<(jb name=Bob; jb name=Alice)
> )
{&quot;owners&quot;:{&quot;people&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}}

$ # Files can be referenced indirectly using a shell variable.
$ # If @var is used and $var is not set, but $var_FILE is, the filename is read
$ # from $var_FILE and the content of the file is used.
$ printf 'secret123' > db_password
$ db_password_FILE=./db_password jb @db_password
{&quot;db_password&quot;:&quot;secret123&quot;}"><pre>$ <span><span><span>#</span> Process substitution can nest multiple times</span></span>
$ <span>jb owners:json@<span><span>&lt;(</span></span></span>
&gt;   <span>jb people:json[]@<span><span>&lt;(</span>jb name=Bob<span>;</span> jb name=Alice<span>)</span></span></span>
&gt; <span>)</span>
<span>{"owners":{"people":[{"name":"Bob"},{"name":"Alice"}]}}</span>

$ <span><span><span>#</span> Files can be referenced indirectly using a shell variable.</span></span>
$ <span><span><span>#</span> If @var is used and $var is not set, but $var_FILE is, the filename is read</span></span>
$ <span><span><span>#</span> from $var_FILE and the content of the file is used.</span></span>
$ <span><span>printf</span> <span><span>'</span>secret123<span>'</span></span> <span>&gt;</span> db_password</span>
$ <span>db_password_FILE=./db_password jb @db_password</span>
<span>{"db_password":"secret123"}</span></pre></div>
<p dir="auto">(This pattern is often used to securely pass secrets via environment variables,
<a href="#environment-variable-exposure">without directly exposing the secret's value itself in the environment</a>,
to avoid accidental exposure.)</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # Nesting lots of process substitution levels can become unwieldy, but we can
$ # flatten the nesting by holding the process substitution filenames in shell
$ #&nbsp;variables, using the _FILE var feature to reference them:
$ people_FILE=<(jb name=Bob; jb name=Alice) \
> owners_FILE=<(jb @people:json[]) \
> jb @owners:json
{&quot;owners&quot;:{&quot;people&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}}"><pre>$ <span><span><span>#</span> Nesting lots of process substitution levels can become unwieldy, but we can</span></span>
$ <span><span><span>#</span> flatten the nesting by holding the process substitution filenames in shell</span></span>
$ <span><span><span>#</span>&nbsp;variables, using the _FILE var feature to reference them:</span></span>
$ <span>people_FILE=<span><span>&lt;(</span>jb name=Bob<span>;</span> jb name=Alice<span>)</span></span> \</span>
&gt; <span>owners_FILE=<span><span>&lt;(</span>jb @people:json[]<span>)</span></span> \</span>
&gt; <span>jb @owners:json</span>
<span>{"owners":{"people":[{"name":"Bob"},{"name":"Alice"}]}}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Argument structure</h3><a id="user-content-argument-structure" aria-label="Permalink: Argument structure" href="#argument-structure"></a></p>
<p dir="auto">Arguments have 3 main parts: a key, type and value. The structure (omitting some
details for clarity) is:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/h4l/json.bash/blob/main/docs/syntax-diagrams/approximate-argument.svg"><img width="100%" src="https://github.com/h4l/json.bash/raw/main/docs/syntax-diagrams/approximate-argument.svg" alt="A railroad syntax diagram showing the key, type and value structure of an argument, in more detail than the minimal argument diagram, but still omitting some details." title="Approximate Argument Structure Diagram"></a></p>
<p dir="auto">The <a href="https://github.com/h4l/json.bash/blob/main/docs/syntax.md">Argument syntax</a> page has more detail.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Error handling</h3><a id="user-content-error-handling" aria-label="Permalink: Error handling" href="#error-handling"></a></p>
<p dir="auto"><code>json.bash</code> aims to fail quickly, cleanly and clearly when problems happen.</p>
<blockquote>
<p dir="auto">Please open an issue if you discover a case where an error goes unreported, is
not reported clearly, or you find it's not easy to prevent incorrect data
getting generated.</p>
</blockquote>
<p dir="auto">Invalid values in typed arguments will cause an error — values are not coerced
if a type is specified. <code>:bool</code> and <code>:null</code> are pedantic — values must be
exactly <code>true</code> / <code>false</code> / <code>null</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ active=tRuE jb @active:bool
json.encode_bool(): not all inputs are bools: 'tRuE'
json(): Could not encode the value of argument '@active:bool' as a 'bool' value. Read from variable $active.
�␘"><pre>$ <span>active=tRuE jb @active:bool</span>
<span>json.encode_bool(): not all inputs are bools: 'tRuE'</span>
<span>json(): Could not encode the value of argument '@active:bool' as a 'bool' value. Read from variable $active.</span>
<span>�␘</span></pre></div>
<p dir="auto">Errors are reported with specific exit statuses:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # Errors in user-provided data fail with status 1
$ jb data:json='invalid'; echo status=$?
json.encode_json(): not all inputs are valid JSON: 'invalid'
json(): Could not encode the value of argument 'data:json=invalid' as a 'json' value. Read from inline value.
�␘
status=1

$ # Errors in developer-provided arguments fail with status 1
$ jb bad_arg:cheese; echo status=$?
json.parse_argument(): type name must be one of auto, bool, false, json, null, number, raw, string or true, but was 'cheese'
json(): Could not parse argument 'bad_arg:cheese'. Argument is not structured correctly, see --help for examples.
�␘
status=2

$ # Arguments referencing variables that don't exist fail with status 3
$ jb @missing; echo status=$?
json(): Could not process argument '@missing'. Its value references unbound variable $missing. (Use the '~' flag after the :type to treat a missing value as empty.)
�␘
status=3

$ # Arguments referencing files that don't exist fail with status 4
$ jb @/does/not/exist; echo status=$?
/.../bin/jb: line ...: /does/not/exist: No such file or directory
json(): Could not open the file '/does/not/exist' referenced as the value of argument '@/does/not/exist'.
�␘
status=4"><pre>$ <span><span><span>#</span> Errors in user-provided data fail with status 1</span></span>
$ <span>jb data:json=<span><span>'</span>invalid<span>'</span></span><span>;</span> <span>echo</span> status=<span>$?</span></span>
<span>json.encode_json(): not all inputs are valid JSON: 'invalid'</span>
<span>json(): Could not encode the value of argument 'data:json=invalid' as a 'json' value. Read from inline value.</span>
<span>�␘</span>
<span>status=1</span>

$ <span><span><span>#</span> Errors in developer-provided arguments fail with status 1</span></span>
$ <span>jb bad_arg:cheese<span>;</span> <span>echo</span> status=<span>$?</span></span>
<span>json.parse_argument(): type name must be one of auto, bool, false, json, null, number, raw, string or true, but was 'cheese'</span>
<span>json(): Could not parse argument 'bad_arg:cheese'. Argument is not structured correctly, see --help for examples.</span>
<span>�␘</span>
<span>status=2</span>

$ <span><span><span>#</span> Arguments referencing variables that don't exist fail with status 3</span></span>
$ <span>jb @missing<span>;</span> <span>echo</span> status=<span>$?</span></span>
<span>json(): Could not process argument '@missing'. Its value references unbound variable $missing. (Use the '~' flag after the :type to treat a missing value as empty.)</span>
<span>�␘</span>
<span>status=3</span>

$ <span><span><span>#</span> Arguments referencing files that don't exist fail with status 4</span></span>
$ <span>jb @/does/not/exist<span>;</span> <span>echo</span> status=<span>$?</span></span>
<span>/.../bin/jb: line ...: /does/not/exist: No such file or directory</span>
<span>json(): Could not open the file '/does/not/exist' referenced as the value of argument '@/does/not/exist'.</span>
<span>�␘</span>
<span>status=4</span></pre></div>
<p dir="auto"><code>jb</code> can detect errors in upstream <code>jb</code> calls that are pulled into a downstream
<code>jb</code> process, such as when several <code>jb</code> calls are fed into each other using
<a href="#file-references">process substitution</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # The jb call 3 levels deep reading the missing file ./not-found fails
$ jb club:json@<(
>   jb name=&quot;jb Users&quot; members:json[]@<(
>     jb name=h4l; jb name@./not-found
>   )
> )
...: ./not-found: No such file or directory
json(): Could not open the file './not-found' referenced as the value of argument 'name@./not-found'.
json.encode_json(): not all inputs are valid JSON: '{&quot;name&quot;:&quot;h4l&quot;}' $'\030'
json(): Could not encode the value of argument 'members:json[]@/dev/fd/...' as an array with 'json' values. Read from file /dev/fd/..., split into chunks on $'\n', interpreted chunks with 'raw' format.
json.encode_json(): not all inputs are valid JSON: $'\030'
json(): Could not encode the value of argument 'club:json@/dev/fd/...' as a 'json' value. Read from file /dev/fd/..., up to the first 0x00 byte or end-of-file.
�␘"><pre>$ <span><span><span>#</span> The jb call 3 levels deep reading the missing file ./not-found fails</span></span>
$ <span>jb club:json@<span><span>&lt;(</span></span></span>
&gt;   <span>jb name=<span><span>"</span>jb Users<span>"</span></span> members:json[]@<span><span>&lt;(</span></span></span>
&gt;     <span>jb name=h4l<span>;</span> jb name@./not-found</span>
&gt;   <span>)</span>
&gt; <span>)</span>
<span>...: ./not-found: No such file or directory</span>
<span>json(): Could not open the file './not-found' referenced as the value of argument 'name@./not-found'.</span>
<span>json.encode_json(): not all inputs are valid JSON: '{"name":"h4l"}' $'\030'</span>
<span>json(): Could not encode the value of argument 'members:json[]@/dev/fd/...' as an array with 'json' values. Read from file /dev/fd/..., split into chunks on $'\n', interpreted chunks with 'raw' format.</span>
<span>json.encode_json(): not all inputs are valid JSON: $'\030'</span>
<span>json(): Could not encode the value of argument 'club:json@/dev/fd/...' as a 'json' value. Read from file /dev/fd/..., up to the first 0x00 byte or end-of-file.</span>
<span>�␘</span></pre></div>
<p dir="auto">Notice the <a href="https://en.wikipedia.org/wiki/Unicode_control_characters" rel="nofollow">␘</a> symbol in the output? It's the Unicode symbol for
the <a href="https://en.wikipedia.org/wiki/Cancel_character" rel="nofollow">Cancel control character</a>.</p>
<p dir="auto"><code>jb</code> propagates errors by emitting a <a href="https://en.wikipedia.org/wiki/Cancel_character" rel="nofollow">Cancel control character</a> when it
fails, which causes its output to be invalid JSON, which prevents the erroneous
output from being parsed by downstream JSON-consuming programs (<code>jb</code> or
otherwise). We call this Stream Poisoning, because the Cancel control character
poisons the output, and this poisoned output flows downstream until it's
detected.</p>
<p dir="auto">The result of this is that it's safe to pipe the output of a <code>jb</code> program into
another JSON-consuming program, with the knowledge that you'll get an error if
something has failed upstream, without needing to meticulously collect and check
the every exit status of every program contributing to the output.</p>
<p dir="auto">See <a href="https://github.com/h4l/json.bash/blob/main/docs/stream-poisoning.md">docs/stream-poisoning.md</a> for more on how this
works.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Security and correctness</h3><a id="user-content-security-and-correctness" aria-label="Permalink: Security and correctness" href="#security-and-correctness"></a></p>
<p dir="auto"><code>jb</code> can safely generate JSON from untrusted user input, but there are some ways
to get this wrong.</p>
<p dir="auto">It's safe to use untrusted input in:</p>
<ul dir="auto">
<li>
<p dir="auto">Inline values — after the value's <code>=</code> in an argument.</p>
<p dir="auto">With argument <code>key:type=value</code> Anything after the value's <code>=</code> in an argument
is used as-is and not interpreted/unescaped, so it can contain untrusted
input.</p>
<p dir="auto">The <code>=</code> must be preceded by a key or <code>:</code> (type section marker), otherwise an
argument starting with a <code>=</code> such as <code>=foo</code> is parsed as a key, which could
allow text inserted into the argument to be parsed as the value if not escaped
correctly.</p>
</li>
<li>
<p dir="auto">Variable references — the <em>value</em> held in a variable referenced by an
argument.</p>
<p dir="auto">With argument <code>@foo</code>, the value of <code>$foo</code> is is used as-is and not
interpreted/unescaped, so it can contain untrusted input.</p>
</li>
<li>
<p dir="auto">File references — the contents of a file referenced by a argument.</p>
<p dir="auto">The contents of files is not interpreted/unescaped, so they can contain
untrusted input.</p>
</li>
</ul>
<p dir="auto">In general, avoid inserting user-provided input into the argument string passed
to jb before the value's <code>=</code>. To create dynamic object property names from user
input, store the user-provided value in a variable or file, and use an <code>@ref</code> to
reference it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ dynamic_prop='Untrusted' jb @dynamic_prop=value
{&quot;Untrusted&quot;:&quot;value&quot;}"><pre>$ <span>dynamic_prop=<span><span>'</span>Untrusted<span>'</span></span> jb @dynamic_prop=value</span>
<span>{"Untrusted":"value"}</span></pre></div>
<p dir="auto">If you format user-input into an argument string, they could insert an <code>@ref</code> of
their choice, and pull in a file or variable they shouldn't have access to. You
can escape special characters in argument values by doubling characters, but
it's safer to use an <code>@ref</code> — if you get an <code>@ref</code> wrong you get an error,
whereas if you get escaping wrong, you may create a vulnerability.</p>
<p dir="auto">References are not supported when specifying argument attributes, like
<code>/empty=x/</code>, so <code>,</code> in these values needs to be escaped by doubling it. E.g. to
use a comma as a split char, use <code>/empty=string='Why,, yes.'/</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ empty= jb msg:/empty=string='Why,, yes.'/??@empty
{&quot;msg&quot;:&quot;Why, yes.&quot;}"><pre>$ <span>empty= jb msg:/empty=string=<span><span>'</span>Why,, yes.<span>'</span></span>/<span>??</span>@empty</span>
<span>{"msg":"Why, yes."}</span></pre></div>
<p dir="auto">To pass a dynamic file location, use a <code>_FILE</code> variable reference or read the
file with a normal shell construct and redirect the input. You must also
separately validate that the referenced file should be accessible.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ printf 'Example\nContent\n' > /tmp/example
$ user_file=/tmp/example

$ user_specified_FILE=$user_file jb user_file_content@user_specified
{&quot;user_file_content&quot;:&quot;Example\nContent\n&quot;}

$ jb user_file_content@<(cat &quot;$user_file&quot;)
{&quot;user_file_content&quot;:&quot;Example\nContent\n&quot;}

$ jb user_file_content=&quot;$(<&quot;$user_file&quot;)&quot;  # $() strips the trailing newline
{&quot;user_file_content&quot;:&quot;Example\nContent&quot;}"><pre>$ <span><span>printf</span> <span><span>'</span>Example\nContent\n<span>'</span></span> <span>&gt;</span> /tmp/example</span>
$ <span>user_file=/tmp/example</span>

$ <span>user_specified_FILE=<span>$user_file</span> jb user_file_content@user_specified</span>
<span>{"user_file_content":"Example\nContent\n"}</span>

$ <span>jb user_file_content@<span><span>&lt;(</span>cat <span><span>"</span><span>$user_file</span><span>"</span></span><span>)</span></span></span>
<span>{"user_file_content":"Example\nContent\n"}</span>

$ <span>jb user_file_content=<span><span>"</span><span><span>$(</span><span>&lt;</span><span><span>"</span><span>$user_file</span><span>"</span></span><span>)</span></span><span>"</span></span>  <span><span>#</span> $() strips the trailing newline</span></span>
<span>{"user_file_content":"Example\nContent"}</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Environment variable exposure</h4><a id="user-content-environment-variable-exposure" aria-label="Permalink: Environment variable exposure" href="#environment-variable-exposure"></a></p>
<p dir="auto"><code>jb</code> <code>@var</code> refs have the advantage over normal shell <code>$var</code> refs in that they
are not expanded by the shell before executing the command, so sensitive values
in shell variables are not exposed as process arguments when using <code>@var</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ password=hunter2

$ # shell $var — secret's value is in process arguments
$ jb password=&quot;$password&quot; visible_args:[]/split=/@/proc/self/cmdline
{&quot;password&quot;:&quot;hunter2&quot;,&quot;visible_args&quot;:[&quot;bash&quot;,&quot;/.../bin/jb&quot;,&quot;password=hunter2&quot;,&quot;visible_args:[]/split=/@/proc/self/cmdline&quot;]}

$ # jb @var — only the variable name is in process arguments
$ password=$password jb @password visible_args:[]/split=/@/proc/self/cmdline
{&quot;password&quot;:&quot;hunter2&quot;,&quot;visible_args&quot;:[&quot;bash&quot;,&quot;/.../bin/jb&quot;,&quot;@password&quot;,&quot;visible_args:[]/split=/@/proc/self/cmdline&quot;]}"><pre>$ <span>password=hunter2</span>

$ <span><span><span>#</span> shell $var — secret's value is in process arguments</span></span>
$ <span>jb password=<span><span>"</span><span>$password</span><span>"</span></span> visible_args:[]/split=/@/proc/self/cmdline</span>
<span>{"password":"hunter2","visible_args":["bash","/.../bin/jb","password=hunter2","visible_args:[]/split=/@/proc/self/cmdline"]}</span>

$ <span><span><span>#</span> jb @var — only the variable name is in process arguments</span></span>
$ <span>password=<span>$password</span> jb @password visible_args:[]/split=/@/proc/self/cmdline</span>
<span>{"password":"hunter2","visible_args":["bash","/.../bin/jb","@password","visible_args:[]/split=/@/proc/self/cmdline"]}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>jb-cat</code>, <code>jb-echo</code>, <code>jb-stream</code> utility programs</h3><a id="user-content-jb-cat-jb-echo-jb-stream-utility-programs" aria-label="Permalink: jb-cat, jb-echo, jb-stream utility programs" href="#jb-cat-jb-echo-jb-stream-utility-programs"></a></p>
<p dir="auto"><code>json.bash</code> has a few single-purpose utility programs that were originally demo
programs for the Bash API, but could be use useful by themselves:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # jb-echo is like echo, but each argument becomes a string element in a JSON array
$ jb-echo foo &quot;bar baz&quot; boz
[&quot;foo&quot;,&quot;bar baz&quot;,&quot;boz&quot;]

$ printf 'The Cat\nsat on\nthe mat.\n' > catmat
$ printf 'The Bat\nhid in\nthe hat.\n' > bathat

$ # jb-cat is like cat, but the output is stream-encoded as a single JSON string
$ jb-cat catmat bathat
&quot;The Cat\nsat on\nthe mat.\nThe Bat\nhid in\nthe hat.\n&quot;

$ # jb-stream is a filter program that encodes each input line as a JSON string
$ cat catmat bathat | jb-stream
&quot;The Cat&quot;
&quot;sat on&quot;
&quot;the mat.&quot;
&quot;The Bat&quot;
&quot;hid in&quot;
&quot;the hat.&quot;"><pre>$ <span><span><span>#</span> jb-echo is like echo, but each argument becomes a string element in a JSON array</span></span>
$ <span>jb-echo foo <span><span>"</span>bar baz<span>"</span></span> boz</span>
<span>["foo","bar baz","boz"]</span>

$ <span><span>printf</span> <span><span>'</span>The Cat\nsat on\nthe mat.\n<span>'</span></span> <span>&gt;</span> catmat</span>
$ <span><span>printf</span> <span><span>'</span>The Bat\nhid in\nthe hat.\n<span>'</span></span> <span>&gt;</span> bathat</span>

$ <span><span><span>#</span> jb-cat is like cat, but the output is stream-encoded as a single JSON string</span></span>
$ <span>jb-cat catmat bathat</span>
<span>"The Cat\nsat on\nthe mat.\nThe Bat\nhid in\nthe hat.\n"</span>

$ <span><span><span>#</span> jb-stream is a filter program that encodes each input line as a JSON string</span></span>
$ <span>cat catmat bathat <span>|</span> jb-stream</span>
<span>"The Cat"</span>
<span>"sat on"</span>
<span>"the mat."</span>
<span>"The Bat"</span>
<span>"hid in"</span>
<span>"the hat."</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Streaming output</h3><a id="user-content-streaming-output" aria-label="Permalink: Streaming output" href="#streaming-output"></a></p>
<p dir="auto">By default <code>jb</code> collects output in a buffer and outputs it all at once at the
end. This has the advantage that it does not emit partial output if an error
occurs mid-way through.</p>
<p dir="auto">However, setting the <code>JSON_BASH_STREAM=true</code> makes <code>jb</code> output content
incrementally. <code>jb</code> can stream-encode values it's pulling from file references:</p>
<ul dir="auto">
<li>Single string values from files are stream-encoded</li>
<li>Arrays of any type coming from files are stream-encoded (individual elements
are fully buffered), but elements are emitted incrementally</li>
<li><code>:raw</code> values from files are streamed</li>
</ul>
<p dir="auto"><code>:json</code> values can't be streamed unfortunately — <code>jb</code> (ab)uses grep to validate
JSON using PCRE's recursive matching features, but sadly grep buffers complete
inputs, even when backtracking and matched-region output are disabled.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Argument syntax details</h3><a id="user-content-argument-syntax-details" aria-label="Permalink: Argument syntax details" href="#argument-syntax-details"></a></p>
<p dir="auto">The full syntax of <code>jb</code> arguments is documented in a (pseudo) grammar in
<a href="https://github.com/h4l/json.bash/blob/main/hack/syntax_patterns.bash">hack/syntax_patterns.bash</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Background &amp; performance notes</h2><a id="user-content-background--performance-notes" aria-label="Permalink: Background &amp; performance notes" href="#background--performance-notes"></a></p>
<p dir="auto">Quite reasonably, you may be wondering why anyone would use Bash to implement a
JSON encoder. Won't that be ridiculously slow? I thought so too. Initially, I
just wanted to encode JSON strings from Bash without needing to depend on a
separate program. My initial few attempts at this were indeed hideously slow.
But after a few iterations I was able to get decent performance by operating
only on entire strings (or arrays of strings) (not byte-by-byte, or
string-by-string for arrays), and absolutely avoiding any forking of subshells.</p>
<p dir="auto">If you don't fork, and minimise the number of Bash-level operations, Bash can do
surprisingly well. Of course, performance still can't compare with a C program.
Well, that depends what you're measuring. Because starting a new process can be
surprisingly slow. So a race between <code>json.bash</code> and program like <a href="https://github.com/jqlang/jq"><code>jq</code></a> or
<a href="https://github.com/jpmens/jo"><code>jo</code></a> is a bit like a 100m race between a tortoise and a hare, where the
tortoise gets a 1 hour headstart.</p>
<p dir="auto">If you care about latency rather than throughput, calling <code>json</code> from an
already-running Bash script is a little faster than running a separate <code>jo</code>
process. And significantly faster than running <code>jq</code>, which is really slow to
start.</p>
<p dir="auto">There's a very basic benchmark script at
<a href="https://github.com/h4l/json.bash/blob/main/hack/hot_loop.bash">hack/hot_loop.bash</a>:</p>
<div data-snippet-clipboard-copy-content="$ time hack/hot_loop.bash json.bash 10000 > /dev/null
json.bash

real    0m8.193s
user    0m8.174s
sys     0m0.019s

$ time hack/hot_loop.bash jo 10000 > /dev/null
jo

real    0m9.393s
user    0m2.566s
sys     0m7.386s

$ # Note: 1000 not 10_000
$ time hack/hot_loop.bash jq 1000 > /dev/null
jq

real    0m20.453s
user    0m19.127s
sys     0m1.386s"><pre><code>$ time hack/hot_loop.bash json.bash 10000 &gt; /dev/null
json.bash

real    0m8.193s
user    0m8.174s
sys     0m0.019s

$ time hack/hot_loop.bash jo 10000 &gt; /dev/null
jo

real    0m9.393s
user    0m2.566s
sys     0m7.386s

$ # Note: 1000 not 10_000
$ time hack/hot_loop.bash jq 1000 &gt; /dev/null
jq

real    0m20.453s
user    0m19.127s
sys     0m1.386s
</code></pre></div>
<p dir="auto">If we just use <code>json.bash</code>'s <code>json.encode_string</code> encoding function to manually
construct the JSON (not the full argument parsing stuff) we can do a lot better
still:</p>
<div data-snippet-clipboard-copy-content="$ time hack/hot_loop.bash custom-json.bash 10000 > /dev/null
custom-json.bash

real    0m1.901s
user    0m1.891s
sys     0m0.011s"><pre><code>$ time hack/hot_loop.bash custom-json.bash 10000 &gt; /dev/null
custom-json.bash

real    0m1.901s
user    0m1.891s
sys     0m0.011s
</code></pre></div>
<p dir="auto">This kind of purpose-specific encoding is what I had in mind when I started
this. I was calling <code>jq</code> lots of times from a Bash script, finding it to be very
slow, and wondering if I could start a single <code>jq</code> process and make a kind of
tiny RPC protocol, sending it JSON from the Bash script to avoid the startup
delay on each operation. That would require some ability to encode JSON from
Bash.</p>
<p dir="auto">I wasn't planning to write something comparable to <code>jo</code> when I started, but idea
of a <code>jo</code>-like program that only depends on bash kind of appealed to me. Maybe I
should port it to a more suitable language though. The program is a now a lot
larger in size and scope than I originally anticipated when starting, I
certainly wouldn't have written it in bash if I'd known how large it'd become.
🙃</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li><a href="https://github.com/jpmens/jo">jo</a> for the general idea of a command-line program that generates JSON</li>
<li><a href="https://github.com/OceanSprint/tesh">tesh</a> which automatically runs and tests the command-line output examples
here — it would not be at all practical to maintain these kind of examples
without it. With tesh the examples become a beneficial second layer of tests,
rather than a maintenance burdon.</li>
<li><a href="https://github.com/jqlang/jq">jq</a> for making it pleasant to use with JSON on the command-line and in shell
scripts</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Received an AI Email (647 pts)]]></title>
            <link>https://timharek.no/blog/i-received-an-ai-email</link>
            <guid>40862865</guid>
            <pubDate>Wed, 03 Jul 2024 05:05:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timharek.no/blog/i-received-an-ai-email">https://timharek.no/blog/i-received-an-ai-email</a>, See on <a href="https://news.ycombinator.com/item?id=40862865">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Why AI Infrastructure Startups Are Insanely Hard to Build (180 pts)]]></title>
            <link>https://nextword.substack.com/p/why-ai-infrastructure-startups-are</link>
            <guid>40862436</guid>
            <pubDate>Wed, 03 Jul 2024 03:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nextword.substack.com/p/why-ai-infrastructure-startups-are">https://nextword.substack.com/p/why-ai-infrastructure-startups-are</a>, See on <a href="https://news.ycombinator.com/item?id=40862436">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Recently, </span><a href="http://adept.ai/" rel="">Adept AI</a><span> announced </span><a href="https://techcrunch.com/2024/06/28/amazon-hires-founders-away-from-ai-startup-adept/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAM7kyimVR-Nntc7w3SCp466ss9rI61B5U68ESaJExyUE65kA2h6sdH5pHKpdJ0oi6Y0SvXy7wg2OgsX1JTB-hZw9n0esnLkFCY_6JckUVqoIbgGFEy2gSjzMw4YJBYwbCFKJqPR19xgviwpcnO8cCVPa99I_tMkjrjBWHoQqPtTI" rel="">they are being acquired by Amazon</a><span>, and this solidified a somewhat controversial opinion I’ve held for a while - </span><strong>that AI infra startups are a tarpit idea</strong><span>, </span><strong><span>especially as a “venture-scale” business.</span></strong></p><p><span>The term “tarpit idea” refers to startup ideas that sound reasonable on the surface, but when put to test against reality or rigorous thought, fail to hold up.</span><br></p><div><p><span>I believe most AI infra startups will also fall into this category, where AI infra refers to the “building blocks” companies </span><strong>between the cloud layer and the application layer</strong><span> - RAG services, finetuning infrastructure, text processing services, TTS APIs, vector databases, etc. I won’t name specific names, but just think of any AI infra startup that raised sizable seed rounds off of open source or social media momentum.</span></p></div><p><span>I also believe </span><strong>many founders agree with this viewpoint</strong><span>, which explains the sale of Adept (to Amazon), </span><a href="https://openai.com/index/openai-acquires-rockset/" rel="">Rockset (to OpenAI)</a><span>, InflectionAI (to Microsoft), as well as the soon to be acquisitions of Stability (if it happens), </span><a href="http://character.ai/" rel="">Character</a><span>AI, etc. Every incumbent is looking at M&amp;A to paint an “end-to-end AI platform” story. Only a lucky few will get bought.</span><br></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;June 28 updated AI Infra market map&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="June 28 updated AI Infra market map" title="June 28 updated AI Infra market map" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Source: Bessemer Venture Partners</figcaption></figure></div><div><p><span>So why is selling AI infrastructure </span><em>as a startup</em><span> a tarpit idea? On paper, it’s perfectly reasonable to sell picks and shovels amidst proliferation of AI startups and enterprises building Gen AI features. After all, there’s over 30K “.ai” domains registered every month.</span></p></div><div><p><strong>In a nutshell, the new AI infra startups will struggle to succeed because they lack significant differentiation and capital to crack the enterprise segment.</strong><span> It’s not the startups’ fault, the real problem is competitive dynamics. There’s simply too many entities offering the same table stakes features within 1-3 months apart from each other, which creates a collective tarpit dynamic, where only the incumbents can keep swimming.</span></p></div><p>The argument goes:</p><ul><li><p>For AI infra startups to be “venture scale”, they will eventually need to win over enterprise customers. No question. That requires the startups to have some sustainable edge that separates their products from the incumbents’ (GCP, AWS, as well as the likes of Vercel, Databricks, Datadog, etc).</p></li><li><p>Unfortunately, most cutting edge innovation either comes from the incumbents or the research / OSS community - and incumbents are in a better position to commercialize the innovations because they have more usage data than startups, as well as the relationships.</p></li><li><p>To add salt to the injury, any good ideas that originate from startups get benchmarked and copied quickly. For example, I was quite surprised how quickly Databricks and Datadog caught up to the leading LLMOps products from the startup world (e.g. Arize AI). </p></li><li><p>Furthermore, OSS community can’t help but create OSS versions of other AI infra startups’ products - perhaps a testament to how easy it has become to write software.</p></li><li><p>Thus, startups struggle to maintain a sustainable lead over the incumbents to buy them time to win enterprise contracts.</p></li><li><p>And enterprise customers are incentivized to “hold off” on onboarding new vendors, because vendor products diminish in value so quickly because AI landscape changes every few months.</p></li><li><p>This ultimately lengthens sales cycles, and increases churn, which hurts startups more than the incumbents.</p></li></ul><div><p><span>There are also some other dynamics at play (to be discussed in the next section) - but essentially the AI infrastructure space becomes a grind that favors players with the longest runways.</span></p></div><div><p><span>My intention here is not to doom-post, but to highlight some real challenges, which I’m happy to be wrong on (DM me if you disagree). Also, I will end by offering some advice to AI infra startups.</span></p></div><p><em><span>To clarify, by “AI infra startup”, I’m referring to “venture scale” AI infrastructure startups. I’m sure founders can create essentially system integration agencies targeting SMB or mid market, and call themselves an AI infra company. But that’s a completely different business with a much smaller upside.</span><br></em></p><p>There’s three other major forces that’s worsening the competitive environment:</p><ol><li><p>Builders are now conditioned to “demand” composability, a.k.a making it easy to switch out your product for others’. This is great for application layer companies, but not infrastructure companies. Developers can rip out Langchain with Llamaindex, OpenAI models with Claude 3.5 through AWS Bedrock, etc. Every layer of the LLM training and inference stack has at least 10+ viable solutions, that it becomes difficult to create any type of lock-in.</p></li><li><p>The ongoing plummeting of inference costs also plays a role. The COGS are dropping fast, so AI infra players need to constantly price-match the incumbents who have the biggest economies of scale. Models or code have little perceived differentiation, so the consumption goes to the lowest cost providers (incumbents).</p></li><li><p>Incumbents seem to all have the same business strategy of creating an “end-to-end AI platform”. Databricks is getting into AI model training and business intelligence, competing with AWS Sagemaker and Tableau. Github Workspaces is getting into AI-powered security reviews, etc.</p><ol><li><p>Everyone’s default product strategy is to own all upstream and downstream workloads from their core product, which unintentionally makes startups’ lives more difficult, since it becomes hard to compete with a point solution.</p></li></ol></li></ol><p><br><span>With all these challenges, some AI infra startups have chosen to go vertical or move to the application layer. For example, I have been tracking a “Business Intelligence with Natural Language” startup since late 2022 that has pivoted three times already from:</span></p><ul><li><p>a general purpose “chat with data” platform, to</p></li><li><p>“chat with business intelligence data” platform, to</p></li><li><p>“chat with financial data” platform.</p></li></ul><div><p><span>The AI infra darlings LlamaIndex and Langchain also took this path of focus when it comes to their enterprise-oriented products. LlamaIndex is focusing on managed document parsing / OCR, whereas Langchain is focusing on LLMOps and agent building solutions. My guess is that both are working on narrowing their focus even further, since even selling a managed document parsing service is a huge scope for a seed-stage startup, given that Google and AWS already have existing vertical text extraction services. It’s not easy.</span></p></div><div><p><span>Narrowing the scope and going vertical is a typical response for AI infra startups - but I argue that these pivots rarely work out and cause new set of problems. Most importantly, these vertical pivots underestimate the importance of deep domain expertise once you go vertical, which many AI infra founders lack. Accumulating domain knowledge is time consuming. Also, your product may need to be heavily customized for the unique needs of the vertical, which means lower margins.</span></p></div><p><span>Not to mention, these application layer ecosystems have even worse competition (e.g. VCs’ LegalTech ecosystem maps ran out of space to put new logos long time ago). There’s not just the other AI startup competition, but competition from the legacy software companies. Pivoting to a vertical does not suddenly get rid of your competitors - you will just have new ones in that vertical who have been there before you. For example, legal tech industry has existed for ages, and many Legal AI companies are now competing with </span><strong>the legacy legal tech providers plus system integrators.</strong></p><div><p><span>So what’s the solution for AI infra startups? Should we all hope to be acquihired, or is it possible for startups to also stay independent for longer and find product market fit?</span></p><p><span>Here’s a somewhat anti-climatic answer, but the solution for startups goes back to the fundamentals: </span><strong>think deeply about how to be different from the incumbents.</strong><span> Here are four ways to iterate from here:</span></p></div><ul><li><p><strong>Narrow down the scope even further:</strong><span> focus on a very tiny segment of enterprise customers, as opposed to serving all customers. Don’t build all the integrations. Be a managed RAG service for customers using Salesforce with on-prem VMWare, as opposed to a general purpose RAG service. Startups don’t have the resources to solve for every environment, at least initially.</span></p></li><li><p><strong>Focus on just one workload:</strong><span> startups shouldn’t try to solve for too many workloads. Do one thing really well. Don’t try to be a platform for finetuning any LLM - there’s already too many of those. Instead, try to be the best platform for finetuning Tagalog models. The catch: the TAM might be too small.</span></p></li><li><p><strong>Raise more VC money than you think you need:</strong><span> long runways are non-negotiable. It can take a while for enterprises to be receptive to buying startup AI infra solutions, if ever. Be prepared for the worst case scenario.</span></p></li><li><p><strong>Or, don’t raise any VC money at all:</strong><span> raising VC money kind of forces you to orient business strategy around selling to the enterprise - which might be not something you can or want to do. You want the flexibility to work on more interesting and promising problems when they arise, given there’s constantly new changes in AI landscape.</span></p></li></ul><p><span>Lastly, AI startups should be open to being acquired by a larger player, even if it’s not a prestigious destination like OpenAI or Google. </span><strong>My view is that M&amp;A landscape for AI infrastructure sector will become worse, not better, over time.</strong></p><p><span>The acquisition market will become more “efficient” as the winners/losers emerge, and the workloads and enterprise needs become more clearly defined. Thus, in order to sell your startup at an “attractive” valuation, it needs to be marketed prior to the dust settles when the market is less efficient. Don’t wait for another 18 months to shop your startup, when all AI infra startups start running out of runway at the same time.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Bridges Don't Sink (268 pts)]]></title>
            <link>https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink</link>
            <guid>40861520</guid>
            <pubDate>Tue, 02 Jul 2024 23:43:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink">https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink</a>, See on <a href="https://news.ycombinator.com/item?id=40861520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="item-668414c1a012432cf6a43a4b" data-layout-label="Post Body" data-type="item" data-updated-on="1719932256684">
  <p><em>[Note that this article is a transcript of the video embedded above.]</em></p><p>The essence of a bridge is not just that it goes over something, but that there’s clear space underneath for a river, railway, or road. Maybe this is already obvious to you, but bridges present a unique structural challenge. In a regular road, the forces are transferred directly into the ground. On a bridge, all those forces on the span get concentrated into the piers or abutments on either side. Because of that, bridge substructures are among the strongest engineered systems on the planet. And yet, bridge foundations are built in some of the least ideal places for heavy loading. Rivers and oceans have soft, mucky soils that can’t hold much weight. Plus, obviously, a lot of them are underwater.</p><p>What happens when you overload soil with a weight it can’t handle? In engineering-speak, it’s called a bearing failure, but it’s as simple as stepping in the mud. The foundation just sinks into the ground. But, what if you just keep loading it and causing it to sink deeper and deeper? Congratulations! You just invented one of the most widely used structural members on earth: the humble foundation pile. How do they work, and how can you install them underwater? I’m Grady, and this is Practical Engineering. Today we’re having piles of fun talking about deep foundations.</p><p>I did a video all about the different types of foundations used in engineering, but I didn’t go too deep into piles. A pile is a fairly simple structural member, just a long pole driven or drilled into the ground. But, behind that simplicity is a lot of terrifically complex engineering. Volume 1 of the Federal Highway Administration’s manual on the Design and Construction of Driven Pile Foundations is over 500 pages long. There are 11 pages of symbols, 2 pages of acronyms, and you don’t even get to the introduction until page 46. And just a little further than that, you get some history of driven piles. Namely that the history has been lost to time. Humans have been hammering sticks into the ground since way before we knew how to write about it. And that’s pretty much all a driven pile is.</p><p>The first piles were made from timber, and wood is still used all these years around the world. Timber piles are cheap, resilient to driving forces, and easy to install. But, wood rots, it has an upper limit on length from the size of the tree, and it’s not that strong compared to the alternatives. Concrete piles solve a lot of those problems. They come in a variety of sizes and shapes, and again, are widely used for deep foundations. One disadvantage of concrete piles is that they have to be pretty big to withstand the force required to drive them into ground. Some concrete piles can be upwards of 30 inches or 75 centimeters wide. It is hard to hit something that big hard enough to drive it downward into soil, and a lot of ground has to either get out of the way or compress in place to make room. Steel piles solve that problem since they can be a lot more slender. Pipe piles are just what they sound like, and the other major alternative is an H-pile. Your guess is as good as mine why the same steel shape is an <em>I</em>-beam but an <em>H</em>-pile. But, no matter the material, all driven piles are installed in basically the same way.&nbsp;</p><p>Newton’s third law applies to piles like everything else. To push one deep into the ground creates an equal and opposite reaction. You <em>would </em>need either an enormous weight to take advantage of gravity or some other strong structure attached to the ground to react against and develop the pushing force required to drive it downward. Instead of those two options, we usually just use a hammer. By dropping a comparatively small weight from a height, we convert the potential energy of the weight at that height into kinetic energy. The force required to stop the hammer as it falls gets transferred into the pile. Hopefully this is intuitive. It’s pretty hard to push a nail into wood, but it’s pretty easy to hammer it in... well, it’s <em>a little bit</em> easier to hammer it in. There are quite a few types of pile drivers, but most of them use a large hammer or vibratory head to create the forces required.</p><p>Maybe it goes without saying, but the main goal of a foundation is to not move. When you apply a load, you want it to stay put. Luckily, piles have two ways to do that (at least for vertical loads). The first is end-bearing. The end, or toe, of a pile can be driven down to a layer of strong soil or hard rock, making it able to withstand greater loads. But there’s not always a firm stratum at a reasonable depth below the ground. Quote-unquote “bedrock” is a simple idea, but in practice, geology is more complicated than that. Luckily, piles have a second type of resistance: skin friction, also known as shaft resistance. When you drive a pile, it compacts and densifies the surrounding soil, not only adding strength to the soil itself, but creating friction along the walls of the pile that hold it in place. The deeper you go, the more friction you get. Let me show you what I mean.</p><p>I have my own pipe pile in the backyard that I’ve marked with an arbitrary scale. When I drop the hammer at a prescribed height, the pile is driven a certain distance into the ground. Do this enough times, and eventually, you reach a point where the pile kind of stops moving with each successive hammer blow. In technical terms, the pile has reached refusal. I can graph the blow count required to drive the pile to each depth, and you get a pretty nice curve. It’s easy to see how it got stronger against vertical loads the deeper I drove it in. Toward the end, it barely moved with each hit. This is a really nice aspect of driven piles, you install them in a similar way to how they’ll be loaded by the final design. Of course, bridges and buildings don’t hammer on their foundations, but they do impose vertical loads. The tagline of the Pile Driving Contractors Association is “A Driven Pile is a Tested Pile” because, just by installing them, you’ve verified that they can withstand a certain amount of force. After all, you had to overcome that force to get them in the ground. And if you’re not seeing enough resistance, in most cases, you can just keep driving downward until you do!</p><p>But piles don’t just resist downward forces. Structures experience loads in other directions too. Buildings have horizontal, or lateral, loads from wind. Bridges see lateral loads from flowing water, and even ice or boats contacting the piers. Both can experience uplift forces that counteract gravity from floods due to buoyancy or strong winds. If you’ve ever hammered in a tent stake, you know that piles can withstand loading from all kinds of directions. And then there’s scour. The soil along a bridge might look like this right after the bridge is built, but after a few floods, it can look completely different. Engineers have to try and predict how the soil around a bridge will scour over time, from natural changes in the streambed and those created by the bridge itself. Then they make sure to design foundations that can accommodate those changes and stay strong over the long term. This is why bridge foundations sometimes look kind of funny. Loads transfer from the superstructure down into the piers. The piers sit on a pile cap that transfers and distributes loads into the piles themselves. Those piles can be vertical, but if the engineer is expecting serious lateral loads, some of the piles are often inclined, also called battered piles. Inclined piles take better advantage of the shaft resistance to make the foundation stronger against horizontal loads.</p><p>As important and beneficial as they are, driven piles have some limitations too. For one, they’re noisy and disruptive to install. Just last year, I had two friends on separate trips to Seattle who sent me a video of the exact same pile-driving operation. It’s good to have friends who know how much you like construction. But my point is, this type of construction is pretty much impossible to ignore. In dense urban areas, most people are just not willing to put up with the constant banging. Plus the vibrations from installing them can disrupt surrounding infrastructure. Pile driving is crude; in many cases, the piles aren’t designed to withstand the forces of the structure they’ll support but rather the forces they’ll have to experience during installation which are much higher. They can’t easily go through hard geological layers, cobbles, or boulders; they can wander off path, since you can’t really see where you’re going, and they can cause the ground to heave because you’re not removing any soil while you force them into the subsurface. The second major category of piles solves a lot of these problems.</p><p>And, wouldn’t you know it? There’s an FHWA manual that has all the juicy details - Drilled Shafts: Construction Procedures and Design Methods. This one a whopping 747 pages long. A drilled shaft is also exactly what it sounds like. The basic process is pretty simple. Drill a long hole into the ground. Place reinforcing steel in the hole. Then fill the whole thing with concrete. But, bridge piers are often, as you probably know, installed underwater. Pouring concrete underwater is a little tricky. Imagine trying to pour a smoothie at the bottom of a pool! Let me show you what I mean.</p><p>This is my garage-special bridge foundation simulator. It has transparent soil in the form of superabsorbent polymer beads… and you know we have to add some blue water too. You can probably imagine how easy it might be to drill a hole in this soil. It’s just going to collapse in on itself. We need a way to keep the hole open so the rebar and concrete can be installed. So, drilled shafts installed in soft soils or wet conditions usually rely on a casing to support the walls. Installing a casing usually happens while the hole is drilled, following the auger downward. I tried that myself, but I only have two hands, and it was pretty unwieldy. So, just for the sake of the demo, I’m advancing the casing into the soil ahead of time. Now I can drill out the soil to open the shaft. And now I’m realizing the limitations of my soil simulant. It was still pretty hard to do, even with the casing in place. It took a few tries, but I managed to get most of it out.</p><p>So now I have an open hole, but it’s still full of water. Even if your casing runs above the water surface, and you try to pump it out, you can still have water leaking in from the bottom. In ideal conditions, you can get a nice seal between the bottom of the casing and the soil, but even then, it’s pretty hard to keep water out of the hole, and luckily it doesn’t matter.</p><p>Instead of concrete, I’m using bentonite clay as a substitute. It’s got a similar density, and it’s perfect for this demo because you can push it through a small tube… if you get the proportions right. Ask me how I know. This is me pondering the life decisions that led up to me holding a gigantic syringe full of bentonite slurry in my garage. You can’t just drop this stuff through the water. It mixes and dilutes, just turning into a mess. Same is true for concrete. The ratio of water to cement in a concrete mix is essential to its strength and performance, so you can’t do anything that would add water to the mix. The trick is a little device called a tremie. Even though it has a funny name, it’s nothing more than a pipe that runs to the bottom of the hole. As long as you keep the end of the tremie below the surface of the concrete that you’re pumping in, or concrete simulant in my case, there’s no chance for it to mix with the water and dilute. I’m just pushing the clay into the casing with a big syringe, making sure to keep the end of the tube buried. Because concrete is a lot more dense than water, it just displaces it upward, out of the hole.&nbsp;</p><p>In underwater installations, the casing is often left in place. One advantage is that you can build a floating pile cap. Instead of building a big cofferdam and drying out the work area to construct a big concrete structure, sometimes you can raise the pile cap into or above the water surface, reducing the complexity of its construction. These “high rise” pile caps are used a lot in offshore wind turbines. But, not all casings are permanent.</p><p>In some situations, it’s possible to pull the casing once the hole is full of concrete, saving the sometimes enormous cost of each gigantic steel tube. I tried to show this in my demo. It’s not beautiful, but it did work. Again, the concrete is dense, so the pressure it exerts on the walls of the hole is enough to keep the soil from collapsing. And because drilled shafts can be much larger than driven piles, sometimes you don’t even need a group of them. Lots of structures, including wind turbines, highway signs, and more, are built on mono-pile foundations. Just a single drilled shaft deep in the ground, eliminating the need for a pile cap altogether. Another interesting aspect of drilled shafts is that you can ream out the bottom, creating an enlarged base that increases the surface area at the toe. This helps reduce a pile’s tendency to sink, and it can help with uplift resistance too.</p><p>Driven piles and drilled shafts are far from the only types of deep foundation systems. There are tons of variations on the idea that have been developed over the years to solve specific challenges: Continuous flight auger piles do the drilling and concreting in essentially one step, using a hollow-stem auger to fill the hole as it’s removed. Then reinforcement is lowered into the wet concrete. You can fill a hole with compacted aggregate instead of concrete, called a stone column or tradename Geopier if you’re only worried about compressive loads. Helical or screw piles twist into the ground, instead of being hammered, reducing vibrations and disturbance. Micropiles are like tiny drilled shafts used when there are access restrictions or geologic constraints. And of course, there are sheet piles that aren’t really used for foundations but are driven piles meant to create a wall or barrier. Let me know if I forgot to mention your favorite flavor of pile.</p><p>Even though they’re usually much stronger than shallow foundations, piles can and do fail. We’ve talked about San Francisco’s famous Millennium Tower in a previous video. That’s a skyscraper on a pile foundation that sank into the ground, causing the building to tilt. It seems like they mostly have it fixed now, but it’s still in the news every so often, so only time will tell. In 2004, a bridge pier on the Lee Roy Selmon Expressway in Tampa, Florida sank 11 feet (more than 3 meters) while it was still under construction because of the complicated geology. It cost 90 million dollars to fix and delayed the project’s completion by a year. These case studies highlight the complexity of geotechnical engineering when we ask the ground to hold up heavier and heavier loads. The science and technology that goes into designing deep foundations are enough to spend an entire career studying, but hopefully, this video gives you a little insight into how they work.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Illustrated Transformer (2018) (151 pts)]]></title>
            <link>https://jalammar.github.io/illustrated-transformer/</link>
            <guid>40861148</guid>
            <pubDate>Tue, 02 Jul 2024 22:42:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a>, See on <a href="https://news.ycombinator.com/item?id=40861148">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><span>Discussions:
<a href="https://news.ycombinator.com/item?id=18351674">Hacker News (65 points, 4 comments)</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/8uh2yz/p_the_illustrated_transformer_a_visual_look_at/">Reddit r/MachineLearning (29 points, 3 comments)</a>
</span>
<br>
<span>Translations: <a href="https://www.mundhor.site/post/post14">Arabic</a>, <a href="https://blog.csdn.net/yujianmin1990/article/details/85221271">Chinese (Simplified) 1</a>, <a href="https://blog.csdn.net/qq_36667170/article/details/124359818">Chinese (Simplified) 2</a>, <a href="https://a-coles.github.io/2020/11/15/transformer-illustre.html">French 1</a>, <a href="https://lbourdois.github.io/blog/nlp/Transformer/">French 2</a>, <a href="https://medium.com/@val.mannucci/il-transformer-illustrato-it-37a78e3e2348">Italian</a>, <a href="https://tips-memo.com/translation-jayalmmar-transformer">Japanese</a>, <a href="https://nlpinkorean.github.io/illustrated-transformer/">Korean</a>, <a href="http://dml.qom.ac.ir/2022/05/17/illustrated-transformer/">Persian</a>, <a href="https://habr.com/ru/post/486358/">Russian</a>, <a href="https://www.ibidemgroup.com/edu/transformer-ilustrado-jay-alammar/">Spanish 1</a>, <a href="https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-espanol-0y73wwp">Spanish 2</a>, <a href="https://trituenhantao.io/tin-tuc/minh-hoa-transformer/">Vietnamese</a></span>
<br>
<span>Watch: MIT’s <a href="https://youtu.be/53YvP6gdD7U?t=432">Deep Learning State of the Art</a> lecture referencing this post</span>
<br>
<span>Featured in courses at <a href="https://web.stanford.edu/class/cs224n/">Stanford</a>, <a href="https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers">Harvard</a>, <a href="https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf">MIT</a>, <a href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/">Princeton</a>, <a href="https://www.cs.cmu.edu/~leili/course/mldl22w/14-Transformer.pdf">CMU</a> and others</span></p>

<p>In the <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">previous post, we looked at Attention</a> – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at <strong>The Transformer</strong> – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their <a href="https://cloud.google.com/tpu/">Cloud TPU</a> offering. So let’s try to break the model apart and look at how it functions.</p>

<p>The Transformer was proposed in the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>. A TensorFlow implementation of it is available as a part of the <a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor</a> package. Harvard’s NLP group created a <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">guide annotating the paper with PyTorch implementation</a>. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.</p>

<p><strong>2020 Update</strong>: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:</p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-QH8fRhqFHM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p>
<h2 id="a-high-level-look">A High-Level Look</h2>
<p>Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.</p>

<p><img src="https://jalammar.github.io/images/t/the_transformer_3.png">
</p>

<!--more-->

<p>Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.</p>

<p><img src="https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png">
</p>

<p>The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.</p>

<p><img src="https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png">
</p>

<p>The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:</p>

<p><img src="https://jalammar.github.io/images/t/Transformer_encoder.png">
</p>

<p>The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.</p>

<p>The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.</p>

<p>The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">seq2seq models</a>).</p>

<p><img src="https://jalammar.github.io/images/t/Transformer_decoder.png">
</p>

<h2 id="bringing-the-tensors-into-the-picture">Bringing The Tensors Into The Picture</h2>

<p>Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.</p>

<p>As is the case in NLP applications in general, we begin by turning each input word into a vector using an <a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca">embedding algorithm</a>.</p>



<p><img src="https://jalammar.github.io/images/t/embeddings.png">
  <br>
  Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.
</p>

<p>The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.</p>

<p>After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.</p>

<p><img src="https://jalammar.github.io/images/t/encoder_with_tensors.png">
  <br>

</p>

<p>Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.</p>

<p>Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.</p>

<h2 id="now-were-encoding">Now We’re Encoding!</h2>

<p>As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.</p>

<p><img src="https://jalammar.github.io/images/t/encoder_with_tensors_2.png">
  <br>
  The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.
</p>

<h2 id="self-attention-at-a-high-level">Self-Attention at a High Level</h2>
<p>Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.</p>

<p>Say the following sentence is an input sentence we want to translate:</p>

<p>”<code>The animal didn't cross the street because it was too tired</code>”</p>

<p>What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.</p>

<p>When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.</p>

<p>As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.</p>

<p>If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png">
  <br>
  As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The Animal", and baked a part of its representation into the encoding of "it".
</p>

<p>Be sure to check out the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Tensor2Tensor notebook</a> where you can load a Transformer model, and examine it using this interactive visualization.</p>

<h2 id="self-attention-in-detail">Self-Attention in Detail</h2>
<p>Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.</p>

<p>The <strong>first step</strong> in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</p>

<p>Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png">
  <br>
  Multiplying <span>x1</span> by the <span>WQ</span> weight matrix produces <span>q1</span>, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.
</p>



<div><p>What are the “query”, “key”, and “value” vectors?
</p><p>

They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.</p></div>

<p>The <strong>second step</strong> in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.</p>

<p>The score is calculated by taking the dot product of the <span>query vector</span> with the <span>key vector</span> of the respective word we’re scoring. So if we’re processing the self-attention for the word in position <span>#1</span>, the first score would be the dot product of <span>q1</span> and <span>k1</span>. The second score would be the dot product of <span>q1</span> and <span>k2</span>.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_self_attention_score.png">
  <br>

</p>



<p>The <strong>third and fourth steps</strong> are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.</p>



<p><img src="https://jalammar.github.io/images/t/self-attention_softmax.png">
  <br>

</p>

<p>This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.</p>



<p>The <strong>fifth step</strong> is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).</p>

<p>The <strong>sixth step</strong> is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).</p>



<p><img src="https://jalammar.github.io/images/t/self-attention-output.png">
  <br>
</p>

<p>That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.</p>

<h2 id="matrix-calculation-of-self-attention">Matrix Calculation of Self-Attention</h2>
<p><strong>The first step</strong> is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix <span>X</span>, and multiplying it by the weight matrices we’ve trained (<span>WQ</span>, <span>WK</span>, <span>WV</span>).</p>

<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png">
  <br>
  Every row in the <span>X</span> matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)
</p>



<p><strong>Finally</strong>, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.</p>

<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png">
  <br>
  The self-attention calculation in matrix form
</p>





<h2 id="the-beast-with-many-heads">The Beast With Many Heads</h2>

<p>The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:</p>

<ol>
  <li>
    <p>It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.</p>
  </li>
  <li>
    <p>It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.</p>
  </li>
</ol>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png">
   <br>
   With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.
 </p>

<p><br>
If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices</p>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png">
  <br>

</p>



<p>This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.</p>

<p>How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png">
  <br>

</p>

<p>That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place</p>



<p><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png">
  <br>

</p>



<p>Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png">
  <br>
  As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".
</p>



<p>If we add all the attention heads to the picture, however, things can be harder to interpret:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png">
  <br>
</p>

<h2 id="representing-the-order-of-the-sequence-using-positional-encoding">Representing The Order of The Sequence Using Positional Encoding</h2>
<p>One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.</p>

<p>To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png">
  <br>
  To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.
</p>


<p>If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png">
  <br>
  A real example of positional encoding with a toy embedding size of 4
</p>



<p>What might this pattern look like?</p>

<p>In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png">
  <br>
  A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.
</p>

<p>The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in <a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py"><code>get_timing_signal_1d()</code></a>. This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).</p>

<p><strong>July 2020 Update:</strong> 
The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. <a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb">Here’s the code to generate it</a>:</p>

<p><img src="https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png">
  <br>
</p>

<h2 id="the-residuals">The Residuals</h2>
<p>One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a <a href="https://arxiv.org/abs/1607.06450">layer-normalization</a> step.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png">
  <br>
</p>

<p>If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png">
  <br>
</p>

<p>This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png">
  <br>
</p>

<h2 id="the-decoder-side">The Decoder Side</h2>
<p>Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.</p>

<p>The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif">
  <br>
  After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).
</p>

<p>The following steps repeat the process until a special <end of="" sentence=""> symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</end></p>

<p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif">
  <br>

</p>

<p>The self attention layers in the decoder operate in a slightly different way than the one in the encoder:</p>

<p>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to <code>-inf</code>) before the softmax step in the self-attention calculation.</p>

<p>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>

<h2 id="the-final-linear-and-softmax-layer">The Final Linear and Softmax Layer</h2>

<p>The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.</p>

<p>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</p>

<p>Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.</p>

<p>The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png">
  <br>
  This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.
</p>



<h2 id="recap-of-training">Recap Of Training</h2>
<p>Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.</p>

<p>During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.</p>

<p>To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “&lt;eos&gt;” (short for ‘end of sentence’)).</p>

<p><img src="https://jalammar.github.io/images/t/vocabulary.png">
   <br>
   The output vocabulary of our model is created in the preprocessing phase before we even begin training.
 </p>

<p>Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:</p>

<p><img src="https://jalammar.github.io/images/t/one-hot-vocabulary-example.png">
  <br>
  Example: one-hot encoding of our output vocabulary
</p>

<p>Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.</p>

<h2 id="the-loss-function">The Loss Function</h2>
<p>Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.</p>

<p>What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_logits_output_and_label.png">
  <br>
  Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.
</p>



<p>How do you compare two probability distributions? We simply subtract one from the other. For more details, look at  <a href="https://colah.github.io/posts/2015-09-Visual-Information/">cross-entropy</a> and <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback–Leibler divergence</a>.</p>

<p>But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:</p>

<ul>
  <li>Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)</li>
  <li>The first probability distribution has the highest probability at the cell associated with the word “i”</li>
  <li>The second probability distribution has the highest probability at the cell associated with the word “am”</li>
  <li>And so on, until the fifth output distribution indicates ‘<code>&lt;end of sentence&gt;</code>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.</li>
</ul>

<p><img src="https://jalammar.github.io/images/t/output_target_probability_distributions.png">
   <br>
   The targeted probability distributions we'll train our model against in the training example for one sample sentence.
 </p>



<p>After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png">
    <br>
    Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: <a href="https://www.youtube.com/watch?v=TIgfjmp-4BA">cross validation</a>). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.
</p>

<p>Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.</p>

<h2 id="go-forth-and-transform">Go Forth And Transform</h2>

<p>I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:</p>

<ul>
  <li>Read the <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> paper, the Transformer blog post (<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a>), and the <a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html">Tensor2Tensor announcement</a>.</li>
  <li>Watch <a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">Łukasz Kaiser’s talk</a> walking through the model and its details</li>
  <li>Play with the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></li>
  <li>Explore the <a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor repo</a>.</li>
</ul>

<p>Follow-up works:</p>

<ul>
  <li><a href="https://arxiv.org/abs/1706.03059">Depthwise Separable Convolutions for Neural Machine Translation</a></li>
  <li><a href="https://arxiv.org/abs/1706.05137">One Model To Learn Them All</a></li>
  <li><a href="https://arxiv.org/abs/1801.09797">Discrete Autoencoders for Sequence Models</a></li>
  <li><a href="https://arxiv.org/abs/1801.10198">Generating Wikipedia by Summarizing Long Sequences</a></li>
  <li><a href="https://arxiv.org/abs/1802.05751">Image Transformer</a></li>
  <li><a href="https://arxiv.org/abs/1804.00247">Training Tips for the Transformer Model</a></li>
  <li><a href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a></li>
  <li><a href="https://arxiv.org/abs/1803.03382">Fast Decoding in Sequence Models using Discrete Latent Variables</a></li>
  <li><a href="https://arxiv.org/abs/1804.04235">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</a></li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to <a href="https://twitter.com/ilblackdragon">Illia Polosukhin</a>, <a href="http://jakob.uszkoreit.net/">Jakob Uszkoreit</a>, <a href="https://www.linkedin.com/in/llion-jones-9ab3064b">Llion Jones </a>, <a href="https://ai.google/research/people/LukaszKaiser">Lukasz Kaiser</a>, <a href="https://twitter.com/nikiparmar09">Niki Parmar</a>, and <a href="https://dblp.org/pers/hd/s/Shazeer:Noam">Noam Shazeer</a> for providing feedback on earlier versions of this post.</p>

<p>Please hit me up on <a href="https://twitter.com/JayAlammar">Twitter</a> for any corrections or feedback.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Engine Sound Simulator (157 pts)]]></title>
            <link>https://markeasting.github.io/engine/</link>
            <guid>40861079</guid>
            <pubDate>Tue, 02 Jul 2024 22:32:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://markeasting.github.io/engine/">https://markeasting.github.io/engine/</a>, See on <a href="https://news.ycombinator.com/item?id=40861079">Hacker News</a></p>
<div id="readability-page-1" class="page">

    <h2>Engine sound simulator</h2>

    <p><i>Please click once to enable sounds :)</i></p>

    <h2>Controls:</h2>
    
    <p>Space: LETS GOOO</p>
    <p>Arrow up/down: change gear</p>
    <p>Numbers: change gear</p>
    <p>B: slow down</p>

    
  


</div>]]></description>
        </item>
    </channel>
</rss>