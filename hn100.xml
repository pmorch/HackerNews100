<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 01 Mar 2024 12:00:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Company forgets why they exist after 11-week migration to Kubernetes (230 pts)]]></title>
            <link>https://www.theolognion.com/p/company-forgets-why-they-exist-after-11-week-migration-to-kubernetes</link>
            <guid>39560033</guid>
            <pubDate>Fri, 01 Mar 2024 09:08:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theolognion.com/p/company-forgets-why-they-exist-after-11-week-migration-to-kubernetes">https://www.theolognion.com/p/company-forgets-why-they-exist-after-11-week-migration-to-kubernetes</a>, See on <a href="https://news.ycombinator.com/item?id=39560033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_120,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dbb5d03-ad09-408b-8ccd-c63d3c1a8521_1000x1000.jpeg"><img src="https://substackcdn.com/image/fetch/w_120,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dbb5d03-ad09-408b-8ccd-c63d3c1a8521_1000x1000.jpeg" sizes="100vw" alt="" loading="lazy" width="120"></picture></div><div><h4>The Olognion</h4><p>www.theolognion.com</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CACM Is Now Open Access (131 pts)]]></title>
            <link>https://cacm.acm.org/news/cacm-is-now-open-access-2/</link>
            <guid>39559411</guid>
            <pubDate>Fri, 01 Mar 2024 07:19:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/news/cacm-is-now-open-access-2/">https://cacm.acm.org/news/cacm-is-now-open-access-2/</a>, See on <a href="https://news.ycombinator.com/item?id=39559411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
			<main id="main">

				
<article id="post-751671">

			
<header>
	<div>
		
		
		
					<p>More than six decades of <em>CACM</em>’s renowned research articles, seminal papers, technical reports, commentaries, real-world practice, and news articles are now open to everyone, regardless of whether they are members of ACM or subscribe to the ACM Digital Library.</p>
		
		

					<figure>
									<p><img width="1024" height="576" src="https://cacm.acm.org/wp-content/uploads/2023/05/091523.Opinion.CACM-Is-Open.jpg" alt="Communications of the ACM logo" loading="eager" decoding="async" srcset="https://cacm.acm.org/wp-content/uploads/2023/05/091523.Opinion.CACM-Is-Open.jpg 2400w, https://cacm.acm.org/wp-content/uploads/2023/05/091523.Opinion.CACM-Is-Open.jpg?resize=300,169 300w, https://cacm.acm.org/wp-content/uploads/2023/05/091523.Opinion.CACM-Is-Open.jpg?resize=768,432 768w, https://cacm.acm.org/wp-content/uploads/2023/05/091523.Opinion.CACM-Is-Open.jpg?resize=1024,576 1024w, https://cacm.acm.org/wp-content/uploads/2023/05/091523.Opinion.CACM-Is-Open.jpg?resize=1536,864 1536w, https://cacm.acm.org/wp-content/uploads/2023/05/091523.Opinion.CACM-Is-Open.jpg?resize=2048,1152 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></p>												</figure>
		
		
	</div>
</header>


		<div>
		<p>We are excited to announce that <em>Communications of the ACM</em> (<em>CACM</em>) is now a fully <strong>Open Access publication</strong>. This means that more than six decades of <em>CACM</em>’s renowned research articles, seminal papers, technical reports, commentaries, real-world practice, and news articles are now open to everyone, regardless of whether they are members of ACM or subscribe to the ACM Digital Library.</p>

<p>But why this change, and why now? For almost 65 years, the contents of <em>CACM</em> have been exclusively accessible to ACM members and individuals affiliated with institutions that subscribe to either <em>CACM</em> or the ACM Digital Library. In 2020, ACM announced its intention to transition to a fully Open Access publisher within a roughly five-year timeframe (January 2026) under a financially sustainable model. The transition is going well: By the end of 2023, approximately 40% of the ~26,000 articles ACM publishes annually were being published Open Access utilizing the <a href="https://libraries.acm.org/subscriptions-access/acmopen#model">ACM Open</a> model. As ACM has progressed toward this goal, it has increasingly opened large parts of the ACM Digital Library, including more than 100,000 articles published between 1951–2000. It is ACM’s plan to open its entire archive of over 600,000 articles when the transition to full Open Access is complete.</p>

<p>As part of this transition and to coincide with the launch of <em>CACM</em>‘s new website, all <em>CACM</em> articles, past, present, and future, will be published in front of the subscription paywall.&nbsp;</p>

<p>By opening <em>CACM</em> to the world, ACM hopes to increase engagement with the broader computer science community and encourage non-members to discover its rich resources and the benefits of joining the largest professional computer science organization. This move will also benefit CACM authors by expanding their readership to a larger and more diverse audience. Of course, the community’s continued support of ACM through membership and the ACM Open model is essential to keeping ACM and <em>CACM</em> strong, so it is critical that current members continue their membership and authors encourage their institutions to join the ACM Open model to keep this effort sustainable.&nbsp;</p>

<p>We invite everyone to explore CACM’s vast collection of articles, columns, and news items on the new website. Thank you for your interest in ACM and <em>CACM</em>!</p>

</div>
		
		<div>
			<div>
			<p>
				Submit an Article to CACM			</p>
			<p>
				CACM welcomes unsolicited <a href="https://cacm.acm.org/submissions">submissions</a> on topics of relevance and value to the computing community.			</p>
		</div>

<div>
		<p>
			You Just Read		</p>
		<h4>
			CACM Is Now Open Access		</h4>
			</div>
		</div>
		<div>
		<h3>Related Reading</h3>
		<!-- Related reading post list -->
		<ul>
							<li>
					<p>
						<a href="https://cacm.acm.org/section/opinion/">Opinion</a>					</p>
					<p>
						<a href="https://cacm.acm.org/opinion/dja-vu-all-over-again/">
							Déja Vu All Over Again						</a>
					</p>
					<p>
						<a href="https://cacm.acm.org/category/computing-applications/">Computing Applications</a>					</p>
				</li>
							<li>
					<p>
						<a href="https://cacm.acm.org/section/blogcacm/">BLOG@CACM</a>					</p>
					<p>
						<a href="https://cacm.acm.org/blogcacm/the-chaos-of-the-internet-as-an-external-brain-and-more/">
							The Chaos of the Internet as an External Brain; and More						</a>
					</p>
					<p>
						<a href="https://cacm.acm.org/category/artificial-intelligence-machine-learning/">Artificial Intelligence and Machine Learning</a>					</p>
				</li>
							<li>
					<p>
						<a href="https://cacm.acm.org/section/opinion/">Opinion</a>					</p>
					<p>
						<a href="https://cacm.acm.org/opinion/looking-for-control/">
							Looking For Control						</a>
					</p>
					<p>
						<a href="https://cacm.acm.org/category/computing-applications/">Computing Applications</a>					</p>
				</li>
							<li>
					<p>
						<a href="https://cacm.acm.org/section/opinion/">Opinion</a>					</p>
					<p>
						<a href="https://cacm.acm.org/opinion/let-us-together-make-cacm-exciting/">
							Let Us – Together – Make CACM Exciting						</a>
					</p>
					<p>
						<a href="https://cacm.acm.org/category/computing-applications/">Computing Applications</a>					</p>
				</li>
					</ul>
	</div>
		
<div>
		<h3>
			Join the Discussion (0)		</h3>
		
<div id="article-discussion">
		<h4>Become a Member or Sign In to Post a Comment</h4>
		
	</div>
		
<!-- #comments -->
	</div>

		

		
<div data-component="ctaMembership">
	
		<div>
				<h3>
					Shape the Future of Computing				</h3>
									<p>
						ACM encourages its members to take a direct hand in shaping the future of the association. There are more ways than ever to get involved.					</p>
													<p><a href="https://www.acm.org/about-acm/get-involved">
						Get Involved											</a>
							</p></div>

		
		<div>
				<h3>
					Communications of the ACM (CACM) is now a fully Open Access publication.				</h3>
									<p>
						By opening CACM to the world, we hope to increase engagement among the broader computer science community and encourage non-members to discover the rich resources ACM has to offer.					</p>
													<p><a href="https://cacm.acm.org/news/cacm-is-becoming-open-access">
						Learn More											</a>
							</p></div>

		</div>

	
</article><!-- #post-## -->

			</main>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You can't make an open source HDMI 2.1 driver (165 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2024/02/hdmi-forum-to-amd-no-you-cant-make-an-open-source-hdmi-2-1-driver/</link>
            <guid>39559318</guid>
            <pubDate>Fri, 01 Mar 2024 07:02:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2024/02/hdmi-forum-to-amd-no-you-cant-make-an-open-source-hdmi-2-1-driver/">https://arstechnica.com/gadgets/2024/02/hdmi-forum-to-amd-no-you-cant-make-an-open-source-hdmi-2-1-driver/</a>, See on <a href="https://news.ycombinator.com/item?id=39559318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      The cord cuts back    —
</h4>
            
            <h2 itemprop="description">Linux users can't hit the same resolutions and speeds as Windows—or DisplayPort.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-1289090180-800x600.jpg" alt="HDMI cables, bundled up and covered in some dust">
      <figcaption><p>Getty Images</p></figcaption>  </figure>

  




<!-- cache hit 9:single/related:4e140d79418f9d0c58cedca2d3f828f6 --><!-- empty -->
<p>Any Linux user trying to send the highest-resolution images to a display at the fastest frame rate is out of luck for the foreseeable future, at least when it comes to an HDMI connection.</p>
<p>The licensing group that controls the HDMI standard, the <a href="https://hdmiforum.org/">HDMI Forum</a>, has reportedly told AMD that it does not allow an open source implementation of the <a href="https://arstechnica.com/gadgets/2017/11/hdmi-2-1-spec-released-ushering-in-new-era-of-dynamic-hdr-video/">HDMI 2.1</a> (or HDMI 2.1+) specification, blocking tools such as AMD's FreeSync from working over HDMI connections at resolution/rate combinations like 4K at 120 Hz, or 5K at 240 Hz.</p>
<p>Linux blog Phoronix <a href="https://www.phoronix.com/news/HDMI-Closed-Spec-Hurts-Open">noted in January 2021</a> that the HDMI Forum did not offer public access to the HDMI 2.1 specification. Alex Deucher, an AMD engineer who has long contributed to the company's open source offerings, has kept <a href="https://gitlab.freedesktop.org/drm/amd/-/issues/1417">a related bug thread</a> alive for <a href="https://gitlab.freedesktop.org/drm/amd/-/issues/1417#note_830547">at least two years</a>, only to deliver the negative outcome yesterday.</p>
<p>In February 2023, Deucher <a href="https://gitlab.freedesktop.org/drm/amd/-/issues/1417#note_1795980">reported</a> that he was "working with our [AMD] legal team to sort out what we can deliver while still complying with our obligations to HDMI Forum." Two months <a href="https://gitlab.freedesktop.org/drm/amd/-/issues/1417#note_1876855">later</a>, he said that AMD got "the basic functionality up and running, now we have to go through each of the features with legal and determine if/how we can expose them while still meeting our obligations." Summer and fall of 2023 went by, with legal review <a href="https://gitlab.freedesktop.org/drm/amd/-/issues/1417#note_2100060">still underway</a>, and in <a href="https://gitlab.freedesktop.org/drm/amd/-/issues/1417#note_2144689">October</a>, the decision was "in the hands of the HDMI Forum."</p>                                            
                                                        
<p>On Wednesday afternoon, Deucher offered the current resolution:</p>
<blockquote><p>The HDMI Forum has rejected our proposal unfortunately. At this time an open source HDMI 2.1 implementation is not possible without running afoul of the HDMI Forum requirements.</p></blockquote>
<p>Ars has reached out to the HDMI Forum, AMD, and Deucher for further comment and will update the post with new information. X.org was also reportedly involved in negotiations with the HDMI Forum.</p>
<p>Membership in the HDMI Forum is a minimum of $15,000. While AMD is <a href="https://hdmiforum.org/members/">a listed member</a>, that likely doesn't extend to offering up an implementation of a specification for public use. The member agreement forbidding such things does not appear to be publicly available, nor does an "<a href="http://www.hdmi.org/manufacturer/adopter_registration.aspx">addendum</a>" for members linked from the Forum's site. A <a href="https://hdmiforum.org/hdmi-forum-adopter-source-code-license/">source code license</a> found on the Forum's site does not appear to be particularly flexible.</p>
<p><a href="https://www.phoronix.com/news/HDMI-2.1-OSS-Rejected">Phoronix</a> and some commenters have suggested potential interference from media firms concerned about digital video ripping. That would seem like a barn door closed years after the horse's departure, but it also exists as one explanation, lacking other detail.</p>
<p>This outcome leaves DisplayPort as the likely best option for Linux users needing the best possible output. It also suggests that AMD has to decide whether to implement newer HDMI support inside closed-source Linux drivers or simply point its most demanding customers to other options.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JPEG XL and the Pareto Front (194 pts)]]></title>
            <link>https://cloudinary.com/blog/jpeg-xl-and-the-pareto-front</link>
            <guid>39559281</guid>
            <pubDate>Fri, 01 Mar 2024 06:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloudinary.com/blog/jpeg-xl-and-the-pareto-front">https://cloudinary.com/blog/jpeg-xl-and-the-pareto-front</a>, See on <a href="https://news.ycombinator.com/item?id=39559281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
<p>Version 0.10 of libjxl, the reference implementation for JPEG XL, has just been <a href="https://github.com/libjxl/libjxl/releases">released</a>. The main improvement this version brings, is that the so-called “streaming encoding” API has now been fully implemented. This API allows encoding a large image in “chunks.” Instead of processing the entire image at once, which may require a significant amount of RAM if the image is large, the image can now be processed in a more memory-friendly way. As a side effect, encoding also becomes faster, in particular when doing lossless compression of larger images.</p>








<p>Before version 0.10, lossless JPEG XL encoding was rather memory-intensive and could take quite some time. This could pose a serious problem when trying to encode large images, like for example <a href="https://earthobservatory.nasa.gov/features/NightLights#:~:text=Marble%20imagery.-,Earth%20at%20Night%3A%20Flat%20Maps,-Global%20Map%20Downloads">this 13500×6750 NASA image of Earth at night</a> (64 MB as a <a href="https://eoimages.gsfc.nasa.gov/images/imagerecords/144000/144898/BlackMarble_2016_3km_geo.tif">TIFF file</a>, 273 MB uncompressed).</p>



<figure><img decoding="async" src="https://res.cloudinary.com/jon/image/fetch/w_1000,f_auto,q_auto/https://eoimages.gsfc.nasa.gov/images/imagerecords/144000/144898/BlackMarble_2016_01deg.jpg" alt="A 13500x6750 NASA image of Earth at night"></figure>



<p>Compressing this image required about 8 gigabytes of RAM in libjxl version 0.9, at the default effort (e7). It took over two minutes, and resulted in a jxl file of 33.7 MB, which is just under 3 bits per pixel. Using more threads did not help much: using a single thread it took 2m40s, using eight threads that was reduced to 2m06s. These timings were measured on a November 2023 Macbook Pro with a 12-core Apple M3 Pro CPU with 36 GB of RAM.</p>



<p>Upgrading to libjxl version 0.10, compressing this same image now requires only 0.7 gigabytes of RAM, takes 30 seconds using a single thread (or 5 seconds using eight threads), and results in a jxl file of 33.2 MB.</p>



<p>For other effort settings, these are the results for this particular image:</p>



<figure><table><tbody><tr><td>effort setting</td><td>memory 0.9.2</td><td>memory 0.10</td><td>time<br>0.9</td><td>time 0.10</td><td>compressed size 0.9</td><td>compressed size 0.10</td><td>memory reduction</td><td>speedup</td></tr><tr><td>e1, 1 thread</td><td>821 MB</td><td>289 MB</td><td>0.65s</td><td>0.3s</td><td>65.26 MB</td><td>67.03 MB</td><td>2.8x</td><td>2.2x</td></tr><tr><td>e1, 8 threads</td><td>842 MB</td><td>284 MB</td><td>0.21s</td><td>0.1s</td><td>65.26 MB</td><td>67.03 MB</td><td>2.9x</td><td>2.1x</td></tr><tr><td>e2, 1 thread</td><td>7,503 MB</td><td>786 MB</td><td>4.3s</td><td>3.6s</td><td>49.98 MB</td><td>44.78 MB</td><td>9.5x</td><td>1.2x</td></tr><tr><td>e2, 8 threads</td><td>6,657 MB</td><td>658 MB</td><td>2.2s</td><td>0.7s</td><td>49.98 MB</td><td>44.78 MB</td><td>10.1x</td><td>3.0x</td></tr><tr><td>e3, 8 threads</td><td>7,452 MB</td><td>708 MB</td><td>2.4s</td><td>1.3s</td><td>45.20 MB</td><td>44.23 MB</td><td>10.5x</td><td>1.8x</td></tr><tr><td>e7, 1 thread</td><td>9,361 MB</td><td>748 MB</td><td>2m40s</td><td>30s</td><td>33.77 MB</td><td>33.22 MB</td><td>12.5x</td><td>4.6x</td></tr><tr><td>e7, 8 threads</td><td>7,887 MB</td><td>648 MB</td><td>2m06s</td><td>5.4s</td><td>33.77 MB</td><td>33.22 MB</td><td>12.2x</td><td>23.6x</td></tr><tr><td>e8, 8 threads</td><td>9,288 MB</td><td>789 MB</td><td>7m38s</td><td>22.2s</td><td>32.98 MB</td><td>32.93 MB</td><td>11.8x</td><td>20.6x</td></tr><tr><td>e9, 8 threads</td><td>9,438 MB</td><td>858 MB</td><td>21m58s</td><td>1m46s</td><td>32.45 MB</td><td>32.20 MB</td><td>11.0x</td><td>12.4x</td></tr></tbody></table></figure>



<p>As you can see in the table above, compression is a game of diminishing returns: as you increase the amount of cpu time spent on the encoding, the compression improves, but not in a linear fashion. Spending one second instead of a tenth of a second (e2 instead of e1) can in this case shave off 22 megabytes; spending five seconds instead of one (e7 instead of e2) shaves off another 11 megabytes. But to shave off one more megabyte, you’ll have to wait almost two minutes (e9 instead of e7).</p>



<p>So it’s very much a matter of trade-offs, and it depends on the use case what makes the most sense. In an authoring workflow, when you’re saving an image locally while still editing it, you typically don’t need strong compression and low-effort encoding makes sense. But in a one-to-many delivery scenario, or for long-term archival, it may well be worth it to spend a significant amount of CPU time to shave off some more megabytes.</p>








<p>When comparing different compression techniques, it doesn’t suffice to only look at the compressed file sizes. The speed of encoding also matters. So there are two dimensions to consider: compression density and encode speed.</p>



<p>A specific method can be called Pareto-optimal if no other method can achieve the same (or better) compression density in less time. There might be other methods that compress better but take more time, or that compress faster but result in larger files. But a Pareto-optimal method delivers the smallest files for a given time budget, which is why it’s called “optimal.”</p>



<p>The set of Pareto-optimal methods is called the “<a href="https://en.wikipedia.org/wiki/Pareto_front">Pareto front</a>.” It can be visualized by putting the different methods on a chart that shows both dimensions — encode speed and compression density. Instead of looking at a single image, which may not be representative, we look at a set of images and look at the average speed and compression density for each encoder and effort setting. For example, for <a href="https://imagecompression.info/test_images/">this set of test images</a>, the chart looks like this:</p>



<figure><img width="1846" height="1596" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1846,h_1596/f_auto,q_auto/v1709058557/Web_Assets/blog/blog-pareto-front-1/blog-pareto-front-1-png?_i=AA" alt="" data-public-id="Web_Assets/blog/blog-pareto-front-1.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1709058557" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058557/Web_Assets/blog/blog-pareto-front-1/blog-pareto-front-1-png?_i=AA 1846w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058557/Web_Assets/blog/blog-pareto-front-1/blog-pareto-front-1-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058557/Web_Assets/blog/blog-pareto-front-1/blog-pareto-front-1-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058557/Web_Assets/blog/blog-pareto-front-1/blog-pareto-front-1-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058557/Web_Assets/blog/blog-pareto-front-1/blog-pareto-front-1-png?_i=AA 1536w" sizes="(max-width: 1846px) 100vw, 1846px"></figure>



<p>The vertical axis shows the encode speed, in megapixels per second. It’s a logarithmic scale since it has to cover a broad range of speeds, from less than one megapixel per second to hundreds of megapixels per second. The horizontal axis shows the average bits per pixel for the compressed image (uncompressed 8-bit RGB is 24 bits per pixel).</p>


<div><p><strong>TL;DR</strong></p><p>Higher means faster, more to the left means better compression.</p>
</div>


<p>For AVIF, the darker points indicate a faster but slightly less dense tiled encoder setting (using –tilerowslog2 2 –tilecolslog2 2), which is faster because it can make better use of multi-threading, while the lighter points indicate the default non-tiled setting. For PNG, the result of libpng with default settings is shown here as a reference point; other PNG encoders and optimizers exist that reach different trade-offs.</p>



<p>The previous version of libjxl already achieved Pareto-optimal results across all speeds, producing smaller files than PNG and lossless AVIF or lossless WebP. The new version beats the previous version by a significant margin.</p>



<p>Not shown on the chart is <a href="https://github.com/phoboslab/qoi">QOI</a>, which clocked in at 154 Mpx/s to achieve 17 bpp, which may be “quite OK” but is quite far from Pareto-optimal, considering the lowest effort setting of libjxl compresses down to 11.5 bpp at 427 Mpx/s (so it is 2.7 times as fast and the result is 32.5% smaller).</p>








<p>Of course in these charts, quite a lot depends on the selection of test images. In the chart above, most images are photographs, which tend to be hard to compress losslessly: the naturally occurring noise in such images is inherently incompressible.</p>



<p>For non-photographic images, things are somewhat different. I took a random collection of manga images in various drawing styles (41 images with an average size of 7.3 megapixels) and these were the results:</p>



<p>These kinds of images compress significantly better, to around 4 bpp (compared to around 10 bpp for photographic images). For these images, lossless AVIF is not useful — it compresses worse than PNG, and reaches about the same density as QOI but is much slower. Lossless WebP on the other hand achieves very good compression for such images. For these types of images, QOI is indeed quite OK for its speed (and simplicity), though far from Pareto-optimal: low-effort JPEG XL encoding is twice as fast and 31% smaller.</p>



<figure><img width="1860" height="1594" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1860,h_1594/f_auto,q_auto/v1709058548/Web_Assets/blog/blog-pareto-front-2/blog-pareto-front-2-png?_i=AA" alt="" data-public-id="Web_Assets/blog/blog-pareto-front-2.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1709058548" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058548/Web_Assets/blog/blog-pareto-front-2/blog-pareto-front-2-png?_i=AA 1860w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058548/Web_Assets/blog/blog-pareto-front-2/blog-pareto-front-2-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058548/Web_Assets/blog/blog-pareto-front-2/blog-pareto-front-2-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058548/Web_Assets/blog/blog-pareto-front-2/blog-pareto-front-2-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058548/Web_Assets/blog/blog-pareto-front-2/blog-pareto-front-2-png?_i=AA 1536w" sizes="(max-width: 1860px) 100vw, 1860px"></figure>



<p>For non-photographic images, the new version of libjxl again improves upon the previous version, by a significant margin. The previous version of libjxl could just barely beat WebP: e.g. default-effort WebP compressed these images to 4.30 bpp at 2.3 Mpx/s, while libjxl 0.9 at effort 5 compressed them to 4.27 bpp at 2.6 Mpx/s — only a slight improvement. However libjxl 0.10 at effort 5 compresses the images to 4.25 bpp at 12.2 Mpx/s (slightly better compression but much faster), and at effort 7 it compresses them to 4.04 bpp at 5.9 Mpx/s (significantly better compression and still twice as fast). Zooming in on the medium-speed part of the Pareto front on the above plot, the improvement going from libjxl 0.9 to 0.10 becomes clear:</p>



<figure><img width="1830" height="1578" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1830,h_1578/f_auto,q_auto/v1709058543/Web_Assets/blog/blog-pareto-front-3/blog-pareto-front-3-png?_i=AA" alt="" data-public-id="Web_Assets/blog/blog-pareto-front-3.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1709058543" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058543/Web_Assets/blog/blog-pareto-front-3/blog-pareto-front-3-png?_i=AA 1830w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058543/Web_Assets/blog/blog-pareto-front-3/blog-pareto-front-3-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058543/Web_Assets/blog/blog-pareto-front-3/blog-pareto-front-3-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058543/Web_Assets/blog/blog-pareto-front-3/blog-pareto-front-3-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058543/Web_Assets/blog/blog-pareto-front-3/blog-pareto-front-3-png?_i=AA 1536w" sizes="(max-width: 1830px) 100vw, 1830px"></figure>








<p>Lossless compression is relatively easy to benchmark: all that matters is the compressed size and the speed. For lossy compression, there is a third dimension: image quality.</p>



<p>Lossy image codecs and encoders can perform differently at different quality points. An encoder that works very well for high-quality encoding does not necessarily also perform well for low-quality encoding, and the other way around.</p>



<p>Of these three dimensions (compression, speed and quality), often speed is simply ignored, and plots are made of compression versus quality (also known as bitrate-distortion plots). But this does not really allow evaluating the trade-offs between encode effort (speed) and compression performance. So if we really want to investigate the Pareto front for lossy compression, one way of doing it is to look at different “slices” of the three-dimensional space, at various quality points.</p>








<p>Image quality is a notoriously difficult thing to measure: in the end, it is subjective and somewhat different from one human to the next. The best way to measure image quality is still to run an experiment involving at least dozens of humans looking carefully at images and comparing or scoring them, according to rigorously defined test protocols. At Cloudinary, we have <a href="https://cloudinary.com/labs/cid22">done such experiments</a> in the past. But while this is the best way to assess image quality, it is a time-consuming and costly process, and it is not feasible to test all possible encoder configurations in this way.</p>



<p>For that reason, so-called objective metrics are being developed, which allow algorithmic estimates of image quality. These metrics are not “more objective” (in the sense of “more correct”) than scores obtained from testing with humans, in fact they are <em>less</em> “correct.” But they can give an indication of image quality much faster and cheaper (and more easily reproducible and consistent) than when humans are involved, which is what makes them useful.</p>



<p>The best metrics currently publicly available are <a href="https://github.com/cloudinary/ssimulacra2">SSIMULACRA2</a>, <a href="https://github.com/google/butteraugli">Butteraugli</a>, and <a href="https://github.com/kornelski/dssim">DSSIM</a>. These metrics try to model the human visual system and have the <a href="https://cloudinary.com/labs/cid22#:~:text=(SSIMULACRA%202),%2D0.7813">best correlation with subjective results</a>. Older, simpler metrics like PSNR or SSIM could also be used, but they do not correlate very well with human opinions about image quality. Care has to be taken not to measure results using a metric an encoder is specifically optimizing for, as that would skew the results in favor of such encoders. For example, higher-effort libjxl optimizes for Butteraugli, while libavif can optimize for PSNR or SSIM. In this respect, SSIMULACRA2 is “safe” since none of the encoders tested is using it internally for optimization.</p>








<p>Different metrics will say different things, but there are also different ways to aggregate results across a set of images. To keep things simple, I selected <a href="https://gist.github.com/jonsneyers/d317f51b4805c1a8f3ce0e86a9bce100">encoder settings</a> such that when using each setting on all images in the set, the average SSIMULACRA2 score was equal to (or close to) a specific value. Another method would have been to adjust the encoder settings per image so for each image the SSIMULACRA2 score is the same, or to select an encoder setting such that the <em>worst-case</em> SSIMULACRA2 score is equal to a specific value.<br>Aligning on worst-case scores is favorable for consistent encoders (encoders that reliably produce the same visual quality given fixed quality settings), while aligning on average scores is favorable for inconsistent encoders (encoders where there is more variation in visual quality when using fixed quality settings). From <a href="https://cloudinary-marketing-res.cloudinary.com/image/upload/v1682016636/wg1m99012-ICQ-AIC3_Contribution_Cloudinary_CID22.pdf">earlier research</a>, we know that AVIF and WebP are more inconsistent than JPEG and HEIC, and that JPEG XL has the most consistent encoder:</p>



<figure><img width="1182" height="1536" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1182,h_1536/f_auto,q_auto/v1709058535/Web_Assets/blog/blog-pareto-front-4/blog-pareto-front-4-png?_i=AA" alt="" data-public-id="Web_Assets/blog/blog-pareto-front-4.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1709058535" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058535/Web_Assets/blog/blog-pareto-front-4/blog-pareto-front-4-png?_i=AA 1182w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058535/Web_Assets/blog/blog-pareto-front-4/blog-pareto-front-4-png?_i=AA 231w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058535/Web_Assets/blog/blog-pareto-front-4/blog-pareto-front-4-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058535/Web_Assets/blog/blog-pareto-front-4/blog-pareto-front-4-png?_i=AA 788w" sizes="(max-width: 1182px) 100vw, 1182px"></figure>



<p>Defining the quality points the way I did (using fixed settings and aligning by average metric score) is in the favor of WebP and AVIF; in practical usage you will likely want to align on worst-case metric score (or rather, worst-case <em>actual visual quality</em>), but I chose not to do that, in order to not favor JPEG XL.</p>








<p>Lossless compression offers 2:1 to 3:1 compression ratios (8 to 12 bpp) for photographic images. Lossy compression can reach much better compression ratios. It is tempting to see how lossy image codecs behave when they are pushed to their limits. Compression ratios of 50:1 or even 200:1 (0.1 to 0.5 bpp) can be obtained, at the cost of introducing compression artifacts. Here is an example of an image compressed to reach a SSIMULACRA2 score of 50, 30, and 10 using libjpeg-turbo, libjxl and libavif:</p>



<figure><a href="https://res.cloudinary.com/jon/qp-low.png" target="_blank" rel="noreferrer noopener"><img decoding="async" src="https://cloudinary-marketing-res.cloudinary.com/image/upload/v1709060139/qp-low.png" alt=""></a></figure>


<div><p><strong>Note:</strong></p><p>Click on the animation to open it in another tab; view it full-size to properly see the artifacts.</p>
</div>


<p>This kind of quality is interesting to look at in experiments, but in most actual usage, it is not desirable to introduce such noticeable compression artifacts. In practice, the range of qualities that is relevant corresponds to SSIMULACRA2 scores ranging from 60 (medium quality) to 90 (visually lossless). These qualities look like this:</p>



<figure><a href="https://res.cloudinary.com/jon/qp.png" target="_blank" rel="noreferrer noopener"><img decoding="async" src="https://cloudinary-marketing-res.cloudinary.com/image/upload/v1709060138/qp.png" alt=""></a></figure>



<p>Visually lossless quality (SSIMULACRA2 = 90) can be reached with a compression ratio of about 8:1 (3 bpp) with modern codecs such as AVIF and JPEG XL, or about 6:1 (4 bpp) with JPEG. At this point, the image is visually not distinguishable from the uncompressed original, even when looking very carefully. In cameras, when not shooting RAW, typically this is the kind of quality that is desired. For web delivery, it is overkill to use such a high quality.</p>



<p>High quality (SSIMULACRA2 = 80) can be reached with a compression ratio of 16:1 (1.5 bpp). When looking carefully, very small differences might be visible, but essentially the image is still as good as the original. This, or perhaps something in between high quality and visually lossless quality, is the highest quality useful for web delivery, for use cases where image fidelity really matters.</p>



<p>Medium-high quality (SSIMULACRA2 = 70) can be reached with a compression ratio of 30:1 (0.8 bpp). There are some small artifacts, but the image still looks good. This is a good target for most web delivery use cases, as it makes a good trade-off between fidelity and bandwidth optimization.</p>



<p>Medium quality (SSIMULACRA2 = 60) can be reached with a compression ratio of 40:1 (0.6 bpp). Compression artifacts start to become more noticeable, but they’re not problematic for casual viewing. For non-critical images on the web, this quality can be “good enough.”</p>



<p>Any quality lower than this is potentially risky: sure, bandwidth will be reduced by going even further, but at the cost of potentially ruining the images. For the web, in 2024, the relevant range is medium to high quality: <a href="https://almanac.httparchive.org/en/2022/media#bits-per-pixel-by-format">according to the HTTP Archive</a>, the median AVIF image on the web is compressed to 1 bpp, which corresponds to medium-high quality, while the median JPEG image is 2.1 bpp, which corresponds to high quality. For most non-web use cases (e.g. cameras), the relevant range is high to (visually) lossless quality.</p>








<p>In the following Pareto front plots, the following encoders were tested:</p>



<figure><table><tbody><tr><td>format</td><td>encoder</td><td>version</td></tr><tr><td>JPEG</td><td>libjpeg-turbo</td><td>libjpeg-turbo 2.1.5.1</td></tr><tr><td>JPEG</td><td>sjpeg</td><td>sjpeg @ e5ab130</td></tr><tr><td>JPEG</td><td>mozjpeg</td><td>mozjpeg version 4.1.5 (build 20240220)</td></tr><tr><td>JPEG</td><td>jpegli</td><td>from libjxl v0.10.0</td></tr><tr><td>AVIF</td><td>libavif / libaom</td><td>libavif 1.0.3 (aom [enc/dec]:3.8.1)</td></tr><tr><td>JPEG XL</td><td>libjxl</td><td>libjxl v0.10.0</td></tr><tr><td>WebP</td><td>libwebp</td><td>libwebp 1.3.2</td></tr><tr><td>HEIC</td><td>libheif</td><td>heif-enc libheif version: 1.17.6 (x265 3.5)</td></tr></tbody></table></figure>



<p>These are the most recent versions of each encoder at the time of writing (end of February 2024).</p>



<p>Encode speed was again measured on a November 2023 Macbook Pro (Apple M3 Pro), using 8 threads. For AVIF, both the tiled setting (with –tilerowslog2 2 –tilecolslog2 2) and the non-tiled settings were tested. The tiled setting, indicated with “MT”, is faster since it allows better multi-threading, but it comes at a cost in compression density.</p>








<p>Let’s start by looking at the results for medium quality, i.e., settings that result in a corpus average SSIMULACRA2 score of 60. This is more or less the lowest quality point that is used in practice. Some images will have visible compression artifacts with these encoder settings, so this quality point is most relevant when saving bandwidth and reducing page weight is more important than image fidelity.</p>



<figure><img width="1942" height="1620" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1942,h_1620/f_auto,q_auto/v1709058526/Web_Assets/blog/blog-pareto-front-5/blog-pareto-front-5-png?_i=AA" alt="" data-public-id="Web_Assets/blog/blog-pareto-front-5.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1709058526" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058526/Web_Assets/blog/blog-pareto-front-5/blog-pareto-front-5-png?_i=AA 1942w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058526/Web_Assets/blog/blog-pareto-front-5/blog-pareto-front-5-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058526/Web_Assets/blog/blog-pareto-front-5/blog-pareto-front-5-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058526/Web_Assets/blog/blog-pareto-front-5/blog-pareto-front-5-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058526/Web_Assets/blog/blog-pareto-front-5/blog-pareto-front-5-png?_i=AA 1536w" sizes="(max-width: 1942px) 100vw, 1942px"></figure>



<p>First of all, note that even for the same format, different encoders and different effort settings can reach quite different results. Historically, the most commonly used JPEG encoder was libjpeg-turbo — often using its default setting (no Huffman optimization, not progressive), which is the point all the way in the top right. When Google first introduced WebP, it outperformed libjpeg-turbo in terms of compression density, as can be seen in the plot above. But Mozilla was not impressed, and they created their own JPEG encoder, mozjpeg, which is slower than libjpeg-turbo but offers better compression results. And indeed, we can see that mozjpeg is actually more Pareto-efficient than WebP (for this corpus, at this quality point).</p>



<p>More recently, the JPEG XL team at Google has built yet another JPEG encoder, jpegli, which is both faster and better than even mozjpeg. It is based on lessons learned from guetzli and libjxl, and offers a very attractive trade-off: it is very fast, compresses better than WebP and even high-speed AVIF, while still producing good old JPEG files that are supported everywhere.</p>



<p>Moving on to the newer codecs, we can see that both AVIF and HEIC can obtain a better compression density than JPEG and WebP, at the cost of slower encoding. JPEG XL can reach a similar compression density but encodes significantly faster. The current Pareto front for this quality point consists of JPEG XL and the various JPEG encoders for the “reasonable” speeds, and AVIF at the slower speeds (though the additional savings over default-effort JPEG XL are small).</p>








<p>At somewhat higher quality settings where the average SSIMULACRA2 score for the corpus is 70, the overall results look quite similar:</p>



<figure><img width="1999" height="1496" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1999,h_1496/f_auto,q_auto/v1709058518/Web_Assets/blog/blog-pareto-front-6/blog-pareto-front-6-png?_i=AA" alt="" data-public-id="Web_Assets/blog/blog-pareto-front-6.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1709058518" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058518/Web_Assets/blog/blog-pareto-front-6/blog-pareto-front-6-png?_i=AA 1999w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058518/Web_Assets/blog/blog-pareto-front-6/blog-pareto-front-6-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058518/Web_Assets/blog/blog-pareto-front-6/blog-pareto-front-6-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058518/Web_Assets/blog/blog-pareto-front-6/blog-pareto-front-6-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058518/Web_Assets/blog/blog-pareto-front-6/blog-pareto-front-6-png?_i=AA 1536w" sizes="(max-width: 1999px) 100vw, 1999px"></figure>








<p>Moving on to the highest quality point that is relevant for the web (corpus average SSIMULACRA2 score of 85, to ensure that most images reach a score above 80), the differences become a little more pronounced.</p>



<figure><img width="1999" height="1612" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1999,h_1612/f_auto,q_auto/v1709058510/Web_Assets/blog/blog-pareto-front-7/blog-pareto-front-7-png?_i=AA" alt="" data-public-id="Web_Assets/blog/blog-pareto-front-7.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1709058510" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058510/Web_Assets/blog/blog-pareto-front-7/blog-pareto-front-7-png?_i=AA 1999w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058510/Web_Assets/blog/blog-pareto-front-7/blog-pareto-front-7-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058510/Web_Assets/blog/blog-pareto-front-7/blog-pareto-front-7-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058510/Web_Assets/blog/blog-pareto-front-7/blog-pareto-front-7-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058510/Web_Assets/blog/blog-pareto-front-7/blog-pareto-front-7-png?_i=AA 1536w" sizes="(max-width: 1999px) 100vw, 1999px"></figure>



<p>At this point, mozjpeg no longer beats WebP, though jpegli still does. The Pareto front is now mostly covered by JPEG XL, though for very fast encoding, good old JPEG is still best. At this quality point, AVIF is not on the Pareto front: at its slowest settings (at 0.5 Mpx/s or slower) it matches the compression density of the second-fastest libjxl setting, which is over 100 times as fast (52 Mpx/s).</p>








<p>So far, we have only looked at compression density and encode speed. Decode speed is not really a significant problem on modern computers, but it is interesting to take a quick look at the numbers. The table below shows the same results as the plot above, but besides bits per pixel and encode speed, it also shows the decode speed. For completeness, the SSIMULACRA2 and Butteraugli 3-norm scores are also given for each encoder setting.</p>


<div>
<figure><img width="846" height="1298" data-public-id="Web_Assets/blog/Screen-Shot-2024-02-29-at-3.35.25-PM.png" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_846,h_1298/f_auto,q_auto/v1709249796/Web_Assets/blog/Screen-Shot-2024-02-29-at-3.35.25-PM/Screen-Shot-2024-02-29-at-3-35-25-PM-png?_i=AA" alt="" data-format="png" data-transformations="f_auto,q_auto" data-version="1709249796" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709249796/Web_Assets/blog/Screen-Shot-2024-02-29-at-3.35.25-PM/Screen-Shot-2024-02-29-at-3-35-25-PM-png?_i=AA 846w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709249796/Web_Assets/blog/Screen-Shot-2024-02-29-at-3.35.25-PM/Screen-Shot-2024-02-29-at-3-35-25-PM-png?_i=AA 196w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709249796/Web_Assets/blog/Screen-Shot-2024-02-29-at-3.35.25-PM/Screen-Shot-2024-02-29-at-3-35-25-PM-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709249796/Web_Assets/blog/Screen-Shot-2024-02-29-at-3.35.25-PM/Screen-Shot-2024-02-29-at-3-35-25-PM-png?_i=AA 667w" sizes="(max-width: 846px) 100vw, 846px"></figure></div>


<p>Sequential JPEG is unbeatable in terms of decode speed — not surprising for a codec that was designed in the 1980s. Progressive JPEG (e.g. as produced by mozjpeg and default jpegli) is somewhat slower to decode, but still fast enough to load any reasonably-sized image in the blink of an eye. JPEG XL is somewhere in between those two.</p>



<p>Interestingly, the decode speed of AVIF depends on how the image was encoded: it is faster when using the faster-but-slightly-worse multi-tile encoding, slower when using the default single-tile encoding. Still, even the slowest decode speed measured here is probably “fast enough,” especially compared to the encode speeds.</p>








<p>Finally, let’s take a look at the results for visually lossless quality:</p>



<figure><img width="1999" height="1613" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1999,h_1613/f_auto,q_auto/v1709058502/Web_Assets/blog/blog-pareto-front-8/blog-pareto-front-8-png?_i=AA" alt="" data-public-id="Web_Assets/blog/blog-pareto-front-8.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1709058502" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058502/Web_Assets/blog/blog-pareto-front-8/blog-pareto-front-8-png?_i=AA 1999w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058502/Web_Assets/blog/blog-pareto-front-8/blog-pareto-front-8-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058502/Web_Assets/blog/blog-pareto-front-8/blog-pareto-front-8-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058502/Web_Assets/blog/blog-pareto-front-8/blog-pareto-front-8-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058502/Web_Assets/blog/blog-pareto-front-8/blog-pareto-front-8-png?_i=AA 1536w" sizes="(max-width: 1999px) 100vw, 1999px"></figure>



<p>WebP is not on this chart since it simply cannot reach this quality point, at least not using its lossy mode. This is because 4:2:0 chroma subsampling is obligatory in WebP. Also clearly mozjpeg was not designed for this quality point, and performs worse than libjpeg-turbo in both compression and speed.</p>



<p>At their default speed settings, libavif is 20% smaller than libjpeg-turbo (though it takes an order of magnitude longer to encode), while libjxl is 20% smaller than libavif and 2.5 times as fast, at this quality point. The Pareto front consists of mostly JPEG XL but at the fastests speeds again also includes JPEG.</p>








<p>In the plots above, the <a href="https://people.xiph.org/~xiphmont/demo/daala/update1-tool2b.shtml">test set</a> consisted of web-sized images of about 1 megapixel each. This is relevant for the web, but for example when storing camera pictures, images are larger than this.</p>



<p>For a test set with larger images (the <a href="https://imagecompression.info/test_images/">same set we used before</a> to test lossless compression), at a high quality point, we get the following results:</p>



<figure><img width="1999" height="1358" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1999,h_1358/f_auto,q_auto/v1709058494/Web_Assets/blog/blog-pareto-front-9/blog-pareto-front-9-png?_i=AA" alt="" data-public-id="Web_Assets/blog/blog-pareto-front-9.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1709058494" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058494/Web_Assets/blog/blog-pareto-front-9/blog-pareto-front-9-png?_i=AA 1999w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058494/Web_Assets/blog/blog-pareto-front-9/blog-pareto-front-9-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058494/Web_Assets/blog/blog-pareto-front-9/blog-pareto-front-9-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058494/Web_Assets/blog/blog-pareto-front-9/blog-pareto-front-9-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1709058494/Web_Assets/blog/blog-pareto-front-9/blog-pareto-front-9-png?_i=AA 1536w" sizes="(max-width: 1999px) 100vw, 1999px"></figure>



<p>Now things look quite different than with the smaller, web-sized images. WebP, mozjpeg, and AVIF are worse than libjpeg-turbo (for these images, at this quality point). HEIC brings significant savings over libjpeg-turbo, though so does jpegli, at a much better speed. JPEG XL is the clear winner, compressing the images to less than 1.3 bpp while AVIF, libjpeg-turbo, and WebP require more than 2 bpp.</p>








<p>While not as dramatic as the improvements in lossless compression, also for lossy compression there have been improvements between libjxl 0.9 and libjxl 0.10. At the default effort setting (e7), this is how the memory and speed changed for a large (39 Mpx) image:</p>



<figure><table><tbody><tr><td>effort setting</td><td>memory 0.9.2</td><td>memory 0.10</td><td>time<br>0.9</td><td>time 0.10</td><td>compressed size 0.9</td><td>compressed size 0.10</td><td>memory reduction</td><td>speedup</td></tr><tr><td>e7, d1, 1 thread</td><td>4,052 MB</td><td>397 MB</td><td>9.6s</td><td>8.6s</td><td>6.57 MB</td><td>6.56 MB</td><td>10.2x</td><td>1.11x</td></tr><tr><td>e7, d1, 8 threads</td><td>3,113 MB</td><td>437 MB</td><td>3.1s</td><td>1.7s</td><td>6.57 MB</td><td>6.56 MB</td><td>7.1x</td><td>1.76x</td></tr></tbody></table></figure>








<p>The new version of libjxl brings a very substantial reduction in memory consumption, by an order of magnitude, for both lossy and lossless compression. Also the speed is improved, especially for multi-threaded lossless encoding where the default effort setting is now an order of magnitude faster.</p>



<p>This consolidates JPEG XL’s position as the best image codec currently available, for both lossless and lossy compression, across the quality range but in particular for high quality to visually lossless quality. It is Pareto-optimal across a wide range of speed settings.</p>



<p>Meanwhile, the old JPEG is still attractive thanks to better encoders. The new jpegli encoder brings a significant improvement over mozjpeg in terms of both speed and compression. Perhaps surprisingly, good old JPEG is still part of the Pareto front — when extremely fast encoding is needed, it can still be the best choice.</p>



<p>At Cloudinary, we are actively participating in improving the state of the art in image compression. We are continuously applying new insights and technologies in order to bring the best possible experience to our end-users. As new codecs emerge and encoders for existing codes improve, we keep making sure to deliver media according to the state of the art.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Docusign just admitted that they use customer data to train AI (178 pts)]]></title>
            <link>https://twitter.com/nixcraft/status/1763124892986474689</link>
            <guid>39558365</guid>
            <pubDate>Fri, 01 Mar 2024 03:53:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/nixcraft/status/1763124892986474689">https://twitter.com/nixcraft/status/1763124892986474689</a>, See on <a href="https://news.ycombinator.com/item?id=39558365">Hacker News</a></p>
Couldn't get https://twitter.com/nixcraft/status/1763124892986474689: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[The "End of Programming" will look a lot like programming (2023) (107 pts)]]></title>
            <link>https://ben11kehoe.medium.com/the-end-of-programming-will-look-a-lot-like-programming-8b877c8efef8</link>
            <guid>39558270</guid>
            <pubDate>Fri, 01 Mar 2024 03:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ben11kehoe.medium.com/the-end-of-programming-will-look-a-lot-like-programming-8b877c8efef8">https://ben11kehoe.medium.com/the-end-of-programming-will-look-a-lot-like-programming-8b877c8efef8</a>, See on <a href="https://news.ycombinator.com/item?id=39558270">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://ben11kehoe.medium.com/?source=post_page-----8b877c8efef8--------------------------------"><div aria-hidden="false"><p><img alt="Ben Kehoe" src="https://miro.medium.com/v2/resize:fill:88:88/1*BJ2oPzPhLv1cvR3TrQeOaA.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="cbdb">Communications of the ACM has <a href="https://m-cacm.acm.org/magazines/2023/1/267976-the-end-of-programming/fulltext" rel="noopener ugc nofollow" target="_blank">a new article titled “The End of Programming”</a> by Matt Welsh. It posits that traditional programs “will be replaced by AI systems that are <em>trained</em> rather than <em>programmed</em>” (emphasis in the original). Welsh is “the CEO and co-founder of Fixie.ai, a recently founded startup developing AI capabilities to support software development teams”.</p><p id="b452">I’m generally skeptical of these broad claims. More than anything, I think when people imagine integrating AI (and especially LLMs) into software development (or any other process), they tend to be overly optimistic. But I want to focus on something in particular here. Welsh says “[w]e are rapidly moving toward a world where the fundamental building blocks of computation are temperamental, mysterious, adaptive agents.”</p><p id="a9f6">I think this is the least likely outcome. If you had to describe characteristics that drive users away from products, “temperamental and mysterious” system behavior would be high on that list (we usually just call it “buggy”). <em>Especially</em> because in this world, the people behind the product would likely say, “Ah, sorry, the AI did that. We’ll see if we can explain to it what needs to be fixed and maybe it’ll get fixed if we ask it the right thing”.</p><p id="4985">In general, a lot of the AI takes I see assert that AI will be able to assume the entire <em>responsibility</em> for a given task for a person, and implicitly assume that the person’s <em>accountability</em> for the task will just sort of…evaporate? Like, if the AI got it wrong, it’s not your fault? But if you have no real way to ensure the task is correctly performed, they’re probably going to find someone else to accomplish that task after it’s failed a few times.</p><p id="abb7">So in a world where software still has to actually do the thing required of it, let’s imagine what it would look like if we no longer needed human software developers. A human is acting as a product manager, dictating their business requirements to an AI, which then constructs software. Let’s assume that the resulting software that is largely functional (that is, free of low-level bugs) — I think this is a long ways off, and there is lots to say about what it will look like until then, but that’s a separate discussion.</p><p id="fe23">If you’ve worked as a software developer, you know that business requirements often come as vague, ill-defined, even contradictory ideas written down in ambiguous language. The primary question about this AI-only software development is, how will it make software that implements what the product manager <em>intends</em> the software to do?</p><p id="4c15">I think there are two general directions, which are not mutually exclusive.</p><p id="e2e4">The first is that the AI has to ask the product manager about every individual choice and ambiguity. It has to do this because it is good enough to know what the choices and ambiguity are, but not good enough to consistently guess the correct answer. This back-and-forth will start in plain language, and take up a lot of time for the product manager. Over time, the AI’s designers will start offering shortcuts that allow the input requirements to mean specific things when framed a certain way, so the product manager can make their choice clear from the outset. So we’ve got a method for expressing system behavior with formal guarantees. That is, we’ve invented a new programming language. At this point, the product manager is now a software developer.</p><p id="f900">The other is that the AI <em>is</em> good enough to consistently correctly guess the right answer to choices and ambiguity in the requirements <em>and</em> good enough to know when it doesn’t have confidence it can guess correctly. To do this requires an enormous amount of human cultural knowledge and probably a high degree of knowledge of the specific person acting as the product manager. The AI is doing the work of translating the business requirements into formal system behavior requirements, as well as implementing them. At this point, the AI is now a software developer.</p><p id="8188">You could argue the second is the same as Welsh’s vision of “the end of programming”, as you’re still working without a formal language. I’m saying it’s different, because it’s not “temperamental and mysterious” any more than the software developers (the ones you <em>like</em> working with) are — it’s reliable and consistent.</p><p id="2bf4">I think the second direction, AI-as-software-developer, is quite a ways off, specifically because cultural context and self-awareness are hard things. I doubt you need AGI to get it, but it seems like it would be a good chunk of the way there. So if you’re bullish on AGI, you can hope we’ll get it sooner rather than later.</p><p id="ec9d">I’d love to see the first direction taken explicitly. One of my problems with GitHub Copilot (and similar systems) is that you still own the resulting code, and it does not provide any path to gaining confidence that it has given you a correct implementation, despite you owning the correctness of the code it generates (this also warrants its own article, I think). I’d love for it to identify common patterns and formalize them, such that developers could use some shorthand to express that complicated logic and have confidence they are getting what they expect. Maybe that can be done with the existing system, or maybe it’s an entirely new Copilot-oriented programming language.</p><p id="a168">I think my main takeaway here is that, when looking at claims AI is going to automate some process, look for what the really hard, inherent complexity of that process is, and whether the process would be successful if a large degree of (new) uncertainty was injected into that complexity. For software development, I think the answer is no. That doesn’t mean AI won’t be successful, it just means we need to look deeper to refine what and how (and when) automation will play a role in it.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nokia is replacing Huawei at Deutsche Telekom sites in Germany (126 pts)]]></title>
            <link>https://www.lightreading.com/open-ran/nokia-is-replacing-huawei-at-deutsche-telekom-sites-in-germany</link>
            <guid>39557500</guid>
            <pubDate>Fri, 01 Mar 2024 01:37:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lightreading.com/open-ran/nokia-is-replacing-huawei-at-deutsche-telekom-sites-in-germany">https://www.lightreading.com/open-ran/nokia-is-replacing-huawei-at-deutsche-telekom-sites-in-germany</a>, See on <a href="https://news.ycombinator.com/item?id=39557500">Hacker News</a></p>
Couldn't get https://www.lightreading.com/open-ran/nokia-is-replacing-huawei-at-deutsche-telekom-sites-in-germany: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Struct – A Feed-Centric Chat Platform (230 pts)]]></title>
            <link>https://struct.ai/blog/introducing-the-struct-chat-platform</link>
            <guid>39557188</guid>
            <pubDate>Fri, 01 Mar 2024 00:49:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://struct.ai/blog/introducing-the-struct-chat-platform">https://struct.ai/blog/introducing-the-struct-chat-platform</a>, See on <a href="https://news.ycombinator.com/item?id=39557188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="all content" id="all-content" name="all content"><div data-framer-name="text column - 1" data-framer-component-type="RichTextContainer" name="text column - 1"><p>Endless notifications. Lost insights. Redundancy. Distraction. Chat platforms play a huge role in digital collaboration, but they're increasingly detrimental to deep work and effective outcomes. Popular options like Slack and Discord are falling short. They create knowledge black holes, where staying up to date is time-consuming and finding what you're looking for is impossible. Real-time shouldn't mean real pain. Chat is conversation! It should be inspiring! It should encourage discovery, enrichment, and productivity, not indifference, agony or frustration.</p><div><p>At Struct, we’re rebuilding the chat experience from the ground up, guided by our <a href="https://struct.ai/blog/knowledge-base" target="_blank">CRISPY design ethos</a> and a relentless pursuit of efficiency. Today, we're thrilled to unveil the Struct Chat platform. </p><p>You can check a <a href="https://struct.ai/demo-video">demo walkthrough</a> or come join us in our Struct org <a href="https://chat.struct.ai/join/DXVTmseDdBkeA6mG" target="_blank" rel="noopener">here</a>. We love to chat!</p><p>Let’s dig into the app.</p></div></div><div data-framer-name="text column - 2" data-framer-component-type="RichTextContainer" name="text column - 2"><h2>Feeds and threads.<br>Not chats and channels</h2><p>Using traditional chat channels is a hassle. We’re forced into clicking through multiple channels just to catch up on our team’s latest updates. Combine this with the always-growing list of new channels, and information starts to get really scattered. Deciding whether a conversation belongs in ‘<code>Contracts</code>’ or ‘<code>Clients</code>’, ‘<code>How-to</code>’ or ‘<code>Getting-started</code>’ can be confusing. In open-source communities, moderators work hard to keep conversations organized, while corporate teams often rely on self-policing, asking folks to "Please move this to another channel" and interrupting everyone's flow. This adds friction to our conversations.</p></div><div data-framer-name="text column - 3" data-framer-component-type="RichTextContainer" name="text column - 3"><p>On the other hand, Feeds have consistently demonstrated their scalability across numerous social platforms. People consume vast amounts of info during a quick scroll of their daily feeds (RSS, Facebook, X, IG etc.) Applied to chat, they allow you to switch from chasing updates by clicking around, to sitting back and letting the chats come to you. At Struct we’re exploring this concept to its maximum potential.</p><p>In Struct, every chat message belongs to a thread. Chats don’t get stuck in channels. Instead, any thread you have access to is displayed in a highly efficient feed. Struct has one of the fastest, most real-time feeds of any chat platform or social network, ensuring you’re always up-to-date on the latest conversations.</p><p>As new threads get created or older threads updated, they automatically make their way to the top of your feed. New chat messages pop-in to show you live activity within the threads. This makes Struct Chat a super-intuitive way to stay on top of daily conversations</p></div><div data-framer-name="Image wrapper - 4" name="Image wrapper - 4"><p data-styles-preset="vBXob0l2T">Struct's live feed keeps you ahead of every conversation  </p></div><div data-framer-name="text column - 4" name="text column - 4"><p>This is great for when you step away. Whether it's a few hours or a couple days, catching up is really easy. Just look through the top N unread threads in your feed, and you're done. These threads could be spread across any number of channels or DMs, it doesn't matter. Struct's dynamic access control system will funnel them right to your feed. By bringing the latest updates directly to you, Struct keeps you informed without the noise, and eliminates the chasing or digging required by other platforms.</p></div><div data-framer-name="Image wrapper - 5" name="Image wrapper - 5"><p data-styles-preset="vBXob0l2T">The chasing and clicking really adds up in other chat platforms</p></div><div data-framer-name="text column - 5" data-framer-component-type="RichTextContainer" name="text column - 5"><h3>Your feed, your way</h3><p>In Struct, you can create a custom feed to follow specific projects, teams, or tasks, isolating only conversations relevant to your needs. This helps you focus on the essentials, while skimming off the clutter.&nbsp;</p><p>Here’s a fun example: Want to align closely with your company's mission? Create a feed for your CEO's threads and stay in sync with your company's latest vision and goals. Jump in the conversation with the right answers and get that promotion you deserve. 💪</p><p>The possibilities are endless. What kind of feeds will you create? We can’t wait to find out.</p></div><div data-framer-name="Image wrapper - 6" name="Image wrapper - 6"><p data-styles-preset="vBXob0l2T">Custom feeds allow you to quickly create different views into your team </p></div><div data-framer-name="text column - 6" data-framer-component-type="RichTextContainer" name="text column - 6"><h3>Use tags for organization, not channels</h3><p>Tags (aka hashtags) introduce another dimension of organization, allowing threads to be further categorized by topic, project, or any identifier you choose. Together, channels and hashtags create a dual-layered system that lets you tag, assign, and streamline any conversation. This turns your chats into a useful tool for project management, task delegation, and issue tracking. All without ever leaving the app.</p></div><div data-framer-name="Image wrapper - 7" name="Image wrapper - 7"><p data-styles-preset="vBXob0l2T">Add channels to your messages to send them to the right people. Add hashtags for more context</p></div><div data-framer-name="text column - 7" data-framer-component-type="RichTextContainer" name="text column - 7"><p>Want some ideas? We use Struct as our singular place for tracking projects, resources, and tasks. I use <code>#design</code> and <code>#assign/jason</code> tags to mark threads requiring my attention. Our development team operates similarly, employing <code>p0</code>, <code>p1</code> and <code>p2</code> tags to set priorities.</p><p>Use a combination of tag filters to create really powerful feeds. For example, Manish, our Founder, created a feed called "<em>Pending Tasks</em>" to monitor our backlog. It tracks <em>any_of</em> <code>p0</code>, <code>p1</code>, and <code>p2</code> tags, while <em>excluding</em> <code>status/resolved</code> tag.</p><p><img alt="Struct app UI featuring discussions between anonymous users whose avatars are in the style of Van Gogh paintings" data-framer-asset="data:framer/asset-reference,kKAfz6DhtgAsDytDuxsl9BS1H0s.webp?originalFilename=blog-launch-tasks_compressed_originalsize.webp" data-framer-height="1796" data-framer-width="3098" height="898" src="https://framerusercontent.com/images/kKAfz6DhtgAsDytDuxsl9BS1H0s.webp" width="1549"></p><h6>User's can tag threads with channels and hashtags to stay organized. </h6><p>Think through all the chat platforms you’ve used in the past. Can you track your entire company’s tasks there? I doubt it. Traditional chat is too chaotic for effective task tracking. Struct aims to bridge this gap, creating an all-in-one solution for team collaboration.</p><h3>Dynamic access control</h3><p>Struct employs a dynamic access control system, unmatched in the chat space. This system allows a thread to belong to one or more channels, or one or more members, or just yourself. More importantly, you can add or remove access at any point in a thread's life.</p><p>This makes threads more versatile than any other platform. A conversation can be accessible to multiple channels to ensure all relevant parties have access. Want to discuss the upcoming premiere of <em>The</em> <em>Three Body Problem</em> with both your <code>@FilmFriends</code> and <code>@BookClub</code>, simply @mention both channels in one thread to chat with both groups, together.</p><p>Looking to adjust your new business contracts? Mention your <code>@legal</code> channel and <code>@derek-the-sales-person</code> in the same thread, pulling in the right people for the job.</p><p>Dynamic access ensures conversations remain targeted and inclusive, removing clutter throughout your organization.</p><h2>Search that<br><em>actually</em> works</h2><p>There's a peculiar kind of agony in knowing precisely where you left something, only to find it inexplicably vanished — a crucial idea now lost to the void. On chat platforms, this is all too familiar: you're haunted by a message you know exists, yet it dodges every attempt at retrieval, leaving sifting through endless, fruitless search results.</p></div><div data-framer-name="Image wrapper - 8" name="Image wrapper - 8"><p data-styles-preset="vBXob0l2T">Find what you need, quickly, with Struct's instant search</p></div><div data-framer-name="text column - 8" data-framer-component-type="RichTextContainer" name="text column - 8"><p>Struct redefines search functionality from the ground up. Our platform leverages advanced search technology, merging the precision of keyword searches and vector embeddings with the intelligence of semantic analysis. This hybrid approach ensures that our search results are not just instant and accurate, but also deeply relevant to your needs.</p><p>With Struct, search is more than a feature — it's a foundation tool to better navigate conversations. Press <code>Cmd/Ctrl+K</code> to quickly find exactly what you're looking for: links, files, chats, all at your fingertips</p><h2>Designed around AI</h2><p>Struct generates a title and summary for every thread, so you can make better decisions on what to skim and what to skip. This empowers everyone involved with context and confidence, making it easier than ever to catch up, follow along, and engage in the conversation at hand. Summaries and titles update in real-time too, evolving as the conversation develops.</p><p><img alt="Struct app UI featuring discussions between anonymous users whose avatars are in the style of Van Gogh paintings" data-framer-asset="data:framer/asset-reference,voF3CGRTJapI2MUWqvj80pkYHxk.webp?originalFilename=blog-launch-thread-summary_compressed_originalsize.webp" data-framer-height="1796" data-framer-width="3098" height="898" src="https://framerusercontent.com/images/voF3CGRTJapI2MUWqvj80pkYHxk.webp" width="1549"></p><h6>AI generated summaries add instant context to every discussion</h6><p>Need answers fast? Structbot is your go-to. Since Struct search indexes past threads, we can leverage past conversations to help provide proactive answers and save you time. Using the power of GPT, Structbot gets the information you need, when you need it, regardless of who's online.&nbsp;</p><p>We use OpenAI's GPT-4 for powering both the threads and the bot. We've found GPT-4 to be the most accurate in our testing.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,YL53w4Kp6X7IwZxHbyOzTZre9o.gif?originalFilename=blog-launch-structbot_1400px.gif" data-framer-height="933" data-framer-width="1400" height="466" src="https://framerusercontent.com/images/YL53w4Kp6X7IwZxHbyOzTZre9o.gif" width="700"></p><h6>Structbot provides instant answer from past references and research</h6><p>Want another life hack? Use Struct instead of using ChatGPT's interface whenever possible. You get the same responses, but now they're co-located with all your other conversations. This means you can reference the bot results together with your peers and further the discussion. Team collaboration meets AI.</p><p>Chatting with GPT doesn't need to be such a lonely affair anymore! Plus, it’d be cheaper than subscribing to ChatGPT individually. Win, win.</p><p>AI is at the core of what we're building a Struct. We’ve got big plans for Structbot, this is only the beginning.</p><h2>Privacy. That's Struct.</h2><p>At Struct, the privacy and permissions of your threads are always paramount. Our platform is designed with the understanding that not every conversation is meant for every eye. Our search functionality and Structbot respect these boundaries. When you search for answers or ask Structbot for help, you'll only see results from threads you're authorized to view. So sensitive information stays with the people it's meant for, guarded from unwanted exposure.</p><p>We also take your data privacy very seriously. Struct is a secure space for your communications.</p><h2>Intelligently designed</h2><p>The Struct app is the result of a meticulous design process, one focused on minimalism and efficiency. Our interface is deliberately subtle to ensure a distraction-free environment for work. It's completely brand-agnostic, free from any disruptive colors or logos.</p><p>We have lots of keyboard shortcuts for fast navigation, and a dark mode to reduce eye strain in low-light environments. We’re also committed to meeting users wherever they are — the app works on Windows, Mac, or Linux.</p></div><div data-framer-name="text column - 9" data-framer-component-type="RichTextContainer" name="text column - 9"><h2>Unleash the power of conversation</h2><p>Our vision for Struct Chat is clear: to create a space where conversations that matter can flourish. We're on a mission to enable better conversations for everyone, and the launch of the Struct Chat Platform marks an exciting step toward that goal. We can't wait for you to experience the difference.</p><p>Welcome to the future of real-time communication.</p><p>Welcome to Struct Chat.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things You Should Never Do, Part I (2000) (129 pts)]]></title>
            <link>https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/</link>
            <guid>39555598</guid>
            <pubDate>Thu, 29 Feb 2024 21:46:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/">https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/</a>, See on <a href="https://news.ycombinator.com/item?id=39555598">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p>Netscape 6.0 is finally going into its first public beta. There never was a version 5.0. The last major release, version 4.0, was released almost three years ago. Three years is an <i>awfully</i> long time in the Internet world. During this time, Netscape sat by, helplessly, as their market share plummeted.</p>
<p>It’s a bit smarmy of me to criticize them for waiting so long between releases. They didn’t do it <i>on purpose</i>, now, did they?</p>
<p>Well, yes. They did. They did it by making the <b>single worst strategic mistake</b> that any software company can make:</p>
<p><img decoding="async" src="https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2000/04/Upper_West_Side_Brownstones_2.jpg?w=730&amp;ssl=1" data-recalc-dims="1">They decided to rewrite the code from scratch.</p>
<p>Netscape wasn’t the first company to make this mistake. Borland made the same mistake when they bought Arago and tried to make it into dBase for Windows, a doomed project that took so long that Microsoft Access ate their lunch, then they made it again in rewriting Quattro Pro from scratch and astonishing people with how few features it had. Microsoft almost made the same mistake, trying to rewrite Word for Windows from scratch in a doomed project called Pyramid which was shut down, thrown away, and swept under the rug. Lucky for Microsoft, they had never stopped working on the old code base, so they had something to ship, making it merely a financial disaster, not a strategic one.</p>
<p>We’re programmers. Programmers are, in their hearts, architects, and the first thing they want to do when they get to a site is to bulldoze the place flat and build something grand. We’re not excited by incremental renovation: tinkering, improving, planting flower beds.</p>
<p>There’s a subtle reason that programmers always want to throw away the code and start over. The reason is that they think the old code is a mess. And here is the interesting observation: <i>they are probably wrong.</i> The reason that they think the old code is a mess is because of a cardinal, fundamental law of programming:</p>
<p>It’s harder to read code than to write it.</p>
<p>This is why code reuse is so hard. This is why everybody on your team has a different function they like to use for splitting strings into arrays of strings. They write their own function because it’s easier and more fun than figuring out how the old function works.</p>
<p><img decoding="async" src="https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2000/04/Columbus_Ave_Barber_Shop.jpg?w=730&amp;ssl=1" data-recalc-dims="1">As a corollary of this axiom, you can ask almost any programmer today about the code they are working on. “It’s a big hairy mess,” they will tell you. “I’d like nothing better than to throw it out and start over.”</p>
<p>Why is it a mess?</p>
<p>“Well,” they say, “look at this function. It is two pages long! None of this stuff belongs in there! I don’t know what half of these API calls are for.” </p>
<p>Before Borland’s new spreadsheet for Windows shipped, Philippe Kahn, the colorful founder of Borland, was quoted a lot in the press bragging about how Quattro Pro would be much better than Microsoft Excel, because it was written from scratch. All new source code! As if source code <i>rusted</i>.</p>
<p>The idea that new code is better than old is patently absurd. Old code has been <i>used</i>. It has been <i>tested</i>. <i>Lots</i> of bugs have been found, and they’ve been <i>fixed</i>. There’s nothing wrong with it. It doesn’t acquire bugs just by sitting around on your hard drive. Au contraire, baby! Is software supposed to be like an old Dodge Dart, that rusts just sitting in the garage? Is software like a teddy bear that’s kind of gross if it’s not made out of <i>all new material</i>?</p>
<p>Back to that two page function. Yes, I know, it’s just a simple function to display a window, but it has grown little hairs and stuff on it and nobody knows why. Well, I’ll tell you why: those are bug fixes. One of them fixes that bug that Nancy had when she tried to install the thing on a computer that didn’t have Internet Explorer. Another one fixes that bug that occurs in low memory conditions. Another one fixes that bug that occurred when the file is on a floppy disk and the user yanks out the disk in the middle. That LoadLibrary call is ugly but it makes the code work on old versions of Windows 95.</p>
<p>Each of these bugs took weeks of real-world usage before they were found. The programmer might have spent a couple of days reproducing the bug in the lab and fixing it. If it’s like a lot of bugs, the fix might be one line of code, or it might even be a couple of characters, but a lot of work and time went into those two characters.</p>
<p>When you throw away code and start from scratch, you are throwing away all that knowledge. All those collected bug fixes. Years of programming work.</p>
<p>You are throwing away your market leadership. You are giving a gift of two or three years to your competitors, and believe me, that is a <i>long</i> time in software years.</p>
<p>You are putting yourself in an extremely dangerous position where you will be shipping an old version of the code for several years, completely unable to make any strategic changes or react to new features that the market demands, because you don’t have shippable code. You might as well just close for business for the duration.</p>
<p>You are wasting an outlandish amount of money writing code that already exists.</p>
<p><img decoding="async" src="https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2000/04/Columbus_Ave.jpg?w=730&amp;ssl=1" data-recalc-dims="1"></p>
<p>Is there an alternative? The consensus seems to be that the old Netscape code base was <i>really </i>bad. Well, it might have been bad, but, you know what? It worked pretty darn well on an awful lot of real world computer systems.</p>
<p>When programmers say that their code is a holy mess (as they always do), there are three kinds of things that are wrong with it.</p>
<p>First, there are architectural problems. The code is not factored correctly. The networking code is popping up its own dialog boxes from the middle of nowhere; this should have been handled in the UI code. These problems can be solved, one at a time, by carefully moving code, refactoring, changing interfaces. They can be done by one programmer working carefully and checking in his changes all at once, so that nobody else is disrupted. Even fairly major architectural changes can be done without <i>throwing away the code</i>. On the Juno project we spent several months rearchitecting at one point: just moving things around, cleaning them up, creating base classes that made sense, and creating sharp interfaces between the modules. But we did it carefully, with our existing code base, and we didn’t introduce new bugs or throw away working code.</p>
<p>A second reason programmers think that their code is a mess is that it is inefficient. The rendering code in Netscape was rumored to be slow. But this only affects a small part of the project, which you can optimize or even rewrite. You don’t have to rewrite the whole thing. When optimizing for speed, 1% of the work gets you 99% of the bang.</p>
<p>Third, the code may be doggone ugly. One project I worked on actually had a data type called a FuckedString. Another project had started out using the convention of starting member variables with an underscore, but later switched to the more standard “m_”. So half the functions started with “_” and half with “m_”, which looked ugly. Frankly, this is the kind of thing you solve in five minutes with a macro in Emacs, not by starting from scratch.</p>
<p>It’s important to remember that when you start from scratch there is <b>absolutely no reason</b> to believe that you are going to do a better job than you did the first time. First of all, you probably don’t even have the same programming team that worked on version one, so you don’t actually have “more experience”. You’re just going to make most of the old mistakes again, and introduce some new problems that weren’t in the original version. </p>
<p><img decoding="async" src="https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2008/01/Lincoln_Center_Trees.jpg?w=730&amp;ssl=1" data-recalc-dims="1">The old mantra <i>build one to throw away</i> is dangerous when applied to large scale commercial applications. If you are writing code experimentally, you may want to rip up the function you wrote last week when you think of a better algorithm. That’s fine. You may want to refactor a class to make it easier to use. That’s fine, too. But throwing away the whole program is a dangerous folly, and if Netscape actually had some adult supervision with software industry experience, they might not have shot themselves in the foot so badly.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Defcon: Preventing overload with graceful feature degradation (2023) (205 pts)]]></title>
            <link>https://www.micahlerner.com/2023/07/23/defcon-preventing-overload-with-graceful-feature-degradation.html</link>
            <guid>39554874</guid>
            <pubDate>Thu, 29 Feb 2024 20:50:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.micahlerner.com/2023/07/23/defcon-preventing-overload-with-graceful-feature-degradation.html">https://www.micahlerner.com/2023/07/23/defcon-preventing-overload-with-graceful-feature-degradation.html</a>, See on <a href="https://news.ycombinator.com/item?id=39554874">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="post-content">
<p><a href="https://www.usenix.org/conference/osdi23/presentation/meza">Defcon: Preventing Overload with Graceful Feature Degradation</a></p>
<p><em>This is one in a series of papers I’m reading from OSDI and Usenix ATC. These paper reviews can be <a href="https://newsletter.micahlerner.com/">delivered weekly to your inbox</a>, or you can subscribe to the <a href="https://www.micahlerner.com/feed.xml">Atom feed</a>. As always, feel free to reach out on <a href="https://twitter.com/micahlerner">Twitter</a> with feedback or suggestions!</em></p>
<h2 id="what-is-the-research">What is the research?</h2>
<p>Severe outages can occur due to system overload<label for="load"></label><span>Discussion of managing load <a href="https://sre.google/workbook/managing-load/">from the SRE book here</a>. </span>, impacting users who rely on a product, and potentially damaging underlying hardware<label for="failslow"></label><span>Damage to hardware can show up as <em>fail-slow</em> situations, where performance degrades overtime. This is also discussed in a previous paper review on <a href="https://www.micahlerner.com/2023/04/16/perseus-a-fail-slow-detection-framework-for-cloud-storage-systems.html">Perseus: A Fail-Slow Detection Framework for Cloud Storage Systems</a> </span>. It can also be difficult to recover from outages involving overloaded system due to additional problems this type of outages cause - in particular, <a href="https://sre.google/sre-book/addressing-cascading-failures/">cascading failures</a>. There are many potential root-causes to a system entering an overloaded state, including seasonal traffic spikes, performance regressions consuming excess capacity<label for="metastable"></label><span>This situation can lead to metastable failures, as discussed in a previous <a href="https://www.micahlerner.com/2022/07/11/metastable-failures-in-the-wild.html">paper review</a>. </span>, or subtle software bugs. As such, limiting the damage caused by overload conditions is a complicated problem.</p>
<p>To prevent overload from impacting its products, Meta developed a system called <em>Defcon</em>. Defcon provides a set of abstractions that allows incident responders to increase available capacity by turning off features, an idea called <em>graceful feature degradation</em>. By dividing product features into different levels of business criticality, Defcon also allows oncallers to take a variety actions depending on the severity of an ongoing incident.</p>
<figure><img src="https://www.micahlerner.com/assets/defcon/figure1.png"><figcaption></figcaption></figure>
<figure><img src="https://www.micahlerner.com/assets/defcon/figure2.png"><figcaption></figcaption></figure>
<p>The Defcon paper describes Meta’s design, implementation, and experience deploying this system at scale across many products (including Facebook, Messenger, Instagram, and Whatsapp) along with lessons from usage during production incidents.</p>
<h2 id="background-and-motivation">Background and Motivation</h2>
<p>The authors of Defcon describe several alternatives they considered when deciding how to mitigate the risk of system overload. Each of the options is evaluated on the amount of additional resources that the approach would consume during an incident, the amount of engineering effort required to implement, and the potential impact to users.</p>
<figure><img src="https://www.micahlerner.com/assets/defcon/table1.png"><figcaption></figcaption></figure>
<p>Given that serious overload events happen on a recurring basis (at least once a year), the authors decided to invest engineering resources in an engineering-intensive effort capable of limiting user impact.</p>
<h2 id="how-does-the-system-work">How does the system work?</h2>
<p>The core abstraction in Defcon is the <em>knob</em>, which represents for each feature: a unique name, whether a feature is turned on or not, the oncall rotation responsible, and a “level” corresponding to business-criticality.</p>
<figure><img src="https://www.micahlerner.com/assets/defcon/listing1.png"><figcaption></figcaption></figure>
<figure><img src="https://www.micahlerner.com/assets/defcon/features.png"><figcaption></figcaption></figure>
<p>After a feature is defined using this configuration, servers or applications (for example, in Web or iOS devices) import the knob into code and implement code paths that handle cases when the <em>knob</em> is turned off - for example, short-circuiting expensive logic.</p>
<figure><img src="https://www.micahlerner.com/assets/defcon/listing2.png"><figcaption></figcaption></figure>
<p>During testing and incident response, operators change a <em>knob</em>’s state via a command-line or user interface, and Defcon handles replicating this state to impacted consumers (like servers and mobile applications). Knob state is also stored in a database.</p>
<figure><img src="https://www.micahlerner.com/assets/defcon/figure3.png"><figcaption></figcaption></figure>
<p>Defcon’s <em>Knob Actuator Service</em> propagates state changes for two types of knobs: <em>server-side knobs</em> and <em>client-side knobs</em>:</p>
<blockquote>
<p><em>Server-side knobs</em> are implemented in binaries running on the servers in data centers. The advantage of server-side knobs is that we can adjust the knobs’ state in seconds without any propagation delays.</p>
</blockquote>
<blockquote>
<p><em>Client-side knobs</em> are implemented in client code running on phones, tablets, wearables, and so on. The advantage of client-side knobs is that they have the capability to reduce network load by stopping requests sent to the server along side reducing server load due to the request.</p>
</blockquote>
<p>Client-side knobs (like those in an iOS application) are slightly more complex to update. Under normal conditions, they change via a push (called <em>Silent Push Notification (SPN)</em>) or routine pull (<em>Mobile Configuration Pull</em>) mechanism. To handle extenuating circumstances (like lower latency response to severe outages), Defcon can also instruct clients to pull a broader set of configuration stored in a specific server-location using a process called <em>Emergency Mobile Configuration</em><label for="serious"></label><span>Under normal operating conditions, a full reset isn’t used because it has the tradeoff of using more resources (in particular networking), which is unfriendly to user mobile plans and device batteries. </span>.</p>
<p>Knobs are, “grouped into three categories: (1) By service name, (2) by product name, and (3) by feature name (such as “search,” “video,” “feed,” and so on)” to simplify testing during development and post-release. Testing occurs through small scale A/B tests (where one “experiment arm” of users experience feature degradation, and the “control” arm does not) and during larger exercises that ensure the Defcon system is working (described later in the paper). These tests also have the side effect of generating data on what capacity a feature or product is using, which serves as an input to capacity planning.</p>
<p>During incidents, oncallers can also use the output of these tests to understand what the potential implications are of turning off different knobs. The</p>
<figure><img src="https://www.micahlerner.com/assets/defcon/figure4.png"><figcaption></figcaption></figure>
<h2 id="how-is-the-research-evaluated">How is the research evaluated?</h2>
<p>The paper uses three main types of datasets to quantify Defcon’s changes:</p>
<ul>
<li><em>Real-time Monitoring System (RMS)</em> and <em>Resource Utilization Metric (RUM)</em>, which aim to measure utilization of Meta infrastructure. The specifics of which one to use depends on the experiment, as discussed below.</li>
<li><em>Transitive Resource Utilization (TRU)</em>, which aims to measure the downstream utilization that a service has of shared Meta systems (like its graph infrastructure described in my previous paper review on <a href="https://www.micahlerner.com/2021/10/13/tao-facebooks-distributed-data-store-for-the-social-graph.html">TAO: Facebook’s Distributed Data Store for the Social Graph</a>).</li>
<li><em>User Behavior Measurement (UBM)</em>, which tracks how changing a knob’s state impacts business metrics like “Video Watch Time”.</li>
</ul>
<p>The first evaluation of Defcon’s impact is at the Product-level. By turning off progressively more business-critical functionality, the system makes greater impact on Meta’s resource usage<label for="mips"></label><span>Represented with <em>mega-instructions per second (MIPS)</em>, a normalized resource representation corresponding to compute. </span>. Entirely turning off critical features (aka “Defcon Level 1”), saves a large amount of capacity, but also significantly impacts critical business metrics.</p>
<figure><img src="https://www.micahlerner.com/assets/defcon/figure8.png"><figcaption></figcaption></figure>
<figure><img src="https://www.micahlerner.com/assets/defcon/table2.png"><figcaption></figcaption></figure>
<p>Defcon is next evaluated for its ability to temporarily decrease capacity required of shared infrastructure. As discussed in a previous paper review of <a href="https://www.micahlerner.com/2021/05/31/scaling-memcache-at-facebook.html">Scaling Memcache at Facebook</a>, Meta uses Memcache extensively. By turning off optional features, oncallers are able to decrease load on this type of core system.</p>
<figure><img src="https://www.micahlerner.com/assets/defcon/figure9.png"><figcaption></figcaption></figure>
<p>Next, the research describes how Meta can decrease capacity requirements by turning off knobs in upstream systems with dependencies on other Meta products. For example, turning off Instagram-level knobs decreases load on Facebook, which ultimately depends on TAO, Meta’s graph service. Testing knobs outside of incident response surfaces resource requirements from these interdependencies.</p>
<figure><img src="https://www.micahlerner.com/assets/defcon/figure12.png"><figcaption></figcaption></figure>
<p>The Defcon paper describes a protocol for forcing Meta systems into overload conditions, and testing the impact of turning progressively more business-critical features off. By ramping user traffic to a datacenter, these experiments place increasing load on infrastructure - turning knobs off then alleviates load.</p>
<figure><img src="https://www.micahlerner.com/assets/defcon/figure15.png"><figcaption></figcaption></figure>
<h2 id="conclusion">Conclusion</h2>
<p>The Defcon paper describes a framework deployed at scale in Meta for disabling features in order to mitigate overload conditions. To reach this state, the authors needed to solve technical challenges of building the system and to collaborate with product teams to define feature criticality - in some ways, the latter seems even more difficult. The paper also mentions issues with maintainability of knobs. On this front, it seems like future work could automate the process of ensuring that knobs cover features inside of deployed code. Lastly, I’m looking forward to learning more about Defon’s integration with other recently published Meta research, like <a href="https://www.usenix.org/conference/osdi23/presentation/eriksen">the company’s capacity management system</a>.</p>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Did you encounter any leap year bugs today? (451 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39554539</link>
            <guid>39554539</guid>
            <pubDate>Thu, 29 Feb 2024 20:22:43 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39554539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="39554915"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39554915" href="https://news.ycombinator.com/vote?id=39554915&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Heard from a friend in China: the age calculation portion of the app to schedule a marriage certificate had a bug where they subtracted 22 (legal minimum age) from the year, which resulted in 2002-02-29 which doesn't exist. The app intends to compare this against the user's birth date. The error handling code assumes all errors are from the comparison. The app then rejected all marriage certificate appointments by complaining that the users are too young to marry legally.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39555513"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555513" href="https://news.ycombinator.com/vote?id=39555513&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Haha, that would be quite the appropriate place to put one of those "Please wait and try again" error messages.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39556338"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39556338" href="https://news.ycombinator.com/vote?id=39556338&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>Now that you mention it, the payment system was not working in a ski resort restaurant in Switzerland this noon.<p>They had to switch to cash.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39555303"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555303" href="https://news.ycombinator.com/vote?id=39555303&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Yes, I have a bot that posts daily San Francisco weather records to Mastodon. It did not post as scheduled today. This is because I am looking at all the high temperatures, low temperatures, and precipitation on today's date from 1875 (about as far back as there are digitized weather records I can work with) to the present. Since there was no such date as February 29, 1875 it is throwing an error.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39554969"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39554969" href="https://news.ycombinator.com/vote?id=39554969&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>No, but some of our software writes data to rotating directories named after the date, and while doing some manual debugging on a test system, it started failing to create these directories the first time it rotated on Feb 29 UTC. Turns out it just happened to run out of disk space at that time, but I had myself convinced that it was a leap year bug for over an hour. :)</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39554867"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39554867" href="https://news.ycombinator.com/vote?id=39554867&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>Yes.<p>&gt; During the morning on Thursday, no ICA store in Sweden could accept card payments. Instead, you had to use cash, Swish or pay via their app.</p><p>&gt; The reason behind the problem was an internal problem in the payment systems at ICA as a result of an extra day in February, leap day.</p><p>ICA being the biggest grocery store chain in Sweden
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39555019"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555019" href="https://news.ycombinator.com/vote?id=39555019&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>This one is rather specific, but a game rhythm based Final Fantasy game called Theatrhythm Final Bar Line is simply not allowing people to play today because it has an internal system that awards prizes for specific days and they didn't handle the case of what to do when it's on a leap day. You can boot it up but can't actually play the game as a result.<p>Not working on the game or anything but found it moderately amusing as someone who owns the game!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39556252"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39556252" href="https://news.ycombinator.com/vote?id=39556252&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>We have a product that uses ChatGPT via the API, using the 3.5 turbo version.  Our query involves some dates.  Instead of giving back text like it usually does, today it has been giving errors because it does not think 2024-02-29 is a valid date.<p>This is easy to reproduce with the web interface, at least sometimes [0].  It start out by saying it's not a valid date and then as it's explaining why it isn't it realizes its mistake and sometimes corrects itself.</p><p>[0] <a href="https://chat.openai.com/share/37490c9f-81d6-499f-b491-11653682856c" rel="nofollow">https://chat.openai.com/share/37490c9f-81d6-499f-b491-116536...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39556318"><td></td></tr>
            <tr id="39554815"><td></td></tr>
                <tr id="39555818"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555818" href="https://news.ycombinator.com/vote?id=39555818&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>I'm sure it's more complicated than "we didn't think about leap years", but it certainly sounds pretty amateurish.<p>Old programmer rant: In my day, we fixed the Y2K bug - we went to the future and back several times a day!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39555430"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555430" href="https://news.ycombinator.com/vote?id=39555430&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>The other way around! Today a few services that don't congratulate me on my birthday (on non-leap years) did. I was born on February 29th.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39556153"><td></td></tr>
                  <tr id="39555199"><td></td></tr>
            <tr id="39556299"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39556299" href="https://news.ycombinator.com/vote?id=39556299&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>I had a few [datetime].replace(year=[current year]+n) in python where n is not divisble by 4 e.g. 2,10<p>This is in code we use for scheduling
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39556119"><td></td></tr>
                <tr id="39556331"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39556331" href="https://news.ycombinator.com/vote?id=39556331&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>These ones are the worst because you'd think they'd be so obvious at time of implementation. Maybe it's just me, I do a lot of comparisons and things like this always are top of mind for me.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39554791"><td></td></tr>
                <tr id="39555167"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555167" href="https://news.ycombinator.com/vote?id=39555167&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>That's more of a design decision than a bug. It's intentional to make the product cheaper. The manual does mention it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39554924"><td></td></tr>
                <tr id="39556196"><td></td></tr>
            <tr id="39555139"><td></td></tr>
                <tr id="39556059"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39556059" href="https://news.ycombinator.com/vote?id=39556059&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>Are you sure? My F91W doesn't even ask for year in settings. How would have it known it is a leap year or not.<p>Some other models (not F91W) does track year.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39556159"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39556159" href="https://news.ycombinator.com/vote?id=39556159&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>I was responding to the claim about the Samsung Watch not showing the right date, I unfortunately have never owned the older kinds of "smartwatches" :)</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="39555153"><td></td></tr>
                  <tr id="39555319"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555319" href="https://news.ycombinator.com/vote?id=39555319&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>I noticed this too and clicked this thread wondering if it would show up! Still an awesome watch.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39555046"><td></td></tr>
                  <tr id="39555407"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555407" href="https://news.ycombinator.com/vote?id=39555407&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>I certainly did. There is a batch process to cull old records. It checks for customers who do not have a date of death recorded but are &gt; 130 years old, as it assumes that we weren't informed of their death.<p>It takes 130 years from the current date and uses that in an SQL statement to compares it to the date of birth. DB2 doesn't like 1894-02-29.</p><p>Apparently it happens every 4 years, but no-one can be bothered to fix it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39554758"><td></td></tr>
            <tr id="39554787"><td></td></tr>
                <tr id="39556157"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39556157" href="https://news.ycombinator.com/vote?id=39556157&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>What happens in a situation like this? Does the staff issue physical keys? Do the doors even have manual locks? Do the staff walk every body to their room?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39554943"><td></td></tr>
            <tr id="39554674"><td></td></tr>
                <tr id="39554976"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39554976" href="https://news.ycombinator.com/vote?id=39554976&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>It's not that python doesn't have a clean way to subtract a year, it's that "subtract a year" is imprecise. There's a clean way to subtract 365 days, and there's a clean way to set the year one year earlier. But if you're doing the second thing, is python supposed to silently change to March 1 when you change the year from a leap day? There's no way around handling edge cases.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39555103"><td></td></tr>
                  <tr id="39554853"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39554853" href="https://news.ycombinator.com/vote?id=39554853&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>What is your definition of "subtracting a year"? Seems like that's a relatively ambiguous operation without more specification.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39554964"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39554964" href="https://news.ycombinator.com/vote?id=39554964&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Can you think of any situation where subtracting a year from today's date is ambiguous when today isn't, well, today?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39556179"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39556179" href="https://news.ycombinator.com/vote?id=39556179&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>Moving bank/festive holidays, first Monday of the year(, first work day of the year not Monday if that's NYD and bank holiday), lunar occasions.<p>'subtract a year' is imprecise and has many meanings, if what you want is 'same day, same month, previous year' then say that and do that, that's conceptually `date.year -= 1` not `date -= 1 year`, and will have this bug.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39555345"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39555345" href="https://news.ycombinator.com/vote?id=39555345&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Yes - on any day, subtracting a year might mean subtracting the average length of a year (which is a bit more than 365 days), or wanting the same day and month number in the previous calendar year, or wanting the same semantic difference ("last Monday of the month in January"), to name a few possible meanings.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39555652"><td></td></tr>
                  <tr id="39554958"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39554958" href="https://news.ycombinator.com/vote?id=39554958&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Since they mentioned a clean up script, I assume they could easily just use 365 days for that use case.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39555068"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39555068" href="https://news.ycombinator.com/vote?id=39555068&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>But then it'll be off by one day for the rest of this year.  And someone will notice that they no longer have March 1 2023-March 1 2024 in their chart, but March 2 2023</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39556284"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39556284" href="https://news.ycombinator.com/vote?id=39556284&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>It's a cleanup script. I bet nobody cares it's off by one day. Also I doubt a cleanup script has a charting function.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39555178"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555178" href="https://news.ycombinator.com/vote?id=39555178&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>&gt; now.replace(year=now.year-1)<p>Yeah but this is bad code. Python certainly does have a "clean" way to subtract a year, you subtract a datetime.timedelta object.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39555346"><td></td></tr>
            <tr id="39554992"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39554992" href="https://news.ycombinator.com/vote?id=39554992&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Okay I am having really odd undefined behavior in Python in UART communications that were working just fine yesterday... My boss joked it could be a leap year thing but at this point it wouldn't surprise me. Switch over to using Rust and no issues at all</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39555220"><td></td></tr>
                  <tr id="39555106"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555106" href="https://news.ycombinator.com/vote?id=39555106&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>I can understand getting the years 2000 (leap), 2100 (not leap), 2200 (not leap), and 2300 (not leap) wrong. But getting the year 2024 wrong is, disappointing, to put it diplomatically.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39555680"><td></td></tr>
                  <tr id="39556328"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39556328" href="https://news.ycombinator.com/vote?id=39556328&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>I did, actually!<p>A couple date form fields on AWS had their date incorrectly set to 2024/02/28 instead of 2024/02/29. Not mission critical, but it is something :D.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39556138"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39556138" href="https://news.ycombinator.com/vote?id=39556138&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Gusto paycheck didn't come in until a lot later than usual... thought this might have been my last day on the job for a little bit, didn't help that my manager and I's one on one was my first meeting of the day heh.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39555733"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555733" href="https://news.ycombinator.com/vote?id=39555733&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Our ETL process is heavily monitored so we never miss a days data, but we got a surprising error "cant build aggregates - missing data, aborting MV refresh, data will be a day old".
It was the year to date (YTD) calculation - no data for 29/2/2023 to compare to today.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39556173"><td></td></tr>
                <tr id="39556273"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39556273" href="https://news.ycombinator.com/vote?id=39556273&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Frankly unless you've considered it ahead of time and thought you'd handled it, IMO you want the error. What's the correct thing to do here? I don't think it's necessarily -365d, it might be, but if I was GP I'd be glad for the chance to consider it and decide what's correct - instead of it just blowing up or even worse silently going whichever way's wrong and undetected for a while.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="39554866"><td></td></tr>
            <tr id="39554995"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39554995" href="https://news.ycombinator.com/vote?id=39554995&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Python.<pre><code>    cls = &lt;class 'datetime.datetime'&gt;, data_string = 'Feb 29 04:55:03.687' format = '%b %d %H:%M:%S.%f'
    E       ValueError: day is out of range for month</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39555127"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555127" href="https://news.ycombinator.com/vote?id=39555127&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>That doesn’t seem incorrect;  given that no year is specified, it seems like it’s evaluating the constraint in the context of an implicit default year. (1970? 0CE?)<p>The confusing part, to me, is that Python would consider the above string to be parsed into a date in the first place, given that it has no year.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39555848"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555848" href="https://news.ycombinator.com/vote?id=39555848&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>As mentioned by sibling comments, it's because you're not specifying a year. If you change the day to the 28th you'll see that it defaults to the year 1900:<pre><code>  &gt;&gt;&gt; datetime.strptime('Feb 28 04:55:03.687', '%b %d %H:%M:%S.%f')
  datetime.datetime(1900, 2, 28, 4, 55, 3, 687000)

  &gt;&gt;&gt; datetime.strptime('Feb 28 13:37:06.942', '%b %d %H:%M:%S.%f') 
  datetime.datetime(1900, 2, 28, 13, 37, 6, 942000)</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39556222"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39556222" href="https://news.ycombinator.com/vote?id=39556222&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>That makes it weird though, because 1900 <i>was</i> a leap year? I sort of get it, but it's a slightly odd and inconsistent decision.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39556333"><td></td></tr>
            <tr id="39556316"><td></td></tr>
                        <tr id="39555098"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555098" href="https://news.ycombinator.com/vote?id=39555098&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Interesting... I suppose that is because there is no year? What year does it default to? Can you show your exact line of code?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39555149"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555149" href="https://news.ycombinator.com/vote?id=39555149&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>confirmed. and interesting/unexpected! this breaks:<p>datetime.strptime('Feb 29 13:37:06.942', '%b %d %H:%M:%S.%f')</p><p>edit: added code example. import datetime from datetime first obvi
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39555156"><td></td></tr>
                  <tr id="39554935"><td></td></tr>
            <tr id="39554826"><td></td></tr>
            <tr id="39554831"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39554831" href="https://news.ycombinator.com/vote?id=39554831&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Payroll software at work had a one-off planned maintenance day today, presumably to avoid worrying about any bugs.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39555635"><td></td></tr>
            <tr id="39554925"><td></td></tr>
                  <tr id="39555298"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555298" href="https://news.ycombinator.com/vote?id=39555298&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>My suite failed this morning on an obscure test of a function that converts between ages and dates of birth. Took me ten minutes of head scratching before I realised it was Feb 29th.<p>’’’
+       if time.Now().Month() == time.February &amp;&amp; time.Now().Day() == 29 {
+               t.SkipNow()
+       }
’’’</p><p>Fixed.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39555255"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555255" href="https://news.ycombinator.com/vote?id=39555255&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>We got a bunch of close-dated yogurt the other day.<p>Several were best by 2-27, 2-28, 3-01, and 3-02, but none by 2-29.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39555397"><td></td></tr>
                  <tr id="39555092"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555092" href="https://news.ycombinator.com/vote?id=39555092&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Yes, a few mildly bad things go wrong on a feb 29th. Everything that handles stuff a year from now for example. Pretty bad for a planning program. But our customers noticed and avoided that. Codebase is too ancient and brittle to even attempt to fix. Or at least boss doesn't want to invest time in it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39555185"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555185" href="https://news.ycombinator.com/vote?id=39555185&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Yes, in T-Mobile billing, I tried to set up automatic payments on the 26th, but the system both told me that this was impossible (because it was "less than 2 days from the end of the month") and then accepted it, because why wouldn't it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39554803"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39554803" href="https://news.ycombinator.com/vote?id=39554803&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>One of the largest food store chains in Sweden had their entire card payment go down because someone forgot to handle leap years!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39554902"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39554902" href="https://news.ycombinator.com/vote?id=39554902&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>Wikipedia says they are "the second largest retail company in the Nordic countries[citation needed]". Pretty big company anyway and embarrassing for them. Would love to learn more about what the bug was, but I guess they will never say.<p><a href="https://en.wikipedia.org/wiki/ICA_AB" rel="nofollow">https://en.wikipedia.org/wiki/ICA_AB</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39554955"><td></td></tr>
                <tr id="39555801"><td></td></tr>
                        <tr id="39555015"><td></td></tr>
                  <tr id="39556102"><td></td></tr>
            <tr id="39555186"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555186" href="https://news.ycombinator.com/vote?id=39555186&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Yes - I triple-checked the calendar to verify my suspicion this February might have 29 days. The result was always negative - it seemed 28. Then February the 29th actually came. A bug apparently occurred in my mind.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39556250"><td></td></tr>
            <tr id="39554772"><td></td></tr>
                <tr id="39555011"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555011" href="https://news.ycombinator.com/vote?id=39555011&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Every time I go to timeanddate.com I feel like I see a link to the Leap Day page (<a href="https://www.timeanddate.com/date/leap-day.html" rel="nofollow">https://www.timeanddate.com/date/leap-day.html</a>) that shows the meme-infamous boyfriend-checking-out-another-girl couple, except she's proposing to him!  Obviously this happened earlier that day since they're wearing the same outfits, and I can't help but feel bad for her knowing what's coming but unable to warn of the impending train wreck.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39554948"><td></td></tr>
                <tr id="39556202"><td></td></tr>
                  <tr id="39556193"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39556193" href="https://news.ycombinator.com/vote?id=39556193&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>I'll take a February 30 if it means every other month is 30 days too, and we just get the extra days as universal, extended winter holiday where no one can legally be required to work because there is no December 31st or January minus-fourth.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39555182"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555182" href="https://news.ycombinator.com/vote?id=39555182&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>My monthly bus passes for both February and March did not work in Dallas today. The driver was aware of the issue and just waved me in.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39554763"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39554763" href="https://news.ycombinator.com/vote?id=39554763&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>The mcdonalds order waiting thing malfunctioned, displayed de52hg04 instead of 088 and I had to wait a lot longer for my order since it flew under the radar for a while until I spoke to them :)</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39554792"><td></td></tr>
            <tr id="39554673"><td></td></tr>
            <tr id="39555856"><td></td></tr>
            <tr id="39555189"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555189" href="https://news.ycombinator.com/vote?id=39555189&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Just AFTER reading about Casio watches in this thread I looked at mine. Sure it displays the date as March 1.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39554636"><td></td></tr>
                <tr id="39554813"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39554813" href="https://news.ycombinator.com/vote?id=39554813&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>Not directly related to leap year, but a couple weeks ago I set up a script for testing notification behavior that used libfaketime to simulate runs at different times.<p>I guess this might be less-trivial if you've got a distributed multi-service architecture and perhaps also depend on APIs that aren't under your control in the first place.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39555864"><td></td></tr>
                  <tr id="39554780"><td></td></tr>
                <tr id="39554952"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39554952" href="https://news.ycombinator.com/vote?id=39554952&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>In a few hours you'll be able to backdate to "yesterday", with very low chances of hitting cert expiry issues (but I wouldn't be surprised if OP's issue involves components outside of their control or ability to test end-to-end)</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39555385"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39555385" href="https://news.ycombinator.com/vote?id=39555385&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>I should have written something along the lines of "validity date ranges" instead of expiration: you're much more likely to run into problems where you run into a certificate that was issued in the future relative to when you think is now.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39555044"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555044" href="https://news.ycombinator.com/vote?id=39555044&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Maybe? My paycheck direct deposit didn't show up until almost 7am. Normally it hits right at midnight.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39555090"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39555090" href="https://news.ycombinator.com/vote?id=39555090&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>One of the most common QA test cases when it comes to testing date and time sensitive applications.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39556255"><td></td></tr>
            <tr id="39555299"><td></td></tr>
            <tr id="39554856"><td></td></tr>
            <tr id="39556099"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39556099" href="https://news.ycombinator.com/vote?id=39556099&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>I am gald that in excel, deleting a day from March 1st automatically gives Feb 29 or 28 depending on the year in formula. Before that, I struggled a lot to find last day of month by tracking if its a leap year or not, and keeping an array of months &amp; number of days. Now I simply add 1 to month, and from resulting daye I subtract 1 day. The Date value of that gives me 31 or 30 or 29 or 28.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39556227"><td></td></tr>
            <tr id="39555232"><td></td></tr>
                <tr id="39556209"><td></td></tr>
            <tr id="39555336"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39555336" href="https://news.ycombinator.com/vote?id=39555336&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><br><div>
                  <p><span>Because programmers take shortcuts. Its easier to type x - 365 than to import Calendar and then date(x) - timedelta(1 year).</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39554720"><td></td></tr>
            <tr id="39554996"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39554996" href="https://news.ycombinator.com/vote?id=39554996&amp;how=up&amp;goto=item%3Fid%3D39554539"></a></center>    </td><td><p><span>My Casio F91W I assumed would know that year YYYY is leap, and would show 29th as Date. No, it showed 1 as in March (it doesn't show months). I had to manually set it back to 28th so that tomorrow it shows correct date.<p>To be fair, it doesn't ask for year anywhere in settings. It simply doesn't know what year it is.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GGUF, the Long Way Around (210 pts)]]></title>
            <link>https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/</link>
            <guid>39553967</guid>
            <pubDate>Thu, 29 Feb 2024 19:36:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/">https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/</a>, See on <a href="https://news.ycombinator.com/item?id=39553967">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/dbbb8ee7-f19f-44df-bce7-2612817cacd2" width="400"></figure><p><strong>Table of Contents</strong></p><ul><li><a href="#how-we-use-llm-artifacts">How We Use LLM Artifacts</a></li><li><a href="#what-is-a-machine-learning-model">What is a machine learning model</a><ul><li><a href="#starting-with-a-simple-model">Starting with a simple model</a></li></ul></li><li><a href="#writing-the-model-code">Writing the model code</a><ul><li><a href="#instantiating-the-model-object">Instantiating the model object</a></li><li><a href="#serializing-our-objects">Serializing our objects</a></li></ul></li><li><a href="#what-is-a-file">What is a file</a></li><li><a href="#how-does-pytorch-write-objects-to-files">How does PyTorch write objects to files?</a><ul><li><a href="#how-pickle-works">How Pickle works</a></li><li><a href="#from-pickle-to-safetensors">From pickle to safetensors</a></li><li><a href="#how-safetensors-works">How safetensors works</a></li><li><a href="#checkpoint-files">Checkpoint files</a></li><li><a href="#ggml">GGML</a></li><li><a href="#finally-gguf">Finally, GGUF</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul><h2 id="how-we-use-llm-artifacts">How We Use LLM Artifacts</h2><p>Large language models today are <a href="https://vickiboykis.com/2024/01/15/whats-new-with-ml-in-production/">consumed in one of several ways</a>:</p><ol><li>As API endpoints for proprietary models hosted by OpenAI, Anthropic, or major cloud providers</li><li>As model artifacts downloaded from HuggingFace’s Model Hub and/or trained/fine-tuned using HuggingFace libraries and hosted on local storage</li><li>As model artifacts available in a format optimized for local inference, typically GGUF, and accessed via applications like <code>llama.cpp</code> or <code>ollama</code></li><li>As <a href="https://onnx.ai/">ONNX</a>, a format which optimizes sharing between backend ML frameworks</li></ol><p>For a side project, I’m using <code>llama.cpp</code>, a <code>C/C++</code>-based LLM inference engine targeting <a href="https://github.com/ggerganov/llama.cpp/discussions/4167">M-series GPUs on Apple Silicon</a>.</p><p>When running <code>llama.cpp</code>, you get a long log that consists primarily of key-value pairs of metadata about your model architecture and then its performance (and <a href="https://twitter.com/vboykis/status/1751307750712156662">no yapping</a>).</p><div><pre tabindex="0"><code data-lang="bash"><span><span>make -j <span>&amp;&amp;</span> ./main -m /Users/vicki/llama.cpp/models/mistral-7b-instruct-v0.2.Q8_0.gguf -p <span>"What is Sanremo? no yapping"</span>
</span></span><span><span>
</span></span><span><span>Sanremo Music Festival <span>(</span>Festival di Sanremo<span>)</span> is an annual Italian music competition held in the city of Sanremo since 1951. It<span>'</span>s considered one of the most prestigious and influential events in the Italian music scene. The festival features both newcomers and established artists competing <span>for</span> various awards, including the Big Award <span>(</span>Gran Premio<span>)</span>, which grants the winner the right to represent Italy in the Eurovision Song Contest. The event consists of several live shows where artists perform their original songs, and a jury composed of musicians, critics, and the public determines the winners through a combination of points. <span>[</span>end of text<span>]</span>
</span></span><span><span>
</span></span><span><span>llama_print_timings:        load time <span>=</span>   11059.32 ms
</span></span><span><span>llama_print_timings:      sample time <span>=</span>      11.62 ms /   <span>140</span> runs   <span>(</span>    0.08 ms per token, 12043.01 tokens per second<span>)</span>
</span></span><span><span>llama_print_timings: prompt eval time <span>=</span>      87.81 ms /    <span>10</span> tokens <span>(</span>    8.78 ms per token,   113.88 tokens per second<span>)</span>
</span></span><span><span>llama_print_timings:        eval time <span>=</span>    3605.10 ms /   <span>139</span> runs   <span>(</span>   25.94 ms per token,    38.56 tokens per second<span>)</span>
</span></span><span><span>llama_print_timings:       total time <span>=</span>    3730.78 ms /   <span>149</span> tokens
</span></span><span><span>ggml_metal_free: deallocating
</span></span><span><span>Log end
</span></span></code></pre></div><p>These logs can be found in the <code>Llama.cpp</code> codebase. There, you’ll also find GGUF. <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md">GGUF (GPT-Generated Unified Format)</a> is the file format used to serve models on <code>Llama.cpp</code> and other local runners like <a href="https://semaphoreci.com/blog/local-llm">Llamafile, Ollama and GPT4All.</a></p><p>To understand how GGUF works, we need to first take a deep dive into machine learning models and the kinds of artifacts they produce.</p><h2 id="what-is-a-machine-learning-model">What is a machine learning model</h2><p>Let’s start by describing a machine learning model. At its simplest, a model is a file or a collection of files that contain the model architecture and weights and biases of the model generated from a training loop.</p><p>In LLM land, we’re generally interested in <a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">transformer-style models and architectures.</a></p><p>In a transformer, we have many moving parts.</p><ul><li><strong>For the input</strong>, we use <a href="https://arxiv.org/abs/2310.20707">training data corpuses aggregated from human-generated nautural language content</a></li><li>For <strong>the algorithm</strong>, we<ul><li>Convert that data <a href="https://vickiboykis.com/what_are_embeddings/">into embeddings</a></li><li><a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html#positional-encoding">Positionally encoding the embeddings</a> to provide information about where the words are in relation to each other in the sequence</li><li>Creating multi-headed <a href="https://ai.stackexchange.com/a/43892">self-attention</a> for each word in relation to each other word in the sequence based on an initialized combinations of weights</li><li><a href="https://arxiv.org/abs/2302.06461">Normalize layers via softmax</a></li><li>Run the resulting matrix through a feedfoward neural network</li><li>Project the output into the correct vector space for the desired task</li><li>Calculate loss and then update model parameters</li></ul></li><li><strong>The output</strong>: Generally for for chat completions tasks, <a href="https://arxiv.org/abs/2311.17301">the model returns the statistical likelihood</a> that any given word completes a phrase. It does this again and again for every word in the phrase, because of its autoregressive nature.</li></ul><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/86da416b-c6d8-4fb7-abf9-fcd2a80a1614" width="400"></figure><a href="https://arxiv.org/abs/2311.17301">Source.</a><p>If the model is served as a consumer end-product, it only returns the actual text output based on the highest probabilities, with numerous strategies for <a href="https://huggingface.co/blog/how-to-generate">how that text is selected.</a></p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/cb311adb-79f3-4eea-85be-7329e4aeb111" width="600"></figure><p>In short, we convert inputs to outputs using an equation. In addition to the model’s output, we also have the model itself that is generated as an artifact of the modeling process.</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/67102dbc-d049-4131-997a-df1fa5378f91" width="600"></figure><h2 id="starting-with-a-simple-model">Starting with a simple model</h2><p>Let’s take a step back from the complexity of transformers and build a small linear regression model in PyTorch. Lucky for us, <a href="https://d2l.ai/chapter_linear-regression/index.html">linear regression is also</a> a (shallow) neural network, so we can work with it in PyTorch and map our simple model to more complex ones using the same framework.</p><p>Linear regression takes a set of numerical inputs and generates a set of numerical outputs. (In contrast to transformers, which take a set of text inputs and generates a set of text inputs and their related numerical probabilities.)</p><p>For example, let’s say that we produce <a href="https://www.greatitalianchefs.com/features/hazelnuts-piedmont">artisinal hazlenut spread</a> for statisticians, and want to predict how many jars of Nulltella we’ll produce on any given day. Let’s say we have some data available to us, and that is, how many hours of sunshine we have per day, and how many jars of Nulltella we’ve been able to produce every day.</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/66ca00e6-1baf-4eb0-9d3d-112966beb797" width="200"></figure><p>It turns out that we feel more inspired to produce hazlenut spread when it’s sunny out, and we can clearly see this relationship between input and output in our data (we do not produce Nulltella Friday-Sunday because we prefer to spend those days writing about data serialization formats):</p><pre tabindex="0"><code>| day_id | hours   | jars |
|--------|---------|------|
| mon    | 1       | 2    |
| tues   | 2       | 4    |
| wed    | 3       | 6    |
| thu    | 4       | 8    |
</code></pre><p>This is the data we’ll use to train our model. We’ll need to split this data into three parts:</p><ol><li>used to train our model (training data)</li><li>used to test the accuracy of our model (test data)</li><li>used to tune our hyperparameters, meta-aspects of our model like the <a href="https://en.wikipedia.org/wiki/Learning_rate">learning rate</a>, (validation set) during the model training phase.</li></ol><p>In the specific case of linear regression, there technically are no hyperparameters, although we can plausibly consider the learning rate we set in PyTorch to be one. Let’s assume we have 100 of these data points values.</p><p>We split the data into train, test, and validation. A <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/sam.11583">usual accepted split</a> is to use 80% of data for training/validation and 20% for testing. We want our model to have access to as much data as possible so it learns a more accurate representation, so we leave most data for train.</p><p>Now that we have our data, we need to write our algorithm. The equation to get output \(Y\) from inputs \(X\) for linear regression is:</p><p>$$y = \beta_0 + \beta_1 x_1 + \varepsilon $$</p><p>This tells us that the output, \(y\) (the number of jars of Nulltella), can be predicted by:</p><ul><li>\(x_1\) - one input variable (or feature), (hours of sunshine)</li><li>\(\beta_1\) - with its given weight, also called parameters, (how important that feature is)</li><li>plus an error term \(\varepsilon\) that is the difference between the observed and actual values in a population that captures the noise of the model</li></ul><p>Our task is to continuously predict and adjust our weights to optimally solve this equation for the difference between our actual \(Y\) as presented by our data and a predicted \(\hat Y\) based on the algorithm to find the smallest sum of squared differences, \(\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}\), between each point and the line. In other words, we’d like to minimize \(\varepsilon\), because it will mean that, at each point, our \(\hat Y\) is as close to our actual \(Y\) as we can get it, given the other points.</p><p>We optimize this function <a href="https://arxiv.org/abs/1609.04747">through gradient descent</a>, where we start with either zeros or randomly-initialized weights and continue recalculating both the weights and error term until we come to an optimal stopping point.
We’ll know we’re succeeding because our loss, as calculated by RMSE should incrementally decrease in every training iteration.</p><p>Here’s the whole model learning process end-to-end (with the exception of tokenization, which we only do for models where features are text and we want to do language modeling):</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/9f8fb4b8-4b19-45e2-bb04-7657e447d42f" width="600"></figure><h2 id="writing-the-model-code">Writing the model code</h2><p>Now, let’s get more concrete and describe these ideas in code. When we train our model, we initialize our function with a set of feature values.</p><p>Let’s add our data into the model by initializing both \(x_1\) and \(Y\) as <a href="https://pytorch.org/docs/stable/tensors.html">PyTorch Tensor objects</a>.</p><div><pre tabindex="0"><code data-lang="python"><span><span>
</span></span><span><span><span># Hours of sunshine</span>
</span></span><span><span>X <span>=</span> torch<span>.</span>tensor([[<span>1.0</span>], [<span>2.0</span>], [<span>3.0</span>], [<span>4.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>
</span></span><span><span><span># Jars of Nulltella</span>
</span></span><span><span>y <span>=</span> torch<span>.</span>tensor([[<span>2.0</span>], [<span>4.0</span>], [<span>6.0</span>], [<span>8.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span></code></pre></div><p>Within code, our input data is <code>X</code>, which is a torch tensor object, and our output data is <code>y</code>. We initialize a LinearRegression which subclasses the PyTorch Module, with one linear layer, which has one input feature (sunshine) and one output feature (jars of Nulltella).</p><p>I’m going to include the code for the whole model, and then we’ll talk through it piece by piece.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> torch
</span></span><span><span><span>import</span> torch.nn <span>as</span> nn
</span></span><span><span><span>import</span> torch.optim <span>as</span> optim
</span></span><span><span>
</span></span><span><span>X <span>=</span> torch<span>.</span>tensor([[<span>1.0</span>], [<span>2.0</span>], [<span>3.0</span>], [<span>4.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>y <span>=</span> torch<span>.</span>tensor([[<span>2.0</span>], [<span>4.0</span>], [<span>6.0</span>], [<span>8.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>
</span></span><span><span><span># Define a linear regression model and its forward pass </span>
</span></span><span><span><span>class</span> <span>LinearRegression</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>def</span> __init__(self):
</span></span><span><span>        super(LinearRegression, self)<span>.</span>__init__()
</span></span><span><span>        self<span>.</span>linear <span>=</span> nn<span>.</span>Linear(<span>1</span>, <span>1</span>)  <span># 1 input feature, 1 output feature</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        <span>return</span> self<span>.</span>linear(x)
</span></span><span><span>
</span></span><span><span><span># Instantiate the model</span>
</span></span><span><span>model <span>=</span> LinearRegression()
</span></span><span><span>
</span></span><span><span><span># Inspect the model's state dictionary</span>
</span></span><span><span>print(model<span>.</span>state_dict())
</span></span><span><span>
</span></span><span><span><span># Define loss function and optimizer</span>
</span></span><span><span>criterion <span>=</span> nn<span>.</span>MSELoss() 
</span></span><span><span><span># setting our learning rate "hyperparameter" here</span>
</span></span><span><span>optimizer <span>=</span> optim<span>.</span>SGD(model<span>.</span>parameters(), lr<span>=</span><span>0.01</span>)  
</span></span><span><span>
</span></span><span><span><span># Training loop that includes forward and backward pass </span>
</span></span><span><span>num_epochs <span>=</span> <span>100</span>
</span></span><span><span><span>for</span> epoch <span>in</span> range(num_epochs):
</span></span><span><span>    <span># Forward pass</span>
</span></span><span><span>    outputs <span>=</span> model(X)
</span></span><span><span>    loss <span>=</span> criterion(outputs, y)
</span></span><span><span>    RMSE_loss  <span>=</span> torch<span>.</span>sqrt(loss)
</span></span><span><span>
</span></span><span><span>    <span># Backward pass and optimization</span>
</span></span><span><span>    optimizer<span>.</span>zero_grad()  <span># Zero out gradients</span>
</span></span><span><span>    RMSE_loss<span>.</span>backward()  <span># Compute gradients</span>
</span></span><span><span>    optimizer<span>.</span>step()  <span># Update weights</span>
</span></span><span><span>
</span></span><span><span>    <span># Print progress</span>
</span></span><span><span>    <span>if</span> (epoch<span>+</span><span>1</span>) <span>%</span> <span>10</span> <span>==</span> <span>0</span>:
</span></span><span><span>        print(<span>f</span><span>'Epoch [</span><span>{</span>epoch<span>+</span><span>1</span><span>}</span><span>/</span><span>{</span>num_epochs<span>}</span><span>], Loss: </span><span>{</span>loss<span>.</span>item()<span>:</span><span>.4f</span><span>}</span><span>'</span>)
</span></span><span><span>
</span></span><span><span><span># After training, let's test the model</span>
</span></span><span><span>test_input <span>=</span> torch<span>.</span>tensor([[<span>5.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>predicted_output <span>=</span> model(test_input)
</span></span><span><span>print(<span>f</span><span>'Prediction for input </span><span>{</span>test_input<span>.</span>item()<span>}</span><span>: </span><span>{</span>predicted_output<span>.</span>item()<span>}</span><span>'</span>)
</span></span></code></pre></div><p>Once we have our input data, we then initialize our model, a <code>LinearRegression</code> which subclasses <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">Module base class</a> specifically for <a href="https://github.com/pytorch/pytorch/blob/372d078f361e726bb4ac0884ac334b04c58179ef/torch/nn/modules/linear.py#L49">linear regression.</a></p><p>A forward pass involves feeding our data into the neural network and making sure it propogagtes through all the layers. Since we only have one, we have to pass our data to a single linear layer. The forward pass is what calculates our predicted <code>Y</code>.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>LinearRegression</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>def</span> __init__(self):
</span></span><span><span>        super(LinearRegression, self)<span>.</span>__init__()
</span></span><span><span>        self<span>.</span>linear <span>=</span> nn<span>.</span>Linear(<span>1</span>, <span>1</span>)  <span># 1 input feature, 1 output feature</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        <span>return</span> self<span>.</span>linear(x)
</span></span></code></pre></div><p>We pick how we’d like to optimize the results of the model, aka how its loss should converge. In this case, we start with <code>mean squared error</code>, and then modify it to use <code>RMSE</code>, the square root of the average squared difference between the predicted values and the actual values in a dataset.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Define loss function and optimizer</span>
</span></span><span><span>criterion <span>=</span> torch<span>.</span>sqrl(nn<span>.</span>MSELoss())  <span># RMSE in the training loop</span>
</span></span><span><span>optimizer <span>=</span> optim<span>.</span>SGD(model<span>.</span>parameters(), lr<span>=</span><span>0.01</span>)
</span></span><span><span>
</span></span><span><span><span>....</span>
</span></span><span><span><span>for</span> epoch <span>in</span> range(num_epochs):
</span></span><span><span>    <span># Forward pass</span>
</span></span><span><span>    outputs <span>=</span> model(X)
</span></span><span><span>    loss <span>=</span> criterion(outputs, y)
</span></span><span><span>    RMSE_loss  <span>=</span> torch<span>.</span>sqrt(loss)
</span></span></code></pre></div><p>Now that we’ve defined how we’d like the model to run, we can instantiate the model object itself:</p><h2 id="instantiating-the-model-object">Instantiating the model object</h2><div><pre tabindex="0"><code data-lang="python"><span><span>model <span>=</span> LinearRegression()
</span></span><span><span>print(model<span>.</span>state_dict())
</span></span></code></pre></div><p>Notice that when we instantiate a <code>nn.Module</code>, it has an attribute called the “state_dict”. This is important. <a href="https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html">The state dict</a> holds the information about each layer and the parameters in each layer, aka the weights and biases.</p><p>At its heart, <a href="https://github.com/pytorch/pytorch/blob/637cf4a3f2cfdd364005681636ca885bdc4d5887/torch/nn/modules/module.py#L1842">it’s a Python dictionary.</a></p><p>In this case, the implementation for LinearRegression returns an ordered dict with each layer of the network and values of those layers. Each of the values is a <code>Tensor</code>.</p><div><pre tabindex="0"><code data-lang="python"><span><span>OrderedDict([(<span>'linear.weight'</span>, tensor([[<span>0.5408</span>]])), (<span>'linear.bias'</span>, tensor([<span>-</span><span>0.8195</span>]))])
</span></span><span><span>
</span></span><span><span><span>for</span> param_tensor <span>in</span> model<span>.</span>state_dict():
</span></span><span><span>    print(param_tensor, <span>"</span><span>\t</span><span>"</span>, model<span>.</span>state_dict()[param_tensor]<span>.</span>size())
</span></span><span><span>
</span></span><span><span>linear<span>.</span>weight    torch<span>.</span>Size([<span>1</span>, <span>1</span>])
</span></span><span><span>linear<span>.</span>bias      torch<span>.</span>Size([<span>1</span>])
</span></span></code></pre></div><p>For our tiny model, it’s a small <code>OrderedDict</code> of tuples. You can imagine that this collection of tensors becomes extremely large and memory-intensive in a large network such as a transformer. If each parameter (each Tensor object) takes up 2 bytes in memory, <a href="https://github.com/ray-project/llm-numbers?tab=readme-ov-file#2x-number-of-parameters-typical-gpu-memory-requirements-of-an-llm-for-serving">a 7-billion parameter model can take up 14GB in GPU.</a></p><p>We then run the forward and backward passes for the model in loops. In each step, we do a forward pass to perform the calculation, a backward pass to update the weights of our model object, and then we add all that information to our model parameters.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Define loss function and optimizer</span>
</span></span><span><span>criterion <span>=</span> nn<span>.</span>MSELoss() 
</span></span><span><span>optimizer <span>=</span> optim<span>.</span>SGD(model<span>.</span>parameters(), lr<span>=</span><span>0.01</span>)  
</span></span><span><span>
</span></span><span><span><span># Training loop </span>
</span></span><span><span>num_epochs <span>=</span> <span>100</span>
</span></span><span><span><span>for</span> epoch <span>in</span> range(num_epochs):
</span></span><span><span>    <span># Forward pass</span>
</span></span><span><span>    outputs <span>=</span> model(X)
</span></span><span><span>    loss <span>=</span> criterion(outputs, y)
</span></span><span><span>    RMSE_loss  <span>=</span> torch<span>.</span>sqrt(loss)
</span></span><span><span>
</span></span><span><span>    <span># Backward pass and optimization</span>
</span></span><span><span>    optimizer<span>.</span>zero_grad()  <span># Zero out gradients</span>
</span></span><span><span>    RMSE_loss<span>.</span>backward()  <span># Compute gradients</span>
</span></span><span><span>    optimizer<span>.</span>step()  <span># Update weights</span>
</span></span><span><span>
</span></span><span><span>    <span># Print progress</span>
</span></span><span><span>    <span>if</span> (epoch<span>+</span><span>1</span>) <span>%</span> <span>10</span> <span>==</span> <span>0</span>:
</span></span><span><span>        print(<span>f</span><span>'Epoch [</span><span>{</span>epoch<span>+</span><span>1</span><span>}</span><span>/</span><span>{</span>num_epochs<span>}</span><span>], Loss: </span><span>{</span>loss<span>.</span>item()<span>:</span><span>.4f</span><span>}</span><span>'</span>)
</span></span></code></pre></div><p>Once we’ve completed these loops, we’ve trained the model artifact. What we now have once we have trained a model is an in-memory object that represents the weights, biases, and metadata of that model, stored within our instance of our <code>LinearRegression</code> module.</p><p>As we run the training loop, we can see our loss shrink. That is, the actual values are getting closer to the predicted:</p><div><pre tabindex="0"><code data-lang="python"><span><span>Epoch [<span>10</span><span>/</span><span>100</span>], Loss: <span>33.0142</span>
</span></span><span><span>Epoch [<span>20</span><span>/</span><span>100</span>], Loss: <span>24.2189</span>
</span></span><span><span>Epoch [<span>30</span><span>/</span><span>100</span>], Loss: <span>16.8170</span>
</span></span><span><span>Epoch [<span>40</span><span>/</span><span>100</span>], Loss: <span>10.8076</span>
</span></span><span><span>Epoch [<span>50</span><span>/</span><span>100</span>], Loss: <span>6.1890</span>
</span></span><span><span>Epoch [<span>60</span><span>/</span><span>100</span>], Loss: <span>2.9560</span>
</span></span><span><span>Epoch [<span>70</span><span>/</span><span>100</span>], Loss: <span>1.0853</span>
</span></span><span><span>Epoch [<span>80</span><span>/</span><span>100</span>], Loss: <span>0.4145</span>
</span></span><span><span>Epoch [<span>90</span><span>/</span><span>100</span>], Loss: <span>0.3178</span>
</span></span><span><span>Epoch [<span>100</span><span>/</span><span>100</span>], Loss: <span>0.2974</span>
</span></span></code></pre></div><p>We can also see if we print out the <code>state_dict</code> that the parameters have changed as we’ve computed the gradients and updated the weights in the backward pass:</p><div><pre tabindex="0"><code data-lang="python"><span><span>
</span></span><span><span><span>"""before"""</span>
</span></span><span><span>OrderedDict([(<span>'linear.weight'</span>, tensor([[<span>-</span><span>0.6216</span>]])), (<span>'linear.bias'</span>, tensor([<span>0.7633</span>]))])
</span></span><span><span>linear<span>.</span>weight    torch<span>.</span>Size([<span>1</span>, <span>1</span>])
</span></span><span><span>linear<span>.</span>bias      torch<span>.</span>Size([<span>1</span>])
</span></span><span><span>{<span>'state'</span>: {}, <span>'param_groups'</span>: [{<span>'lr'</span>: <span>0.01</span>, <span>'momentum'</span>: <span>0</span>, <span>'dampening'</span>: <span>0</span>, <span>'weight_decay'</span>: <span>0</span>, <span>'nesterov'</span>: <span>False</span>, <span>'maximize'</span>: <span>False</span>, <span>'foreach'</span>: <span>None</span>, <span>'differentiable'</span>: <span>False</span>, <span>'params'</span>: [<span>0</span>, <span>1</span>]}]}
</span></span><span><span>Epoch [<span>10</span><span>/</span><span>100</span>], Loss: <span>33.0142</span>
</span></span><span><span>Epoch [<span>20</span><span>/</span><span>100</span>], Loss: <span>24.2189</span>
</span></span><span><span>Epoch [<span>30</span><span>/</span><span>100</span>], Loss: <span>16.8170</span>
</span></span><span><span>Epoch [<span>40</span><span>/</span><span>100</span>], Loss: <span>10.8076</span>
</span></span><span><span>Epoch [<span>50</span><span>/</span><span>100</span>], Loss: <span>6.1890</span>
</span></span><span><span>Epoch [<span>60</span><span>/</span><span>100</span>], Loss: <span>2.9560</span>
</span></span><span><span>Epoch [<span>70</span><span>/</span><span>100</span>], Loss: <span>1.0853</span>
</span></span><span><span>Epoch [<span>80</span><span>/</span><span>100</span>], Loss: <span>0.4145</span>
</span></span><span><span>Epoch [<span>90</span><span>/</span><span>100</span>], Loss: <span>0.3178</span>
</span></span><span><span>Epoch [<span>100</span><span>/</span><span>100</span>], Loss: <span>0.2974</span>
</span></span><span><span>
</span></span><span><span><span>"""after"""</span>
</span></span><span><span>OrderedDict([(<span>'linear.weight'</span>, tensor([[<span>1.5441</span>]])), (<span>'linear.bias'</span>, tensor([<span>1.3291</span>]))])
</span></span></code></pre></div><p>The optimizer, as we see, has its own <code>state_dict</code>, which consists of these hyperparameters we discussed before: the learning rate, the weight decay, and more:</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(optimizer<span>.</span>state_dict())
</span></span><span><span>{<span>'state'</span>: {}, <span>'param_groups'</span>: [{<span>'lr'</span>: <span>0.01</span>, <span>'momentum'</span>: <span>0</span>, <span>'dampening'</span>: <span>0</span>, <span>'weight_decay'</span>: <span>0</span>, <span>'nesterov'</span>: <span>False</span>, <span>'maximize'</span>: <span>False</span>, <span>'foreach'</span>: <span>None</span>, <span>'differentiable'</span>: <span>False</span>, <span>'params'</span>: [<span>0</span>, <span>1</span>]}]}
</span></span></code></pre></div><p>Now that we have a trained model object, we can pass in new feature values for the model to evaluate. For example we can pass in an <code>X</code> value of <code>5</code> hours of sunshine and see how many jars of Nulltella we expect to make.</p><p>We do this by passing in <code>5</code> to the instantiated model object, which is now a combination of the method used to run the linear regression equation and our state dict, the weights, the current set of weights and biases to give a new predicted value. We get <code>9</code> jars, which pretty close to what we’d expect.</p><div><pre tabindex="0"><code data-lang="python"><span><span>test_input <span>=</span> torch<span>.</span>tensor([[<span>5.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>predicted_output <span>=</span> model(test_input)
</span></span><span><span>print(<span>f</span><span>'Prediction for input </span><span>{</span>test_input<span>.</span>item()<span>}</span><span>: </span><span>{</span>predicted_output<span>.</span>item()<span>}</span><span>'</span>)
</span></span><span><span>Prediction <span>for</span> input <span>5.0</span>: <span>9.049455642700195</span>
</span></span></code></pre></div><p>I’m abstracting away <a href="https://horace.io/brrr_intro.html">an enormous amount of detail</a> for the sake of clarity, namely the massive amount of work PyTorch does in moving this data in and out of GPUs and working with <a href="https://developer.nvidia.com/gpugems/gpugems2/part-iv-general-purpose-computation-gpus-primer/chapter-33-implementing-efficient">GPU-efficient datatypes</a> for efficient computing which is a large part of the work of the library. We’ll skip these for now for simplicity.</p><h2 id="serializing-our-objects">Serializing our objects</h2><p>So far, so good. We now have stateful Python objects in-memory that convey the state of our model. But what happens when we need to persist this very large model, that we likely spent 24+ hours training, and use it again?</p><p>This scenario is described <a href="https://blog.nelhage.com/post/pickles-and-ml/">here</a>,</p><blockquote><p>Suppose a researcher is experimenting with a new deep-learning model architecture, or a variation on an existing one. Her architecture is going to have a whole bunch of configuration options and hyperparameters: the number of layers, the types of each layers, the dimensionality of various vectors, where and how to normalize activations, which nonlinearity(ies) to use, and so on. Many of the model components will be standard layers provided by the ML framework, but the researcher will be inserting bits and pieces of novel logic as well.</p></blockquote><blockquote><p>Our researcher needs a way to describe a particular concrete model – a specific combination of these settings – which can be serialized and then reloaded later. She needs this for a few related reasons:</p></blockquote><blockquote><p>She likely has access to a compute cluster containing GPUs or other accelerators she can use to run jobs. She needs a way to submit a model description to code running on that cluster so it can run her model on the cluster.</p></blockquote><blockquote><p>While those models are training, she needs to save snapshots of their progress in such a way that they can be reloaded and resumed, in case the hardware fails or the job is preempted. Once models are trained, the researcher will want to load them again (potentially both a final snapshot, and some of the partially-trained checkpoints) in order to run evaluations and experiments on them.</p></blockquote><p>What do we mean by serialization? It’s the process of writing objects and classes from our programming runtime to a file. Deserialization is the process of converting data on disk to programming language objects in memory. We now need to seralize the data into a bytestream that we can write to a file.</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/47228a07-abc0-410f-b62b-48b756e3b30a" width="600"></figure><p>Why <a href="https://stackoverflow.com/questions/28552540/why-is-serialization-called-serialization">“serialization”</a>? Because back in the Old Days, data used to be stored on tape, which required bits to be in order sequentially on tape.</p><p>Since many transformer-style models are trained using PyTorch these days, artifacts use PyTorch’s <code>save</code> implementation for serializing objects to disk.</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/da1b0b43-de7e-4e78-ae06-32a21a018a08" width="600"></figure><h2 id="what-is-a-file">What is a file</h2><p>Again, let’s abstract away the GPU for simplicity and assume we’re performing all these computations in CPU. Python objects <a href="https://docs.python.org/3/c-api/memory.html">live in memory</a>. This memory is allocated in a special private heap at the beginning of <a href="https://anvil.works/articles/pointers-in-my-python-3">their lifecycle</a>, in <a href="https://stackoverflow.com/questions/10200628/heap-memory-in-c-programming">private heap</a> managed by the Python memory manager, with specialized heaps for different object types.</p><p>When we initialize our PyTorch model object, the operating system allocates memory through lower-level C functions, namely <code>malloc</code>, via <a href="https://docs.python.org/3/c-api/memory.html#default-memory-allocators">default memory allocators</a>.</p><p>When we run our code <a href="https://docs.python.org/3/library/tracemalloc.html">with tracemalloc</a>, we can see how memory for PyTorch is actually allocated on CPU (keep in mind that, again, GPU operations are completely different).</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> tracemalloc
</span></span><span><span>
</span></span><span><span>tracemalloc<span>.</span>start()
</span></span><span><span>
</span></span><span><span><span>.....</span>
</span></span><span><span>pytorch
</span></span><span><span><span>...</span>
</span></span><span><span>
</span></span><span><span>snapshot <span>=</span> tracemalloc<span>.</span>take_snapshot()
</span></span><span><span>top_stats <span>=</span> snapshot<span>.</span>statistics(<span>'lineno'</span>)
</span></span><span><span>
</span></span><span><span>print(<span>"[ Top 10 ]"</span>)
</span></span><span><span><span>for</span> stat <span>in</span> top_stats[:<span>10</span>]:
</span></span><span><span>    print(stat)
</span></span><span><span>
</span></span><span><span>[ Top <span>10</span> ]
</span></span><span><span><span>&lt;</span>frozen importlib<span>.</span>_bootstrap_external<span>&gt;</span>:<span>672</span>: size<span>=</span><span>21.1</span> MiB, count<span>=</span><span>170937</span>, average<span>=</span><span>130</span> B
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>inspect<span>.</span>py:<span>2156</span>: size<span>=</span><span>577</span> KiB, count<span>=</span><span>16</span>, average<span>=</span><span>36.0</span> KiB
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>site<span>-</span>packages<span>/</span>torch<span>/</span>_dynamo<span>/</span>allowed_functions<span>.</span>py:<span>71</span>: size<span>=</span><span>512</span> KiB, count<span>=</span><span>3</span>, average<span>=</span><span>171</span> KiB
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>dataclasses<span>.</span>py:<span>434</span>: size<span>=</span><span>410</span> KiB, count<span>=</span><span>4691</span>, average<span>=</span><span>90</span> B
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>site<span>-</span>packages<span>/</span>torch<span>/</span>_dynamo<span>/</span>allowed_functions<span>.</span>py:<span>368</span>: size<span>=</span><span>391</span> KiB, count<span>=</span><span>7122</span>, average<span>=</span><span>56</span> B
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>site<span>-</span>packages<span>/</span>torch<span>/</span>_dynamo<span>/</span>allowed_functions<span>.</span>py:<span>397</span>: size<span>=</span><span>349</span> KiB, count<span>=</span><span>1237</span>, average<span>=</span><span>289</span> B
</span></span><span><span><span>&lt;</span>frozen importlib<span>.</span>_bootstrap_external<span>&gt;</span>:<span>128</span>: size<span>=</span><span>213</span> KiB, count<span>=</span><span>1390</span>, average<span>=</span><span>157</span> B
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>functools<span>.</span>py:<span>58</span>: size<span>=</span><span>194</span> KiB, count<span>=</span><span>2554</span>, average<span>=</span><span>78</span> B
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>site<span>-</span>packages<span>/</span>torch<span>/</span>_dynamo<span>/</span>allowed_functions<span>.</span>py:<span>373</span>: size<span>=</span><span>136</span> KiB, count<span>=</span><span>2540</span>, average<span>=</span><span>55</span> B
</span></span><span><span><span>&lt;</span>frozen importlib<span>.</span>_bootstrap_external<span>&gt;</span>:<span>1607</span>: size<span>=</span><span>127</span> KiB, count<span>=</span><span>1133</span>, average<span>=</span><span>115</span> B
</span></span></code></pre></div><p>Here, we can see we imported 170k objects from imports, and that the rest of the allocation came from allowed_functions in torch.</p><h2 id="how-does-pytorch-write-objects-to-files">How does PyTorch write objects to files?</h2><p>We can also more explicitly see the types of these objects in memory. Among all the other objects created by PyTorch and Python system libraries, we can see our <code>Linear</code> object here, which has <code>state_dict</code> as a property. We need to serialize this object into a bytestream so we can write it to disk.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> gc
</span></span><span><span><span># Get all live objects</span>
</span></span><span><span>all_objects <span>=</span> gc<span>.</span>get_objects()
</span></span><span><span>
</span></span><span><span><span># Extract distinct object types</span>
</span></span><span><span>distinct_types <span>=</span> set(type(obj) <span>for</span> obj <span>in</span> all_objects)
</span></span><span><span>
</span></span><span><span><span># Print distinct object types</span>
</span></span><span><span><span>for</span> obj_type <span>in</span> distinct_types:
</span></span><span><span>    print(obj_type<span>.</span>__name__)
</span></span><span><span>
</span></span><span><span>InputKind
</span></span><span><span>KeyedRef
</span></span><span><span>ReLU
</span></span><span><span>Manager
</span></span><span><span>_Call
</span></span><span><span>UUID
</span></span><span><span>Pow
</span></span><span><span>Softmax
</span></span><span><span>Options 
</span></span><span><span>_Environ
</span></span><span><span><span>**</span>Linear<span>**</span>
</span></span><span><span>CFunctionType
</span></span><span><span>SafeUUID
</span></span><span><span>_Real
</span></span><span><span>JSONDecoder
</span></span><span><span>StmtBuilder
</span></span><span><span>OutDtypeOperator
</span></span><span><span>MatMult
</span></span><span><span>attrge
</span></span></code></pre></div><p>PyTorch <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">serializes objects to disk</a> using Python’s pickle framework and wrapping the pickle <code>load</code> and <code>dump</code> methods.</p><p>Pickle traverses the object’s inheritance hierarchy and converts each object encountered into streamable artifacts. It does this recursively for nested representations (for example, understanding nn.<code>Module</code> and <code>Linear</code> inheriting from <code>nn.Module</code>) and converting these representations to byte representations so that they can be written to file.</p><p>As an example, let’s take a simple function and write it to a pickle file.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> torch.nn <span>as</span> nn
</span></span><span><span><span>import</span> torch.optim <span>as</span> optim
</span></span><span><span><span>import</span> pickle
</span></span><span><span>
</span></span><span><span>X <span>=</span> torch<span>.</span>tensor([[<span>1.0</span>], [<span>2.0</span>], [<span>3.0</span>], [<span>4.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>
</span></span><span><span><span>with</span> open(<span>'tensors.pkl'</span>, <span>'wb'</span>) <span>as</span> f: 
</span></span><span><span>    pickle<span>.</span>dump(X, f) 
</span></span></code></pre></div><p>when we inspect the <a href="https://docs.python.org/3/library/pickletools.html">pickled object with pickletools</a>, we get an idea of how the data is organized.</p><p>We import some functions that load the data as a tensor, then the actual storage of that data, then its type. The module does the inverse when converting from pickle files to Python objects.</p><pre tabindex="0"><code>python -m pickletools tensors.pkl
    0: \x80 PROTO      4
    2: \x95 FRAME      398
   11: \x8c SHORT_BINUNICODE 'torch._utils'
   25: \x94 MEMOIZE    (as 0)
   26: \x8c SHORT_BINUNICODE '_rebuild_tensor_v2'
   46: \x94 MEMOIZE    (as 1)
   47: \x93 STACK_GLOBAL
   48: \x94 MEMOIZE    (as 2)
   49: (    MARK
   50: \x8c     SHORT_BINUNICODE 'torch.storage'
   65: \x94     MEMOIZE    (as 3)
   66: \x8c     SHORT_BINUNICODE '_load_from_bytes'
   84: \x94     MEMOIZE    (as 4)
   85: \x93     STACK_GLOBAL
   86: \x94     MEMOIZE    (as 5)
   87: B        BINBYTES   b'\x80\x02\x8a\nl\xfc\x9cF\xf9 j\xa8P\x19.\x80\x02M\xe9\x03.\x80\x02}q\x00(X\x10\x00\x00\x00protocol_versionq\x01M\xe9\x03X\r\x00\x00\x00little_endianq\x02\x88X\n\x00\x00\x00type_sizesq\x03}q\x04(X\x05\x00\x00\x00shortq\x05K\x02X\x03\x00\x00\x00intq\x06K\x04X\x04\x00\x00\x00longq\x07K\x04uu.\x80\x02(X\x07\x00\x00\x00storageq\x00ctorch\nFloatStorage\nq\x01X\n\x00\x00\x006061074080q\x02X\x03\x00\x00\x00cpuq\x03K\x04Ntq\x04Q.\x80\x02]q\x00X\n\x00\x00\x006061074080q\x01a.\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x80?\x00\x00\x00@\x00\x00@@\x00\x00\x80@'
  351: \x94     MEMOIZE    (as 6)
  352: \x85     TUPLE1
  353: \x94     MEMOIZE    (as 7)
  354: R        REDUCE
  355: \x94     MEMOIZE    (as 8)
  356: K        BININT1    0
  358: K        BININT1    4
  360: K        BININT1    1
  362: \x86     TUPLE2
  363: \x94     MEMOIZE    (as 9)
  364: K        BININT1    1
  366: K        BININT1    1
  368: \x86     TUPLE2
  369: \x94     MEMOIZE    (as 10)
  370: \x89     NEWFALSE
  371: \x8c     SHORT_BINUNICODE 'collections'
  384: \x94     MEMOIZE    (as 11)
  385: \x8c     SHORT_BINUNICODE 'OrderedDict'
  398: \x94     MEMOIZE    (as 12)
  399: \x93     STACK_GLOBAL
  400: \x94     MEMOIZE    (as 13)
  401: )        EMPTY_TUPLE
  402: R        REDUCE
  403: \x94     MEMOIZE    (as 14)
  404: t        TUPLE      (MARK at 49)
  405: \x94 MEMOIZE    (as 15)
  406: R    REDUCE
  407: \x94 MEMOIZE    (as 16)
  408: .    STOP
highest protocol among opcodes = 4
</code></pre><p>The main issue with pickle as a file format is that it not only bundles executable code, but that there are no checks on the code being read, and without schema guarantees, <a href="https://nedbatchelder.com/blog/202006/pickles_nine_flaws.html">you can pass something to the pickle that’s malicious</a>,</p><blockquote><p>The insecurity is not because pickles contain code, but because they create objects by calling constructors named in the pickle. Any callable can be used in place of your class name to construct objects. Malicious pickles will use other Python callables as the “constructors.” For example, instead of executing “models.MyObject(17)”, a dangerous pickle might execute “os.system(‘rm -rf /’)”. The unpickler can’t tell the difference between “models.MyObject” and “os.system”. Both are names it can resolve, producing something it can call. The unpickler executes either of them as directed by the pickle.'</p></blockquote><h2 id="how-pickle-works">How Pickle works</h2><p>Pickle initially worked for Pytorch-based models because it was also closely coupled to the Python ecosystem and initial ML library artifacts were not the key outputs of deep learning systems.</p><blockquote><p>The primary output of research is knowledge, not software artifacts. Research teams write software to answer research questions and improve their/their team’s/their field’s understanding of a domain, more so than they write software in order to have software tools or solutions.</p></blockquote><p>However, as the use of transformer-based models picked up after the release of the Transformer paper in 2017, so did the use of the <code>transformers</code> library, which delegates the <a href="https://github.com/huggingface/transformers/blob/08cd694ef07d53f6e08e60ea6e1483dbb156924d/src/transformers/models/auto/configuration_auto.py#L1006">load</a> call to PyTorch’s <code>load</code> methods, which uses pickle.</p><p>Once practitioners started creating and uploading <a href="https://arxiv.org/abs/2401.13177">pickled model artifacts to model hubs like HuggingFace</a>, <a href="https://www.youtube.com/watch?v=2ethDz9KnLk&amp;t=1103s">machine learning model supply chain security</a> became an issue.</p><h2 id="from-pickle-to-safetensors">From pickle to safetensors</h2><p>As machine learning with deep learning models trained with PyTorch exploded, these security issues came to a head, and in 2021, Trail of Bits released a post <a href="https://github.com/trailofbits/fickling">the insecurity of pickle files.</a></p><p>Engineers at HuggingFace started developing a library known as <a href="https://github.com/huggingface/safetensors/tree/main">safetensors</a> as an alternative to pickle. Safetensors was a <a href="https://github.com/huggingface/safetensors/discussions/111">developed</a> to be efficient, but, also safer and more ergonomic than pickle.</p><p>First, <code>safetensors</code> is not bound to Python as closely as Pickle: with pickle, you can only read or write files in Python. Safetensors is compatible across languages. Second, safetensors also limits language execution, functionality available on serialization and deserialization. Third, because the backend of safetensors is written in Rust, it enforces type safety more rigorously. Finally, safetensors was optimized for work specifically with tensors as a datatype in a way that Pickle was not. That, combined with the fact that it was wirtten in Rust <a href="https://huggingface.co/docs/safetensors/en/speed.">makes it really fast for reads and writes.</a></p><p>After a concerted push from both <a href="https://www.trailofbits.com/">Trail of Bits</a> and <a href="https://www.eleuther.ai/">EleutherAI</a>, a security audit of safetensors was conducted and found satisfactory, <a href="https://huggingface.co/blog/safetensors-security-audit">which led to HuggingFace adapting it as the default format for models on the Hub.</a> going forward. (Big thanks to <a href="https://twitter.com/vboykis/status/1759268551129452654">Stella and Suha</a> for this history and context, and to everyone who contributed to the Twitter thread.)</p><h2 id="how-safetensors-works">How safetensors works</h2><p>How does the safetensors format work? As with most things in LLMs at the bleeding edge, the code and commit history will do most of the talking. <a href="https://github.com/huggingface/safetensors#yet-another-format-">Let’s take a look at the file spec.</a></p><ul><li><strong>8 bytes</strong>: N, an unsigned little-endian 64-bit integer, containing the size of the header</li><li><strong>N bytes</strong>: a JSON UTF-8 string representing the header.
The header data MUST begin with a { character (0x7B).
The header data MAY be trailing padded with whitespace (0x20).
The header is a dict like {“TENSOR_NAME”: {“dtype”: “F16”, “shape”: [1, 16, 256], “data_offsets”: [BEGIN, END]}, “NEXT_TENSOR_NAME”: {…}, …},
data_offsets point to the tensor data relative to the beginning of the byte buffer (i.e. not an absolute position in the file), with BEGIN as the starting offset and END as the one-past offset (so total tensor byte size = END - BEGIN).
A special key <strong>metadata</strong> is allowed to contain free form string-to-string map. Arbitrary JSON is not allowed, all values must be strings.</li><li>Rest of the file: byte-buffer.</li></ul><p>This is different than <code>state_dict</code> and <code>pickle</code> file specifications, but the addition of safetensors follows the natural evolution from Python objects, to full-fledged file format.</p><p>A file is a way of storing our data generated from programming language objects, in bytes on disk. In looking at different file format specs (<a href="https://arrow.apache.org/docs/format/CDataInterface.html">Arrow</a>,<a href="https://parquet.apache.org/docs/file-format/">Parquet</a>, <a href="https://protobuf.dev/">protobuf</a>), we’ll start to notice some patterns around how they’re laid out.</p><ol><li>In the file, we need some indicator that this is a type of file “X”. Usually this is represented by a <a href="https://en.wikipedia.org/wiki/List_of_file_signatures"><strong>magic byte</strong>.</a></li><li>Then, there is a <strong>header</strong> that represents the metadata of the file (In the case of machine learning, how many layers we have, the learning rate, and other aspects. )</li><li>The actual <strong>data</strong>. (In the case of machine learning files, the tensors)</li><li>We then need a <strong>spec</strong> that tells us what to expect in a file as we read it and what kinds of data types are in the file and how they’re represented as bytes. Essentially, documentation for the file’s layout and API so that we can program a file reader against it.</li><li>One feature the file spec usually tells us is whether data is little or big-endian, that is - whether we store the largest number first or last. This becomes important as we expect files to be read on systems with different default byte layouts.</li><li>We then implement code that reads and writes to that filespec specifically.</li></ol><p>One thing we start to notice from having looked at statedicts and pickle files before, is that machine learning data storage follow a pattern: we need to store:</p><ol><li>a large collection of vectors,</li><li>metadata about those vectors and</li><li>hyperparameters</li></ol><p>We then need to be able to instantiate model objects that we can hydrate (fill) with that data and run model operations on.</p><p>As an example for safetensors from the documentation: We start with a Python dictionary, aka a state dict, save, and load the file.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> torch
</span></span><span><span><span>from</span> safetensors <span>import</span> safe_open
</span></span><span><span><span>from</span> safetensors.torch <span>import</span> save_file
</span></span><span><span>
</span></span><span><span>tensors <span>=</span> {
</span></span><span><span>   <span>"weight1"</span>: torch<span>.</span>zeros((<span>1024</span>, <span>1024</span>)),
</span></span><span><span>   <span>"weight2"</span>: torch<span>.</span>zeros((<span>1024</span>, <span>1024</span>))
</span></span><span><span>}
</span></span><span><span>save_file(tensors, <span>"model.safetensors"</span>)
</span></span><span><span>
</span></span><span><span>tensors <span>=</span> {}
</span></span><span><span><span>with</span> safe_open(<span>"model.safetensors"</span>, framework<span>=</span><span>"pt"</span>, device<span>=</span><span>"cpu"</span>) <span>as</span> f:
</span></span><span><span>   <span>for</span> key <span>in</span> f<span>.</span>keys():
</span></span><span><span>       tensors[key] <span>=</span> f<span>.</span>get_tensor(key)
</span></span></code></pre></div><p>we use the save_file(model.state_dict(), ‘my_model.st’) method to render the file to safetensors</p><p>In the conversion process from pickle to safetensors, we also start <a href="https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py">with the state dict.</a></p><p>Safetensors quickly became the leading format for sharing model weights and architectures to use in further fine-tuning, and in some cases, inference</p><h2 id="checkpoint-files">Checkpoint files</h2><p>We’ve so far taken a look at simple <code>state_dict</code> files and single <code>safetensors</code> files. But if you’re training a long-running model, you’ll likely have more than just weights and biases to save, and you want to save your state every so often so you can revert if you start to see issues in your trianing run. <a href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">PyTorch has checkpoints</a>. A checkpoint is a file that has a model <code>state_dict</code>, but also</p><blockquote><p>the optimizer’s state_dict, as this contains buffers and parameters that are updated as the model trains. Other items that you may want to save are the epoch you left off on, the latest recorded training loss, external torch.nn.Embedding layers, and more. This is also saved as a Dictionary and pickled, then unpickled when you need it. All of this is also saved to a dictionary, the <code>optimizer_state_dict</code>, distinct from the <code>model_state_dict</code>.</p></blockquote><div><pre tabindex="0"><code data-lang="python"><span><span><span># Additional information</span>
</span></span><span><span>EPOCH <span>=</span> <span>5</span>
</span></span><span><span>PATH <span>=</span> <span>"model.pt"</span>
</span></span><span><span>LOSS <span>=</span> <span>0.4</span>
</span></span><span><span>
</span></span><span><span>torch<span>.</span>save({
</span></span><span><span>            <span>'epoch'</span>: EPOCH,
</span></span><span><span>            <span>'model_state_dict'</span>: net<span>.</span>state_dict(),
</span></span><span><span>            <span>'optimizer_state_dict'</span>: optimizer<span>.</span>state_dict(),
</span></span><span><span>            <span>'loss'</span>: LOSS,
</span></span><span><span>            }, PATH)
</span></span></code></pre></div><p>In addition, most large language models also now include accompanying files like tokenizers, and on HuggingFace, metadata, etc. So if you’re working with PyTorch models as artifacts generated via the Transformers library, you’ll get a repo <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1/tree/main">that looks like this</a>.</p><h2 id="ggml">GGML</h2><p>As work to migrate from pickle to safetensors was ongoing for <a href="https://www.reddit.com/r/LocalLLaMA/comments/1ayd4xr/for_those_who_dont_know_what_different_model/">generalized model fine-tuning and inference</a>, Apple Silicon <a href="https://appleinsider.com/articles/23/12/13/apple-silicon-m3-pro-blows-away-nvidia-rtx-4090-gpu-in-ai-benchmark">continued to get a lot better.</a>. As a result, people started bringing modeling work and inference from large GPU-based computing clusters, to local and on-edge devices.</p><p>Georgi Gerganov’s project to make OpenAI’s Whisper run locally with <a href="https://github.com/ggerganov/whisper.cpp">Whisper.cpp.</a> was a success and the catalyst for later projects. The combination of the release of <a href="https://about.fb.com/news/2023/07/llama-2/">Llama-2 as a mostly open-source model</a>, combined with the rise of model compression techniques like <a href="https://huggingface.co/docs/peft/main/en/developer_guides/lora">LoRA</a>, large language models, which were typically only accessible on lab or industry-grade GPU hardware (inspie of the small CPU-based examples we’ve run here), also acted as a catalyst for thinking about working with and running personalized models locally.</p><p>Based on the interest and success of <code>whisper.cpp</code>, Gerganov created <a href="https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022">llama.cpp</a>, a package for working with Llama model weights, originaly in pickle format, in GGML format, for local inference.</p><p>GGML was initialy both a library and a complementary format created specifically for on-edge inference for whisper. You can also <a href="https://www.reddit.com/r/LocalLLaMA/comments/15y9m64/fine_tuningggml_quantiziation_on_apple_silicon/">perform fine-tuning</a> with it, but generally it’s used to read models trained on PyTorch in GPU Linux-based environments and converted to GGML to run on Apple Silicon.</p><p>As an example, here is script for <a href="https://github.com/ggerganov/ggml">GGML</a> which <a href="https://github.com/ggerganov/ggml/blob/master/examples/gpt-2/convert-ckpt-to-ggml.py">converts PyTorch GPT-2 checkpoints</a> to the correct format, <a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L64">read as a <code>.bin</code> file.</a>. The files are <a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/download-model.sh#L41C64-L41C131">downloaded from OpenAI</a>.</p><p><a href="https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022">The resulting GGML file compresses all of these into one and contains</a>:</p><ul><li><p>a magic number with an <a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L91">optional version number</a></p></li><li><p><a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L92">model-specific hyperparameters</a>, including
metadata about the model, such as the number of layers, the number of heads, etc.
a ftype that describes the type of the majority of the tensors,
for GGML files, the quantization version is encoded in the ftype divided by 1000</p></li><li><p>an embedded vocabulary, which is a list of strings with length prepended.</p></li><li><p>finally, a <a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L137">list of tensors</a> with their length-prepended name, type, and tensor data</p></li></ul><p>There are several elements that make GGML more efficient for local inference than checkpoint files. First, <a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/src/ggml-impl.h#L45">it makes use of 16-bit floating point representations</a> of model weights. Generally, <code>torch</code> initializes floating point datatypes in <a href="https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html">32-bit floats by default</a>. 16-bit, or half precision means that model weights use <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">50% less memory</a> at compute and inference time without significant loss in model accuracy. Other architectural choices include using C, which offers <a href="https://www.interviewbit.com/blog/difference-between-c-and-python/">more efficient memory allocation than Python</a>. And finally, GGML was built <a href="https://developer.apple.com/documentation/apple-silicon/tuning-your-code-s-performance-for-apple-silicon">optimized for Silicon.</a></p><p>Unfortunately, in its move to efficiency, GGML contained <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#drawbacks">a number of breaking changes</a> that created issues for users.</p><p>The largest one was that, since everything, both data and metadata and hyperparameters, was written into the same file, if a model added hyperparameters, it would break backward compatibility that the new file couldn’t pick up. Additionally, no model architecture metadata is present in the file, and each architecture required its own conversion script. All of this led to brittle performance and the creation of <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#gguf">GGUF.</a></p><h2 id="finally-gguf">Finally, GGUF</h2><p>GGUF has the same type of layout as GGML, with metadata and tensor data in a single file, but in addition is also designed to be backwards-compatible. The key difference is that previously instead of a list of values for the hyperparameters, the new file format uses a key-value lookup tables which accomodate shifting values.</p><p>The intiution we spent building up around how machine learning models work and file formats are laid out now allows us to understand the <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#file-structure">GGUF format.</a></p><p>First, we know that GGUF models are little-endian by default for specific architectures, which we remember is when the least significant bytes come first and is optimized for different computer hardware architectures.</p><p>Then, we have <code>gguf_header_t</code>, which is the header</p><p>It includes the magic byte that tells us this is a GGUF file:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>Must be <span>`</span>GGUF<span>`</span> at the byte level: <span>`</span>0x47<span>`</span> <span>`</span>0x47<span>`</span> <span>`</span>0x55<span>`</span> <span>`</span>0x46<span>`</span>. 
</span></span></code></pre></div><p>as well as the key-value pairs:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>// The metadata key-value pairs.
</span></span></span><span><span><span></span>    <span>gguf_metadata_kv_t</span> metadata_kv[metadata_kv_count];
</span></span></code></pre></div><p>This file format also offers versioning, in this case we see this is version 3 of the file format.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>// Must be `3` for version described in this spec, which introduces big-endian support.
</span></span></span><span><span><span></span>    <span>//
</span></span></span><span><span><span></span>    <span>// This version should only be increased for structural changes to the format.
</span></span></span></code></pre></div><p>Then, we have the tensors</p><p>The entire file looks like this, and when we work with readers like <code>llama.cpp</code> and <code>ollama</code>, they take this spec and write code to open these files and read them.</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/0da77173-fd21-470c-90d1-fa31bcfc7119" width="600"></figure><h2 id="conclusion">Conclusion</h2><p>We’ve been on a whirlwind adventure to build up our intuition of how machine learning models work, what artifacts they produce, how the machine learning artifact storage story has changed over the past couple years, and finally ended up in GGUF’s documentation to better understand the log that is presented to us when we perform local inference on artifacts in GGUF. Hope this is helpful, and good luck!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Financial systems take a holiday (176 pts)]]></title>
            <link>https://www.bitsaboutmoney.com/archive/financial-systems-take-a-holiday/</link>
            <guid>39553801</guid>
            <pubDate>Thu, 29 Feb 2024 19:21:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitsaboutmoney.com/archive/financial-systems-take-a-holiday/">https://www.bitsaboutmoney.com/archive/financial-systems-take-a-holiday/</a>, See on <a href="https://news.ycombinator.com/item?id=39553801">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>All systems reflect the culture they are created in, in ways great and small. The financial system, and the human and computer systems which compose it, have inherited norms about when work is performed from the diverse societies which built (and build) financial systems.</p><p>Your bank takes holidays. Your conception of a holiday is materially informed by when banks are closed. No system of importance can be accurately described without the context of the culture that created it and no culture can be accurately described without the context of the systems embedded in it. Neither is chicken; neither is egg.</p><p>Cultural commentary aside, this has material consequences. The app in your pocket that moves money gets less useful during holidays and on weekends.</p><p>This sometimes occasions gnashing of teeth. Many people and businesses find it inconvenient when financial systems are down. It also seems unnecessary. Financial systems are inseparably computer systems. Most similarly important computer systems don’t take holidays. Google doesn’t take holidays… or doesn’t seem to, from the perspective of a typical user, at any rate.</p><p>Technologists describe their systems as having “uptime” and measure it in “nines”, such as “We have five nines of uptime”, which means that a system has 99.999% uptime or, equivalently, about five minutes of downtime per year. Five nines is admirable in many circumstances and would be considered <em>disastrously</em> below expectations for e.g. Google Search.</p><p>Nonetheless, many financial systems <em>do</em> have availability which is far closer to five <em>twos</em>. They aren’t fully open for business during holidays, weekends, or outside of business hours. It turns out that “holidays”, “weekends”, and “business hours” are far deeper topics than one might think.</p><p>As always: while I previously worked at Stripe, and am an advisor to it, it does not necessarily endorse any commentary I make. Illustrations of engineering reality made below are for color purposes and taken from general industry knowledge rather than private knowledge of particular design documents for specific systems at any firm.</p><h2 id="what-is-a-holiday-anyway">What is a holiday, anyway?</h2><p>A holiday is any day you and your counterparty mutually agree is a holiday.</p><p>This is often an implicit agreement via <a href="https://en.wikipedia.org/wiki/Focal_point_(game_theory)">Schelling points</a>. No serious person disputes that Christmas Day is a holiday in the United States and accordingly no one needs to ask if Christmas Day is a holiday. (Many salarymen in Japan work on Christmas Day, of course, despite their coworkers Taro [1] and Patrick skipping work to... go to a Jewish friend’s birthday party or something. Taro and Patrick are, of course, <em>eccentric</em>.)</p><p>If I can pre-answer a poindextery observation: yes, I am aware that some people work on holidays. That is why we describe that as “working on a holiday” as opposed to “working.” The distinction often has material consequences.</p><p>For example: one of a million things <a href="https://twitter.com/patio11/status/1654988724353241088" rel="noreferrer">dooming</a> government <a href="https://www.bitsaboutmoney.com/archive/payroll-providers-power-respect/" rel="noreferrer">payroll</a> system modernization projects is that they require—and universally fail to budget for— substantial bespoke work by historians to figure out e.g. which set of documents is the controlling authority with respect to what overtime rate applies to meter maids in Chicago on Good Friday. Holidays can be broadly understood every-child-knows-this facts but they <em>can also</em> be contentious sites of explicit negotiation.</p><p>You might sensibly say “Ahh yes, but <em>legal</em> holidays or <em>federal government</em> holidays or <em>banking</em> holidays are not simply <em>agreements, </em>formal or informal, not in any way that matters. Those are… facts about society. They’re <em>real</em> and <em>legible</em>.”</p><p>I am a sometimes technologist. I predict that many non-specialists would have conversations with technologists about the nature of calendars, clocks, time zones, and whether time flows forward in a linear fashion and swiftly conclude that the night is dark and full of terrors. [2]</p><p>Take banking holidays. You might think there is an authoritative list of days on which bank branches are closed for business. Sure, they are closed <em>by custom</em>, but <em>that list</em> must exist, right? Banking holidays <a href="https://www.bitsaboutmoney.com/archive/seeing-like-a-bank/"><u>are legible to the banking system</u></a>, surely!</p><p><em>That</em> list does generally exist.</p><p>And so you might conclude “OK, then the holidays bank systems need to know about are <em>the banking holidays</em>. Which you have an exhaustive and authoritative list of. Problem solved.” And what’s that behind you? It’s social and engineering reality, come to destroy our sanity.</p><p>Allow me to offer an example: Company Foundation Day. Company Foundation Day is a holiday for many Japanese salarymen. Japan has a public holiday, National Foundation Day (国立記念日), every year on February 11th. It was established by order of the Cabinet more than 50 years ago. Many Japanese companies, including some which predate the issuance of that order, have a high degree of regard for their corporate history. So Company Foundation Day is a holiday, too.</p><p>What day was the company founded? It depends on the company [3]. You will be  unsurprised that Company Foundation Day is whenever a company says it is.</p><p>“Charming bit of salaryman trivia, Patrick, but what does this have to do with banking holidays?”</p><p>Well, you see, Japanese banks <em>are Japanese companies</em>. And so if you are a Japanese bank, it is very possible that <em>some</em> of your human and computer systems are, by your custom and practice, given a day of rest on a day <em>on a day when most Japanese salarymen are working</em>.</p><p>Now <em>it gets worse</em>. If you are a bank or other bit of financial infrastructure which interacts with a Japanese bank, congratulations, <em>you</em> now observe the Company Foundation Day of your counterparty with respect to some (probably relatively small) set of your operations.</p><p>What does that "observation" mean? It means whatever you agreed it means, which could be "Of course if you attempt to call your account representative on a holiday that call may be returned on the next business day." or "Of course if you or your computer system electronically communicates updated <a href="https://www.bitsaboutmoney.com/archive/kyc-and-aml-beyond-the-acronyms/" rel="noreferrer">KYC</a> information on a holiday our computer system will inform your computer system the KYC information has been accepted. 'Accepted' has a very specific meaning here. <em>No action will be taken</em> until the following business day, <em>unless</em> you <em>also</em> call or fax Operations to inform them of an emergency necessitating intervention on a holiday."</p><p>To promote the subtext to text: not all banks/etc which interact with Japanese banks are themselves well-informed about the culture that is Japanese salarymen. (The culture that is Japanese salaryman is often misunderstood to be coextensive with Japanese culture. There is no one single Japanese salaryman culture, though "salaryman" is useful shorthand for a cultural cluster. Reports of Japanese culture being a monolith are greatly exaggerated internally and externally. Sugimoto's Introduction to Japanese Society is a good text on this, if a bit dry.)</p><p>Now clearly your Japanese counterparty would not choose to surprise you with the fact of Company Foundation Day. Japan is considered exotic by many people, but it is a functioning democratic and capitalist society. Work does not constantly grind to a halt as salarymen are ambushed by other companies’ Company Foundation Day. No, salarymen do the sensible thing. They <em>wrote this down</em>. Flip to page 636 of the Operations Manual under the heading Observed Holidays. Look right there, in the middle of the list, exactly where a Japanese salaryman would expect to find his counterparty’s Company Foundation Day.</p><figure><img src="https://lh7-us.googleusercontent.com/maK8F3X_LuzDrdyNISHK2B6ytJ9ghm62pMefrC-5BnHi93nE36IY5g4ND7IunyyulkBnKrFP9AOypl099F8rP2itLeG0Iqtv_BvTua_hW7IWvZlgyae-rYW7ju6OFti4wWCet_Ap6P_hoPF9tXTtUhw" alt="Four panel Anakin/Padme meme which makes an ironic observation about likelihood Operations Manual was read." loading="lazy" width="500" height="500"></figure><p>Now, suppose you are a large company. To ensure that your systems reflect reality, your technologists very likely have created some formal system which tracks holidays. The first time you do business with a Japanese counterparty, one of them will add a list of Japanese banking holidays to the system, and another will check the work. Perhaps the list added will be sourced from a data provider, like e.g. Bloomberg. Perhaps it will be gathered by looking at Google Calendar’s list or a Wikipedia article. Perhaps your technologists, being careful, will say neither of those is good enough, and will attempt to find an <em>authoritative</em> list of holidays in a publication of the government.</p><p>Different companies will adopt different strategies. Guess which holiday <em>none</em> of the above data sources will mention. But it is <em>definitely a holiday </em>because <em>it has all the consequences of being a holiday</em>.</p><p>Not only will one soon find oneself with substantial egg on one’s face, one will very often have to organize one’s engineering team to quickly redefine <em>how one’s systems understand holidays worldwide</em> in response to the incident this fact pattern will create. Because, yes, some holidays exist only per-company, and if your financial institution is sophisticated, a computer querying whether it is a holiday or not on a particular day probably passes the jurisdiction of interest but probably does not pass the counterparty of interest when doing the lookup.</p><p>Now you could, at this point, throw your hands up in the air and say "Other people's culture is not my problem! This is a quirky edge case upon an edge case! Begone!" Goodness knows you would not be the first technologist to say that. But—and this is extremely <em>not</em> legal advice—Compliance has a definite point of view on whether you are allowed to intentionally build a KYC system which could, given your company's positive knowledge that it has accidentally moved money for a terrorist in the past and is in the process of doing so at the present moment, inform its financial parters about the terrorist <em>tomorrow</em>. Compliance also has a definite <a href="https://www.bitsaboutmoney.com/archive/bond-villain-compliance-strategy/" rel="noreferrer">point of view</a> on the wisdom of writing down that one considers a particular foreign nation a Nice To Have, really, on the list of nations where one has addressed one's domestic-to-you responsibilities and routinely follows the domestic-to-them law.</p><h2 id="fun-operational-consequences-of-holidays">Fun operational consequences of holidays</h2><p>Holidays <em>as observed</em> often are used to extend weekends, for both operational and social purposes.</p><p>If a holiday is defined as a fixed date on the calendar, it will periodically fall on a weekend, and in many nations many organizations will add an “as observed” day which is not that fixed date which expands the weekend.</p><p>Many holidays are, of course, not on fixed dates, but change every year. Why do you think we send you a new Operations Manual every year. Did you think we think you lost the old one. We have much higher regard for you than that.</p><p>Easter is March 31st in 2024. Try explaining why it is March 31st this year and not a date in April to a Chinese banker not familiar with Judaism. (If one objects that Easter is not a Jewish holiday, one should not attempt to explain the timing of Easter to a Chinese banker, or anyone else really. If one objects “Easter on which side of which schism?”, one has a good understanding of the challenges here.)</p><p>Anyhow, however they are scheduled, holidays routinely cause long weekends to happen.</p><p>Long weekends have consequences in the material world. Human activity does not stop during weekends or on holidays. Certain human activity that the financial system <em>cares about keenly</em>, such as consumer payments to businesses for goods and services, predictably explodes on or around certain holidays.</p><p>Take Black Friday / Cyber Monday.&nbsp;(BFCM, in some quarters.)</p><p><em>When</em> is Black Friday? The day after Thanksgiving. When is Thanksgiving? Whenever Americans think it is. Many Americans think it is the 4th Thursday in November. But Thanksgiving is so inextricably bound with the American commercial calendar that the reason Americans celebrate it on the 4th Thanksgiving was because <a href="https://www.britannica.com/story/why-is-thanksgiving-in-the-us-celebrated-on-a-thursday">previously we had multiple Thanksgivings</a> and <em>this caused operational problems for retailers</em>.</p><p><em>Why</em> is Black Friday? Because Americans, by well-established custom, get two holidays for Thanksgiving. By well-established custom, they typically spend Thanksgiving with family. Then, the day after, while they are not expected to work, they often attempt to get an early start on shopping for Christmas presents. Retailers have long since adapted to this phenomenon, throwing special promotions to juice sales on Black Friday. Retailers not participating in Black Friday lost share of wallet as customers spent their holiday budgets at ones that did.</p><p>This has thoroughly enshrined Black Friday in the practice of many retailers, including in Japan. In Japan, a certain large e-commerce company you may have heard of instructed teams to appropriately celebrate the holiday. Japanese people do not typically celebrate Thanksgiving, and Japan consumes very little turkey, but Japanese salarymen given an order by their boss are socialized to comply with the utmost diligence and peformative enthusiasm even when that order has a puzzling basis (or no basis at all, for that matter).</p><p>The salarymen did the natural thing: they organized a special promotion on—<em>I swear on my honor as a salaryman, may my fax toner dry out forever if I lie</em>— items which are black. It worked <em>very well</em>.</p><p>And so, by ancient custom, some extremely large Japanese companies celebrate Black Friday, the day after the 4th Thursday in November, the day where people of good will come together to buy black things at attractive prices.</p><p>This is all fascinating for people who work in retail or e-commerce. For financial systems, an interesting knock-on consequence of it is that you will have a sudden, predictable-as-the-sun-rising transaction surge on Black Friday, <em>smack dab in the middle of a four day period during which money is not moving</em>. We will return to that in a moment.</p><p>Money starts moving again on Monday. Cyber Monday.</p><p><em>Why</em> is Cyber Monday? It commemorates a perhaps apocryphal meeting between  very different peoples, not infrequently in conflict but fundamentally joined with each other, and their decision to bond over a universal human experience: shopping.&nbsp;</p><p>Cyber Monday <em>also</em> causes a transaction surge. The more indexed a financial institution is to e-commerce companies relative to non-retailing or not-very-online companies, the larger a transaction surge they will see. As time goes to infinity the Internet economy will be called “the economy”, but time is very far from infinity yet, and so different firms have differential exposure to “cyber.” [4]</p><p>Now let’s ignore the sociology and marketing considerations and focus simply on the operational mechanics: a staggering volume of purchases went through, over a variety of payment systems with very different legal and technical substrates, during a period in which the banking system <em>mostly</em> does not move money between companies.</p><p>Payments companies (and others) owe performance to their customers as defined by contracts, negotiations, market norms, promises, implicit understandings, and similar. And sometimes there is a mismatch between what is expected and what can be easily delivered.</p><p>This problem presents in fractal detail at many firms. Let’s simplify it for the purpose of illustration.</p><p>If you have promised your customer “I will pay out your sales on the next business day” and an underlying “rail” [5] takes <em>two</em> business days to pay you, you have a one-day mismatch. Your promise “consumes float.” [6] There are many, many ways you can deal with this in the ordinary course of business, and they all round to “constantly advance customers a bit of our own money.”</p><p><em>Why</em> do you do that? Many financial institutions insulate their customers from complexity and risk because <em>that is the service the financial industry offers to society</em>. We (the financial industry) teleport value through time and across space and make this look easy. We (every user of every financial system, inclusive of you and me) pay for that. </p><p>Complexity and risk are, like matter and energy, conserved within the system. Moving them from individual businesses to financial providers lets the providers deal with them efficiently for usual specialization-of-labor and comparative advantage reasons.&nbsp;</p><p>Anyhow, once a year, extremely predictable <em>in timing</em> but not necessarily <em>in magnitude</em>, you do not need to float one day of sales, like you do daily. You do not need to float three days, like most weekends. You need to float five-ish days <em>including the largest sale day of the year</em>. And you whisper fervent prayers that all the wires you expect arrive <em>exactly</em> when you expect them.</p><p>What are those wire sized like? I mean, in this sketch of issues that affect a large universe of companies differently, it could vary considerably. Let <a href="https://stripe.com/newsroom/news/bfcm2023"><u>your imagination</u></a> run wild.</p><p>Black Friday would be a bad day for hitches principally because you don’t want to break for customers on a very important day for them. Cyber Monday would be a bad day for hitches for that reason, too, but also because an entirely different kind of breakage <em>at an entirely different company</em> would hit you like a freight train.</p><p>And since you’re aware of that, maybe hundreds of people have spent the last few weeks diligently wargaming out the BFCM scenarios and writing contingency plans. Maybe you also carefully modeled BFCM float needs with finance. Maybe you also did pedestrian but real capital markets work to make sure that you could survive another company having an unfortunately timed operational stumble.</p><p><a href="https://www.sleepfoundation.org/nutrition/what-is-tryptophan" rel="noreferrer">Tryptophan</a> makes us do strange things, after all.</p><h2 id="this-is-crazy-let%E2%80%99s-get-rid-of-holidays">"This is crazy. Let’s get rid of holidays."</h2><p>The culture that is heavily-online technologists is extremely frustrated with systems which go down on a predictable schedule.</p><p>Cryptocurrency enthusiasts in particular enjoy distributed systems that are constantly up and have no single points of failure. For example, you could have every actor in the financial markets open accounts at a single bank. Why would you do that? Well, most trades involve one leg that never sleeps (blockchains, an industry term for slow databases) and one leg which implicates money (which largely exists on fast databases operated by organizations that do have sleep schedules and holidays). Convince one bank to give you an internal API to make book transfers and now money moves 24/7. And just for redundancy, we’ll use <a href="https://ir.silvergate.com/news/news-details/2023/Silvergate-Capital-Corporation-Announces-Intent-to-Wind-Down-Operations-and-Voluntarily-Liquidate-Silvergate-Bank/default.aspx"><u>two</u></a> <a href="https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/signature-ny.html"><u>banks</u></a>. Checkmate, TradFi.</p><p>Less sardonically, there is a lesson here: systems which intermediate between cultures <em>are useful</em>. Intermediating between cultures is a thing the world urgently needs and <em>is extremely prepared to pay for</em>. Systems which intermediate between cultures will frequently need to cross a gap between operating schedules. You cannot simply wish away that gap. You cannot simply assert that the one true operating schedule is 24/7.</p><p>That certainly doesn’t stop technologists from trying. If you want to drive one batty, ask about the IRS applications which have office hours (<a href="https://www.irs.gov/refunds/when-is-wheres-my-refund-available"><u>example</u></a>).</p><p>There is, believe it or not, an engineering reason for some applications to have office hours, which you’ll see in multiple places in the U.S. government. Generally: the application interfaces with a legacy system which ultimately depends on choices made back during the mainframe era. During that era, computer operators could run programs in interactive mode, with an operator at the keyboard, or in batch mode. To avoid impacting the operators of the system (i.e. regular employees doing their day-to-day jobs), batches were designed to be run at night. And so they have been run at night for many, many decades.</p><p>Now, here’s the rub: <em>we don’t know</em> if the interactive mode programs, like say looking up the status of a tax refund, are safe to run while the batches are running. [7] So we continue previous practice and don’t allow the interactive mode programs to run while the batch programs are running. It would be a very, very bad thing if the software which in a very real way <em>is the United States of America</em> suddenly developed data integrity issues because someone hooked a web application to it. The technologists (and managers) in charge of those systems are terrified of e.g. data integrity issues when e.g. sending out Social Security payments because the consequences of that would include e.g. food riots in Kansas.</p><p>But that is more an explanation of an infelicity rather than an argument that there is actually a positive consequence of holidays. There is.</p><p>Would you believe that banks <em>intentionally cause misaligned operating schedules</em>? It is an important tool to detect and discourage particular forms of fraud.</p><p>A particular terrifying genre of fraud is perpetrated by insiders with advanced knowledge of a financial institution’s back office procedures. A teenager with moral flexibility can cheat you out of a pair of sneakers. A professionalized fraud operation based in a non-extradition country can rob you for millions. But a single insider who understands your back office reasonably well can bring down a bank or cause billions of losses. Barrings and Daiwa in ‘95. Société Générale in ‘08. UBS in ‘11. [8]</p><p>A <em>very old</em> control for this sort of thing is forcing holidays, with the goal that the set of staff engaged in a conspiracy don’t have a sufficient number of conspirators at the keyboard in all the right places on all the right days the conspiracy needs to operate to be undetected.</p><p>A financial CEO who <a href="https://www.bloomberg.com/features/2023-ftx-crypto-photos/" rel="noreferrer">sleeps at the office on a beanbag chair</a> might be commendably devoted to his work. But it is no slur against devoted CEOs to say that their companies and their customers would be well-served by them <em>not being allowed to do that</em>. Take the day off. Let someone cover for you. "Cover" in the sense of "handle your work while you are at rest", not in the sense of "cover up" a hole in the balance sheet. Nobody expects that there is a hole in the balance sheet. And since there is not a hole in the balance sheet, a fellow responsible professional who has an enormous personal and professional regard for you will, applying math and procedures in the usual fashion, receive a balance sheet from you when you leave and give an updated one to you when you return. Perfectly balanced as all things should be. No need to snap.</p><p><em>If we didn’t have holidays, we’d have been forced to invent them.</em> We accept degraded performance (very useful humans: not at keyboard!) as an organizational <a href="https://www.techtarget.com/whatis/definition/Chaos-Monkey">chaos monkey</a> to shake out far more serious issues.</p><p>Does it work? Well, there exist financial institutions that haven’t been reduced to smoking craters by insider fraud, so that is a point in its favor. And, like all controls, this one operates in a constellation (different controls reinforce each other) and on a portfolio basis (you win some and you lose some but are judged on how they net rather than judged on absence of losers).</p><h3 id="will-this-change">Will this change?</h3><p>In an increasingly interconnected world where decisions are increasingly made by people who count <em>and value</em> nines, you can reasonably expect financial systems to partially close the gap between historical practice and contemporary practice of e.g. Google Search.</p><p>As I said before: cultures create systems and systems create cultures. Both systems and cultures <em>change over time</em>. The rate of change in infrastructure specifically is much lower than the rate of change we observe in e.g. fashion. But infrastructure <em>does change</em>. Credit cards were invented in a world where Chicago and Los Angeles were considered to be socially distant from each other, to allow Chicagoans in L.A. to enjoy the same trust they would enjoy in Chicago. Cultural change, <em>real observable change with consequences</em>, is not something that can only be measured on generational timescales. </p><p>But should one expect the financial system to operate constantly? Not only should you not expect that, that is not even a coherent thing to expect. The financial system is an interconnected web of individual organizations which contain systems which contain some combination of subsystems etc etc etc and at some level depends on individual people to whom complex sociocultural promises have been made and who have biological need for sleep.</p><p>And those people, for the foreseeable future, will continue to periodically rest and continue to periodically celebrate just like they continue to work on the behalf of the societies their financial systems support.</p><p><br>[1] While Taro is quite a popular name in Japan, Taro is also the usual analog to  the John in John Doe. Jane Doe is frequently rendered as Hanako. When you find them in a particular company's documentation, their family name will frequently be the name of the company, which is delightful for readers of Japanese corporate documentation who are also cyberpunk fans. (This includes many writers of Japanese corporate documentation.)</p><p>[2] The Red Priestess Melisandre could not be reached for comment on whether the Lord of Light's theology encompasses computer systems.</p><p>[3] Serious paperwork connoisseurs know that "It depends" is a deep rabbit hole here, including in the United States. Was the day a company founded the day the founders started working on a project or the day they mutually agreed it should be a company or the day they signed a contract with each other or the day they submitted paperwork to the state of Delaware or the day Delaware declared that paperwork was accepted? <em>Yes</em>. The fact that the day the company was founded and the day the company was founded are frequently months apart is not even a tiny bit weird.</p><p>[4] Readers of a certain age might sensibly ask what “cyber” means. Consider it a way to gesture broadly at technology used almost exclusively by people who both do not understand technology and feel some amount of pride in that. Teams at large retailers, believing online commerce was doomed to be a tiny sideline like catalogs and only worth tens of billions of dollars, were involved in naming Cyber Monday. The other place you’re likely to hear it frequently is American national security circles, which exist in a superposition of understanding that technology can certainly be used to kill people and break things while also believing that it’s not a <em>real</em> way to kill people and break things if it is the sort of technology built by people who look like pre-juice Steve Rogers.</p><p>[5] "Financial rails" are the legal, technical, and organizational infrastructure which allows one to move money around. That's a mouthful; "rails" is one syllable and also communicates "I believe I understand this; you don't need to explain to me that money doesn't actually move when we move money." An illustrative usage: "Did that transaction go over ACH rails?" "No, it was <a href="https://twitter.com/patio11/status/1752054398858022990" rel="noreferrer">on us</a>."</p><p>[6] Positive float is the characteristic that you enjoy the legitimate but temporary use of other people's money as a consequence of their business dealings with you not specifically intended to cause that. The classic example is in the insurance industry, where insurers might get a few years to sit on premiums before paying them back out. Negative float is the opposite condition, where others get to use your money. Negative float <em>isn't bad</em>. It will cause you to incur a cost of doing business, like labor and rent are a cost of doing business, in the service of providing a valuable service to customers at a reasonable price.</p><p>[7] To spare you a long digression into the joys of government system architecture documents, accept this sketch: the IRS' web applications are frequently impersonating a human operator with preternaturally good typing skills.</p><p>[8] Yes, this is a reference to Margin Call, the best movie about finance ever made. The scene it references isn't even in the best five scenes of a single character (a bank CEO played by Jeremy Irons in what might be the best work of a distinguished career).</p>

        

        <div>
          <h2>Want more essays in your inbox?</h2>
          <p>I write about the intersection of tech and finance, approximately biweekly. It's free.</p>
                  </div>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Money bubble (188 pts)]]></title>
            <link>https://www.tbray.org/ongoing/When/202x/2024/02/25/Money-AI-Bubble</link>
            <guid>39553743</guid>
            <pubDate>Thu, 29 Feb 2024 19:17:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tbray.org/ongoing/When/202x/2024/02/25/Money-AI-Bubble">https://www.tbray.org/ongoing/When/202x/2024/02/25/Money-AI-Bubble</a>, See on <a href="https://news.ycombinator.com/item?id=39553743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="centercontent">
<p itemprop="description">I think I’m probably going to lose quite a lot of money in the next year or two. It’s partly AI’s fault, but not
    mostly. Nonetheless I’m mostly going to write about AI, because it intersects the technosphere, where I’ve lived for
    decades.</p>

<p>I’ve given up having a regular job. The family still has income but mostly we’re harvesting our
    savings, built up over decades in a well-paid profession. Which means that we are, willy-nilly, investors. And thus aware of the
    fever-dream finance landscape that is InvestorWorld.</p>

<p id="p-1"><span>The Larger Bubble</span> · 
Put in the simplest way: Things have been too good for too long in InvestorWorld: low interest, high profits, the unending rocket
    rise of the Big-Tech sector, now with AI afterburners. Wile E. Coyote hasn’t actually run off the edge of the cliff yet, but
    there are just way more ways for things to go wrong than right in the immediate future.</p>

<p>If you want to dive a little deeper, <cite>The Economist</cite> has a sharp (but
    paywalled) take in
    <a href="https://www.economist.com/finance-and-economics/2024/02/25/stockmarkets-are-booming-but-the-good-times-are-unlikely-to-last">Stockmarkets
    are booming. But the good times are unlikely to last</a>. Their argument is that profits are overvalued by investors because, in
    recent years, they’ve always gone up. Mr Market ignores the fact that that at least some of those gleaming profits are artifacts of
    tax-slashing by right-wing governments.</p>

<p>That piece considers the observation that “Many investors hope that AI will ride to the rescue” and is politely
    skeptical.</p>

<p id="p-2"><span>Popping the bubble</span> · 
My own feelings aren’t polite; closer to
    <a href="https://finance.yahoo.com/news/yep-you-are-living-in-a-nvidia-led-tech-bubble-110014738.html">Yep, you are living in a
    Nvidia-led tech bubble</a> by Brian Sozzi over at Yahoo! Finance.</p>

<p>Sozzi is fair, pointing out that this bubble feels different from the cannabis and crypto crazes; among other things,
    chipmakers and cloud providers are reporting big high-margin revenues for real actual products. But he hammers the central point:
    What we’re seeing is FOMO-driven dumb money thrown at technology by people who have no hope of
    understanding it. Just because everybody else is and because the GPTs and image generators have cool demos.
    Sozzi has the numbers, looking at valuations through standard old-as-dirt filters and shaking his head at what he sees.</p>

<p>What’s going to happen, I’m pretty sure, is that AI/ML will, inevitably, disappoint; in the financial sense I mean, probably
    doing some useful things, maybe even a lot, but not generating the kind of profit explosions that you’d need to justify
    the bubble. So it’ll pop, and my bet it is takes a bunch of the finance world with it. As bad as 2008? Nobody knows, but it
    wouldn’t surprise me.</p>

<p>The rest of this piece considers the issues facing AI/ML,  with the goal of showing why I see it as
    a bubble-inflator and eventual bubble-popper.</p>

<p>First, a disclosure: I speak as an educated amateur. I’ve never gone much below the surface of the technology, never
    constructed a model or built model-processing software, or looked closely at the math.  But I think the discussion below still
    works.</p>

<p id="p-3"><span>What’s good about AI/ML</span> · 
Spoiler: I’m not the kind of burn-it-with-fire skeptic that I became around anything blockchain-flavored. It is clear
    that generative models manage to embed significant parts of the structure of language, of code, of pictures, of
    many things where that has previously not been the case. The understanding is sufficient to reliably accomplish the objective:
    <i>Produce plausible output</i>.</p>

<p>I’ve read enough Chomsky to believe that facility with language is a defining characteristic of intelligence. More than that, a
    necessary but not sufficient ingredient.  I dunno if anyone will build an AGI in my lifetime, but I am confident that the task
    would remain beyond reach without the functions offered by today’s generative models.</p>

<p>Furthermore, I’m super impressed by something nobody else seems to talk about: Prompt parsing. Obviously, prompts are
    processed into a representation that reliably sends the model-traversal logic down substantially the right
    paths. The LLMbots of this world may regularly be crazy and/or just wrong, but they do consistently if not correctly address the
    substance of the prompt.
    There is seriously good natural-language engineering going on here that AI’s critics aren’t paying enough attention
    to.</p>

<p>So I have no patience with those who scoff at today’s technology, accusing it being a glorified Markov chain. Like the
    song says:  Something’s
    happening here! (What it is ain’t exactly clear.)</p>

<p>It helps that in the late teens I saw neural-net pattern-matching at work on real-world problems from close up and
    developed serious respect for what that technology can do; An example is EC2’s
    <a href="https://aws.amazon.com/blogs/compute/evaluating-predictive-scaling-for-amazon-ec2-capacity-optimization/">Predictive Auto
    Scaling</a> (and gosh, it looks like
    <a href="https://www.google.com/search?rls=en&amp;q=predictive+auto+scaling&amp;ie=UTF-8&amp;oe=UTF-8">the competition has it
    too</a>).</p>

<p>And recently, Adobe Lightroom has shipped a pretty awesome “Select Sky” feature. It makes my M2 MacBook
    Pro think hard for a second or two, but I rarely see it miss even an isolated scrap of sky off in the corner of the frame.  It
    allows me, in a picture like this, to make the sky’s brightness echo the water’s.</p>

<p><a href="https://www.tbray.org/ongoing/When/202x/2024/02/25/-big/PXL_20240111_213727870.jpg.html"><img alt="Brightly-lit boats on dark water under a dark sky" title="Brightly-lit boats on dark water under a dark sky" src="https://www.tbray.org/ongoing/When/202x/2024/02/25/PXL_20240111_213727870.png"></a></p>
<p>And of course I’ve heard about success stories in radiology and other disciplines.</p>

<p>Thus, please don’t call me an “AI skeptic” or some such. There is a there there.</p>

<p id="p-4"><span>But…</span> · 
Given that, why do I still think that the flood of money being thrown at this tech is dumb, and that most of it will be lost?
    Partly just because of that flood. When financial decision makers throw loads of money at things they don’t
    understand, lots of it is <em>always</em> lost.</p>

<p>In the Venture-Capital business, that’s an understood part of the business
    cycle; they’re looking to balance that out with a small number of 10x startup wins.
    But when big old insurance companies and airlines and so on are piling in and releasing effusive statements about building
    the company around some new tech voodoo, the outcome, in my experience, is very rarely good.</p>

<p>But let’s be specific.</p>

<p id="p-5"><span>Meaning</span> · 
As I said above, I think the human mind has a large and important language-processing system.  But that’s not all. It’s also
    a (slow, poorly-understood) computer, with access to a medium-large database of facts and recollections, an ultra-slow numeric
    processor, and a facilities for estimation, prediction, speculation, and invention. Let’s group all this stuff together and call
    it “meaning”.</p>

<p>Have a look at <a href="https://aclanthology.org/2020.acl-main.463.pdf">Climbing towards NLU:
    On Meaning, Form, and Understanding in the Age of Data</a> by Emily Bender and Alexander Koller (July 2000). I don’t agree with
    all of it, and it addresses an earlier generation of generative models, but it’s very thought-provoking. It postulates the
    “Octopus Test”, a good variation on the bad old Chinese-Room analogy. It talks usefully about how human language acquisition
    works. A couple of quotes: “It is instructive to look at the past to appreciate this question. Computational linguistics has
    gone through many fashion cycles over the course of its history” and “In this paper, we have argued that in contrast to some
    current hype, meaning cannot be learned from form alone.”</p>

<p>I’m not saying these problems can’t be solved. Software systems can be equipped with databases of facts, and who knows,
    perhaps some day estimation, prediction, speculation, and invention. But it’s not going to be easy.</p>

<p id="p-7"><span>Difficulty</span> · 
I think there’s a useful analogy between the stories AI and of self-driving cars. As I write this, 
    Apple has apparently decided that 
    <a href="https://arstechnica.com/gadgets/2024/02/after-a-decade-of-stops-and-starts-apple-kills-its-electric-car-project">generative 
    AI is easier than shipping an autonomous car</a>. I’m particularly sensitive to this analogy because back around 2010, as the
    first self-driving prototypes were coming into view, I predicted, loudly and in public, that this technology was about to become
    ubiquitous and turn the economy inside out. Ouch.</p>

<p>There’s a pattern: The technologies that really do change the world tend to have strings of successes, producing obvious
    benefits even in their earliest forms, to the extent that geeks load them in the back floor of organizations just to get shit
    done. As they say, “The CIO is the last to know.”</p>

<p>Contrast cryptocurrencies and blockchains, which limped along from year to year, always promising a brilliant future, never
    doing anything useful.  As to the usefulness of self-driving technology, I still think it’s gonna get there, but it’s surrounded
    by a cloud of litigation.</p>

<p>Anyhow, anybody who thinks that it’ll be easy to teach “meaning” (as I described it above) to today’s generative AI is a fool,
    and you shouldn’t give them your money.</p>

<p id="p-6"><span>Money and carbon</span> · 
Another big problem we’re not talking about enough is the cost of generative AI.
    <cite>Nature</cite> offers    
    <a href="https://www.nature.com/articles/d41586-024-00478-x">Generative AI’s environmental costs are soaring — and mostly
    secret</a>. In a Mastodon thread,
    <a href="https://phanpy.social/#/social.v.st/a/109360452395342558">@Quixoticgeek@social.v.st</a> says 
    <a href="https://phanpy.social/#/social.v.st/s/111991430750212364">We need to talk about data centres</a>, and includes a few
    hard and sobering numbers.</p>

<p>Short form: This shit is <em>expensive</em>, in dollars and in carbon load. Nvidia pulled in
    <a href="https://investor.nvidia.com/news/press-release-details/2024/NVIDIA-Announces-Financial-Results-for-Fourth-Quarter-and-Fiscal-2024/">$60.9
    billion in 2023, up 126% from the previous year</a>, and is heading for a $100B/year run rate, while reporting a 75% margin.</p>

<p>Another thing these articles <em>don’t</em> mention is that building, deploying, and running generative-AI systems requires significant
    effort from a small group of people who now apparently constitute the world’s highest-paid cadre of engineers. And good luck
    trying to hire one if you’re a mainstream company where IT is a cost center.</p>

<p>All this means that for the technology to succeed, it not only has to do something useful, but people and businesses will have to
    be ready to pay a significantly high price for that something.</p>

<p>I’m not saying that there’s nothing that qualifies, but I am betting that it’s not in ad-supported territory.</p>

<p>Also, it’s going to have to deal with pushback from unreasonable climate-change resisters like, for example, me.</p>

<p id="p-8"><span>Anyhow…</span> · 
I kind of flipped out, and was motivated to finish this blog piece, when I saw
    <a href="https://www.engadget.com/uk-government-wants-to-use-ai-to-cut-civil-service-jobs-140031159.html">this</a>: “UK
    government wants to use AI to cut civil service jobs: Yes, you read that right.” The idea<span> —</span> to have
    citizen input processed and responded to by an LLM<span> —</span> is hideously toxic and broken; and usefully
    reveals the kind of thinking that makes morally crippled leaders all across our system love this technology.</p>

<p>The road ahead looks bumpy from where I sit. And when the business community wakes up and realizes that replacing
    people with shitty technology doesn’t show up as a positive on the financials after you factor in the consequences of customer
    rage, that’s when the hot air gushes out of the bubble.</p>

<p>It might not take big chunks of InvestorWorld with it. But I’m betting it does.</p>

<hr>


<hr>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dune: Part Two Is the Best Sci-Fi Film of the Decade (110 pts)]]></title>
            <link>https://www.esquire.com/entertainment/movies/a46885292/dune-part-two-review/</link>
            <guid>39553000</guid>
            <pubDate>Thu, 29 Feb 2024 18:22:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.esquire.com/entertainment/movies/a46885292/dune-part-two-review/">https://www.esquire.com/entertainment/movies/a46885292/dune-part-two-review/</a>, See on <a href="https://news.ycombinator.com/item?id=39553000">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-body="standard-article"><p data-journey-content="true" data-node-id="0">You don’t need to be a studio head or a theater owner to realize that everyone in Hollywood has their fingers crossed for <em><a href="https://www.esquire.com/entertainment/movies/a38040674/dune-2-sequel-details/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/movies/a38040674/dune-2-sequel-details/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="Dune: Part Two">Dune: Part Two</a></em>. Sure, the <a href="https://www.esquire.com/entertainment/movies/a46502302/oscars-2024-snubs-surprises/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/movies/a46502302/oscars-2024-snubs-surprises/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="Oscars">Oscars</a> are right around the corner, and there are certainly plenty of great films from last year that are worth celebrating, but so far 2024 has been an absolute shit show at the box office. Business had been bad and the product has been even worse. Granted, we’re only three weeks into February, so the sample size is small, but you know things are rough in Tinseltown when trash like <em>Argylle</em> opens at No. 1 and the latest <a href="https://www.esquire.com/entertainment/movies/g13441903/all-marvel-cinematic-universe-movies-ranked/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/movies/g13441903/all-marvel-cinematic-universe-movies-ranked/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="Marvel">Marvel</a>-affiliated widget off the production line currently sits on Rotten Tomatoes beside a fat green splat and a woeful 13% favorable rating. </p><p data-journey-content="true" data-node-id="1">What does any of this have to do with <em>Dune: Part Two</em>, you may ask. Well, director Denis Villeneuve’s hotly anticipated follow-up to his blissfully weird 2021 adaptation of <a href="https://www.esquire.com/entertainment/books/g38012512/dune-books-in-order/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/books/g38012512/dune-books-in-order/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="Frank Herbert’s talismanic 1968 novel">Frank Herbert’s talismanic 1968 novel</a> was originally slated to open last November. But because of the <a href="https://www.esquire.com/entertainment/a44544249/sag-aftra-actors-strike-consequences-explained/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/a44544249/sag-aftra-actors-strike-consequences-explained/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="actors strike">actors strike</a>, Warner Bros. opted to push the film to March 2024, no doubt so that Timothée Chalamet, Zendaya, and newcomers Florence Pugh and <a href="https://www.esquire.com/entertainment/movies/a46603112/austin-butler-dune-masters-of-the-air-interview-2024/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/movies/a46603112/austin-butler-dune-masters-of-the-air-interview-2024/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="Austin Butler ">Austin Butler </a>could peacock on the red carpet and promote it to the heavens on the late-night talk show circuit. Of course, sci-fi fans bitched and bellyached, as they do. But in retrospect the delay turned out to be a pretty wise move. After all, back in November, all anyone was talking about was <a href="https://www.esquire.com/entertainment/movies/a44495541/barbenheimer-double-feature/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/movies/a44495541/barbenheimer-double-feature/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="Barbenheimer">Barbenheimer</a> and <a href="https://www.esquire.com/entertainment/movies/a46166568/leonard-bernstein-felicia-montealegre-true-story-maestro/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/movies/a46166568/leonard-bernstein-felicia-montealegre-true-story-maestro/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="Bradley Cooper’s fake schnozz">Bradley Cooper’s fake schnozz</a>. There wasn’t a lot of oxygen left in the room. But now? Now the stage couldn’t be better set for the further adventures of Paul Atreides. If the <a href="https://www.esquire.com/entertainment/movies/g46353374/best-movies-2024/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/movies/g46353374/best-movies-2024/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="movies">movies</a> ever needed a savior, it’s right this second. </p><p data-journey-content="true" data-node-id="3">Glancing back at my <a href="https://www.esquire.com/entertainment/movies/a38023702/dune-movie-2021-review/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/movies/a38023702/dune-movie-2021-review/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="review of the first Dune on this site back in 2021">review of the first <em>Dune</em> on this site back in 2021</a>, I noticed that I called it “the best sci-fi movie of the decade.” Hyperbole? Not really. Remember, the decade wasn’t all that old yet. And to be honest, heading in to the new sequel, I stood by it. But walking out was a different story. Because <em>Dune: Part Two</em> is even better than the first film. The stakes somehow feel exponentially higher, the power struggles are even more mythic and Shakespearean, the onscreen world-building is richer and more exotically filigreed, and the visuals are even more epic and dazzling—something I didn’t think was possible. <em>Dune: Part Two</em> isn’t just an embarrassment of narrative and retinal riches; it’s the sort of big-canvas franchise storytelling we haven’t see since <em>The Lord of the Rings </em>came to a close back at the shire. </p><div size="medium" data-embed="body-image" data-lazy-id="P0-8" data-node-id="4"><p><img alt="dune" title="dune" loading="lazy" width="2100" height="1107" decoding="async" data-nimg="1" sizes="100vw" srcset="https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-t3-0054r-high-res-jpeg-65d638c878f8f.jpeg?crop=0.791xw:1.00xh;0.0612xw,0&amp;resize=640:* 640w, https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-t3-0054r-high-res-jpeg-65d638c878f8f.jpeg?crop=0.791xw:1.00xh;0.0612xw,0&amp;resize=768:* 980w, https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-t3-0054r-high-res-jpeg-65d638c878f8f.jpeg?crop=0.791xw:1.00xh;0.0612xw,0&amp;resize=980:* 1120w, https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-t3-0054r-high-res-jpeg-65d638c878f8f.jpeg?crop=0.791xw:1.00xh;0.0612xw,0&amp;resize=980:* 1200w, https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-t3-0054r-high-res-jpeg-65d638c878f8f.jpeg?crop=0.791xw:1.00xh;0.0612xw,0&amp;resize=980:* 1920w" src="https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-t3-0054r-high-res-jpeg-65d638c878f8f.jpeg?crop=0.791xw:1.00xh;0.0612xw,0&amp;resize=980:*"></p><div><figcaption><span>Niko Tavernise/Warner Bros. Entertainment</span></figcaption><p>Now that Paul Atriedes made the leap to battle-tested hero, Chalamet really lets it fly, summoning a more interesting performance.</p></div></div><p data-journey-content="true" data-node-id="5">If you haven’t revisited the first <em>Dune</em> since it left the multiplex, you don’t need to worry. The opening moments of the film get you right back up to speed without sending you to Wikipedia. Picking up almost exactly where things left off in the opening chapter, we’re reminded that the House of Atreides has fallen with the death of <a href="https://www.esquire.com/entertainment/tv/a39520165/oscar-isaac-interview-moon-knight-star-wars/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/tv/a39520165/oscar-isaac-interview-moon-knight-star-wars/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="Oscar Isaac’s">Oscar Isaac’s</a> Duke. The dreaded, pasty-faced Harkonnens have taken over the lucrative spice-mining trade on the desert planet of Arrakis. And our hero, the Duke’s son Paul Atreides (Chalamet), and his mystic mother Lady Jessica of the Bene Gessirit (Rebecca Ferguson), are now embedded with the local Fremen freedom fighters on Arrakis (Javier Bardem, Zendaya, et al) as they wage guerilla warfare on the colonialist Harkonnens—and now, by extension, the Emperor (Christopher Walken) and his daughter and one-day successor Princess Irulan (Florence Pugh), who’s been groomed for power by Charlotte Rampling’s black-veiled Reverend Mother. I’m sure that last sentence probably sounds like a lot of nerdy gibberish to the uninitiated (not to mention armchair grammarians), but then again no one would ever confuse Frank Herbert with Hemingway. Simplicity wasn’t his thing. But thanks to Villeneuve and co-writer Jon Spaihts’s elegant, economical script, it all scans more easily than you’d expect.  </p><p data-journey-content="true" data-node-id="6">For those who have been tracking the fits and starts of the <em>Dune </em>franchise online, I don’t think I’m giving away anything by saying that <em>Dune: Part Two</em> is a middle chapter in the franchise. Yes, like the first film, it ends on a cliffhanger. But this time around, it feels like a steeper and more rewarding cliff. And, unlike most middle chapters of a trilogy, this doesn’t feel like a jerry-rigged bridge connecting two more interesting stories. In fact, a who’s who of welcome new faces arrive on the scene to add layers the first installment only hinted at. As the Emperor, Walken dials down his worst mannered tendencies to simultaneously convey a heavy-is-the-head-that-wears-the-crown world-weariness and a craven sense of realpolitik expediency. The Emperor is old enough to have seen how these power struggles play out and he knows that his time on the throne is finite, but at the end of the day loyalty is only as valuable as it is useful. </p><section data-embed="pullquote" data-lazy-id="P0-9" data-node-id="7"><blockquote><blockquote>Villeneuve shows us the magic of movies—a brand of magic that’s all too often invoked, but all too rarely felt these days.</blockquote></blockquote></section><p data-journey-content="true" data-node-id="8">As his royal daughter, Pugh seems to bristle at the idea of being a pawn in a bigger game and how she’s only being told part of the story. And as Feyd-Rautha Harkonnen, the psychotically cruel nephew of Stellan Skarsgard’s Jabba the Hutt-like Baron Harkonnen, Austin Butler is all but unrecognizable behind his character’s alabaster skin, shaved eyebrows, and heavy metal bondage gear. He looks like a younger version of Robert Blake’s specter in David Lynch’s <em>Lost Highway</em> crossed with the most badass member of the Borg collective. His ambition is limitless. His morals are nonexistent. And his bloodlust is unquenchable. Butler, <a href="https://www.esquire.com/entertainment/movies/a46650896/austin-butler-elvis-presley-voice/" target="_blank" data-vars-ga-outbound-link="https://www.esquire.com/entertainment/movies/a46650896/austin-butler-elvis-presley-voice/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="so good in Baz Luhrmann’s Elvis">so good in Baz Luhrmann’s <em>Elvis</em></a>, goes big and gives us a villain to really hiss at. Despite all of Villeneuve’s pricey, future-shock CGI, Butler may be the director’s best special effect.  </p><p data-journey-content="true" data-node-id="9">While other new additions include Léa Seydoux and Anya Taylor-Joy (no spoilers here), <em>Dune: Part Two</em> is, at its heart, a hero’s journey. And as original as <em>Dune</em> may be, its arc is straight out of Joseph Campbell. Which bring us to Chalamet’s Paul Atreides. As excellent and against-type as the actor was in the first <em>Dune</em>, his character’s evolution couldn’t really skirt the fact that he had to start off a little bit whiny and petulant, not unlike Luke Skywalker in <em>A New Hope</em>. But now that he’s made the leap to battle-tested hero, Chalamet really lets it fly, summoning a more interesting performance. Torn between avenging his slain father and fulfilling the messianic destiny that many of the Fremen (including Bardem’s Stilgar) want from him, Paul takes on an interesting new complexity that brings to mind Willem Dafoe’s fallible, self-doubting Jesus in <em>The Last Temptation of Christ</em>, right down to his push-pull romantic connection with Zendaya’s Chani. Ferguson, meanwhile, is allowed to let her witchy side loose even more this time around. Her Lady Jessica is now pregnant, and she not only speaks with the baby daughter growing inside of her, she uses the unborn as a pawn to manipulate Paul’s next move. It’s a deliciously freaky puppet-master performance that manages to draw you in and creep you out. </p><div size="medium" data-embed="body-image" data-lazy-id="P0-10" data-node-id="10"><p><img alt="dune" title="dune" loading="lazy" width="2000" height="1333" decoding="async" data-nimg="1" sizes="100vw" srcset="https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-38301r-high-res-jpeg-65d6394b666ea.jpg?crop=1.23xw:1.23xh;0,0&amp;resize=640:* 640w, https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-38301r-high-res-jpeg-65d6394b666ea.jpg?crop=1.23xw:1.23xh;0,0&amp;resize=768:* 980w, https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-38301r-high-res-jpeg-65d6394b666ea.jpg?crop=1.23xw:1.23xh;0,0&amp;resize=980:* 1120w, https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-38301r-high-res-jpeg-65d6394b666ea.jpg?crop=1.23xw:1.23xh;0,0&amp;resize=980:* 1200w, https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-38301r-high-res-jpeg-65d6394b666ea.jpg?crop=1.23xw:1.23xh;0,0&amp;resize=980:* 1920w" src="https://hips.hearstapps.com/hmg-prod/images/rev-1-dun2-38301r-high-res-jpeg-65d6394b666ea.jpg?crop=1.23xw:1.23xh;0,0&amp;resize=980:*"></p><div><figcaption><span>Niko Tavernise/Warner Bros. Entertainment</span></figcaption><p>Paul Atreides (Timothée Chalamet) takes on a complexity that brings to mind Willem Dafoe’s fallible Jesus in <em>The Last Temptation of Christ</em>—right down to his push-pull romantic connection with Chani (Zendaya.)</p></div></div><p data-journey-content="true" data-node-id="11">Still, if Villeneuve’s film was just a gallery of characters fighting for power, warring over spice, and spouting metaphorical mumbo jumbo, it wouldn’t be half the movie it is (although it would still be pretty great). No, the director knows that we’ve paid to go on a ride. A deep, philosophical ride to be sure, but still a ride. And <em>Dune: Part Two</em> never forgets that it’s first and foremost a shock-and-awe eye-candy blockbuster. In an era when we go to the movies only to be bombarded over and over again with the same tired visual tropes and clichés, Villeneuve delivers enthralling, shoot-the-works set pieces that feel like high-wire acts of visual poetry and boundless originality. Witnessing Paul learn how to ride a giant sandworm like a rodeo cowboy waterskiing on a high-speed bullet train is as breathlessly thrilling as watching Charlton Heston racing a chariot in <em>Ben-Hur</em>. </p><p data-journey-content="true" data-node-id="12">Like its predecessor dialed up to eleven, <em>Dune: Part Two</em> is a spectacle that you feel with your head and your heart, but it also never lets your eyes take a break for a minute. It’s a film of grandeur that asks a lot of its audience and rewards us for going on its journey. My advice is don’t just see it, see it on as big a screen as you possibly can and just soak it up. Because Villeneuve shows us the magic of movies—a brand of magic that’s all too often invoked, but all too rarely felt these days. He’s given us nothing less than beautiful and bizarre sci-fi masterpiece bursting with big ideas and even bigger visual wonders. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ford EVs gain access to Tesla Superchargers starting today (181 pts)]]></title>
            <link>https://arstechnica.com/cars/2024/02/ford-evs-gain-access-to-tesla-superchargers-starting-today/</link>
            <guid>39552446</guid>
            <pubDate>Thu, 29 Feb 2024 17:36:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/cars/2024/02/ford-evs-gain-access-to-tesla-superchargers-starting-today/">https://arstechnica.com/cars/2024/02/ford-evs-gain-access-to-tesla-superchargers-starting-today/</a>, See on <a href="https://news.ycombinator.com/item?id=39552446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      Happy leap day    —
</h4>
            
            <h2 itemprop="description">The adapter is free if you order it before June 30 or $230 if you wait.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/02/Fast20Charging20Adapter2028NACS29_03-800x533.jpg" alt="someone plugs a tesla charger cable into an adapter to use with a non-tesla EV">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/02/Fast20Charging20Adapter2028NACS29_03.jpg" data-height="5504" data-width="8256">Enlarge</a> <span>/</span> Ford was the first OEM to announce it was switching to J3400, and it's the first automaker to gain access to the Tesla Supercharger network.</p><p>Ford</p></figcaption>  </figure>

  




<!-- cache hit 2:single/related:4ccc423290dac7d71f6097dc34e9eb05 --><!-- empty -->
<p>Today, Ford electric vehicles gained access to the Tesla Supercharger network. Last&nbsp;May, the Blue Oval was <a href="https://arstechnica.com/cars/2023/05/ford-evs-will-get-access-to-teslas-supercharger-network-in-2024/">the first automaker to throw its lot in</a> with what was then called the North American Charging Standard and is now known as J3400. Ford proved to be the first domino falling, and with Stellantis' announcement <a href="https://arstechnica.com/cars/2024/02/stellantis-will-finally-adopt-tesla-style-fast-charger-plug/">earlier this month</a> that it too would move to J3400, the more compact DC fast-charging plug will be the de facto standard in the next couple of years.</p>
<p>Until Ford made the switch, every non-Tesla EV in North America had settled on the <a href="https://arstechnica.com/cars/2022/07/the-ars-technica-guide-to-electric-vehicle-charging/">Combined Charging Standard 1</a> plug (with the exception of the Nissan Leaf, which still uses CHAdeMO). CCS1 and J3400 use the same electronic communication protocols—only the actual plug and socket are different.</p>
<p>But it will take some time for car makers to start building J3400 ports into their EVs. That should begin next year, probably with the introduction of model year 2026. This means that EVs older than MY26 will need to use a passive adapter to mate a J3400 charger cable with a CCS1-equipped EV.</p>
<p>Ford is making the adapter available for free for <a href="https://arstechnica.com/cars/2023/12/revisiting-the-ford-mustang-mach-e-hows-the-pony-ev-doing-3-years-later/">Mustang Mach-E</a> and F-150 Lightning owners as long as they order one by June 30 of this year. After that date, the adapter will cost $230. Ford says that Ford Pro fleet customers can also order a complimentary adapter for their EV (which includes the <a href="https://arstechnica.com/cars/2022/01/weve-driven-fords-other-electric-workhorse-the-2022-e-transit/">E-Transit</a> as well as the Mach-E and Lightning) by contacting their Ford Pro account manager. (Ford Pro will also contact fleet owners by mail in the coming weeks.)</p>                                            
                                                        
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/02/Fast20Charging20Adapter2028NACS29_05.jpg" data-height="5504" data-width="8256" alt="If you've ever used a dongle before, you should know how to use the charger adapter."><img alt="If you've ever used a dongle before, you should know how to use the charger adapter." src="https://cdn.arstechnica.net/wp-content/uploads/2024/02/Fast20Charging20Adapter2028NACS29_05-980x653.jpg" width="980" height="653"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/02/Fast20Charging20Adapter2028NACS29_05.jpg" data-height="5504" data-width="8256">Enlarge</a> <span>/</span> If you've ever used a dongle before, you should know how to use the charger adapter.</p><p>Ford</p></figcaption></figure>
<p>Ford EVs already use the <a href="https://en.wikipedia.org/wiki/ISO_15118">ISO 15118</a> "plug and charge" protocol, which means they give the charger their billing details as part of the electronic handshake, obviating the need to use an app or credit card to start a charging session. And more than 15,000 Tesla chargers will now show up in the BlueOval charge network, which customers can navigate to via the FordPass smartphone app or the Charge Assist app on their infotainment systems.</p>
<p>Ford EVs aren't compatible with every Tesla Supercharger, however. They must be the more recent units, which are able to charge at up to 250 kW, identified by a black collar at the base of the charging plug. Older chargers, which can only charge at up to 150 kW, have a silver collar instead. Since these older chargers won't appear in the FordPass or ChargeAssist apps, it seems prudent for Ford EV drivers to use either of those to find a suitable charger location.</p>
<p>And the adapter is only for DC fast charging, not for Tesla's AC destination chargers. (Ford's plug-in hybrids are only capable of AC charging, and there is no need for them to have access to an adapter, so they will never be able to use a Supercharger.)</p>
<p>I don't imagine an Ars Technica reader having much trouble with fitting the J3400 adapter, but for people with less dongle experience, Ford has <a href="https://www.ford.com/support/how-tos/electric-vehicles/public-charging/how-do-i-use-the-fast-charging-adapter-nacs/">produced a short tutorial film</a>. Perhaps the only potential pain point will be unplugging one's EV and driving off without the adapter—a $230 lesson to learn—but since the Tesla cable needs to be put back in its holster, even that seems pretty unlikely.</p>
<p>"This move will improve the public charging experience by giving our customers even more choice and is a vital part of our growth as an EV brand," said Ford President and CEO Jim Farley.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Neurosurgeon pioneers Alzheimer's, addiction treatments using ultrasound [video] (205 pts)]]></title>
            <link>https://www.youtube.com/watch?v=7BGtVJ3lBdE</link>
            <guid>39551457</guid>
            <pubDate>Thu, 29 Feb 2024 16:24:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=7BGtVJ3lBdE">https://www.youtube.com/watch?v=7BGtVJ3lBdE</a>, See on <a href="https://news.ycombinator.com/item?id=39551457">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: We built the fastest spreadsheet (243 pts)]]></title>
            <link>https://rowzero.io</link>
            <guid>39551064</guid>
            <pubDate>Thu, 29 Feb 2024 15:57:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rowzero.io">https://rowzero.io</a>, See on <a href="https://news.ycombinator.com/item?id=39551064">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><header><nav aria-label="Global navigation"></nav></header><div><svg width="61" height="44" viewBox="0 0 61 44" fill="none" xmlns="http://www.w3.org/2000/svg"> <path fill-rule="evenodd" clip-rule="evenodd" d="M14.1437 0.833984H30.1789L19.4938 17.5202C21.9667 18.2167 24.0632 19.5048 25.7642 21.3848C27.7332 23.5612 28.694 26.3427 28.694 29.664C28.694 33.5108 27.3181 36.7352 24.5707 39.2816C21.9238 41.8241 18.614 43.086 14.696 43.086C10.7667 43.086 7.40967 41.7687 4.67177 39.1322L4.66667 39.1273L4.66167 39.1223C1.99167 36.4523 0.697998 32.7862 0.697998 28.224C0.697998 23.6994 2.60427 18.6111 6.29129 12.9831C6.29174 12.9824 6.2922 12.9817 6.29265 12.981L14.1437 0.833984ZM14.9603 2.33398L7.54988 13.7991L7.54734 13.803C3.93926 19.3101 2.198 24.1091 2.198 28.224C2.198 32.4907 3.3986 35.735 5.71732 38.0566C8.16246 40.4089 11.1398 41.586 14.696 41.586C18.2635 41.586 21.1919 40.4495 23.5362 38.1954L23.5464 38.1856C25.9795 35.9327 27.194 33.1108 27.194 29.664C27.194 26.6492 26.3308 24.2468 24.6518 22.3912C22.9397 20.4988 20.7798 19.2798 18.144 18.7344L17.0765 18.5136L27.4371 2.33398H14.9603ZM44.8157 0.833984H60.8509L50.1658 17.5202C52.6387 18.2167 54.7352 19.5048 56.4361 21.3848C58.4052 23.5612 59.366 26.3427 59.366 29.664C59.366 33.5108 57.9901 36.7352 55.2426 39.2816C52.5958 41.8241 49.286 43.086 45.368 43.086C41.4387 43.086 38.0817 41.7687 35.3438 39.1322L35.3387 39.1273L35.3337 39.1223C32.6637 36.4523 31.37 32.7862 31.37 28.224C31.37 23.6995 33.2762 18.6113 36.9631 12.9833C36.9636 12.9825 36.9641 12.9818 36.9647 12.981L44.8157 0.833984ZM45.6323 2.33398L38.2219 13.7991L38.2193 13.803C34.6113 19.3101 32.87 24.1091 32.87 28.224C32.87 32.4908 34.0707 35.7351 36.3895 38.0568C38.8346 40.4089 41.8119 41.586 45.368 41.586C48.9355 41.586 51.8639 40.4495 54.2082 38.1954L54.2184 38.1856C56.6515 35.9327 57.866 33.1108 57.866 29.664C57.866 26.6492 57.0028 24.2468 55.3238 22.3912C53.6117 20.4988 51.4518 19.2798 48.816 18.7344L47.7485 18.5136L58.1091 2.33398H45.6323Z" fill="black"></path> </svg><h3>Row Zero is an impressive feat of engineering, making big data feel small in a familiar spreadsheet interface.</h3><p><span>Wes McKinney</span><br>Creator of Pandas and Apache Arrow<!-- --> <!-- --> </p></div><div id="use-cases"><h2>Use cases</h2><p>Give your business access to cloud data sources in a tool they already know how to use. Explore hundreds of millions of rows, perform ad-hoc analyses, and monitor trends from the comfort of a spreadsheet.</p></div><section><div id="features"><p><img alt="Power and speed icon" srcset="https://rz-web.vercel.app/images/icon-rocket.svg?w=64 1x, https://rz-web.vercel.app/images/icon-rocket.svg?w=128 2x" src="https://rz-web.vercel.app/images/icon-rocket.svg?w=128" width="50" height="50" decoding="async" data-nimg="1" loading="lazy"></p><h3>Power and speed</h3><ul><li>Write Excel-compatible formulas to process hundreds of millions of rows instantly</li><li>No more slow dashboards - filter, sort, pivot, and plot in milliseconds</li><li>Upload multi-GB CSV and JSONL files - no need for databases or expensive BI tools</li></ul></div><div id="features"><p><img alt="Familiar UI icon" srcset="https://rz-web.vercel.app/images/icon-spreadsheet.svg?w=64 1x, https://rz-web.vercel.app/images/icon-spreadsheet.svg?w=128 2x" src="https://rz-web.vercel.app/images/icon-spreadsheet.svg?w=128" width="50" height="50" decoding="async" data-nimg="1" loading="lazy"></p><h3>Familiar UI</h3><ul><li>Excel compatible - execute VLOOKUPS, XLOOKUPS, COUNTIFS, INDEX MATCHs,<!-- --> <a href="https://rowzero.io/docs/spreadsheet-functions">and more</a></li><li>Filter, sort, pivot, and plot the way you already know how - no BI tool training required</li><li>Enable your business teams with an analysis tool they already know how to use</li></ul></div><div id="features"><p><img alt="Connect to any data source icon" srcset="https://rz-web.vercel.app/images/icon-connection.svg?w=64 1x, https://rz-web.vercel.app/images/icon-connection.svg?w=128 2x" src="https://rz-web.vercel.app/images/icon-connection.svg?w=128" width="50" height="50" decoding="async" data-nimg="1" loading="lazy"></p><h3>Connect to any data source</h3><ul><li>Row Zero runs in the cloud and connects directly to any data source</li><li>Connect data warehouses, data lakes, APIs, and any other service to build models on live data</li><li>Save time and reduce mistakes by connecting to live data instead of copy/pasting</li></ul></div><div id="features"><p><img alt="Sharing and collaboration icon" srcset="https://rz-web.vercel.app/images/icon-collaboration.svg?w=64 1x, https://rz-web.vercel.app/images/icon-collaboration.svg?w=128 2x" src="https://rz-web.vercel.app/images/icon-collaboration.svg?w=128" width="50" height="50" decoding="async" data-nimg="1" loading="lazy"></p><h3>Sharing and collaboration</h3><ul><li>Collaborate in real time - Share each workbook with editor and viewer permissions</li><li>Govern your data. No more Sharepoint or untraceable emails with .xlsx attachments </li><li>Provide refresh permissions (without revealing credentials) so business teams can build models off live data</li><li>Use the Follow feature when presenting to walk team members through your analysis</li></ul></div><div id="features"><p><img alt="Python icon" srcset="https://rz-web.vercel.app/images/icon-code.svg?w=64 1x, https://rz-web.vercel.app/images/icon-code.svg?w=128 2x" src="https://rz-web.vercel.app/images/icon-code.svg?w=128" width="50" height="50" decoding="async" data-nimg="1" loading="lazy"></p><h3>Python</h3><ul><li>Decompose long spreadsheet formulas with Python helper functions to improve readability and prevent costly errors</li><li>Import popular Python modules like <span>pandas</span>,<!-- --> <span>numpy</span>, <span>scipy</span>, and <span>yfinance</span>, to perform complex analysis in a familiar tool</li><li>Never write VBA again</li></ul></div></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The internet feels fake now. It's all just staged videos and marketing (121 pts)]]></title>
            <link>https://old.reddit.com/r/Millennials/comments/1b301qj/the_internet_feels_fake_now_its_all_just_staged/</link>
            <guid>39551035</guid>
            <pubDate>Thu, 29 Feb 2024 15:56:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/Millennials/comments/1b301qj/the_internet_feels_fake_now_its_all_just_staged/">https://old.reddit.com/r/Millennials/comments/1b301qj/the_internet_feels_fake_now_its_all_just_staged/</a>, See on <a href="https://news.ycombinator.com/item?id=39551035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>All things Generation Y (Millennials).</p>

<p>Millennials are the generation that were born between 1981 to 1996. Our generation comes after Gen X, and before Gen Z.</p>

<p>Check out our sister subreddit, <a href="https://old.reddit.com/r/Zillennials">/r/Zillennials</a>. As well as <a href="https://old.reddit.com/r/decadeology">r/decadeology</a> for more in depth cultural analysis. </p>

<p>Zillennials are the micro-generation between Gen Z and Millennials, years ~1994-1999. Join the discord server here --&gt; <a href="https://discord.gg/Se8Nr5JMbU">https://discord.gg/Se8Nr5JMbU</a></p>

<p>All generations are welcome to post &amp; comment.</p>

<hr>

<p>Rules:</p>

<ul>
<li><p>Treat Others Like A Human Being
Basically, just be cool, and you'll avoid 90% of potential problems. Remember that, with exception of bots/spam, you are talking to a human on the other end, and you should talk to that person as you would in real life.</p></li>
<li><p>No Discrimination, Mud Slinging, or Hate Speech
Direct mistreatment of users for their race, religion, sexuality, and other forms of discrimination are strictly prohibited. Politics are allowed but only for talking about in a civil manner.</p></li>
<li><p>No Personal Attacks or Harrassment
Do not personally attack others, harass others, stalk others, or leak their personal information (Doxxing).</p></li>
<li><p>No Spamming or Low-Level Content
Any instances of spamming, trolling, clearly repetitive content, overtly low-level content, and negatively provocative content will be removed. This is to maintain user experience and to keep the subreddit running smoothly.</p></li>
<li><p>Subreddit Content Should Lean Towards Positive or Nostalgia Focused Discussion
Mostly this serves as a guideline but the content on this subreddit should be more geared towards Millennial nostalgia and the positive aspects of our generation.</p></li>
<li><p>No NSFW Content
Do not post gore, nudity, pornography, links to NSFW sites, etc.</p></li>
<li><p>No Personal Information
Do not share another person's personal information. Anything you share about yourself you share at your own risk. Always keep the safety of yourself and others in mind.</p></li>
<li><p>No Gatekeeping
All forms of gatekeeping will be deleted and the perpetrator will be warned. Further gatekeeping will result in a ban on the perpetrator. It's fine to discuss differences and observations in a civil manner.</p></li>
<li><p>No Discussing Definitions / "What Generation am I?" / "When do Millennials start and end?" / "Who is considered a Millennial?" posts
This has been discussed countless times already. Otherwise, you're free to discuss whatever it is on <a href="https://old.reddit.com/r/generationology">r/generationology</a> or <a href="https://old.reddit.com/r/decadeology">r/decadeology</a>.</p></li>
<li><p>No "surveys" / "data" posts
This is a recurring theme of past posts, questions about "What do Millennials see in a brand?" or "What are your opinions on _____ brand" are not allowed. There are plenty of other research subreddits for these types of posts.</p></li>
<li><p>No Politics
Our community is <strong>not</strong> <a href="https://old.reddit.com/r/politics">r/politics</a> or <a href="https://old.reddit.com/r/antiwork">r/antiwork</a>. For these types of discussions please use other subs. This rule has been implemented to avoid toxic users and discussions here. This may be lifted in the future at some point.</p></li>
<li><p>No discussion of Palestinian v. Israeli conflict.
There are countless different subs to discuss this controversial event happening. To curb repetitive and toxic posts we are NOT allowing this topic here.</p></li>
</ul>

<hr>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unearthing the oldest forest on Earth (104 pts)]]></title>
            <link>https://worldsensorium.com/unearthing-the-oldest-forest-on-earth-two-hours-from-new-york-city-you-can-travel-back-nearly-400-million-years/</link>
            <guid>39550202</guid>
            <pubDate>Thu, 29 Feb 2024 15:01:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://worldsensorium.com/unearthing-the-oldest-forest-on-earth-two-hours-from-new-york-city-you-can-travel-back-nearly-400-million-years/">https://worldsensorium.com/unearthing-the-oldest-forest-on-earth-two-hours-from-new-york-city-you-can-travel-back-nearly-400-million-years/</a>, See on <a href="https://news.ycombinator.com/item?id=39550202">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="g-intro">

                <article class="page" id="post-13733">

    
                
        
                
                        <section>

                                                
                                








<div data-awb-type="image" data-awb-parallax="scroll" data-awb-parallax-speed="0.5" data-awb-parallax-mobile="false" data-awb-image-background-size="cover" data-awb-image-background-position="45% 46%"><p><img decoding="async" src="https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM.png" width="1714" height="1302" srcset="https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM.png 1714w, https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM-300x228.png 300w, https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM-1024x778.png 1024w, https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM-768x583.png 768w, https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM-1536x1167.png 1536w, https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM-500x380.png 500w, https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM-800x608.png 800w, https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM-1280x972.png 1280w, https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM-1320x1003.png 1320w, https://worldsensorium.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-25-at-11.53.36-PM-600x456.png 600w" sizes="(max-width: 1714px) 100vw, 1714px"></p></div>







<figure><blockquote><p>Two Hours from New York City You Can Travel Back Nearly 400 Million Years<br></p></blockquote></figure>







<p><strong>By Gayil Nalls</strong></p>





<p><a href="https://worldsensorium.com/join-us/" data-type="page" data-id="45"><strong>Sign up for our monthly newsletter!</strong></a></p>







<div>
<p>The exploration of an ancient, fossilized forest in a quarry in Cairo, New York, in the region of the Catskill-Hudson Valley, initiated years ago, has revealed a treasure trove of evidence showcasing early plant life coexisting with dinosaurs. The footprint of the ancient forest situated at the bottom of a quarry has become a crucial archaeological window into a distant past. </p>



<p>The site was discovered by Charles Ver Straeten, curator of sedimentary rocks at the New York State Museum, and his colleagues. International researchers came to study the area, led by William Stein, emeritus professor of biological sciences at&nbsp;Binghamton University, and Christopher Berry a paleobotanist at Cardiff University in the UK. They carefully examined fossils of various plants and trees and made the groundbreaking discovery that this forest is the oldest ever found on Earth, predating even well-known ancient forests like the Amazon rainforest and the Yakushima Forest in Japan.</p>



<figure><blockquote><p>The site quickly unveiled its secrets, offering a glimpse into a world that thrived 386 million years ago, the Middle&nbsp;Devonian Epoch (398-385 million years ago), when most life on Earth was still in the ocean. </p></blockquote></figure>



<p>The well-preserved ancient forest, once a thriving ecosystem, presents a unique opportunity for scientists to delve into the mysteries of Earth’s ancient past. </p>



<figure><video controls="" src="https://media.cnn.com/api/v1/loops/stellar/prod/191220161528-20191219-cairo-drone-2.mp4?q=h_402,w_718,x_0,y_0"></video></figure>



<p>The researcher’s primary focus was to examine the fossils of plants and trees found within the ancient forest, discovering evidence of plentiful early plant life. Through meticulous analysis of the impressions, they were able to identify and catalog a diverse array of plant life that flourished during the time of dinosaurs. The discovery included ferns, palms, and tree species that reproduce using spores, and are the ancestors to seed plants. The findings not only provide valuable insights into the biodiversity of the prehistoric world and expand our understanding of ancient ecosystems but also challenge existing notions about the nature of plant life during the dinosaur era. </p>



<p>The groundbreaking aspect of this discovery lies in the  determination of the forest’s age. Advanced dating techniques and scrutiny of the fossilized remains led scientists to conclude that the Cairo forest is the oldest ever found on Earth – 140 million years older than the first dinosaurs, surpassing the age of renowned ancient forests, including the Amazon rainforest and the Yakushima Forest in Japan. </p>



<figure><img decoding="async" src="https://media.cnn.com/api/v1/images/stellar/prod/191220110602-04-fossil-trees-new-york-worlds-oldest-trnd-scn.jpg?q=w_1160,c_fill/f_webp" alt=""><figcaption>Scientists stand on the edge of an Archaeopteris tree root system. They put the bucket where they think the tree’s trunk was located.&nbsp;<br>Charles Ver Straeten</figcaption></figure>



<p>This revelation not only adds a new chapter to the geological history of our planet but also raises questions about the evolution of plant life over time. The scientists think the findings will allow them to understand specifics about how plants and trees draw down carbon dioxide from the atmosphere. </p>



<p>The unearthing of the ancient forest floor in Cairo, New York, stands as a testament to the continuous exploration of our planet’s rich history. This discovery not only sheds light on the diverse plant life that coexisted with dinosaurs but also challenges our preconceptions about the age of ancient forests. As researchers delve deeper into the mysteries held within the fossilized remains, the story of Earth’s oldest forest continues to unfold, offering a unique glimpse into the wonders of our planet’s distant past. </p>



<figure><img decoding="async" src="https://media.cnn.com/api/v1/images/stellar/prod/191220110655-06-fossil-trees-new-york-worlds-oldest-trnd-scn.jpg?q=w_1110,c_fill/f_webp" alt=""><figcaption>Researchers carefully clean and map the surface of the ancient forest discovered in Cairo, New York.&nbsp;<br>William Stein</figcaption></figure>



<p>Exploring ancient forests not only allows us to peer into the past but also imparts valuable insights for the present and future, according to the scientists. Amid contemporary challenges like deforestation, comprehending the biodiversity and dynamics of ancient ecosystems can contribute significantly to understanding the transitional period we are in and provide insights for conservation endeavors.</p>



<p>*The top photograph is an overview of a well-preserved Archaeopteris root system on the left, and a possible Stigmarian Isoetalean lycopsid on the right.</p>







<p><br>© Photographs and videos are courtesy of William E. Stein,<br>Binghamton University&nbsp;Emeritus Professor of Biological Sciences</p>



<p>For more details, refer to:<br><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031018210006565">https://www.sciencedirect.com/science/article/abs/pii/S0031018210006565</a><br><a href="https://www.cell.com/current-biology/pdf/S0960-9822(19)31569-6.pdf">https://www.cell.com/current-biology/pdf/S0960-9822(19)31569-6.pdf</a></p>











<p><strong>Gayil Nalls</strong>, Ph.D., is the creator of World Sensorium and founder of the World Sensorium Conservancy.</p>




</div>










<div data-post-id="4966"><figure><img fetchpriority="high" decoding="async" width="774" height="1024" src="https://worldsensorium.com/wp-content/uploads/2023/02/Plantings-Annual-2023-a-774x1024.jpg" alt="" srcset="https://worldsensorium.com/wp-content/uploads/2023/02/Plantings-Annual-2023-a-774x1024.jpg 774w, https://worldsensorium.com/wp-content/uploads/2023/02/Plantings-Annual-2023-a-600x793.jpg 600w, https://worldsensorium.com/wp-content/uploads/2023/02/Plantings-Annual-2023-a-227x300.jpg 227w, https://worldsensorium.com/wp-content/uploads/2023/02/Plantings-Annual-2023-a-768x1016.jpg 768w, https://worldsensorium.com/wp-content/uploads/2023/02/Plantings-Annual-2023-a-500x661.jpg 500w, https://worldsensorium.com/wp-content/uploads/2023/02/Plantings-Annual-2023-a-800x1058.jpg 800w, https://worldsensorium.com/wp-content/uploads/2023/02/Plantings-Annual-2023-a.jpg 1089w" sizes="(max-width: 774px) 100vw, 774px"></figure><div>
<h2 data-kb-block="kb-adv-heading4966_eab961-c5">Plantings Print Annual 2023</h2>



<p><strong>Do you have the 2023 <em>Plantings</em> print annual?</strong></p>



<p>Plantings cultivates innovative ideas and fresh perspectives, nurturing the global conservation community. Our readers find inspiration in forward-thinking individuals and approaches dedicated to fostering a better life for the planet and all its inhabitants.</p>



<p>The 2023 edition of Plantings is available in our store for shipping.</p>




</div></div>


                
                
                                
                
            </section>
            
                                    
        
    
</article>

            </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A lock-free ring-buffer with contiguous reservations (2019) (188 pts)]]></title>
            <link>https://ferrous-systems.com/blog/lock-free-ring-buffer/</link>
            <guid>39550124</guid>
            <pubDate>Thu, 29 Feb 2024 14:57:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ferrous-systems.com/blog/lock-free-ring-buffer/">https://ferrous-systems.com/blog/lock-free-ring-buffer/</a>, See on <a href="https://news.ycombinator.com/item?id=39550124">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>This is the story of how <a href="https://twitter.com/utaal">Andrea Lattuada</a> (PhD student at ETH Zurich) and <a href="https://twitter.com/bitshiftmask">James Munns</a> (from Ferrous Systems) designed and implemented (two versions!) of an high-perf lock-free ring-buffer for cross-thread communication. If any of those words look scary to you, don't fret, we'll explain everything from the basics.</p>

<p><em>This post is cross-posted on Andrea Lattuada's <a href="http://andrea.lattuada.me/blog/2019/the-design-and-implementation-of-a-lock-free-ring-buffer-with-contiguous-reservations.html">blog</a>.</em></p>

<p>This post is for you if you're interested in (safe!) concurrency, systems programming, and cool ways to write efficient systems software. If you've never written a thread-safe data structure, this post may be a great way to get started!</p>

<h2 id="circular-buffers">Circular buffers</h2>

<p>A <a href="https://www.codeproject.com/Articles/3479/The-Bip-Buffer-The-Circular-Buffer-with-a-Twist"><em>BipBuffer</em></a> is a bi-partite circular buffer that always supports writing a contiguous chunk of data, instead of potentially splitting a write in two chunks when it straddles the buffer's boundaries.</p>

<p>Circular buffers are a common primitive for asynchronous (inter- or intra- thread) communication. Let's start with a very abstract, idealised view of the circular buffer interface, and then consider real-world constraints one by one, till we get to the <em>BipBuffer</em> design.</p>

<h3 id="an-idealised-infinite-buffer">An idealised infinite buffer</h3>

<p>A writer (producer) and a reader (consumer) want to communicate, and have access to the same, contiguous, and infinite array. They both keep a bookmark of which part of the array they've (respectively) written and read. They start with these <code>write</code> and <code>read</code> pointers aligned.</p>

<p>When the writer wants to send data, it appends it after the <code>write</code> pointer and then moves the pointer to the end of the newly written chunk. The reader inspects the <code>write</code> pointer at its leisure (asynchronously). When the <code>write</code> pointer has advanced further than the <code>read</code> pointer, the reader can consume and act on the available data. Once that's done, it moves the <code>read</code> pointer forwards to keep track of which part of the buffer it has already processed.</p>

<p><img src="https://i.imgur.com/jncR6qd.png" alt=""></p>

<p>The reader will never attempt to read past the <code>write</code> pointer, because there's no guarantee there's valid data there (i.e. that the writer has put anything there). This also means that the <code>read</code> pointer can never overtake <code>write</code>. For now, we're assuming an ideal memory system that's always coherent and where writes are visible immediately and sequentially.</p>

<h3 id="a-bounded-circular-buffer">A bounded circular buffer</h3>

<p>Computers don't have magic infinite buffers. We have to allocate a finite amount of memory to use for potentially infinite communication between the writer and reader. In a circular buffer, the <code>write</code> pointer can wrap around the boundaries of the buffer when it reaches the end.</p>

<p>When new data arrives and the <code>write</code> pointer is close to the end, it splits the write in two chunks: one for the remaining buffer space at the end, and one for the remaining data at the beginning. Note that, if the <code>read</code> pointer is still close to the beginning, this has the potential of clobbering data that hasn't yet been processed by the reader. For this reason, the <code>write</code> pointer is not allowed to overtake <code>read</code> after it has wrapped around.</p>

<p><img src="https://i.imgur.com/zfwNfD0.png" alt=""></p>

<p>We end up with two possible memory configurations:</p>

<ol>
  <li><code>write</code> leads and <code>read</code> follows (<code>write</code> ≥ <code>read</code>), the valid data (written, but not yet processed by the reader) is in the section of the buffer after <code>read</code> and before <code>write</code>;</li>
  <li><code>read</code> leads and <code>write</code> follows (<code>read</code> &gt; <code>write</code>), the valid data is after <code>read</code>, till the end, and from the start of the buffer till <code>write</code>.</li>
</ol>

<p>Note that we disallow <code>read</code> == <code>write</code> in the second case, as this would be ambiguous: while <code>read</code> can catch up to <code>write</code>, after a wraparound <code>write</code> has to stay one step behind <code>read</code> to indicate that we're in case 2 instead of case 1.</p>

<p>We repeatedly move from configuration 1 to 2, then back to 1: when <code>read</code> reaches the end of the buffer, it can also wrap around to continue reading at the start.</p>

<h3 id="contiguous-writesreads">Contiguous writes/reads</h3>

<p>This is all great, but what if we have chunks of data that should remain contiguous in memory when written to the buffer? Look here, there's a new message to be written, but it doesn't fit in the remaining buffer space after <code>write</code>.</p>

<p><img src="https://i.imgur.com/szHSC8M.png" alt=""></p>

<p>If, for whatever reason, we aren't allowed to split this write in two, we're stuck. Maybe we can just wait for <code>read</code> to move forwards, and place our new data in a single chunk at the start of the buffer? Well, in fact, yea. But there's a caveat.</p>

<p><img src="https://i.imgur.com/Lb8C3sq.png" alt=""></p>

<p>We've broken the property in configuration 2 earlier: there's a section of the buffer that's between <code>read</code> and the end of the buffer, but doesn't contain any valid data. If we didn't do anything about it, the reader would keep consuming data, moving <code>read</code> forwards, and it would be oblivious to the fact that at some point it would be reading a section of the buffer that doesn't contain any valid information.</p>

<h2 id="a-hardware-interlude">A Hardware Interlude</h2>

<p>Previously we asked:</p>

<blockquote>
  <p>What if we have chunks of data that should remain contiguous in memory when written to the buffer?</p>
</blockquote>

<p>But when would we actually require that data be read or written to in a contiguous manner?</p>

<h3 id="dma---direct-memory-access">DMA - Direct Memory Access</h3>

<p>In embedded microcontroller systems, it is common to have a single core CPU. Instead of having multiple cores, they have a set of features referred to as Memory Mapped Peripherals. These Peripherals act as hardware accelerators for specific behaviors, such as sending or receiving data from a serial port.</p>

<p>In order to minimize the amount of time necessary for the CPU to manually copy data from one place to another, these Peripherals can be configured to perform an action completely autonomously, streaming data to or from a section of memory on the CPU. This action of the hardware directly reading from or writing to the memory is called DMA, or Direct Memory Access.</p>

<p>Instead of reading or writing one byte at a time to the Serial Port, the CPU can instead start the transfer, and when it is complete, process a chunk of bytes at a time. This allows for less time waiting, and is generally a more efficient method of processing data.</p>

<p>A typical usage of DMA (called a DMA transaction) looks like this:</p>

<ol>
  <li>The CPU allocates N bytes of memory to be used for DMA</li>
  <li>The CPU instructs the peripheral, such as a serial port, to receive N bytes of data, and to place those bytes in the memory allocated in step 1</li>
  <li>Once the peripheral is configured, the CPU resumes performing other actions, and the Serial Port begins filling data into the memory buffer as it is received</li>
  <li>When the Serial Port has received all N bytes requested, it notifies the CPU, and stops receiving data</li>
  <li>The CPU may now process all N bytes requested, and if necessary, repeat the process at step one</li>
</ol>

<p>Although we often only have one CPU core in most microcontrollers, we can think of these DMA actors as their own thread. They are able to operate independently of the main CPU's actions, and read and write memory based on their own needs. In these microcontroller systems, there can be tens or hundreds of these hardware actors, all operating in parallel!</p>

<h3 id="stackless-operation">Stackless Operation</h3>

<p>In step one of DMA procedure above, we talked about allocating N bytes of memory. On a non-embedded system, this would generally be done by allocating space on the heap - a <code>Box</code> in Rust, or using <code>malloc()</code> in C. In lightweight or timing critical embedded systems, it is uncommon to have a heap. Instead, all memory must be statically allocated, or allocated through the use of the stack.</p>

<p>In these systems, data structures such as Circular Buffers are used to work around these limitations. A fixed amount of space is reserved for use, and a dynamic amount of data within a fixed maximum region is used to simulate a dynamic memory region.</p>

<p>Unfortunately, these DMA transactions do not understand the concept of a circular buffer. They are only aware of a pointer to where the memory region starts, and how many bytes to use from the starting pointer. This means that a normal circular buffer where the data region could wrap around would not work for DMA transfers.</p>

<h3 id="but-why-is-dma-so-important">But why is DMA so important?</h3>

<p>For operations used with DMA, the speed at which bytes are transferred is often many orders of magnitude slower than the operation of the CPU itself. For a 32 bit ARM CPU, copying 4 bytes from RAM takes a single cycle. In a 64MHz CPU, this means it will take 15.6 nanoseconds to copy these four bytes.</p>

<p>A typical serial port configuration is "115200 8N1", which means 115,200 baud (or raw bits on the wire per second), with no parity, and 1 stop bit. This means that for every data byte sent, there will be 8 data bits, 1 unused parity bit, and 1 stop bit, to signal the end of the byte, sent over the wire.</p>

<p>This means that we will need 40 bits on the wire to receive a 4 data bytes. At 115,200 bits on the wire per second, this means it will take 347,220 nanoseconds to receive the same four bytes, taking <strong>22,222 times as long</strong> as it takes our CPU to copy the same amount of data!</p>

<p>Instead of making our CPU waste all of this time waiting around, we allow the hardware to manage the simple sending and receiving process, allowing our CPU to either process other important tasks, or go into sleep mode, saving power or battery life.</p>

<h3 id="from-embedded-to-datacenters">From embedded to datacenters</h3>

<p>People writing high-performance application for datacenter grade servers have long realised this is also true for the high-grade, power-hungry CPUs they use.</p>

<p>Modern, efficient network stacks for servers use similar DMA techniques to offload all of this work to the network card, so that valuable CPU time can be spent running data-crunching applications.</p>

<h3 id="a-fork-in-the-road">A fork in the road</h3>

<p>Here's where the original BipBuffer design decides to maintain two "regions" of valid data, one at the start and one at the end of the buffer: this way it can keep track of which sections of the buffers contain valid data. Have a look at the <a href="https://www.codeproject.com/Articles/3479/The-Bip-Buffer-The-Circular-Buffer-with-a-Twist">BipBuffer</a> blog post on CodeProject for details on how this works.</p>

<p>The design based on two regions works great in a single threaded environment, but requires swapping the references to two regions when the rightmost one is depleted. This is tricky to do without explicit locking (mutexes) for cases in which the writer and reader reside on different threads.</p>

<p>Our use case is communication between two concurrent threads of control: either two actual OS threads, or a main thread of control and an interrupt handler in embedded or a device driver. This is where our design takes inspiration from the <em>BipBuffer</em>, but goes in a different direction.</p>

<h2 id="concurrency-design">Concurrency design</h2>

<p>A common strategy to reduce the amount of coordination that needs to happen between the two threads (writer, reader) is to associate each coordination variable (pointer) with a single thread that has exclusive write access to it. This also happens to simplify reasoning about the design, because it's always clear who's in charge of changing which variable.</p>

<p>So, let's start with a simple circular buffer that has the <code>write</code> and <code>read</code> pointers from before. The writer is the only one who ever changes <code>write</code>, and the reader is the only one who increments <code>read</code>.</p>

<p><img src="https://i.imgur.com/zfwNfD0.png" alt=""></p>

<p>So far so good. Each thread is only concerned with writing to one variable, and reading from the other.</p>

<h3 id="high-watermark-for-data">High watermark for data</h3>

<p>Now let's re-introduce the requirement that the data written may need to be contiguous. If there's no space available at the end of the buffer, the writer wraps around and writes the whole contiguous chunk at the start.</p>

<p><img src="https://i.imgur.com/Lb8C3sq.png" alt=""></p>

<p>As we've seen, we need a way to tell the reader which part of the buffer is valid, and which was skipped to be able to write a single contiguous chunk. We're tracking the high watermark of valid data in the buffer, so what about a <code>watermark</code> pointer that gets written when the writer wraps around and leaves empty space at the end?</p>

<p>Going back to our idealised infinite buffer from before, here's what things would look like. Whenever the valid region isn't split in two parts (at the beginning and end of the actual buffer) we simply ned to track the write and read pointers, as before. On the other hand, when valid data wraps around the buffer, we leave an artificial "hole" in the "infinite buffer" representation. The <code>watermark</code> lets us keep track of where the "hole" starts, and the end of the physical buffers marks the end.</p>

<p><img src="https://i.imgur.com/lZrudcU.png" alt=""></p>



<p>We have all the necessary elements for our non-blocking implementation. We start with the <code>write</code> and <code>read</code> pointers aligned at the start of the buffer and the <code>watermark</code> aligned with the end.</p>

<p><img src="https://i.imgur.com/nS0tBm1.png" alt=""></p>

<div><pre><code><span>struct</span> <span>ContiguousAsyncBuffer</span> <span>{</span>
  <span>buf</span><span>:</span> <span>*</span><span>mut</span> <span>u8</span><span>,</span>
  <span>len</span><span>:</span> <span>usize</span><span>,</span>
  <span>read</span><span>:</span> <span>AtomicUsize</span><span>,</span>
  <span>write</span><span>:</span> <span>AtomicUsize</span><span>,</span>
  <span>watermark</span><span>:</span> <span>AtomicUsize</span><span>,</span>
<span>}</span>
</code></pre></div>
<p>We use <code>AtomicUsize</code> to let the two threads read and update the pointers concurrently and safely. The writer/sender thread is in charge of <code>write</code> and <code>watermark</code>, the reader/receiver is in charge of <code>read</code>. This is important! Contended writes from multiple threads on the same memory location are a lot harder for the CPU's cache coherence protocol to handle, and will cost latency and throughput.
What's more, it's a lot easier to reason about correctness of these concurrent protocols if each of the shared pointers are always written by a certain thread (their "owner").</p>

<h3 id="writing">Writing</h3>

<p>As long as there's enough contiguous buffer space before the end of the physical buffer, as new data arrives (of length <code>write_len</code>) the sender thread moves the <code>write</code> pointer forwards to signal that a new chunk of the buffer is now valid and can be read.</p>

<div><pre><code><span>// [writer thread]</span>
<span>buffer</span><span>.write</span><span>.store</span><span>(</span><span>buffer</span><span>.write</span><span>.load</span><span>()</span> <span>+</span> <span>write_len</span><span>)</span>
</code></pre></div>
<p>When new data arrives and the <code>write</code> pointer is close to the end, it moves the watermark to its current location, then wraps around. Again, if the <code>read</code> pointer is still close to the beginning, this has the potential of clobbering data that hasn't yet been processed by the reader. For this reason, the <code>write</code> pointer is not allowed to overtake <code>read</code> after it has wrapped around.</p>

<div><pre><code><span>// [writer thread]</span>
<span>if</span> <span>buffer</span><span>.len</span><span>.saturating_sub</span><span>(</span><span>buffer</span><span>.write</span><span>.load</span><span>())</span> <span>&gt;=</span> <span>write_len</span> <span>{</span>
  <span>// not shown: check `read` to make sure there's enough free room</span>
  <span>buffer</span><span>.watermark</span><span>.store</span><span>(</span><span>buffer</span><span>.write</span><span>.load</span><span>()</span> <span>+</span> <span>write_len</span><span>);</span>
  <span>buffer</span><span>.write</span><span>.store</span><span>(</span><span>buffer</span><span>.write</span><span>.load</span><span>()</span> <span>+</span> <span>write_len</span><span>);</span>
<span>}</span> <span>else</span> <span>{</span> <span>// not enough space, wrap around</span>
  <span>// not shown: check `read` to make sure there's enough free room at the beginning of the buffer</span>
  <span>buffer</span><span>.watermark</span><span>.store</span><span>(</span><span>buffer</span><span>.write</span><span>.load</span><span>());</span>
  <span>buffer</span><span>.write</span><span>.store</span><span>(</span><span>0</span> <span>+</span> <span>write_len</span><span>);</span>
<span>}</span>
</code></pre></div>
<p>You may have noticed that the writer also pushes the <code>watermark</code> forward when there's room at the end of the buffer. We need to do this because we may have moved it back on a previous wrap-around and we want to avoid the reader now misinterpreting it as a sign that there's a "hole" at the end.</p>

<h3 id="reading">Reading</h3>

<p>We end up again with two possible memory configurations:</p>

<ol>
  <li><code>write</code> leads and <code>read</code> follows (<code>write</code> ≥ <code>read</code>), the valid data (written, but not yet processed by the reader) is in the section of the buffer after <code>read</code> and before <code>write</code>;</li>
  <li><code>read</code> leads and <code>write</code> follows (<code>read</code> &gt; <code>write</code>), the valid data is after <code>read</code>, till the <code>watermark</code>, and from the start of the buffer till <code>write</code>.</li>
</ol>

<p><img src="https://i.imgur.com/vgqghy7.png" alt=""></p>

<p>This makes the reader thread's logic simple: read till you hit the <code>write</code> pointer, or the <code>watermark</code>, and update the <code>read</code> pointer accordingly.</p>

<h3 id="a-note-on-memory-ordering">A note on memory ordering</h3>

<p>Some of you may have noticed that all of our calls to <code>load</code> don't take arguments and our calls to <code>store</code> take a single argument, the new value for the <code>AtomicBool</code>. This isn't valid code, of course. The real signatures take another argument: <code>ordering: Ordering</code>.
This instructs llvm on how to emit the proper memory fences and <code>sync</code> instructions to drive the cache coherence and synchronization mechanisms built into the CPUs.</p>

<p>The safe thing to do here is to always choose <code>Ordering::SeqCst</code>, "sequential consistency", which provides the strongest guarantees. On x86, due to the hardware design, anything other than <code>Ordering::Relaxed</code> is equivalent to <code>SeqCst</code>. On ARMv7/v8, things get more complicated.</p>

<p>We recommend reading up on <a href="https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html"><code>Ordering</code></a> both in the rust doc and in the documentation for your platform. For the purpose of this post, just assume we used <code>Ordering::SeqCst</code> everywhere. This is often good enough in practice, and switching to a weaker <code>Ordering</code> is only necessary to squeeze out the last bit of performance.</p>

<p>In Andrea's implementation of the lock-free ring-buffer, <a href="https://github.com/utaal/spsc-bip-buffer">spsc-bip-buffer</a>, some of the orderings are relaxed for performance. This has the downside that it can introduce subtle concurrency bugs that may only show up on some platform (ARM, for example): to be a bit more confident that everything's still fine, Andrea's has continous integation tests both on x86 and ARM.</p>

<h3 id="support-for-embedded-systems">Support for embedded systems</h3>

<p>In James' implementation of the lock-free ring-buffer, <a href="https://github.com/jamesmunns/bbqueue">bbqueue</a>, convenience interfaces are provided for statically allocating instances of the ring-buffer. The queue can be split into Producer and Consumer halves, allowing for use of one half in interrupt context, and the other half in non-interrupt (or a different interrupt) context.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rendering protein structures inside cells at the atomic level with Unreal Engine (194 pts)]]></title>
            <link>https://www.biorxiv.org/content/10.1101/2023.12.08.570879v1</link>
            <guid>39549838</guid>
            <pubDate>Thu, 29 Feb 2024 14:38:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.biorxiv.org/content/10.1101/2023.12.08.570879v1">https://www.biorxiv.org/content/10.1101/2023.12.08.570879v1</a>, See on <a href="https://news.ycombinator.com/item?id=39549838">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page" id="page"><div data-node-nid="3537021" id="node-3537021--21973405445" data-pisa="biorxiv;2023.12.08.570879v1" data-pisa-master="biorxiv;2023.12.08.570879" data-apath="/biorxiv/early/2023/12/11/2023.12.08.570879.atom" data-hw-author-tooltip-instance="highwire_author_tooltip">

      <p><span>
        New Results    </span></p>  
      
  
      <p><span><span>doi:</span> https://doi.org/10.1101/2023.12.08.570879 </span></p>
  
  
  </div>

<div data-panels-ajax-tab-preloaded="biorxiv_tab_art" id="panels-ajax-tab-container-highwire_article_tabs"><div xmlns="http://www.w3.org/1999/xhtml" data-highwire-cite-ref-tooltip-instance="highwire_reflinks_tooltip" xmlns:xhtml="http://www.w3.org/1999/xhtml"><div id="abstract-1"><h2>Abstract</h2><p id="p-2">While the recent development of cryogenic electron tomography (CryoET) makes it possible to identify various macromolecules inside cells and determine their structure at near-atomic resolution, it remains challenging to visualize the complex cellular environment at the atomic level. One of the main hurdles in cell visualization is to render the millions of molecules in real time computationally. Here, using a video game engine, we demonstrate the capability of rendering massive biological macromolecules at the atomic level within their native environment. To facilitate the visualization, we also provide tools that help the interactive navigation inside the cells, as well as software that converts protein structures identified using CryoET to a scene that can be explored with the game engine.</p><div id="sec-1"><div><p><a href="https://www.biorxiv.org/content/biorxiv/early/2023/12/11/2023.12.08.570879/F1.large.jpg?width=800&amp;height=600&amp;carousel=1" title="" rel="gallery-fragment-images-816925171" data-figure-caption="<div class=&quot;highwire-markup&quot;></div>" data-icon-position="" data-hide-link-title="0"><span><img alt="Figure" src="https://www.biorxiv.org/content/biorxiv/early/2023/12/11/2023.12.08.570879/F1.medium.gif" width="440" height="217" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></a></p></div><ul><li><a href="https://www.biorxiv.org/content/biorxiv/early/2023/12/11/2023.12.08.570879/F1.large.jpg?download=true" title="Download Figure1" data-icon-position="" data-hide-link-title="0">Download figure</a></li><li><a href="https://www.biorxiv.org/content/biorxiv/early/2023/12/11/2023.12.08.570879/F1.large.jpg" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li></ul></div></div><h3>Competing Interest Statement</h3><p id="p-3">The authors have declared no competing interest.</p></div>
<div><p>Copyright&nbsp;</p><div><p>The copyright holder for this preprint is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.<span> It is made available under a <a href="http://creativecommons.org/licenses/by/4.0/" data-icon-position="" data-hide-link-title="0">CC-BY 4.0 International license</a>.</span></p></div></div>
</div>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You've just inherited a legacy C++ codebase, now what? (253 pts)]]></title>
            <link>https://gaultier.github.io/blog/you_inherited_a_legacy_cpp_codebase_now_what.html</link>
            <guid>39549486</guid>
            <pubDate>Thu, 29 Feb 2024 14:11:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gaultier.github.io/blog/you_inherited_a_legacy_cpp_codebase_now_what.html">https://gaultier.github.io/blog/you_inherited_a_legacy_cpp_codebase_now_what.html</a>, See on <a href="https://news.ycombinator.com/item?id=39549486">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<p>You were minding your own business, and out of nowhere something fell
on your lap. Maybe you started a new job, or perhaps changed teams, or
someone experienced just left.</p>
<p>And now you are responsible for a C++ codebase. It’s big, complex,
idiosyncratic; you stare too long at it and it breaks in various
interesting ways. In a word, legacy.</p>
<p>But somehow bugs still need to be fixed, the odd feature to be added.
In short, you can’t just ignore it or better yet nuke it out of
existence. It matters. At least to someone who’s paying your salary. So,
it matters to you.</p>
<p>What do you do now?</p>
<p>Well, fear not, because I have experience this many times in numerous
places (the snarky folks in the back will mutter: what C++ codebase
isn’t exactly like I described above), and there is a way out, that’s
not overly painful and will make you able to actually fix the bugs, add
features, and, one can dream, even rewrite it some day.</p>
<p>So join me on a recollection of what worked for me and what one
should absolutely avoid.</p>
<p>And to be fair to C++, I do not hate it (per se), it just happens to
be one of these languages that people abuse and invariably leads to a
horrifying mess and poor C++ is just the victim here and the C++
committee will fix it in C++45, worry not, by adding
<code>std::cmake</code> to the standard library and you’ll see how it’s
absolutely a game changer, and - Ahem, ok let’s go back to the topic at
hand.</p>
<p>So here’s an overview of the steps to take:</p>
<ol type="1">
<li>Get it to work locally, by only doing the minimal changes required
in the code and build system, ideally none. No big refactorings yet,
even if itches really bad!</li>
<li>Get out the chainsaw and rip out everything that’s not absolutely
required to provide the features your company/open source project is
advertising and selling</li>
<li>Make the project enter the 21st century by adding CI, linters,
fuzzing, auto-formatting, etc</li>
<li>Finally we get to make small, incremental changes to the code, Rinse
and repeat until you’re not awaken every night by nightmares of Russian
hackers p@wning your application after a few seconds of poking at
it</li>
<li>If you can, contemplate rewrite some parts in a memory safe
language</li>
</ol>
<p>The overarching goal is exerting the least amount of effort to get
the project in an acceptable state in terms of security, developer
experience, correctness, and performance. It’s crucial to always keep
that in mind. It’s not about ‘clean code’, using the new hotness
language features, etc.</p>
<p>Ok, let’s dive in!</p>
<p><em>By the way, everything here applies to a pure C codebase or a
mixed C and C++ codebase, so if that’s you, keep reading!</em></p>
<p><strong>Table of contents</strong></p>
<ul>
<li><a href="#get-buy-in">Get buy-in</a></li>
<li><a href="#write-down-the-platforms-you-support">Write down the
platforms you support</a></li>
<li><a href="#get-the-build-working-on-your-machine">Get the build
working on your machine</a></li>
<li><a href="#get-the-tests-passing-on-your-machine">Get the tests
passing on your machine</a></li>
<li><a href="#write-down-in-the-readme-how-to-build-and-test-the-application">Write
down in the README how to build and test the application</a></li>
<li><a href="#find-low-hanging-fruits-to-speed-up-the-build-and-tests">Find low
hanging fruits to speed up the build and tests</a></li>
<li><a href="#remove-all-unnecessary-code">Remove all unnecessary
code</a></li>
<li><a href="#linters">Linters</a></li>
<li><a href="#code-formatting">Code formatting</a></li>
<li><a href="#sanitizers">Sanitizers</a></li>
<li><a href="#add-a-ci-pipeline">Add a CI pipeline</a></li>
<li><a href="#incremental-code-improvements">Incremental code
improvements</a></li>
<li><a href="#rewrite-in-a-memory-safe-language">Rewrite in a memory
safe language?</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#addendum-dependency-management">Addendum: Dependency
management</a></li>
</ul>
<h2 id="get-buy-in">Get buy-in</h2>
<p>You thought I was going to compare the different sanitizers, compile
flags, or build systems? No sir, before we do any work, we talk to
people. Crazy, right?</p>
<p>Software engineering needs to be a sustainable practice, not
something you burn out of after a few months or years. We cannot do this
after hours, on a death march, or even, alone! We need to convince
people to support this effort, have them understand what we are doing,
and why. And that encompasses everyone: your boss, your coworkers, even
non-technical folks. And who knows, maybe you’ll go on vacation and
return to see that people are continuing this effort when you’re out of
office.</p>
<p>All of this only means: explain in layman terms the problem with a
few simple facts, the proposed solution, and a timebox. Simple right?
For example (to quote South Park: <em>All characters and events in this
show—even those based on real people—are entirely fictional</em>):</p>
<ul>
<li>Hey boss, the last hire took 3 weeks to get the code building on his
machine and make his first contribution. Wouldn’t it be nice if, with
minimal effort, we could make that a few minutes?</li>
<li>Hey boss, I put quickly together a simple fuzzing setup (‘inputting
random data in the app like a monkey and seeing what happens’), and it
manages to crash the app 253 times within a few seconds. I wonder what
would happen if people try to do that in production with our app?</li>
<li>Hey boss, the last few urgent bug fixes took several people and 2
weeks to be deployed in production because the app can only be built by
this one build server with this ancient operating system that has not
been supported for 8 years (FreeBSD 9, for the curious) and it kept
failing. Oh by the way whenever this server dies we have no way to
deploy anymore, like at all. Wouldn’t it be nice to be able to build our
app on any cheap cloud instance?</li>
<li>Hey boss, we had a cryptic bug in production affecting users, it
took weeks to figure out and fix, and it turns out if was due to
undefined behavior (‘a problem in the code that’s very hard to notice’)
corrupting data, and when I run this industry standard linter (‘a
program that finds issues in the code’) on our code, it detects the
issue instantly. We should run that tool every time we make a
change!</li>
<li>Hey boss, the yearly audit is coming up and the last one took 7
months to pass because the auditor was not happy with what they saw. I
have ideas to make that smoother.</li>
<li>Hey boss, there is a security vulnerability in the news right now
about being able to decrypt encrypted data and stealing secrets, I think
we might be affected, but I don’t know for sure because the cryptography
library we use has been vendored (‘copy-pasted’) by hand with some
changes on top that were never reviewed by anyone. We should clean that
up and setup something so that we get alerted automatically if there is
a vulnerability that affects us.</li>
</ul>
<p>And here’s what to avoid, again totally, super duper fictional,
never-really-happened-to-me examples:</p>
<ul>
<li>We are not using the latest C++ standard, we should halt all work
for 2 weeks to upgrade, also I have no idea if something will break
because we have no tests</li>
<li>I am going to change a lot of things in the project on a separate
branch and work on it for months. It’s definitely getting merged at some
point! (<em>narrator’s voice:</em> it wasn’t)</li>
<li>We are going to rewrite the project from scratch, it should take a
few weeks tops</li>
<li>We are going to improve the codebase, but no idea when it will be
done or even what we are going to do exactly</li>
</ul>
<p>Ok, let’s say that now you have buy-in from everyone that matters,
let’s go over the process:</p>
<ul>
<li>Every change is small and incremental. The app works before and
works after. Tests pass, linters are happy, nothing was bypassed to
apply the change (exceptions do happen but that’s what they are,
exceptional)</li>
<li>If an urgent bug fix has to be made, it can be done as usual,
nothing is blocked</li>
<li>Every change is a measurable improvement and can be explained and
demoed to non experts</li>
<li>If the whole effort has to be suspended or stopped altogether
(because of priorities shifting, budget reasons, etc), it’s still a net
gain overall compared to before starting it (and that gain is in some
form <em>measurable</em>)</li>
</ul>
<p>In my experience, with this approach, you keep everyone happy and can
do the improvements that you really need to do.</p>
<p>Alright, let’s get down to business now!</p>
<h2 id="write-down-the-platforms-you-support">Write down the platforms
you support</h2>
<p>This is so important and not many projects do it. Write in the README
(you do have a README, right?). It’s just a list of
<code>&lt;architecture&gt;-&lt;operating-system&gt;</code> pair,
e.g.&nbsp;<code>x86_64-linux</code> or <code>aarch64-darwin</code>, that your
codebase officially supports. This is crucial for getting the build
working on every one of them but also and we’ll see later, removing
cruft for platforms you do <em>not</em> support.</p>
<p>If you want to get fancy, you can even write down which version of
the architecture such as ARMV6 vs ARMv7, etc.</p>
<p>That helps answer important questions such as:</p>
<ul>
<li>Can we rely on having hardware support for floats, or SIMD, or
SHA256?</li>
<li>Do we even care about supporting 32 bits?</li>
<li>Are we ever running on a big-endian platform? (The answer is very
likely: no, never did, never will - if you do, please email me with the
details because that sounds interesting).</li>
<li>Can a <code>char</code> be 7 bits?</li>
</ul>
<p>And an important point: This list should absolutely include the
developers workstations. Which leads me to my next point:</p>
<h2 id="get-the-build-working-on-your-machine">Get the build working on
your machine</h2>
<p>You’d be amazed at how many C++ codebase in the wild that are a core
part of a successful product earning millions and they basically do not
compile. Well, if all the stars are aligned they do. But that’s not what
I’m talking about. I’m talking about reliably, consistently building on
all platforms you support. No fuss, no ‘I finally got it building after
3 weeks of hair-pulling’ (this brings back some memories). It just
works(tm).</p>
<p>A small aparte here. I used to be really into Karate. We are talking
3, 4 training sessions a week, etc. And I distinctly remember one of my
teachers telling me (picture a wise Asian sifu - hmm actually my teacher
was a bald white guy… picture Steve Ballmer then):</p>
<blockquote>
<p>You do not yet master this move. Sometimes you do and sometimes you
don’t, so you don’t. When eating with a spoon, do you miss your mouth
one out of five times?</p>
</blockquote>
<p>And I carried that with me as a Software Engineer. ‘The new feature
works’ means it works every time. Not four out of five times. And so the
build is the same.</p>
<p>Experience has shown me that the best way to produce software in a
fast and efficient way is to be able to build on your machine, and
ideally even run it on your machine.</p>
<p>Now if your project is humongous that may be a problem, your system
might not even have enough RAM to complete the build. A fallback is to
rent a big server somewhere and run your builds here. It’s not ideal but
better than nothing.</p>
<p>Another hurdle is the code requiring some platform specific API, for
example <code>io_uring</code> on Linux. What can help here is to
implement a shim, or build inside a virtual machine on your workstation.
Again, not ideal but better than nothing.</p>
<p>I have done all of the above in the past and that works but building
directly on your machine is still the best option.</p>
<h2 id="get-the-tests-passing-on-your-machine">Get the tests passing on
your machine</h2>
<p>First, if there are no tests, I am sorry. This is going to be really
difficult to do any change at all. So go write some tests before doing
any change to the code, make them pass, and come back. The easiest way
is to capture inputs and outputs of the program running in the real
world and write end-to-end tests based on that, the more varied the
better. It will ensure there are no regressions when making changes, not
that the behavior was correct in the first place, but again, better than
nothing.</p>
<p>So, now you have a test suite. If some tests fail, disable them for
now. Make them pass, even if the whole test suite takes hours to run.
We’ll worry about that later.</p>
<h2 id="write-down-in-the-readme-how-to-build-and-test-the-application">Write
down in the README how to build and test the application</h2>
<p>Ideally it’s one command to build and one for testing. At first it’s
fine if it’s more involved, in that case the respective commands can be
put in a <code>build.sh</code> and <code>test.sh</code> that encapsulate
the madness.</p>
<p>The goal is to have a non C++ expert be able to build the code and
run the tests without having to ask you anything.</p>
<p>Here some folks would recommend documenting the project layout, the
architecture, etc. Since the next step is going to rip out most of it,
I’d say don’t waste your time now, do that at the end.</p>
<h2 id="find-low-hanging-fruits-to-speed-up-the-build-and-tests">Find
low hanging fruits to speed up the build and tests</h2>
<p>Emphasis on ‘low hanging’. No change of the build system, no heroic
efforts (I keep repeating that in this article but this is so
important).</p>
<p>Again, in a typical C++ project, you’d be amazed at how much work the
build system is doing without having to do it at all. Try these ideas
below and measure if that helps or not:</p>
<ul>
<li>Building and running tests <em>of your dependencies</em>. In a
project which was using <code>unittest++</code> as a test framework,
built as a CMake subproject, I discovered that the default behavior was
to build the tests of the test framework, and run them, every time!
That’s crazy. Usually there is a CMake variable or such to opt-out of
this.</li>
<li>Building and running example programs <em>of your dependencies</em>.
Same thing as above, the culprit that time was <code>mbedtls</code>.
Again, setting a CMake variable to opt-out of that solved it.</li>
<li>Building and running the tests of your project by default when it’s
being included as a subproject of another parent project. Yeah the
default behavior we just laughed at in our dependencies? It turns out
we’re doing the same to other projects! I am no CMake expert but it
seems that there is no standard way to exclude tests in a build. So I
recommend adding a build variable called <code>MYPROJECT_TEST</code>
unset by default and only build and run tests when it is set. Typically
only developers working on the project directly will set it. Same with
examples, generating documentation, etc.</li>
<li>Building all of a third-party dependency when you only need a small
part of it: <code>mbedtls</code> comes to mind as a good citizen here
since it exposes many compile-time flags to toggle lots of parts you
might not need. Beware of the defaults, and only build what you
need!</li>
<li>Wrong dependencies listed for a target leading to rebuilding the
world when it does not have to: most build systems have a way to output
the dependency graph from their point of view and that can really help
diagnose these issues. Nothing feels worse than waiting for minutes or
hours for a rebuild, when deep inside, you know it should have only
rebuilt a few files.</li>
<li>Experiment with a faster linker: <code>mold</code> is one that can
dropped in and really help at no cost. However that really depends on
how many libraries are being linked, whether that’s a bottleneck
overall, etc.</li>
<li>Experiment with a different compiler, if you can: I have seen
projects where clang is twice as fast as gcc, and others where there is
no difference.</li>
</ul>
<p>Once that’s done, here are a few things to additionally try, although
the gains are typically much smaller or sometimes negative:</p>
<ul>
<li>LTO: off/on/thin</li>
<li>Split debug information</li>
<li>Make vs Ninja</li>
<li>The type of file system in use, and tweaking its settings</li>
</ul>
<p>Once the iteration cycle feels ok, the code gets to go under the
microscope. If the build takes ages, it’s not realistic to want to
modify the code.</p>
<h2 id="remove-all-unnecessary-code">Remove all unnecessary code</h2>
<p>Dad, I see dead lines of code.</p>
<p>(Get the reference? Well, ok then.)</p>
<p>I have seen 30%, sometimes more, of a codebase, being completely dead
code. That’s lines of code you pay for every time you compile, you want
to make a refactoring, etc. So let’s rip them out.</p>
<p>Here are some ways to go about it:</p>
<ul>
<li>The compiler has a bunch of <code>-Wunused-xxx</code> warnings,
e.g.&nbsp;<code>-Wunused-function</code>. They catch some stuff, but not
everything. Every single instance of these warnings should be addressed.
Usually it’s as easy as deleting the code, rebuilding and re-running the
tests, done. In rare cases it’s a symptom of a bug where the wrong
function was called. So I’d be somewhat reluctant to fully automate this
step. But if you’re confident in your test suite, go for it.</li>
<li>Linters can find unused functions or class fields,
e.g.&nbsp;<code>cppcheck</code>. In my experience there are quite a lot of
false positives especially regarding virtual functions in the case of
inheritance, but the upside is that these tools absolutely find unused
things that the compilers did not notice. So, a good excuse for adding a
linter to your arsenal, if not to the CI (more on that later).</li>
<li>I have seen more exotic techniques were the linker is instructed to
put each function in its own section and print every time a section is
removed because it’s detected to be unused at link time, but that
results in so much noise e.g.&nbsp;about standard library functions being
unused, that I have not found that really practical. Others inspect the
generated assembly and compare which functions are present there with
the source code, but that does not work for virtual functions. So, maybe
worth a shot, depending on your case?</li>
<li>Remember the list of supported platforms? Yeah, time to put it to
use to kill all the code for unsupported platforms. Code trying to
support ancient versions of Solaris on a project that exclusively ran on
FreeBSD? Out of the window it goes. Code trying to provide its own
random number generator because maybe the platform we run on does not
have one (of course it turned out that was never the case)? To the bin.
Hundred of lines of code in case POSIX 2001 is not supported, when we
only run on modern Linux and macOS? Nuke it. Checking if the host CPU is
big-endian and swapping bytes if it is? Ciao (when was the last time you
shipped code for a big-endian CPU? And if yes, how are you finding
IBM?). That code introduced years ago for a hypothetical feature that
never came? Hasta la vista.</li>
</ul>
<p>And the bonus for doing all of this, is not only that you sped up the
build time by a factor of 5 with zero downside, is that, if your boss is
a tiny bit technical, they’ll love seeing PRs deleting thousands of
lines of code. And your coworkers as well.</p>
<h2 id="linters">Linters</h2>
<p>Don’t go overboard with linter rules, add a few basic ones,
incorporate them in the development life cycle, incrementally tweak the
rules and fix the issues that pop up, and move on. Don’t try to enable
all the rules, it’s just a rabbit hole of diminishing returns. I have
used <code>clang-tidy</code> and <code>cppcheck</code> in the past, they
can be helpful, but also incredibly slow and noisy, so be warned. Having
no linter is not an option though. The first time you run the linter,
it’ll catch so many real issues that you’ll wonder why the compiler is
not detecting anything even with all the warnings on.</p>
<h2 id="code-formatting">Code formatting</h2>
<p>Wait for the appropriate moment where no branches are active
(otherwise people will have horrendous merge conflicts), pick a code
style at random, do a one time formatting of the entire codebase (no
exceptions), typically with <code>clang-format</code>, commit the
configuration, done. Don’t waste any bit of saliva arguing about the
actual code formatting. It only exists to make diffs smaller and avoid
arguments, so do not argue about it!</p>
<h2 id="sanitizers">Sanitizers</h2>
<p>Same as linters, it can be a rabbit hole, unfortunately it’s
absolutely required to spot real, production affecting, hard to detect,
bugs and to be able to fix them.
<code>-fsanitize=address,undefined</code> is a good baseline. They
usually do not have false positives so if something gets detected, go
fix it. Run the tests with it so that issues get detected there as well.
I even heard of people running the production code with some sanitizers
enabled, so if your performance budget can allow it, it could be a good
idea.</p>
<p>If the compiler you (have to) use to ship the production code does
not support sanitizers, you can at least use clang or such when
developing and running tests. That’s when the work you did on the build
system comes in handy, it should be relatively easy to use different
compilers.</p>
<p>One thing is for sure: even in the best codebase in the world, with
the best coding practices and developers, the second you enable the
sanitizers, you absolutely will uncover horrifying bugs and memory leaks
that went undetected for years. So do it. Be warned that fixing these
can require a lot of work and refactorings. Each sanitizer also has
options so it could be useful to inspect them if your project is a
special snowflake.</p>
<p>One last thing: ideally, all third-party dependencies should also be
compiled with the sanitizers enabled when running tests, to spot <a href="https://github.com/rxi/microui/pull/67">issues</a> in them as
well.</p>
<h2 id="add-a-ci-pipeline">Add a CI pipeline</h2>
<p>As Bryan Cantrill once said (quoting from memory), ‘I am convinced
most firmware just comes out of the home directory of a developer’s
laptop’. Setting up a CI is quick, free, and automates all the good
things we have set up so far (linters, code formatting, tests, etc). And
that way we can produce in a pristine environment the production
binaries, on every change. If you’re not doing this already as a
developer, I don’t think you really have entered the 21st century
yet.</p>
<p>Cherry on the cake: most CI systems allow for running the steps on a
matrix of different platforms! So you can demonstrably check that the
list of supported platforms is not just theory, it is real.</p>
<p>Typically the pipeline just looks like
<code>make all test lint fmt</code> so it’s not rocket science. Just
make sure that issues that get reported by the tools (linters,
sanitizers, etc) actually fail the pipeline, otherwise no one will
notice and fix them.</p>
<h2 id="incremental-code-improvements">Incremental code
improvements</h2>
<p>Well that’s known territory so I won’t say much here. Just that lots
of code can often be dramatically simplified.</p>
<p>I remember iteratively simplifying a complicated class that manually
allocated and (sometimes) deallocated memory, was meant to handle
generic things, and so on. All the class did, as it turned out, was
allocate a pointer, later check whether the pointer was null or not,
and…that’s it. Yeah that’s a boolean in my book. True/false, nothing
more to it.</p>
<p>I feel that’s the step that’s the hardest to timebox because each
round of simplification opens new avenues to simplify further. Use your
best judgment here and stay on the conservative side. Focus on tangible
goals such as security, correctness and performance, and stray away from
subjective criteria such as ‘clean code’.</p>
<p>In my experience, upgrading the C++ standard in use in the project
can at times help with code simplifications, for example to replace code
that manually increments iterators by a
<code>for (auto x : items)</code> loop, but remember it’s just a means
to an end, not an end in itself. If all you need is
<code>std::clamp</code>, just write it yourself.</p>
<h2 id="rewrite-in-a-memory-safe-language">Rewrite in a memory safe
language?</h2>
<p>I am doing this right now at work, and that deserves an article of
its own. Lots of gotchas there as well. Only do this with a compelling
reason.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Well, there you have it. A tangible, step-by-step plan to get out of
the finicky situation that’s a complex legacy C++ codebase. I have just
finished going through that at work on a project, and it’s become much
more bearable to work on it now. I have seen coworkers, who previously
would not have come within a 10 mile radius of the codebase, now make
meaningful contributions. So it feels great.</p>
<p>There are important topics that I wanted to mention but in the end
did not, such as the absolute necessity of being able to run the code in
a debugger locally, fuzzing, dependency scanning for vulnerabilities,
etc. Maybe for the next article!</p>
<p>If you go through this on a project, and you found this article
helpful, shoot me an email! It’s nice to know that it helped
someone.</p>
<h2 id="addendum-dependency-management">Addendum: Dependency
management</h2>
<p><em>This section is very subjective, it’s just my strong, biased
opinion.</em></p>
<p>There’s a hotly debated topic that I have so far carefully avoided
and that’s dependency management. So in short, in C++ there’s none. Most
people resort to using the system package manager, it’s easy to notice
because their README looks like this:</p>
<pre><code>On Ubuntu 20.04: `sudo apt install [100 lines of packages]`

On macOS: `brew install [100 lines of packages named slightly differently]`

Any other: well you're out of luck buddy. I guess you'll have to pick a mainstream OS and reinstall ¯\_(ツ)_/¯</code></pre>
<p>Etc. I have done it myself. And I think this is a terrible idea.
Here’s why:</p>
<ul>
<li>The installation instructions, as we’ve seen above, are OS and
distribution dependent. Worse, they’re dependent on the version of the
distribution. I remember a project that took months to move from Ubuntu
20.04 to Ubuntu 22.04, because they ship different versions of the
packages (if they ship the same packages at all), and so upgrading the
distribution also means upgrading the 100 dependencies of your project
at the same time. Obviously that’s a very bad idea. You want to upgrade
one dependency at a time, ideally.</li>
<li>There’s always a third-party dependency that has no package and you
have to build it from source anyway.</li>
<li>The packages are never built with the flags you want. Fedora and
Ubuntu have debated for years whether to build packaged with the frame
pointer enabled (they finally do since very recently). Remember the
section about sanitizers? How are you going to get dependencies with
sanitizer enabled? It’s not going to happen. But there are way more
examples: LTO, <code>-march</code>, debug information, etc. Or they were
built with a different C++ compiler version from the one you are using
and they broke the C++ ABI between the two.</li>
<li>You want to easily see the source of the dependency when auditing,
developing, debugging, etc, <em>for the version you are currently
using</em>.</li>
<li>You want to be able to patch a dependency easily if you encounter a
bug, and rebuild easily without having to change the build system
extensively</li>
<li>You never get the exact same version of a package across systems,
e.g.&nbsp;when developer Alice is on macOS, Bob on Ubuntu and the production
system on FreeBSD. So you have weird discrepancies you cannot reproduce
and that’s annoying.</li>
<li>Corollary of the point above: You don’t know exactly which
version(s) you are using across systems and it’s hard to produce a Bill
of Material (BOM) in an automated fashion, which is required (or going
to be required very soon? Anyway it’s a good idea to have it) in some
fields.</li>
<li>The packages sometimes do not have the version of the library you
need (static or dynamic)</li>
</ul>
<p>So you’re thinking, I know, I will use those fancy new package
managers for C++, Conan, vcpkg and the like! Well, not so fast:</p>
<ul>
<li>They require external dependencies so your CI becomes more complex
and slower (e.g.&nbsp;figuring out which exact version of Python they
require, which surely will be different from the version of Python your
project requires)</li>
<li>They do not have all versions of a package. Example: <a href="https://conan.io/center/recipes/mbedtls">Conan and mbedtls</a>, it
jumps from version <code>2.16.12</code> to <code>2.23.0</code>. What
happened to the versions in between? Are they flawed and should not be
used? Who knows! Security vulnerabilities are not listed anyways for the
versions available! Of course I had a project in the past where I had to
use version <code>2.17</code>…</li>
<li>They might not support some operating systems or architectures you
care about (FreeBSD, ARM, etc)</li>
</ul>
<p>I mean, if you have a situation where they work for you, that’s
great, it’s definitely an improvement over using system packages in my
mind. It’s just that I never encountered (so far) a project where I
could make use of them - there was always some blocker.</p>
<p>So what do I recommend? Well, the good old git submodules and
compiling from source approach. It’s cumbersome, yes, but also:</p>
<ul>
<li>It’s dead simple</li>
<li>It’s better than manually vendoring because git has the history and
the diff functionalities</li>
<li>You know exactly, down to the commit, which version of the
dependency is in use</li>
<li>Upgrading the version of a single dependency is trivial, just run
<code>git checkout</code></li>
<li>It works on every platform</li>
<li>You get to choose exactly the compilation flags, compiler, etc to
build all the dependencies. And you can even tailor it per
dependency!</li>
<li>Developers know it already even if they have no C++ experience</li>
<li>Fetching the dependencies is secure and the remote source is in git.
No one is changing that sneakily.</li>
<li>It works recursively (i.e.: transitively, for the dependencies of
your dependencies)</li>
</ul>
<p>Compiling each dependency in each submodule can be as simple as
<code>add_subdirectory</code> with CMake, or
<code>git submodule foreach make</code> by hand.</p>
<p>If submodules are really not an option, an alternative is to still
compile from source but do it by hand, with one script, that fetches
each dependency and builds it. Example in the wild: Neovim.</p>
<p>Of course, if your dependency graph visualized in Graphviz looks like
a Rorschach test and has to build thousands of dependencies, it is not
easily doable, but it might be still possible, using a build system like
Buck2, which does hybrid local-remote builds, and reuses build artifacts
between builds from different users.</p>
<p>If you look at the landscape of package managers for compiled
languages (Go, Rust, etc), all of them that I know of compile from
source. It’s the same approach, minus git, plus the automation.</p>

<blockquote>
  <p>If you liked this article and you want to support me, and can afford it: <a href="https://paypal.me/philigaultier?country.x=DE&amp;locale.x=en_US">Donate</a></p>
</blockquote>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple is turning William Gibson's Neuromancer into a TV series (142 pts)]]></title>
            <link>https://www.theverge.com/24086056/apple-tv-plus-neuromancer-streaming-series-william-gibson</link>
            <guid>39549340</guid>
            <pubDate>Thu, 29 Feb 2024 13:58:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/24086056/apple-tv-plus-neuromancer-streaming-series-william-gibson">https://www.theverge.com/24086056/apple-tv-plus-neuromancer-streaming-series-william-gibson</a>, See on <a href="https://news.ycombinator.com/item?id=39549340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Another sci-fi adaptation is making its way to Apple TV Plus. The streamer announced that it’s adapting William Gibson’s seminal cyberpunk novel <em>Neuromancer</em> into a 10-episode series. Graham Roland (<em>Lost</em>,<em> Jack Ryan</em>) will serve as showrunner, while JD Dillard (<em>Utopia</em>) will direct the first episode. (Both will also be executive producers on the series.)</p><p>That’s about all we know right now. There are no details on when the series might start streaming or who will star. In <a href="https://www.apple.com/tv-pr/news/2024/02/apple-tv-announces-neuromancer-new-drama-based-on-the-multi-award-winning-science-fiction-novel-by-william-gibson/">a press release</a>, Apple said that the show “will follow a damaged, top-rung super-hacker named Case who is thrust into a web of digital espionage and high stakes crime with his partner Molly, a razor-girl assassin with mirrored eyes aiming to pull a heist on a corporate dynasty with untold secrets.”</p><div><p>This is also not the first adaptation of <em>Neuromancer</em>, which originally came out 1984 as Gibson’s debut novel. Its been <a href="https://en.wikipedia.org/wiki/Neuromancer_(video_game)">turned into a video game</a>, a graphic novel, and is <a href="https://www.theverge.com/2017/8/9/16121760/neuromancer-movie-deadpool-director-tim-miller">reportedly being made into a movie</a> as well. (In 2022, Amazon adapted another Gibson novel — <em>The Peripheral</em> — into <a href="https://www.theverge.com/23412567/the-peripheral-review-amazon-prime-video-william-gibson-book">a live-action streaming series</a>.)</p></div><p>Whenever it does premiere, <em>Neuromancer</em> will join an ever-growing lineup of sci-fi on Apple TV Plus. So far that has included series like <a href="https://www.theverge.com/23786940/foundation-season-2-david-goyer-interview-apple-tv-plus"><em>Foundation</em></a>, <em>For All Mankind</em>, <a href="https://www.theverge.com/23711259/silo-review-season-1-apple-tv-plus"><em>Silo</em></a>, <a href="https://www.theverge.com/23932047/invasion-season-2-review-apple-tv-plus"><em>Invasion</em></a>, <a href="https://www.theverge.com/23913217/monarch-legacy-of-monsters-apple-nycc-preview-godzilla"><em>Monarch</em></a>, and <em>Constellation</em>, <a href="https://www.theverge.com/24065198/constellation-review-apple-tv-plus">which premiered earlier this month</a>. An adaptation of Martha Wells’ <em>The Murderbot Diaries</em> is <a href="https://www.theverge.com/2023/12/14/24001803/murderbot-series-apple-tv-plus-alexander-skarsgard">also in the works</a>, starring Alexander Skarsgård.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Workout Tracker – self-hosted, single binary web application (212 pts)]]></title>
            <link>https://github.com/jovandeginste/workout-tracker</link>
            <guid>39549194</guid>
            <pubDate>Thu, 29 Feb 2024 13:45:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/jovandeginste/workout-tracker">https://github.com/jovandeginste/workout-tracker</a>, See on <a href="https://news.ycombinator.com/item?id=39549194">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">A workout tracking web application for personal use (or family, friends), geared towards running and other GPX-based
activities</p>
<p dir="auto">Self-hosted, everything included.</p>
<p dir="auto">Heavily inspired by <a href="https://github.com/SamR1/FitTrackee">FitTrackee</a> ❤️.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Docker</h3><a id="user-content-docker" aria-label="Permalink: Docker" href="#docker"></a></p>
<p dir="auto">Run the latest master image from GitHub Container Registry:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -p 8080:8080 ghcr.io/jovandeginste/workout-tracker:master"><pre>docker run -p 8080:8080 ghcr.io/jovandeginste/workout-tracker:master</pre></div>
<p dir="auto">Open your browser at <code>http://localhost:8080</code></p>
<p dir="auto">To persist data and sessions, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -p 8080:8080 \
    -e WT_JWT_ENCRYPTION_KEY=my-secret-key \
    -v $PWD/data:/data \
    ghcr.io/jovandeginste/workout-tracker:master"><pre>docker run -p 8080:8080 \
    -e WT_JWT_ENCRYPTION_KEY=my-secret-key \
    -v <span>$PWD</span>/data:/data \
    ghcr.io/jovandeginste/workout-tracker:master</pre></div>
<p dir="auto">or use docker compose</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create directory that stores your data
mkdir -p /opt/workout-tracker
cd /opt/workout-tracker

# Download the compose.yaml
curl https://raw.githubusercontent.com/jovandeginste/workout-tracker/master/compose.yaml --output compose.yaml

# Start the server
docker compose up -d"><pre><span><span>#</span> Create directory that stores your data</span>
mkdir -p /opt/workout-tracker
<span>cd</span> /opt/workout-tracker

<span><span>#</span> Download the compose.yaml</span>
curl https://raw.githubusercontent.com/jovandeginste/workout-tracker/master/compose.yaml --output compose.yaml

<span><span>#</span> Start the server</span>
docker compose up -d</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Natively</h3><a id="user-content-natively" aria-label="Permalink: Natively" href="#natively"></a></p>
<p dir="auto">Download a pre-built binary or build it yourself (see <a href="#development">Development</a> below).</p>
<div dir="auto" data-snippet-clipboard-copy-content="wget https://github.com/jovandeginste/workout-tracker/releases/download/v0.3.0/workout-tracker-v0.3.0-linux-amd64.tar.gz
tar xf workout-tracker-v0.3.0-linux-amd64.tar.gz
./workout-tracker"><pre>wget https://github.com/jovandeginste/workout-tracker/releases/download/v0.3.0/workout-tracker-v0.3.0-linux-amd64.tar.gz
tar xf workout-tracker-v0.3.0-linux-amd64.tar.gz
./workout-tracker</pre></div>
<p dir="auto">To persist sessions, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export WT_JWT_ENCRYPTION_KEY=my-secret-key
./workout-tracker"><pre><span>export</span> WT_JWT_ENCRYPTION_KEY=my-secret-key
./workout-tracker</pre></div>
<p dir="auto">This will create a new database file in the current directory and start the web server at <code>http://localhost:8080</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dashboard</h3><a id="user-content-dashboard" aria-label="Permalink: Dashboard" href="#dashboard"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jovandeginste/workout-tracker/blob/master/docs/dashboard.jpg"><img src="https://github.com/jovandeginste/workout-tracker/raw/master/docs/dashboard.jpg" alt=""></a>
Dashboard view with:</p>
<ul dir="auto">
<li>personal totals</li>
<li>running records</li>
<li>a calendar view</li>
<li>recent activities (by you and other users)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Overview of workouts</h3><a id="user-content-overview-of-workouts" aria-label="Permalink: Overview of workouts" href="#overview-of-workouts"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jovandeginste/workout-tracker/blob/master/docs/workout_overview.jpg"><img src="https://github.com/jovandeginste/workout-tracker/raw/master/docs/workout_overview.jpg" alt=""></a>
Overview of all your activities, with summaries. The columns are sortable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Details of a single workout</h3><a id="user-content-details-of-a-single-workout" aria-label="Permalink: Details of a single workout" href="#details-of-a-single-workout"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jovandeginste/workout-tracker/blob/master/docs/single_workout.jpg"><img src="https://github.com/jovandeginste/workout-tracker/raw/master/docs/single_workout.jpg" alt=""></a>
Details of a workout, with:</p>
<ul dir="auto">
<li>a zoomable, dragable map of the GPX track with more details per point</li>
<li>many summarized statistics</li>
<li>a breakdown per kilometer</li>
<li>track color based on elevation of the segment</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tooltips for even more information</h3><a id="user-content-tooltips-for-even-more-information" aria-label="Permalink: Tooltips for even more information" href="#tooltips-for-even-more-information"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jovandeginste/workout-tracker/blob/master/docs/track.gif"><img src="https://github.com/jovandeginste/workout-tracker/raw/master/docs/track.gif" alt="" data-animated-image=""></a></p>
<ul dir="auto">
<li>green and red circle are start and end points of the track</li>
<li>every point on the track has a tooltip with a summary at that moment</li>
<li>hover over the breakdown per kilometer to highlight the point</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Upload your files</h3><a id="user-content-upload-your-files" aria-label="Permalink: Upload your files" href="#upload-your-files"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jovandeginste/workout-tracker/blob/master/docs/upload_workouts.jpg"><img src="https://github.com/jovandeginste/workout-tracker/raw/master/docs/upload_workouts.jpg" alt=""></a></p>
<ul dir="auto">
<li>Upload one or multiple GPX files.</li>
<li>Pick the type (running, cycling, ...) or let the application guess based on average speed</li>
<li>The files are parsed when uploaded: statistics and other information are calculated and stored in the database (serialized).</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Statistics to follow your progress</h3><a id="user-content-statistics-to-follow-your-progress" aria-label="Permalink: Statistics to follow your progress" href="#statistics-to-follow-your-progress"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jovandeginste/workout-tracker/blob/master/docs/statistics.jpg"><img src="https://github.com/jovandeginste/workout-tracker/raw/master/docs/statistics.jpg" alt=""></a></p>
<ul dir="auto">
<li>Graphs showing monthly aggregated statistics.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic multi-language support</h3><a id="user-content-basic-multi-language-support" aria-label="Permalink: Basic multi-language support" href="#basic-multi-language-support"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jovandeginste/workout-tracker/blob/master/docs/i18n.gif"><img src="https://github.com/jovandeginste/workout-tracker/raw/master/docs/i18n.gif" alt="" data-animated-image=""></a></p>
<ul dir="auto">
<li>Switch between (supported) languages</li>
<li>Use the language configured in the browser (default)</li>
<li>Very limited amount of languages supported for now 😄</li>
<li>Re-calculate all previously uploaded workouts (useful while developing)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Responsive design</h3><a id="user-content-responsive-design" aria-label="Permalink: Responsive design" href="#responsive-design"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jovandeginste/workout-tracker/blob/master/docs/responsive.png"><img src="https://github.com/jovandeginste/workout-tracker/raw/master/docs/responsive.png" alt=""></a></p>
<ul dir="auto">
<li>Usable on small and medium screens</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Light and dark mode</h3><a id="user-content-light-and-dark-mode" aria-label="Permalink: Light and dark mode" href="#light-and-dark-mode"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jovandeginste/workout-tracker/blob/master/docs/single_workout-theme.jpg"><img src="https://github.com/jovandeginste/workout-tracker/raw/master/docs/single_workout-theme.jpg" alt=""></a></p>
<ul dir="auto">
<li>Browser decides whether to use light or dark mode, based on your preferences</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">The web server looks for a file <code>workout-tracker.yaml</code> (or <code>json</code> or <code>toml</code>) in the current directory, or takes it's
configuration from environment variables. The most important variable is the JWT encryption key. If you don't provide
it, the key is randomly generated every time the server starts, invalidating all current sessions.</p>
<p dir="auto">Generate a secure key and write it to <code>workout-tracker.yaml</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="echo &quot;jwt_encryption_key: $(pwgen -c 32)&quot; > workout-tracker.yaml"><pre><span>echo</span> <span><span>"</span>jwt_encryption_key: <span><span>$(</span>pwgen -c 32<span>)</span></span><span>"</span></span> <span>&gt;</span> workout-tracker.yaml</pre></div>
<p dir="auto">or export it as an environment variable:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export WT_JWT_ENCRYPTION_KEY=&quot;$(pwgen -c 32)&quot;"><pre><span>export</span> WT_JWT_ENCRYPTION_KEY=<span><span>"</span><span><span>$(</span>pwgen -c 32<span>)</span></span><span>"</span></span></pre></div>
<p dir="auto">See <code>workout-tracker.example.yaml</code> for more options and details.</p>
<p dir="auto">After starting the server, you can access it at <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a> (the default port). A login form is shown.</p>
<p dir="auto">If no users are in the database (eg. when starting with an empty database), a default <code>admin</code> user is created with
password <code>admin</code>. You should change this password in a production environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build and run it yourself</h3><a id="user-content-build-and-run-it-yourself" aria-label="Permalink: Build and run it yourself" href="#build-and-run-it-yourself"></a></p>
<ul dir="auto">
<li>install go</li>
<li>clone the repository</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="go build ./
./workout-tracker"><pre>go build ./
./workout-tracker</pre></div>
<p dir="auto">This does not require npm or Tailwind, since the compiled css is included in the repository.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Do some development</h3><a id="user-content-do-some-development" aria-label="Permalink: Do some development" href="#do-some-development"></a></p>
<p dir="auto">You need to install Golang and npm.</p>
<p dir="auto">Because I keep forgetting how to build every component, I created a Makefile.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Make everything. This is also the default target.
make all # Run tests and build all components

# Install system dependencies
make install-deps

# Testing
make test # Runs all the tests
make test-assets test-go # Run tests for the individual components

# Building
make build # Builds all components
make build-tw # Builds the Tailwind CSS output file
make build-server # Builds the web server
make build-docker # Performs all builds inside Docker containers, creates a Docker image

# Running it
make serve # Runs the compiled binary
make dev # Runs a wrapper that watches for changes, then rebuilds and restarts
make watch-tw # Runs the Tailwind CSS watcher (not useful unless you're debugging Tailwind CSS)

# Cleanin' up
make clean # Removes build artifacts"><pre><span><span>#</span> Make everything. This is also the default target.</span>
make all <span><span>#</span> Run tests and build all components</span>

<span><span>#</span> Install system dependencies</span>
make install-deps

<span><span>#</span> Testing</span>
make <span>test</span> <span><span>#</span> Runs all the tests</span>
make test-assets test-go <span><span>#</span> Run tests for the individual components</span>

<span><span>#</span> Building</span>
make build <span><span>#</span> Builds all components</span>
make build-tw <span><span>#</span> Builds the Tailwind CSS output file</span>
make build-server <span><span>#</span> Builds the web server</span>
make build-docker <span><span>#</span> Performs all builds inside Docker containers, creates a Docker image</span>

<span><span>#</span> Running it</span>
make serve <span><span>#</span> Runs the compiled binary</span>
make dev <span><span>#</span> Runs a wrapper that watches for changes, then rebuilds and restarts</span>
make watch-tw <span><span>#</span> Runs the Tailwind CSS watcher (not useful unless you're debugging Tailwind CSS)</span>

<span><span>#</span> Cleanin' up</span>
make clean <span><span>#</span> Removes build artifacts</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is this, technically?</h2><a id="user-content-what-is-this-technically" aria-label="Permalink: What is this, technically?" href="#what-is-this-technically"></a></p>
<p dir="auto">A single binary that runs on any platform, with no dependencies.</p>
<p dir="auto">The binary contains all assets to serve a web interface, through which you can upload your GPX files, visualize
your tracks and see their statistics and graphs. The web application is multi-user, with a simple registration and
authentication form, session cookies and JWT tokens). New accounts are inactive by default. An admin user can activate
(or edit, delete) accounts. The default database storage is a single SQLite file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What technologies are used</h2><a id="user-content-what-technologies-are-used" aria-label="Permalink: What technologies are used" href="#what-technologies-are-used"></a></p>
<ul dir="auto">
<li>Go, with some notable libraries
<ul dir="auto">
<li><a href="https://github.com/jovandeginste/workout-tracker/blob/master/github.com/tkrajina/gpxgo">gpxgo</a></li>
<li><a href="https://echo.labstack.com/" rel="nofollow">Echo</a></li>
<li><a href="https://gorm.io/" rel="nofollow">Gorm</a></li>
<li><a href="https://github.com/vorlif/spreak">Spreak</a></li>
</ul>
</li>
<li>HTML, CSS and JS
<ul dir="auto">
<li><a href="https://tailwindcss.com/" rel="nofollow">Tailwind CSS</a></li>
<li><a href="https://fontawesome.com/" rel="nofollow">Font Awesome</a></li>
<li><a href="https://fullcalendar.io/" rel="nofollow">FullCalendar</a></li>
<li><a href="https://leafletjs.com/" rel="nofollow">Leaflet</a></li>
<li><a href="https://www.kryogenix.org/code/browser/sorttable/" rel="nofollow">sorttable</a></li>
<li><a href="https://cdn.jsdelivr.net/npm/chart.js" rel="nofollow">Chart.js</a></li>
</ul>
</li>
<li>Docker</li>
</ul>
<p dir="auto">The application uses OpenStreetMap as its map provider and for geocoding a GPS coordinate to a location.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compatiblity</h2><a id="user-content-compatiblity" aria-label="Permalink: Compatiblity" href="#compatiblity"></a></p>
<p dir="auto">This is a work in progress. If you find any problems, please let us know. The application is tested with GPX files from
these sources:</p>
<ul dir="auto">
<li>Garmin Connect (export to GPX)</li>
<li>FitoTrack (automatic export to GPX)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">TODO</h2><a id="user-content-todo" aria-label="Permalink: TODO" href="#todo"></a></p>
<ul dir="auto">
<li>write tests!!!!!</li>
<li>add support for authentication through a reverse proxy</li>
<li>make a dev-flag that doesn't embed all files in the binary</li>
<li>add support for generic database drivers</li>
<li>add support for other types of import files (eg. Garmin fit files)</li>
<li>add support for auto-import from a folder (per user)</li>
<li>see if htmx is worth using</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Billionaire-Fueled Lobbying Group Behind the State Bills to Ban UBI Experiments (104 pts)]]></title>
            <link>https://www.scottsantens.com/billionaire-fueled-lobbying-group-behind-the-state-bills-to-ban-universal-basic-income-experiments-ubi/</link>
            <guid>39549098</guid>
            <pubDate>Thu, 29 Feb 2024 13:35:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scottsantens.com/billionaire-fueled-lobbying-group-behind-the-state-bills-to-ban-universal-basic-income-experiments-ubi/">https://www.scottsantens.com/billionaire-fueled-lobbying-group-behind-the-state-bills-to-ban-universal-basic-income-experiments-ubi/</a>, See on <a href="https://news.ycombinator.com/item?id=39549098">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <article>
      


      <section>
        <h2 id="the-foundation-for-government-accountabilitya-florida-based-lobbying-group-backed-by-the-richest-1is-working-to-get-basic-income-experiments-banned-by-state-legislators-across-the-us">The Foundation for Government Accountability - a Florida-based lobbying group backed by the richest 1% - is working to get basic income experiments banned by state legislators across the U.S.</h2><p>As a well-known quote often wrongly attributed to Mahatma Ghandi says, “First they ignore you, then they laugh at you, then they fight you, then you win.” As of 2024, the basic income movement in the United States is now firmly in the "<strong>then they fight you</strong>" stage thanks to a slew of bills introduced in state after state that are all attempting to ban the basic income experiments that have spread across the country. <a href="https://basicincome.stanford.edu/experiments-map/?ref=scottsantens.com" rel="noreferrer">Over 150 guaranteed basic income pilots</a> are now ongoing or recently completed in 24 states as of this writing, and so far, bills in seven states have been introduced to stop them. All of the bills are the result of efforts by the <strong>Foundation for Government Accountability</strong> (FGA) -  a lobbying group with a billionaire-fueled junk science record every American should know about.</p><p>First, to bring every reader up to speed, <a href="https://basicincome.org/?ref=scottsantens.com" rel="noreferrer">basic income</a> (or UBI) is "<strong>a periodic cash payment, unconditionally delivered to all on an individual basis, without means test or work requirement</strong>." Although such payments without conditions already exist upon a mountain of evidence, post-2020, experiments have exploded in cities across the U.S. thanks to the efforts of Mayors for Guaranteed Income (MGI) which was founded in 2020 by former Mayor Michael Tubbs after the success of the pilot in <a href="https://sp2.upenn.edu/study-guaranteed-income-improved-peoples-health-during-pandemic/?ref=scottsantens.com" rel="noreferrer">Stockton, CA</a> that provided $500 a month to 125 people for 2 years. The biggest findings there were that <strong>full-time employment grew at twice the rate of the control group, and mental health improved significantly</strong>. Yes, despite the common fear that people provided basic income would work less, in Stockton, they worked more, and the mental health impact was comparable to medication.</p><p>Since the Stockton pilot ended, there have been dozens of other completed pilots with completed reports, all of which report the same general findings over and over again. Employment does not go down to any worrisome degree, and often actually goes up, with people finding better jobs and better pay, and where wage work is reduced, people invest in schooling or pursue unpaid work or self-employment. With each experiment's results, the case for UBI becomes stronger, and it's clear that <strong>some very wealthy people don't like those results</strong>.</p><p>In March of 2021 and again in late 2022, <a href="https://www.kxan.com/news/texas-politics/new-texas-bill-aimed-at-austins-guaranteed-income-program/?ref=scottsantens.com" rel="noreferrer">Texas</a> became the first state to attempt to stop more results. <a href="https://capitol.texas.gov/BillLookup/History.aspx?LegSess=87R&amp;Bill=HB4550&amp;ref=scottsantens.com" rel="noreferrer">House Bill 4550</a> in 2021 and then <a href="https://legiscan.com/TX/bill/HB553/2023?ref=scottsantens.com" rel="noreferrer">House Bill 553</a> in 2022 both included the following wording:</p><blockquote>"PROHIBITION ON PROVIDING UNIVERSAL BASIC INCOME. (a) In this section, 'universal basic income' means unconditional cash grants of equal amounts issued on a regular basis to individual residents of a political subdivision. The term includes a basic income, monthly income, or minimum income paid to each individual resident of the political subdivision without regard to the individual ’s circumstances. (b) Notwithstanding any other law, a political subdivision may not adopt or enforce an ordinance, order, or other measure providing for a universal basic income."</blockquote><p>Both bills died in committee. In January 2024, a different approach was taken, with a request for the Texas attorney general to <a href="https://www.kxan.com/news/texas-politics/new-texas-bill-aimed-at-austins-guaranteed-income-program/?ref=scottsantens.com" rel="noreferrer">declare such pilots as unconstitutional</a>. It should be noted that as of Feb 2024, there have been seven basic income pilots launched in Texas. One of those that took place in Austin has already published <a href="https://twitter.com/scottsantens/status/1750178842487009775?ref=scottsantens.com" rel="noreferrer">its results</a>. It found that <strong>a payment of $1,000 a month to 135 people for one year led to 9% of participants working less and 7% working more</strong>, and of those who worked less, half upskilled for better future jobs, and half chose unpaid care work. Housing security also significantly increased, as did food security, participants lived in better housing and ate more balanced meals, and they also felt significantly more connected to the people and places in their neighborhoods. A ban would have prevented these findings.</p><p>In April of 2023, <a href="https://archive.ph/x5umb?ref=scottsantens.com" rel="noreferrer">Wisconsin</a> became the next state to attempt to stop more results. At the time, there was a pilot in the city of Madison that wasn't quite done yet, where 155 parents of kids under age 18 got $500 a month for one year. The bill would not have stopped that pilot because it was privately-funded, but the bill was written to stop any future pilots from using any state funds to test "regular periodic cash payments that are unearned and that may be used for any purpose." The bill passed the Wisconsin House and Senate and died by veto by Governor Evers.</p><p>In 2024, the anti-UBI bill floodgates opened, starting with <a href="https://www.desmoinesregister.com/story/news/politics/2024/01/18/iowa-legislature-bill-would-ban-guaranteed-income-programs/72269962007/?ref=scottsantens.com" rel="noreferrer">Iowa</a> in January and followed in quick succession by <a href="https://www.dominionpost.com/2024/02/07/house-panel-halts-home-based-work-bill-advances-one-prohibiting-universal-basic-income/?ref=scottsantens.com" rel="noreferrer">West Virginia</a>, <a href="https://www.thecentersquare.com/south_dakota/article_c404efee-d26d-11ee-9fd8-e79c3b36aa40.html?ref=scottsantens.com" rel="noreferrer">South Dakota</a>, <a href="https://azmirror.com/blog/arizona-republicans-want-to-ban-guaranteed-basic-income-programs/?ref=scottsantens.com" rel="noreferrer">Arizona</a> and <a href="https://www.businessinsider.com/guaranteed-basic-income-program-ban-south-dakota-affordable-housing-groceries-2024-2?ref=scottsantens.com" rel="noreferrer">Arkansas</a> in February. All of them introduced bills of their own to stop basic income pilots, all with similar language. At this point, it became clear that a lobbying organization of some kind was behind the bills, something like the <a href="https://www.youtube.com/watch?v=aIMgfBZrrZ8&amp;ref=scottsantens.com" rel="noreferrer">American Legislative Exchange Council</a> that writes bills for legislators to put their names on and pass into law. In my research to discover the group responsible, I found it's the Foundation for Government Accountability, which led down a rabbit hole of dark money and a slew of harmful bills desired by the 1% to reduce their taxes and reduce the power of the 99% to stand in their way.</p><h3 id="who-is-the-fga">Who is the FGA?</h3><p>The Foundation for Government Accountability was founded in Florida in 2011 by Tarren Bragdon after cutting his chops in Maine at the Maine Heritage Policy Center and then as adviser to Maine's governor, LePage. It was in Maine where Bragdon and a cohort of fellow young conservatives gained a reputation for outrageous anti-welfare policies. “I remember them as <strong>a pack of inexperienced, activist right-wingers that went crazy on welfare reform</strong>,” said Cynthia Dill, a former state senator to the <a href="https://archive.ph/Q6fln?ref=scottsantens.com" rel="noreferrer">Washington Post in 2018</a>. “It galled me that <strong>they had no expertise whatsoever in health and human services</strong> but were appointed to places of power by the LePage administration.”</p><p>Bragdon's regressive work in Maine was only the beginning for him. He went on to export that work to every state he could and even the federal government too, starting in 2017 when the FGA attempted to expand the work requirements for SNAP to even include parents and limit waivers for states regardless of unemployment rates. The FGA reports now having relationships with <a href="https://thefga.org/about-us/?ref=scottsantens.com" rel="noreferrer">450 policymakers</a> across the country. Bragdon has described FGA's goal as wanting to "return America to a country where entrepreneurship thrives, personal responsibility is rewarded, and <strong>paychecks replace welfare checks</strong>," and that their approach is "to really tackle one big issue: how to give more Americans <strong>the life-changing power of work</strong>, at both the state and federal level.”</p><p>At this point, I will remind readers that <a href="https://www.scottsantens.com/did-spains-b-mincome-experiment-prove-that-unconditional-universal-basic-income-doesnt-work-ubi/" rel="noreferrer"><strong>universal basic income is quite different than welfare</strong></a> in how it doesn't get pulled away with work, which is why <strong>so many UBI pilots show increased employment</strong> for recipients since all wages from work increase their total income, whereas with conditional welfare they can be left barely better off financially, or even worse off. Means-tested welfare creates cliff effects, and cliff effects disincentivize work. I will also mention that if someone's goal is thriving entrepreneurship, it should be considered very intriguing how often <a href="https://www.scottsantens.com/universal-basic-income-will-accelerate-innovation-by-reducing-our-fear-of-failure/" rel="noreferrer"><strong>UBI pilots show large increases in entrepreneurship</strong></a>. That is, it should be interesting to those who truly value empirical evidence.</p><p>The FGA however is clearly not interested in empirical evidence. One of its first "studies" contributed to Florida Governor Rick Scott's defense of his controversial welfare drug-testing law, requiring benefit recipients to take a drug test as a qualification for benefits. A Bush-appointed federal judge threw out that study as evidence, claiming <strong>it was "not competent expert opinion"</strong> and that "even a cursory review of certain assumptions in the pamphlet undermines its conclusions."</p><p>Florida's law requiring drug tests for welfare applicants ended up identifying only 2.6% testing positive, significantly lower than the general population's rate of 8.13% in Florida. This directly contradicted justifications for the law, which also proved financially wasteful. Florida spent over $118,000 reimbursing those who tested negative, exceeding any program savings and resulting in a net cost exceeding $45,000. <a href="https://www.aclu.org/news/smart-justice/just-we-suspected-florida-saved-nothing-drug-testing-welfare?ref=scottsantens.com" rel="noreferrer"><strong>It cost more to apply the condition than it saved</strong></a>. It should also be noted that studies of <a href="https://www.scottsantens.com/what-do-we-do-about-drug-users-with-basic-incomes/" rel="noreferrer">unconditional cash programs tend to show a net reduction in drug use</a>.</p><p>In 2016, the FGA touted a study from Kansas of work requirements on SNAP which was panned by both liberal and conservative economists alike for cherry-picking data. “<strong>Work requirements should be based on credible evidence and attention to policy details — the exact opposite of what FGA produces</strong>,” <a href="https://twitter.com/PeteTheCitizen/status/985142798700761088?ref=scottsantens.com" rel="noreferrer">tweeted</a> Peter Germanis, a conservative&nbsp;economist who served in the Reagan and Bush administrations who went on to <a href="https://twitter.com/PeteTheCitizen/status/1660024728533311489?ref=scottsantens.com" rel="noreferrer">tweet</a>, “Tarren Bragdon bases his arguments to support work requirements on the <strong>junk science</strong> produced by the FGA – <strong>no serious researcher would accept their claims</strong>."</p><figure><blockquote><p lang="en" dir="ltr">REMINDER: Tarren Bragdon bases his arguments to support work requirements on the junk science produced by the FGA -- no serious researcher would accept their claims.<a href="https://t.co/tBkcZKLVVB?ref=scottsantens.com">https://t.co/tBkcZKLVVB</a></p>— Peter the Citizen (@PeteTheCitizen) <a href="https://twitter.com/PeteTheCitizen/status/1660024728533311489?ref_src=twsrc%5Etfw&amp;ref=scottsantens.com">May 20, 2023</a></blockquote>
</figure><p>Although the FGA itself is a 501(c)(3) that is prohibited from lobbying, it has a 501(c)(4) lobbying arm <a href="https://www.guidestar.org/profile/47-3125722?ref=scottsantens.com" rel="noreferrer">FGA Action</a> which operates as the <a href="https://solutionsproject.org/about/?ref=scottsantens.com" rel="noreferrer">Opportunity Solutions Project</a>. Tarren Bragdon is the CEO of both the FGA and OSP. With an annual budget that has been steadily rising from $212,000 in 2011, to over $14 million in 2022, the FGA has been busy attracting a pool of wealthy benefactors as it has pursued: expanded work requirements for everyone in all government assistance programs aka <a href="https://www.vox.com/policy-and-politics/2019/9/4/20835692/conservative-think-tank-foundation-for-government-accountability-food-stamps-snap-poverty-welfare?ref=scottsantens.com" rel="noreferrer">universal work requirements</a>, <a href="https://archive.ph/8Yiil?ref=scottsantens.com#selection-349.0-349.168" rel="noreferrer">the rolling back of child labor protections</a>, the expansion of measures to restrict the vote and <a href="https://thefga.org/blog/these-states-are-banning-ranked-choice-voting-yours-should-too/?ref=scottsantens.com" rel="noreferrer">ban ranked-choice voting</a>, and <a href="https://www.theguardian.com/us-news/2023/jun/23/foundation-government-accountability-democracy?ref=scottsantens.com" rel="noreferrer">the derailing of citizen-led ballot initiatives</a> to protect abortion rights, raise the minimum wage, or expand Medicaid, by pushing for all ballot initiatives to require 60% of the vote instead of a simple majority. And their newest battle is against unconditional basic income, <strong>because of its lack of conditions</strong>.</p><p>It was on February 13, 2024 that the FGA went public about its opposition to UBI with the publication of its paper "<a href="https://thefga.org/research/why-states-should-ban-universal-basic-income-schemes/?ref=scottsantens.com" rel="noreferrer">Why States Should Ban Universal Basic Income Schemes</a>." Their main premise is that basic income discourages work, which of course isn't what the evidence shows. Amusingly, their paper cites a source for virtually every sentence, except for this one, "These programs disincentivize work and promote increased dependency on government handouts, at the expense of individual responsibility," and this one, "By providing generous benefits designed to replace income, universal basic income discourages individuals from working."</p><p>A 2020 <a href="https://www.mdpi.com/2071-1050/12/22/9459?ref=scottsantens.com" rel="noreferrer">peer-reviewed systematic review of 38 studies of basic income</a> reached the following conclusion, my emphasis added in bold:</p><blockquote>"Despite a detailed search, we have not found any evidence of a significant reduction in labor supply. Instead, we found evidence that <strong>labor supply increases</strong> globally among adults, men and women, young and old, and the existence of some insignificant and functional reductions to the system such as a decrease in workers from the following categories: Children, the elderly, the sick, those with disabilities, women with young children to look after, or young people who continued studying. These reductions do not reduce the overall supply since <strong>it is largely offset by increased supply</strong> from other members of the community."</blockquote><p>All the reports of the newer basic income pilots that have been published since that review have only further strengthened the review's conclusion. Over and over again, in city after city, work has either increased or not significantly decreased.</p><p>The FGA anti-UBI paper also compares the boosted unemployment insurance payments during the pandemic to UBI, which anyone who understands UBI knows is quite different. <strong>Paying people on the condition they remain unemployed is not at all the same thing as paying people regardless of their employment status</strong>. One creates a work disincentive and the other doesn't. It is this difference that all the latest generation of pilots are testing. What happens when someone gets to keep a payment in addition to their paycheck, instead of losing it? The basic income pilots are answering that question using the scientific method to compare treatment groups to control groups.</p><p>A week after FGA published its anti-UBI paper, it planted an op-ed in the <a href="https://www.dallasnews.com/opinion/commentary/2024/02/19/guaranteed-universal-basic-income-programas-trap-people-in-dependency/?ref=scottsantens.com" rel="noreferrer">Dallas Morning News</a>, just as it and similar groups often do as part of their overall strategy. The op-ed made no mention of <a href="https://twitter.com/scottsantens/status/1761392672302866456?ref=scottsantens.com" rel="noreferrer">the positive results of the pilot in Texas</a> that had just been published a month prior. Instead it made claims based on the pilots from the 1970s, which were quite different in design, and although do have something to tell us about basic income, need to be looked at <a href="https://www.researchgate.net/publication/222578698_A_failure_to_communicate_What_if_anything_can_we_learn_from_the_negative_income_tax_experiments?ref=scottsantens.com" rel="noreferrer">in their full context</a>, like for example the high marginal tax rates above and beyond 50% that they tested, and the fact that self-reporting working less meant a larger payment.</p><p>From now going forward, if you make a point of looking, you'll find quotes from the FGA in articles about bills to ban basic income pilots at the state level. What you won't find is any mention of <a href="https://iseralaska.org/static/legacy_publication_links/bien_xiii_ak_pfd_lessons.pdf?ref=scottsantens.com" rel="noreferrer">Alaska's UBI</a> which it has had since 1982. You won't find any mention of how studies have shown <a href="https://harris.uchicago.edu/news-events/news/universal-basic-income-policies-dont-cause-people-leave-workforce-study-finds?ref=scottsantens.com" rel="noreferrer">it has increased employment</a> there, or how it has improved the <a href="https://onlinelibrary.wiley.com/doi/10.1111/ecin.12235?ref=scottsantens.com" rel="noreferrer">health of mothers and babies</a>, or how it has reduced <a href="https://scholarworks.alaska.edu/handle/11122/11998?ref=scottsantens.com" rel="noreferrer">obesity</a> and <a href="https://www.nber.org/papers/w31733?ref=scottsantens.com" rel="noreferrer">child abuse</a>. And most importantly of all, something else you won't find in FGA's anti-UBI hit pieces, is the names of FGA's funders.</p><h3 id="who-is-funding-the-fga">Who is Funding the FGA?</h3><p>According to the Center for Media and Democracy's <a href="https://www.sourcewatch.org/index.php?title=Foundation_for_Government_Accountability&amp;ref=scottsantens.com" rel="noreferrer">SourceWatch</a>, the largest single donor to FGA has been the <strong>Ed Uihlein Family Foundation</strong>, with a total contribution from 2014-2021 of <strong>$17.85 million</strong>. Both in their 70s, the Uihleins (pronounced YOU-line) are a husband and wife team, Richard and Liz, worth <a href="https://www.forbes.com/profile/richard-uihlein/?sh=61f890ce161b&amp;ref=scottsantens.com" rel="noreferrer">around $5 billion</a>. Together, the couple is the fourth biggest donor to political campaigns in the U.S., having reported giving over $190 million. The New York Times described them in 2018 as t<a href="https://archive.ph/phzx7?ref=scottsantens.com" rel="noreferrer">he most powerful couple you've never heard of</a>. In 2023 as reported by <a href="https://www.theguardian.com/us-news/2023/jun/23/foundation-government-accountability-democracy?ref=scottsantens.com" rel="noreferrer">The Guardian</a>, the Uihleins were "one of the <a href="https://www.thedailybeast.com/inside-the-billionaire-backed-dick-uihlein-hub-for-election-denial-restoration-action?ref=scottsantens.com">key funders</a> of <a href="https://www.propublica.org/article/uline-uihlein-election-denial?ref=scottsantens.com">election denial</a>," having poured "<a href="https://www.opensecrets.org/news/2022/03/millions-of-dollars-poured-into-a-dark-money-group-tied-to-billionaire-backed-super-pac-and-efforts-to-expose-voter-rolls/?ref=scottsantens.com">tens of millions</a> into the 'Restoration of America' network that <a href="https://www.restorationofamerica.com/restoration-news/eric/left-plans-to-go-postal-on-what-remains-of-americas-election-integrity/?ref=scottsantens.com">promotes</a> <a href="https://www.restorationofamerica.com/restoration-news/eric/eric-the-best-data-money-cant-buy-pt-1/?ref=scottsantens.com">ludicrous</a> <a href="https://www.restorationofamerica.com/restoration-news/eric/the-voter-registration-machine-flipping-the-states-blue/?ref=scottsantens.com">election</a> <a href="https://www.restorationofamerica.com/restoration-news/eric/the-left-wing-election-swamp-huddles/?ref=scottsantens.com">conspiracy</a> <a href="https://www.restorationofamerica.com/restoration-news/eric/the-left-mines-for-votes-in-americas-prisons/?ref=scottsantens.com">theories</a>," and "in the 2022 cycle, <a href="https://www.propublica.org/article/uline-uihlein-election-denial?ref=scottsantens.com">were also top donors</a> to election-denying candidates." The Uihleins were also <a href="https://www.chicagotribune.com/2022/07/26/darren-bailey-declines-to-answer-questions-on-trump-jan-6-committee-and-call-to-censure-adam-kinzinger/?ref=scottsantens.com" rel="noreferrer">one of the biggest contributors</a> to the "March To Save America" rally that preceded the violent insurrection on January 6.</p><p>The second biggest donor to the FGA is <strong>Donors Trust</strong> and its affiliate organization Donors Capital Fund, with a total contribution of <strong>$17.2 million</strong> from 2014 to 2022. Founded by <a href="https://reason.com/2015/08/18/whitney-ball-founder-of-donorstrust-rip/?ref=scottsantens.com" rel="noreferrer">a pair of activist libertarians</a>, the combo are two of the most influential conservative organizations around. In 2013, Mother Jones dubbed them the “<a href="https://www.motherjones.com/politics/2013/02/donors-trust-donor-capital-fund-dark-money-koch-bradley-devos/?ref=scottsantens.com"><u>dark-money ATM of the right</u></a>.” Donors Trust allows wealthy contributors who want to donate millions to do so anonymously,&nbsp;essentially scrubbing&nbsp;the identity of those underwriting organizations like the FGA. If you're a rich person who doesn't like the idea of UBI and how it will likely raise your taxes, you can give to Donors Trust and let them give the FGA your money for you, protecting your identity from those who would like to know you're fighting against boosting their incomes with a basic income floor.</p><p>The FGA's third biggest donor, with a total of <strong>$5.3 million</strong> from 2015 to 2020 is the Vanguard Charitable Endowment which is a large donor advised fund (DAF) where donors can drop money in to get an immediate tax deduction, then request where they'd like that money to go. It's another way of keeping one's anonymity. DAFs also <a href="https://theintercept.com/2023/06/08/christopher-rufo-nonprofit-dark-money/?ref=scottsantens.com" rel="noreferrer">help enable a public charity to stay a public charity</a>. Besides Vanguard, another of FGA's biggest donors is another big DAF - Fidelity Investments Charitable Gift Fund - with <strong>$1.3 million</strong> passed through them. Vanguard and Fidelity are not political DAFs in any way. They're just very popular DAFs to use.</p><p>Coming in at number four, five, and six is the <a href="https://www.sourcewatch.org/index.php?title=Sarah_Scaife_Foundation&amp;ref=scottsantens.com" rel="noreferrer">Sarah Scaife Foundation</a> with a total of <strong>$3.2 million</strong>, the <a href="https://www.sourcewatch.org/index.php?title=Lynde_and_Harry_Bradley_Foundation&amp;ref=scottsantens.com" rel="noreferrer">Lynde and Harry Bradley Foundation</a> with <strong>$2.75 million</strong>, and the <a href="https://www.sourcewatch.org/index.php?title=Searle_Freedom_Trust&amp;ref=scottsantens.com" rel="noreferrer">Searle Freedom Trust</a> with <strong>$2.15 million</strong>. Together, these three organizations have been described as "<a href="https://www.eenews.net/articles/meet-the-dead-industrialists-funding-climate-denialism/?ref=scottsantens.com" rel="noreferrer">a source of 'baseload' funding</a> for organizations that have battled the government for the last 40 years." According to Kert Davies, director of the nonprofit Climate Investigations Center, "<strong>They are fundamentalists about hating the government, hating regulation and trying to stop any progress</strong> on things like climate change because they see it as almost a step toward communism, it’s almost that stark." The Bradley Foundation has provided grants to the FGA specifically to support projects “reducing the welfare state and restoring the working class."</p><p>The seventh biggest FGA funder is the <a href="https://www.sourcewatch.org/index.php?title=Dunn_Foundation&amp;ref=scottsantens.com" rel="noreferrer">Dunn Foundation</a> with <strong>$2.4 million</strong> from 2016 to 2022. Founded in 1993 by Florida multimillionaire <a href="https://www.sourcewatch.org/index.php?title=William_A._Dunn&amp;ref=scottsantens.com">William A. Dunn</a>. According to the trust agreement, the Dunn Foundation aims to "advance the understanding and practice of classical liberalism, market capitalism, free enterprise, individual political and economic liberty and to reduce the impact of the use of threat of force by coercive organizations (both public and private) against the people of America and the world, principally through education and persuasion." It's a shame that the Dunn Foundation apparently doesn't seem to see basic income in the same way <a href="https://iai.tv/articles/why-friedmans-free-market-needs-basic-income-joshua-preiss-auid-2452?_auid=2020&amp;ref=scottsantens.com" rel="noreferrer">Milton Friedman</a> and <a href="https://www.libertarianism.org/columns/why-did-hayek-support-basic-income?ref=scottsantens.com" rel="noreferrer">Friedrich Hayek</a> both did. Here's what libertarian Matt Zwolinski has written about Hayek's support:</p><blockquote>"A&nbsp;basic income gives people an option – to exit the labor market, to relocate to a&nbsp;more competitive market, to invest in training, to take an entrepreneurial risk, and so on. And the existence of that option allows them to escape subjection to the will of others. It enables them to say 'no' to proposals that only extreme desperation would ever drive them to accept. It allows them to govern their lives according to their own plans, their own goals, and their own desires. It enables them to be free."</blockquote><p>Finally, the eighth biggest funder is the <a href="https://www.sourcewatch.org/index.php?title=The_85_Fund&amp;ref=scottsantens.com" rel="noreferrer">85 Fund</a> with a total of <strong>$2 million </strong>in 2020. The 85 Fund is Leonard Leo's, and a rebrand of the Judicial Education Project and the Honest Election Project. Everyone should know who <a href="https://www.propublica.org/article/we-dont-talk-about-leonard-leo-supreme-court-supermajority?ref=scottsantens.com" rel="noreferrer">Leonard Leo</a> is by now. He's the guy chiefly responsible for the transformation of the Supreme Court and the overturning of Roe v. Wade. He's one of the most prolific fundraisers in American politics. Under his watch, groups in his orbit have raised more than $600 million, and in 2021, he was given a war chest of <a href="https://www.propublica.org/article/dark-money-leonard-leo-barre-seid?ref=scottsantens.com" rel="noreferrer">$1.6 billion</a> by billionaire Barre Seid in the largest political advocacy donation in US history.</p><p>Beginning in 2021, a year after Leo's donation provided 19% of FGA's funding for the year, the FGA began filing <a href="https://legaldictionary.net/amicus-brief/?ref=scottsantens.com" rel="noreferrer">amicus briefs</a> in Supreme Court cases. In Biden v. Nebraska, the FGA argued against debt forgiveness. In Consumer Financial Protection Bureau (CFPB) v. Community Financial Services Association of America, the FGA is trying to kill the CFPB for the payday loan industry (which <a href="https://www.scottsantens.com/payday-loan-lenders-are-unstoppable-except-with-the-help-of-universal-basic-income-ubi/" rel="noreferrer">UBI would destroy</a> by the way). And in Loper Bright Enterprises v. Raimondo, the FGA is trying to end the <a href="https://www.scotusblog.com/2024/01/supreme-court-to-hear-major-case-on-power-of-federal-agencies/?ref=scottsantens.com" rel="noreferrer">Chevron Doctrine</a>, which would greatly reduce the power of federal agencies.</p><p>The rest of FGA's donors have all contributed less than $1 million total. For those who would like to see them, you can find the rest of them on SourceWatch.</p><h3 id="who-will-win">Who Will Win?</h3><p>The aforementioned names are only a partial list, and include few actual names because of the nature of dark money, where those with great wealth can pass an unlimited amount of money through various vehicles that hide their identities. But the overall picture painted is a pretty clear one, where an entire ecosystem of nonprofits has been built and is being run for one primary purpose - <strong>to protect the vast wealth of the wealthy from higher taxes</strong>. It is this singular reason that I believe a handful of wealthy donors are supporting the efforts of the FGA to fight against UBI. It's not really about conservative or libertarian against liberal or progressive. If it were, at least some of the above organizations would be interested in basic income as Friedman and Hayek were.</p><p>A UBI would decrease the disincentive to work by reducing marginal tax rates at the low end of the labor market. It would make taking a job actually pay. It would shrink the size of government by replacing government bureaucrats with cash payments directly to citizens to make their own choices. It would greatly boost entrepreneurship and fuel small businesses across the country, restoring Main Street USA in a way nothing else ever could. It could also potentially be an alternative to minimum wage increases by increasing worker bargaining power. All of this should be very appealing to principled conservatives and libertarians seeking market solutions and greater freedom of choice. UBI would even be a tax cut for the bottom 60% to 90% of the country depending on design details. But what UBI will never be is a tax cut for the top 10%, and especially the top 1%.</p><p>There is no politically realistic UBI that would ever cut taxes on billionaires, and that's what's fueling the Foundation for Government Accountability - the fear of a world where things are a bit less unequal, and the bottom half of the country has a bit more power to refuse the domination of others, particularly those with wealth who have grown used to so many people having <a href="https://widerquist.com/mandatory-participation-on-trial-part-1/?ref=scottsantens.com" rel="noreferrer">no power to say anything but yes</a>.</p><p>It's impossible to know if the donor class is thinking this far ahead, but one of the reasons I personally consider UBI so important, is that it also means a greater ability for more working class people to donate. Instead of getting money out of politics, which is never going to happen without a successful constitutional amendment and therefore virtually impossible, <strong>UBI would help even the political donor playing field</strong>. One person donating $10 million dollars could be balanced by a million people donating $10 each. Combined with a national program that <a href="http://nyccfb.info/program/how-it-works/?ref=scottsantens.com" rel="noreferrer">matched small donations</a> with public funding, a billionaire spending $5 billion to get their way could face 100 million people spending $10 each, which becomes $100 after matching, which becomes $10 billion, finally drowning out the power of the billionaire.</p><p>The Catch-22 is how difficult UBI will be to win without UBI, but once UBI is won, so many more things will become possible once the politically motivated billionaires are taxed more, and the power of their dollars is greatly eroded by UBI dollars going to causes, campaigns, and candidates that the People truly support.</p><p>That's the world the FGA and its donors fear. They fear a loss of power, and so they fear the successful results of basic income pilots. They don't want everyone to know how well basic income works. They don't want people to want UBI. <strong>A world full of people empowered by unconditional basic income scares them.</strong></p><p>As for me, I'll see you in that world, because I'm never stopping until we're all in it. Always remember, what comes after "then they fight you" is "then you win."</p><hr><p><em>If you're looking to support a 501(c)(3) that's pretty much the exact opposite of the FGA, consider the </em><a href="https://www.scottsantens.com/introducing-the-income-to-support-all-foundation-itsa-ubi/" rel="noreferrer"><em>Income To Support All Foundation</em></a><em> whose goal is UBI.</em></p><hr><p><em>Want more content like this? Please share it and&nbsp;</em><a href="https://www.scottsantens.com/#/portal/signup"><em>click the subscribe button</em></a><em>. Also&nbsp;<strong>consider making a monthly pledge</strong>&nbsp;in support of all my work.</em></p><p><em><strong>Special thanks to my monthly supporters on Patreon</strong>: Gisele Huff, Haroon Mokhtarzada, Steven Grimm, Judith Bliss, Lowell Aronoff, Katie Moussouris, David Ruark, Tricia Garrett, Zack Sargent, Daryl Smith, Larry Cohen, Fabian Kehrer, Philip Rosedale, Liya Brook, Frederick Weber, John Steinberger, Bridget I Flynn, Laurel gillespie, Dylan J Hirsch-Shell, Tom Cooper, Robert Collins, Joanna Zarach, ace bailey, Daragh Ward, Andrew Yang, Peter T Knight, Michael Finney, David Ihnen, Elizabeth Corker, Gerald Huff, Albert Daniel Brockman, Natalie Foster, Joe Ballou, Arjun , Mark Donovan, Capitalists for Shared Income, Jason Clark, Chuck Cordes, Thomas Fitzsimmons, Mark Broadgate, Leslie Kausch, Jessica Chew, Braden Ferrin , Juro Antal, Austin Begin, Deanna McHugh, Nikolaus Rath, chris heinz, Pavel S, Zachary Weaver, Justin Seifert, Jodi Sarda, Rosa Tran, Ryan Ash-Miller, miki, bradzone, Lee Lor, John Sullivan, Team TJ, Yang Deng, Yan Xie, Marie janicke, engageSimply - Judy Shapiro, Tim , Warren J Polk, Jeffrey Emmett, Stephen Castro-Starkey, Kev Roberts, Walter Schaerer, Loren Sickles, Eric Skiff, Thomas Welsh, Kai Wong, and Laura Ashby.</em></p><p><a href="https://www.patreon.com/scottsantens?ref=scottsantens.com" rel="noreferrer"><strong><em>Become a monthly patron to see your name here too</em></strong></a><strong><em>!</em></strong></p>
      </section>


        
          <section>
    <p><a href="https://www.scottsantens.com/author/scott/">
        <img data-src="/content/images/size/w320/2022/02/HS-late-2021-1.jpg" alt="Scott Santens" width="80" height="80" src="https://www.scottsantens.com/content/images/size/w320/2022/02/HS-late-2021-1.jpg">
      </a>
    </p>

  <div>
    

      <p>Unconditional/Universal Basic Income (UBI) advocate with a crowdfunded basic income; Founder and President of ITSA Foundation, Author of Let There Be Money; Editor of BasicIncomeToday.com</p>
  </div>
</section>
        
        <div>
                <h2>UBI Guide Newsletter</h2>
                <p>Join the newsletter to receive the latest updates in your inbox.</p>
                
<form data-members-form="signup">
  <p><label for="subscribe-box-email">Your email address</label>
    
    
  </p>

  <p>Please check your inbox and click the link to confirm your subscription.</p>
  <p>Please enter a valid email address!</p>
  <p>An error occurred, please try again later.</p>
</form>              </div>

        
    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shave and a Haircut (189 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Shave_and_a_Haircut</link>
            <guid>39548517</guid>
            <pubDate>Thu, 29 Feb 2024 12:31:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Shave_and_a_Haircut">https://en.wikipedia.org/wiki/Shave_and_a_Haircut</a>, See on <a href="https://news.ycombinator.com/item?id=39548517">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">

<div><figure typeof="mw:File"><span><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Gnome-mime-sound-openclipart.svg/50px-Gnome-mime-sound-openclipart.svg.png" decoding="async" width="50" height="50" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Gnome-mime-sound-openclipart.svg/75px-Gnome-mime-sound-openclipart.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Gnome-mime-sound-openclipart.svg/100px-Gnome-mime-sound-openclipart.svg.png 2x" data-file-width="160" data-file-height="160"></span><figcaption></figcaption></figure></div>
<p>"<b>Shave and a Haircut</b>" and the associated response "<b>two bits</b>" is a seven-note musical <a href="https://en.wikipedia.org/wiki/Call_and_response_(music)" title="Call and response (music)">call-and-response</a> <a href="https://en.wikipedia.org/wiki/Couplet" title="Couplet">couplet</a>, <a href="https://en.wikipedia.org/wiki/Riff" title="Riff">riff</a> or <a href="https://en.wikipedia.org/wiki/Fanfare" title="Fanfare">fanfare</a> popularly used at the end of a musical performance, usually for comedic effect. It is used melodically or rhythmically, for example as a <a href="https://en.wikipedia.org/wiki/Door_knocker" title="Door knocker">door knocker</a>.
</p><p>"<a href="https://en.wikipedia.org/wiki/Bit_(money)" title="Bit (money)">Two bits</a>" is a term in the United States and Canada for 25 <a href="https://en.wikipedia.org/wiki/Cent_(currency)" title="Cent (currency)">cents</a>, equivalent to a <a href="https://en.wikipedia.org/wiki/Quarter_(United_States_coin)" title="Quarter (United States coin)">U.S. quarter</a>. "Four bits" and "Six bits" are also occasionally used, for example in the cheer "Two bits, four bits, six bits, a dollar." The final words may also be "get lost", "drop dead" (in Australia),<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2019)">citation needed</span></a></i>]</sup> or some other facetious expression. In the UK, it was often said as "five bob" (slang for five <a href="https://en.wikipedia.org/wiki/Shilling" title="Shilling">shillings</a>), although words are now rarely used to accompany the rhythm or the tune.
</p>
<meta property="mw:PageProp/toc">
<h2><span id="History">History</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Shave_and_a_Haircut&amp;action=edit&amp;section=1" title="Edit section: History"><span>edit</span></a><span>]</span></span></h2>
<p>An early occurrence of the tune is from an 1899 Charles Hale <a href="https://en.wikipedia.org/wiki/Minstrel_show" title="Minstrel show">minstrel</a> song, <i>At a Darktown Cakewalk</i>.<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> Other songs from the same period also used the tune. The same notes form the <a href="https://en.wikipedia.org/wiki/Bridge_(music)" title="Bridge (music)">bridge</a> in the <i>Hot Scotch Rag</i>, written by H. A. Fischler in 1911.
</p><p>An early recording used the 7-note tune at both the beginning and the ending of a humorous 1915 song, by <a href="https://en.wikipedia.org/wiki/Billy_Murray_(singer)" title="Billy Murray (singer)">Billy Murray</a> and the American Quartet, called "<a href="https://en.wikipedia.org/wiki/On_the_5:15" title="On the 5:15">On the 5:15</a>".
</p><p>In his 1933 novel, <i>Hizzoner the Mayor,</i> <a href="https://en.wikipedia.org/wiki/Joel_Sayre" title="Joel Sayre">Joel Sayre</a> wrote of boats "tooting the official Malta welcome blast to the tempo of “Shave-and-a-haircut-two-bits, shave-and-a-haircut-two-bits, shave-and-a-haircut-two-bits”, which was soon taken up by every craft in the harbor that had a boiler.<sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup>
</p><p>In 1939, Dan Shapiro, Lestor Lee and <a href="https://en.wikipedia.org/wiki/Milton_Berle" title="Milton Berle">Milton Berle</a> released "Shave and a Haircut – Shampoo",<sup id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup> which used the tune in the closing bars. In the same year, Rosalind Rosenthal and Herbert Halpert recorded "Shave and a Haircut, Bay Rum".<sup id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup>
</p>
<h2><span id="Popularity">Popularity</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Shave_and_a_Haircut&amp;action=edit&amp;section=2" title="Edit section: Popularity"><span>edit</span></a><span>]</span></span></h2>
<p>The tune can be heard on customized <a href="https://en.wikipedia.org/wiki/Car_horn" title="Car horn">car horns</a>,<sup id="cite_ref-Guide_5-0"><a href="#cite_note-Guide-5">[5]</a></sup><sup id="cite_ref-Ask_6-0"><a href="#cite_note-Ask-6">[6]</a></sup> while the rhythm may be tapped as a door knock<sup id="cite_ref-Hellholes_7-0"><a href="#cite_note-Hellholes-7">[7]</a></sup><sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup><sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup><sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup><sup id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup><sup id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup><sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup><sup id="cite_ref-Slang_14-0"><a href="#cite_note-Slang-14">[14]</a></sup> or as a <a href="https://en.wikipedia.org/wiki/Morse_code" title="Morse code">Morse code</a> "dah-di-di-dah-di, dah-dit" ( <b>–··–· &nbsp; –·</b> )<sup id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup> at the end of an <a href="https://en.wikipedia.org/wiki/Amateur_radio" title="Amateur radio">amateur radio</a> <a href="https://en.wikipedia.org/wiki/Contact_(amateur_radio)" title="Contact (amateur radio)">contact</a>.
</p><p>The former <a href="https://en.wikipedia.org/wiki/Prisoner_of_war" title="Prisoner of war">prisoner of war</a> and U.S. Navy seaman <a href="https://en.wikipedia.org/wiki/Doug_Hegdahl" title="Doug Hegdahl">Doug Hegdahl</a> reports fellow U.S. captives in the Vietnam War would authenticate a new prisoner's U.S. identity by using "Shave and a Haircut" as a <a href="https://en.wikipedia.org/wiki/Shibboleth" title="Shibboleth">shibboleth</a>, tapping the first five notes against a cell wall and waiting for the appropriate response. U.S. POWs were then able to communicate securely with one another via a <a href="https://en.wikipedia.org/wiki/Tap_code" title="Tap code">tap code</a>.<sup id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>
</p><p>The tune has been used innumerable times as a <a href="https://en.wikipedia.org/wiki/Coda_(music)" title="Coda (music)">coda</a> or <a href="https://en.wikipedia.org/wiki/Cadence_(music)" title="Cadence (music)">ending</a> in musical pieces. It is strongly associated with the stringed instruments of <a href="https://en.wikipedia.org/wiki/Bluegrass_music" title="Bluegrass music">bluegrass music</a>, particularly the 5-string <a href="https://en.wikipedia.org/wiki/Banjo" title="Banjo">banjo</a>. <a href="https://en.wikipedia.org/wiki/Earl_Scruggs" title="Earl Scruggs">Earl Scruggs</a> often ended a song with this <a href="https://en.wikipedia.org/wiki/Phrase_(music)" title="Phrase (music)">phrase</a> or a variation of it. On the television show <i><a href="https://en.wikipedia.org/wiki/The_Beverly_Hillbillies" title="The Beverly Hillbillies">The Beverly Hillbillies</a></i>, musical cues signifying the coming of a commercial break (cues which were in <a href="https://en.wikipedia.org/wiki/Bluegrass_music" title="Bluegrass music">bluegrass</a> style) frequently ended with "Shave and a Haircut". It is the most popular bluegrass <a href="https://en.wikipedia.org/wiki/Run_(music)" title="Run (music)">run</a>, after the <a href="https://en.wikipedia.org/wiki/G_run" title="G run">G run</a>.<sup id="cite_ref-Traum_17-0"><a href="#cite_note-Traum-17">[17]</a></sup>
</p><p>"Shave and a Haircut" was used in many early <a href="https://en.wikipedia.org/wiki/Cartoon" title="Cartoon">cartoons</a>, particularly <i><a href="https://en.wikipedia.org/wiki/Looney_Tunes" title="Looney Tunes">Looney Tunes</a></i> cartoons. It was also used as an ending to many cartoon shows, just after the credits. Decades later, the couplet became a plot device to lure-out an intended victim, as used by <a href="https://en.wikipedia.org/wiki/Judge_Doom" title="Judge Doom">Judge Doom</a> in the film <i><a href="https://en.wikipedia.org/wiki/Who_Framed_Roger_Rabbit" title="Who Framed Roger Rabbit">Who Framed Roger Rabbit</a></i>, the idea being that <a href="https://en.wikipedia.org/wiki/Animated_cartoon" title="Animated cartoon">toons</a> cannot resist finishing with the "two bits" when they hear the opening rhythm.<sup id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> The tune was also featured in early <a href="https://en.wikipedia.org/wiki/Nokia" title="Nokia">Nokia</a> phones, like the <a href="https://en.wikipedia.org/wiki/Nokia_3310" title="Nokia 3310">3310 model</a>, as the <i>That's it!</i> ringtone.<sup id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup><sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup>
</p>
<h2><span id="Usage">Usage</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Shave_and_a_Haircut&amp;action=edit&amp;section=3" title="Edit section: Usage"><span>edit</span></a><span>]</span></span></h2>
<p>The phrase has been incorporated into countless recordings and performances. Notable examples include:
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/Johnny%27s_Theme" title="Johnny's Theme">Johnny's Theme</a>, the music that opened <i><a href="https://en.wikipedia.org/wiki/The_Tonight_Show_Starring_Johnny_Carson" title="The Tonight Show Starring Johnny Carson">The Tonight Show Starring Johnny Carson</a>,</i> famously ended with the "shave and a haircut" flourish every weeknight for 30 years and 4,531 episodes.</li>
<li>"That's a Lot of Bunk", a 1920s novelty song composed by Al Wilson, James A. Brennan and Mack Henshaw, and performed by <a href="https://en.wikipedia.org/wiki/Billy_Jones_(singer,_born_1889)" title="Billy Jones (singer, born 1889)">Billy Jones</a> and <a href="https://en.wikipedia.org/wiki/Ernie_Hare" title="Ernie Hare">Ernest Hare</a>, known as "The Happiness Boys", closes with the riff.<sup id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup></li>
<li>The <a href="https://en.wikipedia.org/wiki/Crazy_Gang_(comedy_group)" title="Crazy Gang (comedy group)">Crazy Gang</a> sang "How's your father? Goodbye!" to the same tune at the end of their 1937 movie <i><a href="https://en.wikipedia.org/wiki/O-Kay_for_Sound" title="O-Kay for Sound">O-Kay for Sound</a></i>.<sup id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/R%26B" title="R&amp;B">R&amp;B</a> singer and bandleader <a href="https://en.wikipedia.org/wiki/Dave_Bartholomew" title="Dave Bartholomew">Dave Bartholomew</a> used the phrase on two of his recordings: "Country Boy" (1950) at the very end, and the original version of "<a href="https://en.wikipedia.org/wiki/My_Ding-a-Ling" title="My Ding-a-Ling">My Ding-a-Ling</a>" (1952) as a figure introducing each verse.<sup id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Les_Paul" title="Les Paul">Les Paul</a> and <a href="https://en.wikipedia.org/wiki/Mary_Ford" title="Mary Ford">Mary Ford</a>'s <a href="https://en.wikipedia.org/wiki/Capitol_Records" title="Capitol Records">Capitol</a> recording of "Magic Melody" concluded with the phrase minus the last two notes ("two bits"). Responding to complaints from <a href="https://en.wikipedia.org/wiki/Disc_jockey" title="Disc jockey">disc jockeys</a>, Capitol in 1955 released "Magic Melody Part 2"—consisting solely of the missing notes—on a 45, said to be the shortest tune on record.<sup id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/P._D._Q._Bach" title="P. D. Q. Bach">P. D. Q. Bach</a> ends his "<a href="https://en.wikipedia.org/wiki/Black_Forest_Bluegrass" title="Black Forest Bluegrass">Blaues Gras</a>" ("bluegrass") <a href="https://en.wikipedia.org/wiki/Aria" title="Aria">aria</a> with "Shave and a Haircut", sung in <a href="https://en.wikipedia.org/wiki/Denglisch" title="Denglisch">Denglisch</a> (mangled German and English): "Rasieren und Haarschneiden, zwei bitte" ("Shave and haircut, two please", ungrammatical in either language). "Zwei bitte" is a Denglisch pun, sounding like "two bits" to a speaker of both languages.<sup id="cite_ref-PDQ_Bach_25-0"><a href="#cite_note-PDQ_Bach-25">[25]</a></sup> The melody is also used in <i><a href="https://en.wikipedia.org/wiki/The_Short-Tempered_Clavier_and_other_dysfunctional_works_for_keyboard" title="The Short-Tempered Clavier and other dysfunctional works for keyboard">The Short-Tempered Clavier</a></i>.<sup id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup></li>
<li>The original version of "<a href="https://en.wikipedia.org/wiki/Love_and_Marriage" title="Love and Marriage">Love and Marriage</a>" by <a href="https://en.wikipedia.org/wiki/Frank_Sinatra" title="Frank Sinatra">Frank Sinatra</a> (recorded for <a href="https://en.wikipedia.org/wiki/Capitol_Records" title="Capitol Records">Capitol Records</a> in 1955) ends with the tune.</li>
<li>"<a href="https://en.wikipedia.org/wiki/Unsquare_Dance" title="Unsquare Dance">Unsquare Dance</a>" (1961) by <a href="https://en.wikipedia.org/wiki/Dave_Brubeck" title="Dave Brubeck">Dave Brubeck</a> ends with the tune, and also features part of "<a href="https://en.wikipedia.org/wiki/Turkey_in_the_Straw" title="Turkey in the Straw">Turkey in the Straw</a>".</li>
<li>One of the musical numbers in <i><a href="https://en.wikipedia.org/wiki/Mister_Magoo%27s_Christmas_Carol" title="Mister Magoo's Christmas Carol">Mister Magoo's Christmas Carol</a></i> (1962), "We're Despicable (The Plunderers' March)," incorporates the melody into its chorus.  The characters sing, "we're blank-blankety-blank-blank no good."</li>
<li>Every interview by <a href="https://en.wikipedia.org/wiki/Nardwuar_the_Human_Serviette" title="Nardwuar the Human Serviette">Nardwuar the Human Serviette</a> ends with the melody of the song, with Nardwuar singing "doot doot da loot doo", after which the interviewee is expected to reply with "doot doo".</li>
<li>The ending theme in the credits of <i><a href="https://en.wikipedia.org/wiki/Barney_%26_Friends" title="Barney &amp; Friends">Barney the Dinosaur</a></i> makes use of it from Seasons 1-3.</li>
<li>In a 1960s television comedy sketch called "The Time Window", <a href="https://en.wikipedia.org/wiki/Mike_Wallace" title="Mike Wallace">Mike Wallace</a> interviews <a href="https://en.wikipedia.org/wiki/Victor_Borge" title="Victor Borge">Victor Borge</a> who is portraying composer and pianist <a href="https://en.wikipedia.org/wiki/Franz_Liszt" title="Franz Liszt">Franz Liszt</a>. During the segment, Borge (Liszt) states that his very first composition were two notes; which he plays on the piano. He next demonstrates that without these two notes "we would never have had this", and he plays "Shave and a Haircut".<sup id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup><sup id="cite_ref-28"><a href="#cite_note-28">[28]</a></sup></li>
<li>The animated show <i><a href="https://en.wikipedia.org/wiki/Animaniacs" title="Animaniacs">Animaniacs</a></i> makes frequent use of this theme, in particular at the end of the song "Wakko's America" with the line "That's all the capitals there are".</li>
<li>The song "Gee, Officer Krupke" from <a href="https://en.wikipedia.org/wiki/Leonard_Bernstein" title="Leonard Bernstein">Leonard Bernstein</a>'s musical <i><a href="https://en.wikipedia.org/wiki/West_Side_Story" title="West Side Story">West Side Story</a></i> ends with the tune.</li>
<li>The tune is sampled in several of <a href="https://en.wikipedia.org/wiki/%22Weird_Al%22_Yankovic" title="&quot;Weird Al&quot; Yankovic">"Weird Al" Yankovic</a>'s <a href="https://en.wikipedia.org/wiki/List_of_%22Weird_Al%22_Yankovic_polka_medleys" title="List of &quot;Weird Al&quot; Yankovic polka medleys">polka medleys</a>.</li>
<li>"<a href="https://en.wikipedia.org/wiki/Everything_About_You_(Ugly_Kid_Joe_song)" title="Everything About You (Ugly Kid Joe song)">Everything About You</a>", by <a href="https://en.wikipedia.org/wiki/Ugly_Kid_Joe" title="Ugly Kid Joe">Ugly Kid Joe</a> (recorded for <a href="https://en.wikipedia.org/wiki/Mercury_Records" title="Mercury Records">Mercury Records</a> in 1992), ends with the tune.</li>
<li>The song "Mi Abuela" by Wilfred y La Ganga (BMG Ariola, 1990) opens with the tune as a door knock.</li>
<li>The tune is played as part of the guitar solo in the song "<a href="https://en.wikipedia.org/wiki/Play_with_Me_(song)" title="Play with Me (song)">Play with Me</a>" by <a href="https://en.wikipedia.org/wiki/Extreme_(band)" title="Extreme (band)">Extreme</a>, which is also used in the mall chase scene in <i><a href="https://en.wikipedia.org/wiki/Bill_%26_Ted%27s_Excellent_Adventure" title="Bill &amp; Ted's Excellent Adventure">Bill &amp; Ted's Excellent Adventure</a></i>.</li>
<li><a href="https://en.wikipedia.org/wiki/Cassian_Andor" title="Cassian Andor">Cassian Andor</a> taps the five-note rhythm to signal Bix Caleen, outside her window, in S1:E7 "The Announcement" of the series <a href="https://en.wikipedia.org/wiki/Andor_(TV_series)" title="Andor (TV series)"><i>Star Wars: Andor</i></a>. There is no two-note response.</li></ul>
<h2><span id="Uses_in_other_countries">Uses in other countries</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Shave_and_a_Haircut&amp;action=edit&amp;section=4" title="Edit section: Uses in other countries"><span>edit</span></a><span>]</span></span></h2>
<div>
<div><figure typeof="mw:File"><span><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Gnome-mime-sound-openclipart.svg/50px-Gnome-mime-sound-openclipart.svg.png" decoding="async" width="50" height="50" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Gnome-mime-sound-openclipart.svg/75px-Gnome-mime-sound-openclipart.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Gnome-mime-sound-openclipart.svg/100px-Gnome-mime-sound-openclipart.svg.png 2x" data-file-width="160" data-file-height="160"></span><figcaption></figcaption></figure></div>
<div>


<p>An example of the couplet.</p></div></div>
<p>The <a href="https://en.wikipedia.org/wiki/Italy" title="Italy">Italian</a> version is <i>Ammazza la vecchia … col Flit!</i> (<i>English</i>: "Kill the old lady … with Flit!")—<i><a href="https://en.wikipedia.org/wiki/FLIT" title="FLIT">Flit</a></i> being an old brand of <a href="https://en.wikipedia.org/wiki/DDT" title="DDT">DDT</a> insecticide. This is a humorous popular version of a post-<a href="https://en.wikipedia.org/wiki/World_War_II" title="World War II">World War II</a> commercial <i>Ammazza la mosca... col Flit</i> (<i>English</i>: "Kill the fly with Flit!").<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2013)">citation needed</span></a></i>]</sup>
</p><p>The tune is used in <a href="https://en.wikipedia.org/wiki/Catalan_language" title="Catalan language">Catalan</a> with a different lyric: "Nas de barraca … <a href="https://en.wikipedia.org/wiki/Sant_Boi_de_Llobregat" title="Sant Boi de Llobregat">Sant Boi</a>" (<i>English</i>: "Shack nose … Sant Boi"). It is also tapped, as a door knock. The Catalan lyrics may come from <a href="https://en.wikipedia.org/wiki/Blanes" title="Blanes">Blanes</a>, where it was sung twice with <i>Nas de barraca. Sant Boi. Cinc de carmelos pel noi</i> (<i>English</i>: Shack nose. Sant Boi. Five candies for the boy).<sup id="cite_ref-29"><a href="#cite_note-29">[29]</a></sup>
</p><p>In Spain, it is sung with the lyrics, <i>Una copita … de Ojén</i> (<i>English</i>: "A shot … of schnapps").
</p><p>In Mexico, it means a vulgar insult with the lyrics, <i>Chinga tu madre … cabrón</i> (English: "Fuck your mother … bastard").
</p><p>In <a href="https://en.wikipedia.org/wiki/Irish_pub" title="Irish pub">Irish barroom</a> music, the tune is sometimes tagged at the end of a song. The performer sings the first part to the lyrics, "How is your aul' one?" (read: "old one", a slang term for mother), to which the audience replies, "Gameball!" (A slang term meaning A-OK).<sup id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup>
</p><p>In Sweden, it is well known as <i>Kvart över elva … halv tolv</i>, which means <i>A quarter past eleven … half past eleven</i>. The twist doesn't work as well in English, as the English time system treats 11:30 as a continuation of eleven instead of as the first half of twelve. <i>Halv tolv</i> thus means <i>half twelve</i> and is the correct Swedish equivalent of half past eleven. In Sweden, the melody was also used in a commercial for the <a href="https://en.wikipedia.org/wiki/Lakrisal" title="Lakrisal">Bronzol</a> brand of candy with the slogan <i>Hälsan för halsen — Bronzol</i> (<i>English</i>: Health for the throat — Bronzol).
</p><p>In Icelandic, the lyrics are <i>Saltkjöt og baunir … túkall</i> (<i>English</i>: "Salt meat and split peas … two krona" (króna is the currency in Iceland)).
</p><p>In the Netherlands, the phrase is used when someone leaves with the intention to not return. <i>Die zien we nooit meer, te-rug</i> (<i>English</i>: We shall never see them, a-gain). It is used as a way to make fun of someone/something, if it suddenly disappears from the scene.
</p><p>In Argentina, <a href="https://en.wikipedia.org/wiki/Carlos_Bal%C3%A1" title="Carlos Balá">Carlos Balá</a>, a former children's TV show host, used to include a bit in his routine in which he would whistle the "shave and a haircut" part of the tune, prompting the children in the audience to answer "Ba-lá" to the rhythm of the two final notes.
In the same country in school context to call for silence being sung with the teacher saying the phrase Tapa Tapita (Bottlecap, Small cap) and the students answering Tapon (Plug), followed with the teacher singing the phrase cierro la boca (shutting my mouth) and answering ya está (already done)
</p>
<h2><span id="See_also">See also</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Shave_and_a_Haircut&amp;action=edit&amp;section=5" title="Edit section: See also"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Banjo_roll" title="Banjo roll">Banjo roll</a></li>
<li><a href="https://en.wikipedia.org/wiki/Oriental_riff" title="Oriental riff">Oriental riff</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bo_Diddley_beat" title="Bo Diddley beat">Bo Diddley beat</a></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Shave_and_a_Haircut&amp;action=edit&amp;section=6" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite id="CITEREFFuld2000">Fuld, James (2000). <i>The Book of World-Famous Music: Classical, Popular, and Folk</i> (5th&nbsp;ed.). New York: Dover Publications. p.&nbsp;495.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Book+of+World-Famous+Music%3A+Classical%2C+Popular%2C+and+Folk&amp;rft.place=New+York&amp;rft.pages=495&amp;rft.edition=5th&amp;rft.pub=Dover+Publications&amp;rft.date=2000&amp;rft.aulast=Fuld&amp;rft.aufirst=James&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><cite id="CITEREFSayre1933">Sayre, Joel (1933). <a rel="nofollow" href="https://books.google.com/books?id=m9QIAQAAIAAJ&amp;q=%22shave+and+a+haircut%22"><i>Hizzoner the Mayor: A Novel</i></a>. New York: John Day Company. pp.&nbsp;28–29.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Hizzoner+the+Mayor%3A+A+Novel&amp;rft.place=New+York&amp;rft.pages=28-29&amp;rft.pub=John+Day+Company&amp;rft.date=1933&amp;rft.aulast=Sayre&amp;rft.aufirst=Joel&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3Dm9QIAQAAIAAJ%26q%3D%2522shave%2Band%2Ba%2Bhaircut%2522&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span>"<a rel="nofollow" href="http://members.multimania.nl/catchytune/samples.html">Catchy Tune Central</a> <a rel="nofollow" href="https://web.archive.org/web/20100612111115/http://members.multimania.nl/catchytune/samples.html">Archived</a> 2010-06-12 at the <a href="https://en.wikipedia.org/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>", <i>Members.MultiMania.NL</i>.</span>
</li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><cite id="CITEREFSafire1983">Safire, William (April 3, 1983). <a rel="nofollow" href="https://www.nytimes.com/1983/04/03/magazine/on-language-pray-why-me.html">"ON LANGUAGE; PRAY, WHY ME?"</a>. <i><a href="https://en.wikipedia.org/wiki/The_New_York_Times" title="The New York Times">The New York Times</a></i><span>. Retrieved <span>May 21,</span> 2019</span>. <q>The Book of World-Famous Music," a 1966 work by James J. Fuld, which reveals a 1939 ditty, "Shave and a Haircut - Shampoo," by Dan Shapiro, Lester Lee and Milton Berle, and a similar number in the same year, "Shave and a Haircut, Bay Rum," recorded as a folk melody by Rosalind Rosenthal and Herbert Halpert.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=ON+LANGUAGE%3B+PRAY%2C+WHY+ME%3F&amp;rft.date=1983-04-03&amp;rft.aulast=Safire&amp;rft.aufirst=William&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F1983%2F04%2F03%2Fmagazine%2Fon-language-pray-why-me.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-Guide-5"><span><b><a href="#cite_ref-Guide_5-0">^</a></b></span> <span><cite id="CITEREFFranzHavens2006">Franz, Carl; Havens, Lorena (2006). <i>The People's Guide to Mexico</i>. Avalon Travel Publishing. p.&nbsp;319. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/1-56691-711-5" title="Special:BookSources/1-56691-711-5"><bdi>1-56691-711-5</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+People%27s+Guide+to+Mexico&amp;rft.pages=319&amp;rft.pub=Avalon+Travel+Publishing&amp;rft.date=2006&amp;rft.isbn=1-56691-711-5&amp;rft.aulast=Franz&amp;rft.aufirst=Carl&amp;rft.au=Havens%2C+Lorena&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-Ask-6"><span><b><a href="#cite_ref-Ask_6-0">^</a></b></span> <span><cite id="CITEREFArellano2008"><a href="https://en.wikipedia.org/wiki/Gustavo_Arellano" title="Gustavo Arellano">Arellano, Gustavo</a> (2008). <a rel="nofollow" href="https://archive.org/details/isbn_9781416540021/page/26"><i>Ask a Mexican</i></a>. <a href="https://en.wikipedia.org/wiki/Charles_Scribner%27s_Sons" title="Charles Scribner's Sons">Scribner</a>. p.&nbsp;<a rel="nofollow" href="https://archive.org/details/isbn_9781416540021/page/26">26</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-4165-4003-8" title="Special:BookSources/978-1-4165-4003-8"><bdi>978-1-4165-4003-8</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Ask+a+Mexican&amp;rft.pages=26&amp;rft.pub=Scribner&amp;rft.date=2008&amp;rft.isbn=978-1-4165-4003-8&amp;rft.aulast=Arellano&amp;rft.aufirst=Gustavo&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fisbn_9781416540021%2Fpage%2F26&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-Hellholes-7"><span><b><a href="#cite_ref-Hellholes_7-0">^</a></b></span> <span><cite id="CITEREFThompson2009">Thompson, Chuck (2009). <a rel="nofollow" href="https://archive.org/details/tohellholesbackb00thom/page/220"><i>To Hellholes and Back: Bribes, Lies, and the Art of Extreme Tourism</i></a>. Holt Paperbacks. p.&nbsp;<a rel="nofollow" href="https://archive.org/details/tohellholesbackb00thom/page/220">220</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-8050-8788-8" title="Special:BookSources/978-0-8050-8788-8"><bdi>978-0-8050-8788-8</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=To+Hellholes+and+Back%3A+Bribes%2C+Lies%2C+and+the+Art+of+Extreme+Tourism&amp;rft.pages=220&amp;rft.pub=Holt+Paperbacks&amp;rft.date=2009&amp;rft.isbn=978-0-8050-8788-8&amp;rft.aulast=Thompson&amp;rft.aufirst=Chuck&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Ftohellholesbackb00thom%2Fpage%2F220&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite id="CITEREFStanton1948">Stanton, John (September 20, 1948). "In Mexico City Traffic is Terrific". <i><a href="https://en.wikipedia.org/wiki/Life_(magazine)" title="Life (magazine)">LIFE</a></i>. Time, Inc.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=LIFE&amp;rft.atitle=In+Mexico+City+Traffic+is+Terrific&amp;rft.date=1948-09-20&amp;rft.aulast=Stanton&amp;rft.aufirst=John&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><cite id="CITEREFKeenan2004">Keenan, Joseph John (2004). <span title="Free registration required"><a rel="nofollow" href="https://archive.org/details/breakingoutofbeg0000keen"><i>Breaking Out of Beginner's Spanish</i></a></span>. University of Texas Press. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-292-74322-X" title="Special:BookSources/0-292-74322-X"><bdi>0-292-74322-X</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Breaking+Out+of+Beginner%27s+Spanish&amp;rft.pub=University+of+Texas+Press&amp;rft.date=2004&amp;rft.isbn=0-292-74322-X&amp;rft.aulast=Keenan&amp;rft.aufirst=Joseph+John&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fbreakingoutofbeg0000keen&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><cite id="CITEREFAxtellFornwald1998">Axtell, Roger E.; Fornwald, Mike (1998). <a rel="nofollow" href="https://archive.org/details/gesturesdostaboo00axte/page/101"><i>Gestures: The Do's and Taboos of Body Language Around the World</i></a>. Wiley. p.&nbsp;<a rel="nofollow" href="https://archive.org/details/gesturesdostaboo00axte/page/101">101</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-471-18342-3" title="Special:BookSources/0-471-18342-3"><bdi>0-471-18342-3</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Gestures%3A+The+Do%27s+and+Taboos+of+Body+Language+Around+the+World&amp;rft.pages=101&amp;rft.pub=Wiley&amp;rft.date=1998&amp;rft.isbn=0-471-18342-3&amp;rft.aulast=Axtell&amp;rft.aufirst=Roger+E.&amp;rft.au=Fornwald%2C+Mike&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fgesturesdostaboo00axte%2Fpage%2F101&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-11"><span><b><a href="#cite_ref-11">^</a></b></span> <span><cite id="CITEREFAxtell1998">Axtell, Roger E. (1998). <a rel="nofollow" href="https://archive.org/details/dostaboosofhumor00axte"><i>Do's and Taboos of Humor Around the World</i></a>. Wiley. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-471-25403-7" title="Special:BookSources/0-471-25403-7"><bdi>0-471-25403-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Do%27s+and+Taboos+of+Humor+Around+the+World&amp;rft.pub=Wiley&amp;rft.date=1998&amp;rft.isbn=0-471-25403-7&amp;rft.aulast=Axtell&amp;rft.aufirst=Roger+E.&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fdostaboosofhumor00axte&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-12"><span><b><a href="#cite_ref-12">^</a></b></span> <span><cite id="CITEREFRuiz_FornellsRuiz-Fornells1979">Ruiz Fornells, Enrique; Ruiz-Fornells, Cynthia Y. (1979). <i>The United States and the Spanish World</i>. Sociedad General Española de Librería. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/84-7143-192-0" title="Special:BookSources/84-7143-192-0"><bdi>84-7143-192-0</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+United+States+and+the+Spanish+World&amp;rft.pub=Sociedad+General+Espa%C3%B1ola+de+Librer%C3%ADa&amp;rft.date=1979&amp;rft.isbn=84-7143-192-0&amp;rft.aulast=Ruiz+Fornells&amp;rft.aufirst=Enrique&amp;rft.au=Ruiz-Fornells%2C+Cynthia+Y.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span><cite id="CITEREFWilderSherrier1992">Wilder, Cora Sarjeant; Sherrier, James (1992). <i>Celebrating Diversity</i>. Ginn Press. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-536-58133-9" title="Special:BookSources/0-536-58133-9"><bdi>0-536-58133-9</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Celebrating+Diversity&amp;rft.pub=Ginn+Press&amp;rft.date=1992&amp;rft.isbn=0-536-58133-9&amp;rft.aulast=Wilder&amp;rft.aufirst=Cora+Sarjeant&amp;rft.au=Sherrier%2C+James&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-Slang-14"><span><b><a href="#cite_ref-Slang_14-0">^</a></b></span> <span>Partridge, Eric; Dalzell, Tom; and Victor, Terry (2007). <i>The concise new Partridge dictionary of slang and unconventional English</i>, p.571. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-415-21259-5" title="Special:BookSources/978-0-415-21259-5">978-0-415-21259-5</a>.</span>
</li>
<li id="cite_note-15"><span><b><a href="#cite_ref-15">^</a></b></span> <span><cite id="CITEREFKing1999">King, Thomas W. (1999). <i>Modern Morse Code in Rehabilitation and Education</i>. Allyn &amp; Bacon. p.&nbsp;77. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-205-28751-4" title="Special:BookSources/0-205-28751-4"><bdi>0-205-28751-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Modern+Morse+Code+in+Rehabilitation+and+Education&amp;rft.pages=77&amp;rft.pub=Allyn+%26+Bacon&amp;rft.date=1999&amp;rft.isbn=0-205-28751-4&amp;rft.aulast=King&amp;rft.aufirst=Thomas+W.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-16"><span><b><a href="#cite_ref-16">^</a></b></span> <span><cite id="CITEREFBrace,_Ernest_C.2008">Brace, Ernest C. (May 2, 2008). <a rel="nofollow" href="https://web.archive.org/web/20081201165511/http://www.johnmccain.com//Informing/News/NewsReleases/3168f3a2-e59b-433f-94ea-fb1641323507.htm">"Messages From John"</a>. <i>JohnMcCain.com</i>. Archived from <a rel="nofollow" href="http://www.johnmccain.com//Informing/News/NewsReleases/3168f3a2-e59b-433f-94ea-fb1641323507.htm">the original</a> on December 1, 2008<span>. Retrieved <span>2008-11-26</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=JohnMcCain.com&amp;rft.atitle=Messages+From+John&amp;rft.date=2008-05-02&amp;rft.au=Brace%2C+Ernest+C.&amp;rft_id=http%3A%2F%2Fwww.johnmccain.com%2F%2FInforming%2FNews%2FNewsReleases%2F3168f3a2-e59b-433f-94ea-fb1641323507.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-Traum-17"><span><b><a href="#cite_ref-Traum_17-0">^</a></b></span> <span>Traum, Happy (1974). <i>Bluegrass Guitar</i>, p.26. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-8256-0153-3" title="Special:BookSources/0-8256-0153-3">0-8256-0153-3</a>.</span>
</li>
<li id="cite_note-18"><span><b><a href="#cite_ref-18">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.imdb.com/title/tt0096438/quotes?qt=qt0406091">"Quotes from "Who Framed Roger Rabbit"<span></span>"</a>. <i><a href="https://en.wikipedia.org/wiki/IMDb" title="IMDb">IMDb</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=IMDb&amp;rft.atitle=Quotes+from+%22Who+Framed+Roger+Rabbit%22&amp;rft_id=https%3A%2F%2Fwww.imdb.com%2Ftitle%2Ftt0096438%2Fquotes%3Fqt%3Dqt0406091&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-19"><span><b><a href="#cite_ref-19">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.youtube.com/watch?v=w2KF6QyUgwA">"NOKIA 3310 ringtone That's it!"</a>. <i><a href="https://en.wikipedia.org/wiki/YouTube" title="YouTube">YouTube</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=YouTube&amp;rft.atitle=NOKIA+3310+ringtone+That%27s+it%21&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dw2KF6QyUgwA&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.youtube.com/watch?v=JgqP97caQsA">"Shave and a Haircut (Nokia "That's it!" ringtone) - Piano Quickie"</a>. <i><a href="https://en.wikipedia.org/wiki/YouTube" title="YouTube">YouTube</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=YouTube&amp;rft.atitle=Shave+and+a+Haircut+%28Nokia+%22That%27s+it%21%22+ringtone%29+-+Piano+Quickie&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DJgqP97caQsA&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-21"><span><b><a href="#cite_ref-21">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.youtube.com/watch?v=4W3cPSntmBk">"<span></span>"That's A Lot Of Bunk" - Billy Jones &amp; Ernest Hare (1923 Edison)"</a>. <i>YouTube</i>. <a rel="nofollow" href="https://ghostarchive.org/varchive/youtube/20211221/4W3cPSntmBk">Archived</a> from the original on 2021-12-21<span>. Retrieved <span>4 July</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=YouTube&amp;rft.atitle=%22That%27s+A+Lot+Of+Bunk%22+-+Billy+Jones+%26+Ernest+Hare+%281923+Edison%29&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D4W3cPSntmBk&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-22"><span><b><a href="#cite_ref-22">^</a></b></span> <span><i>O-Kay for Sound</i>, <a rel="nofollow" href="https://archive.org/details/O-kayForSound">https://archive.org/details/O-kayForSound</a>. Retrieved 2019-02-02.</span>
</li>
<li id="cite_note-23"><span><b><a href="#cite_ref-23">^</a></b></span> <span>Bartholomew, Dave, "The King Sides" Collectables (CD) 2883, 2004</span>
</li>
<li id="cite_note-24"><span><b><a href="#cite_ref-24">^</a></b></span> <span><cite id="CITEREFCleveland,_Barry2002">Cleveland, Barry (September 1, 2002). <a rel="nofollow" href="https://web.archive.org/web/20090527060656/http://onstagemag.com/ar/performance_happened_month_11/index.htm">"It Happened This Month"</a>. <i>OnStageMag.com</i>. Archived from <a rel="nofollow" href="http://onstagemag.com/ar/performance_happened_month_11/index.htm">the original</a> on May 27, 2009<span>. Retrieved <span>2008-11-26</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OnStageMag.com&amp;rft.atitle=It+Happened+This+Month&amp;rft.date=2002-09-01&amp;rft.au=Cleveland%2C+Barry&amp;rft_id=http%3A%2F%2Fonstagemag.com%2Far%2Fperformance_happened_month_11%2Findex.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-PDQ_Bach-25"><span><b><a href="#cite_ref-PDQ_Bach_25-0">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.schickele.com/composition/blauesgras.htm">"Cantata 'Blaus Gras'<span></span>"</a>. <i>The Peter Schickele/P.D.Q. Bach Web Site</i>. July 3, 2011<span>. Retrieved <span>2012-12-07</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Peter+Schickele%2FP.D.Q.+Bach+Web+Site&amp;rft.atitle=Cantata+%27Blaus+Gras%27&amp;rft.date=2011-07-03&amp;rft_id=http%3A%2F%2Fwww.schickele.com%2Fcomposition%2Fblauesgras.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-26"><span><b><a href="#cite_ref-26">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.mcgath.com/pdq.html#3.14159265">"The Key of P. D. Q"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Key+of+P.+D.+Q&amp;rft_id=http%3A%2F%2Fwww.mcgath.com%2Fpdq.html%233.14159265&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-27"><span><b><a href="#cite_ref-27">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.youtube.com/watch?v=5bWeSr04Wzs">"Victor Borge - As 'Franz Liszt' with Mike Wallace c.1960"</a>. <i>YouTube</i>. <a rel="nofollow" href="https://ghostarchive.org/varchive/youtube/20211221/4W3cPSntmBk">Archived</a> from the original on 2021-12-21<span>. Retrieved <span>December 26,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=YouTube&amp;rft.atitle=Victor+Borge+-+As+%27Franz+Liszt%27+with+Mike+Wallace+c.1960&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D5bWeSr04Wzs&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-28"><span><b><a href="#cite_ref-28">^</a></b></span> <span><cite id="CITEREFHencken1992">Hencken, John (22 August 1992). <a rel="nofollow" href="https://www.latimes.com/archives/la-xpm-1992-08-22-ca-5071-story.html">"TV Reviews&nbsp;: Borge's 'Then &amp; Now' Is Mostly Now on PBS"</a>. <i><a href="https://en.wikipedia.org/wiki/Los_Angeles_Times" title="Los Angeles Times">Los Angeles Times</a></i><span>. Retrieved <span>December 26,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Los+Angeles+Times&amp;rft.atitle=TV+Reviews+%3A+Borge%27s+%27Then+%26+Now%27+Is+Mostly+Now+on+PBS&amp;rft.date=1992-08-22&amp;rft.aulast=Hencken&amp;rft.aufirst=John&amp;rft_id=https%3A%2F%2Fwww.latimes.com%2Farchives%2Fla-xpm-1992-08-22-ca-5071-story.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-29"><span><b><a href="#cite_ref-29">^</a></b></span> <span><cite id="CITEREFSola_i_Ramos1999">Sola i Ramos, Elisa (December 1999). <a rel="nofollow" href="http://www.blanes.cat/oiapdocs.nsf/a50fa5b68f16871cc12566ff0056d21b/a88903545a9aaaaac12578cc004da7ad/$FILE/Proverbis,%20dites%20i%20frases%20fetes%20de%20Blanes.pdf">"PROVERBIS, DITES I FRASES FETES DE BLANES"</a> <span>(PDF)</span>. Servei de Català de Blanes (CPNL)<span>. Retrieved <span>19 March</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=PROVERBIS%2C+DITES+I+FRASES+FETES+DE+BLANES&amp;rft.pub=Servei+de+Catal%C3%A0+de+Blanes+%28CPNL%29&amp;rft.date=1999-12&amp;rft.aulast=Sola+i+Ramos&amp;rft.aufirst=Elisa&amp;rft_id=http%3A%2F%2Fwww.blanes.cat%2Foiapdocs.nsf%2Fa50fa5b68f16871cc12566ff0056d21b%2Fa88903545a9aaaaac12578cc004da7ad%2F%24FILE%2FProverbis%2C%2520dites%2520i%2520frases%2520fetes%2520de%2520Blanes.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
<li id="cite_note-30"><span><b><a href="#cite_ref-30">^</a></b></span> <span><cite id="CITEREFMartin_Dardis">Martin Dardis. <a rel="nofollow" href="https://www.irish-folk-songs.com/finnegans-wake-lyrics-and-chords.html">"Finnegan's Wake lyrics and chords"</a>. <i>Irish Folk Songs</i><span>. Retrieved <span>16 February</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Irish+Folk+Songs&amp;rft.atitle=Finnegan%27s+Wake+lyrics+and+chords&amp;rft.au=Martin+Dardis&amp;rft_id=https%3A%2F%2Fwww.irish-folk-songs.com%2Ffinnegans-wake-lyrics-and-chords.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AShave+and+a+Haircut"></span></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Shave_and_a_Haircut&amp;action=edit&amp;section=7" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a rel="nofollow" href="https://web.archive.org/web/20050310195336/http://www.dataflo.net/~mpurintun/Tabs/OldTimeTabs/shave_and_a_haircut.htm">Description</a></li>
<li><a rel="nofollow" href="http://www.hum.uva.nl/mmm/groene/">Dutch article on "Shave and a haircut"</a></li>
<li><a rel="nofollow" href="http://webapp1.dlib.indiana.edu/inharmony/detail.do?action=detail&amp;fullItemID=/lilly/devincent/LL-SDV-201006&amp;queryNumber=1">Sheet music for "At A Darktown Cakewalk" from the IN Harmony system at Indiana University</a></li></ul>
<!-- 
NewPP limit report
Parsed by mw1490
Cached time: 20240229061557
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.326 seconds
Real time usage: 0.624 seconds
Preprocessor visited node count: 2195/1000000
Post‐expand include size: 48448/2097152 bytes
Template argument size: 2053/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 79248/5000000 bytes
Lua time usage: 0.192/10.000 seconds
Lua memory usage: 8381903/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  558.475      1 -total
 36.74%  205.188      1 Template:Reflist
 33.35%  186.258      1 Template:Image_frame
 18.48%  103.229     11 Template:Cite_book
 12.48%   69.723      1 Template:Short_description
  7.84%   43.793     13 Template:Cite_web
  7.71%   43.061      2 Template:Citation_needed
  7.43%   41.510      2 Template:Listen
  6.97%   38.932      2 Template:Pagetype
  6.52%   36.407      2 Template:Fix
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:3529553-0!canonical and timestamp 20240229061557 and revision id 1210973992. Rendering was triggered because: api-parse
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Serving my blog posts as Linux manual pages (269 pts)]]></title>
            <link>https://jamesg.blog/2024/02/29/linux-manual-pages/</link>
            <guid>39548410</guid>
            <pubDate>Thu, 29 Feb 2024 12:18:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jamesg.blog/2024/02/29/linux-manual-pages/">https://jamesg.blog/2024/02/29/linux-manual-pages/</a>, See on <a href="https://news.ycombinator.com/item?id=39548410">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
    	 <!-- This page uses microformats to structure different pieces of information.
    I use h-entry to state this is a post. Any class name that starts with h-, p-, or -e is a microformat.
    By specifying microformats, some web tools can better understand this page. For example, IndieNews can use
    the p-name to figure out the title of the post (without the "| James' Coffee Blog" I add to the <title> tag of my website.
    Learn more about h-entry: https://indieweb.org/h-entry
-->
<article>
	<header>
		
		<p><em>Published on
			<a href="https://jamesg.blog/2024/02"><time datetime="2024-02-29T00:00:00">February 29, 2024</time></a>
			 under the <a href="https://jamesg.blog/category/coding">Coding</a> category. <a onclick="document.getElementsByTagName('incoming-links')[0].toggle(); document.getElementsByTagName('outgoing-links')[0].toggle();">Toggle Memex mode</a></em></p>
		  
		
		<a href="https://jamesg.blog/assets/manual_example.png"><img src="https://jamesg.blog/assets/manual_example.png" alt=""></a>
		
		
		
	</header>
	<section>
		<div>
			<p><em>Intended audience: You are likely to enjoy this post the most if you are interested in Linux and/or Linux manual pages, or if you enjoy reading about esoteric programming projects.</em></p>
<p>Linux computers come with pre-installed manual pages that describe how to use specific commands. These pages are readable by typing <code>man &lt;command&gt;</code> into your terminal. For example, you can get the manual for the <code>tac</code> command, which prints out a file in from bottom-to-top by using the command <code>man tac</code>. Some command line software you install adds manual pages, too.</p>
<p>Linux manual pages are formatted using the <code>roff</code> syntax, which you can use to mark up documents. <code>roff</code> was the first typesetting command line software for Unix, developed at Bell Labs. Earlier this week, with a spark for building but no particular idea in mind, I started to think about the Linux manual page. Could I serve my blog posts as Linux manual pages? Herein lay an adventure.</p>
<p>TL;DR: You can request a Linux manual page version of a blog post with the following HTTP request:</p>
<pre><code>
curl -sL -H "Accept: text/roff" https://jamesg.blog/2024/02/28/programming-projects/ &gt; post.page &amp;&amp; man ./post.page
</code></pre>

<h2>Devising the system: Content negotiation</h2>
<p>I had an idea for how I wanted this to work in mind. I wanted a user to be able to request a <code>roff</code> version of a blog post using content negotiation, part of HTTP that lets you specify ixn what format you want a file. For example, you could request an image with an <code>Accept: image/png</code>. This tells a server that, if possible, it should send a PNG file. There are lots of intricacies to content negotiation. You can provide a list of types of content you can accept, and the order in which a server should try to return them</p>
<p>But! That's a rabbit hole for another day. What's important here is that an application can ask a server for content in a specific format using a HTTP header.</p>
<p>With content negotiation, I can route requests if a user sends an <code>Accept</code> header. If a user asks for an <code>text/roff</code> document, I could return a manual page that can be opened with the <code>man</code> command.</p>
<h2>Writing the manual pages</h2>
<p>Manual pages use the <code>roff</code>syntax, so I would need to have versions of my blog posts in that format. To do this, I updated my site to generate <code>man</code> pages for each blog post. The template I used to generate the manual page was as follows:</p>
<pre><code>
.TH jamesg.blog 1 "" "jamesg.blog"
.SH TITLE
...
.SH AUTHOR
James' Coffee Blog (https://jamesg.blog)
.SH PUBLISHED
...
.SH POST
...
.SH URL
...
</code></pre>

<p>Here, I set a header with my domain name and create five sections: title, author, published date, the post content, and the URL of the post. The raw content is markdown. This doesn't always turn out well in a manual as spacing can sometimes be off. But, markdown was more readable than HTML and resulted in less information loss (i.e. titles having no distinction to paragraphs other than being on their own line) than using plain text.</p>
<p>I now had:</p>
<ol>
<li>Properly formatted manual pages, and;</li>
<li>The knowledge that content negotiation could allow someone to request a manual page.</li>
</ol>
<p>Now came the final piece of the puzzle: using content negotiation to facilitate the request for manual pages.</p>
<h2>Requesting a manual page</h2>
<p>You can use the following command to request the <code>roff</code> format of blog posts on this website:</p>
<pre><code>
curl -sL -H "Accept: text/roff" https://jamesg.blog/2024/02/28/programming-projects/ &gt; post.page
</code></pre>

<p>You can then open the result as a Linux manual page:</p>
<pre><code>
man ./post.page
</code></pre>

<p>Let's talk about how this works!</p>
<p>When a browser makes a request to <code>https://jamesg.blog/2024/02/19/personal-website-ideas/</code>, it asks for the HTML version of the page. In the <code>curl</code> command above, the command asks for the <code>text/roff</code> version. I added a few lines of text in my NGINX configuration to change how the server responds when <code>text/roff</code> is requested for a blog post.</p>
<p>First, I declared a few variables in my <code>/etc/nginx/nginx.conf</code> file that let me raise a flag when a specific content type was identified:</p>
<pre><code>
map $uri $redirect_suffix {
    ~^/(.*)/$   $1;
    default     "";
}

map $http_accept $redirect_location {
    default "";
    "~^text/roff" 1;
}
</code></pre>

<p>You can add multiple different redirect locations, but I only need two: the default, and my custom <code>text/roff</code> rule.</p>
<p>In my site NGINX configuration (the file in the <code>/etc/nginx/sites-enabled</code> folder), I used the following code to handle requests differently if a <code>roff</code> page is requested:</p>
<pre><code>
server {
                ...
        location / {
          if ($redirect_location = 1) {
              rewrite ^/(.*)/$ /$1.man last;
          }
            ...
        }
}
</code></pre>

<p>Here, I say: take a URL, and add <code>.man</code> to the end, removing the trailing slash, as long as the <code>Accept: text/roff</code> header is set. This tells NGINX to read from the <code>.man</code> file instead of the <code>index.html</code> file associated with each post on my site.</p>
<p>That is to say you can now read blog posts on this website as a Linux manual page. This was a fun investigation into using content negotiation in NGINX and a reminder of how far we have come with typesetting technology from the command line interfaces to modern-day typesetting software and HTML.</p>
<p><em>Thank you to <a href="https://toddpresta.com/">Todd</a> for providing guidance on setting up my NGINX configuration. Todd's help was sincerely appreciated!</em></p>
			
		</div>
		
		<p><a href="https://notbyai.fyi/"><img src="https://jamesg.blog/assets/ai.png" alt="Written by human, not by AI"></a></p>
	</section>
	
	<section>
    <h2>Responses</h2>
    
    <h2>Comment on this post</h2>
    <p>Respond to this post by sending a <a href="https://indieweb.org/Webmention">Webmention</a>.</p>
    <p>Have a comment? Email me at <a href="mailto:readers@jamesg.blog?subject=Serving%20my%20blog%20posts%20as%20Linux%20manual%20pages">readers@jamesg.blog</a>.</p>  
</section>
</article>
      </div></div>]]></description>
        </item>
    </channel>
</rss>