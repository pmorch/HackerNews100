<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 21 Apr 2024 02:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Two lifeforms merge in once-in-a-billion-years evolutionary event (103 pts)]]></title>
            <link>https://newatlas.com/biology/life-merger-evolution-symbiosis-organelle/</link>
            <guid>40101290</guid>
            <pubDate>Sat, 20 Apr 2024 21:46:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/biology/life-merger-evolution-symbiosis-organelle/">https://newatlas.com/biology/life-merger-evolution-symbiosis-organelle/</a>, See on <a href="https://news.ycombinator.com/item?id=40101290">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Scientists have caught a once-in-a-billion-years evolutionary event in progress, as two lifeforms have merged into one organism that boasts abilities its peers would envy. Last time this happened, Earth got plants.</p><p>The phenomenon is called <a href="https://newatlas.com/biology/synthetic-hybrids-yeast-bacteria-evolution-symbiosis/" data-cms-ai="0">primary endosymbiosis</a>, and it occurs when one microbial organism engulfs another, and starts using it like an internal organ. In exchange, the host cell provides nutrients, energy, protection and other benefits to the symbiote, until eventually it can no longer survive on its own and essentially ends up <i>becoming</i> an organ for the host – or what’s known as an organelle in microbial cells.</p><p>Imagine if kidneys were actually little animals running around, and humans had to manually filter their blood through a dialysis machine. Then one day some guy somehow gets one of these kidney critters stuck... Internally (who are we to judge how?) – and realizes he no longer needs his dialysis machine. Neither do his kids, until eventually we're all born with these helpful little fellas inside us. That’s kind of what’s happening here.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="A diagram of the mitochondria in a cell" width="1440" height="802" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/e47eafd/2147483647/strip/true/crop/1600x891+0+0/resize/440x245!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fdb%2Fd2%2Fa7d70c80494c888fe5f519719327%2Fmitochondria-0.jpeg 440w,https://assets.newatlas.com/dims4/default/9556f7a/2147483647/strip/true/crop/1600x891+0+0/resize/800x446!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fdb%2Fd2%2Fa7d70c80494c888fe5f519719327%2Fmitochondria-0.jpeg 800w,https://assets.newatlas.com/dims4/default/214decf/2147483647/strip/true/crop/1600x891+0+0/resize/1200x668!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fdb%2Fd2%2Fa7d70c80494c888fe5f519719327%2Fmitochondria-0.jpeg 1200w,https://assets.newatlas.com/dims4/default/145c392/2147483647/strip/true/crop/1600x891+0+0/resize/1920x1069!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fdb%2Fd2%2Fa7d70c80494c888fe5f519719327%2Fmitochondria-0.jpeg 1920w" data-src="https://assets.newatlas.com/dims4/default/72bcb4b/2147483647/strip/true/crop/1600x891+0+0/resize/1440x802!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fdb%2Fd2%2Fa7d70c80494c888fe5f519719327%2Fmitochondria-0.jpeg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/e47eafd/2147483647/strip/true/crop/1600x891+0+0/resize/440x245!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fdb%2Fd2%2Fa7d70c80494c888fe5f519719327%2Fmitochondria-0.jpeg 440w,https://assets.newatlas.com/dims4/default/9556f7a/2147483647/strip/true/crop/1600x891+0+0/resize/800x446!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fdb%2Fd2%2Fa7d70c80494c888fe5f519719327%2Fmitochondria-0.jpeg 800w,https://assets.newatlas.com/dims4/default/214decf/2147483647/strip/true/crop/1600x891+0+0/resize/1200x668!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fdb%2Fd2%2Fa7d70c80494c888fe5f519719327%2Fmitochondria-0.jpeg 1200w,https://assets.newatlas.com/dims4/default/145c392/2147483647/strip/true/crop/1600x891+0+0/resize/1920x1069!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fdb%2Fd2%2Fa7d70c80494c888fe5f519719327%2Fmitochondria-0.jpeg 1920w" src="https://assets.newatlas.com/dims4/default/72bcb4b/2147483647/strip/true/crop/1600x891+0+0/resize/1440x802!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fdb%2Fd2%2Fa7d70c80494c888fe5f519719327%2Fmitochondria-0.jpeg">
</p>



    
    

    
        <div><figcaption itemprop="caption">A diagram of the mitochondria in a cell</figcaption><p>National Human Genome Research Institute</p></div>
    
</figure>

                
            </div><p>In the 4-billion-odd-year history of life on Earth, primary endosymbiosis is thought to have only happened twice that we know of, and each time was a massive breakthrough for evolution. The first occurred about 2.2 billion years ago, when an archaea swallowed a bacterium that became the mitochondria. This specialized energy-producing organelle allowed for basically all complex forms of life to evolve. It remains the heralded "powerhouse of the cell" to this day. </p><p>The second time happened about 1.6 billion years ago, when some of these more advanced cells absorbed cyanobacteria that could harvest energy from sunlight. These became organelles called chloroplasts, which gave sunlight-harvesting abilities, as well as a fetching green color, to a group of lifeforms you might have heard of – plants. </p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="Live moss cells under a microscope, showing their chloroplasts (green circles)" width="1440" height="919" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/0da7c66/2147483647/strip/true/crop/2295x1464+0+0/resize/440x281!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F96%2Fca%2F61428d1b484d9360ca9ce87832b0%2Fchloroplasts.jpeg 440w,https://assets.newatlas.com/dims4/default/1581ce0/2147483647/strip/true/crop/2295x1464+0+0/resize/800x511!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F96%2Fca%2F61428d1b484d9360ca9ce87832b0%2Fchloroplasts.jpeg 800w,https://assets.newatlas.com/dims4/default/5ef6141/2147483647/strip/true/crop/2295x1464+0+0/resize/1200x766!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F96%2Fca%2F61428d1b484d9360ca9ce87832b0%2Fchloroplasts.jpeg 1200w,https://assets.newatlas.com/dims4/default/70b7e2d/2147483647/strip/true/crop/2295x1464+0+0/resize/1920x1225!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F96%2Fca%2F61428d1b484d9360ca9ce87832b0%2Fchloroplasts.jpeg 1920w" data-src="https://assets.newatlas.com/dims4/default/dcdfdce/2147483647/strip/true/crop/2295x1464+0+0/resize/1440x919!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F96%2Fca%2F61428d1b484d9360ca9ce87832b0%2Fchloroplasts.jpeg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/0da7c66/2147483647/strip/true/crop/2295x1464+0+0/resize/440x281!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F96%2Fca%2F61428d1b484d9360ca9ce87832b0%2Fchloroplasts.jpeg 440w,https://assets.newatlas.com/dims4/default/1581ce0/2147483647/strip/true/crop/2295x1464+0+0/resize/800x511!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F96%2Fca%2F61428d1b484d9360ca9ce87832b0%2Fchloroplasts.jpeg 800w,https://assets.newatlas.com/dims4/default/5ef6141/2147483647/strip/true/crop/2295x1464+0+0/resize/1200x766!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F96%2Fca%2F61428d1b484d9360ca9ce87832b0%2Fchloroplasts.jpeg 1200w,https://assets.newatlas.com/dims4/default/70b7e2d/2147483647/strip/true/crop/2295x1464+0+0/resize/1920x1225!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F96%2Fca%2F61428d1b484d9360ca9ce87832b0%2Fchloroplasts.jpeg 1920w" src="https://assets.newatlas.com/dims4/default/dcdfdce/2147483647/strip/true/crop/2295x1464+0+0/resize/1440x919!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F96%2Fca%2F61428d1b484d9360ca9ce87832b0%2Fchloroplasts.jpeg">
</p>



    
    

    
        <div><figcaption itemprop="caption">Live moss cells under a microscope, showing their chloroplasts (green circles)</figcaption></div>
    
</figure>

                
            </div><p>And now, scientists have discovered that it’s happening again. A species of algae called <i>Braarudosphaera bigelowii</i> was found to have engulfed a cyanobacterium that lets them do something that algae, and plants in general, can’t normally do – "fixing" nitrogen straight from the air, and combining it with other elements to create more useful compounds.</p><p>Nitrogen is a key nutrient, and normally plants and algae get theirs through symbiotic relationships with bacteria that remain separate. At first it was thought that <i>B. bigelowii</i> had hooked up this kind of situation with a bacterium called UCYN-A, but on closer inspection, scientists discovered that the two have gotten far more intimate.</p><p>In one recent study, a team found that the size ratio between the algae and UCYN-A stays similar across different related species of the algae. Their growth appears to be controlled by the exchange of nutrients, leading to linked metabolisms.</p><p>“That’s exactly what happens with organelles,” said Jonathan Zehr, an author of the studies. “If you look at the mitochondria and the chloroplast, it’s the same thing: they scale with the cell.”</p><p>In a follow-up study, the team and other collaborators used a powerful X-ray imaging technique to view the interior of the living algae cells. This revealed that the replication and cell division was synchronized between the host and symbiote – more evidence of primary endosymbiosis at work.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="X-ray images of Braarudosphaera bigelowii at different stages of cell division. The newly identified nitroplast is highlighted in cyan, the algae nucleus is blue, mitochondria are green and chloroplasts are purple" width="1025" height="685" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/1167d59/2147483647/strip/true/crop/1025x685+0+0/resize/440x294!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Ffc%2F29%2Fbe4497b0484cba59d73433909e61%2Fnitroplasts.png 440w,https://assets.newatlas.com/dims4/default/79f851b/2147483647/strip/true/crop/1025x685+0+0/resize/800x535!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Ffc%2F29%2Fbe4497b0484cba59d73433909e61%2Fnitroplasts.png 800w,https://assets.newatlas.com/dims4/default/82e3fe1/2147483647/strip/true/crop/1025x685+0+0/resize/1200x802!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Ffc%2F29%2Fbe4497b0484cba59d73433909e61%2Fnitroplasts.png 1200w,https://assets.newatlas.com/dims4/default/fc8a14f/2147483647/strip/true/crop/1025x685+0+0/resize/1920x1283!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Ffc%2F29%2Fbe4497b0484cba59d73433909e61%2Fnitroplasts.png 1920w" data-src="https://assets.newatlas.com/dims4/default/3b97965/2147483647/strip/true/crop/1025x685+0+0/resize/1025x685!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Ffc%2F29%2Fbe4497b0484cba59d73433909e61%2Fnitroplasts.png" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/1167d59/2147483647/strip/true/crop/1025x685+0+0/resize/440x294!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Ffc%2F29%2Fbe4497b0484cba59d73433909e61%2Fnitroplasts.png 440w,https://assets.newatlas.com/dims4/default/79f851b/2147483647/strip/true/crop/1025x685+0+0/resize/800x535!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Ffc%2F29%2Fbe4497b0484cba59d73433909e61%2Fnitroplasts.png 800w,https://assets.newatlas.com/dims4/default/82e3fe1/2147483647/strip/true/crop/1025x685+0+0/resize/1200x802!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Ffc%2F29%2Fbe4497b0484cba59d73433909e61%2Fnitroplasts.png 1200w,https://assets.newatlas.com/dims4/default/fc8a14f/2147483647/strip/true/crop/1025x685+0+0/resize/1920x1283!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Ffc%2F29%2Fbe4497b0484cba59d73433909e61%2Fnitroplasts.png 1920w" src="https://assets.newatlas.com/dims4/default/3b97965/2147483647/strip/true/crop/1025x685+0+0/resize/1025x685!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Ffc%2F29%2Fbe4497b0484cba59d73433909e61%2Fnitroplasts.png">
</p>



    
    

    
        <div><figcaption itemprop="caption">X-ray images of <i>Braarudosphaera bigelowii </i>at different stages of cell division. The newly identified nitroplast is highlighted in cyan, the algae nucleus is blue, mitochondria are green and chloroplasts are purple</figcaption><p>Valentina Loconte/Berkeley Lab</p></div>
    
</figure>

                
            </div><p>And finally, the team compared the proteins of isolated UCYN-A to those inside the algal cells. They found that the isolated bacterium can only produce about half of the proteins it needs, relying on the algal host to provide the rest.</p><p>“That’s one of the hallmarks of something moving from an endosymbiont to an organelle,” said Zehr. “They start throwing away pieces of DNA, and their genomes get smaller and smaller, and they start depending on the mother cell for those gene products – or the protein itself – to be transported into the cell.”</p><p>Altogether, the team says this indicates UCYN-A is a full organelle, which is given the name of nitroplast. It appears that this began to evolve around 100 million years ago, which sounds like an incredibly long time but is a blink of an eye compared to mitochondria and chloroplasts.</p><p>The researchers plan to continue studying nitroplasts, to find out if they’re present in other cells and what effects they may have. One possible benefit is that it could give scientists a new avenue to incorporate nitrogen-fixing into plants to grow better crops.</p><p>The research was published in the journals <i><a href="https://www.cell.com/cell/pdf/S0092-8674(24)00182-X.pdf" target="_blank" data-cms-ai="0">Cell</a></i> and <i><a href="https://www.science.org/doi/10.1126/science.adk1075" target="_blank" data-cms-ai="0">Science</a></i>.</p><p>Source: <a href="https://newscenter.lbl.gov/2024/04/17/scientists-discover-first-nitrogen-fixing-organelle/" target="_blank" data-cms-ai="0">Berkeley Lab</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bringing Exchange Support to Thunderbird (176 pts)]]></title>
            <link>https://blog.thunderbird.net/2024/04/adventures-in-rust-bringing-exchange-support-to-thunderbird/</link>
            <guid>40100672</guid>
            <pubDate>Sat, 20 Apr 2024 20:19:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.thunderbird.net/2024/04/adventures-in-rust-bringing-exchange-support-to-thunderbird/">https://blog.thunderbird.net/2024/04/adventures-in-rust-bringing-exchange-support-to-thunderbird/</a>, See on <a href="https://news.ycombinator.com/item?id=40100672">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								<img src="https://blog.thunderbird.net/files/2024/04/Tb-rust1.png" alt="featured post title image">
						<section>
												

				
<p>Microsoft Exchange is a popular choice of email service for corporations and educational institutions, and so it’s no surprise that there’s demand among Thunderbird users to support Exchange. Until recently, this functionality was only available through an add-on. But, in the next ESR (Extended Support) release of Thunderbird in July 2024, we expect to provide this support natively within Thunderbird. Because of the size of this undertaking, the first roll-out of the Exchange support will <a href="https://youtu.be/7jNV1J2pdPc">initially cover only email</a>, with calendar and address book support coming at a later date.</p>



<p>This article will go into technical detail on how we are implementing support for the Microsoft Exchange Web Services mail protocol, and some idea of where we’re going next with the knowledge gained from this adventure.</p>



<p><em>Before we dive in, just a quick note that <strong>Brendan Abolivier, Ikey Doherty</strong>, and <strong>Sean Burke</strong> are the developers behind this effort, and are the authors of this post.</em></p>



<figure></figure>



<h2>Historical context</h2>



<p>Thunderbird is a long-lived project, which means there’s lots of old code. The current architecture for supporting mail protocols predates Thunderbird itself, having been developed more than 20 years ago as part of Netscape Communicator. There was also no paid maintainership from about 2012 — when Mozilla divested and&nbsp; transferred ownership of Thunderbird to its community — until 2017, when Thunderbird rejoined the Mozilla Foundation. That means years of ad hoc changes without a larger architectural vision and a lot of decaying C++ code that was not using modern standards.</p>



<p>Furthermore, in the entire 20 year lifetime of the Thunderbird project, no one has added support for a new mail protocol before. As such, no one has updated the architecture as mail protocols change and adapt to modern usage patterns, and a great deal of institutional knowledge has been lost. Implementing this much-needed feature is the first organization-led effort to actually understand and address limitations of Thunderbird’s architecture in an incremental fashion.</p>



<h2>Why we chose Rust</h2>



<p>Thunderbird is a large project maintained by a small team, so choosing a language for new work cannot be taken lightly. We need powerful tools to develop complex features relatively quickly, but we absolutely must balance this with long-term maintainability. Selecting Rust as the language for our new protocol support brings some important benefits:</p>



<ol>
<li><strong>Memory safety.</strong> Thunderbird takes input from anyone who sends an email, so we need to be diligent about keeping security bugs out.</li>



<li><strong>Performance.</strong> Rust runs as native code with all of the associated performance benefits.</li>



<li><strong>Modularity and Ecosystem.</strong> The built-in modularity of Rust gives us access to a large ecosystem where there are already a lot of people doing things related to email which we can benefit from.</li>
</ol>



<p>The above are all on the standard list of benefits when discussing Rust. However, there are some additional considerations for Thunderbird:</p>



<ol>
<li><strong>Firefox.</strong> Thunderbird is built on top of Firefox code and we use a shared CI infrastructure with Firefox which already enables Rust. Additionally, Firefox provides a language interop layer called XPCOM (Cross-Platform Component Object Model), which has Rust support and allows us to call between Rust, C++, and JavaScript.</li>



<li><strong>Powerful tools.</strong> Rust gives us a large toolbox for building APIs which are difficult to misuse by pushing logical errors into the domain of the compiler. We can easily avoid circular references or provide functions which simply cannot be called with values which don’t make sense, letting us have a high degree of confidence in features with a large scope. Rust also provides first-class tooling for documentation, which is critically important on a small team.</li>



<li><strong>Addressing architectural technical debt.</strong> Introducing a new language gives us a chance to reconsider some aging architectures while benefiting from a growing language community.</li>



<li><strong>Platform support and portability.</strong> Rust supports a broad set of host platforms. By building modular crates, we can reuse our work in other projects, such as Thunderbird for Android/K-9 Mail.</li>
</ol>



<h2>Some mishaps along the way</h2>



<p>Of course, the endeavor to introduce our first Rust component in Thunderbird is not without its challenges, mostly related to the size of the Thunderbird codebase. For example, there is a lot of existing code with idiosyncratic asynchronous patterns that don’t integrate nicely with idiomatic Rust. There are also lots of features and capabilities in the Firefox and Thunderbird codebase that don’t have any existing Rust bindings.</p>



<h3>The first roadblock: the build system</h3>



<p>Our first hurdle came with getting any Rust code to run in Thunderbird at all. There are two things you need to know to understand why:</p>



<p>First, since the Firefox code is a dependency of Thunderbird, you might expect that we pull in their code as a subtree of our own, or some similar mechanism. However, for historical reasons, it’s the other way around: building Thunderbird requires fetching Firefox’s code, fetching Thunderbird’s code as a subtree of Firefox’s, and using a build configuration file to point into that subtree.</p>



<p>Second, because Firefox’s entrypoint is written in C++ and Rust calls happen via an interoperability layer, there is no single point of entry for Rust. In order to create a tree-wide dependency graph for Cargo and avoid duplicate builds or version/feature conflicts, Firefox introduced a hack to generate a single Cargo workspace which aggregates all the individual crates in the tree.</p>



<p>In isolation, neither of these is a problem in itself. However, in order to build Rust into Thunderbird, we needed to define our own Cargo workspace which lives in our tree, and Cargo does not allow nesting workspaces. To solve this issue, we had to define our own workspace and add configuration to the upstream build tool, <code>mach</code>, to build from this workspace instead of Firefox’s. We then use a newly-added <code>mach</code> subcommand to sync our dependencies and lockfile with upstream and to vendor the resulting superset.</p>



<h3>XPCOM</h3>



<p>While the availability of language interop through XPCOM is important for integrating our frontend and backend, the developer experience has presented some challenges. Because XPCOM was originally designed with C++ in mind, implementing or consuming an XPCOM interface requires a lot of boilerplate and prevents us from taking full advantage of tools like rust-analyzer. Over time, Firefox has significantly reduced its reliance on XPCOM, making a clunky Rust+XPCOM experience a relatively minor consideration. However, as part of the previously-discussed maintenance gap, Thunderbird never undertook a similar project, and supporting a new mail protocol requires implementing hundreds of functions defined in XPCOM.</p>



<p>Existing protocol implementations ease this burden by inheriting C++ classes which provide the basis for most of the shared behavior. Since we can’t do this directly, we are instead implementing our protocol-specific logic in Rust and communicating with a bridge class in C++ which combines our Rust implementations (an internal crate called <code>ews_xpcom</code>) with the existing code for shared behavior, with as small an interface between the two as we can manage.</p>



<p>Please visit our&nbsp;<a href="https://source-docs.thunderbird.net/en/latest/rust/index.html" target="_blank" rel="noreferrer noopener">documentation</a>&nbsp;to learn more about how to create Rust components in Thunderbird.</p>



<h2>Implementing Exchange support with Rust</h2>



<p>Despite the technical hiccups experienced along the way, we were able to clear the hurdles, use, and build Rust within Thunderbird. Now we can talk about how we’re using it and the tools we’re building. Remember all the way back to the beginning of this blog post, where we stated that our goal is to support Microsoft’s Exchange Web Services (EWS) API. EWS communicates over HTTP with request and response bodies in XML.</p>



<h2>Sending HTTP requests</h2>



<p>Firefox already includes a full-featured HTTP stack via its <code>necko</code> networking component. However, <code>necko</code> is written in C++ and exposed over XPCOM, which as previously stated does not make for nice, idiomatic Rust. Simply sending a GET request requires a great deal of boilerplate, including nasty-looking unsafe blocks where we call into XPCOM. (XPCOM manages the lifetime of pointers and their referents, ensuring memory safety, but the Rust compiler doesn’t know this.) Additionally, the interfaces we need are callback-based. For making HTTP requests to be simple for developers, we need to do two things:</p>



<ol>
<li><strong>Support native Rust async/await syntax.</strong> For this, we added a new Thunderbird-internal crate, <code>xpcom_async</code>. This is a low-level crate which translates asynchronous operations in XPCOM into Rust’s native async syntax by defining callbacks to buffer incoming data and expose it by implementing Rust’s <code>Future</code> trait so that it can be awaited by consumers. (If you’re not familiar with the <code>Future</code> concept in Rust, it is similar to a JS <code>Promise</code> or a Python coroutine.)</li>



<li><strong>Provide an idiomatic HTTP API.</strong> Now that we had native <code>async</code>/<code>await</code> support, we created another internal crate (<code>moz_http</code>) which provides an HTTP client inspired by <code>reqwest</code>. This crate handles creating all of the necessary XPCOM objects and providing Rustic error handling (much nicer than the standard XPCOM error handling).</li>
</ol>



<h2>Handling XML requests and responses</h2>



<p>The hardest task in working with EWS is translating between our code’s own data structures and the XML expected/provided by EWS. Existing crates for serializing/deserializing XML didn’t meet our needs. <code>serde</code>’s data model doesn’t align well with XML, making distinguishing XML attributes and elements difficult. EWS is also sensitive to XML namespaces, which are completely foreign to <code>serde</code>. Various <code>serde</code>-inspired crates designed for XML exist, but these require explicit annotation of how to serialize every field. EWS defines hundreds of types which can have dozens of fields, making that amount of boilerplate untenable.</p>



<p>Ultimately, we found that existing <code>serde</code>-based implementations worked fine for deserializing XML into Rust, but we were unable to find a satisfactory tool for serialization. To that end, we introduced another new crate, <code>xml_struct</code>. This crate defines traits governing serialization behavior and uses Rust’s procedural derive macros to automatically generate implementations of these traits for Rust data structures. It is built on top of the existing <code>quick_xml</code> crate and designed to create a low-boilerplate, intuitive mapping between XML and Rust.&nbsp; While it is in the early stages of development, it does not make use of any Thunderbird/Firefox internals and is <a href="https://github.com/thunderbird/xml-struct-rs">available on GitHub</a>.</p>



<p>We have also introduced one more new crate, <code>ews</code>, which defines types for working with EWS and an API for XML serialization/deserialization, based on <code>xml_struct</code> and <code>serde</code>. Like <code>xml_struct</code>, it is in the early stages of development, but is <a href="https://github.com/thunderbird/ews-rs">available on GitHub</a>.</p>



<h2>Overall flow chart</h2>



<p>Below, you can find a handy flow chart to help understand the logical flow for making an Exchange request and handling the response.&nbsp;</p>



<figure><a href="https://blog.thunderbird.net/files/2024/04/pasted-image-.png"><img decoding="async" fetchpriority="high" width="1600" height="716" src="https://blog.thunderbird.net/files/2024/04/pasted-image-.png" alt="A bird's eye view of the flow" title="A bird’s eye view of the flow" srcset="https://blog.thunderbird.net/files/2024/04/pasted-image-.png 1600w, https://blog.thunderbird.net/files/2024/04/pasted-image--252x113.png 252w, https://blog.thunderbird.net/files/2024/04/pasted-image--600x269.png 600w, https://blog.thunderbird.net/files/2024/04/pasted-image--768x344.png 768w, https://blog.thunderbird.net/files/2024/04/pasted-image--1536x687.png 1536w" sizes="(max-width: 1600px) 100vw, 1600px"></a></figure>



<p>Fig 1. A bird’s eye view of the flow</p>



<h2>What’s next?</h2>



<h2>Testing all the things</h2>



<p>Before landing our next major features, we are taking some time to build out our automated tests. In addition to unit tests, we just landed a mock EWS server for integration testing. The current focus on testing is already paying dividends, having exposed a couple of crashes and some double-sync issues which have since been rectified. Going forward, new features can now be easily tested and verified.</p>



<h2>Improving error handling</h2>



<p>While we are working on testing, we are also busy improving the story around error handling. EWS’s error behavior is often poorly documented, and errors can occur at multiple levels (e.g., a request may fail as a whole due to throttling or incorrect structure, or parts of a request may succeed while other parts fail due to incorrect IDs). Some errors we can handle at the protocol level, while others may require user intervention or may be intractable. In taking the time now to improve error handling, we can provide a more polished implementation and set ourselves up for easier long-term maintenance.</p>



<h2>Expanding support</h2>



<p>We are working on expanding protocol support for EWS (via <code>ews</code> and the internal <code>ews_xpcom</code> crate) and hooking it into the Thunderbird UI. Earlier this month, we landed a series of patches which allow adding an EWS account to Thunderbird, syncing the account’s folder hierarchy from the remote server, and displaying those folders in the UI. (At present, this alpha-state functionality is gated behind a build flag and a preference.) Next up, we’ll work on fetching message lists from the remote server as well as generalizing outgoing mail support in Thunderbird.</p>



<h2>Documentation</h2>



<p>Of course, all of our work on maintainability is for naught if no one understands what the code does. To that end, we’re producing documentation on how all of the bits we have talked about here come together, as well as describing the existing architecture of mail protocols in Thunderbird and thoughts on future improvements, so that once the work of supporting EWS is done, we can continue building and improving on the Thunderbird you know and love.</p>
				

			</section>
			

			
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AltStore. The first Apple approved alternative App Store (134 pts)]]></title>
            <link>https://altstore.io/#Downloads</link>
            <guid>40100151</guid>
            <pubDate>Sat, 20 Apr 2024 19:17:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://altstore.io/#Downloads">https://altstore.io/#Downloads</a>, See on <a href="https://news.ycombinator.com/item?id=40100151">Hacker News</a></p>
<div id="readability-page-1" class="page">
  <div data-collapse="medium" data-animation="default" data-duration="400" data-easing="ease" data-easing2="ease" role="banner">
        <p><a href="#"><img src="https://altstore.io/images/logo_text.png" alt="" width="124"></a></p>
      </div>
  
  <div><p>AltStore PAL now available!<br>Read the <a href="https://rileytestut.com/blog/2024/04/17/introducing-altstore-pal/" target="_blank">announcement</a>
    </p></div>
  <section>
    <h2>Sideloading for Everyone</h2>
    <p>Discover apps that push the boundaries of iOS.</p>
    
    <p><img src="https://altstore.io/images/Tuesday-23-Jan-2024-165319.png" srcset="https://altstore.io/images/Tuesday-23-Jan-2024-165319-p-500.png 500w, https://altstore.io/images/Tuesday-23-Jan-2024-165319-p-800.png 800w, https://altstore.io/images/Tuesday-23-Jan-2024-165319-p-1080.png 1080w, https://altstore.io/images/Tuesday-23-Jan-2024-165319.png 1339w" width="400" sizes="(max-width: 479px) 90vw, 400px" alt=""></p>
    
  </section>
  
  <div>
        <div>
          <h2>A New Way to Sideload</h2>
          <div><p>AltStore is an app store designed for sideloading. Every app in AltStore gets a beautifully generated store page with detailed information to make sideloading fun and easy. Browse apps from trusted developers, or add additional "sources" to further increase your options.</p><p> Plus, AltStore is made with security in mind. You can view a full list of an app's permissions from its store page, and AltStore will even automatically alert you if they change so you can sideload with confidence.</p></div>
          <p><a href="https://faq.altstore.io/" target="_blank">Learn More</a>
        </p></div><p><img src="https://altstore.io/images/DeltaStorePage.PNG" srcset="https://altstore.io/images/DeltaStorePage-p-500.png 500w, https://altstore.io/images/DeltaStorePage-p-800.png 800w, https://altstore.io/images/DeltaStorePage-p-1080.png 1080w, https://altstore.io/images/DeltaStorePage.PNG 1339w" width="350" sizes="(max-width: 479px) 90vw, 350px" alt="">
      </p></div>
  <div><p><img src="https://altstore.io/images/Monday-08-May-2023-162140.PNG" srcset="https://altstore.io/images/Monday-08-May-2023-162140-p-500.png 500w, https://altstore.io/images/Monday-08-May-2023-162140-p-800.png 800w, https://altstore.io/images/Monday-08-May-2023-162140-p-1080.png 1080w, https://altstore.io/images/Monday-08-May-2023-162140.PNG 1339w" width="350" sizes="(max-width: 479px) 90vw, 350px" alt=""></p><div>
          <h2>Self-Published Apps</h2>
          <div><p>Anyone can distribute their apps with AltStore. All you need is to make a “source”, which you can do by hosting a text file with basic information about your apps. Users can then enter your source URL in AltStore and your apps will automatically appear.</p><p>Follow our complete guide to create your own source and start distributing your apps in minutes!</p></div>
          <p><a href="https://faq.altstore.io/sources/make-a-source" target="_blank">Publish Apps</a>
        </p></div>
      </div>
  <div>
      <h2>By Indies — For Indies</h2>
      <div>
        <p><a id="w-node-a1c5e89b-ef5e-812c-23ad-c7315bcc8782-ed6becd7" href="https://mastodon.social/@rileytestut" target="_blank"><img src="https://altstore.io/images/spaces_Afe8qEztjcTjsjjaMBY2_uploads_UWT8nOrICxoO7OFmzKo0_Untitled.webp" srcset="https://altstore.io/images/spaces_Afe8qEztjcTjsjjaMBY2_uploads_UWT8nOrICxoO7OFmzKo0_Untitled-p-500.webp 500w, https://altstore.io/images/spaces_Afe8qEztjcTjsjjaMBY2_uploads_UWT8nOrICxoO7OFmzKo0_Untitled-p-800.webp 800w, https://altstore.io/images/spaces_Afe8qEztjcTjsjjaMBY2_uploads_UWT8nOrICxoO7OFmzKo0_Untitled-p-1080.webp 1080w, https://altstore.io/images/spaces_Afe8qEztjcTjsjjaMBY2_uploads_UWT8nOrICxoO7OFmzKo0_Untitled.webp 1500w" id="w-node-f271e8bc-a871-6e22-2967-dc732ff82ca8-ed6becd7" sizes="(max-width: 479px) 100vw, (max-width: 767px) 24vw, (max-width: 991px) 17vw, (max-width: 1439px) 19vw, 188px" alt=""></a></p><a id="w-node-_86a4c6a7-f9c4-ad13-f7bf-6e63b1079e20-ed6becd7" href="https://mastodon.social/@rileytestut" target="_blank">
          
        </a>
        <a id="w-node-e12031f2-99ee-5f24-ac27-ea8e62bab55b-ed6becd7" href="https://twitter.com/shanegillio">
          
        </a>
        <p><a id="w-node-_705f1323-e749-6403-8f86-b86a8cfd6f50-ed6becd7" href="https://twitter.com/shanegillio" target="_blank"><img src="https://altstore.io/images/shaneprof.webp" srcset="https://altstore.io/images/shaneprof-p-500.webp 500w, https://altstore.io/images/shaneprof-p-800.webp 800w, https://altstore.io/images/shaneprof-p-1080.webp 1080w, https://altstore.io/images/shaneprof.webp 1500w" id="w-node-a4aad372-35cc-e00f-45b1-66adc412ef93-ed6becd7" sizes="(max-width: 479px) 100vw, (max-width: 767px) 24vw, (max-width: 991px) 17vw, (max-width: 1439px) 19vw, 188px" alt=""></a>
      </p></div>
      
      <div><p>AltStore is an open-source project developed by a dedicated team of two. We are supported entirely by donations from our community and you can follow along with our progress on GitHub.</p><p>We’re continuously working on new updates for our apps, and you can try out in-development features by joining our Patreon.</p></div>
      <p><a href="https://www.patreon.com/rileyshane" target="_blank">Join Patreon</a>
    </p></div>
  <section id="Downloads">
    <h2><span>Downloads</span></h2>
    <p>AltStore, Delta, and Clip are properties of AltStore LLC and are in no way associated with Nintendo Co., Ltd. or Apple Inc.</p>
    <div>
        <p>AltStore PAL</p>
        <p>Available only in Europe. Requires iOS 17.4 or later.</p>
        <p><a href="https://buy.stripe.com/6oEg2u80z5vI0Mg4gg">€1.50/year + VAT</a></p><div><p>Your subscription covers Apple's Core Technology Fee, payment processing, and server costs.</p><p>Don't want to pay, or not in the EU? Download the version of AltStore below.</p></div>
      </div>
    <p>AltStore (World)</p>
    <p>Requires AltServer to install. Follow our step-by-step <a href="http://faq.altstore.io/">Install Guide</a>
    </p>
    
    <div>
      <p>“[AltStore] is clever, has been verified by other developers, and the service has an active community of thousands of users who side-load apps on their devices. For the past few weeks, I’ve been one of them.”</p>
      
    </div>
    
  </section>
  
  
  <div>
        <p data-w-id="c10f4652-e98c-37f6-51a6-bb8725682d07">
          <h2 data-w-id="0fb59a4f-6d90-2333-688a-b06b291420c6">Experience Apps like Never Before</h2>
          <h3 data-w-id="332883b0-f4e4-5f0a-bd2e-026003ed9cc2">AltStore allows apps to exist on iOS&nbsp;that may not otherwise. <br>‍<br>Apple doesn't allow all apps on their store, so AltStore gives those apps a chance.</h3>
        </p><p><img src="https://altstore.io/images/AltStore_Delta_StorePage.png" width="416" alt="" sizes="100vw" data-w-id="ae8b192f-4a13-e725-9af4-854638dc4268" loading="lazy" srcset="https://altstore.io/images/AltStore_Delta_StorePage-p-500.png 500w, https://altstore.io/images/AltStore_Delta_StorePage-p-800.png 800w, https://altstore.io/images/AltStore_Delta_StorePage-p-1080.png 1080w, https://altstore.io/images/AltStore_Delta_StorePage.png 1400w">
      </p></div>
  
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why you Should Not Apply To YC (392 pts)]]></title>
            <link>https://twitter.com/dvassallo/status/1781751108211511680</link>
            <guid>40099585</guid>
            <pubDate>Sat, 20 Apr 2024 18:26:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/dvassallo/status/1781751108211511680">https://twitter.com/dvassallo/status/1781751108211511680</a>, See on <a href="https://news.ycombinator.com/item?id=40099585">Hacker News</a></p>
Couldn't get https://twitter.com/dvassallo/status/1781751108211511680: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Financial Market Applications of LLMs (124 pts)]]></title>
            <link>https://thegradient.pub/financial-market-applications-of-llms/</link>
            <guid>40099344</guid>
            <pubDate>Sat, 20 Apr 2024 18:03:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thegradient.pub/financial-market-applications-of-llms/">https://thegradient.pub/financial-market-applications-of-llms/</a>, See on <a href="https://news.ycombinator.com/item?id=40099344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural understanding emerges from learning next-token prediction, and agents are able to complete tasks such as translation, question answering and generating human-like prose from simple user prompts.</p><p>Not surprisingly, quantitative traders have asked: can we turn these models into the next price or trade prediction [1,9,10]? That is, rather than modeling sequences of words, can we model sequences of prices or trades. This turns out to be an interesting line of inquiry that reveals much about both generative AI and financial time series modeling. Be warned this will get wonky.</p><p>LLMs are known as autoregressive learners -- those using previous tokens or elements in a sequence to predict the next element or token. In quantitative trading, for example in strategies like statistical arbitrage in stocks, most research is concerned with identifying autoregressive structure. That means finding sequences of news or orders or fundamental changes that best predict future prices.</p><p>Where things break down is in the quantity and information content of available data to train the models. At the 2023 NeurIPS conference, Hudson River Trading, a high frequency trading firm, presented a comparison of the number of input tokens used to train GPT-3 with the amount of trainable tokens available in the stock market data per year HRT estimated that, with 3,000 tradable stocks, 10 data points per stock per day, 252 trading days per year, and 23400 seconds in a trading day, there are 177 billion stock market tokens per year available as market data. GPT-3 was trained on 500 billion tokens, so not far off [6].</p><figure><img src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png" alt="" loading="lazy" width="2000" height="368" srcset="https://thegradient.pub/content/images/size/w600/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 600w, https://thegradient.pub/content/images/size/w1000/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 1000w, https://thegradient.pub/content/images/size/w1600/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 1600w, https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 2356w" sizes="(min-width: 720px) 720px"><figcaption>numbers courtesy of HRT 2023 NeuRIPS presentation</figcaption></figure><p>But, in the trading context the tokens will be prices or returns or trades rather than syllables or words; the former is much more difficult to predict. Language has an underlying linguistic structure (e.g., grammar) [7]. It’s not hard to imagine a human predicting the next word in a sentence, however that same human would find it extremely challenging to predict the next return given a sequence of previous trades, hence the lack of billionaire day traders. The challenge is that there are very smart people competing away any signal in the market, making it <em>almost </em>efficient (“efficiently inefficient”, in the words of economist Lasse Pedersen) and hence unpredictable. No adversary actively tries to make sentences more difficult to predict — if anything, authors usually seek to make their sentences easy to understand and hence <em>more</em> predictable.</p><p>Looked at from another angle, there is much more noise than signal in financial data. Individuals and institutions are trading for reasons that might not be rational or tied to any fundamental change in a business. The GameStop episode in 2021 is one such example. Financial time series are also constantly changing with new fundamental information, regulatory changes, and occasional large macroeconomic shifts such as currency devaluations. Language evolves at a much slower pace and over longer time horizons.</p><p>On the other hand, there are reasons to believe that ideas from AI will work well in financial markets. One emerging area of AI research with promising applications to finance is multimodal learning [5], which aims to use different modalities of data, for example both images and textual inputs to build a unified model. With OpenAI’s DALL-E 2 model, a user can enter text and the model will generate an image. In finance, multi-modal efforts could be useful to combine information classical sources such as technical time series data (prices, trades, volumes, etc.) with alternative data in different modes like sentiment or graphical interactions on twitter, natural language news articles and corporate reports, or the satellite images of shipping activity in a commodity centric port. Here, leveraging multi-modal AI, one could potentially incorporate all these types of non-price information to predict well.</p><p>Another strategy called ‘residualization’ holds prominence in both finance and AI, though it assumes different roles in the two domains. &nbsp;In finance, structural `factor’ models break down the contemporaneous observations of returns across different assets into a shared component (the market return, or more generally returns of common, market-wide factors) and an idiosyncratic component unique to each underlying asset. Market and factor returns are difficult to predict and create interdependence, so it is often helpful to remove the common element when making predictions at the individual asset level and to maximize the number of independent observations in the data. </p><p>In residual network architectures such as transformers, there’s a similar idea that we want to learn a function h(X) of an input X, but it might be easier to learn the residual of h(X) to the identity map, i.e., h(X) – X. Here, if the function h(X) is close to identity, its residual will be close to zero, and hence there will be less to learn and learning can be done more efficiently. In both cases the goal is to exploit structure to refine predictions: in the finance case, the idea is to focus on predicting innovations beyond what is implied by the overall market, for residual networks the focus is on predicting innovations to the identity map.</p><p>A key ingredient for the impressive performance of LLMs work is their ability to discern affinities or strengths between tokens over long horizons known as context windows. In financial markets, the ability to focus attention across long horizons enables analysis of multi-scale phenomena, with some aspects of market changes explained across very different time horizons. For example, at one extreme, fundamental information (e.g., earnings) may be incorporated into prices over months, technical phenomena (e.g., momentum) might be realized over days, and, at the other extreme, microstructure phenomena (e.g., order book imbalance) might have a time horizon of seconds to minutes.</p><p>Capturing all of these phenomena involves analysis of multiple time horizons across the context window. However, in finance, prediction over multiple <em>future</em> time horizons is also important. For example, a quantitative system may seek to trade to profit from multiple different anomalies that are realized over multiple time horizons (e.g., simultaneously betting on a microstructure event and an earnings event). This requires predicting not just the next period return of the stock, but the entire term structure or trajectory of expected returns, while current transformer-style predictive models only look one period in the future.</p><p>Another financial market application of LLMs might be synthetic data creation [4,8]. This could take a few directions. Simulated stock price trajectories can be generated that mimic characteristics observed in the market and can be extremely beneficial given that financial market data is scarce relative to other sources as highlighted above in the number of tokens available. Artificial data could open the door for meta-learning techniques which have successfully been applied, for example, in robotics. In the robotic setting controllers are first trained using cheap but not necessarily accurate physics simulators, before being better calibrated using expensive real world experiments with robots. In finance the simulators could be used to coarsely train and optimize trading strategies. The model would learn high level concepts like risk aversion and diversification and tactical concepts such as trading slowly to minimize the price impact of a trade. Then precious real market data could be employed to fine-tune the predictions and determine precisely the optimal speed to trade.</p><figure><img src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png" alt="" loading="lazy" width="2000" height="258" srcset="https://thegradient.pub/content/images/size/w600/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 600w, https://thegradient.pub/content/images/size/w1000/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 1000w, https://thegradient.pub/content/images/size/w1600/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 1600w, https://thegradient.pub/content/images/size/w2400/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Financial market practitioners are often interested in extreme events, the times when trading strategies are more likely to experience significant gains or losses. Generative models where it’s possible to sample from extreme scenarios could find use. However extreme events by definition occur rarely and hence determining the right parameters and sampling data from the corresponding distribution is fraught.</p><p>Despite the skepticism that LLMs will find use in quantitative trading, they might boost fundamental analysis. As AI models improve, it’s easy to imagine them helping analysts refine an investment thesis, uncover inconsistencies in management commentary or find latent relationships between tangential industries and businesses [3]. Essentially these models could provide a Charlie Munger for every investor.</p><p>The surprising thing about the current generative AI revolution is that it’s taken almost everyone – academic researchers, cutting edge technology firms and long-time observers – by surprise. The idea that building bigger and bigger models would lead to emergent capabilities like we see today was totally unexpected and still not fully understood.</p><p>The success of these AI models has supercharged the flow of human and financial capital into AI, which should in turn lead to even better and more capable models. So while the case for GPT-4 like models taking over quantitative trading is currently unlikely, we advocate keeping an open mind. Expecting the unexpected has been a profitable theme in the AI business.</p><hr><h3 id="references">References</h3><ol><li>“Applying Deep Neural Networks to Financial Time Series Forecasting” Allison Koenecke. 2022</li><li>“<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=2960712678066186980&amp;btnI=1&amp;hl=en">Attention is all you need</a>.” A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones… &nbsp;Advances in Neural Information Processing Systems, 2017</li><li>“Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models” . Lopez-Lira, Alejandro and Tang, Yuehua, (April 6, 2023) Available at SSRN</li><li>“<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=8154893591177160457&amp;btnI=1&amp;hl=en">Generating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls</a>.” SA Assefa, D Dervovic, M Mahfouz, RE Tillman… - Proceedings of the First ACM International Conference …, 2020</li><li>“GPT-4V(ision) System Card.” OpenAI. September 2023</li><li>“<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=15953747982133883426&amp;btnI=1&amp;hl=en">Language models are few-shot learners</a>.” T Brown, B Mann, N Ryder, M Subbiah, JD Kaplan… - Advances in Neural Information Processing Systems, 2020</li><li>“Sequence to Sequence Learning with Neural Networks.” I.Sutskever,O.Vinyals,and Q.V.Le in Advances in Neural Information Processing Systems, 2014, pp. 3104–3112.</li><li>“Synthetic Data Generation for Economists”. A Koenecke, H Varian &nbsp;- arXiv preprint arXiv:2011.01374, 2020</li><li>C. C. Moallemi, M. Wang. A reinforcement learning approach to optimal execution. Quantitative Finance, 22(6):1051–1069, March 2022.</li><li>C. Maglaras, C. C. Moallemi, M. Wang. A deep learning approach to estimating fill probabilities in a limit order book. Quantitative Finance, 22(11):1989–2003, October 2022.</li></ol><h3 id="citation">Citation</h3><p>For attribution in academic contexts or books, please cite this work as</p><pre><code>Richard Dewey and Ciamac Moallemi, "Financial Market Applications of LLMs," The Gradient, 2024</code></pre><pre><code>@article{dewey2024financial,
    author = {Richard Dewey and Ciamac Moallemi},
    title = {Financial Market Applications of LLMs},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/financial-market-applications-of-llms},
}</code></pre>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-reasoning tokens: teaching models to think ahead (117 pts)]]></title>
            <link>https://reasoning-tokens.ghost.io/reasoning-tokens/</link>
            <guid>40099252</guid>
            <pubDate>Sat, 20 Apr 2024 17:54:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reasoning-tokens.ghost.io/reasoning-tokens/">https://reasoning-tokens.ghost.io/reasoning-tokens/</a>, See on <a href="https://news.ycombinator.com/item?id=40099252">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p><a href="https://reasoning-tokens.ghost.io/author/felipe/">
                                <img src="https://www.gravatar.com/avatar/c7a32029edda420c1bff08b48f99b2cb?s=250&amp;r=x&amp;d=mp" alt="Felipe Sens Bonetto">
                            </a>
                </p>
                <div>
                    
                    <p><time datetime="2024-04-20">Apr 20, 2024</time>
                            <span><span>—</span> 4 min read</span>
                    </p>
                </div>
            </div><section>
            <p>What is the mathematical formulation of reasoning? How can we make LLMs like chatGPT think before they speak? And how can we make that baked into the model so it can learn to think in a self-supervised way without having to "explain it step by step" (or another famous prompt we use when we want to improve chatGPT performance drastically)? How can we teach models to think ahead? I will share with you the results of some experiments that may cast light on the path of "Reasoning Tokens."</p><p><strong>Introduction</strong></p><p>As the authors of <a href="https://arxiv.org/abs/2211.00593?ref=reasoning-tokens.ghost.io" rel="noreferrer">"Interpretability in the wild"</a> have taught us, from looking inside transformers, we know that the computation of the next token includes some information computed in previous steps.  This may seem obvious at first glance, but there is more to this affirmation than what meets the eye. This means the language model expends some internal "cognitive power" processing and storing information that will be used, not for predicting the very next token but 2, 3, or even 10 tokens ahead.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png" alt="" loading="lazy" width="1520" height="654" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png 1000w, https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png 1520w" sizes="(min-width: 720px) 720px"><figcaption><span>Internal computation of GPT-2, extracted from the "Interpretability in the wild" paper </span></figcaption></figure><p>As we can see from the image above, the attention heads produce computations that will be helpful only in the far future, and even some calculations that "headge" against the wrong answers, exposed in the paper as "Negative Name Mover Heads" or attention heads that suppress specific tokens.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.30.01-AM.png" alt="" loading="lazy" width="920" height="860" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-20-at-12.30.01-AM.png 600w, https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.30.01-AM.png 920w" sizes="(min-width: 720px) 720px"><figcaption><span>Visual explanation extracted from the "Do Language Models Plan for Future Tokens?" paper</span></figcaption></figure><p>Further work has shown that LLMs indeed plan for future tokens. In the paper <a href="https://arxiv.org/abs/2404.00859?ref=reasoning-tokens.ghost.io" rel="noreferrer">"Do Language Models Plan for Future Tokens?"</a> the authors carefully crafted a mathematical formulation to impede what they call "Pre-Caching," or the ability of the model to make intermediary computations that would be useful beyond the very next token. Their experiments found a small performance gap when the model was "myopic" or incapable of planning for future tokens. This is promising but could be better. This indicates that while GPTs plan ahead, most of their power is used to predict only the next word in the sequence. As a sanity check, this gap should increase as the length of the predicted text grows because the model would have more tokens to produce said computations, and indeed, that was what they found in the paper.</p><p><strong>How do we leverage that?</strong></p><p>What if we incentivized those intermediary calculations, which are useful only in future tokens, teaching the model to think ahead in a self-supervised way? It turns out that the formulation for such a task doesn't need to be that complicated.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-1.png" alt="" loading="lazy" width="1312" height="504" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-1.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-1.png 1000w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-1.png 1312w" sizes="(min-width: 720px) 720px"><figcaption><span>Gradient flow of Reasoning tokens!</span></figcaption></figure><p>In this first experiment, we introduce <strong>reasoning tokens</strong>! The model will produce two tokens for each token in the original sequence. As usual, the first token will be used to predict the next token. The second token, however, duplicates the input of the first one and does not receive a gradient "answer" from the very next token, only from future tokens; in fact, this token doesn't even participate in the calculation of the very next token. This incentivizes the model to "pre-cache" or <em>only</em> put information that is useful for the future in this spot. <em>But talk is cheap. Show me the results.</em></p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-3.png" alt="" loading="lazy" width="2000" height="918" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-3.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-3.png 1000w, https://reasoning-tokens.ghost.io/content/images/size/w1600/2024/04/image-3.png 1600w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-3.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>Mini GPT-2 (10M params) trained on 82M tokens.</span></figcaption></figure><p>And the results are very promising, showing a reduction of <strong>35% in the loss</strong>! From 0.621 to 0.401. The experiment also shows that the model benefits from having multiple tokens to do its "reasoning," forecasting the capability to form long-range dependencies. This validates the hypothesis that we can teach the models to plan for the future, an important first step to get to reasoning. </p><p>A GPT-2 Small (124M params) model was also trained on 300B tokens of the "Open Web Text Corpus," and its results were also very promising, resulting in a 0.04 validation loss reduction from 2.85 to 2.81. In context, going from GPT-2 Large (~700M) to GPT-2 XL (1.5B) drops the validation loss by 0.13 in the same dataset. All training code was derived from Andrej Karpathy amazing <a href="https://github.com/karpathy/nanoGPT/tree/master?ref=reasoning-tokens.ghost.io" rel="noreferrer">GPT-2 implementation</a>.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-4.png" alt="" loading="lazy" width="1088" height="628" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-4.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-4.png 1000w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-4.png 1088w" sizes="(min-width: 720px) 720px"><figcaption><span>GPT-2 Small trained on 300B params - 1 Reasoning token</span></figcaption></figure><p><strong>What is next for Reasoning Tokens?</strong></p><p>Currently, I'm experimenting with reasoning tokens in fine-tuned instruction following models, where planning can be much more useful. The formulation is very close to the first experiment. Still, this time, the model can choose when this internal reasoning will start, allowing it to choose when to reason before producing the next word in the sequence.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-5.png" alt="" loading="lazy" width="1714" height="910" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-5.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-5.png 1000w, https://reasoning-tokens.ghost.io/content/images/size/w1600/2024/04/image-5.png 1600w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-5.png 1714w" sizes="(min-width: 720px) 720px"><figcaption><span>Reasoning tokens in instruction tasks</span></figcaption></figure><p>The hypothesis being tested is that the addition of Reasoning Tokens can substitute and outperform models where a "step by step" explanation is included in the training phase. This would be useful because those explanations are expensive to produce/obtain. Although such explanations can be useful to the model, gradient descent could find other ways to do that reasoning using all the internal mathematical dimensions of the model in a way that does not necessarily make sense to us. It would be a great fit for "Mixture of Experts" (MoE) models, where we can have an expert just for the reasoning phase.</p><p>The future is bright. Stay tuned for the next advancements.</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VLC vs. the App Stores (128 pts)]]></title>
            <link>https://mjtsai.com/blog/2024/04/19/vlc-vs-the-app-stores/</link>
            <guid>40098867</guid>
            <pubDate>Sat, 20 Apr 2024 17:15:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mjtsai.com/blog/2024/04/19/vlc-vs-the-app-stores/">https://mjtsai.com/blog/2024/04/19/vlc-vs-the-app-stores/</a>, See on <a href="https://news.ycombinator.com/item?id=40098867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><a href="https://twitter.com/videolan/status/1771104206872555660">VideoLAN</a> (via <a href="https://news.ycombinator.com/item?id=39798565">Hacker News</a>):</p>
<blockquote cite="https://twitter.com/videolan/status/1771104206872555660"><p>App Stores were a mistake.</p><p>Currently, we cannot update VLC on Windows Store, and we cannot update VLC on Android Play Store, without reducing security or dropping a lot of users…</p><p>For now, iOS App Store still allows us to ship for iOS9, but until when?</p></blockquote>

<p><a href="https://twitter.com/videolan/status/1759688788232523788">VideoLAN</a>:</p>
<blockquote cite="https://twitter.com/videolan/status/1759688788232523788"><p>If you do wonder why we don’t update VLC on the Windows Store or why VLC/iOS can’t connect properly to OneDrive shares, it’s because Microsoft Kafkaïesque bureaucracy refuses to help us.</p><p>We’re only trying to contact someone since 2years…</p></blockquote>

<p><a href="https://twitter.com/videolan/status/1771102415279763909">VideoLAN</a> (<a href="https://social.treehouse.systems/@Aissen/112139649840297169">Anisse</a>, <a href="https://news.ycombinator.com/item?id=39827828">Hacker News</a>):</p>
<blockquote cite="https://twitter.com/videolan/status/1771102415279763909"><p>If you wonder why we can’t update the VLC on Android version, it’s because Google refuses to let us update:</p><ul><li>either we give them our private signing keys,</li><li>or we drop support for Android TV before API-30, and all our users on TV API&lt;30 can’t get fixes.</li></ul></blockquote>

<p><a href="https://twitter.com/videolan/status/1771123709366943875">VideoLAN</a>:</p>
<blockquote cite="https://twitter.com/videolan/status/1771123709366943875">
<p>VLC cannot even enter the Mac App Store, because of the restrictions…</p>
</blockquote>

<p>Look at all those platforms competing to benefit users.</p>

<p><a href="https://twitter.com/Florian4Gamers/status/1778742764567429366">Florian Mueller</a>:</p>
<blockquote cite="https://twitter.com/Florian4Gamers/status/1778742764567429366"><p>This here is a European app store for Android and Google’s <a href="https://twitter.com/luishg/status/1778717325652341075">YouTube has just killed their channel</a>. It’s obviously a problem if you depend on the incumbent’s platforms all the way.</p></blockquote>

<p>Previously:</p>
<ul>
<li><a href="https://mjtsai.com/blog/2024/03/21/u-s-sues-apple-over-iphone-monopoly/">U.S. Sues Apple Over iPhone Monopoly</a></li>
<li><a href="https://mjtsai.com/blog/2021/07/01/google-sunsets-the-apk-format-for-new-android-apps/">Google Sunsets the APK Format for New Android Apps</a></li>
</ul><p><a rel="tag" href="https://mjtsai.com/blog/tag/android/">Android</a> <a rel="tag" href="https://mjtsai.com/blog/tag/appstore/">App Store</a> <a rel="tag" href="https://mjtsai.com/blog/tag/google-play-store/">Google Play Store</a> <a rel="tag" href="https://mjtsai.com/blog/tag/ios/">iOS</a> <a rel="tag" href="https://mjtsai.com/blog/tag/ios-17/">iOS 17</a> <a rel="tag" href="https://mjtsai.com/blog/tag/ios-9/">iOS 9</a> <a rel="tag" href="https://mjtsai.com/blog/tag/macapp/">Mac App</a> <a rel="tag" href="https://mjtsai.com/blog/tag/macappstore/">Mac App Store</a> <a rel="tag" href="https://mjtsai.com/blog/tag/macos-14-sonoma/">macOS 14 Sonoma</a> <a rel="tag" href="https://mjtsai.com/blog/tag/onedrive/">Microsoft OneDrive</a> <a rel="tag" href="https://mjtsai.com/blog/tag/vlc/">VLC</a> <a rel="tag" href="https://mjtsai.com/blog/tag/windows-store/">Windows Store</a> <a rel="tag" href="https://mjtsai.com/blog/tag/youtube/">YouTube</a></p>





















</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stop Doing Cloud (103 pts)]]></title>
            <link>https://grski.pl/self-host</link>
            <guid>40098405</guid>
            <pubDate>Sat, 20 Apr 2024 16:12:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grski.pl/self-host">https://grski.pl/self-host</a>, See on <a href="https://news.ycombinator.com/item?id=40098405">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2>STOP DOING CLOUD</h2>
<p>This will be a feisty juicy article, a bit controversial. I think more than a half of the users of the cloud/kubernetes would be better off without it. AWS should stand for <code>how to have people pay for our infra we need once per year during black friday and actually make money out of it</code>. Declouding is a nice trend I'm seeing now. 37signals have some good stuff on it. I'll ad my share as to why what has once been something cool has evolved into an abomination that often adds more complexity and problems than it brings, at least for some people. </p>
<p><img alt="meme about the cloud" src="https://grski.pl/static/articles/cloud/fools.png"></p>
<p>Sure it has it's uses at a certain scale and so on. The problem is almost no one is at such a scale and never will be, but we are blindly following a trend, pretending it's not the reality. </p>
<p>Why not try... Simplicty? Boring old stuff that just works, is easily debuggable and that even one person can grasp? </p>
<p>No your startup with 100k monthly users probably doesn't need all the stuff AWS excells at. To be honest most of you will be fine running a single dedicated bare-metal server.</p>
<p>Cloud pricing is unclear often, performance is not that dependable, especially on shared resources. To bring down costs you need to often sign up for long-term plans. So on so forth. Layers of abstractions upon abstractions.</p>
<p>I still do use the cloud in some of my work, but there's an alternative people have forgotten about - actually hosting your shit. Owning your data. Your architecture. Everything. Today I'll show you an example how we can do that. In this article we will go through setting up a self-hosted postgres instance, replicated/scalable API, load balancer, automatic ssl management, simple deployment that can be automated in 10 minutes and lastly, we will do that for under $200 per month and within 15 minutes. With this setup in some cases I'd argue you can handle up to 1M monthly active users without a hitch. See why the cloud providers and gurus have...</p>
<h2>They have played us for absolute fools.</h2>
<p>In the article I assume you have a server running somewhere. Preferable a dedicated one. In my case it's 80 core 128 gb hetzner, that I got for $200 per month.</p>
<p>Before starting let's install some utils we will need and update our server.</p>
<div><pre><span></span><code>sudo<span> </span>apt<span> </span>update<span> </span><span>&amp;&amp;</span><span> </span>sudo<span> </span>apt<span> </span>upgrade
sudo<span> </span>apt<span> </span>install<span> </span>gnupg2<span> </span>wget<span> </span>vim<span> </span>ca-certificates<span> </span>curl<span> </span>gnupg<span> </span>lsb-release
</code></pre></div>
<h2>Installing postgres</h2>
<h3>Installing needed packages</h3>
<div><pre><span></span><code>sudo<span> </span>sh<span> </span>-c<span> </span><span>'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" &gt; /etc/apt/sources.list.d/pgdg.list'</span>
curl<span> </span>-fsSL<span> </span>https://www.postgresql.org/media/keys/ACCC4CF8.asc<span> </span><span>|</span><span> </span>sudo<span> </span>gpg<span> </span>--dearmor<span> </span>-o<span> </span>/etc/apt/trusted.gpg.d/postgresql.gpg
sudo<span> </span>apt-get<span> </span>--purge<span> </span>remove<span> </span>postgresql<span> </span>postgresql-*<span> </span><span># IF YOU HAD POSTGRES PREVIOUSLY</span>
sudo<span> </span>apt<span> </span>update
sudo<span> </span>apt<span> </span>install<span> </span>postgresql-16<span> </span>postgresql-contrib-16
sudo<span> </span>systemctl<span> </span>start<span> </span>postgresql
sudo<span> </span>systemctl<span> </span><span>enable</span><span> </span>postgresql
</code></pre></div>
<p>Let's quickly walk through what we did here. We've added newest postgres repos so that our server knows what and where to install from. In case of ubuntu 22.04, the default postgres version that the distro repos come with is postgres 14. We want the new fancy shiny stuff, so we had to make that extra step.</p>
<p>Then, we optionally uninstall previous postgres versions. I doubt you had any, but adding this step as it might be helpful for some of you. Be careful though. That <code>--purge</code> thing will purge a lot of stuff. Your data from databases included. If you want to ugprade from existing postgres installation, this guide is not for you.</p>
<p>After that we update our sources and install postgres + some needed packages. </p>
<p>Lastly we start the postgresql service and make it enabled - so it boots after startup. </p>
<p>Now, we have to... Well, actually that's it. AWS marketers have done a good job in making you think installing and running a database was hard and a complex task. In some cases it is, indeed. But for the average IT Joe/startup? I wouldn't say so.</p>
<h3>Creating new database and user</h3>
<p>Now we need to do a little bit of setup on our database. In order to do that, let's connect to it using <code>psql</code>. How?</p>

<p>now you should see something like:</p>
<div><pre><span></span><code>psql<span> </span><span>(</span><span>16</span>.0<span> </span><span>(</span>Ubuntu<span> </span><span>16</span>.0-1.pgdg22.04+1<span>))</span>
Type<span> </span><span>"help"</span><span> </span><span>for</span><span> </span>help.

<span>postgres</span><span>=</span><span>#</span>
</code></pre></div>
<p>Boom. There we are. To test if our efforts in installing the newest postgres version have not failed, type:</p>
<div><pre><span></span><code><span>psql</span><span> </span><span>(</span><span>16</span><span>.</span><span>0</span><span> </span><span>(</span><span>Ubuntu</span><span> </span><span>16</span><span>.</span><span>0</span><span>-</span><span>1</span><span>.</span><span>pgdg22</span><span>.</span><span>04</span><span>+</span><span>1</span><span>))</span>
<span>Type</span><span> </span><span>"help"</span><span> </span><span>for</span><span> </span><span>help</span><span>.</span>

<span>postgres</span><span>=#</span><span> </span><span>SELECT</span><span> </span><span>version</span><span>();</span>
<span>                                                              </span><span>version</span>
<span>-----------------------------------------------------------------------------------------------------------------------------------</span>
<span> </span><span>PostgreSQL</span><span> </span><span>16</span><span>.</span><span>0</span><span> </span><span>(</span><span>Ubuntu</span><span> </span><span>16</span><span>.</span><span>0</span><span>-</span><span>1</span><span>.</span><span>pgdg22</span><span>.</span><span>04</span><span>+</span><span>1</span><span>)</span><span> </span><span>on</span><span> </span><span>x86_64</span><span>-</span><span>pc</span><span>-</span><span>linux</span><span>-</span><span>gnu</span><span>,</span><span> </span><span>compiled</span><span> </span><span>by</span><span> </span><span>gcc</span><span> </span><span>(</span><span>Ubuntu</span><span> </span><span>11</span><span>.</span><span>4</span><span>.</span><span>0</span><span>-</span><span>1</span><span>ubuntu1</span><span>~</span><span>22</span><span>.</span><span>04</span><span>)</span><span> </span><span>11</span><span>.</span><span>4</span><span>.</span><span>0</span><span>,</span><span> </span><span>64</span><span>-</span><span>bit</span>
<span>(</span><span>1</span><span> </span><span>row</span><span>)</span>
</code></pre></div>
<p>As you can see, <code>PostgreSQL 16.0</code>. Congrats, we made it brahs. </p>
<p>Currently you are inside your postgres, running as the default allmighty postgres user on postgres database. Now - we DO NOT EVER want to run our apps on this database. Don't be a lazy bum. It's a big security breach potentailly. So what do we do instead one might ask? That is a trivial question - we need to create a seprate database and a separate user for that database. Usually you have separate db (or multiple dbs actually) for an app/service couple with user just for that db.</p>
<p>That way if someone ever manages to break into your DB, in case you are hosting multiple dbs with data from multiple apps, they only get access to that one particular db. Is it hard? Nope. Check this out:</p>
<div><pre><span></span><code><span>postgres</span><span>=#</span><span> </span><span>CREATE</span><span> </span><span>DATABASE</span><span> </span><span>yourdbname</span><span>;</span>
<span>CREATE</span><span> </span><span>DATABASE</span>
<span>postgres</span><span>=#</span><span> </span><span>CREATE</span><span> </span><span>USER</span><span> </span><span>youruser</span><span> </span><span>WITH</span><span> </span><span>ENCRYPTED</span><span> </span><span>PASSWORD</span><span> </span><span>'yourpass'</span><span>;</span>
<span>CREATE</span><span> </span><span>ROLE</span>
<span>postgres</span><span>=#</span><span> </span><span>GRANT</span><span> </span><span>ALL</span><span> </span><span>PRIVILEGES</span><span> </span><span>ON</span><span> </span><span>DATABASE</span><span> </span><span>yourdbname</span><span> </span><span>TO</span><span> </span><span>youruser</span><span>;</span>
<span>GRANT</span>
<span>postgres</span><span>=#</span><span> </span><span>ALTER</span><span> </span><span>DATABASE</span><span> </span><span>yourdbname</span><span> </span><span>OWNER</span><span> </span><span>TO</span><span> </span><span>youruser</span><span>;</span>
<span>ALTER</span><span> </span><span>DATABASE</span>
</code></pre></div>
<p>We created a new user with a particular password, created a new database. Then we assigned the user privilages to perform all operations on said database, but only on that database.</p>
<p>The last line is needed because we created the database as the postgres user. Which means that while the user can perform actions on the database, he can only perform actions on the database objects that are his own. Because we created the database as the postgres user, and during db creation it gets created with some default schemas/tables, by default the owner of these is the user that created it. In our case - postgres. So other than allowing our new user to perform any action on the said database, we need to now make him an owner of the stuff that's already existing in the database so he can modify it too if needed, and it will be. </p>
<p>By the way interesting concept right? Even when you create an empty database, it's already populated with some stuff, so it ain't empty. IT, right?</p>
<p>That's pretty much it. Or is it? </p>
<h3>Connecting to postgres from outside of localhost</h3>
<p>Postgres, by default, only allows you to connect to itself from localhost/local machine to simplify. Meaning - any connections from other ips, networks etc. will be rejected. It's a very needed security measure that prevents random people from the internet to try and brute force their way into your database. That is the last thing you want.</p>
<p>However we live in a world where everything is running in a container. Containers have their own networks (usually) and when we make requests from inside of the container, the network we are in will be a bit different, meaning we won't be 'marked' as localhost, which in turn currently will make postgres reject our connection, even if we specify correct credentials.</p>
<p>I know it may sound tricky - how come, we are on the same machine, local machine. Why is our request treated as it isn't? This relats to how docker, containers and their networking works. Docker has it's own private network for all the stuff it does, sometimes sharing it with the host (in the host network mode) or having a 'bridge' that acts as a, well, bridge, between the local network and docker network, which allows you to, for example, call services hosted on the host machine, from within docker container.</p>
<p>This way you can have multiple docker containers or docker-composes running, some of which internally are using the same ports, without conflicts and so on. They are usually put in other networks created eg. per docker-compose (unless you specify otherwise).</p>
<p>It's a great thing, however in this case it complicates stuff for us, but not by much. What do we have to do?</p>
<p>Well, first of all, install docker! It'll come in handy, right?</p>
<h2>Installing docker on ubuntu 22.04</h2>
<p>First off, again - if you tried to install something before hand you might want to do this to purge everything and have a clean slate. Again - be careful.</p>
<div><pre><span></span><code>sudo<span> </span>apt<span> </span>remove<span> </span>docker-desktop
rm<span> </span>-r<span> </span><span>$HOME</span>/.docker/desktop
sudo<span> </span>rm<span> </span>/usr/local/bin/com.docker.cli
sudo<span> </span>apt<span> </span>purge<span> </span>docker-desktop
</code></pre></div>
<p>Once we have that, we will add new sources to our repos, this time for docker, similarly as we did for our postgres.</p>
<div><pre><span></span><code>curl<span> </span>-fsSL<span> </span>https://download.docker.com/linux/ubuntu/gpg<span> </span><span>|</span><span> </span>sudo<span> </span>gpg<span> </span>--dearmor<span> </span>-o<span> </span>/etc/apt/keyrings/docker.gpg
<span>echo</span><span> </span><span>\</span>
<span>  </span><span>"deb [arch=</span><span>$(</span>dpkg<span> </span>--print-architecture<span>)</span><span> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \</span>
<span>  </span><span>$(</span>lsb_release<span> </span>-cs<span>)</span><span> stable"</span><span> </span><span>|</span><span> </span>sudo<span> </span>tee<span> </span>/etc/apt/sources.list.d/docker.list<span> </span>&gt;<span> </span>/dev/null
sudo<span> </span>apt<span> </span>update
</code></pre></div>
<p>Now, let's see what versions are available to us:</p>
<div><pre><span></span><code>apt-cache<span> </span>madison<span> </span>docker-ce<span> </span><span>|</span><span> </span>awk<span> </span><span>'{ print $3 }'</span>
<span>5</span>:24.0.6-1~ubuntu.22.04~jammy
<span>5</span>:24.0.5-1~ubuntu.22.04~jammy
<span>5</span>:24.0.4-1~ubuntu.22.04~jammy
<span>5</span>:24.0.3-1~ubuntu.22.04~jammy
<span>5</span>:24.0.2-1~ubuntu.22.04~jammy
<span>5</span>:24.0.1-1~ubuntu.22.04~jammy
<span>5</span>:24.0.0-1~ubuntu.22.04~jammy
<span>5</span>:23.0.6-1~ubuntu.22.04~jammy
<span>5</span>:23.0.5-1~ubuntu.22.04~jammy
<span>5</span>:23.0.4-1~ubuntu.22.04~jammy
<span>5</span>:23.0.3-1~ubuntu.22.04~jammy
<span>5</span>:23.0.2-1~ubuntu.22.04~jammy
<span>5</span>:23.0.1-1~ubuntu.22.04~jammy
<span>(</span>...<span>)</span>
</code></pre></div>
<p>I decided to go with the newest one, if you for some reason want to install another, feel free.</p>
<div><pre><span></span><code><span>VERSION_STRING</span><span>=</span><span>5</span>:24.0.6-1~ubuntu.22.04~jammy
sudo<span> </span>apt<span> </span>install<span> </span>docker-ce<span>=</span><span>$VERSION_STRING</span><span> </span>docker-ce-cli<span>=</span><span>$VERSION_STRING</span><span> </span>containerd.io<span> </span>docker-compose-plugin
</code></pre></div>
<p>aaand done. Let's test our docker installation.</p>
<div><pre><span></span><code>docker<span> </span>run<span> </span>hello-world
Unable<span> </span>to<span> </span>find<span> </span>image<span> </span><span>'hello-world:latest'</span><span> </span>locally
latest:<span> </span>Pulling<span> </span>from<span> </span>library/hello-world
719385e32844:<span> </span>Pull<span> </span><span>complete</span>
Digest:<span> </span>sha256:88ec0acaa3ec199d3b7eaf73588f4518c25f9d34f58ce9a0df68429c5af48e8d
Status:<span> </span>Downloaded<span> </span>newer<span> </span>image<span> </span><span>for</span><span> </span>hello-world:latest

Hello<span> </span>from<span> </span>Docker!
This<span> </span>message<span> </span>shows<span> </span>that<span> </span>your<span> </span>installation<span> </span>appears<span> </span>to<span> </span>be<span> </span>working<span> </span>correctly.

To<span> </span>generate<span> </span>this<span> </span>message,<span> </span>Docker<span> </span>took<span> </span>the<span> </span>following<span> </span>steps:
<span> </span><span>1</span>.<span> </span>The<span> </span>Docker<span> </span>client<span> </span>contacted<span> </span>the<span> </span>Docker<span> </span>daemon.
<span> </span><span>2</span>.<span> </span>The<span> </span>Docker<span> </span>daemon<span> </span>pulled<span> </span>the<span> </span><span>"hello-world"</span><span> </span>image<span> </span>from<span> </span>the<span> </span>Docker<span> </span>Hub.
<span>    </span><span>(</span>amd64<span>)</span>
<span> </span><span>3</span>.<span> </span>The<span> </span>Docker<span> </span>daemon<span> </span>created<span> </span>a<span> </span>new<span> </span>container<span> </span>from<span> </span>that<span> </span>image<span> </span>which<span> </span>runs<span> </span>the
<span>    </span>executable<span> </span>that<span> </span>produces<span> </span>the<span> </span>output<span> </span>you<span> </span>are<span> </span>currently<span> </span>reading.
<span> </span><span>4</span>.<span> </span>The<span> </span>Docker<span> </span>daemon<span> </span>streamed<span> </span>that<span> </span>output<span> </span>to<span> </span>the<span> </span>Docker<span> </span>client,<span> </span>which<span> </span>sent<span> </span>it
<span>    </span>to<span> </span>your<span> </span>terminal.

To<span> </span>try<span> </span>something<span> </span>more<span> </span>ambitious,<span> </span>you<span> </span>can<span> </span>run<span> </span>an<span> </span>Ubuntu<span> </span>container<span> </span>with:
<span> </span>$<span> </span>docker<span> </span>run<span> </span>-it<span> </span>ubuntu<span> </span>bash

Share<span> </span>images,<span> </span>automate<span> </span>workflows,<span> </span>and<span> </span>more<span> </span>with<span> </span>a<span> </span>free<span> </span>Docker<span> </span>ID:
<span> </span>https://hub.docker.com/

For<span> </span>more<span> </span>examples<span> </span>and<span> </span>ideas,<span> </span>visit:
<span> </span>https://docs.docker.com/get-started/
</code></pre></div>
<p>Seems to be working. How about <code>docker-compose</code>?</p>
<div><pre><span></span><code>docker-compose
Command<span> </span><span>'docker-compose'</span><span> </span>not<span> </span>found,<span> </span>but<span> </span>can<span> </span>be<span> </span>installed<span> </span>with:
apt<span> </span>install<span> </span>docker-compose
</code></pre></div>
<p>Not there, weird? Nope. Some of you might be still used to the old <code>docker-compose</code> thingy, however some time ago it got moved to be a part of the docker itself, which means that now instead of <code>docker-compose</code> you do:</p>
<div><pre><span></span><code>docker<span> </span>compose

Usage:<span>  </span>docker<span> </span>compose<span> </span><span>[</span>OPTIONS<span>]</span><span> </span>COMMAND

Define<span> </span>and<span> </span>run<span> </span>multi-container<span> </span>applications<span> </span>with<span> </span>Docker.
</code></pre></div>
<p>Alright! We set. Almost.</p>
<p>Currently, if you sshed on a clean server, which I assume you did, you are running as the root user. You can check this by typing:</p>

<p>The problem with that is similar to the situation with our postgres and the almighty postgres user. </p>
<p>Ideally, we do not want to run our containers as root, to prevent attackers from being able to do bad stuff to the whole server. Let's create a new user where we will be running our containers. You can have user per app or service, but not sure if you need that. Just not running on root is usually good enough.</p>
<p>How?</p>
<h3>Running docker on non-user or rootless docker</h3>
<p>That's all quite simple.</p>
<p>We need to create a new user, add it to the sudoers grup, set a password for it and lastly add it to the docker group. In our case we will create a prod user and then add it to the sudo group and docker group.</p>
<div><pre><span></span><code>sudo<span> </span>useradd<span> </span>prod
sudo<span> </span>usermod<span> </span>-aG<span> </span>sudo<span> </span>prod
sudo<span> </span>passwd<span> </span>prod
sudo<span> </span>usermod<span> </span>-aG<span> </span>docker<span> </span>docker
</code></pre></div>
<p>That's quite much it for now.</p>
<h3>Enabling docker containers to connect to host postgres</h3>
<p>The sane way. Some people deal with the issue described before, the one regarding connections from outside of localhost, by allowing <code>*</code> which means any and all networks/ips. This is a NO GO for production, really. The sane way is, as mentioned, to only allow specific networks. In our case docker network. How to do that?</p>
<p>We have to find out what is the local ip address that our docker network got assigned and simply allow traffic from that network to access. Sounds tricky, but ain't.</p>
<p>Now, before we proceed, I'm not that proficient in networking to be frank. Which means that my solution, while working, might not be the ideal one. Happy for feedback from someone more knowledgeable in the topic.</p>
<p>We want to add our docker network to those permitted inside our postgres. This implies we need to know the docker network address. How to get it?</p>
<div><pre><span></span><code>ip<span> </span>addr<span> </span><span>|</span><span> </span>grep<span> </span>docker
<span>3</span>:<span> </span>docker0:<span> </span>&lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt;<span> </span>mtu<span> </span><span>1500</span><span> </span>qdisc<span> </span>noqueue<span> </span>state<span> </span>DOWN<span> </span>group<span> </span>default
<span>    </span>inet<span> </span><span>172</span>.17.0.1/16<span> </span>brd<span> </span><span>172</span>.17.255.255<span> </span>scope<span> </span>global<span> </span>docker0
</code></pre></div>
<p><code>172.17.0.1</code> in this case is the network address we need. It'll probably be the same in your case, but doesn't have to be. Now that we have it, let's move on. How to edit postgres config?</p>
<p>First, my young padawan, we will need to find the location of our postgresql.conf file - which, surprisingly, is the file used to configure postgres.</p>
<p>We can do that with:</p>
<div><pre><span></span><code>sudo<span> </span>find<span> </span>/<span> </span>-type<span> </span>f<span> </span>-name<span> </span>postgresql.conf
/etc/postgresql/16/main/postgresql.conf
</code></pre></div>
<p>In my case it's in <code>/etc/postgresql/16/main/postgresql.conf</code>. The probability of it being the same for you, if you are running ubuntu 22.04 and followed this guide, as the probability of us living in a simulation or being at the beginning of an AI bubble. Get back to the topic Olaf. Gosh.</p>
<p>Okay, we know where the file is, we need to edit it. Type in:</p>
<div><pre><span></span><code>sudo<span> </span>vim<span> </span>/etc/postgresql/16/main/postgresql.conf
</code></pre></div>
<p>and look for <code>listen_addresses</code> part. In vim you can do a search by typing <code>/{phrase}</code> so <code>/listen_addresses</code> should navigate you to the proper line. In my case it looks like this:</p>
<div><pre><span></span><code><span>#</span><span>listen_addresses</span><span> </span><span>=</span><span> </span><span>'</span><span>localhost</span><span>'</span><span>         </span><span>#</span><span> </span><span>what</span><span> </span><span>IP</span><span> </span><span>address</span><span>(</span><span>es</span><span>)</span><span> </span><span>to</span><span> </span><span>listen</span><span> </span><span>on</span><span>;</span>
</code></pre></div>
<p>we need to uncomment the line and edit it so it allows connections from our docker network ip. So:</p>
<div><pre><span></span><code><span>listen_addresses</span><span> </span><span>=</span><span> </span><span>"localhost,172.17.0.1"</span>
</code></pre></div>
<p>then <code>:wq</code> and done.</p>
<p>Now we also need to edit <code>pg_hba.conf</code> to also allow this particular network to acces our database while authenticating with a password.</p>
<p>First let's find it:</p>
<div><pre><span></span><code>sudo<span> </span>find<span> </span>/<span> </span>-type<span> </span>f<span> </span>-name<span> </span>pg_hba.conf
/etc/postgresql/16/main/pg_hba.conf
</code></pre></div>
<p>and edit it with:</p>
<div><pre><span></span><code>sudo<span> </span>vim<span> </span>/etc/postgresql/16/main/pg_hba.conf
</code></pre></div>
<p>now again, navigate to a section containing "IPv4 local connections":</p>
<div><pre><span></span><code><span># IPv4 local connections:</span>
host<span>    </span>all<span>             </span>all<span>             </span><span>127</span>.0.0.1/32<span>            </span>scram-sha-256
<span># IPv6 local connections:</span>
host<span>    </span>all<span>             </span>all<span>             </span>::1/128<span>                 </span>scram-sha-256
</code></pre></div>
<p>we need to edit the IPv4 section to look like this:</p>
<div><pre><span></span><code><span># IPv4 local connections:</span>
host<span>    </span>all<span>             </span>all<span>             </span><span>127</span>.0.0.1/32<span>            </span>scram-sha-256
host<span>    </span>all<span>             </span>all<span>             </span><span>172</span>.17.0.0/16<span>           </span>scram-sha-256
</code></pre></div>
<p>What does the /16 after the netowrk address mean? Match the first 16 bytes of the address, so the 2 first. numbers, rest can change.</p>
<p>Now, let's restart our postgres.</p>
<div><pre><span></span><code>sudo<span> </span>/etc/init.d/postgresql<span> </span>restart
</code></pre></div>
<p>Important note. You might need to add something like this to your docker-compose:</p>
<div><pre><span></span><code><span>    </span><span>extra_hosts</span><span>:</span>
<span>      </span><span>-</span><span> </span><span>"host.docker.internal:172.17.0.1"</span>
</code></pre></div>
<p>For each service that will access it, and edit the connection string for postgres host to be: host.docker.internal. Either that or just use 172.17.0.1 value directly.</p>
<p>With docker and postgres set up, the world is yours to take. But is this it? I wouldn't be myself if i just ended with this. Let's take it up a notch. I mean we usually want to have something in front of our API, some reverse proxy, maybe capability to scale, have multiple replicas and so on. Performance and scaling stuff. Simple solution for that too.</p>
<h2>BUT MUH SCALABILITY, LOAD BALANCING, SSL and whatnot</h2>
<p>Ye I hear you, all the folks with 1k monthly active users, serving up to 2 requests per second, usually screaming about scalability the loudest. WHERE"S KUBERNETES? WHERE"S MY CLOUD SCALING STUFF, REEE!!one! AND THE DEVOPS TEAM? ARE YOU A FOOL?</p>
<p>I'll give you some love too, fret not.</p>
<p>For the database, the case is simple. With a dedicated bare metal server that I recommend you get, unless you do some horrendeous things in the schema or query, well, you can handle TONS of data &amp; traffic on a single machine.With just one instance of $200 ARM Hetzner with 80 dedicated cores, 128 GB of RAM, 2TB NVME PCIe SSD, how much more do you need in most cases? </p>
<p>Yeah, availability zones and so on, but let's take a step back. How many of you are truly running multi region &amp; multi availability zones DB deployments? HMMM? Thought so. Sorry to break it to you, but hype/conference driven development isn't the only way to go. I'd argue that at least 90-95% of current startups could probably run just fine with this single instance only. Okay, maybe some of you would need something like S3 (Cloudflare R2 maybe?). Outgrowing this setup will probably mean you already got enough tracton, customers and money to actually start your own colocation thingy with a dedicated team. Backups? Survavibility? We'll talk about that part later.</p>
<p>So, in this post, we won't cover how to horizontally scale the db, as I think it's simply not needed for the audience i target this too. What is needed though, is probably replication of the apis/scaling them and then reverse proxy/managing ssl/load balancing. That's what we will do. Let's start with replicating our api to an arbitrary size. How can we do that?</p>
<p>Docker-compose lol.</p>
<h2>Ditch Kubernetes, docker compose for the win</h2>
<p>This part will be quite short, sweet and simple. Probably not many of you know, but docker compose supports replication out of the box. Why wouldn't it. How do we go about it?</p>
<div><pre><span></span><code><span>  </span><span>api</span><span>:</span>
<span>    </span><span>build</span><span>:</span>
<span>      </span><span>context</span><span>:</span><span> </span><span>.</span>
<span>    </span><span>depends_on</span><span>:</span>
<span>      </span><span>database</span><span>:</span>
<span>        </span><span>condition</span><span>:</span><span> </span><span>service_healthy</span>
<span>    </span><span>restart</span><span>:</span><span> </span><span>always</span>
<span>    </span><span>deploy</span><span>:</span>
<span>      </span><span>replicas</span><span>:</span><span> </span><span>4</span>
<span>    </span><span>ports</span><span>:</span>
<span>      </span><span>-</span><span> </span><span>"8000-8003:8000"</span>
</code></pre></div>
<p>the key part here being:</p>
<div><pre><span></span><code><span>    </span><span>deploy</span><span>:</span>
<span>      </span><span>replicas</span><span>:</span><span> </span><span>4</span>
<span>    </span><span>ports</span><span>:</span>
<span>      </span><span>-</span><span> </span><span>"8000-8003:8000"</span>
</code></pre></div>
<p>after that, just do <code>docker compose up</code>. And then boom. You done. Multiple replicas of your api service up and running. With 4 lines of code, 2 of them you already probably have in your code.</p>
<p>Remember what we said - we do not want to run docker as root, so ssh/login into the user we created <code>prod</code> for this purpose.</p>
<p>once you there, just clone the repo and docker compose up.</p>
<p>You'll be amazed how fast the deployments can happen. Also about the secrets. 1password offers some nice options here for such use cases, or in fact, you can even just create .env file, specify it in the docker-compose and be done with it.</p>
<p>Logs can be easily checked with a simple <code>docker compose logs</code> + docker saves them to a file iether way.</p>
<p>But, what about load balancing, reverse proxy and ssl stuff? </p>
<h2>Load Balancing &amp; automatic ssl with Caddyserver</h2>
<p>We will use caddyserver to act as a reverse proxy, load balancer and to automatically take care of the certificates for us. It's a bit less performant than nginx, true, but the ease of use and convenience it provides is well worth it. That plus usually it's not the proxy that will die first. Quite the opposite.</p>
<p>So how do we go about this? Probably complicated? Nope.</p>
<p>We will let ansible handle all the work for us. Ansible? Yes, you read that right. Not terraform.</p>
<p>In order to do that we will need to create a new user for our ansible to run on, enable ssh access and do a bit of ansible dev. Let's go. You already know the drill.</p>
<div><pre><span></span><code>sudo<span> </span>useradd<span> </span>ingres
sudo<span> </span>passwd<span> </span>ingres
sudo<span> </span>usermod<span> </span>-aG<span> </span>sudo<span> </span>ingres
</code></pre></div>
<p>Now, a small change to what we did before.</p>
<p>We make sure our user can do sudo operations without password. How?</p>

<p>then find this piece:</p>
<div><pre><span></span><code><span># User privilege specification</span>
root<span>    </span><span>ALL</span><span>=(</span>ALL:ALL<span>)</span><span> </span>ALL
</code></pre></div>
<p>and add this below (or at the bottom of the file):</p>
<div><pre><span></span><code>ingres<span> </span><span>ALL</span><span>=(</span>ALL<span>)</span><span> </span>NOPASSWD:<span> </span>ALL
</code></pre></div>
<p>We could be more granular about permissions here and what it has access to, but that could come in a 2nd iteration, I consider this good enough.</p>
<p>Let's enable SSH access now. This might not be needed on your machine, depends on the server. I had to do it on my hetzner.</p>
<p>We need to edit the sshd_config file. How to find where it is? You should know by now ;) </p>

<p>and find something like this:</p>
<div><pre><span></span><code><span>#AuthorizedKeysFile      .ssh/authorized_keys</span>
</code></pre></div>
<p>turn it into:</p>
<div><pre><span></span><code>AuthorizedKeysFile<span>      </span>.ssh/authorized_keys
AllowUsers<span> </span>root<span> </span>prod<span> </span>dev
</code></pre></div>
<p>add your ssh key to <code>/home/ingres/.ssh/authorized_keys</code> in order to do that and eg. add the same ssh key you use for your root account (not ideal):</p>
<p>```bashand lastly:
su ingres # we switch the user to make it the owner of the directory we create
mkdir -p /home/ingres/.ssh
cat ~/.ssh/authorized_keys &gt; /home/ingres/.ssh/authorized_keys</p>
<div><pre><span></span><code>aaand lastly:

```bash
service sshd restart
</code></pre></div>
<p>Setup done. Time to install caddy with ansible. But before that, we need to setup ansible.</p>
<p>I'll assume you have pyenv installed and set up running on your local machine. You can read about that <a href="https://grski.pl/pyenv-en">here</a> in my article, or <a href="https://grski.pl/pdf-brag">here</a>.</p>
<p>With that we can:</p>
<div><pre><span></span><code>pyenv<span> </span>virtualenv<span> </span><span>3</span>.11<span> </span>infrastructure-deployment-3-11
mkdir<span> </span>infrastructure-deployment
<span>cd</span><span> </span>infrastructure-deployment
pyenv<span> </span><span>local</span><span> </span>infrastructure-deployment-3-11
python<span> </span>-m<span> </span>pip<span> </span>install<span> </span>ansible
</code></pre></div>
<p>Pyenv set up. Ansible set up. We will need one more thing - install custom role from ansible galaxy.</p>
<div><pre><span></span><code>python<span> </span>-m<span> </span>ansible<span> </span>galaxy<span> </span>role<span> </span>install<span> </span>caddy_ansible.caddy_ansible<span>  </span>
</code></pre></div>
<p>Now inside our <code>infrastructure-deployment</code> directory on our local machine create a file called <code>inventory.yml</code></p>
<div><pre><span></span><code><span>all</span><span>:</span>
<span>  </span><span>hosts</span><span>:</span>
<span>    </span><span>bare-metal-hetzner</span><span>:</span>
<span>      </span><span>ansible_host</span><span>:</span><span> </span><span>"your-host-ip"</span>
<span>      </span><span>ansible_user</span><span>:</span><span> </span><span>"ingres"</span>
<span>      </span><span>ansible_port</span><span>:</span><span> </span><span>22</span>
</code></pre></div>
<p>aaand <code>caddy_install.yml</code>:</p>
<div><pre><span></span><code><span>---</span>
<span>-</span><span> </span><span>name</span><span>:</span><span> </span><span>Install Caddy Server</span>
<span>  </span><span>hosts</span><span>:</span><span> </span><span>all</span>
<span>  </span><span>become</span><span>:</span><span> </span><span>true</span>
<span>  </span><span>roles</span><span>:</span>
<span>     </span><span>-</span><span> </span><span>role</span><span>:</span><span> </span><span>caddy_ansible.caddy_ansible</span>
<span>       </span><span>caddy_conf_filename</span><span>:</span><span> </span><span>Caddyfile</span>
<span>       </span><span>caddy_update</span><span>:</span><span> </span><span>true</span>
<span>       </span><span>caddy_systemd_capabilities_enabled</span><span>:</span><span> </span><span>true</span>
<span>       </span><span>caddy_systemd_capabilities</span><span>:</span><span> </span><span>"CAP_NET_BIND_SERVICE"</span>
<span>       </span><span>caddy_config</span><span>:</span><span> </span><span>|</span>
<span>        </span><span>your-fancy-startup-domain.com {                 </span>
<span>          </span><span># Compress responses according to Accept-Encoding headers</span>
<span>          </span><span>encode gzip zstd</span>

<span>          </span><span># Send API requests to backend</span>
<span>          </span><span>reverse_proxy 127.0.0.1:8000 127.0.0.1:8301 127.0.0.1:8302 127.0.0.1:8303</span>
<span>        </span><span>}</span>
</code></pre></div>
<p>run </p>
<div><pre><span></span><code>python<span> </span>-m<span> </span>ansible<span> </span>playbook<span> </span>-i<span> </span>inventory.yml<span> </span>caddy_install.yml<span>   </span>
</code></pre></div>
<p>aaand done.</p>
<p>Now if you go to your-fancy-startup-domain.com, given that proper docker containers are running, you'll get them.</p>
<p>Automatic SSL. Automatic load balancing. EVERYTHING WORKS.</p>
<h2>BACKUPS, SURVAVIBILITY</h2>
<p>You can have hourly backups with BorgBackup. How? Brilliant tutorial can be found in <a href="https://community.hetzner.com/tutorials/install-and-configure-borgbackup">hetzner docs</a>. Go read them.</p>
<p>On top of that add $4 1 TB <a href="https://www.hetzner.com/storage/storage-box">Hetzner Storage Box</a> linked to your server. Boom. Done. You might want to think about adding pg_dump, but IMO just the BorgBackup for starters is ok.</p>
<p>My borg-backup script looks more or less like this:</p>
<div><pre><span></span><code><span>    </span><span>#!/bin/sh</span>
<span># First init the repo</span>
<span># ssh -p23 ssh://xxxxx.your-storagebox.de mkdir /home/backup</span>
<span># ssh -p23 ssh://<a href="https://grski.pl/cdn-cgi/l/email-protection" data-cfemail="4830303030300830303030306631273d3a653b3c273a292f2d2a2730662c2d">[email&nbsp;protected]</a> mkdir /home/backup/main</span>
<span># borg init --encryption=repokey ssh://<a href="https://grski.pl/cdn-cgi/l/email-protection" data-cfemail="a6dededededee6dedededede88dfc9d3d48bd5d2c9d4c7c1c3c4c9de88c2c3">[email&nbsp;protected]</a>:23/~/backup/main</span>
<span># Setting this, so the repo does not need to be given on the commandline:</span>
<span>export</span><span> </span><span>BORG_REPO</span><span>=</span>ssh://<a href="https://grski.pl/cdn-cgi/l/email-protection" data-cfemail="245c645c0a5d4b51560957504b56454341464b5c0a4041">[email&nbsp;protected]</a>:23/~/backup/main

<span># See the section "Passphrase notes" for more infos.</span>
<span>export</span><span> </span><span>BORG_PASSPHRASE</span><span>=</span>

<span># some helpers and error handling:</span>
info<span>()</span><span> </span><span>{</span><span> </span><span>printf</span><span> </span><span>"\n%s %s\n\n"</span><span> </span><span>"</span><span>$(</span><span> </span>date<span> </span><span>)</span><span>"</span><span> </span><span>"</span><span>$*</span><span>"</span><span> </span>&gt;<span>&amp;</span><span>2</span><span>;</span><span> </span><span>}</span>
<span>trap</span><span> </span><span>'echo $( date ) Backup interrupted &gt;&amp;2; exit 2'</span><span> </span>INT<span> </span>TERM

info<span> </span><span>"Starting backup"</span>

<span># Backup the most important directories into an archive named after</span>
<span># the machine this script is currently running on:</span>

borg<span> </span>create<span>                         </span><span>\</span>
<span>    </span>--verbose<span>                       </span><span>\</span>
<span>    </span>--filter<span> </span>AME<span>                    </span><span>\</span>
<span>    </span>--list<span>                          </span><span>\</span>
<span>    </span>--stats<span>                         </span><span>\</span>
<span>    </span>--show-rc<span>                       </span><span>\</span>
<span>    </span>--compression<span> </span>lz4<span>               </span><span>\</span>
<span>    </span>--exclude-caches<span>                </span><span>\</span>
<span>    </span>--exclude<span> </span><span>'home/*/.cache/*'</span><span>     </span><span>\</span>
<span>    </span>--exclude<span> </span><span>'var/tmp/*'</span><span>           </span><span>\</span>
<span>    </span>--exclude<span> </span><span>'*__pycache__*'</span><span>       </span><span>\</span>
<span>    </span>--exclude<span> </span><span>'*.pyenv*'</span><span>            </span><span>\</span>
<span>                                    </span><span>\</span>
<span>    </span>::<span>'{hostname}-{now}'</span><span>            </span><span>\</span>
<span>    </span>/etc<span>                            </span><span>\</span>
<span>    </span>/home<span>                           </span><span>\</span>
<span>    </span>/root<span>                           </span><span>\</span>
<span>    </span>/var

<span>backup_exit</span><span>=</span><span>$?</span>

info<span> </span><span>"Pruning repository"</span>

<span># Use the `prune` subcommand to maintain 7 daily, 4 weekly and 6 monthly</span>
<span># archives of THIS machine. The '{hostname}-*' matching is very important to</span>
<span># limit prune's operation to this machine's archives and not apply to</span>
<span># other machines' archives also:</span>

borg<span> </span>prune<span>                          </span><span>\</span>
<span>    </span>--list<span>                          </span><span>\</span>
<span>    </span>--glob-archives<span> </span><span>'{hostname}-*'</span><span>  </span><span>\</span>
<span>    </span>--show-rc<span>                       </span><span>\</span>
<span>    </span>--keep-daily<span>    </span><span>7</span><span>               </span><span>\</span>
<span>    </span>--keep-weekly<span>   </span><span>4</span><span>               </span><span>\</span>
<span>    </span>--keep-monthly<span>  </span><span>6</span>

<span>prune_exit</span><span>=</span><span>$?</span>

<span># actually free repo disk space by compacting segments</span>

info<span> </span><span>"Compacting repository"</span>

borg<span> </span>compact

<span>compact_exit</span><span>=</span><span>$?</span>

<span># use highest exit code as global exit code</span>
<span>global_exit</span><span>=</span><span>$((</span><span> </span><span>backup_exit</span><span> </span><span>&gt;</span><span> </span><span>prune_exit</span><span> </span>?<span> </span><span>backup_exit</span><span> </span>:<span> </span><span>prune_exit</span><span> </span><span>))</span>
<span>global_exit</span><span>=</span><span>$((</span><span> </span><span>compact_exit</span><span> </span><span>&gt;</span><span> </span><span>global_exit</span><span> </span>?<span> </span><span>compact_exit</span><span> </span>:<span> </span><span>global_exit</span><span> </span><span>))</span>

<span>if</span><span> </span><span>[</span><span> </span><span>${</span><span>global_exit</span><span>}</span><span> </span>-eq<span> </span><span>0</span><span> </span><span>]</span><span>;</span><span> </span><span>then</span>
<span>    </span>info<span> </span><span>"Backup, Prune, and Compact finished successfully"</span>
<span>elif</span><span> </span><span>[</span><span> </span><span>${</span><span>global_exit</span><span>}</span><span> </span>-eq<span> </span><span>1</span><span> </span><span>]</span><span>;</span><span> </span><span>then</span>
<span>    </span>info<span> </span><span>"Backup, Prune, and/or Compact finished with warnings"</span>
<span>else</span>
<span>    </span>info<span> </span><span>"Backup, Prune, and/or Compact finished with errors"</span>
<span>fi</span>

<span>exit</span><span> </span><span>${</span><span>global_exit</span><span>}</span>
</code></pre></div>
<p>To run it periodically type in <code>crontab -e</code></p>
<p>and then</p>
<div><pre><span></span><code><span>00</span><span> </span><span>2</span><span> </span>*<span> </span>*<span> </span>*<span> </span>/root/borg-backup.sh
</code></pre></div>
<h2>Potential improvements</h2>
<p>Ofcourse the permissions here and there could be more fine-grained for sure. </p>
<p>We could also add a bastion in front of the server. </p>
<p>Automate the deployment so that after each merge stuff gets built &amp; deployed. </p>
<p>Add monitoring, observability, alerts. (Ain't that hard tbh, we will explore that one day)</p>
<p>There's much more than that ofcourse but these are the starters.</p>
<h2>Summary</h2>
<p>We have set up: </p>
<ol>
<li>self-hosted postgres instance with passable initail configuration</li>
<li>replicated api-service with as many replicas as we want</li>
<li>proper load balancing and reverse proxy in front of them</li>
<li>https everywhere</li>
<li>proper certifcates, all handled automatically</li>
<li>1 click deployment of our reverse proxy</li>
<li>blazing fast deployments/build times in the future (for now manual, but can easily be automated)</li>
<li>ability to potentially handle hundreds of thousands of users</li>
<li>very predictable cost &amp; performance</li>
<li>regular FULL backups</li>
<li>no additional deployment code</li>
<li>Absolutely stunning performance with 80 dedicated cores, 128 gb of ram, 2 TB NVMe SSD (you'd be amazed)</li>
</ol>
<p>What more can I say. Cloud IS NOT the solution for everything. Sometimes you can try the alternative path.</p>
<p>Similar setup on AWS would be probably $6-10k upwards just for the postgres. That plus it wouldn't match the performance we have here. One thing not covered here is how much performance you gain when all the services are within one network. No calls outside your network == blazing fast shit.</p>
<p>All of this in 15 minutes and for $200 monthly. </p>
<p>Want some copium cloud bro? </p>
<p>Ofcourse this doesn't adhere to some of you and your companies, but you know that. I've simplified lots of things or generlised. However, for the general public and their needs, I think it's worth rethink the whole cloud sometimes.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Doomscroller.xyz (312 pts)]]></title>
            <link>https://doomscroller.xyz/</link>
            <guid>40098178</guid>
            <pubDate>Sat, 20 Apr 2024 15:40:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doomscroller.xyz/">https://doomscroller.xyz/</a>, See on <a href="https://news.ycombinator.com/item?id=40098178">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <center>
      <p><img src="https://doomscroller.xyz/static/Header.PNG" alt="Header Image">
    </p>
    <!-- <p class="logo-text">Doomscroller</p> -->

    <div>
        <div>
            <p>What does it do???   </p>
            <p>this and <br> only this.  </p>
            <p><img src="https://doomscroller.xyz/static/arrow_blue.png" alt="">
            <img src="https://doomscroller.xyz/static/wheel.png" alt="Wheel"></p>

          <div>
                 
           
      
                     
            <stripe-buy-button buy-button-id="buy_btn_1P5Y1328PU2Wr5nx37n76Me7" publishable-key="pk_live_QHHTMsoPgQZg6Oz7JujHn8pa">
          </stripe-buy-button>


            <stripe-buy-button buy-button-id="buy_btn_1P5Xx628PU2Wr5nxO5bFsP22" publishable-key="pk_live_QHHTMsoPgQZg6Oz7JujHn8pa">
            </stripe-buy-button>
        </div>

      
          <div>
            <p><b>Disclaimers:</b></p><ul>
              <li>Shipping in ~2 weeks</li>
              <li>Only works with Android/PC. Sorry iOS</li>
              <li>Code on github</li>
              <li>Fairly sure the battery won't explode</li>
            </ul>
          </div>
          
      </div>
        <div>
          <p><a data-width="350" data-theme="dark" data-min-width="300" data-max-width="600" href="https://twitter.com/AndrewMcCalip?ref_src=twsrc%5Etfw">Timeline</a>
        </p></div>
    </div>

</center></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub comments abused to push malware via Microsoft repo URLs (106 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/github-comments-abused-to-push-malware-via-microsoft-repo-urls/</link>
            <guid>40097818</guid>
            <pubDate>Sat, 20 Apr 2024 14:55:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/github-comments-abused-to-push-malware-via-microsoft-repo-urls/">https://www.bleepingcomputer.com/news/security/github-comments-abused-to-push-malware-via-microsoft-repo-urls/</a>, See on <a href="https://news.ycombinator.com/item?id=40097818">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="GitHub" height="900" src="https://www.bleepstatic.com/content/hl-images/2021/05/10/GitHub-headpic.jpg" width="1804"></p>
<p>A GitHub flaw, or possibly a design decision, is being abused by threat actors to distribute malware using URLs associated with a Microsoft repository, making the files appear trustworthy.</p>
<p>While most of the malware activity has been based around the Microsoft GitHub URLs, this "flaw" could be abused with any public repository on GitHub, allowing threat actors to create very convincing lures.</p>
<h2>Abusing GitHub's file upload feature</h2>
<p>Yesterday, McAfee released a report on a <a href="https://www.bleepingcomputer.com/news/security/fake-cheat-lures-gamers-into-spreading-infostealer-malware/" target="_blank">new LUA malware loader</a> distributed through what appeared to be a legitimate Microsoft GitHub repository for the "C++ Library Manager for Windows, Linux, and MacOS," known as <a href="https://github.com/microsoft/vcpkg" target="_blank" rel="nofollow noopener">vcpkg</a>.</p>
<p>The URLs for the malware installers, shown below, clearly indicate that they belong to the Microsoft repo, but we could not find any reference to the files in the project's source code.</p>
<pre><code>https://github[.]com/microsoft/vcpkg/files/14125503/Cheat.Lab.2.7.2.zip
https://github[.]com/microsoft/STL/files/14432565/Cheater.Pro.1.6.0.zip
</code></pre>
<p>Finding it strange that a Microsoft repo would be <a href="http://urlhaus.abuse.ch/url/2760438/" target="_blank" rel="nofollow noopener">distributing malware since February</a>, BleepingComputer looked into it and found that the files are not part of <em>vcpkg</em> but were uploaded as part of a comment left on a commit or issue in the project.</p>
<p>When leaving a comment, a GitHub user can attach a file, which will be uploaded to GitHub's CDN and associated with the related project using a unique URL in this format: '<em>https://www.github.com/{project_user}/{repo_name}/files/{file_id}/{file_name}.</em>'</p>
<p>Instead of generating the URL after a comment is posted, GitHub automatically generates the download link after you add the file to an unsaved comment, as shown below. This allows threat actors to attach their malware to any repository without them knowing.</p>
<div>
<figure><img alt="Download link auto-generated when adding a file to a comment" height="300" src="https://www.bleepstatic.com/images/news/security/g/github/github-file-uploads/github-comment-file-upload.jpg" width="795"><figcaption><strong>Download link auto-generated when adding a file to a comment</strong><br><em>Source: BleepingComputer</em></figcaption></figure></div>
<p>Even if you decide not to post the comment or delete it after it is posted, the files are not deleted from GitHub's CDN, and the download URLs continue to work forever.</p>
<p>As the file's URL contains the name of the repository the comment was created in, and as almost every software company uses GitHub, this flaw can allow threat actors to develop extraordinarily crafty and trustworthy lures.</p>
<p>For example, a threat actor could upload a malware executable in <a href="https://github.com/NVIDIA/nvidia-installer" target="_blank" rel="nofollow noopener">NVIDIA's driver installer repo</a> that pretends to be a new driver fixing issues in a popular game. Or a threat actor could upload a file in a comment to the <a href="https://github.com/chromium/chromium" target="_blank" rel="nofollow noopener">Google Chromium source code</a> and pretend it's a new test version of the web browser.</p>
<p>These URLs would also appear to belong to the company's repositories, making them far more trustworthy.</p>
<p>Unfortunately, even if a company learns their repos are abused to distribute malware, BleepingComputer could not find any settings that allow you to manage files attached to your projects.</p>
<p>Furthermore, you can only protect a GitHub account from being abused in this way and tarnishing your reputation by disabling comments. According to this <a href="https://docs.github.com/en/communities/moderating-comments-and-conversations/limiting-interactions-in-your-repository" target="_blank" rel="nofollow noopener">GitHub support document</a>, you can only temporarily disable comments for a maximum of six months at a time.</p>
<p>However, restricting comments can significantly impact a project's development as it will not allow users to report bugs or suggestions.</p>
<p>Sergei Frankoff, of automated malware analysis service UNPACME, did a livestream on Twitch about this bug just last month, saying that threat actors were actively abusing it.</p>
<blockquote>
<p dir="ltr" lang="en">Weeks later... GitHub bug still dropping malware <a href="https://t.co/s165zOAsoI" rel="nofollow noopener">pic.twitter.com/s165zOAsoI</a></p>
— herrcore (@herrcore) <a href="https://twitter.com/herrcore/status/1772988192678969567?ref_src=twsrc%5Etfw" rel="nofollow noopener">March 27, 2024</a></blockquote>
<p>As part of our research into this bug, BleepingComputer could only find one other repo, <a href="https://urlhaus.abuse.ch/url/2780254/" target="_blank" rel="nofollow noopener">httprouter</a>, abused to distribute malware in this way, and it was the same 'Cheater.Pro.1.6.0.zip' as seen in Microsoft's URLs.</p>
<p>However, Frankoff told BleepingComputer that they <a href="https://research.openanalysis.net/github/lua/2024/03/03/lua-malware.html" target="_blank" rel="nofollow noopener">discovered a similar campaign in March</a> that utilizes the same LUA loader malware, which is called <a href="https://www.unpac.me/results/f3a0a729-afcf-4209-9323-fbf470be2835#/" target="_blank" rel="nofollow noopener">SmartLoader</a>, disguised as the Aimmy cheat software.</p>
<p>Frankoff told BleepingComputer that SmartLoader is commonly installed alongside other payloads, such as the RedLine information-stealing malware.</p>
<p>BleepingComputer contacted both GitHub and Microsoft on Thursday about this abuse but did not receive a response.</p>
<p>At the time of this publication, the information-stealing malware is still being distributed through links associated with Microsoft' GitHub repository.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U-M finds students with alphabetically lower-ranked names receive lower grades (350 pts)]]></title>
            <link>https://record.umich.edu/articles/study-alphabetical-order-of-surnames-may-affect-grading/</link>
            <guid>40097375</guid>
            <pubDate>Sat, 20 Apr 2024 13:53:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://record.umich.edu/articles/study-alphabetical-order-of-surnames-may-affect-grading/">https://record.umich.edu/articles/study-alphabetical-order-of-surnames-may-affect-grading/</a>, See on <a href="https://news.ycombinator.com/item?id=40097375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
<p>Knowing your ABCs is essential to academic success, but having a last name starting with A, B or C might also help make the grade.</p>



<p>An analysis by University of Michigan researchers of more than 30 million grading records from U-M finds students with alphabetically lower-ranked names receive lower grades. This is due to sequential grading biases and the default order of students’ submissions in Canvas — the most widely used online learning management system — which is based on alphabetical rank of their surnames.</p>





<p>What’s more, the researchers found, those alphabetically disadvantaged students receive comments that are notably more negative and less polite, and exhibit lower grading quality measured by post-grade complaints from students.</p>



<p>“We spend a lot of time thinking about how to make the grading fair and accurate but even for me it was really surprising,” said Jun Li, associate professor of technology and operations at the Stephen M. Ross School of Business. “It didn’t occur to us until we looked at the data and realized that sequence makes a difference.”</p>



<p>Li co-authored the study with doctoral students Jiaxin Pei from the School of Information and Helen (Zhihan) Wang from Ross. It is under review by the journal Management Science.</p>



<p>The researchers collected available historical data of all programs, students and assignments on Canvas from the fall 2014 semester to the summer 2022 semester. They supplemented the Canvas data with university registrar data, which contains detailed information about students’ backgrounds, demographics and learning trajectories at the university.</p>



<figure><p>
<iframe width="500" height="281" src="https://www.youtube.com/embed/Dxdq1xXFi_M?feature=oembed&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="U-M Researchers find a correlation between student grades and alphabetical ordering of last names"></iframe>
</p></figure>



<p>Although the data is from U-M, the researchers say the findings can be generalized across institutions and courses. They are driven by a common design issue of learning-management systems — the default setting of ranking students’ assignments alphabetically by their names.</p>



<p>Their research uncovered a clear pattern of a decline in grading quality as graders evaluate more assignments. Wang said students whose surnames start with A, B, C, D or E received a 0.3-point higher grade out of 100 possible points than compared with when they were graded randomly. Likewise, students with later-in-the-alphabet surnames received a 0.3-point lower grade — creating a 0.6-point gap.</p>



<p>Wang noted that for a small group of graders (about 5%) that grade from Z to A, the grade gap flips as expected: A-E students are worse off, while W-Z students receive higher grades relative to what they would receive when graded randomly. The researchers said such observations confirm their hypothesis that it’s the order of grading that leads to the initial gap in grades.</p>



<p>A 0.6-point difference might seem small, but such a disparity did affect students’ course grade-point averages, which negatively influences opportunities in their respective career paths.</p>



<p>“Our conclusion is this may be something that happened unconsciously by the graders that’s actually creating a real social impact,” Wang said.</p>



<p>Pei said the idea for the study came up during a discussion he had with Wang in which they were talking about their research: Wang studies educational technology and he studies artificial intelligence. He observed that a fundamental task of machine learning is data labeling, also a sequential task that can be long and tedious, but one that is randomized.</p>



<p>It got them thinking about educational systems like Canvas and led to some pilot studies to see if there was any disparity among grades based on the amount of time spent in the task of grading.</p>



<p>“We kind of suspect that fatigue is one of the major factors that is driving this effect, because when you’re working on something for a long period of time, you get tired and then you start to lose your attention and your cognitive abilities are dropping,” Pei said.</p>



<p>The researchers note the option exists to grade the assignments in a random order, and some educators do, but alphabetical order is the default mode in Canvas and other online learning-management systems. One simple fix would be to make random order the default setting.</p>



<p>They also suggest academic institutions could hire more graders for larger classes, distribute the workload among more people or train them to be aware of and lessen the bias while grading.</p>



<p>Li, Wang and Pei have been sharing their research at conferences and it’s been positively received — many are impressed by their work although it confirms suspicions many harbor. One reaction in particular stands out to Li, no doubt an information-age wrinkle on “the dog ate my homework” excuse.</p>



<p>“A college student emailed us afterward asking us to share the paper with him,” she said. “He mentioned that his last name started with W. He’s going to tell his parents it’s not because of him — it’s because of his last name.”</p>
                    
                                        <dl>
                        <dt>Tags:</dt>
                        <dd>
                            <ul>
                                                                <li><a href="https://record.umich.edu/tags/canvas/">Canvas</a></li>
                                                                <li><a href="https://record.umich.edu/tags/grading/">grading</a></li>
                                                                <li><a href="https://record.umich.edu/tags/school-of-information/">School of Information</a></li>
                                                                <li><a href="https://record.umich.edu/tags/stephen-m-ross-school-of-business/">Stephen M. Ross School of Business</a></li>
                                                            </ul>
                        </dd>
                    </dl>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debugging the Doctor Brain: Who's teaching doctors how to think? (103 pts)]]></title>
            <link>https://bessstillman.substack.com/p/debugging-the-doctor-brain</link>
            <guid>40097111</guid>
            <pubDate>Sat, 20 Apr 2024 13:10:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bessstillman.substack.com/p/debugging-the-doctor-brain">https://bessstillman.substack.com/p/debugging-the-doctor-brain</a>, See on <a href="https://news.ycombinator.com/item?id=40097111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><strong>I walked into the hospital room of “Gladys,” 54-year-old woman</strong><span> who, like practically a quarter of ER patients, had belly pain,</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-143339099" href="https://bessstillman.substack.com/p/debugging-the-doctor-brain#footnote-1-143339099" target="_self" rel="">1</a></span><span> but something didn’t seem right. I’m not trying to be deliberately, annoyingly vague: something I couldn’t articulate kept me in her room.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-143339099" href="https://bessstillman.substack.com/p/debugging-the-doctor-brain#footnote-2-143339099" target="_self" rel="">2</a></span><span>Her vital signs looked normal and she wasn’t in terrible distress. Yet I loitered despite the relentless administrative pressure to see more patients, faster. </span></p><p>Gladys and I were chatting about her impending grandchild when, in less time than it takes you to read this sentence, Gladys sat up, vomited a huge quantity of blood, her blood pressure bottomed out, and she lost consciousness. I had to intubate, attempt a balloon tamponade, and initiate massive transfusion protocol—quickly. The ER team was able to stabilize her—barely. Luckily, I was already in the room.</p><p>Textbooks can’t describe and standardized tests can’t detect gestalt. Or, as Gen Z would say, “vibes.” A lot of ER docs like “spidey-sense”—the tingle that whispers: “order a CT scan to rule out a life-threatening diagnosis,” though the patient’s symptoms don’t exactly indicate it. Gestalt is built and honed by seeing thousands of patients. Do emergency medicine for 80 hours a week for three to four years —the length of an ER residency—and a resident doctor will have spent around 10,000 hours on direct patient care. It’s during those encounters that doctors are (supposed to be) guided towards developing and deepening the fundamental mental models that run in their cognitive background while evaluating each new patient.</p><p>But how does a doctor know if their models are accurate or adequate?</p><p><span>Answering that question really means asking if the education student doctors receive is both adequately teaching the fundamentals as well as teaching resident physicians how to think about and evaluate their own thought processes. And Dan Luu’s “</span><em><a href="https://danluu.com/teach-debugging/" rel="">Why don’t schools teach debugging</a></em><span>” got me thinking about the way science and medical education universally teaches the fundamentals: badly.</span></p><blockquote><p><span>When I suggested to the professor</span><sup> </sup><span>that he spend half an hour reviewing algebra for those students who never had the material covered cogently in high school, I was told in no uncertain terms that it would be a waste of time because some people just can't hack it in engineering. I was told that I wouldn't be so naive once the semester was done, because some people just can't hack it in engineering. I was told that helping students with remedial material was doing them no favors; they wouldn't be able to handle advanced courses anyway because some students just can't hack it in engineering. I was told that Purdue has a loose admissions policy and that I should expect a high failure rate, because some students just can't hack it in engineering.</span></p></blockquote><p><span>There seems to be a mass delusion in the sciences that someone—not you, but someone—must have, or at least should have already, taught a student the fundamentals by the time they get in front of you, so that you can focus on the interesting, juicy, complex conversations, presumably with the “smart” people who already get it, the smart people who can </span><em>hack it.</em><span> But most of the people who get it had to get it somewhere</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-143339099" href="https://bessstillman.substack.com/p/debugging-the-doctor-brain#footnote-3-143339099" target="_self" rel="">3</a></span><span>—why shouldn’t that somewhere be with you?</span></p><p data-attrs="{&quot;url&quot;:&quot;https://bessstillman.substack.com/p/debugging-the-doctor-brain?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://bessstillman.substack.com/p/debugging-the-doctor-brain?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><strong>In medicine, we often mistake the speed of initial understanding</strong><span> with a students’ capacity for mastery. This expectation starts in pre-med courses. Organic chemistry (“O-chem”) is the big pre-med “weed out” course because it both requires high-volume memorization and is one of the first times students have to learn a new way of thinking. In general chemistry, basic chemical equations are algebraic. The process of balancing two sides of an equation works off a mathematical model most students see in high school. O-chem, however, demands that you think in the language O-chem provides, which is a long list of various chemical reactions whose effects you memorize like vocabulary, and then you have to figure out how to use them to solve the puzzle of turning one chemical into another. You memorize the facts, then have to think in a new way to understand the fundamentals. You can’t just regurgitate. O-chem problems, much like complex patients, have multiple solutions. That’s why it’s hard.</span></p><p>Learning the language of chemistry to talk about chemistry resembles the way doctors learn the language of the body so they can think about the body. That’s the only logical reason, apart from a de-facto IQ test, I can come up with for having to take organic chemistry, because there’s no practical utility for O-chem in day-to-day doctoring. I suspects it’s also a test—one of many— to see if we’re willing to torture ourselves and jump through hoops. Extra points to anyone who asks “How high?” Masochism is heavily selected for in medical students, which is an essay topic for another time.</p><p>My O-Chem teacher was a lot like Luu’s engineering prof, who assumed that “hacking it” had more to do with inherent student capabilities rather than the quality of the teaching. On the first day of class, he announced that a quarter of the class would drop by the end of the month, and half of what remained would get a C or lower in a non-curved course. It seems like a sign of a lazy educator to announced he wouldn’t be capable of adequately explaining the material to students. Was half the room too stupid or lazy to understand? “Too stupid to understand” doesn’t usually sign up to be 25% of a 315 person O-Chem class at 7:30 a.m.</p><p><span>I had a really hard time getting it. Although the professor had office hours three times a week, showing up to them meant trading his ire for his help, which I (and many others) did. He said that, if I was having so much trouble understanding early on, I should leave, because some people won’t get it. Instead, I found an outside tutor who helped me understand how to approach problems, and change to a less linear way of thinking, and O-chem finally clicked. It was as if I was able to take all the words I’d memorized and finally speak the language, and think in that language, without having to translate. Was I slow on the uptake? Was my professor a bad teacher? Probably. Neither, however, precluded me from eventually getting it, suggesting both that I had the necessary processing power, and </span><em>someone</em><span> could teach the material.</span></p><p>Speed of understanding, however, only becomes more important as medical education continues into residency. </p><p><strong>A doctor’s foundational clinical mental models are built during residency</strong><span>, but the apprenticeship model of residency has flaws. An attending physician (an attending is a physician who has completed residency ) may be a skilled clinician but a poor educator, or not have the time, patience, or inclination to educate residents. A resident may only have three years in which to gain the practical, clinical knowledge they need to practice independently for the rest of their career. This puts a great deal of pressure on a student. But it should also places the burden of that education on attending physicians, many of whom aren’t given adequate tools and time to teach during their own workdays. </span></p><p>An ER shift usually goes like this: One attending physician oversees an ER “pod\area” and between 1-3 residents. Patients are assigned to that pod, residents assign themselves to patients, and the attending is responsible for seeing and evaluating all the patients while supervising the residents. An ER doctor sees, on average, 2-4 patients per hour, over a 8-12 hour shift, with multiple patients being juggled at once (I’ve cared for more than 20 active patients at a time). Patients who require intensive resuscitation or procedures may need an hour or more of sustained attention, while the board (the list of patients assigned to a pod) backs up. Being an Emergency Physician is about interruptions and fragmented time.</p><p>Even if you haven’t been in an ER lately, the news of worsening overcrowding, boarding times and uptick in patient visits is all over the news. It's worse than you imagine. ER attendings are beholden to any number of administrative metrics—patient satisfaction, charting completion, door to doc times—but especially throughput speed. You have to “move the meat”—ER lingo for getting patients in and out of the department quickly—to try and keep up with that endless stream of patients. Correcting a resident’s incorrect treatment plan only takes a moment, but stopping to interrogate the thought process that led that resident to the wrong answer takes time during which another patient arrives in anaphylactic shock, someone is bleeding onto the floor, another three patients are vomiting and a gunshot wound is being wheeled into the trauma bay. Those patients have to come first.</p><p>Teaching during a shift interrupts a busy workflow and means that attendings have to trade time completing their charts, for example, in order to teach, which then results in having to stay late or bring work home. Teaching or staying late doesn’t (usually) come with extra compensation, so the motivation needs to be intrinsic.</p><p>There are attempts to standardize resident education and overcome the variables that affect teaching on-shift, mostly with weekly “conferences” consistent of educational lectures, the quality of which also varies extensively depending on the lecturer. Let’s just say most wouldn’t be invited onto a TEDx stage. It’s not a bad way to learn the basics and facts: things like the biochemical changes cause by a kidney with stage III kidney disease, how to perform a simple interrupted suture, or how to calculate cardiac risk stratification scores. </p><p><span>But the fundamentals—which I’m defining as how you use and manipulate those basic facts to solve real problems in real patients, like </span><em>when </em><span>to use that simple interrupted suture or whether other factors would recommend a mattress suture, how a patient’s cardiac risk score interacts with other facts to influence a treatment plan, or what do when your kidney failure patient is coding and you don’t know their complete medication history—are learned mostly in real-time, on shift. How to think about facts, how to use facts, is where the art of medicine lies.</span></p><p>Even simulation-lab patient encounters don’t adequately recreate the challenges of rapid decision making in a busy ER, and tend to focus on common patient presentations. But patient’s rarely read the textbook and present accordingly. Basic facts are only tools. The facts don’t teach you to think like a doctor, any more than being able to identify a hammer makes someone a handyman.</p><p>There are a lot of misaligned incentives in resident education: attendings are judged by metrics of speed and patient satisfaction, residents want to learn and be seen as “good” so they can graduate and be recommended for a job, and hospitals want more patients to be seen, faster. </p><p>Because of the top to bottom relentless pressure to move the meat, a “good” resident, a resident who can “hack it” is a resident who is able to work quickly with minimal risk to patients. Who wouldn’t want to work with a resident whose incentives are aligned with yours? I know I feel more optimistic about the day when I see that I’m working with a resident who moves quickly and can help me do my job more smoothly. </p><p>But that means that someone— not me, but someone—must have, or should have already, taught the student the fundamentals by the time they get in front of me, so that I’m not slowed down and can focus on the interesting, juicy, complex conversations with a resident who “gets it.” Residents are smart. They know that’s what’s desirable: already knowing the things that they are really there to learn.</p><p>The residents who master material in a way that allows them to work quickly, whether or not they understand deeply, are prized and praised. Problem is that deep understanding usually requires sacrificing speed (initially), and there’s an inevitable bottleneck that happens when someone is laying the groundwork for fluency in a new skill. </p><p><span>How can we tell when the resident is quick and right via luck or guessing—and when they’re quick and right because they understand? We can’t, really, not until the right situation presents itself. And the truth is, doctors can get away with a lot of algorithmic thinking before a patient presents who is both complex in unexpected ways </span><em>and </em><span>in ways that might kill them if you get it wrong. </span></p><p>That’s what separates the physicians from many other members of the medical team: The training to get away from the algorithm and use a deep understanding to come up with novel solutions. That’s also why algorithmic thinking can be so dangerous. So many patients never bother reading the flowcharts before they arrive and only presenting with the allowed symptoms.</p><p><strong>Skill can be confused with speed of mastery</strong><span> and competence can be confused with confidence, because we want it to be. What I keep coming back to is that so much of science and medical training comes down to perception of skill, as opposed to actual skill.</span></p><p><span>For example, when I was in residency, a friend was given feedback that she wasn’t seeing enough patients on shifts. When she asked how many she saw compared to other residents, she was told that they didn’t have the numbers, but they could see she was slow, and she needed to show she could keep up, or wouldn’t be able to </span><em>hack it</em><span>. The electronic medical record had a search option where you could pull up the number of patient’s you’d seen in the last six months. So she looked. And then she looked up the numbers of all the other residents. Out of thirteen residents, she was ranked #6. When my friend brought this information back to the program director, she was told that it didn’t matter what the numbers showed, she gave the perception of being slow, and needed to fix it. Her program director wouldn’t even look at the data. </span></p><p>I remember presenting a patient to one of my attendings and saying that, given his clinical picture and my list of possible diagnoses, I wasn’t sure what the best next test would be.</p><p>“I want you to be sure,” my attending said.</p><p>“Yes, I want to be sure as well, but I’ve never seen this and so I’m not sure.”</p><p>“You need to be more confident in your plans.”</p><p><span>I countered that I’d be more confident in my plan if I could discuss the few different plans I’m considering and learn which was the most appropriate to the patient and why, so that the next time I saw a similar patient, I had a better understanding of </span><em>why </em><span>I was doing what I’m doing instead of just being perceived as knowing it. That’s when I’d be confident. Plus, I only had three years to get that kind of feedback before I was the one providing that feedback to others. There’s never another time during their career when a doc has the opportunity to run every single patient they see by a more experienced clinician. </span></p><p><span>I’m as skeptical of residents (and attendings) with too much confidence as none: I don’t want my residents to be too confident, to be too thoroughly convinced of their own rightness and way. I want them to have the freedom to admit when they don’t know everything. Because they don’t. I don’t, either. In </span><em>The Name of the Rose</em><span>, William of Baskerville is a monk but also a proto-detective in the mold of Sherlock Holmes, and when he’s trying to solve a series of increasingly bizarre murders, he tells his sidekick, Adso, that “we mustn’t dismiss any hypothesis, no matter how farfetched.” And so it often is in medicine. Being okay with uncertainty will make both residents and attending’s lives better, and more importantly, patient lives safer.</span></p><p><strong>The problem of perception as the most important metric</strong><span> of skill is a problem with most forms of physician evaluation. When going for your quarterly review, your boss, who doesn’t directly watch you interact with patients, uses nursing and peer perception of your skills, as well as certain metrics of speed and patient satisfaction scores, to determine the depth of your knowledge and ability to care for patients. </span></p><p>There’s no direct, objective observation of your interactions with patients, or discussion of how you break down complex problems, or philosophy to approaching new patients, or what you do when you’re faced with an unfamiliar problem, all of which give a much deeper understanding of who someone really is as a physician. When you need a new job, you’re required to get letters of recommendations from colleagues who have also never watched you interact with patients directly, and who don’t have any idea what kind of a doctor you actually are, just what kind of a doctor you appear to be.</p><p>This is a systems problem. Most physicians who are hired to work in residencies didn’t get formal training on how to educate. They happen to work at a site that has residents, and so they have to teach. We base how we teach on how we were taught. We praise for what we were praised for. I’m not immune. I catch myself doing it, too. We teach our residents how to succeed in a system where a doctor’s success and a patient’s successful care don’t always spring from the well of thought.</p><p><strong>Incentivizing deep learning and deep thought means</strong><span> reducing the time pressure on both attendings and residents. If hospitals valued people over profits, they’d hire more attendings to both see patients and supervise, spreading both the patient care and educational workload. The existing arguments that this is cost prohibitive is laughable. For example David Reich, CEO of Mt. Sinai Hospital in NYC made $1,808,577 in 2023 (excluding bonuses, which can be </span><em>impressive</em><span>). According to Glassdoor (and on par with my experience) the salary for a full time ER physician in NYC is between $200-275k a year. </span></p><p><span>The center for Medicare and Medicaid is the primary source of graduate medical education (residency) funding. Per the </span><a href="https://www.graham-center.org/maps-data-tools/gme-data-tables/2000-2021.html" rel="">Graham Center interactive GME data tool</a><span> Mt. Sinai received&nbsp; around $175k a year per resident, and pays them a salary of $84,479\year, leaving 90K to pay for their “education.” Remember that ER patients are being billed, and the physician pay comes from hospital profit. There should be plenty of room to hire a few additional physicians, if administrators stopped to remember that the residents being trained will one day be the attendings caring for them.</span></p><p>Until incentives align, and the hospitals reward and pay physicians for doing the work of educating in addition to their clinical work; until teaching attendings have adequate training on how to educate; until hospitals are willing to staff adequately so there’s time to teach, the system will remain broken.</p><p><em>Part Two (coming soon): How can we teach our students (and ourselves) to think better within the system we have?</em></p><p><em><span>If you’ve gotten this far, </span><a href="https://www.gofundme.com/f/help-the-fight-against-cancer-with-jake-s" rel="">consider the Go Fund Me</a><span> that’s funding my husband Jake’s </span><a href="https://jakeseliger.com/2023/07/22/i-am-dying-of-squamous-cell-carcinoma-and-the-treatments-that-might-save-me-are-just-out-of-reach/" rel="">ongoing cancer treatment</a><span>. Essays and Archives are not paywalled, but your support gives us more time to focus on both writing and each other, which we appreciate!</span></em></p><p data-attrs="{&quot;url&quot;:&quot;https://www.gofundme.com/f/help-the-fight-against-cancer-with-jake-s&quot;,&quot;text&quot;:&quot;Support Jake's Cancer Treatments&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://www.gofundme.com/f/help-the-fight-against-cancer-with-jake-s" rel=""><span>Support Jake's Cancer Treatments</span></a></p><p><em>If you enjoyed reading, let me know by giving the heart button below a tap, commenting, sharing, and subscribing, if you don’t already. </em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:9785955,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa20fed8-6cbb-4fb0-91a9-d72fa651e04f_6240x4160.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Senate passes reauthorization of key US surveillance program after midnight (265 pts)]]></title>
            <link>https://apnews.com/article/fisa-donald-trump-surveillance-congress-johnson-81e991c9f82e77b2fe13f8a3e0e25349</link>
            <guid>40096575</guid>
            <pubDate>Sat, 20 Apr 2024 11:40:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/fisa-donald-trump-surveillance-congress-johnson-81e991c9f82e77b2fe13f8a3e0e25349">https://apnews.com/article/fisa-donald-trump-surveillance-congress-johnson-81e991c9f82e77b2fe13f8a3e0e25349</a>, See on <a href="https://news.ycombinator.com/item?id=40096575">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>WASHINGTON (AP) — Barely missing its midnight deadline, the Senate voted early Saturday to reauthorize a key <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/hub/government-surveillance">U.S. surveillance law</a></span> after divisions over whether the FBI should be restricted from using the program to search for Americans’ data nearly forced the statute to lapse.</p><p>The legislation approved 60-34 with bipartisan support would extend for two years the program known as Section 702 of the <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/fbi-surveillance-section-702-c69db6741ca9f4c26f24ebf5941a5cb0">Foreign Intelligence Surveillance Act</a></span>. It now goes to President Joe Biden’s desk to become law. White House national security adviser Jake Sullivan said Biden “will swiftly sign the bill.”</p><p>“In the nick of time, we are reauthorizing FISA right before it expires at midnight,” Senate Majority Leader Chuck Schumer said when voting on final passage began 15 minutes before the deadline. “All day long, we persisted and we persisted in trying to reach a breakthrough and in the end, we have succeeded.” </p>
    

<p>U.S. officials have said the surveillance tool, first authorized in 2008 and renewed several times since then, is crucial in disrupting terror attacks, cyber intrusions, and foreign espionage and has also produced intelligence that the U.S. has relied on for specific operations, such as the 2022 killing of <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/hub/ayman-al-zawahri">al-Qaida leader Ayman al-Zawahri.</a></span></p>



<p>“If you miss a key piece of intelligence, you may miss some event overseas or put troops in harm’s way,” Sen. Marco Rubio, the top Republican on the Senate Intelligence Committee, said. “You may miss a plot to harm the country here, domestically, or somewhere else. So in this particular case, there’s real-life implications.”</p>
    
<p>The proposal would renew the program, which permits the U.S. government to collect without a warrant the communications of non-Americans located outside the country to gather foreign intelligence. The reauthorization faced a long and bumpy road to final passage Friday after months of clashes between privacy advocates and national security hawks pushed consideration of the legislation to the brink of expiration.</p>
    

<p>Though the spy program was technically set to expire at midnight, the Biden administration had said it expected its authority to collect intelligence to remain operational for at least another year, thanks to an opinion earlier this month from the Foreign Intelligence Surveillance Court, which receives surveillance applications.</p><p>Still, officials had said that court approval shouldn’t be a substitute for congressional authorization, especially since communications companies could cease cooperation with the government if the program is allowed to lapse.</p><p>Hours before the law was set to expire, U.S. officials were already scrambling after two major U.S. communication providers said they would stop complying with orders through the surveillance program, according to a person familiar with the matter, who spoke on the condition of anonymity to discuss private negotiations. </p><p>Attorney General Merrick Garland praised the reauthorization and reiterated how “indispensable” the tool is to the Justice Department. </p><p>“This reauthorization of Section 702 gives the United States the authority to continue to collect foreign intelligence information about non-U.S. persons located outside the United States, while at the same time codifying important reforms the Justice Department has adopted to ensure the protection of Americans’ privacy and civil liberties,” Garland said in a statement Saturday. </p>
    

<p>But despite the Biden administration’s urging and classified briefings to senators this week on the crucial role they say the spy program plays in protecting national security, a group of progressive and conservative lawmakers who were agitating for further changes had refused to accept the version of the bill the House sent over last week.</p><p>The lawmakers had demanded that Majority Leader Chuck Schumer allow votes on amendments to the legislation that would seek to address what they see as civil liberty loopholes in the bill. In the end, Schumer was able to cut a deal that would allow critics to receive floor votes on their amendments in exchange for speeding up the process for passage.</p>
    

<p>The six amendments ultimately failed to garner the necessary support on the floor to be included in the final passage. </p><p>One of the major changes detractors had proposed centered around restricting the FBI’s access to information about Americans through the program. Though the surveillance tool only targets non-Americans in other countries, it also collects communications of Americans when they are in contact with those targeted foreigners. Sen. Dick Durbin, the No. 2 Democrat in the chamber, had been pushing a proposal that would require U.S. officials to get a warrant before accessing American communications. </p><p>“If the government wants to spy on my private communications or the private communications of any American, they should be required to get approval from a judge, just as our Founding Fathers intended in writing the Constitution,” Durbin said. </p>
    

<p>In the past year, U.S. officials have revealed a series of abuses and mistakes by FBI analysts in improperly querying the intelligence repository for information about Americans or others in the U.S., including a <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/fisa-foreign-surveillance-fbi-3f7d4cc0ef413cdf20bc0b70548cde84">member of Congress</a></span> and participants in the <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/justice-department-fbi-surveillance-75c466a64e838ab12eaef96f6335f3cd">racial justice protests of 2020</a></span> and the Jan. 6, 2021, riot at the U.S. Capitol.</p><p>But members on both the House and Senate intelligence committees as well as the Justice Department warned requiring a warrant would severely handicap officials from quickly responding to imminent national security threats. </p><p>“I think that is a risk that we cannot afford to take with the vast array of challenges our nation faces around the world,” Democratic Sen. Mark Warner, chair of the Senate Intelligence Committee, said Friday. </p><h2>__</h2><p>Associated Press writer Eric Tucker contributed to this report. </p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do not buy a Hisense TV (or at least keep them offline) (168 pts)]]></title>
            <link>https://cohost.org/ghoulnoise/post/5286766-do-not-buy-hisense-t</link>
            <guid>40096253</guid>
            <pubDate>Sat, 20 Apr 2024 10:31:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cohost.org/ghoulnoise/post/5286766-do-not-buy-hisense-t">https://cohost.org/ghoulnoise/post/5286766-do-not-buy-hisense-t</a>, See on <a href="https://news.ycombinator.com/item?id=40096253">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-post-body="true" data-testid="post-body"><p>holy fucking shit!!!!!!!!!!<!-- --></p>
<!-- --><p><em>I've had this in my drafts to finish writing up for a few weeks but today I made another discovery that has pushed me to finally write it up lol.<!-- --></em></p>
<!-- --><p>My PC had a few hiccups over the past couple of years. Nothing so serious that I was truly concerned (at first) but, annoyances, to be sure. ("PC? Weren't you talking about a TV??" you might be thinking to yourself.  Yes. I'll get there. Oh, will I <!-- --><em>ever<!-- --></em> get there.)<!-- --></p>
<!-- --><p>For a long time, the most serious hiccup with my PC was being unable to open Display Settings on my PC. I had to use Nvidia control panel to make adjustments. Whatever. Didn't affect my ability to work or anything. I had just updated to Windows 11 so I thought maybe something went wrong in the update. So I did another install (which meant I had to re-install a ton of music plugin stuff on my computer which takes ages given all the stuff I use.)<!-- --></p>
<!-- --><p>The fresh install still didn't fix it. So I simply ignored it for the better part of 2 years.<!-- --></p>
<!-- --><p>Over time, so slowly I didn't really clock them as being related, other things started to fail.<!-- --></p>
<!-- --><p>I have a Komplete Kontrol S88 midi Keyboard that interfaces with Kontakt and Komplete Kontrol in my DAW. I can use the keyboard to adjust settings or select instruments without having to look at my computer screen. At some point last year this stopped working. I could still use it to input midi, and I was afraid I'd have to do a fresh install of all the Native Instruments stuff I use, so I kept putting off trying to fix it until I had less going on with work. It worked, just not as well as it should!<!-- --></p>
<!-- --><p>Then, Task manager started to hang in weird ways. It wouldn't close unless I forced it closed with ProcExp. Whatever! Computers are weird!<!-- --></p>
<!-- --><p>I had trouble getting video capture cards to connect when I considered streaming Splatoon. Whatever! I'm not a streamer, I should probably use that energy on something else.<!-- --></p>
<!-- --><p>But then, in March, I had some things fail that, turns out, are pretty necessary to using the computer.<!-- --></p>
<!-- --><p>I was trying to get remote desktop to work on my tablet so I could work in the living room closer to my cat, Grendel, who was still very much in mourning for his best friend (&lt;3 Guts). Grendel is at his happiest when he's napping on the couch next to a human. So I thought, hey, I could do some stuff in remote desktop mode! It wasn't something I used very often because it doesn't play super nice with audio, it'd been probably over a year since I last used it, but I knew it <!-- --><em>had<!-- --></em> worked and had been easy to set up.<!-- --></p>
<!-- --><p>Well. It refused to connect despite appearing as an option on my tablet. And as I was trying to troubleshoot this problem...<!-- --></p>
<!-- --><p>My task bars on my PC disappeared! If I moused over the spot they should have been I got the loading circle. And my monitors would jitter!<!-- --></p>
<!-- --><p>System settings were nowhere to be found. To navigate I had to open Task Manager (which if you recall was already acting strange and sluggish for the past year) in order to open the command panel, which I'd then use to launch programs. Or, attempt to. If I tried to open the system settings, my PC spat out "Uhhh what's "Settings"? We ain't got settings" (in command I got "C:\Windows\System32&gt;start ms-settings: Access is denied." <!-- --><em>despite being in admin mode<!-- --></em>)<!-- --></p>
<!-- --><p>SO I started to panic a wee bit. It's not a great time for me to possibly need a new PC! Plus, it works great <!-- --><em>when it works<!-- --></em>.<!-- --></p>
<!-- --><p>I manually backed up everything important from my main PC drive (as I could not access the windows backup program because it lived in settings!!!!!!!!) and downloaded the Windows 11 installer to a drive in case I needed to completely wipe my computer and start fresh.<!-- --></p>
<!-- --><p>In a last ditch effort, I tried updating my Nvidia GeForce graphics driver and restarted.<!-- --></p>
<!-- --><p>Lo and behold, my taskbars came back and I could access settings again! Huzzah!<!-- --></p>
<!-- --><p>This lasted for 6 days. And then the taskbars disappeared AGAIN along with Settings.<!-- --></p>
<!-- --><p>At least by now I'd become somewhat comfortable using command to get around, so I was less panicked than I was in round 1. I at least knew the computer wouldn't crash and delete all my shit suddenly (plus I had my backups that were up-to-date).<!-- --></p>
<!-- --><p>I asked my friends if they'd ever heard of anything like this, and Cohost's own <!-- --><a data-testid="mention" href="https://cohost.org/vogon">@<!-- -->vogon<!-- --></a> helped me poke around some additional graphics driver stuff in case updating to the new Nvidia program that's in beta would solve my issue. This felt super promising when I went into safe mode and saw taskbars. So, I did the update, and, as before, when I updated, the taskbars came back! But there were gone again almost immediately. My screen literally started flickering and they vanished. It was so fucking bizarre.<!-- --></p>
<!-- --><p>I turned to google once more, hoping that somehow, as useless as google is these days, maybe I'd find what I needed to fix my damn computer!<!-- --></p>
<!-- --><p>Somehow, against all odds, I found it. I happened to use the exact right string in my search to pull up some reddit threads. The first one didn't have anything useful, but a few results down I spotted "no taskbar and task manager freezes" holy moly!!!<!-- --></p>
<!-- --><p>At first glance however, there wasn't anything useful. But then I spotted some collapsed comments at the bottom of the thread. I knew more than likely they just had comments like "sucks bro" or "this is why i use linux :/"  But I expanded them and inside was a link to a Microsoft forum post.  I kept my expectations in check and clicked on the link. What I saw had my nearly vibrating in my seat.<!-- --></p>
<!-- --><p><img src="https://staging.cohostcdn.org/attachment/d9f5719e-fe05-4247-9c27-39c3381f0648/screenshot%20collage.png?width=675&amp;auto=webp&amp;dpr=1" alt="Screenshots showing the path from my google search to reddit to a minimized comment that contained a link to a microsoft forum post with the solution to my several year long problem - caused by our TV"></p><!-- --><p><a href="https://learn.microsoft.com/en-us/answers/questions/1339707/help-with-figuring-out-what-is-causing-waitchain-d" target="_blank" rel="nofollow noopener">This was the solution.<!-- --></a></p>
<!-- --><p><em>Narayan B<!-- --></em><br>
<!-- --><em>Nov 16, 2023, 12:20 AM<!-- --></em></p>
<!-- --><p><em>So I finally solved it!<!-- --></em></p>
<!-- --><p><em>Source of the problem: an Android TV (HiSense model) connected to my network. Yes. A TV caused this issue.<!-- --></em></p>
<!-- --><p>HISENSE? LIKE THE SAME BRAND OF TV I HAVE HAD SINCE 2020???<!-- --></p>
<!-- --><p>I followed the instructions. I deleted keys generated by our TV for 5 straight minutes. 5 Minutes of like 200BPM clicking. I restarted. Everything worked again. I laughed so hard I cried. I felt like I'd solved a murder. The main suspect was the PC but the culprit was the TV in the other room.  And he almost got away with it!!! If I had spent a few days carrying out a clean install and re-installing all my work stuff, my problem would have come back. If I had taken the PC out back and shot it and replaced it with a fancy new computer, the problem would have come back.<!-- --></p>
<!-- --><p>Because the problem was never the PC. The Problem was my Hisense TV in the next room.<!-- --></p>
<!-- --><p>Once I carried this out, I was able to open display settings for the first time (outside of safe mode) in 2 years. When I deleted the keys, my Task Manager started behaving normally again. I turned around and saw that my keyboard was once again displaying the VST controls on its screen. The fancy midi keyboard was back at full functionality. I was able to connect my CRT as a display again using the HDMI -&gt; RCA converters I'd assumed had stopped working (nope! they still worked, they always worked!)<!-- --></p>
<!-- --><p>Which brings me to today. Almost a month later everything still works. So, I decided to see if remote desktop would magically work again. The answer is: Yes. Yes, in fact, my TV was the reason my remote desktop connection had failed the month before.<!-- --></p>
<!-- --><p>So here I am, sitting on my couch with my tablet and bluetooth keeb and mouse, writing this post in remote desktop mode so I could attach screenshots of this saga to my post without sitting at my PC. Grendel is snoring beside me as I write.<!-- --></p>
<!-- --><p>As a treat for reading this far, please enjoy two screenshots of my friends reacting to the solution of my great mystery.<!-- --></p>
<!-- --><p><img src="https://staging.cohostcdn.org/attachment/a23624ba-f7bc-4b5b-aa53-68b1e16bfd2e/cursed%20TV%20PC%20reactions.png" alt="friends reacting in discord, the overall sentiment: what the fuck that is so cursed"></p><!-- --><p><img src="https://staging.cohostcdn.org/attachment/b9d4234d-4a60-4bb0-b2a9-92b898e75fba/cursed%20TV%20PC%20reactions2.png" alt="friends reacting in discord, the overall sentiment: what the fuck that is so cursed"></p><!-- --><p>adding: the TV in question, in case you're curious: Hisense 50Q8G- 50" Smart 4K ULED™ Android TV with Quantum Dot Technology (Canada Model)<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MuPDF WASM Viewer Demo (263 pts)]]></title>
            <link>https://mupdf.com/wasm/demo/index.html?file=../../docs/mupdf_explored.pdf</link>
            <guid>40096113</guid>
            <pubDate>Sat, 20 Apr 2024 09:52:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mupdf.com/wasm/demo/index.html?file=../../docs/mupdf_explored.pdf">https://mupdf.com/wasm/demo/index.html?file=../../docs/mupdf_explored.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=40096113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="menubar-panel">
		<details>
			<summary>File</summary>
			<menu>
				<li onclick="document.getElementById('open-file-input').click()">Open File...</li>
			</menu>
		</details>
		<details>
			<summary>Edit</summary>
			<menu>
				<li onclick="show_search_panel()">Search...</li>
			</menu>
		</details>
		<details>
			<summary>View</summary>
			<menu>
				<li onclick="toggle_fullscreen()">Fullscreen</li>
				<li onclick="toggle_outline_panel()">Outline</li>
				<li onclick="zoom_to(48)">50%</li>
				<li onclick="zoom_to(72)">75% (72 dpi)</li>
				<li onclick="zoom_to(96)">100% (96 dpi)</li>
				<li onclick="zoom_to(120)">125%</li>
				<li onclick="zoom_to(144)">150%</li>
				<li onclick="zoom_to(192)">200%</li>
			</menu>
		</details>

		<div>
			<p><a href="https://www.npmjs.com/package/mupdf">MuPDF.js on NPM</a>
			<a href="https://mupdfjs.readthedocs.io/">API docs</a>
			<a href="https://github.com/ArtifexSoftware/mupdf.js">Source code on Github</a>
		</p></div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ceefax Simulator (149 pts)]]></title>
            <link>https://www.nathanmediaservices.co.uk/ceefax/</link>
            <guid>40095657</guid>
            <pubDate>Sat, 20 Apr 2024 08:03:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nathanmediaservices.co.uk/ceefax/">https://www.nathanmediaservices.co.uk/ceefax/</a>, See on <a href="https://news.ycombinator.com/item?id=40095657">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>Interactive Viewer by genius <b><a href="https://zxnet.co.uk/">Alistair Cree</a></b>, a.k.a. <b><a href="https://twitter.com/ZXGuesser">ZXGuesser</a></b></p>
			<br>
			<h2>How To Use</h2>
			<p>Remember teletext? This is exactly the same.<br>
			Each page is assigned a three digit number - you'll see navigation lines that give a page description followed by a number (e.g. "Sport Headlines  302")<br>Use the number keys on the on-screen remote (or your keyboard if you're on a PC) to enter a number. The top row turns green as we wait for the page to load.<br>You can also use the "channel up/down" buttons to move up or down one page at a time.</p>
			<hr>
			<div id="shadowed"><ul><a href="https://www.youtube.com/channel/UCNnD5PSSlPGnT3kAa-ljzWw/live"><li><h2>Pages From Ceefax</h2><img src="https://www.nathanmediaservices.co.uk/assets/images/ceefax-pi.jpeg" alt="TV showing Ceefax" width="1200" height="627"><p>View live Pages from Ceefax as shown on BBC2 overnight</p></li></a><a href="https://www.nathanmediaservices.co.uk/ceefax/inthenews/"><li><h2>In The News</h2><img src="https://www.nathanmediaservices.co.uk/assets/images/ceefax-news.jpeg" alt="Newspaper story on Ceefax re-creation" width="1200" height="627"><p>News stories featuring Ceefax</p></li></a><a href="https://www.nathanmediaservices.co.uk/ceefax/history/"><li><h2>History</h2><img src="https://www.nathanmediaservices.co.uk/assets/images/ceefax-history.jpeg" alt="In vision teletext decoder prototype" width="1200" height="627"><p>How this re-creation came to exist</p></li></a></ul></div>
<div id="shadowed"><a href="https://www.nathanmediaservices.co.uk/"><h2>More from NMS</h2></a><ul><a href="https://www.nathanmediaservices.co.uk/stuffimade/"><li><h2>Stuff I Made</h2><img src="https://www.nathanmediaservices.co.uk/assets/images/stuffimade-subrack-inserter.jpeg" alt="Two aluminium-fronted subrack modules. They each have four LEDs labelled 'power', 'act', 'sync' and 'text', a USB port, and an ethernet port." width="1200" height="627"><p>Electronic devices I've created</p></li></a><a href="https://www.nathanmediaservices.co.uk/projects/"><li><h2>Projects</h2><img src="https://www.nathanmediaservices.co.uk/assets/images/projects-connectivity.jpg" alt="A bunch of cables lie on the ground, ready to be pulled through a duct." width="1200" height="627"><p>Projects I've undertaken</p></li></a><a href="https://www.nathanmediaservices.co.uk/about/contact.php"><li><h2>Contact</h2><img src="https://www.nathanmediaservices.co.uk/assets/navigation/nms-logo.png" alt="NMS" width="1200" height="627"><p>How to get in touch and other account information</p></li></a></ul></div>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dehydrated: Letsencrypt/acme client implemented as a shell-script (119 pts)]]></title>
            <link>https://github.com/dehydrated-io/dehydrated</link>
            <guid>40095325</guid>
            <pubDate>Sat, 20 Apr 2024 06:36:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dehydrated-io/dehydrated">https://github.com/dehydrated-io/dehydrated</a>, See on <a href="https://news.ycombinator.com/item?id=40095325">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">dehydrated <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=23P9DSJBTY7C8" rel="nofollow"><img src="https://camo.githubusercontent.com/0283ea90498d8ea623c07906a5e07e9e6c2a5eaa6911d52033687c60cfa8d22f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667" alt="Donate" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-green.svg"></a></h2><a id="user-content-dehydrated-" aria-label="Permalink: dehydrated " href="#dehydrated-"></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/logo.png"><img src="https://github.com/dehydrated-io/dehydrated/raw/master/docs/logo.png" alt=""></a></p>
<p dir="auto">Dehydrated is a client for signing certificates with an ACME-server (e.g. Let's Encrypt) implemented as a relatively simple (zsh-compatible) bash-script.
This client supports both ACME v1 and the new ACME v2 including support for wildcard certificates!</p>
<p dir="auto">It uses the <code>openssl</code> utility for everything related to actually handling keys and certificates, so you need to have that installed.</p>
<p dir="auto">Other dependencies are: cURL, sed, grep, awk, mktemp (all found pre-installed on almost any system, cURL being the only exception).</p>
<p dir="auto">Current features:</p>
<ul dir="auto">
<li>Signing of a list of domains (including wildcard domains!)</li>
<li>Signing of a custom CSR (either standalone or completely automated using hooks!)</li>
<li>Renewal if a certificate is about to expire or defined set of domains changed</li>
<li>Certificate revocation</li>
<li>and lots more..</li>
</ul>
<p dir="auto">Please keep in mind that this software, the ACME-protocol and all supported CA servers out there are relatively young and there might be a few issues. Feel free to report any issues you find with this script or contribute by submitting a pull request,
but please check for duplicates first (feel free to comment on those to get things rolling).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">For getting started I recommend taking a look at <a href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/domains_txt.md">docs/domains_txt.md</a>, <a href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/wellknown.md">docs/wellknown.md</a> and the <a href="#usage">Usage</a> section on this page (you'll probably only need the <code>-c</code> option).</p>
<p dir="auto">Generally you want to set up your WELLKNOWN path first, and then fill in domains.txt.</p>
<p dir="auto"><strong>Please note that you should use the staging URL when experimenting with this script to not hit Let's Encrypt's rate limits.</strong> See <a href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/staging.md">docs/staging.md</a>.</p>
<p dir="auto">If you have any problems take a look at our <a href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/troubleshooting.md">Troubleshooting</a> guide.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Config</h2><a id="user-content-config" aria-label="Permalink: Config" href="#config"></a></p>
<p dir="auto">dehydrated is looking for a config file in a few different places, it will use the first one it can find in this order:</p>
<ul dir="auto">
<li><code>/etc/dehydrated/config</code></li>
<li><code>/usr/local/etc/dehydrated/config</code></li>
<li>The current working directory of your shell</li>
<li>The directory from which dehydrated was run</li>
</ul>
<p dir="auto">Have a look at <a href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/examples/config">docs/examples/config</a> to get started, copy it to e.g. <code>/etc/dehydrated/config</code>
and edit it to fit your needs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage:</h2><a id="user-content-usage" aria-label="Permalink: Usage:" href="#usage"></a></p>
<div data-snippet-clipboard-copy-content="Usage: ./dehydrated [-h] [command [argument]] [parameter [argument]] [parameter [argument]] ...

Default command: help

Commands:
 --version (-v)                   Print version information
 --display-terms                  Display current terms of service
 --register                       Register account key
 --account                        Update account contact information
 --cron (-c)                      Sign/renew non-existent/changed/expiring certificates.
 --signcsr (-s) path/to/csr.pem   Sign a given CSR, output CRT on stdout (advanced usage)
 --revoke (-r) path/to/cert.pem   Revoke specified certificate
 --deactivate                     Deactivate account
 --cleanup (-gc)                  Move unused certificate files to archive directory
 --cleanup-delete (-gcd)          Deletes (!) unused certificate files
 --help (-h)                      Show help text
 --env (-e)                       Output configuration variables for use in other scripts

Parameters:
 --accept-terms                   Accept CAs terms of service
 --full-chain (-fc)               Print full chain when using --signcsr
 --ipv4 (-4)                      Resolve names to IPv4 addresses only
 --ipv6 (-6)                      Resolve names to IPv6 addresses only
 --domain (-d) domain.tld         Use specified domain name(s) instead of domains.txt entry (one certificate!)
 --ca url/preset                  Use specified CA URL or preset
 --alias certalias                Use specified name for certificate directory (and per-certificate config) instead of the primary domain (only used if --domain is specified)
 --keep-going (-g)                Keep going after encountering an error while creating/renewing multiple certificates in cron mode
 --force (-x)                     Force certificate renewal even if it is not due to expire within RENEW_DAYS
 --force-validation               Force revalidation of domain names (used in combination with --force)
 --no-lock (-n)                   Don't use lockfile (potentially dangerous!)
 --lock-suffix example.com        Suffix lockfile name with a string (useful for with -d)
 --ocsp                           Sets option in CSR indicating OCSP stapling to be mandatory
 --privkey (-p) path/to/key.pem   Use specified private key instead of account key (useful for revocation)
 --domains-txt path/to/domains.txt Use specified domains.txt instead of default/configured one
 --config (-f) path/to/config     Use specified config file
 --hook (-k) path/to/hook.sh      Use specified script for hooks
 --preferred-chain issuer-cn      Use alternative certificate chain identified by issuer CN
 --out (-o) certs/directory       Output certificates into the specified directory
 --alpn alpn-certs/directory      Output alpn verification certificates into the specified directory
 --challenge (-t) http-01|dns-01|tls-alpn-01 Which challenge should be used? Currently http-01, dns-01, and tls-alpn-01 are supported
 --algo (-a) rsa|prime256v1|secp384r1 Which public key algorithm should be used? Supported: rsa, prime256v1 and secp384r1"><pre lang="text"><code>Usage: ./dehydrated [-h] [command [argument]] [parameter [argument]] [parameter [argument]] ...

Default command: help

Commands:
 --version (-v)                   Print version information
 --display-terms                  Display current terms of service
 --register                       Register account key
 --account                        Update account contact information
 --cron (-c)                      Sign/renew non-existent/changed/expiring certificates.
 --signcsr (-s) path/to/csr.pem   Sign a given CSR, output CRT on stdout (advanced usage)
 --revoke (-r) path/to/cert.pem   Revoke specified certificate
 --deactivate                     Deactivate account
 --cleanup (-gc)                  Move unused certificate files to archive directory
 --cleanup-delete (-gcd)          Deletes (!) unused certificate files
 --help (-h)                      Show help text
 --env (-e)                       Output configuration variables for use in other scripts

Parameters:
 --accept-terms                   Accept CAs terms of service
 --full-chain (-fc)               Print full chain when using --signcsr
 --ipv4 (-4)                      Resolve names to IPv4 addresses only
 --ipv6 (-6)                      Resolve names to IPv6 addresses only
 --domain (-d) domain.tld         Use specified domain name(s) instead of domains.txt entry (one certificate!)
 --ca url/preset                  Use specified CA URL or preset
 --alias certalias                Use specified name for certificate directory (and per-certificate config) instead of the primary domain (only used if --domain is specified)
 --keep-going (-g)                Keep going after encountering an error while creating/renewing multiple certificates in cron mode
 --force (-x)                     Force certificate renewal even if it is not due to expire within RENEW_DAYS
 --force-validation               Force revalidation of domain names (used in combination with --force)
 --no-lock (-n)                   Don't use lockfile (potentially dangerous!)
 --lock-suffix example.com        Suffix lockfile name with a string (useful for with -d)
 --ocsp                           Sets option in CSR indicating OCSP stapling to be mandatory
 --privkey (-p) path/to/key.pem   Use specified private key instead of account key (useful for revocation)
 --domains-txt path/to/domains.txt Use specified domains.txt instead of default/configured one
 --config (-f) path/to/config     Use specified config file
 --hook (-k) path/to/hook.sh      Use specified script for hooks
 --preferred-chain issuer-cn      Use alternative certificate chain identified by issuer CN
 --out (-o) certs/directory       Output certificates into the specified directory
 --alpn alpn-certs/directory      Output alpn verification certificates into the specified directory
 --challenge (-t) http-01|dns-01|tls-alpn-01 Which challenge should be used? Currently http-01, dns-01, and tls-alpn-01 are supported
 --algo (-a) rsa|prime256v1|secp384r1 Which public key algorithm should be used? Supported: rsa, prime256v1 and secp384r1
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chat</h2><a id="user-content-chat" aria-label="Permalink: Chat" href="#chat"></a></p>
<p dir="auto">Dehydrated has an official IRC-channel <code>#dehydrated</code> on libera.chat that can be used for general discussion and suggestions.</p>
<p dir="auto">The channel can also be accessed with Matrix using the official libera.chat bridge at <code>#dehydrated:libera.chat</code>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Children Need Neighborhoods Where They Can Walk and Bike (129 pts)]]></title>
            <link>https://www.wsj.com/health/wellness/children-need-neighborhoods-where-they-can-walk-and-bike-5f3a9b4a</link>
            <guid>40095217</guid>
            <pubDate>Sat, 20 Apr 2024 06:11:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/health/wellness/children-need-neighborhoods-where-they-can-walk-and-bike-5f3a9b4a">https://www.wsj.com/health/wellness/children-need-neighborhoods-where-they-can-walk-and-bike-5f3a9b4a</a>, See on <a href="https://news.ycombinator.com/item?id=40095217">Hacker News</a></p>
Couldn't get https://www.wsj.com/health/wellness/children-need-neighborhoods-where-they-can-walk-and-bike-5f3a9b4a: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Volkswagen workers vote overwhelmingly to join the UAW (139 pts)]]></title>
            <link>https://www.cnn.com/2024/04/19/business/volkswagen-uaw-vote/index.html</link>
            <guid>40095189</guid>
            <pubDate>Sat, 20 Apr 2024 06:03:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2024/04/19/business/volkswagen-uaw-vote/index.html">https://www.cnn.com/2024/04/19/business/volkswagen-uaw-vote/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=40095189">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/clv7czy9600023b6hbzwa7xr0@published" data-name="11718-VolkswagenChattanoogaproducesone-millionthvehicle.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.5916666666666667" data-original-height="994" data-original-width="1680" data-url="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=original" data-editable="lede" data-freewheel-lede="true">
       <picture><source height="383" width="680" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=16x9&amp;q=h_383,w_680,c_fill/f_webp" type="image/webp"><source height="653" width="1160" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=16x9&amp;q=h_653,w_1160,c_fill/f_webp" type="image/webp"><source height="605" width="1075" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=16x9&amp;q=h_605,w_1075,c_fill/f_webp" type="image/webp"><source height="833" width="1480" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=16x9&amp;q=h_833,w_1480,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=16x9&amp;q=h_833,w_1480,c_fill" alt="Workers at Volkswagen's American factory in Chattanooga, Tennessee voted overwhelmingly to join the United Auto Workers union in an election concluded Friday." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="994" width="1680"></picture>
    </div><div data-editable="content" itemprop="articleBody" data-reorderable="content">
                    <p><cite>
      <span data-editable="location">New York</span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv721hjx000j45ns0bvq3ogw@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Hourly workers at Volkswagen’s plant in Chattanooga, Tennessee, overwhelming voted to join the United Auto Workers late Friday, a major breakthrough in the union’s effort to organize workers at plants nationwide.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7gsxr300073b6hu9szx63r@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Shortly after 11 pm ET<strong> </strong>on Friday the National Labor Relations Board, the federal body that oversees such votes, announced that 73% of the 3,600 workers at the plant who cast ballots had voted in favor of joining the union. There was an 84% turnout among eligible voters.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hn15l000e3b6h2zu57i98@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “This election is big,” said Kelcey Smith, a worker in the paint department at Volkswagen, in a UAW statement. “This is the time; this is the place. Southern workers are ready to stand up and win a better life.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hmx6v000a3b6hwf1235hu@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            There are roughly 150,000 workers at nonunion auto plants in the United States today, roughly the same number as at the American plants of the three unionized automakers — Gener﻿al Motors, Ford and Stellantis. If the union can win the right to represent workers across the broad swath of the nonunion auto plants, it could increase their leverage in future contract negotiations.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hmy5e000c3b6h10cgwvr6@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The UAW’s victory could also provide a high-profile beachhead for unions in Southern states, which have a much lower level of union representation among workers than in Northern industrial states. Most of the nonunion auto plants are spread across the south.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hvnjn000g3b6heg92e70g@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The union has announced an effort to represent workers at not only Volkswagen, but also <a href="https://www.cnn.com/2023/11/29/business/uaw-organize-nonunion-automakers/index.html">nine other foreign automakers</a> with American plants — BMW, Honda, Hyundai, Mazda, Mercedes, Nissan, Subaru, Toyota and Volvo. It has already filed to have another election at the <a href="https://www.cnn.com/2024/04/05/business/uaw-mercedes-union-vote/index.html">Mercedes plant in Vance, Alabama</a>, just outside of Tuscaloosa. That vote is set to take place next month and be concluded on May 17.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hvz0a000i3b6h1gan135c@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            It is also seeking to represent workers at three American automakers making electric vehicles — Tesla, Rivian and Lucid. But it has yet to file to hold votes at those American EV makers or at the American plants of the eight foreign automakers other than Volkswagen and Mercedes.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hycpo000k3b6hvswn9zq5@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Unlike many employers who conduct campaigns against union membership when faced with an organizing effort, Volkswagen had remained neutral in this campaign. Its statement once the vote was announced was similarly even-handed, stating only the vote results and that “We will await certification of the results by the NLRB. Volkswagen thanks its Chattanooga workers for voting in this election.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jf3wa00193b6hm56yhxxw@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            That certification is expected to come within five days if Volkswagen does not file objections to the vote, according to the NLRB. The agency said the company is expected to begin bargaining in good faith with the union at that time.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jhmic001b3b6hjufkmdao@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            One reason the company was more neutral than many employers facing a union vote is the strength of unions in its home country of Germany. The main union for its plants there has a seat on the company’s board.
    </p>

  <h2 data-editable="text" data-uri="cms.cnn.com/_components/subheader/instances/clv7jith8001d3b6heu0mqlcw@published" data-component-name="subheader" id="auto-strikes-and-contracts-helped-win-vote" data-article-gutter="true">
    Auto strikes and contracts helped win vote
</h2>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hygvr000n3b6hib1yfd97@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The union’s organizing efforts follow a <a href="https://www.cnn.com/2023/09/15/business/auto-workers-strike/index.html">six-week strike</a> against the three unionized automakers last fall, which won <a href="https://www.cnn.com/2023/10/30/business/gm-uaw-tentative-agreement/index.html">record pay increases</a> for UAW members at the three companies. They received immediate raises of at least 11% and pay increases totaling more than 30% over the life of the contract, which runs through April 2028.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hygvr000o3b6h1uby897o@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Many of the nonunion automakers, including Volkswagen, gave their workers similar raises in the wake of the UAW contracts. But the workers at the nonunion plants generally earn less than their counterparts at the unionized automakers.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hygvr000p3b6h2c98ljud@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The average worker in Volkswagen’s Chattanooga plant makes about $60,000 a year before bonuses and benefits, according to the company. Production workers working under the recent UAW contract now make about $36 an hour, or about $75,000 a year before overtime, bonuses and benefits.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jrx1y001m3b6ha6yiu59g@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Volkswagen once before had had an American plant where workers were represented by the UAW, in Pennsylvania. But that plant closed in 1988 in the face of weak American sales by Volkswagen.&nbsp;And the UAW has had little success winning the right to represent nonunion auto workers since then, until Friday’s vote.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7i52kc000s3b6hx7dksole@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But the union’s effort had been opposed by a coalition of six southern Republican governors who have nonunion auto plants in their states.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7i5hf2000u3b6h33w47xhh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The six, including Tennessee Gov. Bill Lee as well as the governors of Alabama, Georgia, Mississippi, South Carolina and Texas, signed a letter this week arguing those nonunion jobs would be at risk if the union won Friday’s vote.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7i5hf2000v3b6hmrj1tvvo@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The reality is companies have choices when it comes to where to invest and bring jobs and opportunity. We have worked tirelessly on behalf of our constituents to bring good-paying jobs to our states,” said the letter. “Unionization would certainly put our states’ jobs in jeopardy.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jlcye001f3b6huw8ugqf4@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But the vote represents a run of success by the nation’s unions, which won <a href="https://www.cnn.com/2023/11/21/business/big-paydays-union-members/index.html">pay increases of 10% or more</a> for nearly a million union members last year, according to an analysis by CNN. <a href="https://www.cnn.com/2024/02/21/business/2023-strike-summary">Strikes</a> were at a decade high in 2023, and organizing activity also increased.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jpqas001j3b6hxf6wfjmg@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Friday’s vote results were praised by the AFL-CIO, the main federation of unions in the country.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jncps001h3b6hqgd0xugk@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “This victory sends a powerful message to corporate interests everywhere: Workers will no longer tolerate exploitation and mistreatment,” said the AFL-CIO’s statement. “Whether it’s autoworkers in Tennessee, film crews in Hollywood, hotel workers in Las Vegas or baristas at the local coffee shop, when working people stand together in solidarity, we have the power to enact meaningful change and usher in a brighter future for all.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hf8wi00033b6hn1t69rop@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            <em>This story has been updated with additional context and developments.</em>
    </p>


                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On Terry A. Davis (109 pts)]]></title>
            <link>https://schizophrenic.io/blog/on-terry-a-davis</link>
            <guid>40095070</guid>
            <pubDate>Sat, 20 Apr 2024 05:34:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://schizophrenic.io/blog/on-terry-a-davis">https://schizophrenic.io/blog/on-terry-a-davis</a>, See on <a href="https://news.ycombinator.com/item?id=40095070">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><header></header><p>As someone formally diagnosed with schizophrenia who happens to be a
programmer, I often get questions such as, “What do you think of Terry A. Davis
and his OS, TempleOS?“ While people who ask me this question often mean well,
it is often a catalyst for some complicated thoughts on mental illness and
product development.</p><p>I want to be delicate while also being realistic about Terry. He was, indeed, a
brilliant programmer. Where I disagree with him is in his insistence that there
was a grand conspiracy to hold back his operating system, perhaps orchestrated
by the CIA. Anyone who used his operating system could tell that it was, at the
very least, unpolished. While it did have some recreational value, it did not
provide a product that people could use that would enrich their lives.</p><p>While I certainly can relate to Terry’s sense of fleeting success, I do not
ascribe any agency or conspiracy as the cause of my failures, nor my lack of
successes. Any failures I have made in my life are strictly my own and, if
nobody wants to read my posts or hire me to do a job, that is on me. Of course,
I also disagree with Terry’s use of racial and ethnic slurs. Let’s be clear:
schizophrenia does not cause racism. That failure was his own, as well. [Edit:
Several people from the HN thread mentioned that his usage of these terms might
have stemmed from his persecutory delusions. Well, maybe so. Honestly, that’s
still not good, but I can at least see where that perspective comes from. Sorry
if I appeared judgmental here. :) ]</p><p>So what is the take away from all this? First, that Terry was a complicated
figure, often brilliant, but certainly flawed. Second, that it is not my
intention to present my lack of (current) success as anything other than the
failures (so far) to produce a useful product.</p><p>With that said, I wish to send regards to Terry’s family and say a prayer that
he is now in a better place. May he find many successes in (if it exists) the
afterlife. :)</p></div></div>]]></description>
        </item>
    </channel>
</rss>