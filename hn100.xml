<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 16 Dec 2023 13:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[OpenAI suspends ByteDance's account after it used GPT to train its own AI model (198 pts)]]></title>
            <link>https://www.theverge.com/2023/12/15/24003542/openai-suspends-bytedances-account-after-it-used-gpt-to-train-its-own-ai-model</link>
            <guid>38662160</guid>
            <pubDate>Sat, 16 Dec 2023 06:17:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/12/15/24003542/openai-suspends-bytedances-account-after-it-used-gpt-to-train-its-own-ai-model">https://www.theverge.com/2023/12/15/24003542/openai-suspends-bytedances-account-after-it-used-gpt-to-train-its-own-ai-model</a>, See on <a href="https://news.ycombinator.com/item?id=38662160">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>OpenAI suspends ByteDance’s account after it used GPT to train its own AI model.</p><p>In <a href="https://www.theverge.com/2023/12/15/24003151/bytedance-china-openai-microsoft-competitor-llm">today’s<em> </em>issue of <em>Command Line</em></a>, I reported that ByteDance has been violating the developer license of both Microsoft and OpenAI by using GPT-generated data to train its own, competing model in China.</p><p>After my report was published, OpenAI spokesperson Niko Felix sent the following statement confirming that ByteDance’s account has been suspended:</p><blockquote><p>All API customers must adhere to our usage policies to ensure that our technology is used for good. While ByteDance’s use of our API was minimal, we have suspended their account while we further investigate. If we discover that their usage doesn’t follow these policies, we will ask them to make necessary changes or terminate their account.</p></blockquote><p>As I reported, most of ByteDance’s GPT usage has been done through Microsoft’s Azure platform, not through OpenAI directly. I’ve asked Microsoft if it will follow OpenAI and suspend ByteDance’s access as well.</p><br></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CRDT Concepts: Causal Trees (130 pts)]]></title>
            <link>https://www.farley.ai/posts/causal</link>
            <guid>38661580</guid>
            <pubDate>Sat, 16 Dec 2023 03:55:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.farley.ai/posts/causal">https://www.farley.ai/posts/causal</a>, See on <a href="https://news.ycombinator.com/item?id=38661580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I recently came across an elegant CRDT design that is useful for text collaboration applications that I just had to write about. What follows is an exploration of Causal Trees and an introduction (or refresher) to several important distributed systems concepts.</p><p>CRDTs are a family of algorithms and datastructures that provide eventual consistency guarantees by following some simple mathematical properties. As their popularity has increased over the last decade, so too has their usage. CRDTS are commonly found in collaborative applications, where concurrent updates can be frequent, but they’re also used quite extensively in local network and peer-to-peer environments, due to the algorithms not requiring a central authority to reconcile inconsistencies. That last sentence is quite important, because it means that we can get distributed eventual consistency (things eventually converge to a consistent value among nodes) without complicated processes such as consensus. Don’t fret if you’re a fan of central authority though, <a href="https://www.figma.com/">Figma</a> successfully uses CRDTs server-side to handle the collaborative aspects of their product, as well as <a href="https://github.com/soundcloud/roshi">Soundcloud</a> and many others.</p><p>The CRDT I will be describing today is the Causal Tree (roughly as <a href="https://web.archive.org/web/20190505005829/http://www.st.ewi.tudelft.nl/victor/articles/ctre.pdf">described</a> by Victor Grishchenko). Which we will look at in the context of a simple text collaboration application. But before we get too far ahead of ourselves, have a play with the example below and see if you can get an intuitive sense of how the algorithm operates. Some things to keep an eye out for are the ids associated with each node (on hover), and also what happens when individual characters are deleted.</p><h3>Example 1</h3><p>Each client in the above example has their own local causal tree, the compiled value of which you can see written to their respective text inputs. The implementation we will be discussing is known as a state-based CRDT, or CvRDT, which means that we send the whole tree over the wire to clients instead of just the individual operations as they occur. As a client types, new nodes are added to their tree and sent over to peers who are interested in seeing their updates, when a client receives another client’s tree (which may or may not have new nodes) they merge with their own tree in a way that guarantees a consistent result. One way to think of merge is like a set join, if client 1 has the values {a, b, c} and client 2 has {d}, merging the clients in either order will always result in the same value. But before we go deeper on the merge operation let’s take a step back and see how the tree is structured.</p><h2>Tree structure</h2><p>In our toy example above, each node represents an individual character. You may have noticed that each node’s parent is the letter that directly precedes that node (this isn’t always the case). This ordering is one half of the equation that allows us to position nodes consistently. Take for example an alternative structure where each character has an array index position instead.</p><p>h<span>⁰ </span>e<span>¹ </span>l<span>²	</span>l<span>³	</span>o<span>⁴	</span></p><p>Now imagine that you went on to add an exclamation mark at the end of the string, but concurrently with that operation you merged changes from another client that added the string “Welcome and “ before “hello”. You’re now left with “Welco!me and hello”, which was not your original intention. Now, you _could_ bake in some logic to your application that adjusts your old index to account for the new changes, shifting the index of each character in “hello” by the amount of characters that were inserted, but that’s not the elegant algorithm that I promised. That’s something else known as <a href="https://en.wikipedia.org/wiki/Operational_transformation">Operational Transformation (OT)</a> that folks that like to complicate simple things use (Google Docs uses OT heavily).</p><p>The much simpler approach taken in causal trees is to simply associate a node with the node preceding it. If we do that, it doesn’t matter if the underlying tree changes, it still preserves our original intention. In essence we’re trading an absolute positioning approach for a relative one.</p><p>But how can individual nodes reference each other? If you hover over any of the nodes below you will notice that they have two identifiers, one timestamp and one entity id. A timestamp is a monotonically increasing integer value that gets incremented each time a node is added to a tree. Because wall clocks in distributed systems are unreliable to be used to order operations, we use this method combined with the entity id when necessary to create what’s called a total order of the nodes in our tree. Those familiar with <a href="https://en.wikipedia.org/wiki/Lamport_timestamp">Lamport timestamps</a> will recognise this approach. If a tree is isolated from any other trees and you type a sequence of characters, you will produce a tree similar to a linked list, with each node’s timestamp being 1 higher than its parent. If we get sent another client’s tree that we notice has nodes with higher timestamps we merge those nodes into ours by attaching them to their parents, that will either exist in our tree or be introduced by theirs. We then simply update our local tree’s timestamp to be the max of our current timestamp and the tree’s timestamp that we are merging with. Intuitively, this means that when we start adding nodes after the merge, the id represents how much context, or amount of nodes we have seen before making the decision to add that node, in other words, what ‘caused’ the new node.</p><p>Hover over the nodes in the example below and see if you can figure out what the final value will be, based on the ids alone. Click “Reveal” if you get stuck.</p><h3>Example 2</h3><p>What's illustrated here is that higher timestamps represent operations that happened with more context, or at a later point in the document's history, and as such they are traversed first when building the final value.</p><p>So what did the user type and in what order to create the tree above?</p><ul><li>They first typed “Causatrees”</li><li>Realising they misspelled the word they added an "l " after "Causa"</li></ul><p>Notice how there are two branches after the 6th node? it makes sense we first traverse the branch with the higher timestamp because it was added after the misspelling had occurred. But there's a few more cases we need to be aware of.</p><h2>Traversal</h2><p>Which brings us to an important topic, tree traversal and sibling trees. To produce the correct final value we need to traverse the tree in depth-first pre-order, but with special cases for sibling branches. Those special cases are:</p><ul><li>If a node has multiple children (sibling branches) traverse the branches ordered by timestamp descending order. We saw this in the previous example.</li><li>If branches have the same timestamp, traverse the branch that has the higher entity id first. What you order by here isn’t important but it is critical that we order in a way that is consistent. Ordering by entity id is just an example of that.</li></ul><p>I said at the start of the article that causal trees can be used in collaborative, networked environments, which means that it needs to handle concurrent updates, duplicate writes, partitioning between clients and all of the stuff we love to think about when it comes to distributed system design. So how does it do that?</p><p>When some tree (a) merges with another tree (b), the merge operation is essentially a diff and patch. What that means is we find all of the nodes from tree ‘a’ that aren’t in ‘b’ and add them to ‘a’. This operation is idempotent, which means we can perform it any number of times and the result will be the same, like inserting a new value to a set. Two other properties that are required for a CvRDT are commutativity and associativity. Commutativity ensures that the order of merging does not affect the final result, for instance, merging tree 'a' with 'b' yields the same result as merging 'b' with 'a'. Associativity allows for grouping merge operations in any order, so combining 'a' with 'b', and then with 'c', gives the same result as merging 'a' with the result of merging 'b' with 'c'. When each tree (or node) in a distributed system observes the same set of changes, even if in different orders, they will eventually converge to the same consistent state due to these properties. Let’s look at an example.</p><p>In the example below we have three clients, the first of which has written “I &lt;3” and then gone offline. Concurrently, after receiving the first client’s changes, client 2 and 3 type “ Pears” and “ Apples” respectively. We can see these concurrent changes represented by the sibling trees after the heart. Instead of having both changes interleaved to produce a jumbled concoction of applepear letter-soup, client 1 (and all clients after merging) is left with text that represents both changes. The ids at the sibling branch/fork are the same, which, again, shows what context each client had available to them when they made their change.</p><h3>Example 3</h3><p>This demonstration showcases how causal trees can resolve concurrent updates in a consistent manner. Whether you would want to do that instead of showing conflicts to the user for them to resolve themselves is up to the application.</p><h2>Deletion</h2><p>You may have noticed by now that if you delete a node, the value of that node changes to a 🪦. When a node is deleted we need to keep it around for consistency reasons, but the value is ignored when we create the final value from traversing the tree. This is a common technique found in distributed database design and CRDT designs known as tombstoning. I’ll leave it as an exercise to the reader to try to understand how consistency would be impacted if we removed the node completely from the tree on deletion.</p><h2>Conclusion</h2><p>And that’s a wrap! hopefully you’ve learnt something new about CRDTs! If you’re wondering about real-world implementations you will be happy to know that the ideas we talked about today are integral to the design of such popular general purpose CRDT libraries as <a href="https://github.com/automerge/automerge">Automerge</a> and <a href="https://github.com/yjs/yjs">Yjs</a>.</p><p>If you’re curious about tree size implications, optimizations, and things like node garbage collection, then please check the further reading list below, there’s some gems in there I think you will enjoy.</p><h2>Further reading</h2><ul><li><a href="https://josephg.com/blog/crdts-go-brrr/">CRDTs go brr (optimization exploration).</a></li><li><a href="https://web.archive.org/web/20190505005829/http://www.st.ewi.tudelft.nl/victor/articles/ctre.pdf">The original Causal Tree paper.</a></li><li><a href="http://archagon.net/blog/2018/03/24/data-laced-with-history/">Data laced with history (exploration of causal trees and optimizations).</a></li><li><a href="https://github.com/sno6/causal/blob/main/simpletree/simpletree.go">Educational Go implementation that I wrote.</a></li></ul><h2>Thanks</h2><p>Thanks to the folks at <a href="https://reactflow.dev/">React Flow</a> for letting me use a free Pro account to build the interactive demos.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Advancements in machine learning for machine learning (214 pts)]]></title>
            <link>https://blog.research.google/2023/12/advancements-in-machine-learning-for.html</link>
            <guid>38661296</guid>
            <pubDate>Sat, 16 Dec 2023 02:50:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2023/12/advancements-in-machine-learning-for.html">https://blog.research.google/2023/12/advancements-in-machine-learning-for.html</a>, See on <a href="https://news.ycombinator.com/item?id=38661296">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-6088118107306075362">
<p><span>Posted by Phitchaya Mangpo Phothilimthana, Staff Research Scientist, Google DeepMind, and Bryan Perozzi, Senior Staff Research Scientist, Google Research</span>

</p><p>
With the recent and accelerated advances in machine learning (ML), machines can <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">understand natural language</a>, <a href="https://blog.google/technology/ai/lamda/">engage in conversations</a>, <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview">draw images</a>, <a href="https://arxiv.org/abs/2210.02303">create videos</a> and more. Modern ML models are programmed and trained using ML programming frameworks, such as <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://github.com/google/jax">JAX</a>, <a href="https://pytorch.org/">PyTorch</a>, among many others. These libraries provide high-level instructions to ML practitioners, such as linear algebra operations (e.g., matrix multiplication, convolution, etc.) and neural network layers (e.g., <a href="https://keras.io/api/layers/convolution_layers/convolution2d/">2D convolution layers</a>, <a href="https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/">transformer layers</a>). Importantly, practitioners need not worry about how to make their models run efficiently on hardware because an ML framework will automatically optimize the user's model through an underlying <em>compiler</em>. The efficiency of the ML workload, thus, depends on how good the compiler is. A compiler typically relies on heuristics to solve complex optimization problems, often resulting in suboptimal performance.
</p> <p>
In this blog post, we present exciting advancements in ML for ML. In particular, we show how we use ML to improve efficiency of ML workloads! Prior works, both internal and external, have shown that we can use ML to improve performance of ML programs by selecting better ML compiler decisions. Although there exist a few datasets for program performance prediction, they target small sub-programs, such as basic blocks or kernels. We introduce “<a href="https://arxiv.org/abs/2308.13490">TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs</a>” (presented at <a href="https://nips.cc/Conferences/2023">NeurIPS 2023</a>), which we recently released to fuel more research in ML for program optimization. We hosted a <a href="https://www.kaggle.com/competitions/predict-ai-model-runtime/overview">Kaggle competition</a> on the dataset, which recently completed with 792 participants on 616 teams from 66 countries. Furthermore, in “<a href="https://arxiv.org/abs/2305.12322">Learning Large Graph Property Prediction via Graph Segment Training</a>”, we cover a novel method to scale <a href="https://arxiv.org/abs/2005.03675">graph neural network</a> (GNN) training to handle large programs represented as graphs. The technique both enables training arbitrarily large graphs on a device with limited memory capacity and improves generalization of the model.
</p>





<h2>ML compilers</h2>


<p>
ML compilers are software routines that convert user-written programs (here, mathematical instructions provided by libraries such as TensorFlow) to executables (instructions to execute on the actual hardware). An ML program can be represented as a computation graph, where a node represents a tensor operation (such as <a href="https://en.wikipedia.org/wiki/Matrix_multiplication">matrix multiplication</a>), and an edge represents a tensor flowing from one node to another. ML compilers have to solve many complex optimization problems, including <em>graph-level </em>and <em>kernel-level</em> optimizations. A graph-level optimization requires the context of the entire graph to make optimal decisions and transforms the entire graph accordingly. A kernel-level optimization transforms one kernel (a fused subgraph) at a time, independently of other kernels.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s1999/image6.png"><img data-original-height="465" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s16000/image6.png"></a></td></tr><tr><td>Important optimizations in ML compilers include graph-level and kernel-level optimizations.</td></tr></tbody></table>


<p>
To provide a concrete example, imagine a <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">matrix</a> (2D tensor):
</p>





<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s1999/image4.png"><img data-original-height="290" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s16000/image4.png"></a></p>



<p>
It can be stored in computer memory as [A B C a b c] or [A a B b C c], known as <a href="https://en.wikipedia.org/wiki/Row-_and_column-major_order">row- and column-major memory layout</a>, respectively. One important ML compiler optimization is to assign memory layouts to all intermediate tensors in the program. The figure below shows two different layout configurations for the same program. Let’s assume that on the left-hand side, the assigned layouts (in red) are the most efficient option for each individual operator. However, this layout configuration requires the compiler to insert a <em>copy</em> operation to transform the memory layout between the <em>add</em> and <em>convolution</em> operations. On the other hand, the right-hand side configuration might be less efficient for each individual operator, but it doesn’t require the additional memory transformation. The layout assignment optimization has to trade off between local computation efficiency and layout transformation overhead.
</p>





<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s1999/image3.png"><img data-original-height="343" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s16000/image3.png"></a></td></tr><tr><td>A node represents a tensor operator, annotated with its output tensor shape [<em>n<sub>0</sub></em>, <em>n<sub>1</sub></em>, ...], where <em>n<sub>i </sub></em>is the size of dimension <em>i</em>. Layout {<em>d<sub>0</sub></em>, <em>d<sub>1</sub></em>, ...} represents minor-to-major ordering in memory. Applied configurations are highlighted in red, and other valid configurations are highlighted in blue. A layout configuration specifies the layouts of inputs and outputs of influential operators (i.e., convolution and reshape). A copy operator is inserted when there is a layout mismatch.</td></tr></tbody></table>



<p>
If the compiler makes optimal choices, significant speedups can be made. For example, we have seen <a href="https://ieeexplore.ieee.org/document/9563030">up to a 32% speedup</a> when choosing an optimal layout configuration over the default compiler’s configuration in the <a href="https://www.tensorflow.org/xla">XLA</a> benchmark suite.
</p>




<h2>TpuGraphs dataset</h2>


<p>
Given the above, we aim to improve ML model efficiency by improving the ML compiler. Specifically, it can be very effective to equip the compiler<strong> </strong>with a <a href="https://arxiv.org/abs/2008.01040">learned cost model</a><strong> </strong>that takes in an input program and compiler configuration and then outputs the predicted runtime of the program. 
</p>
<p>
With this motivation, we <a href="https://arxiv.org/abs/2308.13490">release TpuGraphs</a>, a dataset for learning cost models for programs running on Google’s custom <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">Tensor Processing Units</a> (TPUs). The dataset targets two XLA compiler configurations: <em>layout</em> (generalization of row- and column-major ordering, from matrices, to higher dimension tensors) and <em>tiling</em> (configurations of tile sizes). We provide download instructions and starter code on the <a href="https://github.com/google-research-datasets/tpu_graphs">TpuGraphs GitHub</a>. Each example in the dataset contains a computational graph of an ML workload, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the dataset are collected from open-source ML programs, featuring popular model architectures, e.g., <a href="https://arxiv.org/abs/1512.03385">ResNet</a>, <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a>, <a href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a>, and <a href="https://arxiv.org/abs/1706.03762">Transformer</a>. The dataset provides 25× more graphs than the largest (earlier) graph property prediction dataset (with comparable graph sizes), and graph size is 770× larger on average compared to existing performance prediction datasets on ML programs. With this greatly expanded scale, for the first time we can explore the graph-level prediction task on large graphs, which is subject to challenges such as scalability, training efficiency, and model quality.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s2868/image18.png"><img data-original-height="1546" data-original-width="2868" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s16000/image18.png"></a></td></tr><tr><td>Scale of TpuGraphs compared to other graph property prediction datasets.</td></tr></tbody></table>




<p>
We provide baseline learned cost models with our dataset (architecture shown below). Our baseline models are based on a GNN since the input program is represented as a graph. Node features, shown in blue below, consist of two parts. The first part is an <em>opcode id</em>, the most important information of a node, which indicates the type of tensor operation. Our baseline models, thus, map an opcode id to an <em>opcode embedding</em> via an embedding lookup table. The opcode embedding is then concatenated with the second part, the rest of the node features, as inputs to a GNN. We combine the node embeddings produced by the GNN to create the fixed-size embedding of the graph using a simple graph pooling reduction (i.e., sum and mean). The resulting graph embedding is then linearly transformed into the final scalar output by a feedforward layer.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s2284/image20.png" imageanchor="1"><img data-original-height="1106" data-original-width="2284" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s16000/image20.png"></a></td></tr><tr><td>Our baseline learned cost model employs a GNN since programs can be naturally represented as graphs.</td></tr></tbody></table>




<p>
Furthermore we present <a href="https://arxiv.org/abs/2305.12322">Graph Segment Training</a> (GST), a method for scaling GNN training to handle large graphs on a device with limited memory capacity in cases where the prediction task is on the entire-graph (i.e., graph-level prediction). Unlike scaling training for node- or edge-level prediction, scaling for graph-level prediction is understudied but crucial to our domain, as computation graphs can contain hundreds of thousands of nodes. In a typical GNN training (“Full Graph Training”, on the left below), a GNN model is trained using an entire graph, meaning all nodes and edges of the graph are used to compute gradients. For large graphs, this might be computationally infeasible. In GST, each large graph is partitioned into smaller segments, and a random subset of segments is selected to update the model; embeddings for the remaining segments are produced without saving their intermediate activations (to avoid consuming memory). The embeddings of all segments are then combined to generate an embedding for the original large graph, which is then used for prediction. In addition, we introduce the historical embedding table to efficiently obtain graph segments’ embeddings and segment dropout to mitigate the staleness from historical embeddings. Together, our complete method speeds up the end-to-end training time by 3×.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s790/image2.png"><img data-original-height="434" data-original-width="790" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s16000/image2.png"></a></td></tr><tr><td>Comparing Full Graph Training (typical method) vs Graph Segment Training (our proposed method).</td></tr></tbody></table>




<h2>Kaggle competition</h2>


<p>
Finally, we ran the “<a href="https://kaggle.com/competitions/predict-ai-model-runtime">Fast or Slow? Predict AI Model Runtime</a>” competition over the TpuGraph dataset. This competition ended with 792 participants on 616 teams. We had 10507 submissions from 66 countries. For 153 users (including 47 in the top 100), this was their first competition. We learned many interesting new techniques employed by the participating teams, such as:
</p>
<ul>

<li><em>Graph pruning / compression</em>: Instead of using the GST method, many teams experimented with different ways to compress large graphs (e.g., keeping only subgraphs that include the configurable nodes and their immediate neighbors).

</li><li><em>Feature padding value</em>: Some teams observed that the default padding value of 0 is problematic because 0 clashes with a valid feature value, so using a padding value of -1 can improve the model accuracy significantly.

</li><li><em>Node features</em>: Some teams observed that additional node features (such as <a href="https://www.tensorflow.org/xla/operation_semantics#dot">dot general’s contracting dimensions</a>) are important. A few teams found that different encodings of node features also matter.

</li><li><em>Cross-configuration attention</em>: A winning team designed a simple layer that allows the model to explicitly "compare" configs against each other. This technique is shown to be much better than letting the model infer for each config individually. 
</li>
</ul>
<p>
We will debrief the competition and preview the winning solutions at the competition session at the <a href="https://mlforsystems.org/">ML for Systems workshop</a> at NeurIPS on December 16, 2023. Finally, congratulations to all the winners and thank you for your contributions to advancing research in ML for systems!
</p>





<h2>NeurIPS expo</h2>


<p>
If you are interested in more research about structured data and artificial intelligence, we hosted the NeurIPS Expo panel <a href="https://nips.cc/Expo/Conferences/2023/talk%20panel/78252">Graph Learning Meets Artificial Intelligence</a> on December 9, which covered advancing learned cost models and more! 
</p>




<h2>Acknowledgements</h2>


<p>
<em>Sami Abu-el-Haija (Google Research) contributed significantly to this work and write-up.   The research in this post describes joint work with many additional collaborators including Mike Burrows, Kaidi Cao, Bahare Fatemi, Jure Leskovec, Charith Mendis, Dustin Zelle, and Yanqi Zhou.</em>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bluetooth keystroke-injection in Android, Linux, macOS and iOS (215 pts)]]></title>
            <link>https://github.com/skysafe/reblog/tree/main/cve-2023-45866</link>
            <guid>38661182</guid>
            <pubDate>Sat, 16 Dec 2023 02:26:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/skysafe/reblog/tree/main/cve-2023-45866">https://github.com/skysafe/reblog/tree/main/cve-2023-45866</a>, See on <a href="https://news.ycombinator.com/item?id=38661182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:skysafe/reblog" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="IGY1jHWvj45J7PAhwmSm3IcdAgcoSkzoDHuOMXnnxEauLpqeRxlktc30r9E6XkyKEnfEhMXshXrYQTnMOd0Ehg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="skysafe/reblog" data-current-org="skysafe" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=toL9V8ml0zAn3gyLixuwanlGPvABX29m4y6axKT6ADVEjH%2BuLJR8xyvWLnEkNA4snWBUWa6l8P%2FpF%2FqMgUQABQ%2B3BHZBAA9lqAkIEbLtsrZLoxJPFFyV6uU4%2FiDs1h9p6S7O7W5ZDMdAnZc93EOuvBZcMn9YQoJJ0FGvBofnKfScuEUcupBsn3Q%2FIlxhy%2F4uPxsPpC5fFTA%2FhNXYWoWKslM3bsJq0dXRv5eAS9Yo%2FpF30ZqlMkC3%2FXeFZ035pO5xWQlmoDd3Kpm83zRwBGqh8v%2FBJaR1lbtCSlaO1pSho2Ap9oml%2BQazM0LtupFlArIPbYFZxvGaMwTBvdwE51c6mOt232n73aZh38jQ8NpXYTEw7SChMz0gdFR%2B8gWUGbRd%2FHkNlmXRw9Gf4OkrzJRIwVGbe6IKanPl8gTJITm3S5U1dIkH8j2pRVCbqupbP0HLQa4E5rJ%2F8grpMc1iPmuRHo%2FpDk8HXdm%2BXbaYxGhFBPy%2Fmbd%2Bg2lTlJhwMTkb6zpxwcUwWhSjy%2B3q6sZSgX%2F7VOTkR6ZxV0Qjqys1aCEf3ruShkX1MnH1ZubU--mPjgXi7ZKrurNwYN--QRIQ1I18v%2BMEZ6XhBYKZHA%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Ffiles%2Fdisambiguate&amp;source=header-repo&amp;source_repo=skysafe%2Freblog" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/skysafe/reblog/tree/main/cve-2023-45866&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="2a181ee65596245633167bff86efb2eedc190dbf33fef3e419c987eda2894bcf" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/files/disambiguate;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Location history data in Google Maps will soon be stored on user devices (136 pts)]]></title>
            <link>https://www.businessinsider.com/google-maps-location-data-history-stored-locally-2023-12</link>
            <guid>38660646</guid>
            <pubDate>Sat, 16 Dec 2023 00:39:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/google-maps-location-data-history-stored-locally-2023-12">https://www.businessinsider.com/google-maps-location-data-history-stored-locally-2023-12</a>, See on <a href="https://news.ycombinator.com/item?id=38660646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="piano-inline-content-wrapper" data-piano-inline-content-wrapper=""> 
                    
                    
                    
                          
                          
                          <section data-offer-key="pre-churn-offer" data-component-type="inline-offer" data-place-after-element-selector=".post-content .content-lock-content > p">
                            <article>
                              <img src="https://www.businessinsider.com/public/assets/subscription/marketing/banner-overlay/top-left.svg" alt="">
                              <img src="https://www.businessinsider.com/public/assets/subscription/marketing/banner-overlay/bottom-right.svg" alt="">
                          
                                        </article>
                          </section>
                    
                    <div data-component-type="content-lock" data-load-strategy="exclude">
                                  <ul><li>Location history data in <a target="_blank" href="https://www.businessinsider.com/google-maps-best-features-tips-tricks-2019-5" data-analytics-product-module="summary_bullets" rel="">Google Maps</a> will soon be stored directly on user devices.</li><li>Google itself will no longer have access to the data.</li><li>This also means law enforcement won't be able to request it from Google anymore.</li></ul><!-- Excluded mobile ad on desktop --><div id="formContainer" data-component-type="inline-newsletter-module" data-event-label="insider_today" data-newsletter-id="1" data-newsletter-title="Insider Today" data-acq-source="techinlinesignup">
                        
                        
                          <div>
                              <p>Thanks for signing up!</p>
                              
                              <p>
                              Access your favorite topics in a personalized feed while you're on the go.
                                    </p>
                            </div>
                        
                            <div>
                                <p><img src="https://www.businessinsider.com/public/assets/rebrand/newsletter-bull.png" data-old-src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1 1'%3E%3C/svg%3E" data-src="/public/assets/rebrand/newsletter-bull.png">
                              
                              
                              
                              </p>    </div>
                        
                          
                        </div><p>Google is making some changes in Google Maps that will increase user privacy.</p><p>Data from the Timeline feature in Google Maps, which is controlled by the Location History setting and keeps a record of routes and trips users have taken, will soon be stored directly on users' devices instead of by Google.</p><p>That means Google itself will no longer have access to user location history data. And by extension, neither will law enforcement, which has often <a target="_blank" href="https://www.businessinsider.com/police-getting-help-social-media-to-prosecute-people-seeking-abortions-2023-2" data-analytics-product-module="body_link" rel="">requested user location data from Google</a> — for example, through "geofence" orders, which request data about every user who was near a specific place at a specific time.</p><p><span></span><span><p>Google has come under increasing pressure to stop collecting user location data, especially since <a target="_blank" href="https://www.businessinsider.com/supreme-court-abortion-decision-final-ruling-scotus-roe-v-wade-2022-6#:~:text=The%20Supreme%20Court%20overturned%20the,Friday%2C%20while%20Republicans%20celebrated%20it." data-analytics-product-module="body_link" rel="">Roe v. Wade was overturned</a>. Location data, along with internet search history and even <a target="_blank" href="https://www.businessinsider.com/roe-abortion-surveillance-location-data-scotus-computer-search-history-2022-6" data-analytics-product-module="body_link" rel="">messaging history can be used as criminal evidence</a> against individuals who get an abortion in states where abortion is illegal.</p><p>42 Democrats from the US House and Senate signed a letter last May addressed to Google CEO Sundar Pichai urging the company to <a target="_blank" href="https://www.businessinsider.com/democrats-demand-google-stop-collecting-location-data-abortion-rights-2022-5" data-analytics-product-module="body_link" rel="">stop collecting and retaining user location information</a>.</p><p>"Google's current practice of collecting and retaining extensive records of cell phone location data will allow it to become a tool for far-right extremists looking to crack down on people seeking reproductive health care," the letter read.</p><p>Last July, Google announced it would <a target="_blank" href="https://www.businessinsider.com/google-says-delete-location-data-of-users-visiting-abortion-clinics-2022-7" data-analytics-product-module="body_link" rel="">delete the location history data of users who visited abortion clinics</a>, drug treatment centers, domestic violence shelters, weight loss clinics, and other sensitive health-related locations. The company said that if its systems identified that a user had visited one of these sensitive locations, it would then delete the entry from that user's location history "soon after they visit."</p><p>Now this control is back in the hands of individual users.</p><p>Google told Business Insider that the update is part of a larger effort by the company to increase user privacy and give individuals more control over their data, pointing to other tools like auto-delete and Incognito Mode. It says the response to the Location History update has been positive.</p><p>The Location History setting is turned off by default in Google Maps, but here's how to find it in the app, toggle it on or off, and delete specific entry:</p><p><strong>1) Click on the icon in the top right corner of the screen.</strong></p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/png" data-srcs="{&quot;https://i.insider.com/657c875f0ec98e92f75000a0&quot;:{&quot;contentType&quot;:&quot;image/png&quot;,&quot;aspectRatioW&quot;:1179,&quot;aspectRatioH&quot;:2556}}" alt="Google Maps" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Click on your user icon.
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Kylie Kirschner
                          
                          </span>
                              </span>
                          </figure><p><strong>2) Click on "Your data in Maps."</strong></p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/png" data-srcs="{&quot;https://i.insider.com/657c89237a3c8094d5dd8cf4&quot;:{&quot;contentType&quot;:&quot;image/png&quot;,&quot;aspectRatioW&quot;:1179,&quot;aspectRatioH&quot;:2556}}" alt="Google Maps" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Find "Your data in Maps."
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Kylie Kirschner
                          
                          </span>
                              </span>
                          </figure><p><strong>3) Scroll down to "Location History."</strong></p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/png" data-srcs="{&quot;https://i.insider.com/657c88c950edbc52a864cb77&quot;:{&quot;contentType&quot;:&quot;image/png&quot;,&quot;aspectRatioW&quot;:1179,&quot;aspectRatioH&quot;:2556}}" alt="Google maps" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Find "Location History."
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Kylie Kirschner
                          
                          </span>
                              </span>
                          </figure><p>Here you can turn the Timeline feature and your location history on or off, and change your backup and auto-delete settings.</p><p>Clicking on "See &amp; delete activity" will allow you to see any location history that's already been saved in Google Maps and give you the option to delete specific entries.</p></span></p><!-- Excluded mobile ad on desktop --><!-- Excluded mobile ad on desktop -->
                          
                        <!-- Excluded mobile ad on desktop -->
                          
                        
                          
                        
                      </div>
                    
                    
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New plans for self-hosted Zulip customers (109 pts)]]></title>
            <link>https://blog.zulip.com/2023/12/15/new-plans-for-self-hosted-customers/</link>
            <guid>38659529</guid>
            <pubDate>Fri, 15 Dec 2023 22:18:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.zulip.com/2023/12/15/new-plans-for-self-hosted-customers/">https://blog.zulip.com/2023/12/15/new-plans-for-self-hosted-customers/</a>, See on <a href="https://news.ycombinator.com/item?id=38659529">Hacker News</a></p>
<div id="readability-page-1" class="page"><section> <p><a href="https://zulip.com/">Zulip</a> is a 100% open-source team chat application designed
for efficient communication. Zulip is available as a
<a href="https://zulip.com/plans/">cloud service</a> or a
<a href="https://zulip.com/self-hosting/">self-hosted solution</a>. Organizations that
self-host Zulip can do so on their own (relying on
<a href="https://zulip.readthedocs.io/en/stable/production/index.html">extensive documentation</a>
and best-effort <a href="https://zulip.com/development-community/">community</a> support),
or purchase commercial support for their installation. You can learn more about
Zulip in
<a href="https://blog.zulip.com/2023/12/15/zulip-8-0-released/">today’s blog post</a>
announcing the 8.0 release.</p>
<h2 id="what-is-changing">What is changing</h2>
<p>Starting today, new self-hosted customers will no longer get unlimited free
access to
<a href="https://zulip.readthedocs.io/en/latest/production/mobile-push-notifications.html">Zulip’s Mobile Push Notification Service</a>.
Organizations with more than 10 users will need to sign up for a plan in order
to access the service. For current users of the service, unlimited free access
will end on <strong>February 15, 2024.</strong></p>
<p>We now offer two <a href="https://zulip.com/plans/#self-hosted">new plans</a> that include
unlimited access to mobile push notifications. The <strong>Business</strong> plan also
includes expert commercial support. The <strong>Community</strong> plan is free of charge,
and is
<a href="https://zulip.com/help/self-hosted-billing#free-community-plan">available</a> to
open-source projects, research groups, and communities.</p>
<p>As always, we remain fully <a href="https://zulip.com/values/">committed</a> to Zulip’s
100% open-source model. Organizations that do not require technical support or
services from us can freely install and use a complete version of Zulip. We are
not turning Zulip into a proprietary product with an “open core” demo version —
Zulip’s open-source software <em>is the product</em>.</p>
<h2 id="why-we-are-introducing-a-new-business-plan">Why we are introducing a new Business plan</h2>
<p>Our aim is to make signing up for the new Business plan be the norm for
businesses relying on self-hosted Zulip as their mission-critical communication
platform. This will generate revenue for the company that stewards and
financially supports Zulip’s development.</p>
<p>Zulip is proudly independent, with
<a href="https://zulip.com/values/#building-a-sustainable-business-aligned-with-our-values">no venture capital funding</a>,
which means that our revenue strongly impacts the pace of Zulip’s development.
In addition to supporting the project as a whole, we expect this change will let
us dedicate more resources to product improvements requested by Zulip’s
self-hosted customers.</p>
<p>We also believe the new Business plan will establish a better framework for our
relationship with self-hosted customers. Until now, purchasing commercial
support required a conversation with our sales team. In practice, this process
felt too heavy for most small- and medium-sized businesses, and they would
instead end up self-managing their Zulip installation.</p>
<p>Self-managing might be OK for a while, but an extended team chat outage can stop
work in its tracks; Slack incidents regularly trigger a
<a href="https://www.nytimes.com/2023/07/27/technology/slack-down.html">flurry</a>
<a href="https://www.forbes.com/sites/siladityaray/2023/07/27/office-messaging-app-slack-is-down-company-says-its-investigating/">of</a>
<a href="https://www.theverge.com/2023/8/17/23835971/slack-major-issues-outage-send-messages">media</a>
<a href="https://www.independent.co.uk/tech/slack-down-not-working-status-latest-b2425211.html">coverage</a>
about the disruption. This puts a big burden on system administrators who are
responsible for securing, maintaining, and upgrading a Zulip server on their
own.</p>
<p>Commercial support will now be included when organizations buy Business plan
access to
<a href="https://zulip.readthedocs.io/en/latest/production/mobile-push-notifications.html">Zulip’s Mobile Push Notification Service</a>.
Our support team can answer questions about installation and upgrades, provide
guidance in tricky situations, and help avoid painful complications before they
happen. We can also guide organizations on how best to use dozens of Zulip
features and configuration options.</p>
<h2 id="pricing">Pricing</h2>
<p>The Business plan is priced at $6.67/user/month with annual billing (or $8/month
billed monthly), which is the same price as
<a href="https://zulip.com/plans/">Zulip Cloud Standard</a>. This model lets organizations
budget for using Zulip without having to decide whether self-hosting or Zulip
Cloud will <a href="https://zulip.com/help/zulip-cloud-or-self-hosting">better suit</a>
their needs. On our side, the resources required to provide an excellent support
experience for a wide range of self-hosted configurations more than balance
Zulip Cloud hosting costs.</p>
<p>We offer a 30-day free trial of Zulip Business to make it convenient to run a
full-scale Zulip evaluation with your team, and never have to pay if you go with
a different chat product.</p>
<p>We created Zulip to empower teams to collaborate effectively, and are confident
that the <a href="https://zulip.com/why-zulip/">increased productivity from using Zulip</a>
makes our plans well worthwhile for organizations that pay people to get work
done.</p>
<p>As an example, the annual plan will pay for itself if an employee making
$65K/year saves just <em>3 minutes per week</em> [1] by using Zulip. Put another way,
the cost of the plan is <em>0.12%</em> of that employee’s direct compensation, not even
counting overhead.</p>
<blockquote>
<p>“In fact now it seems strange to me to just fire off messages in Slack with no
subject – that’s chaos, madness. The genius of subject lines is that you can
quickly and easily catch up on the messages you missed in your off-hours… This
feature alone saves me hours a week.”</p>
<p><span>—
<a href="https://www.theregister.com/2021/07/28/zulip_open_source_chat_collaboration_software/">Zulip review in <em>The Register</em></a></span></p>
</blockquote>
<p>In addition to providing a free Community plan, we offer
<a href="https://zulip.com/help/self-hosted-billing#business-plan-discounts">generous discounts</a>
for organizations whose circumstances make the Business plan pricing hard to
afford. Please reach out to <a href="mailto:sales@zulip.com">sales@zulip.com</a> if you
have any questions.</p>
<h2 id="why-a-mobile-push-notification-service-is-needed">Why a mobile push notification service is needed</h2>
<p>As Zulip’s maintainers, we publish the
<a href="https://github.com/zulip/zulip-mobile#readme">open-source</a> Zulip mobile apps
for <a href="https://zulip.com/apps/ios">iOS</a> and
<a href="https://zulip.com/apps/android">Android</a>, and offer a
<a href="https://zulip.readthedocs.io/en/latest/production/mobile-push-notifications.html">service</a>
that allows self-hosted Zulip servers to send mobile push notifications to their
users. Mobile push notifications must be centrally managed due to iOS and
Android
<a href="https://zulip.readthedocs.io/en/latest/production/mobile-push-notifications.html#why-a-push-notification-service-is-necessary">app store policies</a>.</p>
<p>We will continue to offer the
<a href="https://zulip.readthedocs.io/en/latest/production/mobile-push-notifications.html">Zulip’s Mobile Push Notification Service</a>
free of charge for organizations with up to 10 users, and for organizations
eligible for the Community plan.</p>
<h2 id="what-these-changes-mean-for-self-hosted-customers">What these changes mean for self-hosted customers</h2>
<p>If you are a current user of the Mobile Push Notification Service, you will
continue to have unlimited free access to the service until <strong>February
15, 2024.</strong> Starting today, you also can:</p>
<ul>
<li>
<p><strong>Do nothing.</strong> We remain <a href="https://zulip.com/values/">committed</a> to Zulip’s
100% open source model, so you do not have to purchase a plan in order to use
Zulip. We are not turning Zulip into a proprietary product with an “open core”
demo version — Zulip’s open-source software <em>is the product</em>. This option
doesn’t include support, and access to the Mobile Push Notification Service
will end on February 15, 2024 for organizations with more than 10 users.</p>
</li>
<li>
<p><strong>Apply for Zulip Community.</strong> The Community plan is free of charge, and
<a href="http://(details)/">available</a> to open-source projects, research groups, and
community organizations. It includes unlimited push notifications and
community support for many Zulip features.</p>
</li>
<li>
<p><strong>Upgrade to Zulip Business or Zulip Enterprise.</strong> Both plans include
unlimited access to the Mobile Push Notification Service, and commercial
support for dozens of features and integrations that help businesses take full
advantage of their Zulip implementation. Zulip Enterprise additionally
includes real-time support during installation and upgrades, technical support
for advanced deployment options, and more. Discounts are available for
education, non-profits, customers based in the developing world, and other
situations where the standard Zulip Business pricing would be a stretch for an
organization.</p>
</li>
<li>
<p><strong>Move to Zulip Cloud</strong>. The Business plan is priced the same as
<a href="https://zulip.com/plans/">Zulip Cloud Standard</a>, which we hope will help you
<a href="https://zulip.com/help/zulip-cloud-or-self-hosting">choose</a> between
self-hosting and Zulip Cloud based on what best suits your needs. If you would
like to switch to Zulip Cloud hosting, with its professionally managed, always
up-to-date experience, our high-quality
<a href="https://zulip.com/help/export-your-organization">export</a> tools make it easy
to move your organization.</p>
</li>
</ul>
<p>We’ve put a lot of care into designing a pricing model that we believe offers
great value compared to other team chat options on the market, and allows
organizations without an IT budget to continue using Zulip unchanged, for free.
If this announcement makes you consider migrating off Zulip, please
<a href="mailto:sales@zulip.com">get in touch with us</a> before you make your decision —
we may be able to help!</p>
<h3 id="next-steps">Next steps</h3>
<p>You can <a href="https://zulip.com/plans/#self-hosted">learn more</a> about the new plans,
and <a href="https://zulip.com/help/self-hosted-billing">read</a> detailed instructions on
how to sign up. When you upgrade to the Business plan, you can start the plan
right away (if you’d like support to start immediately), or schedule a February
15 start date.</p>
<p>Upgrading your Zulip server to version 8.0
(<a href="https://blog.zulip.com/2023/12/15/zulip-8-0-released/">released today</a>!) makes
it significantly more convenient to manage your plan, but you do <em>not</em> have to
upgrade your Zulip installation in order to sign up for a plan. The same plans
are offered for all Zulip versions.</p>
<p>If you wish to apply for the Community plan or a discounted Business plan, we
strongly recommend submitting your application by January 31 in case additional
information is required before your application is approved.</p>
<h3 id="let-us-know-if-we-can-help">Let us know if we can help</h3>
<p>We’d love to work with you to make the transition as smooth as possible for your
organization. If you have any questions or concerns about the next steps, the
timeline, which plan is appropriate for you, or anything else, send us a note at
<a href="mailto:sales@zulip.com">sales@zulip.com</a>!</p>
<p>[1] $80 annual Business plan cost / ($65,000 salary per year / 2400 work minutes
in a week) = 2.95 minutes</p> </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Smallest USB-C MIDI Synth (228 pts)]]></title>
            <link>https://mitxela.com/projects/smsc</link>
            <guid>38658497</guid>
            <pubDate>Fri, 15 Dec 2023 20:36:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitxela.com/projects/smsc">https://mitxela.com/projects/smsc</a>, See on <a href="https://news.ycombinator.com/item?id=38658497">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mxmain"><p><a href="https://mitxela.com/projects/hardware"><img onload="this.style.opacity=1;" src="https://mitxela.com/img/titles/mitxela_dot_com-65.png" title="Back to Hardware" alt="Back to Hardware"></a></p><p>15 Dec 2023<br><b>Progress: Complete</b></p><p>
A new entrant in my series of "smallest and worst" MIDI synthesizers. (I'm not including the <a href="https://mitxela.com/projects/flash_synth">flash synth</a> in that list, which isn't supposed to be the worst!)</p><p>

Here's a video of the creation:</p><p>

<iframe width="704" height="396" src="https://www.youtube.com/embed/bmFmsn6VZSM" allowfullscreen=""></iframe></p><h3>Story</h3><p>
The last few weeks I've been dabbling around with the CH32V003, a 32-bit RISC-V microcontroller that's unbelievably cheap.</p><p>

One of the first things that occurred to me, when I noticed it didn't have hardware USB but the processor is clocked at 48MHz, is that it would be awesome to write a software USB stack for it. I have wanted, for a long time, to dig deep and write a bit-banged USB library, just because it's the best way to learn. I greatly enjoyed writing my <a href="https://mitxela.com/projects/kiloboot">ethernet bootloader in assembly</a>. It's hard to justify writing a USB stack from scratch when one already exists, however, so when I saw the CH32V003 I thought this was the perfect time to do something both educational and <i>useful</i>.</p><p>

Picture my surprise to find that CNLohr has <a href="https://github.com/cnlohr/rv003usb">already done it</a>! I can't exactly complain, that's a fantastic achievement and makes the chip even more useful and impressive than it already was.</p><p>

The very least I can do is get some USB-MIDI working on the chip. At the time of writing, the USB-MIDI demo was unfinished, so I tried it out by soldering a dev board together. It started out a little smaller, but by the end of the experimentation my board looked like this:</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/ch32v003-usb1.jpg" alt="Messy dev board creation"></p><p>

That's a TSOP20 breakout and a Micro-USB breakout, superglued together with some other scrap circuit board. A regulator, capacitors and a few resistors complete the circuit. The two header pins are my programming header (though with the USB plugged in, we don't even need to connect the ground pin, and could get away with just one pin to program).</p><p>

On the right is a set of buttons. I configured the USB-MIDI device to play notes when these buttons are pressed.</p><p>

At the bottom is a piezo buzzer. Naturally, when MIDI data arrives at the chip, it produces a square wave. I did this with one of the hardware timers of the chip, outputting in differential mode to maximise volume.</p><p>

USB MIDI messages are four bytes, and our low-speed USB endpoint can be eight bytes, so we could (and normally would) send two MIDI messages per packet. However for this simple demo I just blocked until the next USB interrupt for each message.</p><p>

A bit of MIDI loopback on the host side, and we have a really terrible toy keyboard!</p><h3>USB dev board</h3><p>
There are a few dev boards available for the CH32V003, but it doesn't look like any of them wire up the USB pins, probably because there's no hardware USB. I doubt this is the last USB project I'll do with it, so to avoid having to repeatedly wire up that mess above, a simple dev board is in order. I tried to make it as small as possible.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/ch32v003-usb3.png" alt="KiCad screenshot of dev board"></p><p>

All the necessary pins are broken out, all pins are labelled on both sides. The 1.5K resistor can be soldered in one of two positions, either to D5, or direct to VDD if you don't need software reconnection of USB and want to use D5 for something else. On the underside, the USB data lines can be cut and series resistors can be added, if needed.</p><p>

The three pins in the top right corner are 3V3, GND and D1, which is what I've been using as a programming header. You can either connect all three (to program it with USB unplugged), or D1 and ground, or just D1 if the programmer is on the same machine that the USB is plugged into.</p><p>

The pins around the edge are .1 inch pitch, and the board is 15.2mm by 20.3mm total.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/ch32v003-usb2.jpg" alt="Clean dev board creation"></p><p>

Here's the obligatory 3D model of the board:</p><p>


<model-viewer src="/img/uploads/sw/model-viewer/ch32v003usb.wrl" poster="/img/uploads/sw/model-viewer/ch32v003usb.jpg" loading="onclick">
  Sorry, your browser doesn't support embedded 3D models.
</model-viewer></p><p>

The KiCad design files for this board are published <a href="https://github.com/mitxela/ch32v003usb-hw">here</a> and <a href="https://git.mitxela.com/ch32v003usb-hw">here</a>.</p><h3>More USB, all the USB</h3><p>
Given how cheap the part is and how few supporting components it needs (the USB stack even does the same oscillator calibration frame timing trick that V-USB does), I thought it might be fun to recreate some of my USB ATtiny projects.</p><p>

The <a href="https://mitxela.com/projects/stylocard">stylocard</a> comes to mind first. I've done a few unpublished redesigns of that board in the past, and the best improvement was getting rid of the analog input method which was a little unreliable once the thing got dirty, and switching to direct readout, which means a microcontroller with at least 22 GPIO. One day I should publish all that. Unfortunately our CH32V003 doesn't have enough pins to read the keyboard and do USB together.</p><p>

We could just go with the same as before, a bunch of resistors and make the reading analog, or we could try and do something clever, or even just add a shift register, but it then occurred to me that since the CH32V003 is <i>so</i> cheap, why not just stick two of them on the board? It would be hilarious. One of them could read half the keyboard, the other would read the rest and also do USB.</p><p>

The possibilities are not unlimitless!</p><p>

Recreating my <a href="https://mitxela.com/projects/silly_synth">smallest USB MIDI synth</a> was the next thought. Something I've wanted to do for a while is produce a thin circuit board as they do for some dongles, sliding the thin circuit board inside of the USB-A plug, essentially creating the circuit I did before but in a way that can be mass-produced and easily assembled.</p><p>

But a more interesting idea was to bring the synth forwards through time into the age of USB-C. Electronically this just means adding a couple of resistors and fitting the right connector. You can't mount electronics inside a USB-C plug so easily though. I did find some mid-mount USB-C plugs which may have worked, but after a bit more searching I settled on this vertical mount type, intended for building docks.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/vertical-usb-c.jpg" alt="Vertical mount USB-C plug"></p><p>

The part number is USB4151 although there are a few similar parts from different suppliers.</p><p>

When USB-C was introduced, a lot of engineers complained about the difficulty of fanning out the connections. It seems the designers of the connector assumed that everyone would be using high density boards with microvias. The footprint alone of this connector is technically beyond the spec of a standard 6/6mil process, and that's before we've added any traces.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/usb-footprint0.jpg" alt="Footprint for the USB-C connector from the datasheet"></p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/usb-footprint.png" alt="Footprint for the USB-C connector in KiCad"></p><p>

The plastic studs require a non-plated through hole in very close proximity of a plated hole. For this joke project I'm not going to pay for tighter tolerances, so I decided to just ignore the DRC violation and if they aren't able to manufacture the holes I can trim off the plastic studs with a knife.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/vertical-usb-c2.jpg" alt="Vertical mount USB-C plug underside"></p><p>

The difficulty of fanout on a two layer board is such that special USB-C connectors are available, that don't break out all of the pins, if all you need is USB 2.0 or just power. However, they're not available in this vertical format, and besides I eat tricky routing problems for breakfast.</p><h3>Routing</h3><p>
With the vertical-mount USB-C plug, our ambition is to make the smallest possible circuit board underneath it, that can fit within the diameter of an ordinary piezo buzzer. The buzzer has a pin pitch of 7.62mm, or 0.3 inches, and the outer diameter is 13.8mm, but we want our circuit to fit inside the depression, that meniscus of the potting compound, which means a maximum diameter of about 12mm.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/piezo.jpg" alt="Piezo buzzer"></p><p>

There's no possible orientation where the piezo's pins don't foul the USB support pins. To deal with this, I widened the footprint spacing. It should be fine to bend the pins outwards a little, but if it doesn't fit we can file them down too. As the design iterated, I reshuffled this a few times, eventually I got them down to just 8mm apart.</p><p>

We don't need to connect the USB 3 pins, that's the four superspeed pairs and the two SBU pins, but we do need to wire up CC1 and CC2, which totals 14 pins to connect. Keeping the copper annulus around each plated hole as small as possible, it's <i>just</i> possible to have all the necessary tracks escape.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/routing1.png" alt="KiCad screenshot"></p><p>

Naturally the tracks are rounded with teardrops, because I have standards to live up to.</p><p>

As the shielding pins are all connected, we could cheat and not connect the grounds together on the board, but in the end it was fine to route these all together too.</p><p>

On the underside, routing is just as tight, with the QFN part shoved off-centre to make enough room for the tracks. Thankfully it doesn't hurt if we connect unused GPIO to ground (or to other signals really), so we can conveniently route ground right through the middle of the chip instead of going around.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/routing2.png" alt="KiCad screenshot"></p><p>

The regulator is an SC-70 package, that's like the miniature version of SOT-23. You can get even smaller regulators but it didn't seem like it would be an issue. Similarly, around the periphery I've used resistors and capacitors in 0603 format, just because there's no real pressure for space once we're outside of the piezo/USB/QFN footprint mess.</p><p>

On the front side I put three test pads, for power, ground and D1 (SWIO) for programming. In reality only one pad is needed, I'm just going to plug the USB-C in with an extension cable and touch a single wire to the programming pad.</p><h3>Panelization</h3><p>
I wanted to do the panel myself for three reasons.</p><p>

When you get boards made that are this small, and you plan to stencil solder paste onto them, it's extremely fiddly to hold things if you don't have a frame. A 12mm circle would be very tedious to hold at the best of times, but here we have components on both sides so after one side is soldered it'll be almost impossible to stencil the other.</p><p>

Secondly, I specifically wanted the panel to have explicit symmetry. We use the frame and the mounting holes to align the stencil. To save on the tedium of doing this twice, I put down two copies of the design, with the second flipped over. The whole panel is symmetric, so we can stencil one side, flip the board and stencil the other.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/routing3.jpg" alt="KiCad screenshot of panel"></p><p>

The third reason to do the panel myself is that I wanted to also use the frame as a jig. There's an oval hole in the middle designed to be a tight fit around a USB-C plug. Once the board is part-soldered and broken out of the panel, it's going to be a real pain to do anything to it, so this at least should give us a basic grip on the thing. We could have made a jig ourselves out of something else, but there aren't many materials that can be laser-cut and would survive the reflow oven. In a sense, FR4 is the perfect support material.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/render2.jpg" alt="Render of panel with piezos hidden"></p><p>

I should take this moment to praise just how useful the 3D model viewer is. I can remember once having a bizarre argument with a contractor who was a little old-school and failed to see the benefit of it. He'd grown up with OrCAD in the 90s and insisted that setting up 3D models for part footprints was a waste of time, or at the very least, not his job. 3D modelling is for the mechanical engineer, he kept saying.</p><p>

But being able to look at a realistic render provides such a huge safety buffer against silly mistakes. In the old days, we used to spend hours poring over gerber files looking for common mistakes because if you made one, it could set everything back by weeks. And they happened all the time! Things like missing the soldermask aperture on a footprint, or exporting shapes onto the wrong layers. Ever since we shifted to KiCad and made full use of the 3D viewer, I don't think I've ever made one of those mistakes. I still check the gerber files religiously, but the 3D viewer is a second layer of defence against mistakes.</p><p>

In the render below, I've highlighted the piezo, and you can immediately see that the pins don't quite line up with the footprint. This is because I've intentionally altered the footprint in the hope that we can bend those pins outward a little, as mentioned above. But it's exactly the kind of thing that the 3D viewer can help you with, to get a visual on the interference and whether it looks like it's going to work. Or at the very least, it might make you go back and check the 3D model is correct.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/render1.jpg" alt="Render of panel, with piezo highlighted in green"></p><p>

Here's an interactive 3D model if you're especially keen:</p><p>

<model-viewer src="/img/uploads/sillysynth/c/sillysynth2.wrl" poster="/img/uploads/sillysynth/c/render3.jpg" loading="onclick">
  Sorry, your browser doesn't support embedded 3D models.
</model-viewer></p><h3>Assembly</h3><p>
If you get your boards made at the lowest tolerances and they're below a certain size, manufacturers will subsidise the price. It's essentially a promotional deal, they charge you almost nothing because it costs them almost nothing to chuck tiny boards into the corners and crooks of other panels. I wonder how many other people have tried to produce a board with a (nominally) USB 3.2 Gen 2 connector on a two layer, 6/6mil tolerance board.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/bareboard.jpg" alt="Bare board in hand"></p><p>

They didn't question my footprint at all, and it seems to have been produced without problems.</p><p>

The correct order of assembly is to do the tiny parts first, and the USB connector last. The through-mount aspect of the USB connector means it's not possible to stencil the other side once it's fitted.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly1.jpg" alt="One board assembled still in the panel"></p><p>

The USB connector comes with a plastic cap, to allow you to pick it up with a vacuum nozzle.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly2.jpg" alt="Close up of USB-C connector on board"></p><p>

I had planned to reflow a bunch of these, but it's so small I ended up doing all of them with the heat gun. It's possible (and not that unusual) to reflow a board with components underneath already soldered. Even if the solder melts, surface tension holds them in place. It's also possible to use two different alloys of solder with different melting points if it's a concern. But hitting it with the heatgun it's easy enough to direct the heat only to where needed.</p><p>

If I had reflowed it though, the plan was to stencil and place components on both sides, then reflow the whole thing at once. Something like the following picture:</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly3.jpg" alt="Mockup of how the panel could be used to support the board"></p><p>

The connector itself would poke through the grating that makes up the bed of the reflow oven.</p><p>

Anyway I didn't do that, I just soldered them all in place as it was way less tricky than I'd imagined.
 
<img src="https://mitxela.com/img/uploads/sillysynth/c/production-line.jpg" alt="Two boards being assembled"></p><p>

Carefully snap them out of the panel and file the rough edges a little.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly4.jpg" alt="Three assembled boards"></p><p>

After assembling them, I did wonder if perhaps I should have gone with smaller capacitors after all. They're the tallest single components, and smaller caps are easily available. Oops, too late now.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly5.jpg" alt="Close up of the capacitors, board held between fingers"></p><p>

As expected, the buzzer pins were a tight fit, but there was enough play to jam them into place and get the board flush. I then trimmed them to length and delicately soldered the stubs.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/finished1.jpg" alt="Fully assembled"></p><p>

Comically I waited until this moment to realise I didn't have enough piezo buzzers in stock. I ordered some more and the new ones are a minutely different design, which was inevitable. Never mind.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/finished2.jpg" alt="Fully assembled"></p><p>

That vertical-mount USB-C connector was designed to go inside a dock for a phone or tablet. It's supposed to poke through a moulded plastic case, which means it's a little longer than it needed to be. I did have a think about 3D printing a little plastic cover to go over the circuit board and the lowest part of the connector, but I doubt it would look very good. We wouldn't want to cover that mitxela logo anyway.</p><p>

USB-C extension cables are technically against the spec, but that doesn't mean you can't buy them and all kinds of other nonsensical cables and connectors. I have one that only works in certain orientations, which is just so distressing and the opposite of what USB-C was supposed to be, but it'll do to give us power while I poke that SWIO pin with a probe. I flashed the four different synths with different device names, which helps us differentiate them in a DAW. And by DAW I mean the 1998 edition of Cakewalk running in wine.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/foursynths1.jpg" alt="Four silly USB-C synths"></p><p>

I then went out of my way to buy a four-port USB-C hub. Surprisingly difficult to find, most of them turn USB-C into various more helpful connectors like USB-A, HDMI, SD card, and so on. </p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/foursynths2.jpg" alt="Four silly USB-C synths"></p><p>

I have a bunch of the PCBs left, maybe I should make another handful and get even more hubs?</p><p>

Interestingly the design works with the hub, and it works if I plug it into my phone, but it doesn't enumerate if I plug it straight into my laptop. But it does enumerate on the end of that noncompliant USB-C extension cable I've concocted. It's entirely possible I've wired up the USB-C port marginally wrong, or perhaps the resistors are not exactly the right value – the type of USB connection is determined by the strength of some of the pull resistors. Either way I don't think I care enough about this comedy synth to look into it much further. It's just something to keep in mind for the next USB-C device.</p><div><p>

I have put the source code for this project in the <a href="https://github.com/mitxela/smsc">usual</a> <a href="https://git.mitxela.com/smsc">places</a>.</p></div><nav>
<a href="https://mitxela.com/projects/random" title="random project">~</a>
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/">mitxela.com</a></span> » 
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects">Projects</a></span> » 
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects/hardware">Hardware</a></span> »
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects/smsc">Smallest USB-C MIDI Synth</a></span>
<p>Questions? Comments? Check out the <a href="https://mitxela.com/forum">Forum</a>
</p><p><a href="https://mitxela.com/support">Support mitxela.com</a>
</p></nav></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leave work slightly unfinished for easier flow the next day (211 pts)]]></title>
            <link>https://read.engineerscodex.com/p/simple-software-engineering-habits</link>
            <guid>38658262</guid>
            <pubDate>Fri, 15 Dec 2023 20:13:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://read.engineerscodex.com/p/simple-software-engineering-habits">https://read.engineerscodex.com/p/simple-software-engineering-habits</a>, See on <a href="https://news.ycombinator.com/item?id=38658262">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>“Your outcomes are a lagging indicator of your habits.” - James Clear</em></p><p>As I became a better software engineer, I noticed 4 key habits in my daily workflow that had made me much more productive.</p><blockquote><p><em><span>Friendly plug: </span><strong><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> is a compilation of 450+ software engineering and system design questions covering databases, authentication, caching, etc. They’ve been vetted by engineers from Google, Meta, Apple, and more.</span></strong></em></p><p><em><span>I’m a core contributor to </span><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> and it’s helped many of my peers (including myself) pass the “software trivia questions” during interviews and feel more confident at work.</span></em></p></blockquote><p data-attrs="{&quot;url&quot;:&quot;https://swequiz.com&quot;,&quot;text&quot;:&quot;Check it out&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://swequiz.com/" rel=""><span>Check it out</span></a></p><p>“Flow” is the root of productivity when programming.&nbsp;</p><p><span>Since software engineering is a </span><a href="http://www.paulgraham.com/makersschedule.html" rel="">“maker” activity</a><span> where I’m producing something, I generally perform best when I have a large block of uninterrupted “flow” time to work on a project.</span></p><p>However, it can often be really hard to get into flow if you’re stuck scrambling on what tasks your project goals entail. Ambiguity is difficult to deal with. Not even knowing where to start can make reaching that “flow state” much harder.</p><p>Each successful action snowballs into more.</p><p>There are a few techniques I use to do this:</p><ul><li><p><strong>Stop right before a “sticking point.”</strong><span> A sticking point is a task that’s part of a project where I know the steps to do to complete it, but I don’t know if there are hidden costs.&nbsp;</span></p><ul><li><p>For example, if my sticking point is deploying my ML model and HTTP server to a dev instance and verifying that it processes requests properly, then the hidden costs are deployment errors, authentication errors, resource constraints, etc.</p></li></ul></li><li><p><strong>Write down the next steps extremely clearly.</strong><span> Writing down steps makes regaining context and the state of mind from the day before easier.</span></p><ul><li><p><strong>Make them actionable and unambiguous.</strong></p></li></ul></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png" width="1456" height="572" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:572,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:65283,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>My first experience with a real “shortcut ninja” was actually not with a software engineer. It was with my investment banker friend, who sped around his Excel sheets without ever touching the mouse.&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png" width="516" height="516" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:516,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>This, except I finally did appreciate it years later. Source: </span><a href="https://workchronicles.com/keyboard-shortcuts/" rel="">Work Chronicles</a></figcaption></figure></div><p>After that, I took the time to learn keyboard shortcuts, to the point where I grab my mouse ~60% less than I used to. (Yes, I tracked this.)</p><p>Every editor and tool in my workflow has keyboard shortcuts for pretty much any action you can think of. This doesn’t just apply to your IDE, but also your version control systems, your web browser, and your docs.</p><blockquote><p>For example, my IDE has a linter/formatter/cleaner all in one shortcut, which I use often as I write code to make sure lines stay neat.</p><p>I commonly use Command/Ctrl + Shift + V to paste in text without formatting in docs and chats.</p><p><span>Pressing </span><code>.</code><span> (period) on a GitHub repo page will automatically open up the repo in a VSCode Web instance.</span></p></blockquote><p>When I do need to touch my mouse, it’s configured with shortcuts also. I’m lucky enough to have a mouse with buttons on the sides. I’ve programmed these buttons to switch between Spaces on my Mac, though you can program them for whatever feels intuitive for you. (You can even program them to be different per program.)</p><p><span>The best way to learn shortcuts? </span><strong>Introduce the most common parts of a program that you use, one a time.</strong><span> </span></p><p>For example, if you find yourself right-clicking to format your code often, that can be the first one you “practice.” Every so often, add a new shortcut to your repertoire and use it naturally as you code throughout the weeks. Over time, the shortcuts will be muscle memory.</p><p>I commonly have to run a set of common commands on my terminal.</p><p>I have certain pages that I always visit and some notes about various languages that I always come back to. For example, I simultaneously use templates too rarely and yet too often when writing C++, meaning I usually need to reference the docs when using them.</p><p><span>Instead of digging around documentation pages or constantly looking through my terminal history, I keep commands and common doc lookups in a giant doc with one word describing the command. I call it my </span><em>Big Book of Commands</em><span>, which is around ~10 pages long now. I’m easily able to find any command I need with a quick Ctrl+F. Then, a Shift + Command + ➡️ is a full line-select for an easy copy-paste.</span></p><p>I also have a few common macros programmed into my keyboard for the commands and terms, like hard-to-remember ACL groups. Sometimes, I utilize Terminal aliases.</p><p><span>My friend </span></p><p><span> wrote a great article that dives into his own workflow tips, which starts off with a great primer into aliases, keyboard shortcuts, and tools: </span><a href="https://careercutler.substack.com/p/the-top-7-software-engineering-workflow" rel="">The top 7 software engineering workflow tips I wish I knew earlier 🧰</a></p><p>This is less directly programming related, but I learned to say “no” to things. </p><blockquote><p>I said no to novel technology when boring technology would do the job. </p><p>I said no to automating something when it only needed to be done once manually. </p><p>I said no to more tasks when I knew I was already overloaded with work (even though my people-pleasing mind pleaded to take them on). </p><p>I said no to scope creep suggested by our designers. </p><p>I said no to low-impact tasks.</p></blockquote><p>Learning to say no was harder than I expected, yet one of the most valuable skills I’ve applied in both the workplace and in my personal life.</p><p>Sometimes, it’s painful to say no to things. In both my career, hobbies, and personal life, there are times I say no to things I really want to do. But I don’t because I know my time and energy is better spent on what I’m currently focusing on.</p><p>It’s a cliche to quote Steve Jobs, but I remind myself of his famous quote “focus is about saying no” often.</p><p><span>My friend </span></p><p><span> also has a fantastic article about saying no, which I highly recommend: </span><a href="https://www.thecaringtechie.com/p/software-eng-guide-to-saying-no" rel="">The Software Engineer's guide to saying "no"</a></p><p><em>When you’re ready, here's how I can help:</em></p><p><em><strong><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> is a compilation of 450+ software engineering and system design questions covering databases, authentication, caching, etc. They’ve been vetted by engineers from Google, Meta, Apple, and more.</span></strong></em></p><p><em><span>I’m a core contributor to </span><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> and it’s helped many of my peers (including myself) pass the “software trivia questions” during interviews and feel 10x more confident at work.</span></em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Suspects can refuse to provide phone passcodes to police, court rules (499 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/12/suspects-can-refuse-to-provide-phone-passcodes-to-police-court-rules/</link>
            <guid>38657577</guid>
            <pubDate>Fri, 15 Dec 2023 19:16:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/12/suspects-can-refuse-to-provide-phone-passcodes-to-police-court-rules/">https://arstechnica.com/tech-policy/2023/12/suspects-can-refuse-to-provide-phone-passcodes-to-police-court-rules/</a>, See on <a href="https://news.ycombinator.com/item?id=38657577">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/phone-passcode-800x533.jpeg" alt="A person's hand holding a smartphone while entering the screen-lock passcode.">
      <figcaption><p>Getty Images | releon8211</p></figcaption>  </figure>

  




<!-- cache miss 88:single/related:5898e440fb979d41242bf9b8a83f0ba0 --><!-- empty -->
<p>Criminal suspects can refuse to provide phone passcodes to police under the US Constitution's Fifth Amendment privilege against self-incrimination, according to a unanimous <a href="https://legacy.utcourts.gov/opinions/supopin/State%20v.%20Valdez20231214.pdf">ruling issued today</a> by Utah's state Supreme Court. The questions addressed in the ruling could eventually be taken up by the US Supreme Court, whether through review of this case or a similar one.</p>
<p>The case involves Alfonso Valdez, who was arrested for kidnapping and assaulting his ex-girlfriend. Police officers obtained a search warrant for the contents of Valdez's phone but couldn't crack his passcode.</p>
<p>Valdez refused to provide his passcode to a police detective. At his trial, the state "elicited testimony from the detective about Valdez's refusal to provide his passcode when asked," today's ruling said. "And during closing arguments, the State argued in rebuttal that Valdez's refusal and the resulting lack of evidence from his cell phone undermined the veracity of one of his defenses. The jury convicted Valdez."</p>
<p>A court of appeals reversed the conviction, agreeing "with Valdez that he had a right under the Fifth Amendment to the United States Constitution to refuse to provide his passcode, and that the State violated that right when it used his refusal against him at trial." The Utah Supreme Court affirmed the court of appeals ruling.<br>
                                            </p>
                                                        
<h2>Case possibly ripe for Supreme Court review</h2>
<p>The ruling offered some commentary on the developing legal questions about device passcodes:</p>
<blockquote><p>The prevalence of passcodes that encrypt the information on electronic devices—which are often seized by law enforcement while investigating criminal conduct—has raised important questions about how the Fifth Amendment extends to law enforcement's efforts to unlock these devices and decrypt the contents inside. These questions have proven to be especially complex where law enforcement attempts to access the contents of a seized device by means that do not require the suspect to disclose the actual passcode—like, for example, obtaining an order to compel the suspect to provide an unlocked device.</p></blockquote>
<p>The Valdez case does not involve an order to compel a suspect to unlock a device. Instead, "law enforcement asked Valdez to verbally provide his passcode," Utah justices wrote. "While these circumstances involve modern technology in a scenario that the Supreme Court has not yet addressed, we conclude that these facts present a more straightforward question that is answered by settled Fifth Amendment principles."</p>
<p>Ruling against the state, the Utah Supreme Court said it "agree[s] with the court of appeals that verbally providing a cell phone passcode is a testimonial communication under the Fifth Amendment."</p>
<p>Berkeley Law Professor Orin Kerr <a href="https://reason.com/volokh/2023/12/14/is-compelled-decryption-heading-to-the-supreme-court/">wrote today</a> that the case could head to the US Supreme Court. "One of the major issues in the law of digital evidence investigations is how the Fifth Amendment privilege against self-incrimination applies to unlocking phones," Kerr wrote.</p>
<p>So far, "the lower court case law is a total mess," according to Kerr. "No one can say what the law is. And I've been waiting for a case to come down that might be a good candidate for US Supreme Court review to clear up the mess."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[McDonald's ice cream machine hackers say they found 'smoking gun' (195 pts)]]></title>
            <link>https://www.wired.com/story/kytch-taylor-mcdonalds-ice-cream-machine-smoking-gun/</link>
            <guid>38657192</guid>
            <pubDate>Fri, 15 Dec 2023 18:47:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/kytch-taylor-mcdonalds-ice-cream-machine-smoking-gun/">https://www.wired.com/story/kytch-taylor-mcdonalds-ice-cream-machine-smoking-gun/</a>, See on <a href="https://news.ycombinator.com/item?id=38657192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>A little over three years have passed since McDonald's sent out an email to thousands of its restaurant owners around the world that abruptly cut short the future of <a href="https://www.wired.com/story/they-hacked-mcdonalds-ice-cream-makers-started-cold-war/">a three-person startup called Kytch</a>—and with it, perhaps one of McDonald's best chances for fixing its famously out-of-order ice cream machines.</p><p>Until then, Kytch had been selling McDonald's restaurant owners a popular internet-connected gadget designed to attach to their notoriously fragile and often broken soft-serve McFlurry dispensers, manufactured by McDonalds equipment partner Taylor. The Kytch device would essentially hack into the ice cream machine's internals, monitor its operations, and send diagnostic data over the internet to an owner or manager to help keep it running. But despite Kytch's efforts to solve the Golden Arches’ intractable ice cream problems, a McDonald’s email in November 2020 warned its franchisees not to use Kytch, stating that it represented a safety hazard for staff. Kytch says its sales dried up practically overnight.</p><p>Now, after years of litigation, the ice-cream-hacking entrepreneurs have unearthed evidence that they say shows that Taylor, the soft-serve machine maker, helped engineer McDonald's Kytch-killing email—kneecapping the startup not because of any safety concern, but in a coordinated effort to undermine a potential competitor. And Taylor's alleged order, as Kytch now describes it, came all the way from the top.</p><div data-testid="GenericCallout"><p><span><picture><img alt="Image may contain: Food, Creme, Dessert, and Cream" src="https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_775%2Cc_limit/web_hp_IceCream_15046_2.jpg" srcset="https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_120,c_limit/web_hp_IceCream_15046_2.jpg 120w, https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_240,c_limit/web_hp_IceCream_15046_2.jpg 240w, https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_320,c_limit/web_hp_IceCream_15046_2.jpg 320w, https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_640,c_limit/web_hp_IceCream_15046_2.jpg 640w" sizes="100vw"></picture></span></p><div><p>Secret codes. Legal threats. Betrayal. How one couple built a device to fix McDonald’s notoriously broken soft-serve machines—and how the fast-food giant froze them out.</p></div></div><p>On Wednesday, Kytch filed a newly unredacted motion for summary adjudication in its lawsuit against Taylor for alleged trade libel, tortious interference, and other claims. The new motion, which replaces a redacted version from August, refers to internal emails Taylor released in the discovery phase of the lawsuit, which were quietly unsealed over the summer. The motion focuses in particular on one email from Timothy FitzGerald, the CEO of Taylor parent company Middleby, that appears to suggest that either Middleby or McDonald's send a communication to McDonald's franchise owners to dissuade them from using Kytch's device.</p><p>“Not sure if there is anything we can do to slow up the franchise community on the other solution,” FitzGerald wrote on October 17, 2020. “Not sure what communication from either McD or Midd can or will go out.”</p><p>In their legal filing, the Kytch cofounders, of course, interpret “the other solution” to mean their product. In fact, FitzGerald's message was sent in an email thread that included Middleby's then COO, David Brewer, who had wondered earlier whether Middleby could instead acquire Kytch. Another Middleby executive responded to FitzGerald on October 17 to write that Taylor and McDonald’s had already met the previous day to discuss sending out a message to franchisees about McDonald’s lack of support for Kytch.</p><p>But Jeremy O'Sullivan, a Kytch cofounder, claims—and Kytch argues in its legal motion—that FitzGerald’s email nonetheless proves Taylor's intent to hamstring a potential rival. “It's the smoking gun,” O'Sullivan says of the email. “He's plotting our demise.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Although FitzGerald's email doesn't actually order anyone to act against Kytch, the company’s motion argues that Taylor played a key role in what happened next. It's an “ambiguous yet direct message to his underlings,” argues Melissa Nelson, Kytch's other cofounder. “It's just like a mafia boss giving coded instructions to his team to whack someone."</p><p>On November 2, 2020, a little over two weeks after FitzGerald's open-ended suggestion that perhaps a “communication” from McDonald's or Middleby to franchisees could “slow up” adoption of “the other solution,” McDonald's <a href="https://www.wired.com/story/mcdonalds-ice-cream-machine-hacking-kytch-taylor-internal-emails/">sent out its email blast</a> cautioning restaurant owners not to use Kytch's product.</p><p>The email stated that the Kytch gadget “allows complete access to all aspects of the equipment’s controller and confidential data”—meaning Taylor’s and McDonald’s data, not the restaurant owners’ data; that it “creates a potential very serious safety risk for the crew or technician attempting to clean or repair the machine"; and finally, that it could cause “serious human injury.” The email concluded with a warning in italics and bold: “McDonald’s strongly recommends that you remove the Kytch device from all machines and discontinue use.”</p><p>Kytch has long argued that McDonald’s safety warning was bogus: In its legal complaint, it noted that its devices received certification from Underwriters Laboratory, an independent product safety nonprofit, including meeting its safety standards. It also countered in the complaint any claim that a Kytch device's remote connection to an ice cream machine could result in the machine turning on while a person's hand was inside—in fact, Taylor's own manual advises unplugging the machine before servicing it, and removing the door of the machine to access its rotating barrels automatically disables its motor.</p><p>Kytch's legal motion now argues that FitzGerald's email reveals that the McDonald's warning to restaurant owners was never really about safety, so much as protecting its equipment partner from a startup that might represent competition. The CEO's email “essentially put into place their plan to defame us," Nelson says.</p><p>She and O’Sullivan also argue that the internal email directly contradicts FitzGerald’s public statements that Middleby hadn’t sought to kill Kytch. “We’re not in business to put other companies out of business,” FitzGerald <a href="https://www.nytimes.com/2022/03/12/business/mcdonald-kytch-ice-cream-lawsuit.html">told <em>The New York Times</em></a> early last year.</p><p>When WIRED reached out to Middleby, Taylor’s parent company, for comment, a spokesperson responded in a statement disputing Kytch’s interpretation of its internal emails. “McDonald’s decided to issue the November 2020 field brief on its own accord, not at Middleby or Taylor’s direction,” the statement reads. “Taylor stood, and continues to stand, by the accuracy of statements made in the field brief.” The spokesperson also notes that Taylor won an early ruling in the lawsuit against Kytch’s request for a preliminary injunction—which would have prevented Taylor from developing a device that Kytch claims was <a href="https://www.wired.com/story/mcdonalds-ice-cream-machine-hacking-kytch-taylor-internal-emails/">copied from its product</a>—and promises an upcoming filing responding to Kytch’s argument, which court documents say will happen in early 2024.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>At the time of McDonald's warning email to franchisees about Kytch, Taylor was developing its own internet-connected ice cream machine, what it referred to as Taylor Shake/Sundae Connectivity, which McDonald's recommended in the same email. But, even now, more than two years after it was promised for delivery, that device has yet to arrive in restaurants—and the <a href="https://arstechnica.com/gadgets/2023/08/mcdonalds-ice-cream-machine-teardown-shows-error-codes-dmca-keep-it-broken/">publicly documented ice cream headaches</a> at McDonald’s appear to have continued. According to the website <a data-offer-url="https://mcbroken.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://mcbroken.com/&quot;}" href="https://mcbroken.com/" rel="nofollow noopener" target="_blank">McBroken</a>, which tracks ice cream machine downtime at McDonald's restaurants across the US, between 13 percent and 17 percent of McDonald's restaurants have had broken ice cream machines at any given time just this month. That percentage has recently been as high as 35 percent in New York City and 28 percent in Washington, DC.</p><p>Taylor declined to comment on any upcoming internet-connected ice cream machine model. But that long-touted solution to the problem has still not been made available to franchisees, according to one McDonald's restaurant owner who goes by the handle McFranchisee (and previously used the handle McD Truth) on X. But McFranchisee says that Taylor has integrated those new features into its next model, which is expected to be available in four to six months. (McFranchisee has also criticized Kytch, claiming that the startup's failure was due to its own reliability problems and an increase in its prices, not a Taylor or McDonald's conspiracy against them.)</p><p>Despite the email from Middleby's CEO that Kytch claims suggests dissuading franchisees from using Kytch's product, Kytch argues that <a href="https://www.wired.com/story/mcdonalds-ice-cream-machine-hacking-kytch-taylor-internal-emails/">other documents released in the lawsuit’s discovery phase</a> show McDonald's itself was also eager to stymie Kytch from the beginning. In February 2020, Taylor president Jeremy Dobrowolski wrote in another email that “McDonald's is all hot and heavy about” Kytch's growing use in restaurants. Before the company sent out its November 2 email warning franchisees about Kytch, Taylor and McDonald’s executives had a meeting to discuss the message, and a McDonald's exec also sent a draft to Taylor for its approval. A Taylor executive wrote to others within the ice cream machine company, “I am a bit in shock they are willing to take such a strong position.”</p><p>When WIRED reached out to McDonald’s for comment on Kytch’s new argument about the “smoking gun” email from Taylor’s CEO, a spokesperson responded with a statement: “McDonald’s won’t speculate about the intent behind this email discussion that we weren't a part of. The intent of our Nov. 2020 communication was to bring awareness to potential safety concerns regarding the unapproved Kytch device.”</p><p>In addition to its lawsuit against Taylor, Kytch is still pursuing a <a href="https://www.wired.com/story/kytch-ice-cream-machine-hackers-sue-mcdonalds-900-million/">bigger lawsuit against McDonald's itself</a>, asking for $900 million in damages for what it describes in its legal complaint as McDonald’s effort to “<a data-offer-url="https://www.courtlistener.com/docket/63127533/1/kytch-inc-v-mcdonalds-corporation/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.courtlistener.com/docket/63127533/1/kytch-inc-v-mcdonalds-corporation/&quot;}" href="https://www.courtlistener.com/docket/63127533/1/kytch-inc-v-mcdonalds-corporation/" rel="nofollow noopener" target="_blank">drive Kytch out of the marketplace</a>.” That lawsuit against McDonald's, if it moves forward, may soon produce more answers explaining Kytch’s legal claims that McDonald's appears to have cooperated with Taylor in telling its customers not to use Kytch—even as many of its restaurants took a significant hit from lost ice cream sales.</p><p>In the meantime, Kytch says it plans, if necessary, to take the lawsuit against Taylor to trial, currently set to take place in May at Alameda County Superior Court in Oakland, California. “The conspiracy described in Kytch’s complaint involved folks at the highest levels of leadership, not just at Taylor but also at Middleby and at McDonald’s,” says Daniel Watkins, Kytch’s attorney. “We’re really looking forward to the opportunity to present it to an Oakland jury trial.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I bricked my Christmas lights (441 pts)]]></title>
            <link>https://www.whizzy.org/2023-12-14-bricked-xmas/</link>
            <guid>38657126</guid>
            <pubDate>Fri, 15 Dec 2023 18:41:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.whizzy.org/2023-12-14-bricked-xmas/">https://www.whizzy.org/2023-12-14-bricked-xmas/</a>, See on <a href="https://news.ycombinator.com/item?id=38657126">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      

      

      <article role="main">
        <h2 id="reverse-engineering-bluetooth-le-led-light-controllers-or-how-i-bricked-my-christmas-lights">Reverse engineering Bluetooth LE LED light controllers, or How I Bricked My Christmas Lights</h2>

<p>If a device communicates via Bluetooth LE and has an app, it deserves to be integrated into my home automation system.</p>

<p>I’ve spent a significant amount of time reverse engineering various budget-friendly LED light strips to automate them. The process is generally repetitive, but I find it enjoyable. Recently, I successfully connected the cheapest lights I’ve ever come across — a £2.38 Bluetooth LE-controlled 5M non-addressable strip — to Home Assistant in just a few hours. You can buy some <a href="https://www.aliexpress.com/item/1005005485885067.html">here</a> and the code is <a href="https://github.com/8none1/bj_led">here</a>.</p>

<p>There is also the LEDnetWF controller I did the reverse engineering for <a href="https://github.com/raulgbcr/lednetwf_ble">here</a>.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/ha.jpg" alt="AliExpress Lights"></p>

<p>I also had another set of addressable lights on my desk. While decorating my office for Christmas, I decided to invest some time in connecting them to Home Assistant using the BJ_LED code as a template. It should have been straightforward, right? Well, yes, but also no.</p>

<p>These lights consist of a 10M long string of addressable LEDs controlled by the “iDeal LED” app. The app is feature-rich and works reasonably well. The LEDs are likely WS2812 or similar. I was quite pleased with these lights, which you can <a href="https://www.aliexpress.com/item/1005004829475855.html">find on AliExpress</a>.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/aliex_lights.jpg" alt="AliExpress Lights"></p>

<p>Now, let me share a cautionary tale. While I’m omitting some details for brevity, there are no secrets here, and additional instructions are readily available online. I understand this might feel a bit like <a href="https://knowyourmeme.com/memes/how-to-draw-an-owl">drawing the rest of the owl</a> but the provided links should serve as a starting point for anyone interested in reverse engineering their own LED lights.</p>

<h2 id="step-1-the-bytes-over-the-wire">Step 1. The bytes over the wire</h2>

<p>To control devices from your own software, the first step is to examine the bytes sent over Bluetooth to the device from the app. Typically, lights use a simple protocol with a header, command bytes (for actions like turning on/off, changing color), and a footer, which might be a checksum.</p>

<p>Android makes this process easy. Enable developer mode on your Android device, install the app for your lights, and enable <code>Bluetooth HCI snoop</code> in the developer settings. This logs Bluetooth bytes to a file readable by <a href="https://www.wireshark.org/">Wireshark</a>. Perform actions in the app, such as turning the lights on and off, and use <code>adb</code> to copy the logs to your computer.</p>

<p>For example:</p>

<pre><code>adb pull sdcard/btsnoop_hci.log .
</code></pre>

<p>Open the log in Wireshark to see the exact bytes sent to the device. Look for patterns in the values, and you’ll likely identify a series of bytes for each action, with one byte alternating between two values (e.g. <code>1</code> and <code>0</code> for <code>on</code> and <code>off</code>). Here’s a useful Wireshark filter:</p>

<pre><code>bluetooth.dst == ff:ff:ff:ff:ff:ff &amp;&amp; btatt.opcode.method==0x12
</code></pre>

<p>Change MAC address to be the MAC of your lights.  <code>btatt.opcode.method==0x12</code> is a write from the Android device to the lights.</p>

<p>Congratulations, you are now a reverse engineer!</p>

<p>Pro-tip:  You can speed things up a bit by using <a href="https://tshark.dev/">tshark</a> instead of Wireshark.  What you really care about is the values being written to the LED controller.  <code>tshark -r &lt;filename&gt; -T fields -e btatt.value</code> will dump the payload to the terminal for easy interrogation.</p>

<p>Sometimes your bytes will look like this:</p>

<pre><code>69 96 02 01 01
69 96 02 01 00
69 96 02 01 01
69 96 02 01 00
69 96 02 01 01
69 96 02 01 00
69 96 02 01 01
69 96 02 01 00
</code></pre>

<p>On, off, on, off, on, off, on, off.</p>

<p>Sometimes your bytes will look like this:</p>

<pre><code>84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
79 d1 db a4 09 19 c2 46 a8 58 0a e7 d1 1b 78 84
84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
79 d1 db a4 09 19 c2 46 a8 58 0a e7 d1 1b 78 84
84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
79 d1 db a4 09 19 c2 46 a8 58 0a e7 d1 1b 78 84
84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
</code></pre>

<p>There is still a repeating pattern here.  There are two distinct sets of bytes, one for on &amp; one for off, but… what?  Why is it so noisy?
Who designs their protocol like this?
The answer is: someone who is trying to hide something.</p>

<h2 id="step-2--replay-attacks">Step 2.  Replay attacks</h2>

<p>If your goal is simply turning the lights on and off, the repeating series of bytes you observed might be sufficient for power control. Test this with <code>gatttool</code>, which lets you connect to a BLE device and send bytes. You’ll need to know the handle to send bytes to, which you can find using Wireshark.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/wireshark.jpg" alt=""></p>

<p>For more control, understanding all those bytes is essential. Let’s go to the source…</p>

<h2 id="step-3-decompile-the-android-app">Step 3. Decompile the Android app</h2>

<p>Download the app’s APK and open it in <a href="https://github.com/skylot/jadx">jadx</a>. Witness the secrets within!</p>

<p>In my case, I noticed references to AES in the source, indicating a potentially encrypted protocol. If the data is encrypted, some assumptions can be made:</p>

<ul>
  <li>The encrypted data doesn’t change every time, suggesting a consistent key.</li>
  <li>The data needs quick decryption on a low-power MCU, favouring shorter keys.</li>
  <li>The key is likely not unique to each device, making a fixed key plausible.</li>
</ul>

<p>The source code contained a compiled AES library <code>libAES.so</code>, which <code>jadx</code> can’t help me with.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/jadx.jpg" alt="">
<img src="https://www.whizzy.org/wp-content/uploads/2023/12/jadx2.jpg" alt=""></p>

<p>This is where I got stuck.  For about 5 minutes.</p>

<p>I asked <a href="https://ubuntu.social/@popey">@popey</a> and <a href="https://mastodon.social/@sil">@sil</a> for some ideas.  @sil Googled some of the decompiled app code and found <a href="https://habr-com.translate.goog/ru/articles/722412/?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&amp;_x_tr_pto=wapp&amp;_x_tr_hist=true">this</a> page.  On closer examination the code looks identical.  This chap used <a href="https://hex-rays.com/ida-free/">ida free</a> to decompile the AES library and found the key embedded in it.  Let’s try that key.</p>

<pre><code>from Crypto.Cipher import AES

key = [
    0x34, 0x52, 0x2A, 0x5B, 0x7A, 0x6E, 0x49, 0x2C,
    0x08, 0x09, 0x0A, 0x9D, 0x8D, 0x2A, 0x23, 0xF8
]

def decrypt_aes_ecb(ciphertext, key):
    cipher = AES.new(key, AES.MODE_ECB)
    plaintext = cipher.decrypt(ciphertext)
    return plaintext
</code></pre>

<p>When we try and decrypt the <code>on</code> and <code>off</code> packets we get:</p>

<pre><code>05 54 55 52 4E 01 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 00 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 01 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 00 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 01 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 00 00 00 00 00 00 00 00 00 00 00
</code></pre>

<p>Success!  This is a lot more sensible.  A fixed header, byte 5 switching between a <code>1</code> and a <code>0</code> for on and off, and a bunch of zeros.</p>

<p>We can now decrypt all the packets being sent to the device and we can encrypt our own bytes so that we can duplicate the controls from the Android app in our own code.  It’s pretty much mission accomplished at this point.</p>

<h2 id="step-4-all-the-functions">Step 4. All the functions</h2>

<p>Now, work through each app function, recording the bytes sent. Write down each action, do it multiple times, and use separators like turning the lights on and off. This helps spot patterns and correlate notes with captured bytes.</p>

<p>For example, your process might be:</p>

<pre><code>turn off, turn on - [start of function]
set to red
set to green
set to blue
set to red
set to green
set to blue
set to red
set to green
set to blue
turn off, turn on - [end of colour changing]
set brightness to 100%
set brightness to 50%
set brightness to 10%
set brightness to 50%
set brightness to 100%
turn off, turn on - [end of brightness]
</code></pre>

<p>This will help you to spot patterns in the data and see which bytes change depending on what you are doing.</p>

<h2 id="step-5--automated-e-waste-generator">Step 5.  Automated e-waste generator</h2>

<p>While exploring color changes, I observed that the app never sent a value higher than 0x1F (5 bits) for red, green, or blue. Curious, I tried sending 8-bit values, and it worked remarkably well — brighter colors!</p>

<p>Great success!</p>

<p>Excited by my discovery I got to wondering what other secrets this light controller was hiding from me.  I wonder if there are any additional effects beyond the 10 that the app uses?
A good way to try this out would be a simple loop.</p>

<pre><code>    for n in range(20):
        print(f"Setting effect {n}")
        set_effect(n)
        time.sleep(20)
</code></pre>

<p>I ran this and watched 1 to 10.  So far so good, then it ticked over to 11 and AH HA!  I have found a secret mode!
Then it ticked over to 12 and… darkness.</p>

<p>Oh well, I guess there are only 11 effects, that’s fine.  I’ll reboot it and finish off the rest of the code.</p>

<p>And that was then end of my fun.</p>

<p>The lights never came back.</p>

<p>They don’t advertise on Bluetooth any more and I can’t connect to them.  I’ve tried holding down the button when turning them on.  I’ve left them unplugged over night to see if that helps, but no.</p>

<p>They are dead.</p>

<p>I guess I overflowed some buffer and I’ve corrupted the firmware.</p>

<p>All is not lost however.  The LEDs themselves are standard addressable LEDs so I can at least hook the string up to a different microcontroller and use them.</p>

<h2 id="tell-me-how-i-can-break-my-own-lights">Tell me how I can break my own lights</h2>

<p>Despite the setback, I documented most of the protocol and created a Github project with a Home Assistant custom component. It works, but proceed at your own risk.</p>

<p><a href="https://github.com/8none1/idealLED">Github: 8none1/idealLED</a></p>

      </article>

      

      

      
        <!-- Check if any share-links are active -->








      

      
      
  
  
  

  


  

  



    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI: Prompt Engineering (256 pts)]]></title>
            <link>https://platform.openai.com/docs/guides/prompt-engineering</link>
            <guid>38657029</guid>
            <pubDate>Fri, 15 Dec 2023 18:30:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://platform.openai.com/docs/guides/prompt-engineering">https://platform.openai.com/docs/guides/prompt-engineering</a>, See on <a href="https://news.ycombinator.com/item?id=38657029">Hacker News</a></p>
Couldn't get https://platform.openai.com/docs/guides/prompt-engineering: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Fastmail Employees Form a Union (186 pts)]]></title>
            <link>https://union.place/@fastmailunited/111563614375789166</link>
            <guid>38656727</guid>
            <pubDate>Fri, 15 Dec 2023 18:01:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://union.place/@fastmailunited/111563614375789166">https://union.place/@fastmailunited/111563614375789166</a>, See on <a href="https://news.ycombinator.com/item?id=38656727">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Do large language models need all those layers? (170 pts)]]></title>
            <link>https://www.amazon.science/blog/do-large-language-models-really-need-all-those-layers</link>
            <guid>38656039</guid>
            <pubDate>Fri, 15 Dec 2023 17:00:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amazon.science/blog/do-large-language-models-really-need-all-those-layers">https://www.amazon.science/blog/do-large-language-models-really-need-all-those-layers</a>, See on <a href="https://news.ycombinator.com/item?id=38656039">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Large language models (<a href="https://www.amazon.science/tag/large-language-models" data-cms-ai="0">LLMs</a>) have been around for a while but have really captured the attention of the public this year, with the advent of ChatGPT. LLMs are typically pretrained on massive volumes of data; recent variants are additionally tuned to follow instructions and incorporate human feedback using <a href="https://www.amazon.science/tag/reinforcement-learning" data-cms-ai="0">reinforcement learning</a>.</p><p>A fascinating ability that these LLMs demonstrate is in-context learning, where a model can learn to perform a task just by following a few (or sometimes even zero) good examples provided along with a new input. Following this paradigm of learning, larger LLMs also proved more capable of performing a wide variety of tasks than smaller ones, when the amount of pretraining data was fixed.</p><p>In a <a href="https://www.amazon.science/publications/rethinking-the-role-of-scale-for-in-context-learning-an-interpretability-based-case-study-at-66-billion-scale" data-cms-ai="0">paper</a> we’re presenting at this year’s meeting of the Association for Computational Linguistics (<a href="https://www.amazon.science/conferences-and-events/acl-2023" data-cms-ai="0">ACL</a>), we investigate the importance of model scale for in-context learning, from the perspective of architectural interpretability. We specifically ask the question <i>Are all LLM components really needed to perform in-context learning?</i></p><div data-align-right-rail=""><ps-related-content>
  <div data-media="" data-content-type="blog post">
        
            <div>
                
                    <a aria-label="20B-parameter Alexa model sets new marks in few-shot learning" href="https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning" data-cms-ai="0"><picture><source type="image/webp" width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/42f31a5/2147483647/strip/true/crop/865x489+2+0/resize/400x226!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F8d%2Fbf%2Fbb7071ba45ba92c7197cdd81285c%2F20b-encoder-decoder.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4="><source width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/f27a5d6/2147483647/strip/true/crop/865x489+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F8d%2Fbf%2Fbb7071ba45ba92c7197cdd81285c%2F20b-encoder-decoder.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    
        <img data-image-size="relatedContent" alt="20B-encoder-decoder.gif" width="400" height="226" data-src="https://assets.amazon.science/dims4/default/f27a5d6/2147483647/strip/true/crop/865x489+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F8d%2Fbf%2Fbb7071ba45ba92c7197cdd81285c%2F20b-encoder-decoder.gif" data-lazy-load="true" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    </picture>
</a>
                
            </div>
        

        <div>
            
              <p>
                  Related content
              </p>
            
            
                
            

            
    <p>With an encoder-decoder architecture — rather than decoder only — the Alexa Teacher Model excels other large language models on few-shot tasks such as summarization and machine translation.</p>


          </div>
    </div>
</ps-related-content>
</div><p>We conducted our investigation as a case study of the OPT-66B model, a 66-billion-parameter LLM that was open-sourced by Meta last year to serve as an open replica of GPT-3 (and was the largest publicly available decoder-only LLM at the time of our study). We found that a significant portion of the model could be discarded without affecting performance, indicating that OPT-66B and quite likely other prominent LLMs are undertrained.</p><p>We believe our findings are useful in helping build more powerful LLMs by identifying (or more generally providing methods to identify) architectural elements that may need to be trained better.</p><h2><p>LLM building blocks</p></h2><p>Modern LLMs use the Transformer architecture, which depends on an attention mechanism: the model learns to predict which prior tokens in the sequence it should <i>attend</i> to when predicting the current token.</p><div data-align-left-rail=""><ps-related-content>
  <div data-media="" data-content-type="blog post">
        
            <div>
                
                    <a aria-label="ACL: Computational linguistics in the age of large language models" href="https://www.amazon.science/blog/acl-computational-linguistics-in-the-age-of-large-language-models" data-cms-ai="0"><picture><source type="image/webp" width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/754777b/2147483647/strip/true/crop/1117x631+2+0/resize/400x226!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fc6%2Fcd%2F7eb8d5be4213b78e95877b929f65%2Fyang.16x9.png" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4="><source width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/1035f0b/2147483647/strip/true/crop/1117x631+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fc6%2Fcd%2F7eb8d5be4213b78e95877b929f65%2Fyang.16x9.png" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    
        <img data-image-size="relatedContent" alt="Yang.16x9.png" width="400" height="226" data-src="https://assets.amazon.science/dims4/default/1035f0b/2147483647/strip/true/crop/1117x631+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fc6%2Fcd%2F7eb8d5be4213b78e95877b929f65%2Fyang.16x9.png" data-lazy-load="true" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    </picture>
</a>
                
            </div>
        

        <div>
            
              <p>
                  Related content
              </p>
            
            
                
            

            
    <p>Amazon’s Yang Liu, general chair of this year’s meeting of the Association for Computational Linguistics, on the road ahead for LLMs.</p>


          </div>
    </div>
</ps-related-content>
</div><p>Specifically, LLMs use multihead attention, meaning that they apply multiple attention mechanisms, or heads, in parallel. OPT-66B has 64 layers with 72 attention heads in each layer. The output of the multihead attention passes through a separate feed-forward network (FFN) at each layer.</p><p>Our first method for analyzing OPT-66B was to assign a score to each attention head and FFN indicating how important they were to a given task. On the basis of those scores, we then pruned the model.</p><p>We found that important attention heads are primarily clustered in the model’s intermediate layers, and important FFNs are primarily in later layers. The ability to perform zero-/few-shot in-context learning on 14 different natural-language-processing (NLP) datasets/tasks stayed nearly intact when up to 70% (~15.7B parameters in OPT-66B) of the attention heads are removed.</p><div data-align-center=""><figure>
    

    
        <p><figcaption>A heat map representing attention heads’ aggregate importance scores for five-shot in-context learning across 14 NLP tasks, at each layer of the OPT-66B model.</figcaption></p>
    
</figure></div><p>The attention heads that are important (and unimportant) for in-context learning also seemed to overlap across tasks and shots. This indicates that a common task-agnostic subset of the attention heads is responsible for in-context learning. We also found that up to 20% of the FFNs (~8.5B parameters) can be removed with minimal decline in performance on zero-/few-shot in-context learning.</p><p>Our second analytic technique was to quantify the capacity of all attention heads in OPT-66B to perform a pair of task-agnostic primitive operations associated with in-context learning. Those primitives are <i>prefix matching</i> and <i>copying</i>: explicitly searching for a prior occurrence of the current token in context and copying over the token that succeeded it (its <i>suffix</i>).</p><div data-align-center=""><figure>
    

    
        <p><figcaption>Prefix matching and copying.</figcaption></p>
    
</figure></div><p>Heads specialized for these two operations were first discovered by the machine learning research company Anthropic and termed induction heads. We found that a small set of heads in OPT-66B have nontrivial scores for both primitives. We also found that these heads overlap (to varying degrees) with the heads important for specific tasks identified earlier. This indicates that induction heads are capable of more sophisticated behaviors associated with in-context learning, such as latent concept matching, but are not the only heads with such capabilities.</p><div data-align-right-rail=""><ps-related-content>
  <div data-media="" data-content-type="blog post">
        
            <div>
                
                    <a aria-label="Responsible AI in the generative era" href="https://www.amazon.science/blog/responsible-ai-in-the-generative-era" data-cms-ai="0"><picture><source type="image/webp" width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/d247ab4/2147483647/strip/true/crop/1912x1080+4+0/resize/400x226!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fe5%2F96%2Fab6a9ad44e6fa1758b3493ada7b6%2Fllm-watermarking.AI.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4="><source width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/7337ef8/2147483647/strip/true/crop/1912x1080+4+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fe5%2F96%2Fab6a9ad44e6fa1758b3493ada7b6%2Fllm-watermarking.AI.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    
        <img data-image-size="relatedContent" alt="LLM watermarking.AI.gif" width="400" height="226" data-src="https://assets.amazon.science/dims4/default/7337ef8/2147483647/strip/true/crop/1912x1080+4+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fe5%2F96%2Fab6a9ad44e6fa1758b3493ada7b6%2Fllm-watermarking.AI.gif" data-lazy-load="true" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    </picture>
</a>
                
            </div>
        

        <div>
            
              <p>
                  Related content
              </p>
            
            
                
            

            
    <p>Generative AI raises new challenges in defining, measuring, and mitigating concerns about fairness, toxicity, and intellectual property, among other things. But work has started on the solutions.</p>


          </div>
    </div>
</ps-related-content>
</div><p>Our overarching observation that only a core nucleus of attention heads and FFNs seem to be important for in-context learning indicates that OPT-66B and quite likely other prominent LLMs are undertrained. This also reinforces recent research that questions the efficacy of keeping the amount of pretraining data fixed when scaling models up, suggesting that the amount of pretraining data seen must be scaled hand-in-hand with the models themselves to attain optimal performance. It would be interesting to see how newer variants of LLMs released since the publication of our study, such as those tuned to follow instructions, fare in such analyses.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Not even LinkedIn is that keen on Microsoft's cloud: Shift to Azure abandoned (130 pts)]]></title>
            <link>https://www.theregister.com/2023/12/14/linkedin_abandons_migration_to_microsoft/</link>
            <guid>38656038</guid>
            <pubDate>Fri, 15 Dec 2023 17:00:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/12/14/linkedin_abandons_migration_to_microsoft/">https://www.theregister.com/2023/12/14/linkedin_abandons_migration_to_microsoft/</a>, See on <a href="https://news.ycombinator.com/item?id=38656038">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>LinkedIn has abandoned its efforts to migrate its datacenter infrastructure to Microsoft Azure four years after announcing the planned move.</p>
<p>Citing sources familiar with the matter, CNBC <a target="_blank" rel="nofollow" href="https://www.cnbc.com/2023/12/14/linkedin-shelved-plan-to-migrate-to-microsoft-azure-cloud.html">reports</a> the effort, codenamed "Blueshift," had run up against numerous challenges in the years since Microsoft acquired the professional networking site in 2016 for $27 billion.</p>
<p>In a statement to <em>The Register</em>, LinkedIn confirmed its plans to invest in its own datacenters while using Azure services where appropriate.</p>

    

<p>"This includes our running 100 employee-facing applications on Azure, leveraging Azure FrontDoor and ongoing work to consolidate our datacenter locations that are currently spread across multiple buildings under a single roof," the spokesperson said. "Azure has been crucial to support and scale collaboration and productivity for our teams to deliver value to our members."</p>

        


        

<p>The decision marks a reversal of LinkedIn's plans, <a target="_blank" href="https://engineering.linkedin.com/blog/2019/building-next-infra">announced</a> in a 2019 blog post, to migrate its workloads to a public cloud. At the time Mohak Shroff, the social network's SVP of engineering, touted the move as an opportunity to better support the site's growing membership.</p>
<p>"With the incredible member and business growth we're seeing, we've decided to begin a multi-year migration of all LinkedIn workloads to the public cloud," he wrote. "Moving to Azure will give us access to a wide array of hardware and software innovations, and unprecedented global scale."</p>
<ul>

<li><a href="https://www.theregister.com/2023/12/13/aws_sees_direct_uk_government/">In just one year, UK.gov's direct spend on AWS rises 76 percent</a></li>

<li><a href="https://www.theregister.com/2023/12/13/google_gemini_duet_ai/">Like Microsoft, Google can't stop its cloud from pouring AI all over your heads</a></li>

<li><a href="https://www.theregister.com/2023/12/07/aws_says_only_microsoft_has/">AWS accuses Microsoft of clipping customers' cloud freedoms</a></li>

<li><a href="https://www.theregister.com/2023/12/11/microsoft_union_ai_partnership/">Microsoft partners with labor unions to shape and regulate AI</a></li>
</ul>
<p>Over the past few years LinkedIn has deployed some services in Azure. In early 2022, the social network <a target="_blank" rel="nofollow" href="https://engineering.linkedin.com/blog/2022/accelerating-the-linkedin-experience-with-azure-front-door">tapped up</a> Azure FrontDoor, Microsoft's content delivery network, which caches commonly accessed content across a global network of edge datacenters reducing the bandwidth and access latencies required to serve users.</p>
<p>However, by mid-2022, CNBC reports, the cracks in LinkedIn's migration strategy were beginning to show. In a memo last summer, LinkedIn CTO Raghu Hiremagalur reportedly told employees LinkedIn was moving to a hybrid-cloud model with some services running in the cloud and others in the company's dedicated datacenters.</p>

        

<p>As it turned out, while Azure's scale may have presented a tantalizing opportunity at first blush, LinkedIn was having a hard time taking advantage of the cloud provider's software. Sources told CNBC that issues arose when LinkedIn attempted to lift and shift its existing software tools to Azure rather than refactor them to run on the cloud provider's ready made tools. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Threestudio – Framework for 3D content generation (106 pts)]]></title>
            <link>https://github.com/threestudio-project/threestudio</link>
            <guid>38655536</guid>
            <pubDate>Fri, 15 Dec 2023 16:11:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/threestudio-project/threestudio">https://github.com/threestudio-project/threestudio</a>, See on <a href="https://news.ycombinator.com/item?id=38655536">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
    <themed-picture data-catalyst-inline="true"><picture>
    <img alt="threestudio" src="https://user-images.githubusercontent.com/19284678/236847132-219999d0-4ffa-4240-a262-c2c025d15d9e.png" width="50%">
    </picture></themed-picture>
</p>
<p dir="auto"><b>
threestudio is a unified framework for 3D content creation from text prompts, single images, and few-shot images, by lifting 2D text-to-image generation models.
</b></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/245695565-f48eca9f-45a7-4092-a519-6bb99f4939e4.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1NjUtZjQ4ZWNhOWYtNDVhNy00MDkyLWE1MTktNmJiOTlmNDkzOWU0LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmMGI2NDA2Y2VmZDYzZWE1MzM2YjY3YTVlODk5NTlmZTI4N2U5MDc5YmY2YWYxMjEwMGM1M2ZhODhmMWNkYzUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8JamM8fetrNTI8koVeCEwpkvtKlCINhFKAbJfHt3FIM"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/245695565-f48eca9f-45a7-4092-a519-6bb99f4939e4.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1NjUtZjQ4ZWNhOWYtNDVhNy00MDkyLWE1MTktNmJiOTlmNDkzOWU0LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmMGI2NDA2Y2VmZDYzZWE1MzM2YjY3YTVlODk5NTlmZTI4N2U5MDc5YmY2YWYxMjEwMGM1M2ZhODhmMWNkYzUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8JamM8fetrNTI8koVeCEwpkvtKlCINhFKAbJfHt3FIM" width="100%" content-type-secured-asset="image/gif" data-animated-image=""></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/245695584-01a00207-3240-4a8e-aa6f-d48436370fe7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1ODQtMDFhMDAyMDctMzI0MC00YThlLWFhNmYtZDQ4NDM2MzcwZmU3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY0OWZlMTczNGFiYmZhMDBhMDY4NTQwYmY1YzJiMjU4OGFkMmNlNDNhMjM0ZThjNTk1N2U3N2IzOGVlMWE2MWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ExiA4qTu-NdNyREC3APhNUegdwYkr-ZXtxRSuX5S3VE"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/245695584-01a00207-3240-4a8e-aa6f-d48436370fe7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1ODQtMDFhMDAyMDctMzI0MC00YThlLWFhNmYtZDQ4NDM2MzcwZmU3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY0OWZlMTczNGFiYmZhMDBhMDY4NTQwYmY1YzJiMjU4OGFkMmNlNDNhMjM0ZThjNTk1N2U3N2IzOGVlMWE2MWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ExiA4qTu-NdNyREC3APhNUegdwYkr-ZXtxRSuX5S3VE" width="100%"></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047444-1dbdebab-43d5-4830-872c-66b38d9fda92.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0NDQtMWRiZGViYWItNDNkNS00ODMwLTg3MmMtNjZiMzhkOWZkYTkyLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJmZmFkZmQ5Y2EwZDE1NmQxZWQ5M2Y1OWIyZjFkYjA2YWY5ZGIxNTUwN2ZiMTFlMjhjZDYxNWI4ZTQzZGIzNGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.YEGdWJPEFq_I3_dXZs8VrpBWQL15OTSvEJSBQx8VmLw"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047444-1dbdebab-43d5-4830-872c-66b38d9fda92.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0NDQtMWRiZGViYWItNDNkNS00ODMwLTg3MmMtNjZiMzhkOWZkYTkyLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJmZmFkZmQ5Y2EwZDE1NmQxZWQ5M2Y1OWIyZjFkYjA2YWY5ZGIxNTUwN2ZiMTFlMjhjZDYxNWI4ZTQzZGIzNGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.YEGdWJPEFq_I3_dXZs8VrpBWQL15OTSvEJSBQx8VmLw" width="60%" data-animated-image=""></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047384-437b4044-142c-4e5d-a406-4d9bad0205e1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDczODQtNDM3YjQwNDQtMTQyYy00ZTVkLWE0MDYtNGQ5YmFkMDIwNWUxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUwMWZlY2YxZGVhZTAyNWI3YmNmYWRkMDA5MTIxYmQ4ZjZjYmNlNzI5YTAwYjNkYTg1NTMyNjU2ODM2YTU5NTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.or4eWogG-TmYCRFnSpMrLxDbVjwhnPP8E4dJAgkzmpE"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047384-437b4044-142c-4e5d-a406-4d9bad0205e1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDczODQtNDM3YjQwNDQtMTQyYy00ZTVkLWE0MDYtNGQ5YmFkMDIwNWUxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUwMWZlY2YxZGVhZTAyNWI3YmNmYWRkMDA5MTIxYmQ4ZjZjYmNlNzI5YTAwYjNkYTg1NTMyNjU2ODM2YTU5NTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.or4eWogG-TmYCRFnSpMrLxDbVjwhnPP8E4dJAgkzmpE" width="60%"></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047426-4f4d62c5-2304-4e20-b632-afe6d144a203.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0MjYtNGY0ZDYyYzUtMjMwNC00ZTIwLWI2MzItYWZlNmQxNDRhMjAzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExZTkyMmI3NzFjMjlkNTEyMjc2NmMwOGFmYmU1OWNlYWNmOWYzZGU0NzlkNDAxMzYxYmJmZWM0NmY0ZDUyMTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VWK9TDoRp7g-3L3TCbGjkoTDZFD--DA8GZNOLBEe2h4"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047426-4f4d62c5-2304-4e20-b632-afe6d144a203.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0MjYtNGY0ZDYyYzUtMjMwNC00ZTIwLWI2MzItYWZlNmQxNDRhMjAzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExZTkyMmI3NzFjMjlkNTEyMjc2NmMwOGFmYmU1OWNlYWNmOWYzZGU0NzlkNDAxMzYxYmJmZWM0NmY0ZDUyMTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VWK9TDoRp7g-3L3TCbGjkoTDZFD--DA8GZNOLBEe2h4" width="68%" data-animated-image=""></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047241-2f36ddbd-e3cf-4431-b269-47a9cb3d6e6e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDcyNDEtMmYzNmRkYmQtZTNjZi00NDMxLWIyNjktNDdhOWNiM2Q2ZTZlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmODlmY2I0MmFhYWYwODk0YTgwMjNhMDZjZjQxYWM1ZWRjMmJlNjgxOTM1ZDI1YTE0ZTI3OGUyN2I4NDA2OTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.diZG2o4ut4OpZoM3INnDyk2Vkq5U0W4MVr7IMzGOyQc"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047241-2f36ddbd-e3cf-4431-b269-47a9cb3d6e6e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDcyNDEtMmYzNmRkYmQtZTNjZi00NDMxLWIyNjktNDdhOWNiM2Q2ZTZlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmODlmY2I0MmFhYWYwODk0YTgwMjNhMDZjZjQxYWM1ZWRjMmJlNjgxOTM1ZDI1YTE0ZTI3OGUyN2I4NDA2OTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.diZG2o4ut4OpZoM3INnDyk2Vkq5U0W4MVr7IMzGOyQc" width="68%"></a>
</p>
<p dir="auto"><b>
👆 Results obtained from methods implemented by threestudio 👆 <br>
| <a href="https://ml.cs.tsinghua.edu.cn/prolificdreamer/" rel="nofollow">ProlificDreamer</a> | <a href="https://dreamfusion3d.github.io/" rel="nofollow">DreamFusion</a> | <a href="https://research.nvidia.com/labs/dir/magic3d/" rel="nofollow">Magic3D</a> | <a href="https://pals.ttic.edu/p/score-jacobian-chaining" rel="nofollow">SJC</a> | <a href="https://github.com/eladrich/latent-nerf">Latent-NeRF</a> | <a href="https://fantasia3d.github.io/" rel="nofollow">Fantasia3D</a> | <a href="https://fabi92.github.io/textmesh/" rel="nofollow">TextMesh</a> |
<br>
| <a href="https://zero123.cs.columbia.edu/" rel="nofollow">Zero-1-to-3</a> | <a href="https://guochengqian.github.io/project/magic123/" rel="nofollow">Magic123</a> |
<br>
| <a href="https://instruct-nerf2nerf.github.io/" rel="nofollow">InstructNeRF2NeRF</a> | <a href="https://control4darxiv.github.io/" rel="nofollow">Control4D</a> |
</b>
</p><p dir="auto">
  <a href="https://colab.research.google.com/github/threestudio-project/threestudio/blob/main/threestudio.ipynb" rel="nofollow">
  <img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg">
  </a>
  <a href="https://huggingface.co/spaces/bennyguo/threestudio" rel="nofollow"><img src="https://camo.githubusercontent.com/d62c84d474b9ca5604efd7987fe4a377b12835d0274154b3c6addfa140ec4809/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323047726164696f25323044656d6f2d48756767696e67666163652d6f72616e6765" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Gradio%20Demo-Huggingface-orange"></a>
  <a href="http://t23-g-01.threestudio.ai/" rel="nofollow"><img src="https://camo.githubusercontent.com/95c56fd6e110f90d44c0a85ef004d02a627e0db551b8c5ff88b2bdd24ca01a5f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47726164696f25323044656d6f2d54656e63656e742d626c75653f6c6f676f3d74656e63656e747171266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Gradio%20Demo-Tencent-blue?logo=tencentqq&amp;logoColor=white"></a>
  <a href="https://discord.gg/ejer2MAB8N" rel="nofollow"><img src="https://camo.githubusercontent.com/4d4aaf8201525ce15823a9d09c37ecbd84dfa70300a9b42c247dbe0a00d78388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d3538363546323f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;logoColor=white"></a>
</p>
<p dir="auto">
    Did not find what you want? Checkout <a href="https://threestudio-project.github.io/threestudio-extensions/" rel="nofollow"><b>threestudio-extension</b></a> or submit a feature request <a href="https://github.com/threestudio-project/threestudio/discussions/46" data-hovercard-type="discussion" data-hovercard-url="/threestudio-project/threestudio/discussions/46/hovercard">here</a>!
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/24589363/287081695-ac6089a7-d88f-414c-96d6-a5e75616115a.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODE2OTUtYWM2MDg5YTctZDg4Zi00MTRjLTk2ZDYtYTVlNzU2MTYxMTVhLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc3NmI1YmJlNzhjZGE1ZmZkMDUwZjZhZWFhMTg3ZGNlYjliZTQ4OTY3N2UwZWJlODQzNThmZTk1ZDI0YjUwMjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nF3tdOu7fiGcX4EYiCyQiEbpIOw9ZFc8tbNdz34cnL4"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/24589363/287081695-ac6089a7-d88f-414c-96d6-a5e75616115a.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODE2OTUtYWM2MDg5YTctZDg4Zi00MTRjLTk2ZDYtYTVlNzU2MTYxMTVhLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc3NmI1YmJlNzhjZGE1ZmZkMDUwZjZhZWFhMTg3ZGNlYjliZTQ4OTY3N2UwZWJlODQzNThmZTk1ZDI0YjUwMjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nF3tdOu7fiGcX4EYiCyQiEbpIOw9ZFc8tbNdz34cnL4" width="68%"></a>
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/24589363/287080628-8892898f-8bd8-43dc-a4ec-dd8d078af860.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODA2MjgtODg5Mjg5OGYtOGJkOC00M2RjLWE0ZWMtZGQ4ZDA3OGFmODYwLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI2YTQ0YjNmMDQ5ZWMzNjMxYTg2Y2I5ZjgzYjY2M2Y4NzViMzA4OTM5Nzc5MDQ0NTU1YzkzYjllMmMzM2RiM2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.0HWYlYXHYbpuyoTxGxvKmeGyub_4kUOM_s9J6g7PCus"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/24589363/287080628-8892898f-8bd8-43dc-a4ec-dd8d078af860.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODA2MjgtODg5Mjg5OGYtOGJkOC00M2RjLWE0ZWMtZGQ4ZDA3OGFmODYwLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI2YTQ0YjNmMDQ5ZWMzNjMxYTg2Y2I5ZjgzYjY2M2Y4NzViMzA4OTM5Nzc5MDQ0NTU1YzkzYjllMmMzM2RiM2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.0HWYlYXHYbpuyoTxGxvKmeGyub_4kUOM_s9J6g7PCus" width="50%" data-animated-image=""></a>
</p>
<h2 tabindex="-1" dir="auto">News</h2>
<ul dir="auto">
<li>11/30/2023 Implementation of <a href="https://github.com/DSaurus/threestudio-mvdream">MVDream</a>, <a href="https://github.com/DSaurus/threestudio-3dgs">Gaussian Splatting</a> as the custom extensions. You can also use neural representation to fit a mesh by <a href="https://github.com/DSaurus/threestudio-meshfitting">Mesh-Fitting</a>.</li>
<li>11/30/2023: Implementation of <a href="https://threestudio-project.github.io/threestudio-extensions/" rel="nofollow">custom extension system</a> and you can add your extensions in <a href="https://github.com/threestudio-project/threestudio-extensions">this project</a>.</li>
<li>08/25/2023: Implementation of <a href="https://guochengqian.github.io/project/magic123/" rel="nofollow">Magic123</a>! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#magic123-">here</a> to give it a try.</li>
<li>07/06/2023: Join our <a href="https://discord.gg/ejer2MAB8N" rel="nofollow">Discord server</a> for lively discussions!</li>
<li>07/03/2023: Try text-to-3D online in <a href="https://huggingface.co/spaces/bennyguo/threestudio" rel="nofollow">HuggingFace Spaces</a> or using our <a href="http://t23-g-01.threestudio.ai/" rel="nofollow">self-hosted service</a> (GPU support from Tencent). To host the web interface locally, see <a href="https://github.com/threestudio-project/threestudio#gradio-web-interface">here</a>.</li>
<li>06/20/2023: Implementations of Instruct-NeRF2NeRF and Control4D for high-fidelity 3D editing! Follow the instructions for <a href="https://github.com/threestudio-project/threestudio#control4d-">Control4D</a> and <a href="https://github.com/threestudio-project/threestudio#instructnerf2nerf-">Instruct-NeRF2NeRF</a> to give it a try.</li>
<li>06/14/2023: Implementation of TextMesh! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#textmesh-">here</a> to give it a try.</li>
<li>06/14/2023: Implementation of <a href="https://arxiv.org/abs/2303.15413" rel="nofollow">prompt debiasing</a> and <a href="https://perp-neg.github.io/" rel="nofollow">Perp-Neg</a>! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#tips-on-improving-quality">here</a> to give it a try.</li>
<li>05/29/2023: An experimental implementation of using <a href="https://zero123.cs.columbia.edu/" rel="nofollow">Zero-1-to-3</a> for 3D generation from a single image! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#zero-1-to-3-">here</a> to give it a try.</li>
<li>05/26/2023: Implementation of <a href="https://ml.cs.tsinghua.edu.cn/prolificdreamer/" rel="nofollow">ProlificDreamer</a>! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#prolificdreamer-">here</a> to give it a try.</li>
<li>05/14/2023: You can experiment with the SDS loss on 2D images using our <a href="https://github.com/threestudio-project/threestudio/blob/main/2dplayground.ipynb">2dplayground</a>.</li>
<li>05/13/2023: You can now try threestudio on <a href="https://colab.research.google.com/github/threestudio-project/threestudio/blob/main/threestudio.ipynb" rel="nofollow">Google Colab</a>!</li>
<li>05/11/2023: We now support exporting textured meshes! See <a href="https://github.com/threestudio-project/threestudio#export-meshes">here</a> for instructions.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/237616977-ccae2820-e702-484c-a43f-81678a365427.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzc2MTY5NzctY2NhZTI4MjAtZTcwMi00ODRjLWE0M2YtODE2NzhhMzY1NDI3LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdlNWQ1YzhlMTMzNTIxMzgxMDQ4NDk3YWMzNmUwYWM4OWEzY2RhOGExZjU2ZmFkNzBjM2Q1MWI2N2U1ZTVhMTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Z5ohn3i68p0q-RmIKMwOUq-vSFejlKGL7eJd32OUXKI"><img src="https://private-user-images.githubusercontent.com/19284678/237616977-ccae2820-e702-484c-a43f-81678a365427.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzc2MTY5NzctY2NhZTI4MjAtZTcwMi00ODRjLWE0M2YtODE2NzhhMzY1NDI3LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdlNWQ1YzhlMTMzNTIxMzgxMDQ4NDk3YWMzNmUwYWM4OWEzY2RhOGExZjU2ZmFkNzBjM2Q1MWI2N2U1ZTVhMTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Z5ohn3i68p0q-RmIKMwOUq-vSFejlKGL7eJd32OUXKI" alt="export-blender"></a></p>
<h2 tabindex="-1" dir="auto">Installation</h2>
<p dir="auto">See <a href="https://github.com/threestudio-project/threestudio/blob/main/docs/installation.md">installation.md</a> for additional information, including installation via Docker.</p>
<p dir="auto">The following steps have been tested on Ubuntu20.04.</p>
<ul dir="auto">
<li>You must have an NVIDIA graphics card with at least 6GB VRAM and have <a href="https://developer.nvidia.com/cuda-downloads" rel="nofollow">CUDA</a> installed.</li>
<li>Install <code>Python &gt;= 3.8</code>.</li>
<li>(Optional, Recommended) Create a virtual environment:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m virtualenv venv
. venv/bin/activate

# Newer pip versions, e.g. pip-23.x, can be much faster than old versions, e.g. pip-20.x.
# For instance, it caches the wheels of git packages to avoid unnecessarily rebuilding them later.
python3 -m pip install --upgrade pip"><pre>python3 -m virtualenv venv
<span>.</span> venv/bin/activate

<span><span>#</span> Newer pip versions, e.g. pip-23.x, can be much faster than old versions, e.g. pip-20.x.</span>
<span><span>#</span> For instance, it caches the wheels of git packages to avoid unnecessarily rebuilding them later.</span>
python3 -m pip install --upgrade pip</pre></div>
<ul dir="auto">
<li>Install <code>PyTorch &gt;= 1.12</code>. We have tested on <code>torch1.12.1+cu113</code> and <code>torch2.0.0+cu118</code>, but other versions should also work fine.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# torch1.12.1+cu113
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113
# or torch2.0.0+cu118
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"><pre><span><span>#</span> torch1.12.1+cu113</span>
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113
<span><span>#</span> or torch2.0.0+cu118</span>
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118</pre></div>
<ul dir="auto">
<li>(Optional, Recommended) Install ninja to speed up the compilation of CUDA extensions:</li>
</ul>

<ul dir="auto">
<li>Install dependencies:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<ul dir="auto">
<li>
<p dir="auto">(Optional) <code>tiny-cuda-nn</code> installation might require downgrading pip to 23.0.1</p>
</li>
<li>
<p dir="auto">(Optional, Recommended) The best-performing models in threestudio use the newly-released T2I model <a href="https://github.com/deep-floyd/IF">DeepFloyd IF</a>, which currently requires signing a license agreement. If you would like to use these models, you need to <a href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0" rel="nofollow">accept the license on the model card of DeepFloyd IF</a>, and login into the Hugging Face hub in the terminal by <code>huggingface-cli login</code>.</p>
</li>
<li>
<p dir="auto">For contributors, see <a href="https://github.com/threestudio-project/threestudio#contributing-to-threestudio">here</a>.</p>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Quickstart</h2>
<p dir="auto">Here we show some basic usage of threestudio. First let's train a DreamFusion model to create a classic pancake bunny.</p>
<p dir="auto"><strong>If you are experiencing unstable connections with Hugging Face, we suggest you either (1) setting environment variable <code>TRANSFORMERS_OFFLINE=1 DIFFUSERS_OFFLINE=1 HF_HUB_OFFLINE=1</code> before your running command after all needed files have been fetched on the first run, to prevent from connecting to Hugging Face each time you run, or (2) downloading the guidance model you used to a local folder following <a href="https://huggingface.co/docs/huggingface_hub/v0.14.1/guides/download#download-an-entire-repository" rel="nofollow">here</a> and <a href="https://huggingface.co/docs/huggingface_hub/v0.14.1/guides/download#download-files-to-local-folder" rel="nofollow">here</a>, and set <code>pretrained_model_name_or_path</code> of the guidance and the prompt processor to the local path.</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# if you have agreed the license of DeepFloyd IF and have >20GB VRAM
# please try this configuration for higher quality
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot;
# otherwise you could try with the Stable Diffusion model, which fits in 6GB VRAM
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot;"><pre><span><span>#</span> if you have agreed the license of DeepFloyd IF and have &gt;20GB VRAM</span>
<span><span>#</span> please try this configuration for higher quality</span>
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span>
<span><span>#</span> otherwise you could try with the Stable Diffusion model, which fits in 6GB VRAM</span>
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span></pre></div>
<p dir="auto">threestudio uses <a href="https://github.com/omry/omegaconf">OmegaConf</a> for flexible configurations. You can easily change any configuration in the YAML file by specifying arguments without <code>--</code>, for example the specified prompt in the above cases. For all supported configurations, please see our <a href="https://github.com/threestudio-project/threestudio/blob/main/DOCUMENTATION.md">documentation</a>.</p>
<p dir="auto">The training lasts for 10,000 iterations. You can find visualizations of the current status in the trial directory which defaults to <code>[exp_root_dir]/[name]/[tag]@[timestamp]</code>, where <code>exp_root_dir</code> (<code>outputs/</code> by default), <code>name</code> and <code>tag</code> can be set in the configuration file. A 360-degree video will be generated after the training is completed. In training, press <code>ctrl+c</code> one time will stop training and head directly to the test stage which generates the video. Press <code>ctrl+c</code> the second time to fully quit the program.</p>
<h3 tabindex="-1" dir="auto">Multi-GPU training</h3>
<p dir="auto">Multi-GPU training is supported, but may still be <a href="https://github.com/threestudio-project/threestudio/issues/195" data-hovercard-type="issue" data-hovercard-url="/threestudio-project/threestudio/issues/195/hovercard">buggy</a>. Note that <code>data.batch_size</code> is the batch size <strong>per rank (device)</strong>. Also remember to</p>
<ul dir="auto">
<li>Set <code>data.n_val_views</code> to be a multiple of the number of GPUs.</li>
<li>Set a unique <code>tag</code> as timestamp is disabled in multi-GPU training and will not be appended after the tag. If you the same tag as previous trials, saved config files, code and visualizations will be overridden.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# this results in an effective batch size of 4 (number of GPUs) * 2 (data.batch_size) = 8
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0,1,2,3 system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot; data.batch_size=2 data.n_val_views=4"><pre><span><span>#</span> this results in an effective batch size of 4 (number of GPUs) * 2 (data.batch_size) = 8</span>
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0,1,2,3 system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span> data.batch_size=2 data.n_val_views=4</pre></div>
<p dir="auto">If you define the <code>CUDA_VISIBLE_DEVICES</code> environment variable before you call <code>launch.py</code>, you don't need to specify <code>--gpu</code> - this will use all available GPUs from <code>CUDA_VISIBLE_DEVICES</code>. For instance, the following command will automatically use GPUs 3 and 4:</p>
<p dir="auto"><code>CUDA_VISIBLE_DEVICES=3,4 python launch.py --config configs/dreamfusion-if.yaml --train system.prompt_processor.prompt="a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes"</code></p>
<p dir="auto">This is particularly useful if you run <code>launch.py</code> in a cluster using a command that automatically picks GPU(s) and exports their IDs through CUDA_VISIBLE_DEVICES, e.g. through SLURM:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd git/threestudio
. venv/bin/activate
srun --account mod3d --partition=g40 --gpus=1 --job-name=3s_bunny python launch.py --config configs/dreamfusion-if.yaml --train system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot;"><pre><span>cd</span> git/threestudio
<span>.</span> venv/bin/activate
srun --account mod3d --partition=g40 --gpus=1 --job-name=3s_bunny python launch.py --config configs/dreamfusion-if.yaml --train system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span></pre></div>
<h3 tabindex="-1" dir="auto">Resume from checkpoints</h3>
<p dir="auto">If you want to resume from a checkpoint, do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# resume training from the last checkpoint, you may replace last.ckpt with any other checkpoints
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
# if the training has completed, you can still continue training for a longer time by setting trainer.max_steps
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt trainer.max_steps=20000
# you can also perform testing using resumed checkpoints
python launch.py --config path/to/trial/dir/configs/parsed.yaml --test --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
# note that the above commands use parsed configuration files from previous trials
# which will continue using the same trial directory
# if you want to save to a new trial directory, replace parsed.yaml with raw.yaml in the command

# only load weights from saved checkpoint but dont resume training (i.e. dont load optimizer state):
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 system.weights=path/to/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> resume training from the last checkpoint, you may replace last.ckpt with any other checkpoints</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
<span><span>#</span> if the training has completed, you can still continue training for a longer time by setting trainer.max_steps</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt trainer.max_steps=20000
<span><span>#</span> you can also perform testing using resumed checkpoints</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --test --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
<span><span>#</span> note that the above commands use parsed configuration files from previous trials</span>
<span><span>#</span> which will continue using the same trial directory</span>
<span><span>#</span> if you want to save to a new trial directory, replace parsed.yaml with raw.yaml in the command</span>

<span><span>#</span> only load weights from saved checkpoint but dont resume training (i.e. dont load optimizer state):</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 system.weights=path/to/trial/dir/ckpts/last.ckpt</pre></div>
<h3 tabindex="-1" dir="auto">Export Meshes</h3>
<p dir="auto">To export the scene to texture meshes, use the <code>--export</code> option. We currently support exporting to obj+mtl, or obj with vertex colors.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# this uses default mesh-exporter configurations which exports obj+mtl
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter
# specify system.exporter.fmt=obj to get obj with vertex colors
# you may also add system.exporter.save_uv=false to accelerate the process, suitable for a quick peek of the result
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.exporter.fmt=obj
# for NeRF-based methods (DreamFusion, Magic3D coarse, Latent-NeRF, SJC)
# you may need to adjust the isosurface threshold (25 by default) to get satisfying outputs
# decrease the threshold if the extracted model is incomplete, increase if it is extruded
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_threshold=10.
# use marching cubes of higher resolutions to get more detailed models
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_method=mc-cpu system.geometry.isosurface_resolution=256"><pre><span><span>#</span> this uses default mesh-exporter configurations which exports obj+mtl</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter
<span><span>#</span> specify system.exporter.fmt=obj to get obj with vertex colors</span>
<span><span>#</span> you may also add system.exporter.save_uv=false to accelerate the process, suitable for a quick peek of the result</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.exporter.fmt=obj
<span><span>#</span> for NeRF-based methods (DreamFusion, Magic3D coarse, Latent-NeRF, SJC)</span>
<span><span>#</span> you may need to adjust the isosurface threshold (25 by default) to get satisfying outputs</span>
<span><span>#</span> decrease the threshold if the extracted model is incomplete, increase if it is extruded</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_threshold=10.
<span><span>#</span> use marching cubes of higher resolutions to get more detailed models</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_method=mc-cpu system.geometry.isosurface_resolution=256</pre></div>
<p dir="auto">For all the options you can specify when exporting, see <a href="https://github.com/threestudio-project/threestudio/blob/main/DOCUMENTATION.md#exporters">the documentation</a>.</p>
<p dir="auto">See <a href="https://github.com/threestudio-project/threestudio#supported-models">here</a> for example running commands of all our supported models. Please refer to <a href="https://github.com/threestudio-project/threestudio#tips-on-improving-quality">here</a> for tips on getting higher-quality results, and <a href="https://github.com/threestudio-project/threestudio#vram-optimization">here</a> for reducing VRAM usage.</p>
<h3 tabindex="-1" dir="auto">Gradio Web Interface</h3>
<p dir="auto">Launch the Gradio web interface by</p>
<div data-snippet-clipboard-copy-content="python gradio_app.py launch"><pre><code>python gradio_app.py launch
</code></pre></div>
<p dir="auto">Parameters:</p>
<ul dir="auto">
<li><code>--listen</code>: listens to all addresses by setting <code>server_name="0.0.0.0"</code> when launching the Gradio app.</li>
<li><code>--self-deploy</code>: enables changing arbitrary configurations directly from the web.</li>
<li><code>--save</code>: enables checkpoint saving.</li>
</ul>
<p dir="auto">For feature requests, bug reports, or discussions about technical problems, please <a href="https://github.com/threestudio-project/threestudio/issues/new">file an issue</a>. In case you want to discuss the generation quality or showcase your generation results, please feel free to participate in the <a href="https://github.com/threestudio-project/threestudio/discussions">discussion panel</a>.</p>
<h2 tabindex="-1" dir="auto">Supported Models</h2>
<h3 tabindex="-1" dir="auto">ProlificDreamer <a href="https://arxiv.org/abs/2305.16213" rel="nofollow"><img src="https://camo.githubusercontent.com/c50bc699e2a0a94d97f8594bf640ecd1f2b1732cd84367106970fcef26f3a61c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330352e31363231332d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2305.16213-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>This is an unofficial experimental implementation! Please refer to <a href="https://github.com/thu-ml/prolificdreamer">https://github.com/thu-ml/prolificdreamer</a> for official code release.</strong></p>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion, 256x256 Stage1)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description prolificdreamer.mp4">prolificdreamer.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/243357000-27b42d8f-4aa4-4b47-8ea0-0f77db90fd1e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMDAtMjdiNDJkOGYtNGFhNC00YjQ3LThlYTAtMGY3N2RiOTBmZDFlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBiYjUxMDdlZTZhMDJmNzlhYWY2ZmM4MTU3MDYwMWI3MWYxYjI1YmVjMjgxN2FiMTNmZjRhM2I4ZTAyMGExY2UmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Q-2NK5juuNSn7NG22DuLEh1tlNpOOqDFbI7rV_izums" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/243357000-27b42d8f-4aa4-4b47-8ea0-0f77db90fd1e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMDAtMjdiNDJkOGYtNGFhNC00YjQ3LThlYTAtMGY3N2RiOTBmZDFlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBiYjUxMDdlZTZhMDJmNzlhYWY2ZmM4MTU3MDYwMWI3MWYxYjI1YmVjMjgxN2FiMTNmZjRhM2I4ZTAyMGExY2UmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Q-2NK5juuNSn7NG22DuLEh1tlNpOOqDFbI7rV_izums" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    
    <span aria-label="Video description prolificdreamer-scene.mp4">prolificdreamer-scene.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/243357021-ffcbbb01-3817-4663-a2bf-5e21a076bc3d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMjEtZmZjYmJiMDEtMzgxNy00NjYzLWEyYmYtNWUyMWEwNzZiYzNkLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY2NWRhYWNmZTc4ZjAyOTg5OWQ1ODMyYTEzODJiNjc3NDgxZDkxNTNiYmU1YTgyNzViYjFmMzE1ZGM3NWQ4YzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.u1mDjx6qaWce6cvEFYtL_QiLcgJMZrJX5B11yW0FSik" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/243357021-ffcbbb01-3817-4663-a2bf-5e21a076bc3d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMjEtZmZjYmJiMDEtMzgxNy00NjYzLWEyYmYtNWUyMWEwNzZiYzNkLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY2NWRhYWNmZTc4ZjAyOTg5OWQ1ODMyYTEzODJiNjc3NDgxZDkxNTNiYmU1YTgyNzViYjFmMzE1ZGM3NWQ4YzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.u1mDjx6qaWce6cvEFYtL_QiLcgJMZrJX5B11yW0FSik" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion, 256x256 Stage1, 512x512 Stage2+3)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description prolificdreamer-full.mp4">prolificdreamer-full.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/243357051-cfab881e-18dc-45fc-8384-7476f835b36e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwNTEtY2ZhYjg4MWUtMThkYy00NWZjLTgzODQtNzQ3NmY4MzViMzZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZiZjQ4YTRlZDAzNzg2NzE5MjBjZWMxMGY0MzU5ZmNjNzZhMzFkNTQ4ZjM5ZDBiODlhMTRkYWJjZTllZWExZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.uDVbLmY5OiDwVhzA-Lv5Z7V3vdq6u38vIO5AnXxOWfI" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/243357051-cfab881e-18dc-45fc-8384-7476f835b36e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwNTEtY2ZhYjg4MWUtMThkYy00NWZjLTgzODQtNzQ3NmY4MzViMzZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZiZjQ4YTRlZDAzNzg2NzE5MjBjZWMxMGY0MzU5ZmNjNzZhMzFkNTQ4ZjM5ZDBiODlhMTRkYWJjZTllZWExZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.uDVbLmY5OiDwVhzA-Lv5Z7V3vdq6u38vIO5AnXxOWfI" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Notable differences from the paper:</p>
<ul dir="auto">
<li>ProlificDreamer adopts a two-stage sampling strategy with 64 coarse samples and 32 fine samples, while we only use 512 coarse samples.</li>
<li>In the first stage, we only render 64x64 images at the first 5000 iterations. After that, as the empty space has been effectively pruned, rendering 512x512 images wouldn't cost too much VRAM.</li>
<li>We currently don't support multiple particles.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- Stage 1 (NeRF) --------- #
# object generation with 512x512 NeRF rendering, ~30GB VRAM
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot;
# if you don't have enough VRAM, try training with 64x64 NeRF rendering, ~15GB VRAM
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; data.width=64 data.height=64 data.batch_size=1
# using the same model for pretrained and LoRA enables 64x64 training with <10GB VRAM
# but the quality is worse due to the use of an epsilon prediction model for LoRA training
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; data.width=64 data.height=64 data.batch_size=1 system.guidance.pretrained_model_name_or_path_lora=&quot;stabilityai/stable-diffusion-2-1-base&quot;
# Using patch-based renderer to reduce memory consume, 512x512 resolution, ~20GB VRAM
python launch.py --config configs/prolificdreamer-patch.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot;
# scene generation with 512x512 NeRF rendering, ~30GB VRAM
python launch.py --config configs/prolificdreamer-scene.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;Inside of a smart home, realistic detailed photo, 4k&quot;

# --------- Stage 2 (Geometry Refinement) --------- #
# refine geometry with 512x512 rasterization, Stable Diffusion SDS guidance
python launch.py --config configs/prolificdreamer-geometry.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; system.geometry_convert_from=path/to/stage1/trial/dir/ckpts/last.ckpt

# --------- Stage 3 (Texturing) --------- #
# texturing with 512x512 rasterization, Stable Difusion VSD guidance
python launch.py --config configs/prolificdreamer-texture.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; system.geometry_convert_from=path/to/stage2/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> --------- Stage 1 (NeRF) --------- #</span>
<span><span>#</span> object generation with 512x512 NeRF rendering, ~30GB VRAM</span>
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span>
<span><span>#</span> if you don't have enough VRAM, try training with 64x64 NeRF rendering, ~15GB VRAM</span>
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> data.width=64 data.height=64 data.batch_size=1
<span><span>#</span> using the same model for pretrained and LoRA enables 64x64 training with &lt;10GB VRAM</span>
<span><span>#</span> but the quality is worse due to the use of an epsilon prediction model for LoRA training</span>
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> data.width=64 data.height=64 data.batch_size=1 system.guidance.pretrained_model_name_or_path_lora=<span><span>"</span>stabilityai/stable-diffusion-2-1-base<span>"</span></span>
<span><span>#</span> Using patch-based renderer to reduce memory consume, 512x512 resolution, ~20GB VRAM</span>
python launch.py --config configs/prolificdreamer-patch.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span>
<span><span>#</span> scene generation with 512x512 NeRF rendering, ~30GB VRAM</span>
python launch.py --config configs/prolificdreamer-scene.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>Inside of a smart home, realistic detailed photo, 4k<span>"</span></span>

<span><span>#</span> --------- Stage 2 (Geometry Refinement) --------- #</span>
<span><span>#</span> refine geometry with 512x512 rasterization, Stable Diffusion SDS guidance</span>
python launch.py --config configs/prolificdreamer-geometry.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> system.geometry_convert_from=path/to/stage1/trial/dir/ckpts/last.ckpt

<span><span>#</span> --------- Stage 3 (Texturing) --------- #</span>
<span><span>#</span> texturing with 512x512 rasterization, Stable Difusion VSD guidance</span>
python launch.py --config configs/prolificdreamer-texture.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> system.geometry_convert_from=path/to/stage2/trial/dir/ckpts/last.ckpt</pre></div>
<h3 tabindex="-1" dir="auto">DreamFusion <a href="https://arxiv.org/abs/2209.14988" rel="nofollow"><img src="https://camo.githubusercontent.com/0a00143f284574687e6b04ef66124417c8da00ce27e46ee6dfc3dc8fe7e2465d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323230392e31343938382d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2209.14988-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (DeepFloyd IF, batch size 8)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description dreamfusion-if.mp4">dreamfusion-if.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694848-38ae4ea4-554b-4c9d-b4c7-fba5bee3acb3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NDgtMzhhZTRlYTQtNTU0Yi00YzlkLWI0YzctZmJhNWJlZTNhY2IzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWViNjEzM2NkOTA4NDM4OGZjZjJjY2I1YTFiZGFiMjBiOGNhMGNiMGYzMTg5NjYxMmU0MTI0YjcxMjI5Mjk0MTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.5Ifmg-F54IVJFg-Uc9oKpcXsT3h9VjGy2wFfjz2kTP4" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694848-38ae4ea4-554b-4c9d-b4c7-fba5bee3acb3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NDgtMzhhZTRlYTQtNTU0Yi00YzlkLWI0YzctZmJhNWJlZTNhY2IzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWViNjEzM2NkOTA4NDM4OGZjZjJjY2I1YTFiZGFiMjBiOGNhMGNiMGYzMTg5NjYxMmU0MTI0YjcxMjI5Mjk0MTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.5Ifmg-F54IVJFg-Uc9oKpcXsT3h9VjGy2wFfjz2kTP4" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>We use open-source T2I models (StableDiffusion, DeepFloyd IF), while the paper uses Imagen.</li>
<li>We use a guidance scale of 20 for DeepFloyd IF, while the paper uses 100 for Imagen.</li>
<li>We do not use sigmoid to normalize the albedo color but simply scale the color from <code>[-1,1]</code> to <code>[0,1]</code>, as we find this help convergence.</li>
<li>We use HashGrid encoding and uniformly sample points along rays, while the paper uses Integrated Positional Encoding and sampling strategy from MipNeRF360.</li>
<li>We adopt camera settings and density initialization strategy from Magic3D, which is slightly different from the DreamFusion paper.</li>
<li>Some hyperparameters are different, such as the weighting of loss terms.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training
# here we adopt random background augmentation to improve geometry quality
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.background.random_aug=true
# uses StableDiffusion, requires ~6GB VRAM in training
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;"><pre><span><span>#</span> uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training</span>
<span><span>#</span> here we adopt random background augmentation to improve geometry quality</span>
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.background.random_aug=true
<span><span>#</span> uses StableDiffusion, requires ~6GB VRAM in training</span>
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span></pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>DeepFloyd IF performs <strong>way better than</strong> StableDiffusion.</li>
<li>Validation shows albedo color before <code>system.material.ambient_only_steps</code> and shaded color after that.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_sparsity</code> if your scene is stuffed with floaters/becoming empty.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_orient</code> if you object is foggy/over-smoothed.</li>
<li>Try replacing the background to random colors with a probability 0.5 by setting <code>system.background.random_aug=true</code> if you find the model incorrectly treats the background as part of the object.</li>
<li>DeepFloyd IF uses T5-XXL as its text encoder, which consumes ~15GB VRAM even when using 8-bit quantization. This is currently the bottleneck for training with less VRAM. If anyone knows how to run the text encoder with less VRAM, please file an issue. We're also trying to push the text encoder to <a href="https://replicate.com/" rel="nofollow">Replicate</a> to enable extracting text embeddings via API, but are having some network connection issues. Please <a href="mailto:imbennyguo@gmail.com">contact bennyguo</a> if you would like to help out.</li>
</ul>
<h3 tabindex="-1" dir="auto">Magic3D <a href="https://arxiv.org/abs/2211.10440" rel="nofollow"><img src="https://camo.githubusercontent.com/92872accb7b8db7a3702adf6bebcf7b02c66650ec96f1d3aabf1843d39b2d171/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323231312e31303434302d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2211.10440-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (DeepFloyd IF, batch size 8; first row: coarse, second row: refine)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description magic3d-if.mp4">magic3d-if.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694858-0ed6939e-cd7a-408f-a94b-406709ae90c0.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NTgtMGVkNjkzOWUtY2Q3YS00MDhmLWE5NGItNDA2NzA5YWU5MGMwLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA5ZTAyMGU3MmYyZDNiNTY5ZTA5M2RiNTUwMDk4ODEzNTZlZjNhMDA4Y2U4MzJhZTU3MDQ4YzhlMjFhZWZmZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VCyXtsj2LL4k6vgdUHSi8ltI1yIeKuq-3lXquWeW7VE" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694858-0ed6939e-cd7a-408f-a94b-406709ae90c0.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NTgtMGVkNjkzOWUtY2Q3YS00MDhmLWE5NGItNDA2NzA5YWU5MGMwLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA5ZTAyMGU3MmYyZDNiNTY5ZTA5M2RiNTUwMDk4ODEzNTZlZjNhMDA4Y2U4MzJhZTU3MDQ4YzhlMjFhZWZmZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VCyXtsj2LL4k6vgdUHSi8ltI1yIeKuq-3lXquWeW7VE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>We use open-source T2I models (StableDiffusion, DeepFloyd IF) for the coarse stage, while the paper uses eDiff-I.</li>
<li>In the coarse stage, we use a guidance scale of 20 for DeepFloyd IF, while the paper uses 100 for eDiff-I.</li>
<li>In the coarse stage, we use analytic normal, while the paper uses predicted normal.</li>
<li>In the coarse stage, we use orientation loss as in DreamFusion, while the paper does not.</li>
<li>There are many things that are omitted from the paper such as the weighting of loss terms and the DMTet grid resolution, which could be different.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<p dir="auto">First train the coarse stage NeRF:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training
python launch.py --config configs/magic3d-coarse-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;
# uses StableDiffusion, requires ~6GB VRAM in training
python launch.py --config configs/magic3d-coarse-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;"><pre><span><span>#</span> uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training</span>
python launch.py --config configs/magic3d-coarse-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span>
<span><span>#</span> uses StableDiffusion, requires ~6GB VRAM in training</span>
python launch.py --config configs/magic3d-coarse-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span></pre></div>
<p dir="auto">Then convert the NeRF from the coarse stage to DMTet and train with differentiable rasterization:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# the refinement stage uses StableDiffusion, and requires ~5GB VRAM in training
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
# if you're unsatisfied with the surface extracted using the default threshold (25)
# you can specify a threshold value using `system.geometry_convert_override`
# decrease the value if the extracted surface is incomplete, increase if it is extruded
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10."><pre><span><span>#</span> the refinement stage uses StableDiffusion, and requires ~5GB VRAM in training</span>
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
<span><span>#</span> if you're unsatisfied with the surface extracted using the default threshold (25)</span>
<span><span>#</span> you can specify a threshold value using `system.geometry_convert_override`</span>
<span><span>#</span> decrease the value if the extracted surface is incomplete, increase if it is extruded</span>
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10.</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>For the coarse stage, DeepFloyd IF performs <strong>way better than</strong> StableDiffusion.</li>
<li>Magic3D uses a neural network to predict the surface normal, which may not resemble the true geometric normal and degrade geometry quality, so we use analytic normal instead.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_sparsity</code> if your scene is stuffed with floaters/becoming empty.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_orient</code> if you object is foggy/over-smoothed.</li>
<li>Try replacing the background with random colors with a probability 0.5 by setting <code>system.background.random_aug=true</code> if you find the model incorrectly treats the background as part of the object.</li>
</ul>
<h3 tabindex="-1" dir="auto">Score Jacobian Chaining <a href="https://arxiv.org/abs/2212.00774" rel="nofollow"><img src="https://camo.githubusercontent.com/61e153afb36cca33506ce0f5fd1560c6fe94155476c855965212182a1808e6f5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323231322e30303737342d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2212.00774-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description sjc.mp4">sjc.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694871-87a247c1-2d3d-4cbf-89df-450bfeac3aca.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzEtODdhMjQ3YzEtMmQzZC00Y2JmLTg5ZGYtNDUwYmZlYWMzYWNhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjNzlkOWExZWI5MGRjYTY2NDYzODhiMDA5ZjVmMDM5MDQxZDMwMTc1ODRjZGVlYWVjOWY2YzVkYzY1ZGNhODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.JK95IOQiZYkdb49L26wWkn-UarGBmxQDgMB2Dbh7ops" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694871-87a247c1-2d3d-4cbf-89df-450bfeac3aca.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzEtODdhMjQ3YzEtMmQzZC00Y2JmLTg5ZGYtNDUwYmZlYWMzYWNhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjNzlkOWExZWI5MGRjYTY2NDYzODhiMDA5ZjVmMDM5MDQxZDMwMTc1ODRjZGVlYWVjOWY2YzVkYzY1ZGNhODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.JK95IOQiZYkdb49L26wWkn-UarGBmxQDgMB2Dbh7ops" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Notable differences from the paper: N/A.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# train with sjc guidance in latent space
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;A high quality photo of a delicious burger&quot;
# train with sjc guidance in latent space, trump figure
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;Trump figure&quot; trainer.max_steps=30000 system.loss.lambda_emptiness=&quot;[15000,10000.0,200000.0,15001]&quot; system.optimizer.params.background.lr=0.05 seed=42"><pre><span><span>#</span> train with sjc guidance in latent space</span>
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>A high quality photo of a delicious burger<span>"</span></span>
<span><span>#</span> train with sjc guidance in latent space, trump figure</span>
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>Trump figure<span>"</span></span> trainer.max_steps=30000 system.loss.lambda_emptiness=<span><span>"</span>[15000,10000.0,200000.0,15001]<span>"</span></span> system.optimizer.params.background.lr=0.05 seed=42</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>SJC uses subpixel rendering which decodes a <code>128x128</code> latent feature map for better visualization quality. You can turn off this feature by <code>system.subpixel_rendering=false</code> to save VRAM in validation/testing.</li>
</ul>
<h3 tabindex="-1" dir="auto">Latent-NeRF <a href="https://arxiv.org/abs/2211.07600" rel="nofollow"><img src="https://camo.githubusercontent.com/2cc556c25ad825d7078271e0eb352715d1a528320b10d315bc1f1866c129a96c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323231312e30373630302d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2211.07600-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description latent-nerf.mp4">latent-nerf.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694876-5a270347-6a41-4429-8909-44c90c554e06.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzYtNWEyNzAzNDctNmE0MS00NDI5LTg5MDktNDRjOTBjNTU0ZTA2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTkyYWVjN2VlZjY1NzMwOTlhMTQ2NWE5MDc3Nzc1MGRhNmFkMThlZDFlZjMxOTdjMTNhMGJmZGU5MjkzMzZkMjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.yZzVykUB51Io13qb0I6VjWOYM91vw1uk0Kzt917FVqg" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694876-5a270347-6a41-4429-8909-44c90c554e06.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzYtNWEyNzAzNDctNmE0MS00NDI5LTg5MDktNDRjOTBjNTU0ZTA2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTkyYWVjN2VlZjY1NzMwOTlhMTQ2NWE5MDc3Nzc1MGRhNmFkMThlZDFlZjMxOTdjMTNhMGJmZGU5MjkzMzZkMjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.yZzVykUB51Io13qb0I6VjWOYM91vw1uk0Kzt917FVqg" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Notable differences from the paper: N/A.</p>
<p dir="auto">We currently only implement Latent-NeRF for text-guided and Sketch-Shape for (text,shape)-guided 3D generation. Latent-Paint is not implemented yet.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# train Latent-NeRF in Stable Diffusion latent space
python launch.py --config configs/latentnerf.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;
# refine Latent-NeRF in RGB space
python launch.py --config configs/latentnerf-refine.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt

# train Sketch-Shape in Stable Diffusion latent space
python launch.py --config configs/sketchshape.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=&quot;a teddy bear in a tuxedo&quot;
# refine Sketch-Shape in RGB space
python launch.py --config configs/sketchshape-refine.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=&quot;a teddy bear in a tuxedo&quot; system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> train Latent-NeRF in Stable Diffusion latent space</span>
python launch.py --config configs/latentnerf.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span>
<span><span>#</span> refine Latent-NeRF in RGB space</span>
python launch.py --config configs/latentnerf-refine.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt

<span><span>#</span> train Sketch-Shape in Stable Diffusion latent space</span>
python launch.py --config configs/sketchshape.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=<span><span>"</span>a teddy bear in a tuxedo<span>"</span></span>
<span><span>#</span> refine Sketch-Shape in RGB space</span>
python launch.py --config configs/sketchshape-refine.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=<span><span>"</span>a teddy bear in a tuxedo<span>"</span></span> system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt</pre></div>
<h3 tabindex="-1" dir="auto">Fantasia3D <a href="https://arxiv.org/abs/2303.13873" rel="nofollow"><img src="https://camo.githubusercontent.com/98f846baf085f2ee0b2b0e21e7522d380f3b53e7b71174e2ba574f1b00bc1858/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330332e31333837332d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2303.13873-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description fantasia-3d.mp4">fantasia-3d.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694880-33b0db21-4530-47f1-9c3b-c70357bc84b3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4ODAtMzNiMGRiMjEtNDUzMC00N2YxLTljM2ItYzcwMzU3YmM4NGIzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk1MWNkODMxZjM2NGM4ODEzZmRlMTJhYzZjNDA2YjU3NTkzNDM0ZDhhNDA1NmE2OGMzM2RiN2Y1MzU5OGQzMWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.RXvCegjzK4rbcg-xAUfin2BznxNnh9T1xJ1DMLN_okE" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694880-33b0db21-4530-47f1-9c3b-c70357bc84b3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4ODAtMzNiMGRiMjEtNDUzMC00N2YxLTljM2ItYzcwMzU3YmM4NGIzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk1MWNkODMxZjM2NGM4ODEzZmRlMTJhYzZjNDA2YjU3NTkzNDM0ZDhhNDA1NmE2OGMzM2RiN2Y1MzU5OGQzMWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.RXvCegjzK4rbcg-xAUfin2BznxNnh9T1xJ1DMLN_okE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion, mesh initialization)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description fantasia3d-mesh.mp4">fantasia3d-mesh.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/242636697-762903c1-665b-47b5-a2c2-bd7021a9e548.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzY2OTctNzYyOTAzYzEtNjY1Yi00N2I1LWEyYzItYmQ3MDIxYTllNTQ4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkZjA0NTYwNjk4ZDA3MDY3OGJjZmE3OTU2OTE2M2YyMGYwZGM4YWE1OWFlMjI4OTljZjdkZmY1OTEyMWQyZDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.G78odD1DOTH_iZiw492CGCEmIDvF-KSG1lvUVSKLSdU" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/242636697-762903c1-665b-47b5-a2c2-bd7021a9e548.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzY2OTctNzYyOTAzYzEtNjY1Yi00N2I1LWEyYzItYmQ3MDIxYTllNTQ4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkZjA0NTYwNjk4ZDA3MDY3OGJjZmE3OTU2OTE2M2YyMGYwZGM4YWE1OWFlMjI4OTljZjdkZmY1OTEyMWQyZDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.G78odD1DOTH_iZiw492CGCEmIDvF-KSG1lvUVSKLSdU" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/242637322-2d22e30f-4a32-454a-a06e-d6e6bd2a1b96.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzczMjItMmQyMmUzMGYtNGEzMi00NTRhLWEwNmUtZDZlNmJkMmExYjk2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjMDBkOWZmYTM4MjZjMWY4YTcwODdkNTVkN2RkYjEwMDg4ZmRjNmFlMmE3NDU4ZmExMDMzODE0M2YzNWQyOGEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.mIXWeTV9bsJ7NdDyI4ZhZ4S3xK1aVy8PX89j-oGne9w"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/242637322-2d22e30f-4a32-454a-a06e-d6e6bd2a1b96.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzczMjItMmQyMmUzMGYtNGEzMi00NTRhLWEwNmUtZDZlNmJkMmExYjk2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjMDBkOWZmYTM4MjZjMWY4YTcwODdkNTVkN2RkYjEwMDg4ZmRjNmFlMmE3NDU4ZmExMDMzODE0M2YzNWQyOGEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.mIXWeTV9bsJ7NdDyI4ZhZ4S3xK1aVy8PX89j-oGne9w" width="100%"></a>
</p>
<p dir="auto">Notable differences from the paper:</p>
<ul dir="auto">
<li>We enable tangent-space normal perturbation by default, which can be turned off by appending <code>system.material.use_bump=false</code>.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- Geometry --------- #
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a DSLR photo of an ice cream sundae&quot;
# Fantasia3D highly relies on the initialized SDF shape
# the default shape is a sphere with radius 0.5
# change the shape initialization to match your input prompt
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;The leaning tower of Pisa&quot; system.geometry.shape_init=ellipsoid system.geometry.shape_init_params=&quot;[0.3,0.3,0.8]&quot;
# or you can initialize from a mesh
# here shape_init_params is the scale of the shape
# also make sure to input the correct up and front axis (in +x, +y, +z, -x, -y, -z)
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;hulk&quot; system.geometry.shape_init=mesh:load/shapes/human.obj system.geometry.shape_init_params=0.9 system.geometry.shape_init_mesh_up=+y system.geometry.shape_init_mesh_front=+z
# --------- Texture --------- #
# to train PBR texture continued from a geometry checkpoint:
python launch.py --config configs/fantasia3d-texture.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a DSLR photo of an ice cream sundae&quot; system.geometry_convert_from=path/to/geometry/stage/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> --------- Geometry --------- #</span>
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a DSLR photo of an ice cream sundae<span>"</span></span>
<span><span>#</span> Fantasia3D highly relies on the initialized SDF shape</span>
<span><span>#</span> the default shape is a sphere with radius 0.5</span>
<span><span>#</span> change the shape initialization to match your input prompt</span>
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>The leaning tower of Pisa<span>"</span></span> system.geometry.shape_init=ellipsoid system.geometry.shape_init_params=<span><span>"</span>[0.3,0.3,0.8]<span>"</span></span>
<span><span>#</span> or you can initialize from a mesh</span>
<span><span>#</span> here shape_init_params is the scale of the shape</span>
<span><span>#</span> also make sure to input the correct up and front axis (in +x, +y, +z, -x, -y, -z)</span>
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>hulk<span>"</span></span> system.geometry.shape_init=mesh:load/shapes/human.obj system.geometry.shape_init_params=0.9 system.geometry.shape_init_mesh_up=+y system.geometry.shape_init_mesh_front=+z
<span><span>#</span> --------- Texture --------- #</span>
<span><span>#</span> to train PBR texture continued from a geometry checkpoint:</span>
python launch.py --config configs/fantasia3d-texture.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a DSLR photo of an ice cream sundae<span>"</span></span> system.geometry_convert_from=path/to/geometry/stage/trial/dir/ckpts/last.ckpt</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>If you find the shape easily diverge in early training stages, you may use a lower guidance scale by setting <code>system.guidance.guidance_scale=30.</code>.</li>
</ul>
<h3 tabindex="-1" dir="auto">TextMesh <a href="https://arxiv.org/abs/2304.12439" rel="nofollow"><img src="https://camo.githubusercontent.com/1ef5ad9a7578c2b914416b439572e33f7443a2746362edf2a0b232e59a085351/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330342e31323433392d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2304.12439-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (DeepFloyd IF, batch size 4)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description textmesh-if.mp4">textmesh-if.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/245696160-72217cdd-765a-475b-92d0-4ab62bf0f57a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTYxNjAtNzIyMTdjZGQtNzY1YS00NzViLTkyZDAtNGFiNjJiZjBmNTdhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE0ZjIxZTc0MDQxNGM3ODBlNzk0MWViNDdkM2EwYTdhNDQwZjdiZDYwMTU5ZTQyOTk3MTk3ZmMyZjliMTc4YWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.IxiDrL4A-nvPYk0JfPFjyb7mGPR0uuQR2SsQW5jSIo0" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/245696160-72217cdd-765a-475b-92d0-4ab62bf0f57a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTYxNjAtNzIyMTdjZGQtNzY1YS00NzViLTkyZDAtNGFiNjJiZjBmNTdhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE0ZjIxZTc0MDQxNGM3ODBlNzk0MWViNDdkM2EwYTdhNDQwZjdiZDYwMTU5ZTQyOTk3MTk3ZmMyZjliMTc4YWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.IxiDrL4A-nvPYk0JfPFjyb7mGPR0uuQR2SsQW5jSIo0" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>Most of the settings are the same as the DreamFusion model. Please refer to the notable differences of the DreamFusion model.</li>
<li>We use NeuS as the geometry representation while the original paper uses VolSDF.</li>
<li>We adopt techniques from <a href="https://arxiv.org/abs/2306.03092" rel="nofollow">Neuralangelo</a> to stablize normal computation when using hash grids.</li>
<li>We currently only implemented the coarse stage of TextMesh.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# uses DeepFloyd IF, requires ~15GB VRAM
python launch.py --config configs/textmesh-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;lib:cowboy_boots&quot;"><pre><span><span>#</span> uses DeepFloyd IF, requires ~15GB VRAM</span>
python launch.py --config configs/textmesh-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>lib:cowboy_boots<span>"</span></span></pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>TextMesh uses a surface-based geometry representation, so you don't need to manually tune the isosurface threshold when exporting meshes!</li>
</ul>
<h3 tabindex="-1" dir="auto">Control4D <a href="https://arxiv.org/abs/2305.20082" rel="nofollow"><img src="https://camo.githubusercontent.com/fd4b0abaf42e9ee37229eb60b5910b3e4148c1c96d4bb7ff7ac204e6426221d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330352e32303038322d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2305.20082-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>This is an experimental implementation of Control4D using threestudio! Control4D will release the full code including static and dynamic editing after paper acceptance.</strong></p>
<p dir="auto"><strong>Results obtained by threestudio (512x512)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description origin_1.mp4">origin_1.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/24589363/247185185-97d9aadd-32c7-488f-9543-6951b285d588.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODUxODUtOTdkOWFhZGQtMzJjNy00ODhmLTk1NDMtNjk1MWIyODVkNTg4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgyZWQ5MTM4ODVlMGNjNTcxMmJkNDkxYjcxYTYzNDVmNjZkZWU0OTZkZjMyMjA4MDA0YzIzNmZhMTY5MTE3ODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.x-8BtwUBdXv9YKKpy3Pf0KxOPYTX3DSTWgFOeETn16o" data-canonical-src="https://private-user-images.githubusercontent.com/24589363/247185185-97d9aadd-32c7-488f-9543-6951b285d588.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODUxODUtOTdkOWFhZGQtMzJjNy00ODhmLTk1NDMtNjk1MWIyODVkNTg4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgyZWQ5MTM4ODVlMGNjNTcxMmJkNDkxYjcxYTYzNDVmNjZkZWU0OTZkZjMyMjA4MDA0YzIzNmZhMTY5MTE3ODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.x-8BtwUBdXv9YKKpy3Pf0KxOPYTX3DSTWgFOeETn16o" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">We currently don't support dynamic editing.</p>
<p dir="auto">Download the data sample of control4D using this <a href="https://mailstsinghuaeducn-my.sharepoint.com/:u:/g/personal/shaorz20_mails_tsinghua_edu_cn/EcqOaEuNwH1KpR0JTzL4Ur0BO_iJr8RiY2rNAGVC7h3fng?e=Dyr2gu" rel="nofollow">link</a>.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- Control4D --------- #
# static editing with 128x128 NeRF + 512x512 GAN rendering, ~20GB VRAM
python launch.py --config configs/control4d-static.yaml --train --gpu 0 data.dataroot=&quot;YOUR_DATAROOT/twindom&quot; system.prompt_processor.prompt=&quot;Elon Musk wearing red shirt, RAW photo, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3&quot;"><pre><span><span>#</span> --------- Control4D --------- #</span>
<span><span>#</span> static editing with 128x128 NeRF + 512x512 GAN rendering, ~20GB VRAM</span>
python launch.py --config configs/control4d-static.yaml --train --gpu 0 data.dataroot=<span><span>"</span>YOUR_DATAROOT/twindom<span>"</span></span> system.prompt_processor.prompt=<span><span>"</span>Elon Musk wearing red shirt, RAW photo, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3<span>"</span></span></pre></div>
<h3 tabindex="-1" dir="auto">InstructNeRF2NeRF <a href="https://arxiv.org/abs/2303.12789" rel="nofollow"><img src="https://camo.githubusercontent.com/9187e8930897819d323fd5d972e6473d03185795beeec7cc9091f417dc0cc8d2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330332e31323738392d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2303.12789-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description in2n.mp4">in2n.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/24589363/247187180-7aa43a2d-87d7-4ef5-94b6-f778ddb041b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODcxODAtN2FhNDNhMmQtODdkNy00ZWY1LTk0YjYtZjc3OGRkYjA0MWI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEzM2YzZTgwNDY1YjY0MGU0NGJlYzkzMDJiZTQ3MDBkYTE2N2IyYzc5ZTdjNGVmMzhiODRjNDNiYzdjZjQ3ZDUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lwgDP0NkmZkjvvfzEJoM54FEZ3NgSHeWoLBNaC3Z0RQ" data-canonical-src="https://private-user-images.githubusercontent.com/24589363/247187180-7aa43a2d-87d7-4ef5-94b6-f778ddb041b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODcxODAtN2FhNDNhMmQtODdkNy00ZWY1LTk0YjYtZjc3OGRkYjA0MWI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEzM2YzZTgwNDY1YjY0MGU0NGJlYzkzMDJiZTQ3MDBkYTE2N2IyYzc5ZTdjNGVmMzhiODRjNDNiYzdjZjQ3ZDUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lwgDP0NkmZkjvvfzEJoM54FEZ3NgSHeWoLBNaC3Z0RQ" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Download the data sample of InstructNeRF2NeRF using this <a href="https://mailstsinghuaeducn-my.sharepoint.com/:u:/g/personal/shaorz20_mails_tsinghua_edu_cn/EbNazeNAYsBIvxGeXuCmOXgBiLv8KM-hfRNbNS7DtTvSvA?e=C1k4bM" rel="nofollow">link</a>.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- InstructNeRF2NeRF --------- #
# 3D editing with NeRF patch-based rendering, ~20GB VRAM
python launch.py --config configs/instructnerf2nerf.yaml --train --gpu 0 data.dataroot=&quot;YOUR_DATAROOT/face&quot; data.camera_layout=&quot;front&quot; data.camera_distance=1 data.eval_interpolation=[1,3,50] system.prompt_processor.prompt=&quot;Turn him into Albert Einstein&quot;"><pre><span><span>#</span> --------- InstructNeRF2NeRF --------- #</span>
<span><span>#</span> 3D editing with NeRF patch-based rendering, ~20GB VRAM</span>
python launch.py --config configs/instructnerf2nerf.yaml --train --gpu 0 data.dataroot=<span><span>"</span>YOUR_DATAROOT/face<span>"</span></span> data.camera_layout=<span><span>"</span>front<span>"</span></span> data.camera_distance=1 data.eval_interpolation=[1,3,50] system.prompt_processor.prompt=<span><span>"</span>Turn him into Albert Einstein<span>"</span></span></pre></div>
<h3 tabindex="-1" dir="auto">Magic123 <a href="https://arxiv.org/abs/2306.17843" rel="nofollow"><img src="https://camo.githubusercontent.com/fc721c573072ceed6e5ffadac512640054425b25449d462163de96d1e99800b8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330362e31373834332d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2306.17843-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Zero123 + Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description magic123.mp4">magic123.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/263146263-335a58a8-8fee-485b-ac27-c55a16f4a673.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMxNDYyNjMtMzM1YTU4YTgtOGZlZS00ODViLWFjMjctYzU1YTE2ZjRhNjczLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWJiNTM0NTczZWE1ZjFiNmExOTA2YWNkNjE3ZThjNTI4M2JmMmFiZjE5OTNkMDlmYWI4MjVkNjVhZDI4MTc5NjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nztBsQhofQ6SkQg_7wiJjoW5bxuAZGm9qEACjFU8d14" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/263146263-335a58a8-8fee-485b-ac27-c55a16f4a673.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMxNDYyNjMtMzM1YTU4YTgtOGZlZS00ODViLWFjMjctYzU1YTE2ZjRhNjczLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWJiNTM0NTczZWE1ZjFiNmExOTA2YWNkNjE3ZThjNTI4M2JmMmFiZjE5OTNkMDlmYWI4MjVkNjVhZDI4MTc5NjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nztBsQhofQ6SkQg_7wiJjoW5bxuAZGm9qEACjFU8d14" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>This is an unofficial re-implementation which shares the same overall idea with the <a href="https://github.com/guochengqian/Magic123">official implementation</a> but differs in some aspects like hyperparameters.</li>
<li>Textual Inversion is not supported, which means a text prompt is needed for training.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<p dir="auto">First train the coarse stage NeRF:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Zero123 + Stable Diffusion, ~12GB VRAM
# data.image_path must point to a 4-channel RGBA image
# system.prompt_proessor.prompt must be specified
python launch.py --config configs/magic123-coarse-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=&quot;a delicious hamburger&quot;"><pre><span><span>#</span> Zero123 + Stable Diffusion, ~12GB VRAM</span>
<span><span>#</span> data.image_path must point to a 4-channel RGBA image</span>
<span><span>#</span> system.prompt_proessor.prompt must be specified</span>
python launch.py --config configs/magic123-coarse-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span></pre></div>
<p dir="auto">Then convert the NeRF from the coarse stage to DMTet and train with differentiable rasterization:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Zero123 + Stable Diffusion, ~10GB VRAM
# data.image_path must point to a 4-channel RGBA image
# system.prompt_proessor.prompt must be specified
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
# if you're unsatisfied with the surface extracted using the default threshold (25)
# you can specify a threshold value using `system.geometry_convert_override`
# decrease the value if the extracted surface is incomplete, increase if it is extruded
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10."><pre><span><span>#</span> Zero123 + Stable Diffusion, ~10GB VRAM</span>
<span><span>#</span> data.image_path must point to a 4-channel RGBA image</span>
<span><span>#</span> system.prompt_proessor.prompt must be specified</span>
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
<span><span>#</span> if you're unsatisfied with the surface extracted using the default threshold (25)</span>
<span><span>#</span> you can specify a threshold value using `system.geometry_convert_override`</span>
<span><span>#</span> decrease the value if the extracted surface is incomplete, increase if it is extruded</span>
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10.</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>If the image contains non-front-facing objects, specifying the approximate elevation and azimuth angle by setting <code>data.default_elevation_deg</code> and <code>data.default_azimuth_deg</code> can be helpful. In threestudio, top is elevation +90 and bottom is elevation -90; left is azimuth -90 and right is azimuth +90.</li>
</ul>
<h3 tabindex="-1" dir="auto">Stable Zero123</h3>
<p dir="auto"><strong>Installation</strong></p>
<p dir="auto">Download pretrained Stable Zero123 checkpoint <code>stable-zero123.ckpt</code> into <code>load/zero123</code> from <a href="https://huggingface.co/stabilityai/stable-zero123" rel="nofollow">https://huggingface.co/stabilityai/stable-zero123</a></p>
<p dir="auto"><strong>Results obtained by threestudio (Stable Zero123 vs Zero123-XL)</strong>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/22424247/289674233-bf2d2213-5027-489c-a6ba-1c56c14ee8b7.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yODk2NzQyMzMtYmYyZDIyMTMtNTAyNy00ODljLWE2YmEtMWM1NmMxNGVlOGI3LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwOTY5NDY4ZTM2OTk4Mzg0YmE2YTFkMmUzNWQ0ODY4M2FlYmM0MDEwZjk2ZjJjYjQ1MzMyYjkzODI2NTg3ODQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.v6OV91vhNJ8WxaWudRqGQ8M0JDSnTzK09dXnDhhnlVg"><img src="https://private-user-images.githubusercontent.com/22424247/289674233-bf2d2213-5027-489c-a6ba-1c56c14ee8b7.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yODk2NzQyMzMtYmYyZDIyMTMtNTAyNy00ODljLWE2YmEtMWM1NmMxNGVlOGI3LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwOTY5NDY4ZTM2OTk4Mzg0YmE2YTFkMmUzNWQ0ODY4M2FlYmM0MDEwZjk2ZjJjYjQ1MzMyYjkzODI2NTg3ODQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.v6OV91vhNJ8WxaWudRqGQ8M0JDSnTzK09dXnDhhnlVg" alt="Final_video_v01" data-animated-image=""></a></p>
<p dir="auto"><strong>Example running commands</strong></p>
<ol dir="auto">
<li>Take an image of your choice, or generate it from text using your favourite AI image generator such as SDXL Turbo (<a href="https://clipdrop.co/stable-diffusion-turbo" rel="nofollow">https://clipdrop.co/stable-diffusion-turbo</a>) E.g. "A simple 3D render of a friendly dog"</li>
<li>Remove its background using Clipdrop (<a href="https://clipdrop.co/remove-background" rel="nofollow">https://clipdrop.co/remove-background</a>)</li>
<li>Save to <code>load/images/</code>, preferably with <code>_rgba.png</code> as the suffix</li>
<li>Run Zero-1-to-3 with the Stable Zero123 ckpt:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python launch.py --config configs/stable-zero123.yaml --train --gpu 0 data.image_path=./load/images/hamburger_rgba.png"><pre>python launch.py --config configs/stable-zero123.yaml --train --gpu 0 data.image_path=./load/images/hamburger_rgba.png</pre></div>
<p dir="auto"><strong>IMPORTANT NOTE: This is an experimental implementation and we're constantly improving the quality.</strong></p>
<p dir="auto"><strong>IMPORTANT NOTE: This implementation extends the Zero-1-to-3 implementation below, and is heavily inspired from the Zero-1-to-3 implementation in <a href="https://github.com/threestudio-project/threestudio/blob/main/stable-dreamfusion">https://github.com/ashawkey/stable-dreamfusion</a>! <code>extern/ldm_zero123</code> is borrowed from <code>stable-dreamfusion/ldm</code>.</strong></p>
<h3 tabindex="-1" dir="auto">Zero-1-to-3 <a href="https://arxiv.org/abs/2303.11328" rel="nofollow"><img src="https://camo.githubusercontent.com/cff3bb989636a7adcd8dfd9b30f2be23d690ed419b7e6b0a5fd65f147731cbc1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330332e31313332382d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2303.11328-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Installation</strong></p>
<p dir="auto">Download pretrained Zero123XL weights into <code>load/zero123</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd load/zero123
wget https://zero123.cs.columbia.edu/assets/zero123-xl.ckpt"><pre><span>cd</span> load/zero123
wget https://zero123.cs.columbia.edu/assets/zero123-xl.ckpt</pre></div>
<p dir="auto"><strong>Results obtained by threestudio (Zero-1-to-3)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description ezgif-3-355a192487.mp4">ezgif-3-355a192487.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/22424247/252868915-f4e7b66f-7a46-4f9f-8fcd-750300cef651.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNTI4Njg5MTUtZjRlN2I2NmYtN2E0Ni00ZjlmLThmY2QtNzUwMzAwY2VmNjUxLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk5YmI2MGRhZGY0Zjc5MWMzZDUwNTNhMjgyNDk0ODhkZWQ5NDE1ZjJmYzhkOThhN2YxNDEyNTJmMDg0NzY0NGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.NZxBwlaLEPYsqbRhn4QJwiV6iF5_0AnZaSGKlg-cF2c" data-canonical-src="https://private-user-images.githubusercontent.com/22424247/252868915-f4e7b66f-7a46-4f9f-8fcd-750300cef651.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNTI4Njg5MTUtZjRlN2I2NmYtN2E0Ni00ZjlmLThmY2QtNzUwMzAwY2VmNjUxLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk5YmI2MGRhZGY0Zjc5MWMzZDUwNTNhMjgyNDk0ODhkZWQ5NDE1ZjJmYzhkOThhN2YxNDEyNTJmMDg0NzY0NGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.NZxBwlaLEPYsqbRhn4QJwiV6iF5_0AnZaSGKlg-cF2c" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>IMPORTANT NOTE: This is an experimental implementation and we're constantly improving the quality.</strong></p>
<p dir="auto"><strong>IMPORTANT NOTE: This implementation is heavily inspired from the Zero-1-to-3 implementation in <a href="https://github.com/threestudio-project/threestudio/blob/main/stable-dreamfusion">https://github.com/ashawkey/stable-dreamfusion</a>! <code>extern/ldm_zero123</code> is borrowed from <code>stable-dreamfusion/ldm</code>.</strong></p>
<p dir="auto"><strong>Example running commands</strong></p>
<ol dir="auto">
<li>Take an image of your choice, or generate it from text using your favourite AI image generator such as Stable Diffusion XL (<a href="https://clipdrop.co/stable-diffusion" rel="nofollow">https://clipdrop.co/stable-diffusion</a>) E.g. "A simple 3D render of a friendly dog"</li>
<li>Remove its background using Clipdrop (<a href="https://clipdrop.co/remove-background" rel="nofollow">https://clipdrop.co/remove-background</a>)</li>
<li>Save to <code>load/images/</code>, preferably with <code>_rgba.png</code> as the suffix</li>
<li>Run Zero-1-to-3:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python launch.py --config configs/zero123.yaml --train --gpu 0 data.image_path=./load/images/dog1_rgba.png"><pre>python launch.py --config configs/zero123.yaml --train --gpu 0 data.image_path=./load/images/dog1_rgba.png</pre></div>
<p dir="auto">For more scripts for Zero-1-to-3, please check <code>threestudio/scripts/run_zero123.sh</code>.</p>
<p dir="auto">Previous Zero-1-to-3 weights are available at <code>https://huggingface.co/cvlab/zero123-weights/</code>. You can download them to <code>load/zero123</code> as above, and replace the path at <code>system.guidance.pretrained_model_name_or_path</code>.</p>
<p dir="auto"><strong>Guidance evaluation</strong></p>
<p dir="auto">Also includes evaluation of the guidance during training. If <code>system.freq.guidance_eval</code> is set to a value &gt; 0, this will save rendered image, noisy image (noise added mentioned at top left), 1-step-denoised image, 1-step prediction of original image, fully denoised image. For example:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/22424247/242720649-c8e7d835-4937-4852-bfb0-3e906e6b66b7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNDI3MjA2NDktYzhlN2Q4MzUtNDkzNy00ODUyLWJmYjAtM2U5MDZlNmI2NmI3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIzMzIxNzM1ZmQwODgzODJkYTgxMmVjNDE1OTUzODU5OWY2MTFmNWMwZmQzZjEwOWI5NmFmMzJhODdhMmY3OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xBAJWPzi1QbuZgb3ujJScfy8tyKgNhkQMXyRw-vp4LU"><img src="https://private-user-images.githubusercontent.com/22424247/242720649-c8e7d835-4937-4852-bfb0-3e906e6b66b7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNDI3MjA2NDktYzhlN2Q4MzUtNDkzNy00ODUyLWJmYjAtM2U5MDZlNmI2NmI3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIzMzIxNzM1ZmQwODgzODJkYTgxMmVjNDE1OTUzODU5OWY2MTFmNWMwZmQzZjEwOWI5NmFmMzJhODdhMmY3OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xBAJWPzi1QbuZgb3ujJScfy8tyKgNhkQMXyRw-vp4LU" alt="it143-train"></a></p>
<h3 tabindex="-1" dir="auto">More to come, please stay tuned.</h3>

<p dir="auto"><strong>If you would like to contribute a new method to threestudio, see <a href="https://github.com/threestudio-project/threestudio#contributing-to-threestudio">here</a>.</strong></p>
<h2 tabindex="-1" dir="auto">Prompt Library</h2>
<p dir="auto">For easier comparison, we collect the 397 preset prompts from the website of <a href="https://dreamfusion3d.github.io/gallery.html" rel="nofollow">DreamFusion</a> in <a href="https://github.com/threestudio-project/threestudio/blob/main/load/prompt_library.json">this file</a>. You can use these prompts by setting <code>system.prompt_processor.prompt=lib:keyword1_keyword2_..._keywordN</code>. Note that the prompt should starts with <code>lib:</code> and all the keywords are separated by <code>_</code>. The prompt processor will match the keywords to all the prompts in the library, and will only succeed if there's <strong>exactly one match</strong>. The used prompt will be printed to the console. Also note that you can't use this syntax to point to every prompt in the library, as there are prompts that are subset of other prompts lmao. We will enhance the use of this feature.</p>
<h2 tabindex="-1" dir="auto">Tips on Improving Quality</h2>
<p dir="auto">It's important to note that existing techniques that lift 2D T2I models to 3D cannot consistently produce satisfying results. Results from great papers like DreamFusion and Magic3D are (to some extent) cherry-pickled, so don't be frustrated if you do not get what you expected on your first trial. Here are some tips that may help you improve the generation quality:</p>
<ul dir="auto">
<li><strong>Increase batch size</strong>. Large batch sizes help convergence and improve the 3D consistency of the geometry. State-of-the-art methods claim using large batch sizes: DreamFusion uses a batch size of 4; Magic3D uses a batch size of 32; Fantasia3D uses a batch size of 24; some results shown above use a batch size of 8. You can easily change the batch size by setting <code>data.batch_size=N</code>. Increasing the batch size requires more VRAM. If you have limited VRAM but still want the benefit of large batch sizes, you may use <a href="https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#accumulate-gradients" rel="nofollow">gradient accumulation provided by PyTorch Lightning</a> by setting <code>trainer.accumulate_grad_batches=N</code>. This will accumulate the gradient of several batches and achieve a large effective batch size. Note that if you use gradient accumulation, you may need to multiply all step values by N times in your config, such as values that have the name <code>X_steps</code> and <code>trainer.val_check_interval</code>, since now N batches equal to a large batch.</li>
<li><strong>Train longer.</strong> This helps if you can already obtain reasonable results and would like to enhance the details. If the result is still a mess after several thousand steps, training for a longer time often won't help. You can set the total training iterations by <code>trainer.max_steps=N</code>.</li>
<li><strong>Try different seeds.</strong> This is a simple solution if your results have correct overall geometry but suffer from the multi-face Janus problem. You can change the seed by setting <code>seed=N</code>. Good luck!</li>
<li><strong>Tuning regularization weights.</strong> Some methods have regularization terms which can be essential to obtaining good geometry. Try tuning the weights of these regularizations by setting <code>system.loss.lambda_X=value</code>. The specific values depend on your situation, you may refer to <a href="https://github.com/threestudio-project/threestudio#supported-models">tips for each supported model</a> for more detailed instructions.</li>
<li><strong>Try debiasing methods.</strong> When conventional SDS techniques like DreamFusion, Magic3D, SJC, and others fail to produce the desired 3D results, Debiased Score Distillation Sampling (D-SDS) can be a solution. D-SDS is devised to tackle challenges such as artifacts or the Janus problem, employing two strategies: score debiasing and prompt debiasing. You can activate score debiasing by just setting <code>system.guidance.grad_clip=[0,0.5,2.0,10000]</code>, where the order is <code>start_step, start_value, end_value, end_step</code>. You can enable prompt debiasing by setting <code>system.prompt_processor.use_prompt_debiasing=true</code>. When using prompt debiasing, it's recommended to set a list of indices for words that should potentially be removed by <code>system.prompt_processor.prompt_debiasing_mask_ids=[i1,i2,...]</code>. For example, if the prompt is <code>a smiling dog</code> and you only want to remove the word <code>smiling</code> for certain views, you should set it to <code>[1]</code>. You could also manually specify the prompt for each view by setting <code>system.prompt_processor.prompt_side</code>, <code>system.prompt_processor.prompt_back</code> and <code>system.prompt_processor.prompt_overhead</code>. For a detailed explanation of these techniques, refer to <a href="https://arxiv.org/abs/2303.15413" rel="nofollow">the D-SDS paper</a> or check out <a href="https://susunghong.github.io/Debiased-Score-Distillation-Sampling/" rel="nofollow">the project page</a>.</li>
<li><strong>Try Perp-Neg.</strong> The <a href="https://perp-neg.github.io/" rel="nofollow">Perp-Neg algorithm</a> can potentially alleviate the multi-face Janus problem. We now support Perp-Neg for <code>stable-diffusion-guidance</code> and <code>deep-floyd-guidance</code> by setting <code>system.prompt_processor.use_perp_neg=true</code>.</li>
</ul>
<h2 tabindex="-1" dir="auto">VRAM Optimization</h2>
<p dir="auto">If you encounter CUDA OOM error, try the following in order (roughly sorted by recommendation) to meet your VRAM requirement.</p>
<ul dir="auto">
<li>If you only encounter OOM at validation/test time, you can set <code>system.cleanup_after_validation_step=true</code> and <code>system.cleanup_after_test_step=true</code> to free memory after each validation/test step. This will slow down validation/testing.</li>
<li>Use a smaller batch size or use gradient accumulation as demonstrated <a href="https://github.com/threestudio-project/threestudio#tips-on-improving-quality">here</a>.</li>
<li>If you are using PyTorch1.x, enable <a href="https://huggingface.co/docs/diffusers/optimization/fp16#memory-efficient-attention" rel="nofollow">memory efficient attention</a> by setting <code>system.guidance.enable_memory_efficient_attention=true</code>. PyTorch2.0 has built-in support for this optimization and is enabled by default.</li>
<li>Enable <a href="https://huggingface.co/docs/diffusers/optimization/fp16#sliced-attention-for-additional-memory-savings" rel="nofollow">attention slicing</a> by setting <code>system.guidance.enable_attention_slicing=true</code>. This will slow down training by ~20%.</li>
<li>If you are using StableDiffusionGuidance, you can use <a href="https://github.com/dbolya/tomesd">Token Merging</a> to <strong>drastically</strong> speed up computation and save memory. You can easily enable Token Merging by setting <code>system.guidance.token_merging=true</code>. You can also customize the Token Merging behavior by setting the parameters <a href="https://github.com/dbolya/tomesd/blob/main/tomesd/patch.py#L183-L213">here</a> to <code>system.guidance.token_merging_params</code>. Note that Token Merging may degrade generation quality.</li>
<li>Enable <a href="https://huggingface.co/docs/diffusers/optimization/fp16#offloading-to-cpu-with-accelerate-for-memory-savings" rel="nofollow">sequential CPU offload</a> by setting <code>system.guidance.enable_sequential_cpu_offload=true</code>. This could save a lot of VRAM but will make the training <strong>extremely slow</strong>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">threestudio use <a href="https://github.com/omry/omegaconf">OmegaConf</a> to manage configurations. You can literally change anything inside the yaml configuration file or by adding command line arguments without <code>--</code>. We list all arguments that you can change in the configuration in our <a href="https://github.com/threestudio-project/threestudio/blob/main/DOCUMENTATION.md">documentation</a>. Happy experimenting!</p>
<h2 tabindex="-1" dir="auto">wandb (Weights &amp; Biases) logging</h2>
<p dir="auto">To enable the (experimental) wandb support, set <code>system.loggers.wandb.enable=true</code>, e.g.:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python launch.py --config configs/zero123.yaml --train --gpu 0 system.loggers.wandb.enable=true`"><pre>python launch.py --config configs/zero123.yaml --train --gpu 0 system.loggers.wandb.enable=true<span><span>`</span></span></pre></div>
<p dir="auto">If you're using a corporate wandb server, you may first need to login to your wandb instance, e.g.:
<code>wandb login --host=https://COMPANY_XYZ.wandb.io --relogin</code></p>
<p dir="auto">By default the runs will have a random name, recorded in the <code>threestudio</code> project. You can override them to give a more descriptive name, e.g.:</p>
<p dir="auto"><code>python launch.py --config configs/zero123.yaml --train --gpu 0 system.loggers.wandb.enable=true system.loggers.wandb.name="zero123xl_accum;bs=4;lr=0.05"</code></p>
<h2 tabindex="-1" dir="auto">Contributing to threestudio</h2>
<ul dir="auto">
<li>Fork the repository and create your branch from <code>main</code>.</li>
<li>Install development dependencies:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements-dev.txt"><pre>pip install -r requirements-dev.txt</pre></div>
<ul dir="auto">
<li>
<p dir="auto">If you are using VSCode as the text editor: (1) Install <code>editorconfig</code> extension. (2) Set the default linter to mypy to enable static type checking. (3) Set the default formatter to black. You could either manually format the document or let the editor format the document each time it is saved by setting <code>"editor.formatOnSave": true</code>.</p>
</li>
<li>
<p dir="auto">Run <code>pre-commit install</code> to install pre-commit hooks which will automatically format the files before commit.</p>
</li>
<li>
<p dir="auto">Make changes to the code, update README and DOCUMENTATION if needed, and open a pull request.</p>
</li>
</ul>
<h3 tabindex="-1" dir="auto">Code Structure</h3>
<p dir="auto">Here we just briefly introduce the code structure of this project. We will make more detailed documentation about this in the future.</p>
<ul dir="auto">
<li>All methods are implemented as a subclass of <code>BaseSystem</code> (in <code>systems/base.py</code>). There typically are six modules inside a system: geometry, material, background, renderer, guidance, and prompt_processor. All modules are subclass of <code>BaseModule</code> (in <code>utils/base.py</code>) except for guidance, and prompt_processor, which are subclass of <code>BaseObject</code> to prevent them from being treated as model parameters and better control their behavior in multi-GPU settings.</li>
<li>All systems, modules, and data modules have their configurations in their own dataclasses.</li>
<li>Base configurations for the whole project can be found in <code>utils/config.py</code>. In the <code>ExperimentConfig</code> dataclass, <code>data</code>, <code>system</code>, and module configurations under <code>system</code> are parsed to configurations of each class mentioned above. These configurations are strictly typed, which means you can only use defined properties in the dataclass and stick to the defined type of each property. This configuration paradigm (1) naturally supports default values for properties; (2) effectively prevents wrong assignments of these properties (say typos in the yaml file) or inappropriate usage at runtime.</li>
<li>This projects use both static and runtime type checking. For more details, see <code>utils/typing.py</code>.</li>
<li>To update anything of a module at each training step, simply make it inherit to <code>Updateable</code> (see <code>utils/base.py</code>). At the beginning of each iteration, an <code>Updateable</code> will update itself, and update all its attributes that are also <code>Updateable</code>. Note that subclasses of <code>BaseSystem</code>, <code>BaseModule</code> and <code>BaseObject</code> are by default inherited to <code>Updateable</code>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Known Problems</h2>
<ul dir="auto">
<li>Gradients of Vanilla MLP parameters are empty in AMP (temporarily fixed by disabling autocast).</li>
<li>FullyFused MLP may cause NaNs in 32 precision.</li>
</ul>
<h2 tabindex="-1" dir="auto">Credits</h2>
<p dir="auto">threestudio is built on the following amazing open-source projects:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/Lightning-AI/lightning">Lightning</a></strong> Framework for creating highly organized PyTorch code.</li>
<li><strong><a href="https://github.com/omry/omegaconf">OmegaConf</a></strong> Flexible Python configuration system.</li>
<li><strong><a href="https://github.com/KAIR-BAIR/nerfacc">NerfAcc</a></strong> Plug-and-play NeRF acceleration.</li>
</ul>
<p dir="auto">The following repositories greatly inspire threestudio:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/ashawkey/stable-dreamfusion">Stable-DreamFusion</a></strong></li>
<li><strong><a href="https://github.com/eladrich/latent-nerf">Latent-NeRF</a></strong></li>
<li><strong><a href="https://github.com/pals-ttic/sjc">Score Jacobian Chaining</a></strong></li>
<li><strong><a href="https://github.com/ashawkey/fantasia3d.unofficial">Fantasia3D.unofficial</a></strong></li>
</ul>
<p dir="auto">Thanks to the maintainers of these projects for their contribution to the community!</p>
<h2 tabindex="-1" dir="auto">Citing threestudio</h2>
<p dir="auto">If you find threestudio helpful, please consider citing:</p>
<div data-snippet-clipboard-copy-content="@Misc{threestudio2023,
  author =       {Yuan-Chen Guo and Ying-Tian Liu and Ruizhi Shao and Christian Laforte and Vikram Voleti and Guan Luo and Chia-Hao Chen and Zi-Xin Zou and Chen Wang and Yan-Pei Cao and Song-Hai Zhang},
  title =        {threestudio: A unified framework for 3D content generation},
  howpublished = {\url{https://github.com/threestudio-project/threestudio}},
  year =         {2023}
}"><pre><code>@Misc{threestudio2023,
  author =       {Yuan-Chen Guo and Ying-Tian Liu and Ruizhi Shao and Christian Laforte and Vikram Voleti and Guan Luo and Chia-Hao Chen and Zi-Xin Zou and Chen Wang and Yan-Pei Cao and Song-Hai Zhang},
  title =        {threestudio: A unified framework for 3D content generation},
  howpublished = {\url{https://github.com/threestudio-project/threestudio}},
  year =         {2023}
}
</code></pre></div>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Database Fundamentals (680 pts)]]></title>
            <link>https://tontinton.com/posts/database-fundementals/</link>
            <guid>38655066</guid>
            <pubDate>Fri, 15 Dec 2023 15:28:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tontinton.com/posts/database-fundementals/">https://tontinton.com/posts/database-fundementals/</a>, See on <a href="https://news.ycombinator.com/item?id=38655066">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>About a year ago, I tried thinking which database I should choose for my next project, and came to the realization that I don't really know the differences of databases enough. I went to different database websites and saw mostly marketing and words I don't understand.</p><p>This is when I decided to read the excellent books <code>Database Internals</code> by Alex Petrov and <code>Designing Data-Intensive Applications</code> by Martin Kleppmann.</p><p>The books piqued my curiosity enough to write my own little database I called <a href="https://github.com/tontinton/dbeel">dbeel</a>.</p><p>This post is basically a short summary of these books, with a focus on the fundamental problems a database engineer thinks about in the shower.</p><h2 id="bashdb">bashdb</h2><p>Let's start with the simplest database program ever written, just 2 bash functions (we'll call it <code>bashdb</code>):</p><pre data-lang="bash"><code data-lang="bash"><span>#!/bin/bash
</span><span>
</span><span>db_set</span><span>() {
</span><span>    </span><span>echo </span><span>"$</span><span>1</span><span>,$</span><span>2</span><span>" </span><span>&gt;&gt;</span><span> database
</span><span>}
</span><span>
</span><span>db_get</span><span>() {
</span><span>    </span><span>grep </span><span>"^$</span><span>1</span><span>,"</span><span> database </span><span>| </span><span>sed</span><span> -e </span><span>"s/^$</span><span>1</span><span>,//" </span><span>| </span><span>tail</span><span> -n</span><span> 1
</span><span>}
</span></code></pre><p>Try it out:</p><pre data-lang="sh"><code data-lang="sh"><span>$</span><span> db_set 500 </span><span>'{"movie": "Airplane!", "rating": 9}'
</span><span>
</span><span>$</span><span> db_set 111 </span><span>'{"movie": "Tokio Drift", "rating": 6}'
</span><span>
</span><span>$</span><span> db_get 500
</span><span>{</span><span>"movie"</span><span>: </span><span>"Airplane!"</span><span>, </span><span>"rating"</span><span>: 9}
</span></code></pre><p>Before you continue reading, I want you to pause and think about why you wouldn't use <code>bashdb</code> in production.</p><pre><code><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>Some space for you to think :)
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span></code></pre><p>You probably came up with at least a dozen issues in <code>bashdb</code>. Now I won't go over <em>all</em> of the possible issues, for this post I will focus on the following ones:</p><ul><li><strong>Durability</strong> - If the machine crashes after a successful <code>db_set</code>, the data might be lost, as it was not flushed to disk.</li><li><strong>Atomicity</strong> - If the machine crashes while you call <code>db_set</code>, data might be written partially, corrupting our data.</li><li><strong>Isolation</strong> - If one process calls <code>db_get</code>, while another calls <code>db_set</code> concurrently on the same item, the first process might read only part of the data, leading to a corrupt result.</li><li><strong>Performance</strong> - <code>db_get</code> uses <code>grep</code>, so search goes line by line and is <code>O(n)</code>, <code>n</code> = all items saved.</li></ul><p>Could you figure out these problems yourself? If you could, well done, you don't need me, you already understand databases 😀</p><p>In the next section, we'll try get rid of these problems, to make <code>bashdb</code> a <em>real</em> database we might use in production (not really, please don't, just use <code>PostgreSQL</code>).</p><h2 id="improving-bashdb-to-be-acid">Improving bashdb to be ACID</h2><p>Before we begin, know that I did not come up with most of these problems on my own, they are part of an acronym named <code>ACID</code>, which almost all databases strive to guarantee:</p><ul><li><strong>Atomicity</strong> - Not to be confused with multi-threading's definition of atomicity (which is more similar to isolation), a transaction is considered atomic when a fault happens in the middle of a write, and the database either undos or aborts it completely, as if the write never started, leaving no partially written data.</li><li><strong>Consistency</strong> - This one doesn't really belong on ACID as a property of databases, as it is a property of the application.</li><li><strong>Isolation</strong> - No race conditions in concurrent accesses to the same data. There are multiple isolation levels, and we will discuss some of them later.</li><li><strong>Durability</strong> - The first thing that comes to mind when talking about a database. It should store data you wrote to it, forever, even in the event of monkeys pulling the power plug out.</li></ul><blockquote><p>Not all databases need to guarantee ACID, for some use cases, it is fine to drop guarantees for performance reasons.</p></blockquote><p>But <em>how</em> can we make <code>bashdb</code> ACID?</p><p>We can start with durability, as it's pretty easy to make <code>bashdb</code> durable by running <code>sync</code> right after writing in <code>db_set</code>:</p><pre data-lang="bash"><code data-lang="bash"><span>db_set</span><span>() {
</span><span>    </span><span>echo </span><span>"$</span><span>1</span><span>,$</span><span>2</span><span>" </span><span>&gt;&gt;</span><span> database </span><span>&amp;&amp; </span><span>sync</span><span> -d</span><span> database
</span><span>}
</span></code></pre><p>But wait a minute, what is going on, what is <code>sync</code> really doing? And what is that <code>-d</code>?</p><h3 id="durability">Durability</h3><p>The <code>write</code> syscall writes a buffer to a file, but who said it writes to disk?</p><p>The buffer you write could end up in any cache along the way to the non volatile memory. For example, the kernel stores the buffer in the page cache with each page marked as dirty, meaning it will flush it to disk sometime in the future.</p><p>To make matters worse, the disk device, or something managing your disks (for example a RAID system), might have a write cache as well.</p><p>So how do you tell all the systems in the middle to flush all dirty pages to the disk? For that we have <code>fsync</code> / <code>fdatasync</code>, let's see what <code>man</code> has to say:</p><pre><code><span>$ man 2 fsync
</span><span>
</span><span>...
</span><span>
</span><span>fsync() transfers ("flushes") all modified in-core data of (i.e., modified buffer cache pages for)
</span><span>the file referred to by the file descriptor fd to the disk device (or other permanent storage
</span><span>device) so that all changed information can be retrieved even if the system crashes or is rebooted.
</span><span>This includes writing through or flushing a disk cache if present.
</span><span>The call blocks until the device reports that the transfer has completed.
</span><span>
</span><span>...
</span><span>
</span><span>fdatasync() is similar to fsync(), but does not flush modified metadata unless that metadata itself
</span><span>in order to allow a subsequent data  retrieval to be correctly handled.
</span><span>
</span><span>...
</span></code></pre><p>In short, <code>fdatasync</code> flushes the dirty raw buffers we gave <code>write</code>. <code>fsync</code> also flushes the file's metadata like <code>mtime</code>, which we don't really care about.</p><p>The <code>sync</code> program is basically like running <code>fsync</code> on all dirty pages, unless a specific file is specified as one of the arguments. It has the <code>-d</code> flag for us to call <code>fdatasync</code> instead of <code>fsync</code>.</p><p>The biggest drawback in adding <code>sync</code> is that we get worse performance. Usually sync is slower than even the write itself. But hey, at least we are now <em>durable</em>.</p><blockquote><p>A short but important note about fsync. When fsync() returns success it means "all writes since the last fsync have hit disk" when you might have assumed it means "all writes since the last SUCCESSFUL fsync have hit disk". PostgreSQL learned about this only recently (2018), which led to them modifying the behavior of syncing from retrying fsync until a success is returned, to simply panic on fsync failure. This incident got famous and was named fsyncgate. You can learn a lot more about fsync failures <a href="https://www.usenix.org/system/files/atc20-rebello.pdf">here</a>.</p></blockquote><blockquote><p>Dear <code>MongoDB</code> users, know that by default writes are <a href="https://www.mongodb.com/docs/manual/core/journaling/writes">synced every 100ms</a>, meaning it is not 100% durable.</p></blockquote><h3 id="isolation">Isolation</h3><p>The simplest way to have multiprocess isolation in <code>bashdb</code> is to add a lock before we read / write to the storage file.</p><p>There's a program in linux called <code>flock</code>, which locks a file, and you can even provide it with the <code>-s</code> flag, to specify that you will not modify the file, meaning all callers who specify <code>-s</code> are allowed to read the file concurrently. <code>flock</code> blocks until it has taken the lock.</p><blockquote><p>flock simply calls the flock syscall</p></blockquote><p>With such an awesome program, <code>bashdb</code> can guarantee <em>isolation</em>, here's the code:</p><pre data-lang="bash"><code data-lang="bash"><span>db_set</span><span>() {
</span><span>    (
</span><span>        </span><span>flock</span><span> 9 </span><span>&amp;&amp; </span><span>echo </span><span>"$</span><span>1</span><span>,$</span><span>2</span><span>" </span><span>&gt;&gt;</span><span> database
</span><span>    ) </span><span>9</span><span>&gt;</span><span>database.lock
</span><span>}
</span><span>
</span><span>db_get</span><span>() {
</span><span>    (
</span><span>        </span><span>flock</span><span> -s</span><span> 9 </span><span>&amp;&amp; </span><span>grep </span><span>"^$</span><span>1</span><span>,"</span><span> database </span><span>| </span><span>sed</span><span> -e </span><span>"s/^$</span><span>1</span><span>,//" </span><span>| </span><span>tail</span><span> -n</span><span> 1
</span><span>    ) </span><span>9</span><span>&gt;</span><span>database.lock
</span><span>}
</span></code></pre><p>The biggest drawback is that we are now locking the entire database whenever we write to it.</p><p>The only things left are atomicity and improving the algorithm to not be <code>O(n)</code>.</p><h2 id="bad-news">Bad News</h2><p>I'm sorry, this is as far as I could get with <code>bashdb</code>, I could not find a simple way to ensure atomicity in bash ☹️</p><p>And even if it was possible, we still need to fix the <code>O(n)</code> situation.</p><p>Before beginning the <code>bashdb</code> adventure, I knew that we won't be able to easily solve all these problems in less than 10 lines of bash, but by trying to, you've hopefully started to get a feel for the problems database engineers face.</p><h2 id="storage-engine">Storage Engine</h2><p>Let's start with the first big component of a database, the <code>Storage Engine</code>.</p><p>The purpose of the storage engine is to provide an abstraction over reading and writing data to persistent storage, with the main goal to be <strong>fast</strong>, i.e. have <strong>high throughput</strong> and <strong>low latency</strong> on requests.</p><p>But what makes software slow?</p><pre><code><span>Latency Comparison Numbers (~2012)
</span><span>----------------------------------
</span><span>L1 cache reference                           0.5 ns
</span><span>Branch mispredict                            5   ns
</span><span>L2 cache reference                           7   ns                      14x L1 cache
</span><span>Mutex lock/unlock                           25   ns
</span><span>Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache
</span><span>Compress 1K bytes with Zippy             3,000   ns        3 us
</span><span>Send 1K bytes over 1 Gbps network       10,000   ns       10 us
</span><span>Read 4K randomly from SSD              150,000   ns      150 us          ~1GB/sec SSD
</span><span>Read 1 MB sequentially from memory     250,000   ns      250 us
</span><span>Round trip within same datacenter      500,000   ns      500 us
</span><span>Read 1 MB sequentially from SSD      1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory
</span><span>Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip
</span><span>Read 1 MB sequentially from disk    20,000,000   ns   20,000 us   20 ms  80x memory, 20X SSD
</span><span>Send packet CA-&gt;Netherlands-&gt;CA    150,000,000   ns  150,000 us  150 ms
</span></code></pre><p>If L1 cache reference took as long as a heart beat (around half a second), reading 1 MB sequentially from SSD would take ~12 days and reading 1 MB sequentially from disk would take ~8 months.</p><p>This is why the main limitation of storage engines is the disk itself, and thus all designs try to minimize disk I/O and disk seeks as much as possible. Some designs even get rid of disks in favor of SSDs (although they are much more expensive).</p><p>A storage engine design usually consists of:</p><ul><li>The underlying data structure to store items on disk.</li><li>ACID transactions. <ul><li>Some may skip this to achieve better performance for specific use cases where ACID is not important.</li></ul></li><li>Some cache - to not read from disk <em>every</em> time. <ul><li>Most use buffered I/O to let the OS cache for us.</li></ul></li><li>API layer - SQL / document / graph / ...</li></ul><p>Storage engine data structures come in all shapes and sizes, I'm going to focus on the 2 categories you will most likely find in the wild - mutable and immutable data structures.</p><p>Mutable means that after writing data to a file, the data can be overwritten later in the future, while immutable means that after writing data to a file, it can only be read again.</p><h2 id="mutable-b-trees">Mutable B-Trees</h2><p>To achieve the goal of maintaining good performance as the amount of data scales up, the data structure we use should be able to search an item in at most logarithmic time, and not linear time like in <code>bashdb</code>.</p><p>A simple data structure you are probably familiar with is the BST (binary search tree), where lookups are made in <code>O(log n)</code> time.</p><p>The problem with BSTs is nodes are placed randomly apart from each other, which means that after reading a node while traversing the tree, the next node is most likely going to be somewhere far away on disk. To minimize disk I/O &amp; seeks, each page read from disk should be read as much as possible from memory again, without reaching to disk.</p><p>The property we're looking for is called "spatial locality", and one of the most famous "spatially local" variations of BSTs are B-trees.</p><p>B-tree generalizes BST, allowing for nodes with more than two children. Here's what they look like:</p><pre><code><span>                  ------------------------------------
</span><span>                  |     7     |     16     |    |    |
</span><span>                  ------------------------------------
</span><span>                 /            |             \
</span><span>-----------------     ----------------       -----------------
</span><span>| 1 | 2 | 5 | 6 |     | 9 | 12 |  |  |       | 18 | 21 |  |  |
</span><span>-----------------     ----------------       -----------------
</span></code></pre><p>With the search algorithm in pseudo python code:</p><pre data-lang="python"><code data-lang="python"><span>def </span><span>get</span><span>(</span><span>node</span><span>, </span><span>key</span><span>):
</span><span>    </span><span>for </span><span>i, child </span><span>in </span><span>enumerate</span><span>(node</span><span>.</span><span>children):
</span><span>        </span><span>if not </span><span>child:
</span><span>            </span><span>return </span><span>None
</span><span>
</span><span>        </span><span>if </span><span>child</span><span>.</span><span>key </span><span>== </span><span>key:
</span><span>            </span><span># Found it!
</span><span>            </span><span>return </span><span>child</span><span>.</span><span>value
</span><span>
</span><span>        </span><span>if </span><span>child</span><span>.</span><span>key </span><span>&gt; </span><span>key:
</span><span>            </span><span>return </span><span>get</span><span>(node</span><span>.</span><span>nodes[i], key)
</span><span>
</span><span>    </span><span>return </span><span>get</span><span>(node</span><span>.</span><span>nodes[</span><span>-</span><span>1</span><span>], key)
</span></code></pre><p>On each read of a page from disk (usually 4KB or 8KB), we iterate over multiple nodes sequentially from memory and the various CPU caches, trying to keep the least amount of bytes read go to waste.</p><p>Remember, reading from memory and the CPU caches is a few order of magnitudes faster than disk, so much faster in fact, that it can be considered to be basically free in comparison.</p><p>I know some of you reading this right now think to themselves <em>"Why not binary search instead of doing it linearly?"</em>, to you I say, please look at the L1 / L2 cache reference times in the latency comparison numbers table again. Also, modern CPUs execute multiple operations in parallel when it operates on sequential memory thanks to <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a>, <a href="https://en.wikipedia.org/wiki/Instruction_pipelining">instruction pipelining</a> and <a href="https://en.wikipedia.org/wiki/Cache_prefetching">prefetching</a>. You would be surprised just how far reading sequential memory can take you in terms of performance.</p><p>There's a variation of the B-tree that takes this model even further, called a B+ tree, where the final leaf nodes hold a value and all other nodes hold only keys, thus fetching a page from disk results in a lot more keys to compare.</p><p>B-trees, to be space optimized, need to sometimes reclaim space as a consequence of data fragmentation created by operations on the tree like:</p><ul><li>Big value updates - updating a value into a larger value might overwrite data of the next node, so the tree relocates the item to a different location, leaving a "hole" in the original page.</li><li>Small value updates - updating a value to a smaller value leaves a "hole" at the end.</li><li>Deletes - deletion causes a "hole" right where the deleted value used to reside.</li></ul><p>The process that takes care of space reclamation and page rewrites can sometimes be called vacuum, compaction, page defragmentation, and maintenance. It is usually done in the background to not interfere and cause latency spikes to user requests.</p><blockquote><p>See for example how in <code>PostgreSQL</code> you can configure an <a href="https://www.postgresql.org/docs/current/routine-vacuuming.html">auto vacuum daemon</a>.</p></blockquote><p>B-trees are most commonly used as the underlying data structure of an index (<code>PostgreSQL</code> creates B-tree indexes by default), or all data (I've seen <code>DynamoDB</code> once jokingly called <em>"a distributed B-tree"</em>).</p><h2 id="immutable-lsm-tree">Immutable LSM Tree</h2><p>As we have already seen in the latency comparison numbers table, disk seeks are really expensive, which is why the idea of sequentially written immutable data structures got so popular.</p><p>The idea is that if you only append data to a file, the disk needle doesn't need to move as much to the next position where data will be written. On write heavy workloads it has been proven very beneficial.</p><p>One such append only data structure is called the <code>Log Structured Merge tree</code> or <code>LSM tree</code> in short, and is what powers <em>a lot</em> of modern database storage engines, such as <code>RocksDB</code>, <code>Cassandra</code> and my personal favorite <code>ScyllaDB</code>.</p><p>LSM trees' general concept is to buffer writes to a data structure in memory, preferably one that is easy to iterate in a sorted fashion (for example <code>AVL tree</code> / <code>Red Black tree</code> / <code>Skip List</code>), and once it reaches some capacity, flush it sorted to a new file called a <code>Sorted String Table</code> or <code>SSTable</code>. An SSTable stores sorted data, letting us leverage binary search and sparse indexes to lower the amount of disk I/O.</p><img src="https://tontinton.com/lsm_tree_write.svg"><p>To maintain durability, when data is written to memory, the action is stored in a <code>Write-Ahead Log</code> or <code>WAL</code>, which is read on program's startup to reset state to as it was before shutting down / crashing.</p><p>Deletions are also appended the same way a write would, it simply holds a tombstone instead of a value. The tombstones get deleted in the compaction process detailed later.</p><p>The read path is where it a bit wonky, reading from an LSM tree is done by first searching for the item of the provided key in the data structure in memory, if not found, it then searches for the item by iterating over all SSTables on disk, from the newest one to the oldest.</p><img src="https://tontinton.com/lsm_tree_read.svg"><p>You can probably already tell that as more and more data is written, there will be more SSTables to go through to find an item of a specific key, and even though each file is sorted, going over a lot of small files is slower than going over one big file with all items (lookup time complexity: <code>log(num_files * table_size) &lt; num_files * log(table_size)</code>). This is another reason why LSM trees require compaction, in addition to removing tombstones.</p><p>In other words: compaction combines a few small SSTables into one big SSTable, removing all tombstones in the process, and is usually run as a background process.</p><img src="https://tontinton.com/lsm_tree_compact.svg"><p>Compaction can be implemented using a binary heap / priority queue, something like:</p><pre data-lang="python"><code data-lang="python"><span>def </span><span>compact</span><span>(</span><span>sstables</span><span>, </span><span>output_sstable</span><span>): 
</span><span>    </span><span># Ordered by ascending key. pop() results in the item of the smallest key.
</span><span>    heap </span><span>= </span><span>heapq</span><span>.</span><span>heapify</span><span>([(sstable</span><span>.</span><span>next</span><span>(), sstable) </span><span>for </span><span>sstable </span><span>in </span><span>sstables])
</span><span>
</span><span>    </span><span>while </span><span>(item, sstable) </span><span>:= </span><span>heap</span><span>.</span><span>pop</span><span>()
</span><span>        </span><span>if not </span><span>item</span><span>.</span><span>is_tombstone</span><span>():
</span><span>            output_sstable</span><span>.</span><span>write</span><span>(item)
</span><span>
</span><span>        </span><span>if </span><span>item </span><span>:= </span><span>sstable</span><span>.</span><span>next</span><span>():
</span><span>            </span><span># For code brevity, imagine pushing an item with a key that exists
</span><span>            </span><span># in the heap removes the item with the smaller timestamp,
</span><span>            </span><span># resulting in last write wins.
</span><span>            heap</span><span>.</span><span>push</span><span>((item, sstable))
</span></code></pre><blockquote><p>For a real working example in rust 🦀, <a href="https://github.com/tontinton/dbeel/blob/ee3de152a5/src/storage_engine/lsm_tree.rs#L1038">click here</a>.</p></blockquote><p>To optimize an LSM tree, you should decide <em>when</em> to compact and on <em>which</em> sstable files. <code>RocksDB</code> for example implements <a href="https://github.com/facebook/rocksdb/wiki/Leveled-Compaction">Leveled Compaction</a>, where the newly flushed sstables are said to reside in level 0, and once a configured N number of files are created in a level, they are compacted and the new file is promoted to the next level.</p><p>It's important to handle removal of tombstones with care to not cause data resurrection. An item might be removed and then resurrected on compaction with another file that holds that item, even if the write happened before the deletion, there is no way to know once deleted in a previous compaction. <code>RocksDB</code> keeps tombstones around until a compaction of files that result in a promotion to the last level.</p><h3 id="bloom-filters">Bloom Filters</h3><p>LSM trees can be further optimized by something called a bloom filter.</p><p>A bloom filter is a probabilistic set data structure that lets you to efficiently check whether an item doesn't exist in a set. Checking whether an item exists in the set results in either <code>false</code>, which means the item is definitely not in the set, or in <code>true</code>, which means the item is <strong>maybe</strong> in the set, and that's why it's called a <em>probabilistic</em> data structure.</p><p>The beauty is that the space complexity of a bloom filter set of <code>n</code> items is <code>O(log n)</code>, while a regular set with <code>n</code> items is <code>O(n)</code>.</p><p>How do they work? The answer is hash functions! On insertion, they run multiple different hash functions on the inserted key, then take the results and store 1 in the corresponding bit (<code>result % number_of_bits</code>).</p><pre data-lang="python"><code data-lang="python"><span># A bloom filter's bitmap of size 8 (bits).
</span><span>bloom </span><span>= </span><span>[</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>]
</span><span>
</span><span># Inserting key - first run 2 hash functions.
</span><span>Hash1</span><span>(key1) </span><span>= </span><span>100
</span><span>Hash2</span><span>(key1) </span><span>= </span><span>55
</span><span>
</span><span># Then calculate corresponding bits.
</span><span>bits </span><span>= </span><span>[</span><span>100 </span><span>% </span><span>8</span><span>, </span><span>55 </span><span>% </span><span>8</span><span>] </span><span>= </span><span>[</span><span>4</span><span>, </span><span>7</span><span>]
</span><span>
</span><span># Set 1 to corresponding bits.
</span><span>bloom[</span><span>4</span><span>] </span><span>= </span><span>1
</span><span>bloom[</span><span>7</span><span>] </span><span>= </span><span>1
</span><span>
</span><span># After insertion it should look like:
</span><span>[</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>]
</span></code></pre><p>Now comes the exciting part - checking!</p><pre data-lang="python"><code data-lang="python"><span>bloom </span><span>= </span><span>[</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>]
</span><span>
</span><span># To check a key, simply run the 2 hash functions and find the corresponding
</span><span># bits, exactly like you would on insertion:
</span><span>Hash1</span><span>(key2) </span><span>= </span><span>34
</span><span>Hash2</span><span>(key2) </span><span>= </span><span>35
</span><span>
</span><span>bits </span><span>= </span><span>[</span><span>34 </span><span>% </span><span>8</span><span>, </span><span>35 </span><span>% </span><span>8</span><span>] </span><span>= </span><span>[</span><span>2</span><span>, </span><span>3</span><span>]
</span><span>
</span><span># And then check whether all the corresponding bits hold 1, if true, the item
</span><span># maybe exists in the set, otherwise it definitely isn't.
</span><span>result </span><span>= </span><span>[bloom[</span><span>2</span><span>], bloom[</span><span>3</span><span>]] </span><span>= </span><span>[</span><span>0</span><span>, </span><span>0</span><span>] </span><span>= </span><span>false
</span><span>
</span><span># false. key2 was never inserted in the set, otherwise those exact same bits
</span><span># would have all been set to 1.
</span></code></pre><blockquote><p>Think about why it is that even when all checked bits are 1, it doesn't guarantee that the same exact key was inserted before.</p></blockquote><p>A nice benefit of bloom filters is that you can control the chance of being certain that the item doesn't exist in the set, by allocating more memory for the bitmap and by adding more hash functions. There's even <a href="https://hur.st/bloomfilter/">calculators</a> for it.</p><p>LSM trees can store a bloom filter for each SSTable, to skip searching in SSTables if their bloom filter validates that an item doesn't exist in it. Otherwise, we search the SSTable normally, even if the item doesn't necessarily exist in it.</p><h2 id="write-ahead-log">Write Ahead Log</h2><p>Remember ACID? Let's talk briefly about how storage engines achieve ACID transactions.</p><p>Atomicity and durability are properties of whether data is correct at all times, even when power shuts down the machine.</p><p>The most popular method to survive sudden crashes is to log all transaction actions into a special file called a <code>Write-Ahead Log</code> / <code>WAL</code> (we touched on this briefly in the <code>LSM tree</code> section).</p><p>When the database process starts, it reads the <code>WAL</code> file, and reconstructs the state of the data, skipping all transactions that don't have a commit log, thus achieving atomicity.</p><p>Also, as long a write request's data is written + flushed to the <code>WAL</code> file before the user receives the response, the data is going to be 100% read at startup, meaning you also achieve durability.</p><p>WALs are basically a sort of <a href="https://martinfowler.com/eaaDev/EventSourcing.html">event sourcing</a> of the transactional events.</p><h2 id="isolation-1">Isolation</h2><p>To achieve isolation, you can either:</p><ul><li>Use pessimistic locks - Block access to data that is currently being written to.</li><li>Use optimistic locks - Update a copy of the data and then commit it only whether the data was not modified during the transaction, if it did, retry on the new data. Also known as optimistic concurrency control.</li><li>Read a copy of the data - MVCC (Multiversion concurrency control) is a common method used to avoid blocking user requests. In MVCC when data is mutated, instead of locking + overwriting it, you create a new version of the data that new requests read from. Once no readers remain that are reading the old data it can be safely removed. With MVCC, each user sees a <em>snapshot</em> of the database at a specific instant in time.</li></ul><p>Some applications don't require perfect isolation (or <code>Serializable Isolation</code>), and can relax their read isolation levels.</p><p>The ANSI/ISO standard SQL 92 includes 3 different possible outcomes from reading data in a transaction, while another transaction might have updated that data:</p><ul><li><strong>Dirty reads</strong> - A dirty read occurs when a transaction retrieves a row that has been updated by another transaction that is not yet committed.</li></ul><pre data-lang="sql"><code data-lang="sql"><span>BEGIN</span><span>;
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves 20
</span><span>
</span><span>
</span><span>                                        </span><span>BEGIN</span><span>;
</span><span>                                        </span><span>UPDATE</span><span> users </span><span>SET</span><span> age </span><span>= </span><span>21 </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>                                        </span><span>-- no commit here
</span><span>
</span><span>
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves in 21
</span><span>COMMIT</span><span>;
</span></code></pre><ul><li><strong>Non-repeatable reads</strong> - A non-repeatable read occurs when a transaction retrieves a row twice and that row is updated by another transaction that is committed in between.</li></ul><pre data-lang="sql"><code data-lang="sql"><span>BEGIN</span><span>;
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves 20
</span><span>
</span><span>
</span><span>                                        </span><span>BEGIN</span><span>;
</span><span>                                        </span><span>UPDATE</span><span> users </span><span>SET</span><span> age </span><span>= </span><span>21 </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>                                        </span><span>COMMIT</span><span>;
</span><span>
</span><span>
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves 21
</span><span>COMMIT</span><span>;
</span></code></pre><ul><li><strong>Phantom reads</strong> - A phantom read occurs when a transaction retrieves a set of rows twice and new rows are inserted into or removed from that set by another transaction that is committed in between.</li></ul><pre data-lang="sql"><code data-lang="sql"><span>BEGIN</span><span>;
</span><span>SELECT</span><span> name </span><span>FROM</span><span> users </span><span>WHERE</span><span> age </span><span>&gt; </span><span>17</span><span>;
</span><span>-- retrieves Alice and Bob
</span><span>	
</span><span>
</span><span>                                        </span><span>BEGIN</span><span>;
</span><span>                                        </span><span>INSERT INTO</span><span> users </span><span>VALUES</span><span> (</span><span>3</span><span>, </span><span>'Carol'</span><span>, </span><span>26</span><span>);
</span><span>                                        </span><span>COMMIT</span><span>;
</span><span>
</span><span>
</span><span>SELECT</span><span> name </span><span>FROM</span><span> users </span><span>WHERE</span><span> age </span><span>&gt; </span><span>17</span><span>;
</span><span>-- retrieves Alice, Bob and Carol
</span><span>COMMIT</span><span>;
</span></code></pre><p>Your application might not need a guarantee of no dirty reads for example in a specific transaction, so it can choose a different isolation level to allow greater performance, as to achieve higher isolation levels, you usually sacrifice performance.</p><p>Here are isolation levels defined by the ANSI/SQL 92 standard from highest to lowest (higher levels guarantee at least everything lower levels guarantee):</p><ul><li><strong>Serializable</strong> - The highest isolation level. Reads always return data that is committed, including range based writes on multiple rows (avoiding phantom reads).</li><li><strong>Repeatable reads</strong> - Phantom reads are acceptable.</li><li><strong>Read committed</strong> - Non-repeatable reads are acceptable.</li><li><strong>Read uncommitted</strong> - The lowest isolation level. Dirty reads are acceptable.</li></ul><blockquote><p>The ANSI/SQL 92 standard isolation levels are often criticized for not being complete. For example, many MVCC implementations offer <a href="https://en.wikipedia.org/wiki/Snapshot_isolation">snapshot isolation</a> and not serializable isolation (for the differences, read the provided wikipedia link). If you want to learn more about MVCC, I recommend reading about <a href="https://db.in.tum.de/~muehlbau/papers/mvcc.pdf">HyPer</a>, a fast serializable MVCC algorithm.</p></blockquote><p>So to conclude the storage engine part of this post, the fundamental problem you solve writing a storage engine are: how to store / retrieve data while trying to guarantee some ACID transactions in the most performant way.</p><blockquote><p>One topic I left out is the API to choose when writing a database / storage engine, but I'll leave a post called <a href="https://www.scattered-thoughts.net/writing/against-sql/">"Against SQL"</a> for you to start exploring the topic yourself.</p></blockquote><h2 id="distributed-systems">Distributed Systems</h2><p>Going distributed should be a last mile resort, introducing it to a system adds a <strong>ton</strong> of complexity, as we will soon learn. Please avoid using distributed systems when non distributed solutions suffice.</p><blockquote><p>A distributed system is one in which the failure of a computer you didn’t even know existed can render your own computer unusable. ~Leslie Lamport</p></blockquote><p>The common use cases of needing to distribute data across multiple machines are:</p><ul><li><strong>Availability</strong> - If for some reason the machine running the database crashes / disconnects from our users, we might still want to let users use the application. By distributing data, when one machine fails, you can simply point requests to another machine holding the "redundant" data.</li><li><strong>Horizontal Scaling</strong> - Conventionally, when an application needed to serve more user requests than it can handle, we would have upgraded the machine's resources (faster / more disk, RAM, CPUs). This is called <code>Vertical Scaling</code>. It can get very expensive and for some workloads there just doesn't exist hardware to match the amount of resources needed. Also, most of the time you don't need all those resources, except in peaks of traffic (imagine shopify on black Friday). Another strategy called <code>Horizontal Scaling</code>, is to operate on multiple separate machines connected over a network, seemingly working as a single machine.</li></ul><p>Sounds like a dream, right? What can go wrong with going distributed?</p><p>Well, you have now introduced operational complexity (deployments / etc...) and more importantly partitioning / network partitioning, infamous for being the P in something called the CAP theorem.</p><p>The CAP theorem states that a system can guarantee only 2 of the following 3:</p><ul><li><strong>Consistency</strong> - Reads receive the most recent write.</li><li><strong>Availability</strong> - All requests succeed, no matter the failures.</li><li><strong>Partition Tolerance</strong> - The system continues to operate despite dropped / delayed messages between nodes.</li></ul><p>To understand why this is, imagine a database operating on a single machine. It is definitely <em>partition tolerant</em>, as messages in the system are not sent through something like a network, but through function calls operating on the same hardware (CPU / memory). It is also <em>consistent</em>, as the state of the data is saved on the same hardware (memory / disk) that all other read / write requests operate on. Once the machine fails (be it software failures like SIGSEGV or hardware failures like the disk overheating) all new requests to it fail, violating <em>availability</em>.</p><p>Now imagine a database operating on 2 machines with separate CPUs, memory and disks, connected through some cable. When a request to one of the machines fails, for whatever reason, the system can choose to do one of the following:</p><ul><li>Cancel the request, thus sacrificing <em>availability</em> for <em>consistency</em>.</li><li>Allow the request to continue only on the working machine, meaning once the other machine will now have inconsistent data (reads from it will not return the most recent write), thus sacrificing <em>consistency</em> for <em>availability</em>. When a system does this, it is called eventually consistent.</li></ul><p>The original <a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">dynamo paper</a> is famous for many things, one of them being amazon stating that amazon.com's shopping cart should be highly available, and that it's more important to them than consistency. In the unlikely scenario a user sees 2 of the same item in the shopping cart, they will simply remove one of them, which is a better situation then them not being able to purchase and pay money!</p><blockquote><p>I really enjoy out of the box thinking of sacrificing something that adds software complexity (like consistency in amazon's shopping cart) for a simpler human solution like the user getting a refund. Software complexity can get more expensive to operate than having a refund budget for example.</p></blockquote><p>To achieve <em>availability</em> it's not enough to have multiple nodes together combining all the data, there must also be data redundancy, or in other words, for each item a node stores there must be at least 1 other node to store a copy of that item. These nodes are usually called <strong>replicas</strong>, and the process of copying the data is called <strong>replication</strong>.</p><p>Assigning more replica nodes means that the system will be more <em>available</em>, with the obvious drawback of needing more resources to store all these copies.</p><blockquote><p>Copies of data don't need to be stored "whole", they can be split and scattered across multiple nodes using a technique called erasure coding, which also has some interesting <a href="https://brooker.co.za/blog/2023/01/06/erasure.html">latency characteristics</a> (by the way brooker's blog is simply amazing for learning distributed systems).</p></blockquote><h2 id="consistent-hashing">Consistent Hashing</h2><p>Now that you have multiple nodes, you need some kind of load balancing / data partitioning method. When a request to store some data comes in, how do you determine which node receives the request?</p><p>You could go for the simplest solution, which is to simply always take a primary key (some id) in addition to the data, hash the key and modulo the result by the number of available nodes, something like:</p><pre data-lang="python"><code data-lang="python"><span>def </span><span>get_owning_node</span><span>(</span><span>nodes</span><span>, </span><span>key</span><span>):
</span><span>    </span><span>return </span><span>nodes[</span><span>hash</span><span>(key) </span><span>% </span><span>len</span><span>(nodes)] 
</span></code></pre><p>This modulo method works fine, until a node is either added or removed from the cluster. Once that happens, the calculation returns a different result because the number of available nodes changed, meaning a different node will be selected for the same key. To accommodate, each node can migrate keys that should now live on different nodes, but then almost all items are migrated, which is really expensive.</p><p>One method to lower the amount of items to be migrated on node addition / removal that is used by some databases (e.g. <code>Dynamo</code> and <code>Cassandra</code>) is <code>Consistent Hashing</code>.</p><p>Consistent hashing creates a ring of nodes instead of an array, placing each node's name hash on the ring. Then each request's key is hashed just like before, but instead of doing the modulo operation, we get the first node in the ring whose name's hash is greater or equal to the request key hash:</p><pre data-lang="python"><code data-lang="python"><span># Assume nodes are sorted, with the first node having the smallest hash value.
</span><span>def </span><span>get_owning_node</span><span>(</span><span>nodes</span><span>, </span><span>key</span><span>):
</span><span>    </span><span>if </span><span>len</span><span>(nodes) </span><span>== </span><span>0</span><span>:
</span><span>        </span><span>return </span><span>None
</span><span>
</span><span>    key_hash </span><span>= </span><span>hash</span><span>(key)
</span><span>
</span><span>    </span><span>for </span><span>node </span><span>in </span><span>nodes:
</span><span>        </span><span>if </span><span>node</span><span>.</span><span>hash </span><span>&gt;= </span><span>key_hash:
</span><span>            </span><span>return </span><span>node
</span><span>
</span><span>    </span><span>return </span><span>nodes[</span><span>0</span><span>]
</span></code></pre><p>For a visual explanation, imagine a ring that goes from 0 -&gt; 99, holding nodes with the names "half", "quarter" and "zero" whose hashes are 50, 25 and 0 respectively:</p><pre><code><span>   zero
</span><span> /      \
</span><span>|     quarter 
</span><span> \      /
</span><span>   half
</span></code></pre><p>Let's say a user now wants to set an item with the key "four-fifths", with a hash value of 80. The first node with a name hash greater or equal to 80 is "half" (with hash value of 50), so that's the node to receive the request!</p><p>Choosing replicas is very simple, when an item is set to be stored on a specific node, go around the ring counter-clockwise, the next node will store a copy of that item. In our example, "zero" is the replica node for all items "half" owns, so when "half" dies and requests will now be routed to "zero", it can serve these requests, keeping our system <em>available</em>. This method is sometimes called <code>Leaderless Replication</code> and is used by "Dynamo" style databases like <code>Cassandra</code>.</p><blockquote><p>Another method is to choose a leader node and replica nodes is <code>Leader Election</code>, which is a huge topic on its own that I won't get into in this post.</p></blockquote><p>Now, what happens when a node is added to the cluster? Let's add a node named "three-quarters" with a hash value of 75, the item "four-fifths" should be migrated to the new "three-quarters" node, as new requests to it will now point to it.</p><p>This migration process is a lot less expensive than what we previously had in the modulo solution. The number of keys that need to be migrated is equal to <code>num_keys / num_nodes</code> on average.</p><p>A cool trick is to introduce the concept of virtual nodes, where you add multiple instances of a node to the ring, to lower the chances of some nodes owning more items than other nodes (in our example "half" will store twice as many items on average than the other nodes). You can generate virtual node names by for example adding an index as a suffix to the node name ("half-0", "half-1", etc...) and then the hash will result in a completely different location on the ring.</p><p>Here's a more detailed example of a migration in a cluster with a replication factor of 3:</p><img src="https://tontinton.com/migration.svg"><blockquote><p>Same colored nodes are virtual nodes of the same node, green arrows show to which node an item is being migrated to, red arrows show item deletions from nodes and the brown diamonds are items.</p></blockquote><h2 id="leaderless-replication">Leaderless Replication</h2><p>In a leaderless setup, you get amazing <em>availability</em>, while sacrificing <em>consistency</em>. If the owning node is down on a write request, it will be written to the replica, and once the owning node is up and running again, a read request will read stale data.</p><p>When <em>consistency</em> is needed for a specific request, read requests can be sent in parallel to several replica nodes as well as to the owning node. The client will pick the most up to date data. Write requests are usually sent in parallel to all replica nodes but wait for an acknowledgement from only some of them. By choosing the number of read requests and number of write requests acknowledge, you can tune the <em>consistency</em> level on a request level.</p><p>To know whether a request is <em>consistent</em>, you just need to validate that <code>R + W &gt; N/2 + 1</code>, where:</p><ul><li><strong>N</strong> - Number of nodes holding a copy of the data.</li><li><strong>W</strong> - Number of nodes that will acknowledge a write for it to succeed.</li><li><strong>R</strong> - Number of nodes that have to respond to a read operation for it to succeed.</li></ul><blockquote><p>Sending a request to a majority of nodes (where <code>W</code> or <code>R</code> is equal to <code>N/2 + 1</code>) is called a quorum.</p></blockquote><p>Picking the correct read as the latest written one is called <code>Conflict Resolution</code> and it is not a simple task, you might think that simply comparing timestamps and choosing the biggest one is enough, but using times in a distributed system are unreliable.</p><blockquote><p>This didn't stop <a href="https://cassandra.apache.org/doc/latest/cassandra/architecture/dynamo.html#data-versioning">Cassandra from using timestamps</a> though.</p></blockquote><p>Each machine has its own hardware clock, and the clocks <em>drift</em> apart as they are not perfectly accurate (usually a quartz crystal oscillator). Synchronizing clocks using NTP (Network Time Protocol), where a server returns the time from a more accurate time source such as a GPS receiver, is not enough to provide accurate results, as the NTP request is over the network (another distributed system) and we can't know exactly how much time will pass before receiving a response.</p><blockquote><p>Google's <code>Spanner</code> actually did achieve consistency with clocks, by uses special high precision time hardware and its API exposes the time range uncertainty of each timestamp. You can read more about it <a href="https://research.google/pubs/pub39966.pdf">here</a>.</p></blockquote><p>But if clocks are so unreliable, how else are we supposed to know which value is correct?</p><p>Some systems (for example <code>Dynamo</code>) try to solve this partially using <code>Version Vectors</code>, where you attach a (node, counter) pair for each version of an item, which gives you the ability to find causality between the different versions. By finding versions of values that are definitely newer (have a higher counter) you can remove some versions of a value, which makes the problem easier.</p><img src="https://tontinton.com/version_vector.svg"><blockquote><p>An example showing how easily conflicts arise. At the end we are left with {v2, v3} as the conflicting values for the same key. The reason I removed v1 is to show that by using something like <code>Version Vectors</code>, versions of values can be safely removed to minimize the amount of conflicts. To learn more on <code>Version Vectors</code> and their implementations, I recommend reading <a href="https://github.com/ricardobcl/Dotted-Version-Vectors">Dotted Version Vectors</a>.</p></blockquote><p>We could also decide to simply let the application decide how to deal with conflicts, by returning all conflicting values for the requested item. The application might know a lot more on the data than the database, so why not let it resolve conflicts? This is what <code>Riak KV</code> does for example.</p><blockquote><p>An idea I think about often is that you could even allow users to compile conflict resolution logic as a WASM module, and upload it to the database, so that when conflicts occur, the database resolves them, never relying on the application.</p></blockquote><p>There are lots of different ideas to reduce conflicts in an eventually consistent system, they usually fall under the umbrella term <code>Anti Entropy</code>.</p><h2 id="anti-entropy">Anti Entropy</h2><p>Here are examples of some of the most popular <code>Anti Entropy</code> mechanisms:</p><p><strong>Read Repair</strong> - After a client chooses the "latest" value from a read request that went to multiple nodes (by conflict resolution), it sends that value back to all the nodes that don't currently store that value, thus <em>repairing</em> them.</p><p><strong>Hinted Handoff</strong> - When a write request can't reach one of the target nodes, send it instead as a "hint" to some other node. As soon as that target node is available again, send it the saved "hint". On a quorum write, this mechanism is also called <code>Sloppy Quorum</code>, which provides even better <em>availability</em> for quorum requests.</p><p><strong>Merkle Trees</strong> - Because read repair only fixes queried data, a lot of data can still become inconsistent for a long time. Nodes can choose to start a synchronization process by talking to each other and see the differences in data. This is really expensive when there is a lot of data (<code>O(n)</code>). To make the sync algorithm faster (<code>O(log n)</code>) we can introduce <a href="https://en.wikipedia.org/wiki/Merkle_tree">merkle trees</a>. A merkle tree stores the hash of a range of the data in lowest leaf nodes, with the parent leaf nodes being a combined hash of the 2 of its children, thus creating a hierarchy of hashes up to the root of the tree. The sync process now starts by one node comparing the root of the merkle tree to another node's merkle tree, if the hashes are the same, it means they have exactly the same data. If the hashes differ, the leaf hashes are checked the same way, recursively until the inconsistent data is found.</p><p><strong>Gossip Dissemination</strong> - Send broadcast events to all nodes in the cluster in a simple and reliable way, by imitating how humans spread rumors or a disease. You send the event message to a configured number of randomly chosen nodes (called the "fanout"), then when they receive the message they repeat the process and send the message to another set of randomly chosen <code>N</code> nodes. To not repeat the message forever in the cluster, a node stops broadcasting a gossip message when it sees it a configured number of times. To get a feel for how data converges using gossip, head over to the <a href="https://www.serf.io/docs/internals/simulator.html">simulator</a>! As an optimization, gossip messages are usually sent using UDP, as the mechanism is just that reliable.</p><h2 id="conclusion">Conclusion</h2><p>There is a lot more to talk about databases, be it the use of <a href="https://yarchive.net/comp/linux/o_direct.html">O_DIRECT</a> in linux and implementing your own page cache, failure detection in distributed systems, consensus algorithms like <a href="https://raft.github.io/">raft</a>, distributed transactions, leader election, and an almost infinite amount more.</p><p>I hope I have piqued your curiosity enough to explore the world of databases further, or provided the tools for you to better understand which database to pick in your next project 😀</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scheme in Scheme on WASM in the browser (167 pts)]]></title>
            <link>https://spritely.institute/news/scheme-in-scheme-on-wasm-in-the-browser.html</link>
            <guid>38655047</guid>
            <pubDate>Fri, 15 Dec 2023 15:26:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spritely.institute/news/scheme-in-scheme-on-wasm-in-the-browser.html">https://spritely.institute/news/scheme-in-scheme-on-wasm-in-the-browser.html</a>, See on <a href="https://news.ycombinator.com/item?id=38655047">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://spritely.institute/static/images/blog/hoot-meta-repl.gif" alt="Hoot metacircular evaluator demo recording"></p><p>Hey, folks!  Today we want to talk about the wonderful
<a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">read-eval-print-loop
(REPL)</a>.
Thanks to WebAssembly (Wasm), it's becoming increasingly common for
programming language websites to embed a REPL in which passersby can
easily evaluate code and get a feel for the language without having to
install anything on their computer.  We'd like to do the same thing
for our language of choice, <a href="https://gnu.org/software/guile">Guile
Scheme</a>, using <a href="https://spritely.institute/hoot">Guile Hoot</a>.</p><p>Guile Hoot is a Scheme-to-Wasm compiler that leverages the <a href="https://developer.chrome.com/blog/wasmgc">Wasm
garbage collection (GC)
extension</a> that has been
rolling out to major browsers recently.  Wasm GC has finally made it
possible to use <a href="https://en.wikipedia.org/wiki/Dynamic_programming_language">dynamic
languages</a>
<em>besides JavaScript</em> on the client-side web, but it will still take
some additional effort to bring our favorite tools along for the ride.
In this post, we'll walk through building a tiny Scheme interpreter
with a simple user interface and explain what's in store for the
future.</p><p>To learn more about Hoot, check out the <a href="https://spritely.institute/news/guile-hoot-v020-released.html">0.2.0 release
announcement</a> from a couple of
weeks ago!</p><h3>The case of the missing REPL</h3><p>For Scheme programmers (and Lisp programmers in general), the REPL is
at the core of our development workflow.  We modify some code in our
text editor, evaluate it in the REPL, inspect the output or enter a
debugger, and repeat the process until the program behaves the way we
want.</p><p>We're used to having a native REPL process running directly on our
operating system, but since Hoot is putting Scheme on the web, we'd
really like a REPL available in the browser, too.  A web REPL would be
convenient for seasoned Schemers and newcomers alike.  However, at
this early stage in Hoot's development, a fully-featured Scheme REPL
is not yet possible.  We don't have access to the macro expander,
interpreter, etc. from the comfort of our Wasm runtime... yet.</p><p>We'll have these things eventually, but what can we have now?</p><p>Well, we could implement an interpreter for a small subset of
Scheme... in Scheme!  Sounds pretty recursive, and if you know
anything about Schemers you know we love recursion (and also, that we
love recursion), so let's give it a shot!</p><h3>Scheme in Scheme</h3><p>In Chapter 4 of the classic computer science textbook <a href="https://mitp-content-server.mit.edu/books/content/sectbyfn/books_pres_0/6515/sicp.zip/full-text/book/book-Z-H-25.html#%_chap_4">Structure and
Interpretation of Computer
Programs</a>
(SICP), Gerald Sussman and Hal Abelson walk through building a small
Scheme interpreter embedded in Scheme.  They called this the
"metacircular evaluator", meaning that it's an interpreter written in
the language that it is interpreting.</p><p>This sounds pretty fancy — the kind of task reserved for MIT
professors that write iconic textbooks — but implementing a Scheme
interpreter is simpler than one might think!  For one thing, since
Scheme can manipulate its own syntax by design, we don't need to spend
any time parsing, a notoriously complex subject in its own right.  We
can skip directly to the fun part instead: evaluation!</p><p>Spritely's own <a href="https://spritely.institute/static/papers/scheme-primer.html">Scheme
Primer</a>
distills SICP's interpreter into a much simpler form intended for
people who are brand new to Scheme.  The code examples in the rest of
this post are derived from the "Scheme in Scheme" section of the
Primer.  If you're completely new to Scheme and want to better
understand the code in this post, consider working through our Primer
sometime!</p><p><img src="https://spritely.institute/static/images/blog/eval-apply.png" alt="eval/apply diagram from Andres Raba's unofficial SICP, licensedunder Creative Commons Attribution-ShareAlike 4.0 InternationalLicense"></p><p>The metacircular evaluator is composed of two main components: <code>eval</code>
and <code>apply</code>:</p><ul><li><code>eval</code> processes an expression in the context of an environment.</li><li><code>apply</code> calls a procedure (function) with a list of arguments.</li></ul><p>The output of <code>eval</code> is fed to <code>apply</code>, which may feed more input to
<code>eval</code>, and this mutually recursive cycle continues until the program
runs out of expressions to evaluate.</p><p>In order to implement <code>eval</code>, we first need a way to represent
environments.  An environment maps variable names to their bound
values.  We'll use <a href="https://en.wikipedia.org/wiki/Association_list">association
lists</a> for our
environments.</p><p>An empty environment with no variable bindings is represented by the
empty list <code>'()</code>.</p><p>The first bit of code from the metacircular evaluator we'll cover is
the <code>extend-env</code> procedure, which creates a new environment by
extending an existing one:</p><pre><code><span>(</span><span>define</span> <span>(</span><span>extend-env</span> <span>env</span> <span>names</span> <span>vals</span><span>)</span>
  <span>;; If there are no more variables to add to env, return env.
</span>  <span>(</span><span>if</span> <span>(</span><span>null?</span> <span>names</span><span>)</span>
      <span>env</span>
      <span>;; Otherwise, add the first binding to the recursive
</span>      <span>;; extension of env with the rest of the bindings.
</span>      <span>(</span><span>cons</span> <span>(</span><span>cons</span> <span>(</span><span>car</span> <span>names</span><span>)</span> <span>(</span><span>car</span> <span>vals</span><span>)</span><span>)</span>
            <span>(</span><span>extend-env</span> <span>env</span> <span>(</span><span>cdr</span> <span>names</span><span>)</span> <span>(</span><span>cdr</span> <span>vals</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>We can extend the empty environment to make a new environment that
includes the variable <code>foo</code>:</p><pre><code><span>(</span><span>extend-env</span> <span>'</span><span>(</span><span>)</span> <span>'</span><span>(</span><span>foo</span><span>)</span> <span>'</span><span>(</span><span>1</span><span>)</span><span>)</span> <span>; =&gt; ((foo . 1))</span></code></pre><p>We can further extend that environment to make a new environment that
includes the variable <code>bar</code>:</p><pre><code><span>(</span><span>extend-env</span> <span>'</span><span>(</span><span>(</span><span>foo</span> <span>.</span> <span>1</span><span>)</span><span>)</span> <span>'</span><span>(</span><span>bar</span><span>)</span> <span>'</span><span>(</span><span>2</span><span>)</span><span>)</span> <span>; =&gt; ((bar . 2) (foo . 1))</span></code></pre><p>Note that environment manipulation is implemented in a
<a href="https://en.wikipedia.org/wiki/Functional_programming">functional</a>
style.  Extending an environment does not mutate the original one.
This is because mutating the original environment would propagate the
new bindings to other evaluation contexts, potentially breaking
Scheme's <a href="https://en.wikipedia.org/wiki/Scope_(computer_science)">lexical scoping
rules</a>.</p><p>We can look up a variable's value using the <code>env-lookup</code> procedure:</p><pre><code><span>(</span><span>define</span> <span>(</span><span>env-lookup</span> <span>env</span> <span>name</span><span>)</span>
  <span>;; Lookup name in association list.
</span>  <span>(</span><span>match</span> <span>(</span><span>assq</span> <span>name</span> <span>env</span><span>)</span>
    <span>;; Success: return the bound value.
</span>    <span>(</span><span>(</span><span>_</span> <span>.</span> <span>val</span><span>)</span> <span>val</span><span>)</span>
    <span>;; Failure: throw an error.
</span>    <span>(</span><span>#f</span> <span>(</span><span>error</span> <span>"Variable unbound:"</span> <span>name</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>Example:</p><pre><code><span>(</span><span>env-lookup</span> <span>'</span><span>(</span><span>(</span><span>foo</span> <span>.</span> <span>1</span><span>)</span><span>)</span> <span>'foo</span><span>)</span> <span>; =&gt; 1</span></code></pre><p>And now for the rest of the owl.  Our toy <code>eval</code> supports:</p><ul><li>booleans</li><li>numbers</li><li>strings</li><li>quoted expressions</li><li>conditionals</li><li>procedures as values</li><li>procedure calls</li></ul><pre><code><span>(</span><span>define</span> <span>(</span><span>eval</span> <span>expr</span> <span>env</span><span>)</span>
  <span>(</span><span>match</span> <span>expr</span>
    <span>;; Booleans, numbers, strings
</span>    <span>(</span><span>(</span><span>or</span> <span>(</span><span>?</span> <span>boolean?</span><span>)</span> <span>(</span><span>?</span> <span>number?</span><span>)</span> <span>(</span><span>?</span> <span>string?</span><span>)</span><span>)</span>
     <span>expr</span><span>)</span>
    <span>;; Quoted expressions
</span>    <span>(</span><span>(</span><span>'quote</span> <span>quoted-expr</span><span>)</span>
     <span>quoted-expr</span><span>)</span>
    <span>;; Variable lookup
</span>    <span>(</span><span>(</span><span>?</span> <span>symbol?</span> <span>name</span><span>)</span>
     <span>(</span><span>env-lookup</span> <span>env</span> <span>name</span><span>)</span><span>)</span>
    <span>;; Conditionals
</span>    <span>(</span><span>(</span><span>'if</span> <span>test</span> <span>consequent</span> <span>alternate</span><span>)</span>
     <span>(</span><span>if</span> <span>(</span><span>eval</span> <span>test</span> <span>env</span><span>)</span>
         <span>(</span><span>eval</span> <span>consequent</span> <span>env</span><span>)</span>
         <span>(</span><span>eval</span> <span>alternate</span> <span>env</span><span>)</span><span>)</span><span>)</span>
    <span>;; Procedures
</span>    <span>(</span><span>(</span><span>'lambda</span> <span>args</span> <span>body</span><span>)</span>
     <span>(</span><span>lambda</span> <span>vals</span>
       <span>(</span><span>eval</span> <span>body</span> <span>(</span><span>extend-env</span> <span>env</span> <span>args</span> <span>vals</span><span>)</span><span>)</span><span>)</span><span>)</span>
    <span>;; Procedure application
</span>    <span>(</span><span>(</span><span>proc-expr</span> <span>.</span> <span>arg-exprs</span><span>)</span>
     <span>;; Recursively evaluate the procedure expression and all
</span>     <span>;; argument expressions, then apply the procedure with the
</span>     <span>;; arguments.
</span>     <span>(</span><span>apply</span> <span>(</span><span>eval</span> <span>proc-expr</span> <span>env</span><span>)</span>
            <span>;; Recursively evaluate the arguments.
</span>            <span>(</span><span>map</span> <span>(</span><span>lambda</span> <span>(</span><span>arg-expr</span><span>)</span>
                   <span>(</span><span>eval</span> <span>arg-expr</span> <span>env</span><span>)</span><span>)</span>
                 <span>arg-exprs</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>Fortunately, we don't need to implement <code>apply</code> ourselves.  Since
<code>lambda</code> forms in our interpreter evaluate to regular Scheme
procedures, we can just use Scheme's built-in <code>apply</code> to pass
arguments to them.  Calling a procedure will recursively call <code>eval</code>
on the procedure's body expression.</p><p>Example:</p><pre><code><span>(</span><span>eval</span> <span>'</span><span>(</span><span>+</span> <span>1</span> <span>2</span> <span>3</span><span>)</span> <span>`</span><span>(</span><span>(</span><span>+</span> <span>.</span> <span>,+</span><span>)</span><span>)</span><span>)</span> <span>; =&gt; 6</span></code></pre><p>Incidentally, we've created a
<a href="https://en.wikipedia.org/wiki/Capability-based_security">capability-secure</a>
system reminiscent of <a href="https://webassembly.github.io/spec/core/intro/introduction.html#security-considerations">Wasm's security
model</a>!
The program <code>(+ 1 2 3)</code> only has access to the <code>+</code> procedure from our
Scheme runtime.  Aside from creating unbounded loops, a devious
developer can't do any harm if their code is evaluated in this
restricted environment.  Neat!</p><h3>Scheme in Scheme on Wasm</h3><p>Now that we have a simple evaluator, we can compile it with Hoot to
run it inside a web browser.  But to actually use it, we need a user
interface.  To make one, we can borrow the React-like rendering code
we walked through in our previous tutorial on rendering web pages with
Wasm, <a href="https://spritely.institute/news/building-interactive-web-pages-with-guile-hoot.html">"Building interactive web pages with Guile
Hoot"</a>!</p><p>For simplicity, we'll go for a minimalist design reminiscent of a
terminal.  As for the REPL output log, we can store it as a list of
strings.  Let's pre-populate it with a friendly welcome message:</p><pre><code><span>(</span><span>define</span> <span>*log*</span>
  <span>'</span><span>(</span><span>"Welcome to the Hoot metacircular evaluator demo!

This is a miniature Scheme interpreter written in Scheme that's been
compiled to WebAssembly, and is running directly in the browser!  Its
UI is also written in Scheme, and uses the Hoot FFI to render itself
to the DOM.

"</span><span>)</span><span>)</span>

<span>(</span><span>define</span> <span>(</span><span>log-append!</span> <span>.</span> <span>lines</span><span>)</span>
  <span>(</span><span>set!</span> <span>*log*</span> <span>(</span><span>append</span> <span>*log*</span> <span>lines</span><span>)</span><span>)</span><span>)</span></code></pre><p>Here's the rendering template, which prints all the log lines, and
appends the classic <code>&gt;</code> prompt and a <code>textarea</code> to the bottom so the
user can write some Scheme code:</p><pre><code><span>(</span><span>define</span> <span>prompt</span> <span>"&gt; "</span><span>)</span>

<span>(</span><span>define</span> <span>(</span><span>render</span><span>)</span>
  <span>`</span><span>(</span><span>div</span> <span>(</span><span>@</span> <span>(</span><span>class</span> <span>"container"</span><span>)</span><span>)</span>
        <span>(</span><span>h1</span> <span>"Hoot REPL"</span><span>)</span>
        <span>(</span><span>div</span> <span>(</span><span>@</span> <span>(</span><span>id</span> <span>"repl"</span><span>)</span>
                <span>(</span><span>class</span> <span>"repl repl-text"</span><span>)</span><span>)</span>
             <span>(</span><span>div</span> <span>(</span><span>@</span> <span>(</span><span>class</span> <span>"log"</span><span>)</span><span>)</span> <span>,@*log*</span><span>)</span>
             <span>(</span><span>div</span> <span>(</span><span>@</span> <span>(</span><span>class</span> <span>"prompt"</span><span>)</span><span>)</span>
                  <span>,prompt</span>
                  <span>(</span><span>textarea</span> <span>(</span><span>@</span> <span>(</span><span>id</span> <span>"expression"</span><span>)</span>
                               <span>(</span><span>class</span> <span>"repl-text"</span><span>)</span>
                               <span>(</span><span>placeholder</span> <span>"(+ 1 2 3)"</span><span>)</span>
                               <span>(</span><span>keyup</span> <span>,maybe-eval</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>Every time the user presses a key while focused on the <code>textarea</code>, we
call the <code>maybe-eval</code> procedure shown below.  When the user presses
the Enter key, the REPL</p><ul><li>evaluates their code</li><li>clears the <code>textarea</code></li><li>appends any new output</li><li>scrolls to the bottom of the log to keep the terminal screen up to
date</li></ul><pre><code><span>(</span><span>define</span> <span>(</span><span>maybe-eval</span> <span>event</span><span>)</span>
  <span>;; Get the event's key.
</span>  <span>(</span><span>let</span> <span>(</span><span>(</span><span>key</span> <span>(</span><span>keyboard-event-key</span> <span>event</span><span>)</span><span>)</span><span>)</span>
    <span>;; Evaluate user code when Enter is pressed, but not when
</span>    <span>;; Shift is being held so the user can edit across multiple
</span>    <span>;; lines.
</span>    <span>(</span><span>when</span> <span>(</span><span>and</span> <span>(</span><span>string-=?</span> <span>key</span> <span>"Enter"</span><span>)</span>
               <span>(</span><span>not</span> <span>(</span><span>keyboard-event-shift?</span> <span>event</span><span>)</span><span>)</span><span>)</span>
      <span>;; Get the text within the expression textarea.
</span>      <span>(</span><span>let*</span> <span>(</span><span>(</span><span>input</span> <span>(</span><span>get-element-by-id</span> <span>"expression"</span><span>)</span><span>)</span>
             <span>(</span><span>exp</span> <span>(</span><span>element-value</span> <span>input</span><span>)</span><span>)</span><span>)</span>
        <span>;; If the textarea is empty, do nothing.
</span>        <span>(</span><span>unless</span> <span>(</span><span>string-=?</span> <span>exp</span> <span>""</span><span>)</span>
          <span>;; Clear the textarea.
</span>          <span>(</span><span>set-element-value!</span> <span>input</span> <span>""</span><span>)</span>
          <span>;; Evaluate and append output to log.
</span>          <span>(</span><span>eval!</span> <span>exp</span><span>)</span>
          <span>;; Update UI.
</span>          <span>(</span><span>refresh!</span><span>)</span>
          <span>;; Scroll the log to show the next output.
</span>          <span>(</span><span>scroll-to-bottom!</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>At the bottom, you'll notice <code>maybe-eval</code> uses a new procedure we've
called <code>eval!</code> (how exciting).  <code>eval!</code> is <code>eval</code> + <code>read</code> — it parses
the user's text using Scheme's built-in <code>read</code> procedure, calls
<code>eval</code>, and prints the output to the log:</p><pre><code><span>(</span><span>define</span> <span>(</span><span>eval!</span> <span>str</span><span>)</span>
  <span>;; Parse user input.
</span>  <span>(</span><span>let</span> <span>(</span><span>(</span><span>exp</span> <span>(</span><span>read</span> <span>(</span><span>open-input-string</span> <span>str</span><span>)</span><span>)</span><span>)</span>
        <span>;; Open output port.
</span>        <span>(</span><span>output</span> <span>(</span><span>open-output-string</span><span>)</span><span>)</span><span>)</span>
    <span>;; Redirect all output to our output port.
</span>    <span>(</span><span>parameterize</span> <span>(</span><span>(</span><span>current-output-port</span> <span>output</span><span>)</span><span>)</span>
      <span>;; Echo the prompt and user code.
</span>      <span>(</span><span>display</span> <span>prompt</span><span>)</span>
      <span>(</span><span>display</span> <span>str</span><span>)</span>
      <span>;; Invoke the interpreter.
</span>      <span>(</span><span>call-with-values</span> <span>(</span><span>lambda</span> <span>(</span><span>)</span> <span>(</span><span>eval</span> <span>exp</span> <span>init-env</span><span>)</span><span>)</span>
        <span>;; Display each returned value on its own line.
</span>        <span>(</span><span>lambda</span> <span>vals</span>
          <span>(</span><span>if</span> <span>(</span><span>null?</span> <span>vals</span><span>)</span>
              <span>(</span><span>display</span> <span>"\n"</span><span>)</span>
              <span>(</span><span>for-each</span> <span>(</span><span>lambda</span> <span>(</span><span>val</span><span>)</span>
                          <span>(</span><span>unless</span> <span>(</span><span>unspecified?</span> <span>val</span><span>)</span>
                            <span>(</span><span>display</span> <span>"=&gt; "</span><span>)</span>
                            <span>(</span><span>write</span> <span>val</span><span>)</span><span>)</span>
                          <span>(</span><span>newline</span><span>)</span><span>)</span>
                        <span>vals</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span>
    <span>;; Append output to log.
</span>    <span>(</span><span>log-append!</span> <span>(</span><span>get-output-string</span> <span>output</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>(Note how, thanks to <code>read</code>, we've elided the messy work of parsing.)</p><p>Expressions are evaluated within the context of the following
environment that provides capabilities for basic arithmetic, lists,
multiple value return, and printing:</p><pre><code><span>(</span><span>define</span> <span>init-env</span>
  <span>`</span><span>(</span><span>(</span><span>+</span> <span>.</span> <span>,+</span><span>)</span>
    <span>(</span><span>-</span> <span>.</span> <span>,-</span><span>)</span>
    <span>(</span><span>*</span> <span>.</span> <span>,*</span><span>)</span>
    <span>(</span><span>/</span> <span>.</span> <span>,/</span><span>)</span>
    <span>(</span><span>=</span> <span>.</span> <span>,=</span><span>)</span>
    <span>(</span><span>cons</span> <span>.</span> <span>,cons</span><span>)</span>
    <span>(</span><span>car</span> <span>.</span> <span>,car</span><span>)</span>
    <span>(</span><span>cdr</span> <span>.</span> <span>,cdr</span><span>)</span>
    <span>(</span><span>list</span> <span>.</span> <span>,list</span><span>)</span>
    <span>(</span><span>pair?</span> <span>.</span> <span>,pair?</span><span>)</span>
    <span>(</span><span>null?</span> <span>.</span> <span>,null?</span><span>)</span>
    <span>(</span><span>values</span> <span>.</span> <span>,values</span><span>)</span>
    <span>(</span><span>display</span> <span>.</span> <span>,display</span><span>)</span>
    <span>(</span><span>newline</span> <span>.</span> <span>,newline</span><span>)</span><span>)</span><span>)</span></code></pre><p>The initial environment describes the primitive functionality of our
interpreter.  Primitives are simple things that the programmer can
take for granted.  This environment doesn't provide much, but it's
enough to have a bit of fun.</p><p>And...</p><p>🥁</p><p>🥁</p><p>🥁</p><p>...here's the finished program!</p><p>Some expressions you could try:</p><p>Greet the world:</p><pre><code><span>(</span><span>display</span> <span>"Hello, world!"</span><span>)</span></code></pre><p>Make a list of the veggies you want on your sandwich:</p><pre><code><span>'</span><span>(</span><span>lettuce</span> <span>tomato</span> <span>onion</span> <span>pepper</span> <span>pickles</span><span>)</span></code></pre><p>Display all your favorite pets as multiple return values:</p><pre><code><span>(</span><span>values</span> <span>'cat</span> <span>'dog</span> <span>'chicken</span><span>)</span></code></pre><p>Apply a procedure that squares numbers:</p><pre><code><span>(</span><span>(</span><span>lambda</span> <span>(</span><span>x</span><span>)</span> <span>(</span><span>*</span> <span>x</span> <span>x</span><span>)</span><span>)</span> <span>4</span><span>)</span></code></pre><p>Recursively apply a procedure to compute the nth Fibonacci number:</p><pre><code><span>(</span><span>(</span><span>lambda</span> <span>(</span><span>f</span> <span>x</span><span>)</span> <span>(</span><span>f</span> <span>f</span> <span>x</span><span>)</span><span>)</span>
 <span>(</span><span>lambda</span> <span>(</span><span>fib</span> <span>n</span><span>)</span>
   <span>(</span><span>if</span> <span>(</span><span>=</span> <span>n</span> <span>0</span><span>)</span>
       <span>0</span>
       <span>(</span><span>if</span> <span>(</span><span>=</span> <span>n</span> <span>1</span><span>)</span>
           <span>1</span>
           <span>(</span><span>+</span> <span>(</span><span>fib</span> <span>fib</span> <span>(</span><span>-</span> <span>n</span> <span>1</span><span>)</span><span>)</span>
              <span>(</span><span>fib</span> <span>fib</span> <span>(</span><span>-</span> <span>n</span> <span>2</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span>
 <span>10</span><span>)</span></code></pre><p>Whoa, that last one was kinda wild, huh?  That's because our little
interpreter lacks even the ability to bind variables outside of
<code>lambda</code>.  There is no <code>let</code> nor <code>define</code>, so recursive code gets
weird fast!  Talk about minimalism...</p><h3>Scheming ahead</h3><p>The interpreter we've shown here is just a toy, but in the coming
months we expect to add support for the <a href="https://small.r7rs.org/attachment/r7rs.pdf">R7RS-small
standard</a>'s <code>eval</code>, taking
our REPL from toy to full Scheme interpreter.</p><p>Once we do so, we could embed Scheme REPLs directly into documents
like the <em>Scheme Primer</em>, further reducing the activation energy
required to get started with Scheme.  And who knows what exciting
integrations our community will come up with?</p><p>In the meantime, if you'd like to start immediately hacking on the
REPL demo, you can find the complete source code on
<a href="https://gitlab.com/spritely/guile-hoot-meta-repl">GitLab</a>.  If you're
into homework assignments, try adding support for more Scheme syntax
such as <code>let</code>, or adding more procedures to the environment to grant
more power to the interpreter.</p><p>And be sure to show off what you build with Hoot in our <a href="https://community.spritely.institute/">community
forum</a>!  (Use OCAPN2023 for the
invite code when you join.)</p><p>Happy hooting! 🦉</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cognitive distortions that undermine clear thinking (105 pts)]]></title>
            <link>https://www.leadingsapiens.com/cognitive-distortions-leaders/</link>
            <guid>38655010</guid>
            <pubDate>Fri, 15 Dec 2023 15:22:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.leadingsapiens.com/cognitive-distortions-leaders/">https://www.leadingsapiens.com/cognitive-distortions-leaders/</a>, See on <a href="https://news.ycombinator.com/item?id=38655010">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <div>

      <p><em>Leaders are paid to think clearly. And a big impediment to clear thinking is what CBT/REBT calls <strong>Cognitive Distortions: errors in thinking or thinking traps we commonly fall for</strong>. Most high-performers are not pathological, but instead fall for them when upset or in high-stress situations.</em></p><p><em> In this post, I share <strong>17 of the most common cognitive distortions</strong>. I highlight the <strong>use-cases of leaders and job-hunters</strong>, what causes them, and strategies for countering them.</em></p><hr><h2 id="what-are-cognitive-distortions">What are cognitive distortions?</h2><p>Also known as thinking errors or thought traps, cognitive distortions are defined as "<em>errors of processing in which the person cognitively focuses on insufficient or inappropriate data and draws illogical conclusions, makes inaccurate inferences or bases predicted outcomes upon little or no empirical evidence</em>." [3]</p><p>Put simply, these are habitual but unhelpful ways of thinking. Why are they important? </p><p>Because the way we feel, and how well we operate, are directly dependent on the quality of our thinking. It's not external events or others that make us feel a certain way, but instead our thoughts about them. But this process is so fast and automatic that <strong>we forget our own role in creating these problems</strong>.  </p><blockquote>Your emotions result entirely from the way you look at things. It is an obvious neurological fact that before you can experience any event, you must process it with your mind and give it meaning. You must understand what is happening to you before you can feel it. <br>...<br>It is not the actual events but your perceptions that result in changes in mood. When you are sad, your thoughts will represent a realistic interpretation of negative events. When you are depressed or anxious, your thoughts will always be illogical, distorted, unrealistic, or just plain wrong.<p>— David Burns [1]</p></blockquote><h3 id="are-cognitive-distortions-applicable-to-leadership-coaching"><strong>Are cognitive distortions applicable to leadership coaching</strong></h3><p>In leadership, staying alert to cognitive distortions is important from two key perspectives: the leader's performance, and more importantly, how it affects others.</p><p>Elite athletes have to train constantly to maintain peak physical and mental conditioning. Their training routines focus on maintaining correct form. The equivalent of good form in leadership is clear thinking. Most high-functioning folks are not necessarily pathological, but nevertheless fall for these subtle but hidden patterns. </p><p>Additionally, leadership behavior has ripple-effects throughout the organization. <a href="https://www.leadingsapiens.com/why-people-copy-leaders-complexity-view/" rel="noreferrer">People not only copy leaders</a>, but also seek answers from them, both direct and implied. Whether by design or accident, <a href="https://www.leadingsapiens.com/mastery-of-context-in-leadership/" rel="noreferrer">leaders are always setting the context</a> for their teams. It's why they simply cannot afford to get derailed by thought traps.</p><div><p>💡</p><p><b><strong>A caveat: </strong></b>When going through this list and the examples, it's easy to dismiss them as simplistic and extreme. And they mostly are. However, most of us fall for them in subtle ways that often go undetected. </p></div><h2 id="common-cognitive-distortions">Common cognitive distortions</h2><p>Just as exercise improves physical strength, there are practices to improve mental conditioning — aka clear thinking. One such practice is to learn, identify, and interrupt common but unhelpful thinking patterns that we fall for.</p><p>Coaching borrows from a range of disciplines including cognitive behavioral methods. What follows are the most common cognitive distortions that CBT/REBT recognizes.</p><p>Each cognitive distortion is followed by examples from the perspective of a job-hunter and a leader. Why? Thinking errors are most likely when operating in high-stress situations which are rich territory for thought traps.</p><figure><img src="https://www.leadingsapiens.com/content/images/2023/12/Cognitive-Distortions--lr-2.png" alt="17 Cognitive Distortions from CBT and REBT" loading="lazy" width="1080" height="1950" srcset="https://www.leadingsapiens.com/content/images/size/w600/2023/12/Cognitive-Distortions--lr-2.png 600w, https://www.leadingsapiens.com/content/images/size/w1000/2023/12/Cognitive-Distortions--lr-2.png 1000w, https://www.leadingsapiens.com/content/images/2023/12/Cognitive-Distortions--lr-2.png 1080w" sizes="(min-width: 720px) 720px"></figure><h3 id="1-all-or-nothing-thinking">(1) All-or-Nothing Thinking</h3><p><strong>Definition</strong><br>We see the world in dichotomies — it's either black or white with no shades of grey in between. Anything short of perfect is a failure. Characterized by thinking in polarities, extremes, and absolutes. It's either excellent or awful, and forms the basis of perfectionism.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"If my team doesn't achieve 100% of the goals, it's a complete disaster."</em><br>⦿ Job-Hunter:&nbsp;<em>"If I don't get this job, I'll never find a good career opportunity."</em></p><p><strong>How to counter</strong><br>Cultivate multi-category thinking and a richer understanding to see the shades of grey instead of falling for simplistic explanations. Learn to discern between facts vs hunches or interpretations. Resolve to examine your negative interpretations by testing them out. </p><h3 id="2-overgeneralization">(2) Overgeneralization</h3><p><strong>Definition</strong><br>Drawing broad conclusions based on isolated negative events. Characterized by drawing global beliefs and never-ending patterns from a single situation.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"My proposal got rejected ; I'm terrible at pitching ideas."</em><br>⦿ Job-Hunter:&nbsp;<em>"I failed this interview; I'll never get hired anywhere."</em></p><p><strong>How to counter</strong><br>Avoid extrapolation of both good and bad. And when doing so, check if the data actually supports your stance.</p><h3 id="3-mental-filtering">(3) Mental filtering</h3><p><strong>Definition</strong><br>Picking out and focusing exclusively on a single negative aspect while ignoring any positives.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"Despite other successful projects, I can't stop thinking about that one recent failure."&nbsp;</em><br>⦿ Job-Hunter:&nbsp;<em>"I had an excellent interview, but I stumbled on one question, and now I feel like I ruined it all."</em></p><p><strong>How to counter</strong><br>Recognize the complexity of situations and acknowledge positive aspects as well that balance out the negative.</p><h3 id="4-disqualifying-the-positive">(4) Disqualifying the positive</h3><p><strong>Definition</strong><br>Rejecting positive experiences as if they don't count for one reason or another. A simple example is dismissing compliments.<strong> </strong>Success is seen as a fluke. This make it difficult to build on anything because everything is happenstance.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"The team's success was due to external factors, not my leadership."</em><br>⦿ Job-Hunter:&nbsp;<em>"The interviewer praised my skills, but I think they were just being polite."</em></p><p><strong>How to counter</strong><br>Learn to accept and acknowledge positive feedback and explanations, in addition to negative and neutral ones.</p><h3 id="5-jumping-to-conclusions">(5) Jumping to Conclusions</h3><p><strong>Definition</strong><br>Assuming negative outcomes, or negative interpretation without facts or supporting evidence. This is characterized by certainty in interpreting situations despite little evidence.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"He's late today; he must not care about work."</em><br>⦿ Job-Hunter:&nbsp;<em>"The interviewer looked uninterested; they probably think I'm unqualified."</em></p><p><strong>How to counter</strong><br>Notice your inferences and how far removed they might be from observable facts. Learn to <a href="https://www.leadingsapiens.com/ground-rules-effective-meetings-groups/#1-test-assumptions-inferences-and-attributions" rel="noreferrer">test your inferences and hunches</a>.</p><h3 id="6-magnification-catastrophizing-or-minimization">(6) Magnification (Catastrophizing) Or Minimization</h3><p><strong>Definition</strong><br>Exaggerating or downplaying the importance of events. Also known as <strong>the&nbsp;"binocular trick"</strong>.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"Missing this deadline will ruin my career."</em><br>⦿ Job-Hunter:<em>"If I don't get this job, I'll never find any job, and my life will be ruined."</em></p><p><strong>How to counter</strong><br>Use a balanced perspective instead of a skewed one that catastrophes or  minimizes. How would an outside observer view your situation? What are your strengths?</p><h3 id="7-emotional-reasoning">(7) Emotional Reasoning</h3><p><strong>Definition</strong><br>Believing feelings and negative emotions accurately define or reflect reality. The logic goes: "I feel it, so it must be true."</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I feel like a failure, so I must be incompetent."</em><br>⦿ Job-Hunter:&nbsp;<em>"I'm nervous about the interview, which means I'll perform poorly."</em></p><p><strong>How to counter</strong><br>Get separation between facts and how you feel about something. Discern between emotionally laden assessments vs. factual takes. </p><h3 id="8-should-statements">(8) Should Statements</h3><p><strong>Definition</strong><br>Trying to motivate yourself by holding rigid or unrealistic expectations on oneself or others. Characterized by should(s), shouldn't(s), musts, oughts and "have to's",  as if one has to be punished and whipped before correct behavior. Albert Ellis (REBT founder) called this&nbsp;MUSTerbation.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I should always have the solution to every problem."</em><br>⦿ Job-Hunter:&nbsp;<em>"Employers should always provide feedback after interviews."</em></p><p><strong>How to counter</strong><br>Stay alert to your use of language and how that influences your interpretations, choice of actions, and behavior.</p><h3 id="9-labeling-and-mislabeling">(9) Labeling and Mislabeling</h3><p><strong>Definition</strong><br>This is an extreme form of overgeneralization, where we assign global, negative labels to oneself or others. Instead of describing an error, we attach negative labels: "I'm a loser", or: "what a louse". It's often characterized by highly colored and emotionally laden language. Over time, this can also lead to a <strong>lack of separation between the label and </strong><a href="https://www.leadingsapiens.com/leadership-identity-development-choices-actions/#identity-as-an-effect-rather-than-a-cause" rel="noreferrer"><strong>identity</strong></a>. Eg. "I'm a nervous nelly". </p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I'm a total failure as a manager."&nbsp;</em><br>⦿ Job-Hunter:&nbsp;<em>"I'm a slouch when it comes to interviews."</em></p><p><strong>How to counter</strong><br>Avoid using labels or rating yourself/others globally. Almost no-one is a "total idiot" or a "complete failure". Question the validity of your labels and <a href="https://www.leadingsapiens.com/effective-constructive-feedback/#principle-6-sharing-emotions-owning-opinions-and-avoiding-generalities" rel="noreferrer">separate the behavior from the person</a>.</p><h3 id="10-personalization">(10) Personalization</h3><p><strong>Definition</strong><br>Also known as self-blame: you assume undue responsibility for negative external events for which you were not responsible. This is an easy trap for conscientious leaders – <strong>they've confused influence with control</strong>. </p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"The project's failure is entirely my fault."&nbsp;</em><br>⦿ Job-Hunter:&nbsp;<em>"I didn't get the job; I must have said something wrong during the interview."</em></p><p><strong>How to counter</strong><br>Try to understand the whole picture instead of one particular aspect that you might have contributed to. Look for alternative explanations that might highlight <a href="https://www.leadingsapiens.com/linear-causality-vs-circular-causality-in-decision-making/" rel="noreferrer">different causal factors</a>.</p><h3 id="11-mind-reading">(11) Mind-reading</h3><p><strong>Definition</strong><br>Believing you know what others are thinking about you without evidence. It also means assuming that others know what you are thinking. It's characterized by a lack of direct communication (they should know), and expecting others to read your body language.</p><p><strong>Examples</strong><br>⦿ Leader:<em>&nbsp;"My team thinks I'm incompetent."&nbsp;</em><br>⦿ Job-Hunter:&nbsp;<em>"The interviewer didn't like my answer."</em></p><p><strong>How to counter</strong><br>Check your interpretations of others' reactions instead of assuming it's obvious. Look for alternative explanations that are more generous.  </p><h3 id="12-fortune-teller-error">(12) Fortune-teller error</h3><p><strong>Definition</strong><br>Predicting future events, usually negative, without evidence. And acting as if your prediction is a fact. Often characterized by anticipation of bad events and assuming outcomes as foregone conclusions. There's a false sense of hopelessness and defeatism.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I know the client will reject our proposal." </em><br>⦿ Job-Hunter:&nbsp;<em>"I'm certain they won't hire me, so why apply."</em></p><p><strong>How to counter</strong><br>Discern between what's in your control and what's not. Avoid making predictions that are not actually in your control. <strong>Let the decision-makers do their job while you do yours</strong>.</p><h3 id="13-alwaysnever-thinking">(13) Always/Never Thinking</h3><p><strong>Definition</strong><br>Believing that negative events will always happen, or that good events will never happen.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"My team always makes mistakes." </em><br>⦿ Job-Hunter:<em>&nbsp;"I'll never get hired."</em></p><p><strong>How to counter</strong><br>Take a balanced view of the past, present, and future. Recognize that while bad events are always possible, they're not inevitable and can be mitigated. Meanwhile, positive events are equally likely.</p><h3 id="14-entitlement">(14) Entitlement</h3><p><strong>Definition</strong><br>Believing one deserves special treatment or privileges, and characterized by unrealistic expectations and demanding behavior based on status.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I should be promoted without having to prove myself."</em><br>⦿ Job-Hunter:<em>&nbsp;"The employer should hire me because of my qualifications."</em></p><p><strong>How to counter</strong><br>Watch out for status expectations and wanting to skirt effort in getting to your goals.</p><h3 id="15-outsourcing-happiness">(15) <strong>Outsourcing Happiness</strong></h3><p><strong>Definition</strong><br>Emotional dependence and reliance on external circumstances for happiness instead of internal control. Characterized by the logic: "I'll be happy when/if..."</p><p><strong>Examples</strong><br>⦿ Leader:<em>&nbsp;"I'll be happy when my team performs well."</em><br>⦿ Job-Hunter:&nbsp;<em>"Getting this job will make me happy."</em></p><p><strong>How to counter</strong><br>Watch out for when you make your happiness contingent on something outside your control.</p><h3 id="16-control-fallacy">(16) <strong>Control Fallacy</strong></h3><p><strong>Definition</strong><br>Believing external forces have complete control over one's life (helplessness). Or the opposite of everything being in our control.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I'm powerless against the company's decisions."</em><br>⦿ Job-Hunter:&nbsp;<em>"The job market controls my job prospects."</em></p><p><strong>How to counter</strong><br>Most of the time, <a href="https://www.leadingsapiens.com/ask-before-setting-goals/#the-nature-of-choice" rel="noreferrer">we have more choice than we realize</a>. But often this choice is hidden behind consequences that we might not be ok with.</p><h3 id="17-fairness-fallacy">(17) <strong>Fairness Fallacy</strong></h3><p><strong>Definition</strong><br>Believing that life should always be fair and just. Characterized by resentment and frustration.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"It's not fair that my colleague got promoted, and I didn't."</em><br>⦿ Job-Hunter:&nbsp;<em>"I deserved that job more than the other candidates."</em></p><p><strong>How to counter</strong><br>Culture programs us to expect fairness, but the world is far from being fair, let alone predictable. Don't allow expectations of fairness to scuttle your level of commitment and effort.</p>
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<h2 id="what-causes-cognitive-distortions"><strong>What causes Cognitive Distortions?</strong></h2><p>While recognizing and countering cognitive distortions is useful, it's also valuable to go further upstream and understand what causes them to begin with. REBT has two constructs that explain what causes distortions: levels of cognitions and irrational beliefs. </p><h3 id="a-levels-of-cognitions">(a) Levels of cognitions</h3><p>REBT postulates that our disturbed mental states and unhelpful emotions follow from thinking that happens at three levels [6].</p><ol><li><strong>First level</strong>: perceptions, automatic thoughts, and negative attributions (inferential)<ol><li>This is the level at which cognitive distortions occur. It's what we're aware of and that forms our stream of consciousness. These thoughts are inferential in nature. Eg. "He didn't like my work"/"I fail at everything"/"I am stupid". Because they're inferences, they can be tested. </li></ol></li><li><strong>Second level</strong>: awfulizing, global evaluations, and frustration intolerance (evaluative), that are derived from the third level.<ol><li>They evaluate the importance of the inferences: the badness of the inference(awfulizing), evaluation of your ability to tolerate the situation (frustration intolerance), or worth of person involved (global evaluations).</li></ol></li><li><strong>Third level</strong>: the core irrational belief of "demandingness" from which everything else follows. (imperative/schematic)<ol><li>Also known as "imperative demands" , these are thoughts about the way reality "should" be. They are core beliefs that follow the construct of " I must", "you must", or "the world must". Albert Ellis calls this <strong>MUSTerbation — expectations about the way the world is, ought to be, and what is good or bad about what is/ought to be</strong>. Words like "should", "ought", "must" and "have to" represent demandingness.</li></ol></li></ol><h3 id="b-types-of-irrational-beliefs">(b) Types of irrational beliefs</h3><p>REBT recognizes 5 basic types of irrational beliefs that underlie cognitive distortions: <em>demandingness, awfulizing, frustration intolerance, self-worth ratings, and other-worth ratings</em>. </p><blockquote><em><strong>Demandingness</strong> is an unrealistic and absolute expectation of events or individuals being the way a person desires them to be.&nbsp;<p><strong>Awfulizing</strong> is an exaggeration of the negative consequences of a situation to an extreme degree, so that an unfortunate occurrence becomes “terrible".</p><p><strong>Frustration Intolerance (FI)</strong> stems from demands for ease and comfort, and reflects an intolerance of discomfort.&nbsp;</p><p><strong>Global evaluations of human worth</strong>, either of the self or others, imply that human beings can be rated, and that some people are worthless, or at least less valuable than others.&nbsp;</p><p>[6]</p></em></blockquote><p>The most common situations where we use these irrational belief processes happen in these contexts: <strong><em>social relationships, achievement, comfort, and fairness</em></strong>.</p><h2 id="how-to-use-this-list-of-cognitive-distortions">How to use this list of Cognitive Distortions</h2><p>The key is to understand that thought patterns are akin to muscle movements.</p><p>Habitual ways of thinking have an evolutionary purpose — they help our brains use less energy, draw conclusions faster, and move on with our lives. We get used to a certain way of doing (in this case thinking), and the only way to change/improve is to do it differently, and getting the new “movement” into muscle memory. In this case, your neural pathways.</p><p>But to do that, you have to first recognize them. We cannot improve something we don’t notice. Identifying the usual suspects beforehand helps raise our level of awareness, which in turn helps in disrupting habitual patterns.</p><p>⦿ Familiarize yourself with the common ones.<br>⦿ Identify 2-3 patterns you regularly fall for.<br>⦿ Target those in the coming weeks.<br>⦿ Practice identifying them.<br>⦿ Replace those recurring patterns with counter narratives.<br>⦿ Practice just as you would practice scales in music, or weights for building muscle. </p><p>Reading about them raises awareness, but changes are not sustained without practice.</p><p>In addition to the specific countering of cognitive distortions, below are some general approaches you can use to improve your thinking and avoid these thought traps.</p><h3 id="thought-record-journals">Thought record journals</h3><p>Writing down your thoughts and identifying what distortion you might be falling for is super helpful. Done consistently, this practice in itself can literally rewire your neural system.</p><h3 id="self-compassion">Self-Compassion</h3><p>If you were advising your friend in a similar situation, what might you tell them? We're often much harsher on ourselves than with others.</p><h3 id="relative-thinking">Relative thinking</h3><p>Learn to recognize the complexity of problems and situations and how your take might be too simplistic. Extreme interpretations require extreme simplification.</p><h3 id="scientific-thinking">Scientific thinking</h3><p>Always be searching for evidence. Test out your interpretations and get feedback from multiple sources. A useful tool for seeking critical feedback and identifying potential blindspots is <a href="https://www.leadingsapiens.com/johari-window-complete-guide-for-leaders/" rel="noreferrer">the Johari Window</a>. </p><h3 id="watch-out-for-emotive-language">Watch out for emotive language</h3><p>Be extra alert to “musts”, “should”, “oughts”, “have to’s”, “it’s awful”, “I can’t stand it”, “never”, and so on.</p><h3 id="identify-your-level-of-thinking">Identify your level of thinking</h3><p>Follow the chain of inferences down the levels of thinking and identify what irrational belief  might be driving your distortion. Another useful tool is <a href="https://www.leadingsapiens.com/ladder-of-inference-decision-making/" rel="noreferrer">the ladder of inference</a>.</p><h3 id="cost-benefit-analysis">Cost-benefit analysis</h3><p>Is your particular approach or way of thinking helpful or harmful? Are there alternate explanations that might be more beneficial?</p><hr><div><p>💡</p><div><p>Liked this article? <b><strong>Try</strong></b> <b><strong>my free newsletter</strong></b>. Every edition covers essential frameworks on <b><strong>leadership, careers, and organizations </strong></b>in bite-sized form.</p><p>📚 <b><strong>HBR 100 Best Reads:</strong></b> You also get a curated spreadsheet of the best articles Harvard Business Review has ever published. Spans 70 years, comes complete with categories and short summaries.</p></div></div>
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<hr><h2 id="further-reading">Further reading</h2><p>Below are some related articles that build and add to the concepts discussed above.</p><ul><li>Instead of learning new mental models, it's often more productive to identify and dismantle <a href="https://www.leadingsapiens.com/mental-models-subtractive-approach/" rel="noreferrer">hidden but unhelpful mental models</a>:</li></ul><figure><a href="https://www.leadingsapiens.com/mental-models-subtractive-approach/"><div><p>Mental Models - A Subtractive Approach</p><p>Most common discourse on mental models takes an additive approach. But this tends to be half-baked, often useless in practice. There is equal value, even more so, in a subtractive approach to mental models. The key is in understanding the difference between hard and soft mental models. How is the</p><p><img src="https://www.leadingsapiens.com/content/images/size/w256h256/2022/05/LS-logo.png" alt=""><span>Leading Sapiens</span><span>Sheril Mathews</span></p></div><p><img src="https://www.leadingsapiens.com/content/images/2023/02/Mental-Models---Hard-vs-Soft-2.png" alt=""></p></a></figure><ul><li><a href="https://www.leadingsapiens.com/double-loop-learning-leadership-performance/" rel="noreferrer">Double-loop learning: a simple model of performance</a> and what lies upstream</li></ul><figure><a href="https://www.leadingsapiens.com/double-loop-learning-leadership-performance/"><div><p>How Double-Loop Learning Improves Performance</p><p>Our actions, and by extension performance, stem from thinking that is based on a set of hidden mental models. How do you uncover these mental models and change them? One way is to understand and practice the concepts of single-loop and double-loop learning. Professional sports teams use postgame films and</p><p><img src="https://www.leadingsapiens.com/content/images/size/w256h256/2022/05/LS-logo.png" alt=""><span>Leading Sapiens</span><span>Sheril Mathews</span></p></div><p><img src="https://www.leadingsapiens.com/content/images/size/w1200/2023/03/Double-loop-learning---Argyris-Schon-1.png" alt=""></p></a></figure><ul><li>The <a href="https://www.leadingsapiens.com/ladder-of-inference-decision-making/" rel="noreferrer">ladder of inference</a> is a useful framework that shows how we go from facts to inferences to actions:</li></ul><figure><a href="https://www.leadingsapiens.com/ladder-of-inference-decision-making/"><div><p>Using the ladder of inference to make better decisions</p><p>The ladder of inference is a powerful tool to make better decisions by uncovering hidden mental models and understanding how we reach conclusions.</p><p><img src="https://www.leadingsapiens.com/content/images/size/w256h256/2022/05/LS-logo.png" alt=""><span>Leading Sapiens</span><span>Sheril Mathews</span></p></div><p><img src="https://www.leadingsapiens.com/content/images/2022/12/Ladder-of-Inference.png" alt=""></p></a></figure><ul><li><a href="https://www.leadingsapiens.com/dont-overcome-self-doubt/#dunning-kruger-effect-and-impostor-syndrome" rel="noreferrer">Imposter syndrome</a> is one kind of cognitive distortion/thought trap:</li></ul><figure><a href="https://www.leadingsapiens.com/dont-overcome-self-doubt/"><div><p>There’s no need to eliminate or even overcome self-doubt.</p><p>Self-doubt is not an impediment to be eliminated as is commonly thought of. It’s a condition of the game and can even be a positive indicator.</p><p><img src="https://www.leadingsapiens.com/content/images/size/w256h256/2022/05/LS-logo.png" alt=""><span>Leading Sapiens</span><span>Sheril Mathews</span></p></div><p><img src="https://www.leadingsapiens.com/content/images/2022/12/Doubt--Dunning-Kruger-Imposter-Syndrome.png" alt=""></p></a></figure><h2 id="sources">Sources</h2><ol><li><a href="https://amzn.to/3TjOMss?ref=leadingsapiens.com" rel="noreferrer">Feeling Good</a> by David Burns</li><li> <a href="https://amzn.to/473A3oY?ref=leadingsapiens.com" rel="noreferrer">The Anxious Achiever</a> by Morra Aarons-Mele</li><li><a href="https://amzn.to/48eSasR?ref=leadingsapiens.com" rel="noreferrer">Handbook of Coaching Psychology </a> by Stephen Palmer, Alison Whybrow</li><li><a href="https://amzn.to/3uWpZ3x?ref=leadingsapiens.com" rel="noreferrer">Dealing with Emotional Problems&nbsp;Using RECBT</a> by Windy Dryden</li><li><a href="https://amzn.to/4afpuBP?ref=leadingsapiens.com" rel="noreferrer">Evidence Based Coaching Handbook</a> by Dianne Stober, Anthony Grant</li><li><a href="https://amzn.to/3TtpdVM?ref=leadingsapiens.com" rel="noreferrer">A Practitioner's Guide to REBT</a> by Raymond DiGiuseppe, Kristen Doyle, Windy Dryden, Wouter Backx</li></ol>
    </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Double-mirror illusion (113 pts)]]></title>
            <link>https://journalofillusion.net/index.php/joi/article/view/9839/16407</link>
            <guid>38654968</guid>
            <pubDate>Fri, 15 Dec 2023 15:17:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journalofillusion.net/index.php/joi/article/view/9839/16407">https://journalofillusion.net/index.php/joi/article/view/9839/16407</a>, See on <a href="https://news.ycombinator.com/item?id=38654968">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="customblock-about">
		<p><span>About the journal</span></p>
<p>Journal of Illusion (ISSN: 2436-4045) is an open-access journal that aims at gathering resources to promote the study of illusion. JOI defines an illusion as the perception of an object or phenomenon that is considered to be inconsistent with individual’s or group’s prior knowledge, recognition, or belief as to what the object or phenomenon should be in perception, cognition, and/or physics.&nbsp; Therefore, JOI focuses on perceptual illusions, cognitive illusions (e.g. magic or misunderstanding) or physical illusions (e.g. mirage or the Doppler effect). For perceptual illusions, not only visual illusions but also illusions at various sensory modalities are welcome. Trompe l’oeil as well as illusion artworks are also welcome. <strong><a href="https://journals.openacademia.net/index.php/joi/about">Learn more &gt;&gt;</a></strong></p>
	</div><div id="customblock-whypublish">
		<p><span>Why publish with <strong><em>Journal of Illusion</em></strong>?</span></p>
<p><span>Open Access</span>&nbsp;–&nbsp;<em>Journal of Illusion</em>&nbsp;is free from all access barriers, allowing for the widest possible dissemination of your work.</p>
<p><span>Retain copyright&nbsp;</span>– you are free to disseminate your work, make unlimited copies, and deposit it in any repository.&nbsp;</p>
<p><span>Personal service</span>&nbsp;–&nbsp;<em>Journal of Illusion</em>&nbsp;is published in partnership with <a href="https://openacademia.net/index.html">Open Academia</a>, a Publishing Partner dedicated to giving you excellent service.&nbsp;</p>
<p><span>Self-archiving</span>&nbsp;– you can deposit&nbsp;<em>any</em>&nbsp;version of your manuscript in any required repository or archive, or post it to your personal or institutional website.&nbsp;</p>
<p><strong>Post-publication statistics</strong> – metrics shown with each article make it easy to check how often your paper is being downloaded via the JOI website.</p>
<p><strong>Add supplementary material</strong> – you can make data sets, protocols, very large illustrations, videos, questionnaires etc. available to readers alongside your article, free of charge.&nbsp;</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Delta Dental says data breach exposed info of 7M people (233 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/delta-dental-says-data-breach-exposed-info-of-7-million-people/</link>
            <guid>38654805</guid>
            <pubDate>Fri, 15 Dec 2023 14:59:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/delta-dental-says-data-breach-exposed-info-of-7-million-people/">https://www.bleepingcomputer.com/news/security/delta-dental-says-data-breach-exposed-info-of-7-million-people/</a>, See on <a href="https://news.ycombinator.com/item?id=38654805">Hacker News</a></p>
Couldn't get https://www.bleepingcomputer.com/news/security/delta-dental-says-data-breach-exposed-info-of-7-million-people/: Error: Request failed with status code 503]]></description>
        </item>
        <item>
            <title><![CDATA[September 11th That You Have Never Seen (133 pts)]]></title>
            <link>https://medium.com/@jeremiahjw/31-photos-from-september-11th-that-you-have-never-seen-9514aaac2d54</link>
            <guid>38654567</guid>
            <pubDate>Fri, 15 Dec 2023 14:34:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@jeremiahjw/31-photos-from-september-11th-that-you-have-never-seen-9514aaac2d54">https://medium.com/@jeremiahjw/31-photos-from-september-11th-that-you-have-never-seen-9514aaac2d54</a>, See on <a href="https://news.ycombinator.com/item?id=38654567">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://medium.com/@jeremiahjw?source=post_page-----9514aaac2d54--------------------------------"><div aria-hidden="false"><p><img alt="Jeremiah Warren" src="https://miro.medium.com/v2/resize:fill:88:88/1*cBiTRH-yRdfE6I0-iClrkg.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="3f35">In 2001, digital cameras were a rare commodity. They were expensive, bulky and captured images that were inferior to the organic look of film. After you downloaded and edited those whopping 3.1 megapixel images, you had very few options of where you could publish them online. Remember when Shutterfly and Snapfish were a thing?</p><p id="d82a">Contrast that to today where you have people shooting magazine covers on cell phones and uploading over 500,000 images to Instagram, Snapchat, Twitter, and Facebook every minute. We don’t hear about national tragedies on the news anymore, we read about them in our Twitter and Facebook feeds. Seconds after they happen.</p><p id="69e2">When the events of 9/11 took place there were thousands of photographs taken by professional photographers and members of the press. These images were shown on the news and published in magazines and newspapers all across the country. Yet, few of these featured photographs were taken by everyday people.</p><p id="0dc5">I wanted to set about curating a selection of photographs that most of you haven’t seen. Photographs captured by everyday people. Thanks to the internet these individuals have been able to publish their photos to Flickr, but most of them have less than a thousand, or even less than a hundred views. As I searched for these images it was like I was witnessing history again, but from an angle that no one had ever been shown. I decided to share these photographs with all of you.</p><p id="0b95">For the images that were captured on a digital camera, I’ve made a note of the model of the camera. All images are hosted on the account of the person that owns the photographs, none of them were taken down and re-hosted.</p><figure></figure><p id="968f">Seconds after flight 175 struck the South Tower. Taken with a Canon PowerShot S100 by <a href="http://www.flickr.com/people/georgeweld/" rel="noopener ugc nofollow" target="_blank">George Weld</a>.</p><figure></figure><p id="b5e1"><a href="http://www.flickr.com/people/since1968/" rel="noopener ugc nofollow" target="_blank">Marc Garrett</a>, about this image he captured. “The second plane flew directly over my head and slammed into the south tower. It took me a few seconds to get my head together, and this was the shot I took. I’m not a professional photojournalist, but I believe having a camera in my hand and feeling like a I had a “job” to do helped me keep my head.”</p><figure></figure><p id="faff"><a href="http://www.flickr.com/people/hiro_oshima/" rel="noopener ugc nofollow" target="_blank">The photographer</a> who took this photo mentions that at the time it didn’t occur to him how bad of an idea it was to walk so close to the tower right after it was struck. Later he discovered that he had been hit in the leg by a piece of falling metal, but didn’t notice it until hours later after he had settled down. If you read the comments you’ll find one by the owner of the open delivery truck you see in this image. He mentioned that the driver of the truck, seen in the blue shirt and pants survived the ordeal. The truck, however, was crushed. This image and the following were taken on an Olympus E-10.</p><figure></figure><p id="2f29">This image struck me on a deep emotion level. In the midst of the chaos and destruction there were still people willing to show their selflessness and cover the remains of the victims.</p><figure></figure><p id="7cb7">Taken a few moments after the second tower was hit, you can see the cloud of paper floating through the air. Photograph by <a href="http://www.flickr.com/people/79579905@N00/" rel="noopener ugc nofollow" target="_blank">Ronald Smits</a>.</p><figure></figure><p id="a71d">You can see the outline of the plane’s wing span. Photograph by <a href="http://www.flickr.com/people/hiro_oshima/" rel="noopener ugc nofollow" target="_blank">Hiro</a>.</p><figure></figure><p id="7135">I think this image speaks for itself. Photograph by <a href="http://www.flickr.com/people/lukekurtis/" rel="noopener ugc nofollow" target="_blank">Luke Kurtis</a>.</p><figure></figure><p id="ea71">Photographer <a href="http://www.flickr.com/people/strippednuts/" rel="noopener ugc nofollow" target="_blank">Jay Boucher says</a>: “My wife had called me that morning to let me know she was safe. “Huh?” I said. She told me to turn on the TV and there was the Trade Center, burning. I grabbed my cameras and ran out to Hoboken’s Pier A. This is what I saw”.</p><figure></figure><p id="8a49">Photograph by <a href="http://www.flickr.com/people/georgeweld/" rel="noopener ugc nofollow" target="_blank">George Weld</a>, taken on a Canon PowerShot S100.</p><figure></figure><p id="df48">Photograph by <a href="http://www.flickr.com/people/pixorama/" rel="noopener ugc nofollow" target="_blank">Michael Foran</a>, taken on an Olympus C2000Z.</p><figure></figure><p id="a850">Photographed by <a href="http://www.flickr.com/people/hbomb/" rel="noopener ugc nofollow" target="_blank">Harvey Silikovitz</a> on Houston Street in Greenwich Village. “An out-of-town TV reporter who is covering the 9/11 tragedy looks at the smoke emanating from the wreckage of the World Trade Center, a couple of miles to the south. Taken during my pre-digital days, this picture happened to be on a roll that for some reason I had gotten burned onto a CD when I got it developed.”</p><figure></figure><p id="696f">A rescue team taking off to attempt a rooftop rescue. They never made it. Photograph by <a href="http://www.flickr.com/people/bryanthatcher/" rel="noopener ugc nofollow" target="_blank">Bryan Thatcher</a>, taken with a Sony Cybershot.</p><figure></figure><p id="ecaf">Photographer <a href="http://www.flickr.com/people/pixorama/" rel="noopener ugc nofollow" target="_blank">Michael Foran</a> says “This man was overcome with emotion as we listened to the calls of the Firemen and Police trapped in the rubble of the collapsed towers on his police scanner radio.”</p><figure></figure><p id="dd24">Photograph by <a href="http://www.flickr.com/people/13189502@N02/" rel="noopener ugc nofollow" target="_blank">Eddy</a>, taken on an Olympus C3000Z</p><figure></figure><p id="3be9">It looks like this woman is shooting with an Olympus film camera. I think I still have the same lens and camera. Photograph by <a href="http://www.flickr.com/people/theactionitems/" rel="noopener ugc nofollow" target="_blank">Marc AuMarc</a>.</p><figure></figure><p id="2aa1">Photograph by <a href="http://www.flickr.com/photos/georgeweld/" rel="noopener ugc nofollow" target="_blank">George Weld</a>, taken on a Canon PowerShot S100.</p><figure></figure><p id="be8f">There are a lot of photographs of messages scrawled into the dust covering the cars. I can’t make out what the note says. Photograph by <a href="http://www.flickr.com/people/theactionitems/" rel="noopener ugc nofollow" target="_blank">Marc AuMarc</a>.</p><figure></figure><p id="dc1b">Photographer <a href="http://www.flickr.com/people/hiro_oshima/" rel="noopener ugc nofollow" target="_blank">Hiro</a> says “The firemen were utterly covered by the debris. We all could tell that a lot of it was asbestos, though no one said it outloud. It crossed my mind that this could be the real terror, if all the people around became ill after the fact.”</p><figure></figure><p id="ac30">Taken with a Nikon E990 by <a href="http://www.flickr.com/people/georgehjr/" rel="noopener ugc nofollow" target="_blank">George Hackett</a>.</p><figure></figure><p id="ace7">Photograph by <a href="http://www.flickr.com/people/malarchie/" rel="noopener ugc nofollow" target="_blank">Shayna Marchese</a>. Her father posted this image on his Flickr account, he says “This is 6th Avenue and there was no traffic on it at all. Just pedestrians beginning to realize that the first tower had fallen.”</p><figure></figure><p id="dd43">Photographer <a href="http://www.flickr.com/people/boyds/" rel="noopener ugc nofollow" target="_blank">Brian Boyd</a> says “I’m running North on West Side Highway, just one block from Chambers street. The tower just collapsed seconds before this photo.”</p><figure></figure><p id="699b">Photograph by <a href="http://www.flickr.com/people/georgehjr/" rel="noopener ugc nofollow" target="_blank">George Hackett</a></p><figure></figure><p id="a5e1">Photograph by <a href="http://www.flickr.com/people/georgehjr/" rel="noopener ugc nofollow" target="_blank">George Hackett</a>.</p><figure></figure><p id="e60f">Photograph by <a href="http://www.flickr.com/people/bryanthatcher/" rel="noopener ugc nofollow" target="_blank">Bryan Thatcher</a>, taken on a Sony Cybershot.</p><figure></figure><p id="4890">Photographer <a href="http://www.flickr.com/people/santijose/" rel="noopener ugc nofollow" target="_blank">Santi-Jose</a> says “I never go down to that area of the city during the week, but there I was on that morning. chance or fate? I was to witness this moment in history. ever since that day seven years ago I almost never leave the house without my camera.”</p><figure></figure><p id="5964">Brooklyn onlookers. Photograph by <a href="http://www.flickr.com/people/x-hibition/" rel="noopener ugc nofollow" target="_blank">Hans</a>.</p><figure></figure><p id="43af">Photograph by <a href="http://www.flickr.com/people/kenengimaging/" rel="noopener ugc nofollow" target="_blank">Ken Eng</a>. Taken on a Fujifilm FinePixS1 Pro.</p><figure></figure><p id="5936">Photograph by <a href="http://www.flickr.com/people/kenengimaging/" rel="noopener ugc nofollow" target="_blank">Ken Eng</a>.</p><figure></figure><p id="374e">Photograph by <a href="http://www.flickr.com/people/kenengimaging/" rel="noopener ugc nofollow" target="_blank">Ken Eng</a>.</p><figure></figure><p id="b14f">Photographed by <a href="http://www.flickr.com/people/demonbaby/" rel="noopener ugc nofollow" target="_blank">Rob Sheridan</a> from his Brooklyn apartment, on a Canon EOS D30.</p><figure></figure><p id="4ec1">This was taken the day after 9/11, on September 12th, by <a href="http://www.flickr.com/people/13189502@N02/" rel="noopener ugc nofollow" target="_blank">Eddy</a>.</p></div><div><p id="8b70"><strong>If you enjoyed this piece, please consider clicking the little clapping hands icon below, so someone else will see and read it too. Thank you! :)</strong></p><p id="c2ff"><em>Follow me on Twitter </em><a href="http://twitter.com/jeremiahjw" rel="noopener ugc nofollow" target="_blank"><em>@JeremiahJW </em></a><em>.</em></p><p id="565c"><em>See more of my work at </em><a href="http://jeremiahwarren.com/" rel="noopener ugc nofollow" target="_blank"><em>JeremiahWarren.com</em></a><em>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Data exfiltration from Writer.com with indirect prompt injection (201 pts)]]></title>
            <link>https://promptarmor.substack.com/p/data-exfiltration-from-writercom</link>
            <guid>38654533</guid>
            <pubDate>Fri, 15 Dec 2023 14:31:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://promptarmor.substack.com/p/data-exfiltration-from-writercom">https://promptarmor.substack.com/p/data-exfiltration-from-writercom</a>, See on <a href="https://news.ycombinator.com/item?id=38654533">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>This vulnerability can allow attackers to steal a user’s private documents by manipulating the language model used for content generation. As of now, it has not been fixed as it was not triaged as a security vulnerability by Writer.com after disclosure (more details in Responsible Disclosure section at the end). </em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png" width="480" height="289.1208791208791" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:877,&quot;width&quot;:1456,&quot;resizeWidth&quot;:480,&quot;bytes&quot;:1399454,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>(the attack, disguised in white text on an attacker controlled website)</figcaption></figure></div><p><span>Writer.com is an application that can be used by enterprises and consumers alike. Users can upload data files, share links, and ask questions in order to generate tailored content for their business needs. It has access to your brand and knowledge base and as such can maintain consistency when writing articles for you. They emphasize its data security given the sensitivity of information its clients upload throughout its website: </span><a href="https://writer.com/product/data-security-privacy/" rel="nofollow ugc noopener">https://writer.com/product/data-security-privacy/</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png" width="480" height="255.14950166112956" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:1204,&quot;resizeWidth&quot;:480,&quot;bytes&quot;:67733,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 1456w" sizes="100vw"></picture></div></a><figcaption>(screenshot from writer.com/security on Dec 13)</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png" width="470" height="129.12087912087912" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:400,&quot;width&quot;:1456,&quot;resizeWidth&quot;:470,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 1456w" sizes="100vw"></picture></div></a><figcaption>(screenshot from writer.com on Dec 5)</figcaption></figure></div><p>In Writer, users can enter a ChatGPT-like session to edit or create their documents. In this chat session, the LLM can retrieve information from sources on the web to assist users in creation of their documents. We show that attackers can prepare websites that, when a user adds them as a source, manipulate the LLM into sending private information to the attacker or perform other malicious activities.&nbsp;</p><p>The data theft can include documents the user has uploaded, their chat history or potentially specific private information the chat model can convince the user to divulge at the attacker's behest.</p><p><span>This type of attack is called </span><a href="https://arxiv.org/abs/2302.12173" rel="nofollow ugc noopener">indirect prompt injection</a><span>, initially coined by Kai Greshake.</span></p><p>To prove the feasibility of such an attack, we uploaded a file that contains mocked sensitive information (SSN numbers, revenue figures, salary information), and were able to exfiltrate all of it:&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png" width="1456" height="84" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:84,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:96853,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot from our pentesting exfiltration server)</figcaption></figure></div><p>The website that hosts the payload looks like any other website, and the payload is hidden from any user visiting it. In the following screenshot, the hidden text of the payload is highlighted (it has white font, but there are other methods to hide it or embed payloads on other websites such as social media platforms).:&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png" width="1456" height="363" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:363,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot from the website with the injection)</figcaption></figure></div><p>Note that *.cloudfront.net is one of the locations allowed by the CSP. </p><p>A typical user use case would be the following:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png" width="1456" height="258" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:258,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1083806,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>However, here’s what actually happens in the background with the injection</p><p>A) They ask Writer to write a report for them based on some sources and some data they upload.&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png" width="1456" height="1147" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1147,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot from our chat session at writer.com)</figcaption></figure></div><p>B) They find a nice source on the web which has the information they need</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png" width="1456" height="1398" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1398,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>C) They upload some sensitive data</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png" width="1360" height="832" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b856d97d-56ff-4027-9307-06cc92003da6_1360x832.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:832,&quot;width&quot;:1360,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>D) They get Writer to write the report</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png" width="1456" height="428" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:428,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot from our chat window at writer.com)</figcaption></figure></div><p>E) Writer reads the webpage, but it contains a hidden injection in small white text:&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png" width="1456" height="877" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:877,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>F) Writer follows the instructions, and overrides the initial instructions of the user and any security filters Writer.com has enforced. The user never asked for this image and it was not on the webpage that they initially asked for. Nevertheless, Writer.com has automatically rendered the attacker-controlled image in markdown and you can see in the network activity that it has appended the contents of the uploaded client data file to the HTTP parameters, just as instructed: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png" width="1456" height="758" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:758,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4418822,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot of our chat session at writer.com with network activity expanded)</figcaption></figure></div><p>Here’s a side by side comparison between the uploaded client data file, and a zoomed in image of the HTTP parameters from the above screenshot: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png" width="1456" height="255" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:255,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1406652,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>G) Without their knowledge, their data has now been exfiltrated to the attacker’s server. Rendering the image in markdown automatically created a GET request with the HTTP parameters including the content of the file. The attacker can read their logs to extract the sensitive client data from the file.&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png" width="1456" height="1634" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1634,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2070420,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot from pentesting exfiltration server logs)</figcaption></figure></div><p><span>Rendering an image to exfiltrate data is only one method of exfiltrating data. Note that while, to our knowledge, Writer does not use OpenAI for text generation, </span><a href="https://embracethered.com/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/#:~:text=Responsible%20Disclosure" rel="nofollow ugc noopener">OpenAI has said the same issue in their system is a “won’t fix.”</a></p><p>Please see below for other example attacks which use other mediums (like links) to exfiltrate data.&nbsp;</p><h5>Example 1: Exfiltration of uploaded files</h5><p>In this example, an attacker is able to exfiltrate a confidential file that the user uploads via&nbsp;a link, using this injection: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png" width="1456" height="284" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:284,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:121493,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://www.loom.com/share/d8ffe66d9dfb44429be8a60182372f38" rel="nofollow ugc noopener">Video explanation (sent with disclosure)</a></p><h5>Example 2: Exfiltration of chat history</h5><p>In this example, an attacker is able to exfiltrate the chat history from a user via a link, using this injection: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png" width="1456" height="309" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:309,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://www.loom.com/share/678d76f03c5244a1aee592ba41e744dd" rel="nofollow ugc noopener">Video explanation (sent with disclosure)</a></p><p><span>These type of attacks have been done in other LLM surfaces, such as the </span><a href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/" rel="nofollow ugc noopener">Bard attack by Thacker, Rehberger and Greshake </a><span>, which was resolved promptly by the Google Security and Bard team.&nbsp;</span></p><p>For more information on these attacks and relevant information here are some great sources:&nbsp;</p><ul><li><p><a href="https://kai-greshake.de/" rel="nofollow ugc noopener">https://kai-greshake.de/</a><span> (twitter: @KGreshake)</span></p></li><li><p><a href="https://embracethered.com/blog/index.html" rel="nofollow ugc noopener">https://embracethered.com/blog/index.html</a><span> (twitter: @wunderwuzzi23)</span></p></li><li><p><a href="https://josephthacker.com/" rel="nofollow ugc noopener">https://josephthacker.com/</a><span> (twitter: @rez0_) </span></p></li><li><p><a href="https://promptarmor.com/" rel="nofollow ugc noopener">https://promptarmor.com/ </a><span>(twitter: @promptarmor)</span></p></li></ul><p>And to learn more about LLM security risks feel free to check out:&nbsp;</p><ul><li><p><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/" rel="nofollow ugc noopener">https://owasp.org/www-project-top-10-for-large-language-model-applications/</a></p></li><li><p><a href="https://atlas.mitre.org/" rel="nofollow ugc noopener">https://atlas.mitre.org/</a></p></li></ul><p>Responsible Disclosure Timeline</p><ul><li><p>Nov 29: We disclose issue to CTO &amp; Security team with video examples</p></li><li><p>Nov 29: Writer responds, asking for more details</p></li><li><p>Nov 29: We respond describing the exploit in more detail with screenshots</p></li><li><p>Dec 1: We follow up</p></li><li><p>Dec 4: We follow up with re-recorded video with voiceover asking about their responsible disclosure policy</p></li><li><p>Dec 5: Writer responds “We do not consider this to be a security issue since the real customer accounts do not have access to any website.”</p></li><li><p>Dec 5: We explain that paid customer accounts have the same vulnerability, and inform them that we are writing a post about the vulnerability so consumers are aware. No response from the Writer team after this point in time. </p></li></ul><p>Feel free to reach out to us at founders@promptarmor.com or at https://kai-greshake.de/about</p><h6>Disclaimer: The content of this blog is intended solely for research and educational use, aimed at enhancing knowledge, understanding, and awareness regarding attacks and their countermeasures to bolster the security of Large Language Models (LLMs)</h6></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Happens in the Brain While Daydreaming? (179 pts)]]></title>
            <link>https://hms.harvard.edu/news/what-happens-brain-while-daydreaming</link>
            <guid>38654388</guid>
            <pubDate>Fri, 15 Dec 2023 14:18:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hms.harvard.edu/news/what-happens-brain-while-daydreaming">https://hms.harvard.edu/news/what-happens-brain-while-daydreaming</a>, See on <a href="https://news.ycombinator.com/item?id=38654388">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><span lang="EN" xml:lang="EN"><strong>At a glance:</strong></span></p><ul><li><span lang="EN" xml:lang="EN"></span><span lang="EN" xml:lang="EN"><strong>During quiet waking, brain activity in mice suggests the animals are daydreaming about a recent image.</strong></span></li><li><span lang="EN" xml:lang="EN"></span><span lang="EN" xml:lang="EN"><strong>Having daydreams about a recently viewed image predicted how the brain would respond to the image in the future.</strong></span></li><li><p><span lang="EN" xml:lang="EN"></span><span lang="EN" xml:lang="EN"><strong>The findings provide a clue that daydreams may play a role in brain plasticity. </strong></span></p><hr></li></ul><p><span lang="EN" xml:lang="EN"></span><span lang="EN" xml:lang="EN">You are sitting quietly, and suddenly your brain tunes out the world and wanders to something else entirely — perhaps a recent experience, or an old memory. You just had a daydream. </span></p><p><span lang="EN" xml:lang="EN">Yet despite the ubiquity of this experience, what is happening in the brain while daydreaming is a question that has largely eluded neuroscientists. </span></p><p><span lang="EN" xml:lang="EN">Now, a study in mice, </span><a href="https://www.nature.com/articles/s41586-023-06810-1"><span lang="EN" xml:lang="EN">published Dec. 13 in </span><em><span lang="EN" xml:lang="EN">Nature</span></em></a><em><span lang="EN" xml:lang="EN">,</span></em><span lang="EN" xml:lang="EN"> has brought a team led by researchers at Harvard Medical School one step closer to figuring it out. </span></p><p><a href="https://hms.harvard.edu/news-events/sign-email-communications"><span lang="EN" xml:lang="EN"><strong>Get more HMS news here</strong></span></a><span lang="EN" xml:lang="EN"></span></p><p><span lang="EN" xml:lang="EN">The researchers tracked the activity of neurons in the visual cortex of the brains of mice while the animals remained in a quiet waking state. They found that occasionally these neurons fired in a pattern similar to one that occurred when a mouse looked at an actual image, suggesting that the mouse was thinking — or daydreaming — about the image. Moreover, the patterns of activity during a mouse’s first few daydreams of the day predicted how the brain’s response to the image would change over time. </span></p><p><span lang="EN" xml:lang="EN">The research provides tantalizing, if preliminary, evidence that daydreams can shape the brain’s future response to what it sees. This causal relationship needs to be confirmed in further research, the team cautioned, but the results offer an intriguing clue that daydreams during quiet waking may play a role in brain plasticity — the brain’s ability to remodel itself in response to new experiences.</span></p><p><span lang="EN" xml:lang="EN">“We wanted to know how this daydreaming process occurred on a neurobiological level, and whether these moments of quiet reflection could be important for learning and memory,” said lead author Nghia Nguyen, a PhD student in neurobiology in the Blavatnik Institute at HMS. </span></p><h3><span lang="EN" xml:lang="EN"><strong>An overlooked brain region</strong></span></h3><div><p><span lang="EN" xml:lang="EN">Scientists have spent considerable time studying how neurons replay past events to form memories and map the physical environment in the hippocampus, a seahorse-shaped brain region that plays a key role in memory and spatial navigation.</span></p><p><span lang="EN" xml:lang="EN">By contrast, there has been little research on the replay of neurons in other brain regions, including the visual cortex. Such efforts would provide valuable insights about how visual memories are formed. </span></p></div><p><span lang="EN" xml:lang="EN">“My lab became interested in whether we could record from enough neurons in the visual cortex to understand what exactly the mouse is remembering — and then connect that information to brain plasticity,” said senior author </span><a href="https://www.andermannlab.com/"><span lang="EN" xml:lang="EN">Mark Andermann</span></a><span lang="EN" xml:lang="EN">, professor of medicine at Beth Israel Deaconess Medical Center, and professor of neurobiology at HMS. </span></p><figure><article><img loading="lazy" src="https://hms.harvard.edu/sites/default/files/2023-12/MouseMovie-1-and-2-400x320.gif" width="400" height="320" alt="A checkerboard pattern of gray and black and white squares that morphs into another, similar pattern"></article><figcaption>During the experiments, mice repeatedly looked at one of two images, shown here, with one-minute breaks in between. The images were selected based on their ability to elicit a strong response from neurons in the visual cortex. Video: Andermann lab</figcaption></figure><p><span lang="EN" xml:lang="EN">In the new study, the researchers repeatedly showed mice one of two images, each consisting of a different checkerboard pattern of gray and dappled black and white squares. Between images, the mice spent a minute looking at a gray screen. The team simultaneously recorded activity from around 7,000 neurons in the visual cortex. </span></p><p><span lang="EN" xml:lang="EN">The researchers found that when a mouse looked at an image, the neurons fired in a specific pattern, and the patterns were different enough to discern image one from image two. More important, when a mouse looked at the gray screen between images, the neurons sometimes fired in a similar, but not identical, pattern, as when the mouse looked at the image, a sign that it was daydreaming about the image. These daydreams occurred only when mice were relaxed, characterized by calm behavior and small pupils. </span></p><p><span lang="EN" xml:lang="EN">Unsurprisingly, mice daydreamed more about the most recent image — and they had more daydreams at the beginning of the day than at the end, when they had already seen each image dozens of times. </span></p><figure><article><img loading="lazy" src="https://hms.harvard.edu/sites/default/files/2023-12/FiringMouseNeurons-400x274.gif" width="400" height="274" alt="A grayscale image showing a cloud-like group of neurons lighting up"></article><figcaption>Between images, mice spent a minute looking at a gray screen. During this time, neurons in the visual cortex of the brain, shown here, occasionally fired in a pattern similar to one seen when the mice were looking at an image, suggesting that mice were daydreaming about the image. Video: Andermann lab</figcaption></figure><p><span lang="EN" xml:lang="EN">But what the researchers found next was completely unexpected. </span></p><p><span lang="EN" xml:lang="EN">Throughout the day, and across days, the activity patterns seen when the mice looked at the images changed — what neuroscientists call “representational drift.” Yet this drift wasn’t random. Over time, the patterns associated with the images became even more different from each other, until each involved an almost entirely separate set of neurons. Notably, the pattern seen during a mouse’s first few daydreams about an image predicted what the pattern would become when the mouse looked at the image later. </span></p><p><span lang="EN" xml:lang="EN">“There’s drift in how the brain responds to the same image over time, and these early daydreams can predict where the drift is going,” Andermann said. </span></p><p><span lang="EN" xml:lang="EN">Finally, the researchers found that the visual cortex daydreams occurred at the same time as replay activity occurred in the hippocampus, suggesting that the two brain regions were communicating during these daydreams. </span></p><h3><span lang="EN" xml:lang="EN"><strong>To sit, perchance to daydream</strong></span></h3><p><span lang="EN" xml:lang="EN">Based on the results of the study, the researches suspect that these daydreams may be actively involved in brain plasticity. </span></p><p><span lang="EN" xml:lang="EN">“When you see two different images many times, it becomes important to discriminate between them. Our findings suggest that daydreaming may guide this process by steering the neural patterns associated with the two images away from each other,” Nguyen said, while noting that this relationship needs to be confirmed. </span></p><p><span lang="EN" xml:lang="EN">Nguyen added that learning to differentiate between the images should help the mouse respond to each image with more specificity in the future. </span></p><p><span lang="EN" xml:lang="EN">These observations align with a growing body of </span><a href="https://www.sciencedirect.com/science/article/pii/S0149763422002883?casa_token=MBHEs_LeUGwAAAAA:KWgpbzTsGl7jv55nOZFGPssjIi-0j8-H1Th5rZ52Jxt71QglVXVsmU4BpcIhUAjtDw4QPyhXYHQ"><span lang="EN" xml:lang="EN">evidence in rodents and humans</span></a><span lang="EN" xml:lang="EN"> that entering a state of quiet wakefulness after an experience can improve learning and memory. </span></p><p><span lang="EN" xml:lang="EN">Next, the researchers plan to use their imaging tools to visualize the connections between individual neurons in the visual cortex and to examine how these connections change when the brain “sees” an image. </span></p><p><span lang="EN" xml:lang="EN">“We were chasing this 99 percent of unexplored brain activity and discovered that there’s so much richness in the visual cortex that nobody knew anything about,” Andermann said. </span></p><p><span lang="EN" xml:lang="EN">Whether daydreams in people involve similar activity patterns in the visual cortex is an open question, and the answer will require additional experiments. However, there is preliminary evidence that an analogous process occurs in humans when they recall visual imagery.</span></p><p><a href="https://bucknerlab.fas.harvard.edu/"><span lang="EN" xml:lang="EN">Randy Buckner</span></a><span lang="EN" xml:lang="EN">, the Sosland Family Professor of Psychology and of Neuroscience at Harvard University, has shown that </span><a href="https://www.pnas.org/doi/10.1073/pnas.97.20.11125"><span lang="EN" xml:lang="EN">brain activity in the visual cortex increases</span></a><span lang="EN" xml:lang="EN"> when people are asked to recall an image in detail. Other studies have recorded </span><a href="https://www.science.org/doi/10.1126/science.aax1030"><span lang="EN" xml:lang="EN">flurries of electrical activity</span></a><span lang="EN" xml:lang="EN"> in the visual cortex and the hippocampus during such recall. </span></p><p><span lang="EN" xml:lang="EN">For the researchers, the results of their study and others suggest that it may be important to make space for moments of quiet waking that lead to daydreams. For a mouse, this may mean taking a pause from looking at a series of images and, for a human, this could mean taking a break from scrolling on a smartphone. </span></p><p><span lang="EN" xml:lang="EN">“We feel pretty confident that if you never give yourself any awake downtime, you’re not going to have as many of these daydream events, which may be important for brain plasticity,” Andermann said. </span></p><p><span lang="EN" xml:lang="EN"><strong>Authorship, funding, disclosures</strong></span></p><p><span lang="EN" xml:lang="EN">Additional authors on the paper include Andrew Lutas, Oren Amsalem, Jesseba Fernando, Andy Young-Eon Ahn, Richard Hakim, Josselyn Vergara, Justin McMahon, Jordane Dimidschstein, and Bernardo Sabatini.</span></p><p><span lang="EN" xml:lang="EN">The research was supported by a National Defense Science and Engineering Fellowship, a Howard Hughes Medical Institute Gilliam Fellowship, the National Institutes of Health (F32 DK112589; DP2 DK105570; DP1 AT010971-02S1; R01 MH12343), a Davis Family Foundation award, a McKnight Scholar Award, a Harvard Mind Brain Behavior Interfaculty Initiative Faculty Research Award, the Harvard Brain Science Initiative Bipolar Disorder Seed Grant, and by Kent and Liz Dauten. </span></p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WWII code breaker Mary Ratcliffe has died (146 pts)]]></title>
            <link>https://www.dailymail.co.uk/news/article-12867531/code-breaker-Alan-Turing-Nazi-secrets-dies-aged-98.html</link>
            <guid>38654375</guid>
            <pubDate>Fri, 15 Dec 2023 14:17:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dailymail.co.uk/news/article-12867531/code-breaker-Alan-Turing-Nazi-secrets-dies-aged-98.html">https://www.dailymail.co.uk/news/article-12867531/code-breaker-Alan-Turing-Nazi-secrets-dies-aged-98.html</a>, See on <a href="https://news.ycombinator.com/item?id=38654375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p>Tributes have been paid to a female code breaker who helped Alan Turing reveal the Nazi war machine's encrypted secrets during the&nbsp;<a target="_self" href="https://www.dailymail.co.uk/news/world-war-ii/index.html" id="mol-41a53fd0-9b28-11ee-b875-4dff807a2589">Second World War</a> after she died aged 98.</p><p>Mary Ratcliffe worked at a secret code breaking base in Middlesex, helping to decipher coded messages intercepted from the Nazis.</p><p>She&nbsp;decoded messages which were encrypted by German Enigma machines using Bombe machines invented by <a target="_self" href="https://www.dailymail.co.uk/news/article-12435889/Alan-Turing-OBE-King-George-VI-returned.html">Alan Turing</a> at Bletchley Park.</p><p>Many historians have credited Turing's work with shortening the war and say that he and the people who operated his machines saved millions of lives.</p><p>The main base for codebreaking was at Bletchley Park in Milton Keynes, but Mary's site in Eastcote was one of several others established to ensure that if one was bombed or sabotaged, the rest would still be operational.</p><div>  <p>Tributes have been paid to a code breaker&nbsp;Mary Ratcliffe who helped Alan Turing reveal the Nazi war machine's encrypted secrets during the Second World War after she died aged 98</p></div><div>  <p>Mrs Ratcliffe worked at a secret code breaking base in Middlesex, helping to decipher&nbsp;coded messages intercepted from the Nazis</p></div><p>Throughout the decades that followed, Mrs Ratcliffe become a familiar face in her home town of Swindon, Wiltshire, thanks to her willingness to support good causes and her years of public appearances dressed as <a target="_self" href="https://www.dailymail.co.uk/news/queen-victoria/index.html" id="mol-41727000-9b28-11ee-b875-4dff807a2589">Queen Victoria</a>.</p><p>According to her family, she took great pride in her work, but spoke very little about it due to the secrecy that surrounded the profession.</p><p>She later worked as an acupuncturist and was most famed for her portrayal of Queen Victoria at local events, which she did for 30 years.</p><p>She opened fetes, appeared in parades and graced Swindon with her presence as Queen Victoria, free of charge, at the request of event organisers.</p><p>When word of her handwritten royal tributes and Queen Victoria portrayal reached Buckingham Palace, she was invited to meet the living royals herself at Queen Elizabeth II's Garden Party.</p><p>In 2008, Mrs Ratcliffe then became one of the first-ever recipients of the Pride of Swindon award for her work doing soup runs for the homeless with the Simon Community, and her campaigns for various social causes.</p><p>Paying tribute, her family said: 'Whether as Mary or Queen Victoria, she championed underdogs with eloquent ferocity and actively supported humanitarian causes ranging from elder abuse to homelessness.</p><p>'She tackled grave issues, where others feared to tread and as such was always true to herself.'</p><p>Mrs Ratcliffe moved to Kings Court Care Centre in her 90s after an accident left her in need of care, and it was there that she died on November 29, aged 98.</p><p>She now leaves behind three adult children and many grandchildren who say they will sorely miss her warm presence and appetite for life.</p><p>'She was fiercely independent and climbed the stairs to bed until the very last of her life,' her family have said.</p><p>Her family would like to say a particular heartfelt thanks to the staff at Kings Court Care Centre who cared for Mrs Ratcliffe right up until her last day.</p><p>In a previous interview, Mrs Ratcliffe told her local paper the Swindon Advertiser about her wartime exploits.</p><div>  <p>Mrs Ratcliffe used the Bombe machines invented by Alan Turing (pictured), credited with shortening the war by helping decipher messages produced by German Enigma enciphering devices.</p></div><div>  <p>The registration room in hut 6 at Bletchley Park, Buckinghamshire where codebreakers used Bombe machines to break the German enigma</p></div><p>She said: 'Joining the Women's Royal Naval Service at 19 was a defining moment for me.</p><p>'At the Mill Hill recruiting station in North London I was interviewed and assigned to a base.</p><p>'I was not told where I was going, or the nature of the work I would be doing.</p><p>'We were bundled into an Army lorry. The flap was pulled down. Our 'secret' destination was Eastcote, in Middlesex.</p><p>'We were immediately taken into a room where we were instructed to take the Oath of Allegiance to our God, King and country.</p><p>'Our vow of silence was absolute. We were not allowed to discuss our work with anybody. We were not allowed to wear a category badge; if asked, we were told to say we were recruits, which, of course, would not stimulate any further interest.</p><p>'The 30 years vow of silence was sacrosanct, even after the end of the war.'</p><p>Bletchley Park is now a major heritage attraction which houses a refurbished Bombe, but speaking previously Mrs Ratcliffe said she had clear memories of operating their banks of drums in earnest.</p><p>The work was constant and done in rolling eight-hour shifts.</p><p>She added: 'Our task was to follow a menu that instructed the setting of each drum on which the letters of the alphabet were displayed.</p><p>'There were nine rows of coloured drums on every Bombe machine. Each time it stopped, the position of the drums was recorded on the checking machine before restarting the Bombe machine.</p><p>'A team of technicians was assigned to every bay. The daylight lighting was sometimes a strain.</p><p>'Many colleagues found the work boring, but for me, the rhythm of the drums stimulated my creative thoughts. Many amongst us were mavericks or eccentrics. Both apply to me!</p><p>'We were not told what we had achieved. All our successful decoding was immediately wired back to Bletchley Park.'</p><div>  <p>A working replica of one of Turing's Bombe machines which was used to break the Nazi enigma</p></div><p>She also had vivid memories of VE Day: 'The atmosphere was euphoric. We made our way towards the Mall.'</p><p>The group were offered a lift by some young men who had a horse-dawn cart.</p><p>Mrs Ratcliffe added: 'So, in style, we made our way towards Buckingham Palace where the Royal Family were on the balcony with Winston Churchill, who was then left alone so that we could loudly applaud him for his unique, inspiring leadership in defence of our precious core freedoms throughout six years of conflict, that had claimed so many lives who were the creme de la creme of our nation.'</p><p>Mrs Ratcliffe visited Bletchley Park and wrote a tribute to Alan Turing in the form of a poem. Copies were sent to Bletchley Park, GCHQ and the author of a book about Turing's work.</p><mol-permabox id="mol-a4b6a820-9b5a-11ee-bf66-c75e34c59e82"><div data-version="2" id="mol-a7d0d1e0-e2ad-11e8-a337-f9af42da1b43" data-permabox-url="/sciencetech/fb-6364119/WHO-ALAN-TURING.html"><h3><a href="https://www.dailymail.co.uk/sciencetech/fb-6364119/WHO-ALAN-TURING.html">Who was Alan Turing? Pioneering scientist who helped crack Hitler's enigma machine only to be convicted for homosexuality after WWII</a></h3><div><div>  <p>Alan Turing (pictured) was a British mathematician best known for his work cracking the enigma code during the Second World War</p></div><p>Alan Turing was a British mathematician born on June 23, 1912 In Maida Vale, London, to father Julius, a civil servant, and mother Ethel, the daughter of a railway engineer.&nbsp;</p><p>His talents were recognised early on at school but he struggled with his teachers when he began boarding at Sherborne School aged 13 because he was too fixated on science.&nbsp;</p><p>Turing continued to excel at maths but his time at Sherborne was also rocked by the death of his close friend Christopher Morcom from tuberculosis. Morcom was described as Turing's 'first love' and he remained close with his mother following his death, writing to her on Morcom's birthday each year.&nbsp;</p><p>He then moved on to Cambridge where he studied at King's College, graduating with a first class degree in mathematics.&nbsp;&nbsp;</p><p>During the Second World War, Turing was pivotal in cracking the Enigma codes used by the German military to encrypt their messages.</p><p>His work gave Allied leaders vital information about the movement and intentions of Hitler’s forces.</p><p>Historians credit the work of Turing and his fellow codebreakers at Bletchley Park in Buckinghamshire with shortening the war by up to two years, saving countless lives, and he was awarded an OBE in 1946 for his services.&nbsp;</p><p>Turing is also widely seen as the father of computer science and artificial intelligence due to his groundbreaking work in mathematics in the 1930s.</p><p>He was able to prove a 'universal computing machine' would be able to perform equations if they were presented as an algorithm - and had a paper published on the subject in 1936 in the&nbsp;Proceedings of the London Mathematical Society Journal when he was aged just 23.&nbsp;</p><p>But he was disgraced in 1952 when he was convicted for homosexual activity, which was illegal at the time and would not be decriminalised until 1967.</p><p>To avoid prison, Turing agreed to ‘chemical castration’ – hormonal treatment designed to reduce libido.</p><p>As well as physical and emotional damage, his conviction had led to the removal of his security clearance and meant he was no longer able to work for GCHQ, the successor to the Government Code and Cypher School, based at Bletchley Park.&nbsp;</p><div>  <p>Turing was awarded an OBE in 1946 for his codebreaking work at Bletchley Park, pictured, which is credited with ending World War II two years early</p></div><p>Then In 1954, aged 41, he died of cyanide poisoning. An inquest recorded a verdict of suicide, although his mother and others maintained that his death was accidental.&nbsp;</p><p>When his body was discovered, an apple laid half-eaten next to his bed. It was never tested for cyanide but it is speculated it was the source of the fatal dose.&nbsp;</p><p>Some more peculiar theories suggest Turing was 'obsessed' with fairytale Snow White and the Seven Dwarfs and his death was inspired by the poisoned apple in the story.&nbsp;</p><p>Following a public outcry over his treatment and conviction, the then Prime Minister Gordon Brown issued a public apology in 2009.&nbsp;</p><p>He then received a posthumous Royal pardon in 2014, only the fourth to be issued since the end of the Second World War.</p><p>It was requested by Justice Secretary Chris Grayling, who described Turing as a national hero who fell foul of the law because of his sexuality.</p><p>An e-petition demanding a pardon for Turing had previously received 37,404 signatures.&nbsp;</p><p>A 2017 law, that retroactively pardoned all men cautioned or convicted for homosexual acts under historical legislation, was named in his honour.&nbsp;</p> </div></div></mol-permabox></div></div>]]></description>
        </item>
    </channel>
</rss>