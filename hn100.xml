<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 29 Oct 2023 01:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Elixir and Phoenix can do it all (129 pts)]]></title>
            <link>https://fly.io/phoenix-files/elixir-and-phoenix-can-do-it-all/</link>
            <guid>38052864</guid>
            <pubDate>Sat, 28 Oct 2023 19:57:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/phoenix-files/elixir-and-phoenix-can-do-it-all/">https://fly.io/phoenix-files/elixir-and-phoenix-can-do-it-all/</a>, See on <a href="https://news.ycombinator.com/item?id=38052864">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
         <dl>
             <dt>Author</dt>
             <dd>
                 <img alt="Jason Stiebs" src="https://fly.io/static/images/jason.webp">
               <dl>
                 <dt>Name</dt>
                 <dd>
                   Jason Stiebs
                 </dd>
                   <dt>Twitter</dt>
                   <dd>
                     <a href="https://twitter.com/peregrine" target="_blank">
                       @peregrine
                     </a>
                   </dd>
               </dl>
             </dd>
         </dl>

        <section>
            <figure>
                <img src="https://fly.io/phoenix-files/elixir-and-phoenix-can-do-it-all/assets/elixir-cover.webp" alt="A fly bird wearing indiana jones clothing, in an ancient tomb discovering elixir and a lightsaber.">
                <figcaption>
                  <span>Image by</span>
                  
<svg role="img" style="pointer-events: none; width: 17px; height: 17px;" viewBox="0 0 20 20" fill="currentColor" fill-rule="evenodd">
  <g buffered-rendering="static">
    <path fill-rule="evenodd" d="M1 8a2 2 0 012-2h.93a2 2 0 001.664-.89l.812-1.22A2 2 0 018.07 3h3.86a2 2 0 011.664.89l.812 1.22A2 2 0 0016.07 6H17a2 2 0 012 2v7a2 2 0 01-2 2H3a2 2 0 01-2-2V8zm13.5 3a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM10 14a3 3 0 100-6 3 3 0 000 6z" clip-rule="evenodd"></path>
  </g>
</svg>

                    <a href="https://annieruygtillustration.com/" target="_blank">
                      Annie Ruygt
                    </a>
                </figcaption>
            </figure>
          <p>We’re Fly.io. We run apps for our users on hardware we host around the world. Fly.io happens to be a great place to run Phoenix applications. Check out how to <a href="https://fly.io/docs/elixir/">get started</a>!</p>
<p>As an Elixir/Phoenix developer going on 10 years it is very easy to take for granted everything that Elixir and Phoenix can do for us. So I wanted to take a step back and list all of the stuff that we get for free when we choose Elixir and Phoenix.</p>

<p>My goal with this post is dump an list of information and links and let you choose your own path. If you are unfamiliar with any of these topics or Elixir and Phoenix please click any of these links and take a peak, I think you might be surprised at the quality and depth of what you find!</p>

<p>The inspiration for this post was a <a href="https://twitter.com/wojtekmach/status/1710020808523846087">Saša Jurić</a> re-tweet where he shared the following slide from his <a href="https://www.youtube.com/watch?v=JvBT4XBdoUE">GOTO Conference Talk</a>.</p>

<p><img src="https://fly.io/phoenix-files/elixir-and-phoenix-can-do-it-all/assets/elixir-does-all.jpeg"></p>

<p>This image enumerates some of the technical requirements that the <a href="https://www.erlang.org/">Erlang Virtual Machine</a> can replace in your tech stack. Not to say that these other projects can’t do these things well, they can and do in our own stack here at <a href="https://fly.io/">Fly.io</a>. The point is that in those ecosystems you <em>have</em> to solve these problems in the first place, when reviewing this article José Valim said it best:</p>

<blockquote>
<p>removing the problem altogether instead of solving the problem</p>
</blockquote>
<h2 id="the-beam"><a href="#the-beam" aria-label="Anchor"></a>The BEAM</h2>
<p>This above list doesn’t even cover the built in support for incredible bits like:</p>

<ul>
<li>Actor Process Model, meaning do to concurrency you have a Process and it sends messages to other processes. No Shared memory. Fully managed by the BEAM’s built in scheduler. Millions of processess are no issue. Fearless concurrency!
</li><li>Distribution: meaning two servers can communicate seamlessly and automatically. It <em>just</em> works and <a href="https://fly.io/phoenix-files/beam-clustering-made-easy/">is incredible</a>.
</li><li>RPC: Calling a function or process remotely is as easy as calling a local one, no need to concern ourselves with connections or serialization or deserialization. This is one of the many reasons Fly is so committed to using Elixir and Phoenix, the <a href="https://fly.io/docs/reference/private-networking/">wireguard based network</a> and globally hosting makes using this trivial and desirable.
</li><li>Parallel Garbage Collection, meaning the VM is Soft Real Time, minimal pausing!
</li><li>Emphasis on low resource usage in general
</li><li>Built in UDP/TCP/SSL server support.
</li><li>Built in Key Value Store <a href="https://www.erlang.org/doc/man/ets.html">ETS</a>.
</li><li>Tooling for <a href="https://www.erlang.org/doc/apps/erts/tracing">tracing</a> and <a href="https://www.erlang.org/doc/man/observer">observing</a> your code.
</li><li>37 Years of constant development and production use.
</li></ul>

<p>The beauty of this list is that if Elixir and Phoenix team have done our Job right you don’t need to think about any of this at all. It just works for you and lets you stand on the shoulders of this giant.</p>
<h3 id="elixir"><a href="#elixir" aria-label="Anchor"></a>Elixir</h3>
<p>If we go higher level the Elixir programming language provides us even more as developers:</p>

<ul>
<li>A full dependency management and build tool with <a href="https://elixir-lang.org/getting-started/mix-otp/introduction-to-mix.html">mix</a> and <a href="https://hex.pm/">Hex</a>.
</li><li>With modern tooling built in:

<ul>
<li><a href="https://hexdocs.pm/ex_unit/1.15/ExUnit.html">Testing</a>
</li><li><a href="https://github.com/elixir-lang/ex_doc">Documentation</a>
</li><li>Formatting
</li></ul>
</li><li>Language with a clean, <a href="https://fly.io/phoenix-files/elixir-docs-are-built-different/">documented</a> and fully exampled <a href="https://hexdocs.pm/elixir/1.15.7/Kernel.html">Standard Library</a>. Its remarkable how <em>little</em> cruft there is in this Language. Go into any other language to find random functions or modules that are marked <em>do not use</em>, or without examples or documentation. Not the case for Elixir.
</li><li>Modern <a href="https://fly.io/phoenix-files/elixir-docs-are-built-different/">Documentation</a>:

<ul>
<li>Built in Cheatsheets
</li><li>Built in Custom Markdown
</li><li>Links to source
</li><li>Search
</li><li>Support for Mermaid, Math.js, Vega-Lite and more!
</li></ul>
</li><li>Modern language features like:

<ul>
<li><a href="https://elixir-lang.org/getting-started/protocols.html">Protocols</a> enabling high level collections and functions like <a href="https://hexdocs.pm/elixir/1.15.7/Enum.html">Enum</a>, <a href="https://hexdocs.pm/elixir/1.15.7/Stream.html">Streams</a>, and <a href="https://hexdocs.pm/elixir/1.15.7/Access.html">Access</a>
</li><li><a href="https://hexdocs.pm/elixir/1.15.7/Stream.html">Stream</a>, enabling lazy and performant data processing
</li><li>Full featured <a href="https://hexdocs.pm/elixir/1.15.7/Date.html">Date</a>/<a href="https://hexdocs.pm/elixir/1.15.7/Time.html">Time</a>/<a href="https://hexdocs.pm/elixir/1.15.7/DateTime.html">DateTime</a>/<a href="https://hexdocs.pm/elixir/1.15.7/Calendar.html">Calendar</a> modules.
</li><li><a href="https://hexdocs.pm/elixir/1.15.7/Task.html">Task</a> for trivial parallel processing.
</li><li><a href="https://hexdocs.pm/elixir/1.15.7/Agent.html">Agent</a> for trivial shared state management.
</li><li><a href="https://hexdocs.pm/elixir/1.15.7/Config.html">Config</a> for environment specific configuration
</li><li><a href="https://hexdocs.pm/elixir/1.15.7/Registry.html">Registry</a> for local, decentralized and scalable key-value process storage.
</li></ul>
</li></ul>

<p>This is just a subset of what Elixir the Language provides. It doesn’t include the <em>deep</em> catalog of libraries within package registry <a href="https://hex.pm/">Hex</a> enabling so much more.</p>

<p>Some examples from Hex:</p>

<ul>
<li>Machine Learning with <a href="https://github.com/elixir-nx/bumblebee">Bumblebee</a> with HuggingFace integration and it works with batching and distribution globally.
</li><li>Math/Science comparable to NumPy using <a href="https://github.com/elixir-nx/scholar">Scholar</a>.
</li><li>Complex distributed big data processing using <a href="https://hexdocs.pm/gen_stage/GenStage.html">GenStage</a> or <a href="https://github.com/dashbitco/broadway">Broadway</a>
</li><li>Embedded and IoT Programming using <a href="https://nerves-project.org/">Nerves</a>.
</li><li><a href="https://hex.pm/packages/stream_data">Stream-Data</a> for property and generative based testing.
</li><li><a href="https://hex.pm/packages/sobelow">Sobelow</a> for security and static Analysis.
</li><li><a href="https://hex.pm/packages/oban">Oban</a> for a Worker/Job Queue implementation.
</li><li><a href="https://hex.pm/packages/req">Req</a> for high level HTTP Clients.
</li><li><a href="https://github.com/elixir-plug/plug/">Plug</a> for HTTP Servers.
</li><li> <a href="https://github.com/mtrudel/bandit">Bandit</a> for pure Elixir HTTP1/2 Server!
</li><li><a href="https://livebook.dev/">LiveBook</a> for Juypter like workbooks with Elixir!
</li><li><a href="https://membrane.stream/">Membrane</a> for video stream processing.
</li><li><a href="https://hexdocs.pm/decimal/readme.html">Decimal</a> for arbitrary precision decimal arithmetic.
</li><li><a href="https://hexdocs.pm/jason/readme.html">Jason</a> for highly performant JSON Encoding/Decoding.
</li><li><a href="https://hex.pm/packages/image">Image</a> for image manipulation.
</li><li><a href="https://github.com/elixir-cldr/cldr">CLRD</a> for maybe the <em>most</em> complete Internationalization and Locale libraries outside of maybe the Web Browser. Numbers, List, Units, Date\Time\DateTimes, Collation, Territories, and more.
</li><li><a href="https://github.com/rusterlium/rustler">Rustler</a> and <a href="https://github.com/E-xyza/zigler">Zigler</a> for truly simple Rust and Zig FFI support, for when you need to drop down and bash bits.
</li></ul>

<p>There is very little that the Elixir Ecosystem hasn’t tackled!</p>
<h2 id="phoenix-and-liveview"><a href="#phoenix-and-liveview" aria-label="Anchor"></a>Phoenix and LiveView</h2>
<p>The fun doesn’t stop there because Phoenix simply builds on all of his incredible tooling to provide us with:</p>

<ul>
<li>A fully featured HTTP library with

<ul>
<li>Advanced <a href="https://hexdocs.pm/phoenix/Phoenix.Router.html">Routing</a>
</li><li><a href="https://hexdocs.pm/phoenix/Phoenix.Router.html#module-pipelines-and-plugs">Plug-able steps</a>
</li><li>Built in <a href="https://hexdocs.pm/phoenix/components.html">HTML</a> and <a href="https://hexdocs.pm/phoenix/json_and_apis.html">JSON</a> handling
</li><li>With <a href="https://hexdocs.pm/phoenix/using_ssl.html">HTTPS</a> with sane security defaults
</li><li><a href="https://hexdocs.pm/phoenix/mix_phx_gen_auth.html#tracking-sessions">Secure Cookie Sessions</a>
</li><li><a href="https://hexdocs.pm/phoenix/Phoenix.Token.html">Secure Tokens</a>
</li><li><a href="https://hexdocs.pm/phoenix/controllers.html">Controller</a> Actions for MVC Style apps.
</li><li>Built in <a href="https://hexdocs.pm/gettext/Gettext.html">Internationalization Tooling</a>
</li></ul>
</li><li>Websockets with

<ul>
<li><a href="https://hexdocs.pm/phoenix/channels.html">Multiplexing</a> via Channels
</li><li>JavaScript library that handles backoff and common error modes
</li><li>Serialization
</li><li><a href="https://hexdocs.pm/phoenix/Phoenix.Socket.html#module-socket-fields">Sessions</a>
</li><li><a href="https://www.phoenixframework.org/blog/the-road-to-2-million-websocket-connections">Scalable to millions of clients.</a>
</li><li><a href="https://hexdocs.pm/phoenix_pubsub/Phoenix.PubSub.html">PubSub</a>Fully Distributed
</li><li>With optimizations for “hot paths”
</li><li><a href="https://hexdocs.pm/phoenix/Phoenix.Presence.html">Presence</a> Monitoring and tracking <em>who</em> is in a channel
</li><li>Fully Distributed
</li><li>Built using CRDT’s
</li><li>JavaScript support
</li></ul>
</li><li>Development Tooling:

<ul>
<li><a href="https://hexdocs.pm/phoenix/Phoenix.CodeReloader.html">Live Code Reloading</a>
</li><li><a href="https://hexdocs.pm/plug/Plug.Debugger.html">Helpful and actionable</a> Errors
</li><li>Built in End to End Testing. (No Chromedriver needed)
</li></ul>
</li><li>JavaScript

<ul>
<li>Bundling/Minification/Integrity/Cache Busting

<ul>
<li>Using <a href="https://hex.pm/packages/esbuild">esbuild</a> that is installed for you
</li></ul>
</li><li>NPM support
</li><li>Custom JS is trivial
</li></ul>
</li><li>CSS via Tailwind

<ul>
<li>Bundling/Minification/Integrity/Cache Busting

<ul>
<li>Using <a href="https://hex.pm/packages/tailwind">tailwind</a> that is installed for you
</li></ul>
</li><li>Custom CSS is trivial
</li></ul>
</li><li><a href="https://hexdocs.pm/phoenix_live_view/welcome.html">LiveView</a>

<ul>
<li>Simple, server rendered Live HTML and CSS, in pure Elixir.
</li><li>HTML and Html Attribute Compile Time <a href="https://hexdocs.pm/phoenix/components.html#heex">Verification</a>
</li><li><a href="https://dashbit.co/blog/latency-rendering-liveview">Highly Optimized</a> for the modern web
</li><li>Debug tooling for Dev
</li><li><a href="https://hexdocs.pm/phoenix_live_view/js-interop.html">JS Interop</a> is Trivial
</li><li><a href="https://hexdocs.pm/phoenix_live_view/js-interop.html#simulating-latency">Latency Simulation</a>
</li><li><a href="https://fly.io/phoenix-files/a-liveview-is-a-process/">Single BEAM Process</a> per connection
</li><li>Trivial <a href="https://hexdocs.pm/phoenix/file_uploads.html">File Uploads</a> to disk or the cloud.
</li></ul>
</li><li>Databases with <a href="https://hexdocs.pm/ecto/Ecto.html">Ecto</a>

<ul>
<li><a href="https://github.com/elixir-ecto/ecto_sql">Postgres/MySQL</a>/<a href="https://github.com/elixir-sqlite/ecto_sqlite3">Sqlite</a>/and <a href="https://hex.pm/packages?search=ecto&amp;sort=recent_downloads">more</a>
</li><li><a href="https://hexdocs.pm/ecto/Ecto.Query.html">Full Query DSL</a> with <a href="https://hexdocs.pm/ecto_sql/Ecto.Adapters.SQL.html#query!/4">Raw SQL</a> escape <a href="https://hexdocs.pm/ecto/Ecto.Query.html#module-fragments">hatch</a>
</li><li><a href="https://hexdocs.pm/ecto_sql/Ecto.Migration.html">Migrations Up/Down/Schema dump</a>
</li><li><a href="https://hexdocs.pm/ecto/Ecto.Schema.html">Full Schema</a> and <a href="https://hexdocs.pm/ecto/Ecto.Changeset.html">Change</a> Validation
</li><li><a href="https://hexdocs.pm/ecto/Ecto.Repo.html#transaction-api">Transaction</a> Support
</li><li><a href="https://hexdocs.pm/ecto/Ecto.Multi.html">Multi-step</a> Queries/Transactions
</li></ul>
</li><li><a href="https://hexdocs.pm/phoenix/mix_phx_gen_auth.html">Full Authorization/Authentication generator</a>

<ul>
<li>Register, Login, Forgot Password, Email Verification and Sessions
</li><li>LiveView or Server HTML
</li></ul>
</li><li><a href="https://github.com/swoosh/swoosh">Email Rendering and Sending</a>
</li><li>Built in <a href="https://hexdocs.pm/phoenix/telemetry.html#metrics">Metrics Tracking</a>
</li><li>Single Line to add a full <a href="https://github.com/phoenixframework/phoenix_live_dashboard">Metrics Dashboard</a>
</li><li><a href="https://hexdocs.pm/phoenix/releases.html#containers">Dockerfile generator</a>
</li></ul>

<p>Just going through this list there is everything you’d need to start a company or build a website to solve nearly any problem. Coupled with the BEAM’s ability to scale from the smallest server to a globally distributed network with millions of customers on every continent! I may sound a little breathless but I am out of a breath just collating all of these links.</p>
<h2 id="wrap-up"><a href="#wrap-up" aria-label="Anchor"></a>Wrap up</h2>
<p>Stepping back it’s incredible that a small team is able to accomplish so much. Every one of Phoenix’s incredible features is built on the shoulders of the fantastic Elixir project underneath. Further backed by 37 years of constant development by the Erlang project.</p>

<p>If you haven’t looked into Phoenix or Elixir I urge you to explore any one of the above links that catch your eye. Installing and setting up Elixir has never been easier with <a href="https://livebook.dev/">LiveBook</a> or <a href="https://elixir-lang.org/install.html#by-operating-system">Natively</a>!</p>
<figure>
  <figcaption>
    <h2>Fly.io ❤️ Elixir</h2>
    <p>Fly.io is a great way to run your Phoenix LiveView apps. It’s really easy to get started. You can be running in minutes.</p>
    <a href="https://fly.io/docs/elixir/">
      Deploy a Phoenix app today!  <span>→</span>
    </a>
  </figcaption>
  <p><img src="https://fly.io/static/images/cta-turtle.webp" srcset="https://fly.io/static/images/cta-turtle@2x.webp 2x" alt="">
  </p>
</figure>


          
        </section>
        <dl>
            <dt>
              Previous post  ↓
            </dt>
            <dd>
              <a href="https://fly.io/phoenix-files/chatgpt-doesnt-know-what-day-it-is/">
                ChatGPT doesn't know what day it is?
              </a>
            </dd>
        </dl>
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FCC wants to bolster amateur radio (236 pts)]]></title>
            <link>https://www.radioworld.com/news-and-business/business-and-law/fcc-wants-to-bolster-amateur-radio</link>
            <guid>38052577</guid>
            <pubDate>Sat, 28 Oct 2023 19:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.radioworld.com/news-and-business/business-and-law/fcc-wants-to-bolster-amateur-radio">https://www.radioworld.com/news-and-business/business-and-law/fcc-wants-to-bolster-amateur-radio</a>, See on <a href="https://news.ycombinator.com/item?id=38052577">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
		<p><span>FCC Chairwoman Jessica Rosenworcel says the FCC plans to “incentivize innovation and experimentation in the amateur radio bands” by&nbsp;getting rid of outdated restrictions&nbsp;and providing licensees with the flexibility to use modern digital emissions.</span></p>
<p><span>The commission at its November meeting is expected to take action on a Report and Order that would eliminate the baud rate limitation and establish a bandwidth limitation in the amateur radio bands below 29.7 MHz.&nbsp;</span></p>
<p><span>The order being circulated for tentative consideration by the commission would remove the baud rate limitation — the rate at which the carrier waveform amplitude, frequency and/or phase is varied to transmit information — for data emissions in the amateur radio bands, the FCC says. The current baud rate limits were adopted in 1980.</span></p>
<p><span>The order would implement a 2.8 kilohertz bandwidth limitation in place of the baud rate in amateur radio bands. The 2.8 kHz limitation is consistent with the commission’s treatment of other wireless radio services, the FCC says.</span></p>
<p><span>The current rules limit the baud rate for high-frequency amateur radioteletype/data transmissions to 300 baud for frequencies below 28 MHz (except in the 60-meter band), and 1200 baud in the 10 meter (28-29.7 MHZ) band.</span></p>
<p><span>The Wireless Telecommunications Bureau says the change in technical standards would allow the amateur radio service to operate more efficiently, including during times of emergency to support public safety.</span></p>
<p><span>The American Radio Relay League (ARRL) in 2013 asked the commission to delete references to the baud rate and to establish a bandwidth limitation of 2.8 kHz. The group argued the public safety benefits of making the change. ARRL stated: “[i]ncreasing speed is especially important when amateurs voluntarily assist during and after hurricanes, forest fires and other disasters.”</span></p>
<p><span>At the time, ARRL also told the FCC that eliminating the baud rate limitation will “incentivize innovation by allowing more data to be transmitted within each signal without increasing bandwidth from that currently used.”</span></p>
<p><span>A spokesperson for ARRL says the organization supports the proposed action and the <a href="https://www.radioworld.com/wp-content/uploads/2023/10/DOC-397992A1.pdf" target="_blank" rel="noopener">proposal of further notice.</a>&nbsp;</span></p>
<p><span>In a subsequent Notice of Proposed Rulemaking in 2016 (WT Docket No. 16-239), the FCC tentatively concluded that a 2.8 kilohertz bandwidth limitation for radioteletype and data emissions in the MF/HF bands was not necessary, and sought comment.&nbsp;</span></p>
<p><a href="https://www.radioworld.com/news-and-business/business-and-law/"><b><i>[See Our Business and Law Page]</i></b></a></p>
<p><span>A few commenters at the time of the NPRM opposed any rule change, arguing that the existing rules should be retained in order to protect access to amateur bands by Morse code and other narrowband transmissions.</span></p>
<p><span>However, the commission writes in the order: “Based on the record in this proceeding, we find that the baud rate limitation has become outdated and hampers, rather than promotes, innovation and robust use of the amateur bands.”</span></p>
<p><span>The commission continued in the new order: “We are persuaded by the weight of the record in this proceeding that, without a baud rate or bandwidth limit, data stations using a large amount of spectrum for a single emission could do so to the detriment of simultaneous use by other stations using narrowband emission modes.”&nbsp;</span></p>
<p><span>In essence, the technical change mean amateurs will require less time to transmit messages, which in turn will open up more spectrum in the time domain for more amateurs to use, said David Siddall, general counsel for ARRL.&nbsp;</span></p>
<p><span>“This is a very simple change. In 1980, at the inception of digital technologies that could be used by radio amateurs, the FCC adopted a speed limit of 300 baud for the stated purpose of limiting the amount of spectrum occupied by any single signal,” Siddall said. “Radio amateurs, being tinkerers and experimenters, worked to develop faster and faster speeds that still fit within the standard spectrum bandwidth. Eventually their innovations to the technology significantly increased spectrum efficiency but ran up against the FCC baud rate limit.”</span></p>
<p><span>One of the benefits of the changes will be allowing for “faster emergency communications” by volunteer ham radio operators during emergencies, the FCC says.</span></p>
<p><span>The agency says its Wireless Telecommunications Bureau’s Mobility Division has previously issued waivers allowing amateur operators directly involved with disaster relief efforts to exceed the baud rate limitation in the interest of public safety.</span></p>
<p><span>The FCC at its November meeting will also consider a Further Notice of Proposed Rulemaking (FNPRM) that proposes to remove the baud rate limitation in the 2200 meter and 630 meter bands. The commission also proposes to remove the baud rate limitation in the very high frequency (VHF) and ultra-high frequency (UHF) bands.&nbsp;</span></p>
<p><span>The commission says it expects to seek comment on the appropriate bandwidth limitation for the 2200 meter band, the 630 meter band, and the VHF and UHF bands.</span></p>
<p><span>Steve Stroh, editor of amateur radio newsletter Zero Retries, says the need for improved data communications in amateur radio also coincides with spectrum becoming more “noisy” due to “pollution” by systems such as LED lighting, small switching power supplies and even solar panels.&nbsp;</span></p>
<p><span>“That noise has an outsize impact on analog modes such as voice and very low power transmissions. Improved data communications modes, including digital voice modes, can overcome the noise issues,” Stroh said in an email to Radio World.</span></p>
<p><span>Stroh says he is happy to see the FCC address the same limitation on the amateur radio VHF and UHF bands —&nbsp;where there is arguably much greater potential for technological innovation in data communications technology — if it wasn’t for the data rate and mode limitations.</span></p>
<p><span>“Fortunately, in its proposal, the FCC recognizes that the symbol rate and mode issue does include the Amateur Radio VHF and UHF bands. Thus the FCC’s proposal is a very good one that will significantly benefit Amateur Radio,” he said.&nbsp;</span></p>
<p><span>The FCC’s next meeting is scheduled for November 15. A comment period on the <a href="https://www.radioworld.com/wp-content/uploads/2023/10/DOC-397992A1.pdf" target="_blank" rel="noopener">FNPRM</a> will commence 30 days after the date of publication in the Federal Register.&nbsp;</span></p>
<p><a href="https://www2.smartbrief.com/signupSystem/subscribe.action?pageSequence=1&amp;briefName=RW&amp;campaign=pm_optin_promo_website_RW"><b><i>[Sign Up for Radio World’s SmartBrief Newsletter]</i></b></a></p>
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arena Allocation in SBCL (137 pts)]]></title>
            <link>https://github.com/sbcl/sbcl/blob/master/doc/internals-notes/arena-allocation.txt</link>
            <guid>38052564</guid>
            <pubDate>Sat, 28 Oct 2023 19:14:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sbcl/sbcl/blob/master/doc/internals-notes/arena-allocation.txt">https://github.com/sbcl/sbcl/blob/master/doc/internals-notes/arena-allocation.txt</a>, See on <a href="https://news.ycombinator.com/item?id=38052564">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:sbcl/sbcl" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Mnx9tRbarTOt8ngCvgvDHpPMwnPOcyotD1Y-tGLHJ3hWfsevvWeUFl1JpEwpb9VJLEVz0HTZ5Ro8PFLfRyEwfg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="sbcl/sbcl" data-current-org="sbcl" data-current-owner="" data-logged-in="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=sbcl%2Fsbcl" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/sbcl/sbcl/blob/master/doc/internals-notes/arena-allocation.txt&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="75976cb7304d17e24f1210cf14eadc28bcd586b4271b511ee93c19d50712ce9c" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: OpenSign – Open source alternative to DocuSign (143 pts)]]></title>
            <link>https://github.com/OpenSignLabs/OpenSign</link>
            <guid>38052344</guid>
            <pubDate>Sat, 28 Oct 2023 18:47:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/OpenSignLabs/OpenSign">https://github.com/OpenSignLabs/OpenSign</a>, See on <a href="https://news.ycombinator.com/item?id=38052344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-opensign" dir="auto"><a href="#opensign"></a><a href="https://www.opensignlabs.com/" rel="nofollow">OpenSign™</a></h2>

<h3 tabindex="-1" id="user-content-table-of-contents" dir="auto"><a href="#table-of-contents">Table of Contents</a></h3>
<ol dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#contribution-guidelines">Contribution Guidelines</a></li>
<li><a href="#license">License</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
</ol>
<hr>
<h3 tabindex="-1" id="user-content-introduction" dir="auto"><a href="#introduction">Introduction</a></h3>
<p dir="auto">Welcome to OpenSign, an open-source document e-signing solution designed to provide a secure, reliable, and free alternative to commercial platforms like DocuSign, PandaDoc, SignNow, Adobe Sign, Smartwaiver, SignRequest, HelloSign &amp; Zoho sign. Developed under the OpenSignLabs organization, our mission is to democratize the e-signing process, making it accessible and straightforward for everyone.</p>
<hr>
<h3 tabindex="-1" id="user-content-features" dir="auto"><a href="#features">Features</a></h3>
<ul dir="auto">
<li><strong>Secure Signing</strong>: Utilizes state-of-the-art cryptographic algorithms to ensure the security &amp; integrity of your documents.</li>
<li><strong>User-Friendly Interface</strong>: Designed with usability in mind, making it easy for both technical and non-technical users.</li>
<li><strong>Multi-Platform Support</strong>: Compatible with various browsers and devices.</li>
<li><strong>Invite &amp; collaborate users</strong>: Bring multiple people from your team into the signing process, all within your own infrastructure.</li>
<li><strong>Secure</strong>: Allows for the easy, secure and seamless organization of your documents using 'OpenSigDrive'.</li>
<li><strong>Audit Trails</strong>: Keeps a detailed log of all activities related to the document signing process including IP addresses and access timings.</li>
<li><strong>Completion Certificate</strong>: Generate secure completion certificate as soon as a document is signed by all participants.</li>
<li><strong>API Support</strong>: Provides a robust API for integration into other software and services.</li>
</ul>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/5486116/278610472-b37ff443-7099-4273-9aeb-21c46d7154cf.png"><img src="https://user-images.githubusercontent.com/5486116/278610472-b37ff443-7099-4273-9aeb-21c46d7154cf.png" height="200"></a>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/5486116/278610525-86db91b7-6c2f-4885-a33c-58f4fd35ec89.png"><img src="https://user-images.githubusercontent.com/5486116/278610525-86db91b7-6c2f-4885-a33c-58f4fd35ec89.png" height="200"></a>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/5486116/278610557-56835efd-c40e-42de-a206-20202c293876.png"><img src="https://user-images.githubusercontent.com/5486116/278610557-56835efd-c40e-42de-a206-20202c293876.png" height="200"></a>
<hr>
<h3 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h3>
<p dir="auto">Please refer to the <a href="https://github.com/OpenSignLabs/OpenSign/blob/main/INSTALLATION.md">Installation Guide</a> for detailed instructions on how to install OpenSign on your system.</p>
<hr>
<h3 tabindex="-1" id="user-content-usage" dir="auto"><a href="#usage">Usage</a></h3>
<p dir="auto">For comprehensive guidelines on how to use OpenSign, please consult our <a href="https://github.com/OpenSignLabs/OpenSign/blob/main/USAGE.md">User Manual</a>.</p>
<hr>
<h3 tabindex="-1" id="user-content-contribution-guidelines" dir="auto"><a href="#contribution-guidelines">Contribution Guidelines</a></h3>
<p dir="auto">We welcome contributions from the open-source community. For more information on how to contribute, please read our <a href="https://github.com/OpenSignLabs/OpenSign/blob/main/CONTRIBUTING.md">Contribution Guidelines</a>.</p>
<hr>
<h3 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License</a></h3>
<p dir="auto">OpenSign is licensed under the AGPL License. For more details, see the <a href="https://github.com/OpenSignLabs/OpenSign/blob/main/LICENSE.md">LICENSE</a> file.</p>
<hr>
<h3 tabindex="-1" id="user-content-acknowledgments" dir="auto"><a href="#acknowledgments">Acknowledgments</a></h3>
<p dir="auto">We would like to thank all our contributors and users for their support and feedback. Special thanks to <a href="https://github.com/OpenSignLabs">OpenSignLabs</a> for spearheading this initiative.</p>
<hr>
<h2 tabindex="-1" id="user-content-contributors" dir="auto"><a href="#contributors">Contributors</a></h2>



<table>
  <tbody>
    <tr>
      <td><a href="https://aleksandarjakovljevic.com/" rel="nofollow"><img src="https://avatars.githubusercontent.com/u/2115393?v=4?s=100" width="100px;" alt="Aleksandar Jakovljevic"><br><sub><b>Aleksandar Jakovljevic</b></sub></a><br><a href="#code-ajakov" title="Code">💻</a></td>
      <td><a href="https://github.com/nxglabs"><img src="https://avatars.githubusercontent.com/u/5486116?v=4?s=100" width="100px;" alt="Amol"><br><sub><b>Amol</b></sub></a><br><a href="#code-nxglabs" title="Code">💻</a></td>
      <td><a href="https://github.com/BuilderPrid"><img src="https://avatars.githubusercontent.com/u/106882895?v=4?s=100" width="100px;" alt="Priyanshu Dwivedi"><br><sub><b>Priyanshu Dwivedi</b></sub></a><br><a href="#code-BuilderPrid" title="Code">💻</a></td>
      <td><a href="https://github.com/Tashuuuu"><img src="https://avatars.githubusercontent.com/u/85075827?v=4?s=100" width="100px;" alt="Akriti Sengar"><br><sub><b>Akriti Sengar</b></sub></a><br><a href="#code-Tashuuuu" title="Code">💻</a></td>
      <td><a href="https://github.com/parthrc"><img src="https://avatars.githubusercontent.com/u/101104958?v=4?s=100" width="100px;" alt="Parth Chawande"><br><sub><b>Parth Chawande</b></sub></a><br><a href="#code-parthrc" title="Code">💻</a></td>
      <td><a href="https://github.com/Rishabh-git10"><img src="https://avatars.githubusercontent.com/u/107680241?v=4?s=100" width="100px;" alt="Rishabh Dewangan"><br><sub><b>Rishabh Dewangan</b></sub></a><br><a href="#code-Rishabh-git10" title="Code">💻</a></td>
      <td><a href="https://github.com/LemonDrop847"><img src="https://avatars.githubusercontent.com/u/106615670?v=4?s=100" width="100px;" alt="Nitin Mishra"><br><sub><b>Nitin Mishra</b></sub></a><br><a href="#code-LemonDrop847" title="Code">💻</a></td>
    </tr>
    <tr>
      <td><a href="http://jobinselvanose.com/" rel="nofollow"><img src="https://avatars.githubusercontent.com/u/63976083?v=4?s=100" width="100px;" alt="Jobin Selvanose"><br><sub><b>Jobin Selvanose</b></sub></a><br><a href="#doc-Jobin-S" title="Documentation">📖</a></td>
    </tr>
  </tbody>
</table>



</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[McDonalds is giving free French fries in return for waiving the right to sue (119 pts)]]></title>
            <link>https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/</link>
            <guid>38052212</guid>
            <pubDate>Sat, 28 Oct 2023 18:27:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/">https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/</a>, See on <a href="https://news.ycombinator.com/item?id=38052212">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
    <div>
																	
								
							
				
							

							
				<div data-post-url="https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/" data-post-title="McDonald's New Terms And Conditions Have People Deleting The App" data-slide-num="0" data-post-id="1432093">
											
															<picture>
																			<source media="(min-width: 429px)" srcset="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/intro-1698426237.webp" type="image/webp">
										<source media="(max-width: 428px)" srcset="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/intro-1698426237.sm.webp" type="image/webp">
																		<img src="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/intro-1698426237.jpg" data-slide-url="https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/" data-post-id="1432093" data-slide-num="0" data-slide-title="McDonald's New Terms And Conditions Have People Deleting The App: " width="780" height="438" alt="phone displaying McDonald's mobile app">
								</picture>
																					<p><span>Vladimka production/Shutterstock</span>									</p></div>

																													
				<div>
															<p dir="ltr">Practically everyone has clicked "agree" on terms and conditions they didn't read through. Unfortunately, the latest terms and conditions for using the <a href="https://www.mcdonalds.com/us/en-us/terms-and-conditions.html" target="_blank">McDonald's</a> app contain many&nbsp;customer-affecting changes: updates to McDonald's liability in cases of injury, third-party errors, and app malfunction; waivers for a customer's right to a jury trial or class action lawsuit; and an agreement to solve disputes through a strict arbitration process.</p>

<p dir="ltr">Essentially, the new terms state that, if a customer tries to <a href="https://www.mashed.com/1398637/mcdonalds-sued-hot-coffee-again/" target="_blank">sue over hot coffee</a>, for example, they can't take their case to trial. Rather, as laid out in McDonald's 12-step outline, the customer must notify the company of their intent to seek arbitration, meet with the company to discuss the problem, and only then have an arbitrator enter the equation.&nbsp;As you can imagine, this process eliminates decision-making by a dozen jurors, instead giving a single arbitrator discretion in deciding the outcome. The very nature of the arbitration process makes class action nearly impossible, meaning each customer has to file their own dispute with McDonald's.</p>
<p dir="ltr">All that said, perhaps the most controversial piece of these updates is that there's no way to opt out of accepting the new terms and conditions — no box to check saying you disagree. The only choices? Agree or delete the app.</p>


					
									</div>
					</div>

    <div>
			

							                    <h2>McDonald's customers are opting to delete the app</h2>
				
				<div data-slide-num="1" data-post-id="1432093">
											
																					<picture>
																	<source media="(min-width: 429px)" data-srcset="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/mcdonalds-customers-are-opting-to-delete-the-app-1698426091.webp" type="image/webp">
									<source media="(max-width: 428px)" data-srcset="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/mcdonalds-customers-are-opting-to-delete-the-app-1698426091.sm.webp" type="image/webp">
																<img data-lazy-src="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/mcdonalds-customers-are-opting-to-delete-the-app-1698426091.jpg" data-slide-url="https://www.mashed.com/1432093/mcdonalds-new-app-terms-conditions-reactions/slide/mcdonalds-customers-are-opting-to-delete-the-app/" data-post-id="1432093" data-slide-num="1" data-slide-title="McDonald's New Terms And Conditions Have People Deleting The App: McDonald's customers are opting to delete the app" width="780" height="438" alt="McDonald's customer on phone" src="https://www.mashed.com/img/gallery/mcdonalds-new-terms-and-conditions-have-people-deleting-the-app/mcdonalds-customers-are-opting-to-delete-the-app-1698426091.jpg">
							</picture>
																				<p><span>Sorbis/Shutterstock</span>									</p></div>

				
				<div>
															<p dir="ltr">Understandably, McDonald's customers have been very vocal about the new terms and conditions online, with many saying they're choosing to delete the app rather than sign their legal rights away. Several people pointed out that McDonald's has been pushing its app more frequently in recent months, particularly through app-only offers like <a href="https://www.mashed.com/1427915/mcdonalds-free-french-fry-fridays/" target="_blank">free fries on Fridays</a>. At the same time, one <a href="https://www.tiktok.com/@seansvv/video/7294149648397241642" target="_blank">TikTok</a> user commented that the terms and conditions include a great deal of legal jargon that the "typical fast food patron [can't] read," much less understand. Similarly, a <a href="https://www.reddit.com/r/AskReddit/comments/17fx4b2/comment/k6ct4gh/" target="_blank">Reddit</a> user noted that these terms are intended to stack the deck against anyone who tries to sue McDonald's.</p>

<p dir="ltr">Customers questioned whether or not the terms and conditions are actually legally binding, given that the agreement does not require a signature or date. Unfortunately, if McDonald's has laid out all necessary information and provided app users with the ability to agree or disagree (the latter requiring customers to jump through hoops to delete their account and the app itself), the terms and conditions are assumedly enforceable by law in most states.</p>

					
									</div>
					</div>


</article>


	
	
	
	
	

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fixing the volume on my Bluetooth earbuds (116 pts)]]></title>
            <link>https://blog.ornx.net/post/bluetooth-volume-fix/</link>
            <guid>38051488</guid>
            <pubDate>Sat, 28 Oct 2023 17:06:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ornx.net/post/bluetooth-volume-fix/">https://blog.ornx.net/post/bluetooth-volume-fix/</a>, See on <a href="https://news.ycombinator.com/item?id=38051488">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <header>
        <p><a href="https://blog.ornx.net/post/bluetooth-volume-fix/">
                <img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_hexdump_rofs_banner_hu9f10eb583e5810089a711ba6408d8f55_39932_800x0_resize_box_3.png" srcset="https://blog.ornx.net/post/bluetooth-volume-fix/fw_hexdump_rofs_banner_hu9f10eb583e5810089a711ba6408d8f55_39932_800x0_resize_box_3.png 800w, https://blog.ornx.net/post/bluetooth-volume-fix/fw_hexdump_rofs_banner_hu9f10eb583e5810089a711ba6408d8f55_39932_1600x0_resize_box_3.png 1600w" width="800" height="273" loading="lazy" alt="Featured image of post Fixing the Volume on my Bluetooth Earbuds">
                
            </a>
        </p>
    

    <div>
    
    <header>
        
            <a href="https://blog.ornx.net/categories/reverse-engineering/">
                reverse engineering
            </a>
        
    </header>
    

    <p>
        
    
        
        <h3>
            A bit of reverse engineering goes a long way
        </h3>
        
    </p>

    
    
    
    
    
    

    
</div>

</header>

    <section>
    
    
    <h2 id="introduction">Introduction</h2>
<p>I recently got a pair of Tozo T6 earbuds. They’re great and I love them but they play a sound whenever you pair, unpair, or connect them, and it’s way too loud for my preference.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> I also wasn’t able to fix it by e.g. setting the equalizer to subtract a few decibels across the board. I asked them via email about this and they responded promptly and said that there was nothing they could do, which is understandable - it’s probably not a common request. But it was too loud for me to continue using them, so I decided to try to solve the problem myself.</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/tozo_t6_earbuds.jpg" alt="Tozo T6 earbuds. They&amp;rsquo;re great. I only have one complaint&amp;hellip;" width="66%"><figcaption>
            <p>Tozo T6 earbuds. They’re great. I only have one complaint…</p>
        </figcaption>
</figure>

<p>To solve the problem, I need to modify the firmware that runs on the device. My initial expectation of how this would work was:</p>
<ol>
<li>I would get a binary file from somewhere for my device. People often share firmware files online, maybe I could find a copy using a search engine.</li>
<li>The firmware file would be in some easily-understood binary structure like ELF.</li>
<li>The audio files would be contained in the binary somewhere, maybe as an ELF symbol (going from the last point). Knowing how the image format worked would let me modify the data within it while making sure that I didn’t accidentally give it a corrupt image and possibly brick my device.</li>
<li>The audio files would be in a format that would be easy to transform, maybe PCM since it’s running on an embedded device with perhaps limited computational power to decode audio (of course, it’s also a headset, so probably it can decode compressed audio just fine).</li>
<li>Once I can modify data within the firmware image (either unpack/repack it or modify data in-situ by knowing its offset and length within the image), do so to make the audio quieter (e.g. if it’s PCM then maybe halve each sample, etc).</li>
<li>Finally, I would flash my modified firmware to the device with some kind of tool made for my model of equipment or its underlying chipset, etc.</li>
</ol>
<p>Some of these assumptions ended up being completely false and unwarranted (I have no idea why I hoped that the audio would be uncompressed on a low-power device like this, for example), but this was my thinking starting out. It also doesn’t include any reverse engineering, which ended up taking up most of the time that wasn’t spent on setting up infrastructure (such as an intercepting proxy), but this was mostly just going down rabbitholes. In the end, I actually didn’t need to reverse engineer much at all. So this post is less about reverse engineering and more about the general process of solving my particular problem.</p>
<h2 id="first-steps-initial-research">First steps: initial research</h2>
<p>The first step is to collect information about what exactly the device is. There seem to be several different entities involved in the production of cheap electronics:</p>
<ul>
<li>The vendor, who actually brands and sells the device - Tozo, in this case.</li>
<li>The chipset, a specific piece of hardware the device is mostly designed around, which runs the firmware code and might have special features for the application at hand.</li>
<li>The ISA - the chipset will run code of a particular instruction set, with a “core” derived from some other base tech like ARM, MIPS, etc (or perhaps they rolled their own).</li>
<li>Additional features - the chipset might integrate technology from other people onto their chip, such as additional coprocessors, chips to interface with different kinds of hardware, etc.</li>
</ul>
<p>The chipset for my device ended up being an Airoha AB1562, which apparently is based on an Tensilica Xtensa ISA and includes a “Cadence HiFi DSP coprocessor”. I wasn’t able to find this out searching for my device model however, so I ended up just looking through the disassembly for their Android app. There I found an SDK for a company called “Airoha”, with references to specific chip models and containing all of the primitives for talking to devices. I wasn’t able to find any other chipset SDK’s, so I assumed this was the chipset manufacturer.</p>
<p>I still had to find the specific model though. After a bit more searching, I found a Reddit community for discussing AirPods clones called /r/airreps, which gave me some good advice about how to proceed. They have also written an Android application called “AirReps156X”<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> which also uses the Airoha SDK, and can provide diagnostic information about Airoha devices<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>. I was able to connect to this app with my device, so it’s definitely an Airoha chipset, and one of the diagnostic strings was “QW_1562U_SDK1.5.1”, which leads me to believe that my device chipset is in the Airoha AB1562 series:</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/airreps_156x_diagnostic.png" alt="The &amp;ldquo;Airreps 156X&amp;rdquo; app display device information." width="33%"><figcaption>
            <p>The “Airreps 156X” app display device information.</p>
        </figcaption>
</figure>

<p>The application also lets you flash new firmware, which is a fairly critical item on the checklist. So now that we’ve gotten that prerequisite out of the way and identified the chipset all that’s left to do is to find the actual firmware and modify it.</p>
<h2 id="finding-the-actual-binaries">Finding the actual binaries</h2>
<p>The Tozo application is my first lead. When you connect your earbuds to the Tozo app, it displays the firmware version and whether or not it’s “current”. So it must talk to a server somewhere, which knows what the latest version of the firmware is:</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/tozo_app_current.png" alt="Tozo&amp;rsquo;s app displaying the current firmware version." width="33%"><figcaption>
            <p>Tozo’s app displaying the current firmware version.</p>
        </figcaption>
</figure>

<p>In theory, I could probably just read through the decompiled code in jadx or something until I find out what servers it’s talking to, how, and what it does when it checks for updates (such as, presumably, downloading the firmware files I’m looking for). But that’s a lot of work, so I have a better idea - when the app checks if the firmware is up to date, it might also make API requests that could shed some light on where to find the actual firmware files. It might even be able to be tricked into trying to update, which would also lead me to the right URLs. So some quick and dirty “dynamic analysis” via traffic interception is the logical next step.</p>
<h2 id="setting-up-an-intercepting-proxy">Setting up an intercepting proxy</h2>
<p>To this end I set up an intercepting proxy using my wireless NIC with hostapd and mitmproxy, and patched the Tozo app with apktool + uber apk signer so that it’ll let us strip the TLS and snoop on its raw network traffic.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p>
<p>Patching the APK is pretty standard stuff - I just followed <a href="https://gist.github.com/unoexperto/80694ccaed6dadc304ad5b8196cbbd2c" target="_blank" rel="noopener">this gist</a>. The idea is that Android applications have two CA stores, one that can be easily modified by the user, and one that can’t, and by default most Android apps only check TLS certificates against the latter. But, by patching the APK, we can tell it to use the former too, which is where we put our mitmproxy-provided TLS certificate that we’re going to use to snoop on all of the network traffic used by our app. Then we have to sign it so Android will deal with it.</p>
<p>The intercepting proxy setup was pretty straightforward - just set up the AP, set up some iptables rules to direct traffic to mitmproxy’s listening port, and do the typical NAT song and dance:</p>
<div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="make"><span><span><span>.PHONY</span><span>:</span> <span>start</span>
</span></span><span><span><span>start</span><span>:</span>
</span></span><span><span>	nmcli device <span>set</span> wlp3s0 managed no
</span></span><span><span>	sudo sysctl -w net.ipv4.conf.all.send_redirects<span>=</span><span>0</span>
</span></span><span><span>	sudo sysctl -w net.ipv4.ip_forward<span>=</span><span>1</span>
</span></span><span><span>	sudo iptables -I INPUT -i wlp3s0 -j ACCEPT
</span></span><span><span>	sudo iptables -t nat -A PREROUTING -i wlp3s0 -p tcp --dport <span>80</span> -j REDIRECT --to-port <span>8080</span>
</span></span><span><span>	sudo iptables -t nat -A PREROUTING -i wlp3s0 -p tcp --dport <span>443</span> -j REDIRECT --to-port <span>8080</span>
</span></span><span><span>	sudo iptables -A FORWARD -i wlp3s0 -o enp5s0f3u1u3c2 -j ACCEPT
</span></span><span><span>	sudo iptables -t nat -A POSTROUTING -o enp5s0f3u1u3c2 -j MASQUERADE
</span></span><span><span>	sudo iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
</span></span><span><span>	sudo hostapd hostapd.conf
</span></span><span><span>
</span></span><span><span><span>.PHONY</span><span>:</span> <span>stop</span>
</span></span><span><span><span>stop</span><span>:</span>
</span></span><span><span>	sudo sysctl -w net.ipv4.conf.all.send_redirects<span>=</span><span>1</span>
</span></span><span><span>	sudo sysctl -w net.ipv4.ip_forward<span>=</span><span>0</span>
</span></span><span><span>	sudo iptables -D INPUT -i wlp3s0 -j ACCEPT
</span></span><span><span>	sudo iptables -t nat -D PREROUTING -i wlp3s0 -p tcp --dport <span>80</span> -j REDIRECT --to-port <span>8080</span>
</span></span><span><span>	sudo iptables -t nat -D PREROUTING -i wlp3s0 -p tcp --dport <span>443</span> -j REDIRECT --to-port <span>8080</span>
</span></span><span><span>	sudo iptables -D FORWARD -i wlp3s0 -o enp5s0f3u1u3c2 -j ACCEPT
</span></span><span><span>	sudo iptables -t nat -D POSTROUTING -o enp5s0f3u1u3c2 -j MASQUERADE
</span></span><span><span>	sudo iptables -D FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
</span></span><span><span>	nmcli device <span>set</span> wlp3s0 managed yes
</span></span><span><span>				
</span></span><span><span><span>.PHONY</span><span>:</span> <span>mitmproxy</span>
</span></span><span><span><span>mitmproxy</span><span>:</span>
</span></span><span><span>	<span>SSLKEYLOGFILE</span><span>=</span><span>"sslkeylogfile-attempt1.txt"</span> mitmdump --mode transparent --showhost -k
</span></span></code></pre></td></tr></tbody></table>
</div>

<p>


<em>The script I used to start and stop the TLS-stripping wireless AP. I know <code>make</code> isn’t really the right tool for this, but it’s a force of habit at this point.</em>

</p>


<h2 id="snooping-on-the-applications-network-traffic">Snooping on the application’s network traffic</h2>
<p>Once it was all working, I saw that when I connected the device to the app and the “current” string popped up next to the firmware version, it made a request to an endpoint /api/v1/getOtaVersionV3. And, lo and behold, the response contains links to all of the firmware bins we’re looking for! How nice. No trickery needed.</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/tozo_netcap_checkota_request_redacted.png" alt="getOtaVersionV3 Request in Wireshark" width="92%"><figcaption>
            <p>getOtaVersionV3 Request in Wireshark</p>
        </figcaption>
</figure>

<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/tozo_mitmproxy_response.png" alt="getOtaVersionV3 Response in mitmproxy" width="92%"><figcaption>
            <p>getOtaVersionV3 Response in mitmproxy</p>
        </figcaption>
</figure>

<h2 id="analyzing-the-firmware">Analyzing the firmware</h2>
<p>There are four files, two per earbud, each having a “FotaPackage” and a “FileSystemImage”. The two filesystem images are identical, so we wind up with three distinct files - two fotapackages for L and R earbuds, and the filesystem image.</p>
<p>The first thing anybody does when they get a weird file is, they run the Linux command “file” on it, to see if it has a magic number that indicates its file format:</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_file.png" alt="Unhelpful." width="92%"><figcaption>
            <p>Unhelpful.</p>
        </figcaption>
</figure>

<p>…and, if that doesn’t help, they might run strings or hexdump over it, to see if there’s any interesting human-readable ASCII strings in it:</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_strings_rofs.png" alt="From the filesystem image - some filenames, at least." width="92%"><figcaption>
            <p>From the filesystem image - some filenames, at least.</p>
        </figcaption>
</figure>

<p>and then binwalk to see if there’s any files embedded in it:</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_binwalk.png" alt="The LZ4 region contains the NVROM, which isn&amp;rsquo;t useful to us - it doesn&amp;rsquo;t contain any audio files." width="92%"><figcaption>
            <p>The LZ4 region contains the NVROM, which isn’t useful to us - it doesn’t contain any audio files.</p>
        </figcaption>
</figure>

<p>Unfortunately, binwalk didn’t find anything, even the mp3 files whose filenames are referenced directly in the image. They are indeed in there, it’s just that the mp3 file format is just not very easy to carve out of arbitrary binary data because it doesn’t have any kind of magic number (it can start with either 0xFFFF or 0xFFFE, neither of which are particularly distinctive, and there’s no footer). So even though you can tell they’re in there, it’s not immediately obvious how to unambiguously calculate the offset and length for each mp3 file. So, I decided the best way to figure this out would be to decipher the filesystem image format, which probably has information that tells you where each file begins and ends.</p>
<h2 id="entropy-analysis">Entropy analysis</h2>
<p>The next step for this is entropy analysis. This basically tells you what parts of a file are constant (0x00 or 0xFF are popular bytes for this), which parts resemble random noise, which parts are valid ASCII text, and the offsets at which one of those things changes into another. It’s useful because it sometimes lets you visualize the structure of something without actually knowing anything about it.</p>
<p>The filesystem image looked promising (generated with <a href="http://binvis.io/%29" target="_blank" rel="noopener">http://binvis.io/)</a>:</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_binvisio_entropy_rofs.png" alt="It does some grouping of the data into blocks to make the structure more visible, so it&amp;rsquo;s not row-by-row like you might expect." width="10%"><figcaption>
            <p>It does some grouping of the data into blocks to make the structure more visible, so it’s not row-by-row like you might expect.</p>
        </figcaption>
</figure>

<p>Unfortunately, the FotaPackage files however were obviously encrypted or compressed somehow:</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_binwalk_entropy_l.png" alt="It&amp;rsquo;s not looking good." width="92%"><figcaption>
            <p>It’s not looking good.</p>
        </figcaption>
</figure>

<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_binvisio_entropy_fota_l.png" alt="Oof." width="10%"><figcaption>
            <p>Oof.</p>
        </figcaption>
</figure>

<p>I also noticed that the left and right FotaPackage binaries had some curious differences - their headers only differed sporadically, while the body was identical except for the end, where there was about 7KB of complete difference.</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_binwalk_lr_diff_start.png" alt="The header (0x00-0x1000) appears to be unencrypted and only differs in small segments." width="92%"><figcaption>
            <p>The header (0x00-0x1000) appears to be unencrypted and only differs in small segments.</p>
        </figcaption>
</figure>

<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_binwalk_lr_diff_end.png" alt="Then, at the end, the footer suddenly changes to completely different, at around 0xC4E38." width="92%"><figcaption>
            <p>Then, at the end, the footer suddenly changes to completely different, at around 0xC4E38.</p>
        </figcaption>
</figure>

<p>I wasn’t completely sure as to the meaning of this, beyond the fact that there was obviously some kind of opaque transformation at work. My initial guess was encryption, with the same key/IV but different plaintexts, and that the sudden difference corresponds to a single-byte difference (perhaps an #ifdef EARBUD_R doBluetoothMasterThings(); #else doBluetoothSlaveThings(); #endif sort of thing) that then leads to the rest of the file being different, but I wasn’t able to verify this. Regardless of why, it was obvious I wasn’t going to get anything out of them without serious effort.</p>
<h2 id="a-quick-appraisal-of-the-situation">A quick appraisal of the situation</h2>
<p>The fact that we know the audio is mp3 was actually pretty bad news to me at first glance. My understanding of media encoders is that they generally have a lot of options for how to encode something, while sometimes a given decoder will barf on a perfectly well-formed file that happens to use a feature it wasn’t expecting.</p>
<p>This is very bad for us for two reasons:</p>
<ol>
<li>Our decoder is of utterly unknown provenance, who knows what it might barf on.</li>
<li>The audio gets played right when the device first pairs, so if we produce an mp3 file that the decoder does not like and the device crashes before we can connect to it again, then we risk putting it into an unrecoverable state.</li>
</ol>
<p>Furthermore, even if we use precisely the expected encoding parameters while producing our volume-adjusted mp3 files, if we change the length of the file while modifying it then we must also make sure that we account for this when we modify the filesystem image, where what that specifically means depends on the exact structure of the filesystem format. Presumably, it records the length of each file somewhere, and we need to make sure we adjust this number to accurately refect the new length of the file (or else it will either be truncated or have garbage added to the end). This is a lot of work, and with an uncertain outcome.</p>
<p>So, at this point I was somewhat worried about the future of this project, and was desperately trying to figure out how to proceed without any re-encoding.</p>
<p>Luckily, it turns out that you can indeed modify the volume (or “gain”) of an mp3 file without changing its length, or re-encoding it, or even modifying its metadata. It’s kind of like how you can rotate a JPEG file without re-encoding it as well - you can just look inside of its data structures and modify them for this one particular transformation without needing to change anything else. Pretty neat!</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/mp3_binwalk_gainadjusted.png" alt="An mp3 file transformed with mp3gain - see, only a few bytes of difference." width="75%"><figcaption>
            <p>An mp3 file transformed with mp3gain - see, only a few bytes of difference.</p>
        </figcaption>
</figure>

<h2 id="rofs">ROFS</h2>
<p>Back to the filesystem image - it seems to contain the mp3 files that correspond to the sounds I am trying to enquiet, and I want to replace them with modified versions, so at a bare minimum I need to know where files begin and end inside of the image. At this point, binwalk couldn’t identify them, so I thought that the problem was that either they were either obfuscated somehow (compressed/encrypted) or the structure of the filesystem might be making it difficult to identify them. So, I decided that the next point of attack was to understand the structure of the filesystem image, which begins with the ASCII string “ROFS”:</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_hexdump_rofs.png" alt="Hexdump of ROFS image." width="50%"><figcaption>
            <p>Hexdump of ROFS image.</p>
        </figcaption>
</figure>

<p>The first step is to search for information about anything with that name online, but no dice. I’m pretty sure it’s bespoke to this particular chipset manufacturer, as I was completely unable to find any reference or documentation of something called “ROFS” that would describe the file I have and the Airoha SDK I would later find contains an implementation of an interface for reading files from it.</p>
<p>At this point, I made a somewhat regrettable decision that the next course of action was to try to attack the firmware, so that I could get to the code that presumably understands the filesystem image format. But the firmware code was (seemingly) encrypted, so I decided to see if they were doing something silly with their encryption and check if maybe the FotaPackage files were decrypted client-side by the SDK before being sent out over the wire. I was eventually able to ascertain with some certainty that the SDK does not transform the firmware in any way before sending it out, but it took me a few hours of reading decompiled code before I came to this conclusion. So of course, I didn’t succeed in attacking the firmware crypto and it was all a waste of time. Oh well.</p>
<h2 id="sdk-breakthrough">SDK Breakthrough</h2>
<p>The final breakthrough occurred when I searched for the chipset name online, and found a copy of their SDK. Looking through it, I could see that it had a bunch of .mp3 files in it - the same ones I could hear on the device. I wrote a quick python program to check if a file was contained within another file (probably a tool already exists for this?) and verified that the mp3 files in the SDK were contained in the filesystem image verbatim.</p>
<div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>sys</span>
</span></span><span><span>
</span></span><span><span><span>assert</span><span>(</span><span>len</span><span>(</span><span>sys</span><span>.</span><span>argv</span><span>)</span> <span>==</span> <span>3</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span># returns if a is contained within b</span>
</span></span><span><span><span>a</span><span>=</span><span>sys</span><span>.</span><span>argv</span><span>[</span><span>1</span><span>]</span>
</span></span><span><span><span>b</span><span>=</span><span>sys</span><span>.</span><span>argv</span><span>[</span><span>2</span><span>]</span>
</span></span><span><span>
</span></span><span><span><span>x</span> <span>=</span> <span>False</span>
</span></span><span><span><span>i</span> <span>=</span> <span>0</span>
</span></span><span><span><span>with</span> <span>open</span><span>(</span><span>a</span><span>,</span> <span>mode</span><span>=</span><span>'rb'</span><span>)</span> <span>as</span> <span>af</span><span>:</span>
</span></span><span><span>  <span>ab</span> <span>=</span> <span>af</span><span>.</span><span>read</span><span>()</span>
</span></span><span><span>  <span>with</span> <span>open</span><span>(</span><span>b</span><span>,</span> <span>mode</span><span>=</span><span>'rb'</span><span>)</span> <span>as</span> <span>bf</span><span>:</span>
</span></span><span><span>    <span>bb</span> <span>=</span> <span>bf</span><span>.</span><span>read</span><span>()</span>
</span></span><span><span>    
</span></span><span><span>    <span>while</span> <span>i</span> <span>&lt;</span> <span>len</span><span>(</span><span>bb</span><span>):</span>
</span></span><span><span>      <span>o</span> <span>=</span> <span>0</span>
</span></span><span><span>      <span>while</span> <span>((</span><span>o</span> <span>&lt;</span> <span>len</span><span>(</span><span>ab</span><span>))</span> <span>and</span> <span>((</span><span>i</span> <span>+</span> <span>o</span><span>)</span> <span>&lt;</span> <span>len</span><span>(</span><span>bb</span><span>))</span> <span>and</span> <span>ab</span><span>[</span><span>o</span><span>]</span> <span>==</span> <span>bb</span><span>[</span><span>i</span> <span>+</span> <span>o</span><span>]):</span>
</span></span><span><span>        <span>o</span> <span>=</span> <span>o</span> <span>+</span> <span>1</span>
</span></span><span><span>      <span>if</span> <span>(</span><span>o</span> <span>==</span> <span>len</span><span>(</span><span>ab</span><span>)):</span>
</span></span><span><span>        <span>x</span> <span>=</span> <span>True</span>
</span></span><span><span>        <span>break</span>
</span></span><span><span>      <span>else</span><span>:</span>
</span></span><span><span>        <span>i</span> <span>=</span> <span>i</span> <span>+</span> <span>1</span>
</span></span><span><span>
</span></span><span><span><span>if</span> <span>x</span><span>:</span>
</span></span><span><span>  <span>print</span><span>(</span><span>"found: </span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>i</span><span>))</span>
</span></span><span><span>  <span>exit</span><span>(</span><span>0</span><span>)</span>
</span></span><span><span><span>else</span><span>:</span>
</span></span><span><span>  <span>print</span><span>(</span><span>"not found"</span><span>)</span>
</span></span><span><span>  <span>exit</span><span>(</span><span>1</span><span>)</span></span></span></code></pre></td></tr></tbody></table>
</div>

<p>


<em>bincontains.py</em>

</p>


<h2 id="modifying-the-filesystem-image">Modifying the filesystem image</h2>
<p>I was slightly worried that the ROFS image might contain additional data, such as checksums for the files inside of it, but I briefly skimmed the ROFS-related code from the SDK (unfortunately it only seemed to exist as prebuilt object files) and it only had a few symbols in it for operating on the filesystem, none of which suggested the presence of checksumming:</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/ghidra_rofs_obj_symboltable.png" alt="There&amp;rsquo;s not a lot of code in here, and there&amp;rsquo;s no external references to anything relating to checksumming." width="92%"><figcaption>
            <p>There’s not a lot of code in here, and there’s no external references to anything relating to checksumming.</p>
        </figcaption>
</figure>

<p>So with that out of the way, at this point I actually have everything I need to complete the task of modifying the volume of the sound files in the firmware image with no further reverse engineering. I have:</p>
<ul>
<li>A way to flash updated firmware to the device, as well as the firmware files themselves.</li>
<li>Knowledge that the mp3 files from the SDK are included verbatim in the filesystem image (no compression, splitting into blocks, etc). This means I have their lengths and offsets in the filesystem image.</li>
<li>Knowledge that you can modify the gain of an mp3 file without re-encoding it or changing its length.</li>
<li>The assumption that the filesystem format doesn’t include any checksumming or additional information about its files that would be invalidated upon modifying their occupant byte range in the image.</li>
</ul>
<p>Then, it’s just as simple as looping over the mp3 files, and if it’s contained in the image, running mp3gain on the file and then replacing it in the image with the gain-modified version. I used an adjustment of -19.5 decibels.</p>
<div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="bash"><span><span><span>#!/usr/bin/env bash
</span></span></span><span><span><span></span>
</span></span><span><span><span>NS</span><span>=</span><span>"</span><span>$1</span><span>"</span>
</span></span><span><span><span>HS</span><span>=</span><span>"</span><span>$2</span><span>"</span>
</span></span><span><span><span>DB</span><span>=</span><span>"</span><span>$3</span><span>"</span>
</span></span><span><span>
</span></span><span><span><span>for</span> N in <span>$(</span>find <span>"</span><span>$NS</span><span>"</span> -type f <span>|</span> grep -E <span>'mp3$'</span><span>)</span><span>;</span> <span>do</span>
</span></span><span><span>	<span>F</span><span>=</span><span>$(</span>python3 bincontains.py <span>"</span><span>$N</span><span>"</span> <span>"</span><span>$HS</span><span>"</span><span>)</span><span>;</span>
</span></span><span><span>	<span>X</span><span>=</span><span>$?</span>
</span></span><span><span>	<span>if</span> <span>[</span> <span>$X</span> -eq <span>0</span> <span>]</span><span>;</span> <span>then</span>
</span></span><span><span>		<span>START</span><span>=</span><span>$(</span><span>echo</span> <span>"</span><span>$F</span><span>"</span> <span>|</span> awk <span>'{print $2;}'</span><span>)</span>
</span></span><span><span>		<span>LEN</span><span>=</span><span>$(</span>stat -c <span>'%s'</span> <span>"</span><span>$N</span><span>"</span><span>)</span>
</span></span><span><span>		<span>TMP</span><span>=</span><span>$(</span>mktemp <span>"XXXXXXX.mp3"</span><span>)</span>
</span></span><span><span>		cp <span>"</span><span>$N</span><span>"</span> <span>"</span><span>$TMP</span><span>"</span>
</span></span><span><span>		mp3gain -q -T -s r -s s -g <span>"</span><span>$DB</span><span>"</span> <span>"</span><span>$TMP</span><span>"</span>
</span></span><span><span>		<span>LEN2</span><span>=</span><span>$(</span>stat -c <span>'%s'</span> <span>"</span><span>$TMP</span><span>"</span><span>)</span>
</span></span><span><span>		<span>printf</span> <span>"%s,%s,%s,%s\n"</span> <span>"</span><span>$N</span><span>"</span> <span>"</span><span>$START</span><span>"</span> <span>"</span><span>$LEN</span><span>"</span> <span>"</span><span>$LEN2</span><span>"</span>
</span></span><span><span>		dd <span>if</span><span>=</span><span>"</span><span>$TMP</span><span>"</span> <span>of</span><span>=</span><span>"</span><span>$HS</span><span>"</span> <span>bs</span><span>=</span><span>1</span> <span>seek</span><span>=</span><span>"</span><span>$START</span><span>"</span> <span>count</span><span>=</span><span>"</span><span>$LEN</span><span>"</span> <span>conv</span><span>=</span>notrunc
</span></span><span><span>	<span>fi</span>
</span></span><span><span><span>done</span></span></span></code></pre></td></tr></tbody></table>
</div>

<p>


<em>binsearch.sh</em>

</p>


<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/fw_binwalk_modified_diff.png" alt="A quick binary diff of the final firmware image - only a few bytes of difference, as expected." width="92%"><figcaption>
            <p>A quick binary diff of the final firmware image - only a few bytes of difference, as expected.</p>
        </figcaption>
</figure>

<p>Finally, I flashed it to the device and it worked!</p>
<figure><img src="https://blog.ornx.net/post/bluetooth-volume-fix/airreps_156x_fotasucceed.png" alt="Hooray!" width="33%"><figcaption>
            <p>Hooray!</p>
        </figcaption>
</figure>

<p>I shortly verified that the device was fully operational and the sound was, indeed, much quieter than it was when I started.</p>
<p>Mission accomplished!</p>
<h2 id="thats-a-wrap">That’s a wrap!</h2>
<p>I didn’t end up having to decrypt the firmware (probably impossible for me) or understand the ROFS image format at all - most of the time spent reverse-engineering was actually going down rabbitholes that didn’t help me in the end.</p>
<p>I also kind of wish that volume control of system sounds was a first-class feature - from a UI perspective, I think it’s an error for a device that plays audio to not have a volume control that modifies all of the sound that gets produced by the device. But apparently it has a workaround, so I guess it’s fine.</p>
<p>But yeah, all in all this was a pretty fun little project, would do again/10.</p>


</section>


    


    
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting my library cards onto my phone the hard way (201 pts)]]></title>
            <link>https://iliana.fyi/blog/ios-wallet-library-card/</link>
            <guid>38050535</guid>
            <pubDate>Sat, 28 Oct 2023 15:26:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iliana.fyi/blog/ios-wallet-library-card/">https://iliana.fyi/blog/ios-wallet-library-card/</a>, See on <a href="https://news.ycombinator.com/item?id=38050535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Our local libraries, The Seattle Public Library and the King County Library System, issue pieces of plastic with barcodes printed on the back assigned to your borrower account. These cards are not <em>strictly</em> necessary in 2023; most everything at Seattle libraries is self-service, including circulation, and these self-service entrypoints usually have a way to type in a library barcode manually. But having the barcode is far more convenient, and I’d like to have it without having to keep yet another plastic card I rarely use in my wallet.</p><p>So I put it on my phone, in my iPhone’s Wallet app. This became extremely silly extremely quickly, so I’ve decided to document it here for myself and others.</p><h2>A brief introduction to passes</h2><p>The Wallet app can manage many things: payment cards, government/employee/student IDs, house/car/hotel room keys; none of these were part of what Wallet, initially called Passbook, could do at its 2012 launch. At that time, Passbook only managed “passes”.</p><p><a href="https://developer.apple.com/library/archive/documentation/UserExperience/Conceptual/PassKit_PG/index.html">Apple’s documentation on passes</a> covers this in more detail, but they are self-contained zip files full of JSON and PNGs designed to be distributed through email or the web from a vendor to its user. If you have a pass on your phone, you can usually go to Pass Details and find a share icon in the top right, allowing you to send the .pkpass file to somewhere you can unzip it and inspect it.</p><p>The contents are pretty simple. <a href="https://developer.apple.com/library/archive/documentation/UserExperience/Conceptual/PassKit_PG/Creating.html#//apple_ref/doc/uid/TP40012195-CH4-SW52">There’s a specific list of supported images</a>, there’s a <code>pass.json</code> file which describes all of the non-image content of the pass, there’s a <code>manifest.json</code> file which lists the SHA-1 checksum of all the other files, and a <code>signature</code> which is an S/MIME signature of the contents of the <code>manifest.json</code>.</p><p>Our first interesting problem is one of barcode formats. Passes support four types of barcodes: <a href="https://en.wikipedia.org/wiki/QR_code">QR code</a>, <a href="https://en.wikipedia.org/wiki/PDF417">PDF417</a> (commonly used on United States driver licenses), <a href="https://en.wikipedia.org/wiki/Aztec_Code">Aztec Code</a> (used for boarding passes by the airline industry), and <a href="https://en.wikipedia.org/wiki/Code_128">Code 128</a> (the only supported linear symbology). My library card uses… <em>[stares at Wikipedia for half an hour]</em> <a href="https://en.wikipedia.org/wiki/Codabar">Codabar</a>, widely used in libraries<sup><a href="#user-content-fn-blood" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-blood">1</a></sup>, and perhaps one of the cutest barcode symbologies (and names) I’ve ever seen. It’s possible that the barcode scanners at the library support other linear symbologies, but Codabar is the only one I know guaranteed to work at all of them. So we will need to fake it by providing some image that functions as a scannable barcode.</p><p>Our second interesting problem, which is a much worse, “oh no”-level problem: for some reason, passes are cryptographically signed, and they have to be signed with a key known to one of Apple’s certificate authorities. Cryptographically signing these files makes some sense when you consider that passes were designed to get automatic updates from their vendors; for example, your boarding pass for a flight reflecting gate changes or changing your seat assignment.</p><p>If you are already an Apple developer you can get yourself a pass signing key pretty trivially, but I am not, and I do not intend to drop $99 on this.</p><h2>Perfection is the enemy of something or other</h2><p>There are other people who are already Apple developers who have made various apps for designing passes. They are… passable? Unfortunately I am a perfectionist.</p><p>For one thing, there is the matter of the logo in the top left of the pass. Apple has designed this somewhat flexibly, with a maximum height of 50 device-independent pixels, but a square logo with text to the right side is going to most comfortably fit at about 40 pixels tall. Pass developers are expected to provide correctly-sized logos at <code>logo.png</code>, <code>logo@2x.png</code>, <code>logo@3x.png</code><sup><a href="#user-content-fn-1" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-1">2</a></sup> for different device pixel ratios, but these apps tend to let you select a single logo and not give you any control over how it’s scaled. If you give it a 40-pixel image, it’ll be blurry on any currently-supported iPhone; if you give it an 80-pixel image, it’ll be too large. Not great!</p><p>For another thing, I’d really like to have the screen be brighter as I open the pass. Passes with normal, supported barcodes do this to support scanners that need better contrast. To me, the ideal situation here is to trick iOS into making the screen brighter without actually having a non-functional barcode present. I’m not going to be able to get away with this kind of JSON fuzzing without digging into the JSON myself.</p><p>And, these apps tend to be free to download, but only let you save a limited number of passes to Wallet before asking you to pay up. I am not here to judge the developers for doing this but I am probably also not going to pay for your app unless it does what I want it to (and unfortunately what I want is kind of extreme).</p><h2>Finding a key</h2><p>Well, I did just download half a dozen free-to-start pass generator apps.</p><p>You could make these one of two ways. Probably the “correct” way is to have some web service which performs the signing, so that you don’t ship a private key with the application itself. But surely one of these apps I’ve downloaded lets you generate passes offline? Sometimes you want the app to work without having to also maintain a web service; that sounds like a one-way ticket to dealing with a ton of bad reviews and refunds when it inevitably goes down.</p><p>I turned on Airplane Mode, turned off WiFi, and tried them all. Sure enough, at least one does. I’m not going to draw attention to the specific app I used in this post because I don’t want their key to get revoked<sup><a href="#user-content-fn-2" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-2">3</a></sup>. But it was kind of funny how simple the process was:</p><ol><li>Download the app on my Mac, since Apple silicon Macs let you run iOS apps.</li><li>In the wrapped iOS app bundle, observe that there is a very obvious <code>.p12</code> file.</li><li>Run <code>strings</code> on the main binary and look for anything that might be a password (as PKCS#12 files require an import password).</li></ol><p>And, well:</p><pre><code>$ openssl pkcs12 -info -in [redacted].p12 -legacy -nodes
Enter Import Password:
MAC: sha1, Iteration 1
MAC length: 20, salt length: 8
PKCS7 Encrypted data: pbeWithSHA1And40BitRC2-CBC, Iteration 2048
Certificate bag
Bag Attributes
    friendlyName: Pass Type ID: [redacted]
    localKeyID: [redacted]
subject=UID = [redacted], CN = Pass Type ID: [redacted], OU = [redacted], O = [redacted], C = [redacted]
issuer=CN = Apple Worldwide Developer Relations Certification Authority, OU = G4, O = Apple Inc., C = US
...
</code></pre><p>We also need a certificate chain; this certificate is signed with an intermediate. The app needs it too, so it’s probably somewhere in the bundle, but the certificate contains within its X.509 extension fields the URL to download the intermediate if you need it (you can view a certificate’s various fields with <code>openssl x509 -noout -text -in whatever.pem</code>).</p><h2>Laying out the pass</h2><p>First we need to pick a <a href="https://developer.apple.com/library/archive/documentation/UserExperience/Conceptual/PassKit_PG/Creating.html#//apple_ref/doc/uid/TP40012195-CH4-SW45">pass style</a> out of “boarding pass”, “coupon”, “event ticket”, “generic”, or “store card”. We want a layout that lets us put a large horizontal image across the pass somewhere. This limits us to layouts that support the “strip” image: coupon, event ticket, or store card. Out of these three, the store card is most skeuomorphic to our physical library card.</p><p>Let’s type up the start of a <code>pass.json</code>. The documentation for this file is found at the <a href="https://developer.apple.com/library/archive/documentation/UserExperience/Reference/PassKit_Bundle/Chapters/Introduction.html">PassKit Package Format Reference</a>.</p><pre><code data-syntax-highlighted=""><span>{</span>
<span>  </span><span>"passTypeIdentifier"</span><span>: </span><span>"[redacted]"</span><span>,</span>
<span>  </span><span>"teamIdentifier"</span><span>: </span><span>"[redacted]"</span><span>,</span>
<span>  </span><span>"formatVersion"</span><span>: </span><span>1</span><span>,</span>
<span>  </span><span>"serialNumber"</span><span>: </span><span>"whatever"</span><span>,</span>
<span>  </span><span>"organizationName"</span><span>: </span><span>"me!"</span><span>,</span>
<span>  </span><span>"logoText"</span><span>: </span><span>"The Seattle Public Library"</span><span>,</span>
<span>  </span><span>"description"</span><span>: </span><span>"Library Card"</span><span>,</span>
<span>  </span><span>"storeCard"</span><span>: {</span>
<span>    </span><span>"headerFields"</span><span>: [],</span>
<span>    </span><span>"primaryFields"</span><span>: [],</span>
<span>    </span><span>"backFields"</span><span>: [],</span>
<span>    </span><span>"secondaryFields"</span><span>: [],</span>
<span>    </span><span>"auxiliaryFields"</span><span>: []</span>
<span>  },</span>
<span>  </span><span>"backgroundColor"</span><span>: </span><span>"rgb(255, 255, 255)"</span><span>,</span>
<span>  </span><span>"foregroundColor"</span><span>: </span><span>"rgb(0, 0, 0)"</span><span>,</span>
<span>  </span><span>"sharingProhibited"</span><span>: </span><span>false</span>
<span>}</span>
</code></pre><p>The <code>passTypeIdentifier</code> and <code>teamIdentifier</code> must match the <code>UID</code> and <code>OU</code> fields, respectively, of the certificate subject you got from Apple and/or found lying around. <code>serialNumber</code> needs to be unique for each pass you generate with the same <code>passTypeIdentifier</code>. <code>organizationName</code> is ostensibly supposed to be who made and signed the pass, but if you’re never distributing the pass then it probably doesn’t matter.</p><p>Now for some images. <code>icon.png</code> is required but is not shown on the pass itself. <code>logo.png</code> is the logo displayed at the top left. I generated three logo files: a 40×40 <code>logo.png</code>, an 80×80 <code>logo@2x.png</code>, and a 120×120 <code>logo@3x.png</code>; then I copied <code>logo@3x.png</code> to <code>icon.png</code>.</p><p>Finally, we’ll need the <code>strip.png</code>, which will contain our pre-generated barcode.</p><h2>Generating the barcode</h2><p>Fortunately iOS scales and crops the <code>strip.png</code> we generate to fit whatever size box it is on a device, so we don’t need to worry about making three different versions of it.</p><p>Both my library cards use “A” and “D” as the start and stop symbols. If you already have a barcode scanner handy this is the easiest way to figure out what your start and stop symbols are, but you can also compare the beginning and end of the barcode against <a href="https://en.wikipedia.org/wiki/Codabar#Encoding">the symbology table on Wikipedia</a> by eye pretty easily.</p><p>There aren’t many ready-to-use Codabar generators online, but the format is pretty simple to implement yourself. While prototyping I used <a href="https://lib.rs/crates/barcoders">the Barcoders library for Rust</a> to generate an SVG, then tweaked the SVG and exported a PNG. After some experimentation I settled on the following layout (where 1 unit is the width of a narrow bar):</p><ul><li>The barcode height is, in units, twice the number of total symbols (including the start and stop symbols) in the barcode. (For example: a 13-digit barcode number is 15 symbols, and so I made my barcode height 30 units tall.)</li><li>15 units of quiet space is placed before the start and after the end of the barcode. (Various reader documentation I’ve seen suggests 10 units is sufficient but I had a harder time scanning it with my fancy 2D barcode scanner.)</li><li>50 units of padding are placed above and below the barcode. This is overkill, but helps ensure the image is cropped on the top and bottom, not on the left and right.</li><li>Each unit is scaled up to 8 pixels to ensure iOS is always scaling the image down. (This makes the final image size 1040 pixels tall and, for my example 15-symbol barcode, the barcode 240 pixels tall.)</li></ul><p>Codabar has such a simple encoding that I felt an overwhelming urge to write a 69-line shell script that generates a bitmap of an encoded Codabar barcode in the above layout:</p><pre><code data-syntax-highlighted=""><span>#!/usr/bin/env bash</span>
<span>if</span><span> [[ </span><span>$#</span><span> </span><span>-ne</span><span> </span><span>2</span><span> ]]; </span><span>then</span>
<span>    </span><span>&gt;&amp;2</span><span> </span><span>echo</span><span> </span><span>"usage: </span><span>$0</span><span> BARCODE OUTPUT"</span><span>; </span><span>exit</span><span> </span><span>1</span>
<span>fi</span>

<span>scale_factor</span><span>=</span><span>8</span><span>  </span><span># needs to be multiple of 4 for BMP reasons</span>
<span>quiet_space</span><span>=</span><span>15</span>
<span>vert_padding</span><span>=</span><span>50</span>

<span>draw_black</span><span>() { </span><span>head</span><span> </span><span>-c</span><span> </span><span>$((</span><span>$1</span><span> </span><span>*</span><span> scale_factor </span><span>*</span><span> </span><span>3</span><span>))</span><span> </span><span>/dev/zero</span><span>; }</span>
<span>draw_white</span><span>() { </span><span>draw_black</span><span> </span><span>"</span><span>$1</span><span>"</span><span> </span><span>|</span><span> LANG</span><span>=</span><span>C</span><span> tr </span><span>'\0'</span><span> </span><span>'\377'</span><span>; }</span>
<span>encode_long</span><span>() {</span>
<span>    </span><span>for</span><span> __x </span><span>in</span><span> </span><span>0</span><span> </span><span>8</span><span> </span><span>16</span><span> </span><span>24</span><span>; </span><span>do</span>
<span>        </span><span>echo</span><span> </span><span>-en</span><span> </span><span>"\x$(</span><span>printf</span><span> %x $(((</span><span>$1</span><span> </span><span>&gt;&gt;</span><span> __x) </span><span>%</span><span> </span><span>256</span><span>)))"</span>
<span>    </span><span>done</span>
<span>}</span>

<span>workdir</span><span>=</span><span>$(</span><span>mktemp</span><span> </span><span>-d</span><span>)</span>
<span>trap</span><span> </span><span>'rm -rf "$workdir"'</span><span> </span><span>EXIT</span>

<span>{</span>
<span>    </span><span>draw_white</span><span> $quiet_space</span>
<span>    </span><span>echo</span><span> </span><span>-n</span><span> </span><span>"</span><span>$1</span><span>"</span><span> </span><span>|</span><span> </span><span>while</span><span> read -r -N1 symbol; </span><span>do</span>
<span>        </span><span>case</span><span> $symbol </span><span>in</span>
<span>            0|2|6|C|</span><span>\*</span><span>|B|N|.) bars</span><span>=</span><span>0</span><span>001</span><span> ;;&amp;</span>
<span>            1|-|7|D|E|/)      bars</span><span>=</span><span>0</span><span>010</span><span> ;;&amp;</span>
<span>            4|$|8|A|T|:)      bars</span><span>=</span><span>0</span><span>100</span><span> ;;&amp;</span>
<span>            5|9|3|+)          bars</span><span>=</span><span>1000</span><span> ;;&amp;</span>
<span>            0|1|4|5)          spaces</span><span>=</span><span>0</span><span>01</span><span> ;;</span>
<span>            2|-|$|9)          spaces</span><span>=</span><span>0</span><span>10</span><span> ;;</span>
<span>            6|7|8|3)          spaces</span><span>=</span><span>100</span><span> ;;</span>
<span>            C|</span><span>\*</span><span>|D|E|A|T)     spaces</span><span>=</span><span>0</span><span>11</span><span> ;;</span>
<span>            B|N)              spaces</span><span>=</span><span>110</span><span> ;;</span>
<span>            .|/|:|+)          spaces</span><span>=</span><span>0</span><span>00</span><span> ;;</span>
<span>            </span><span>*</span><span>) </span><span>&gt;&amp;2</span><span> echo</span><span> "</span><span>$0</span><span>: warning: ignoring symbol </span><span>$symbol</span><span>"</span><span>; </span><span>continue</span><span> ;;</span>
<span>        </span><span>esac</span>
<span>        </span><span>for</span><span> i </span><span>in</span><span> {0..3}; </span><span>do</span>
<span>            </span><span>draw_black</span><span> </span><span>$((${bars</span><span>:</span><span>$i</span><span>:</span><span>1} </span><span>+</span><span> </span><span>1</span><span>))</span>
<span>            </span><span>draw_white</span><span> </span><span>$((${spaces</span><span>:</span><span>$i</span><span>:</span><span>1} </span><span>+</span><span> </span><span>1</span><span>))</span>
<span>        </span><span>done</span>
<span>    </span><span>done</span>
<span>    </span><span>draw_white</span><span> </span><span>$((quiet_space </span><span>-</span><span> </span><span>1</span><span>))</span>
<span>} </span><span>&gt;</span><span>"</span><span>$workdir</span><span>/line"</span>

<span>image_width</span><span>=</span><span>$(($(wc </span><span>-</span><span>c </span><span>&lt;</span><span>"</span><span>$workdir</span><span>/</span><span>line") </span><span>/</span><span> </span><span>3</span><span>))</span>
<span>barcode_height</span><span>=</span><span>$((${</span><span>#</span><span>1} </span><span>*</span><span> </span><span>2</span><span>))</span>
<span>image_height</span><span>=</span><span>$(((barcode_height </span><span>+</span><span> vert_padding </span><span>*</span><span> </span><span>2</span><span>) </span><span>*</span><span> scale_factor))</span>

<span>{</span>
<span>    </span><span># BMP header</span>
<span>    </span><span>printf</span><span> </span><span>'BM'</span>
<span>    </span><span>encode_long</span><span> </span><span>$((image_width </span><span>*</span><span> image_height </span><span>*</span><span> </span><span>3</span><span> </span><span>+</span><span> </span><span>54</span><span>))</span>
<span>    </span><span>printf</span><span> </span><span>'\0\0\0\0\x36\0\0\0\x28\0\0\0'</span>
<span>    </span><span>encode_long</span><span> $image_width</span>
<span>    </span><span>encode_long</span><span> $image_height</span>
<span>    </span><span>printf</span><span> </span><span>'\x01\0\x18\0\0\0\0\0'</span>
<span>    </span><span>encode_long</span><span> </span><span>$((image_width </span><span>*</span><span> image_height </span><span>*</span><span> </span><span>3</span><span>))</span>
<span>    </span><span>head</span><span> </span><span>-c</span><span> </span><span>16</span><span> </span><span>/dev/zero</span>

<span>    </span><span>draw_white</span><span> </span><span>$((image_width </span><span>*</span><span> vert_padding))</span><span>  </span><span># top vertical padding</span>

<span>    lines</span><span>=</span><span>$((barcode_height </span><span>*</span><span> scale_factor))</span>
<span>    </span><span>while</span><span> ((lines</span><span>--</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>)); </span><span>do</span><span> cat</span><span> "</span><span>$workdir</span><span>/line"</span><span>; </span><span>done</span>

<span>    </span><span>draw_white</span><span> </span><span>$((image_width </span><span>*</span><span> vert_padding))</span><span>  </span><span># bottom vertical padding</span>
<span>} </span><span>&gt;</span><span>"</span><span>$workdir</span><span>/barcode.bmp"</span>

<span># if not on macOS, replace with your image conversion tool of choice</span>
<span>sips</span><span> </span><span>-s</span><span> </span><span>format</span><span> </span><span>png</span><span> </span><span>"</span><span>$workdir</span><span>/barcode.bmp"</span><span> </span><span>--out</span><span> </span><span>"</span><span>$2</span><span>"</span>
</code></pre><p>Writing this script’s output to <code>strip.png</code> is all we need.</p><h2>Adding the barcode number</h2><p>I also wanted the barcode number to display under the barcode; this is simple enough to do with the secondary fields:</p><pre><code data-syntax-highlighted=""><span>    </span><span>"secondaryFields"</span><span>: [</span>
<span>      {</span>
<span>        </span><span>"key"</span><span>: </span><span>"number"</span><span>,</span>
<span>        </span><span>"label"</span><span>: </span><span>"CARD NUMBER"</span><span>,</span>
<span>        </span><span>"value"</span><span>: </span><span>"6942069420"</span>
<span>      }</span>
<span>    ],</span>
</code></pre><h2>Faking the barcode UX the rest of the way</h2><p>When a user selects a barcoded pass, the phone screen gets brighter to assist with scanners. iOS doesn’t think we have a barcode yet. I hoped that specifying an <em>empty</em> barcode would do the trick, and… yeah! It does! Specifying this in the top level keys of <code>pass.json</code> works:</p><pre><code data-syntax-highlighted=""><span>  </span><span>"barcodes"</span><span>: [</span>
<span>    {</span>
<span>      </span><span>"message"</span><span>: </span><span>""</span><span>,</span>
<span>      </span><span>"format"</span><span>: </span><span>"PKBarcodeFormatCode128"</span><span>,</span>
<span>      </span><span>"messageEncoding"</span><span>: </span><span>"iso-8859-1"</span>
<span>    }</span>
<span>  ],</span>
</code></pre><p>It’s the perfect workaround: no barcode is displayed at the bottom of the pass, but the phone acts like there’s a barcode present.</p><h2>Signing and packaging the pass</h2><p>Once all our files are in place, we need to generate the <code>manifest.json</code>, which is an object with filenames as keys and SHA-1 checksums as values. I wrote a terrifying jq filter to generate the manifest for me:</p><pre><code data-syntax-highlighted=""><span>sha1sum</span><span> </span><span>*</span><span>.png</span><span> </span><span>pass.json</span><span> </span><span>|</span><span> </span><span>\</span>
<span>    </span><span>jq</span><span> </span><span>-Rs</span><span> </span><span>'split("\n") | [ .[] | select(. != "") | split("  ") | {(.[1]): .[0]} ] | add'</span>
</code></pre><p>Now we need to sign the manifest, placing the signature at <code>signature</code>. This is done with the <code>openssl smime</code> command.</p><pre><code data-syntax-highlighted=""><span>openssl</span><span> </span><span>smime</span><span> </span><span>-binary</span><span> </span><span>-sign</span><span> </span><span>\</span>
<span>    </span><span>-signer</span><span> </span><span>pkpass.crt</span><span> </span><span>-inkey</span><span> </span><span>pkpass.key</span><span> </span><span>-certfile</span><span> </span><span>wwdrg4.pem</span><span> </span><span>\</span>
<span>    </span><span>-in</span><span> </span><span>manifest.json</span><span> </span><span>-outform</span><span> </span><span>der</span><span> </span><span>-out</span><span> </span><span>signature</span>
</code></pre><p>It’s possible to use whatever signing time you like<sup><a href="#user-content-fn-3" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-3">4</a></sup> with the <code>-attime</code> option, so if the certificate you got from Apple and/or found lying around is expired, you can still sign with it. The <code>-attime</code> option takes a UNIX epoch.</p><p>And now, we create a zip!</p><pre><code data-syntax-highlighted=""><span>zip</span><span> </span><span>-r</span><span> </span><span>out.pkpass</span><span> </span><span>*</span><span>.png</span><span> </span><span>pass.json</span><span> </span><span>manifest.json</span><span> </span><span>signature</span>
</code></pre><p>macOS comes with a pass previewer tool, so you can open this pass to check that it is valid and mostly looks right. (If it’s not valid, look for an error in Console.app.) The previewer isn’t 100% accurate, but it does have a button to send it to your phone via iCloud, which is pretty cool. You can also get it onto your phone in whatever manner is convenient.</p><figure><p><img alt="The final pass, which displays a logo, the text &quot;The Seattle Public Library&quot;, a barcode, and an obviously fake barcode number under it." src="https://iliana.fyi/blog/ios-wallet-library-card/pass.png"></p><figcaption>Not my real card number, sadly.</figcaption></figure><p>I will note that I have not yet tested this pass in a real library yet, but my barcode scanner can read it off my phone just as well as it can read it from my physical plastic card if I turn the screen brightness all the way up (yes, even further than the zero-length barcode workaround causes the screen to get brighter).</p><h2>Closing thoughts</h2><p>I think it’s pretty neat that the pass specification has remained pretty much unchanged in a decade. But I wish, like many things within the Apple ecosystem, that this didn’t require a $99 USD/year membership to get a certificate in order to sign an otherwise harmless pile of PNGs and JSON. There’s a few features you can use in passes that I’m glad require signing, but nothing I did here should require it, and I hope that changes someday.</p><p>Also I think Apple should add Codabar support to Wallet. I’m not aware of any library that supports using a digital form of a library card in-person, and I think with some tweaks the platform could support libraries without requiring an audit to ensure every barcode scanner across the system can support Code 128.</p><section data-footnotes=""><ol><li id="user-content-fn-blood"><p>And blood banks? <a href="#user-content-fnref-blood" aria-label="Back to content" data-footnote-backref="">↩︎</a></p></li><li id="user-content-fn-1"><p><code>@3x</code> was news to me! Apparently some newer phones have a 3× ratio now. <a href="#user-content-fnref-1" aria-label="Back to content" data-footnote-backref="">↩︎</a></p></li><li id="user-content-fn-2"><p>Given that most of these kinds of apps do not make the passes updatable via the internet, that these keys are limited to signing passes, and that the keys are specifically used in “make whatever pass you want” apps, I do not think there’s any reason to revoke the key I found. Unfortunately I do not trust Apple will accept this reasoning. <a href="#user-content-fnref-2" aria-label="Back to content" data-footnote-backref="">↩︎</a></p></li><li id="user-content-fn-3"><p><a href="https://stackoverflow.com/questions/66989389/consequences-of-the-expiration-of-the-signing-certificate-for-a-already-issued-p/66989932#comment130045743_66989932">Astute observation, Michael.</a> <a href="#user-content-fnref-3" aria-label="Back to content" data-footnote-backref="">↩︎</a></p></li></ol></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Samsung disables customer phones remotely, holds data hostage [video] (183 pts)]]></title>
            <link>https://www.youtube.com/watch?v=Ln4rsxWq3WM</link>
            <guid>38050381</guid>
            <pubDate>Sat, 28 Oct 2023 15:08:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=Ln4rsxWq3WM">https://www.youtube.com/watch?v=Ln4rsxWq3WM</a>, See on <a href="https://news.ycombinator.com/item?id=38050381">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Demystifying Advanced RAG Pipelines (110 pts)]]></title>
            <link>https://github.com/pchunduri6/rag-demystified</link>
            <guid>38050326</guid>
            <pubDate>Sat, 28 Oct 2023 15:01:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pchunduri6/rag-demystified">https://github.com/pchunduri6/rag-demystified</a>, See on <a href="https://news.ycombinator.com/item?id=38050326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-demystifying-advanced-rag-pipelines" dir="auto"><a href="#demystifying-advanced-rag-pipelines">Demystifying Advanced RAG Pipelines</a></h2>
<p dir="auto">Retrieval-Augmented Generation (RAG) pipelines powered by large language models (LLMs) are gaining popularity for building end-to-end question answering systems. Frameworks such as <a href="https://github.com/run-llama/llama_index">LlamaIndex</a> and <a href="https://github.com/deepset-ai/haystack">Haystack</a> have made significant progress in making RAG pipelines easy to use. While these frameworks provide excellent abstractions for building advanced RAG pipelines, they do so at the cost of transparency. From a user perspective, it's not readily apparent what's going on under the hood, particularly when errors or inconsistencies arise.</p>
<p dir="auto">In this <a href="https://github.com/georgia-tech-db/evadb">EvaDB</a> application, we'll shed light on the inner workings of advanced RAG pipelines by examining the mechanics, limitations, and costs that often remain opaque.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/pchunduri6/rag-demystified/blob/main/images/intro.png"><img width="70%" src="https://github.com/pchunduri6/rag-demystified/raw/main/images/intro.png" title="llama working on a laptop to retrieve data"></a>
  <br>
  <b><i>Llama working on a laptop</i> 🙂</b>
</p>
<h2 tabindex="-1" id="user-content-quick-start" dir="auto"><a href="#quick-start">Quick start</a></h2>
<p dir="auto">If you want to jump right in, use the following commands to run the application:</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt

export OPENAI_API_KEY='yourkey'
python complex_qa.py"><pre><code>pip install -r requirements.txt

export OPENAI_API_KEY='yourkey'
python complex_qa.py
</code></pre></div>
<h2 tabindex="-1" id="user-content-rag-overview" dir="auto"><a href="#rag-overview">RAG Overview</a></h2>
<p dir="auto">Retrieval-augmented generation (RAG) is a cutting-edge AI paradigm for LLM-based question answering.
A RAG pipeline typically contains:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Data Warehouse</strong> - A collection of data sources (e.g., documents, tables etc.) that contain information relevant to the question answering task.</p>
</li>
<li>
<p dir="auto"><strong>Vector Retrieval</strong> - Given a question, find the top K most similar data chunks to the question. This is done using a vector store (e.g., <a href="https://faiss.ai/index.html" rel="nofollow">Faiss</a>).</p>
</li>
<li>
<p dir="auto"><strong>Response Generation</strong> - Given the top K most similar data chunks, generate a response using a large language model (e.g. GPT-4).</p>
</li>
</ol>
<p dir="auto">RAG provides two key advantages over traditional LLM-based question answering:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Up-to-date information</strong> - The data warehouse can be updated in real-time, so the information is always up-to-date.</p>
</li>
<li>
<p dir="auto"><strong>Source tracking</strong> - RAG provides clear traceability, enabling users to identify the sources of information, which is crucial for accuracy verification and mitigating LLM hallucinations.</p>
</li>
</ol>
<h2 tabindex="-1" id="user-content-building-advanced-rag-pipelines" dir="auto"><a href="#building-advanced-rag-pipelines">Building advanced RAG Pipelines</a></h2>
<p dir="auto">To enable answering more complex questions, recent AI frameworks like LlamaIndex have introduced more advanced abstractions such as the <a href="https://gpt-index.readthedocs.io/en/latest/examples/query_engine/sub_question_query_engine.html" rel="nofollow">Sub-question Query Engine</a>.</p>
<p dir="auto">In this application, we'll demystify sophisticated RAG pipelines by using the Sub-question Query Engine as an example. We'll examine the inner workings of the Sub-question Query Engine and simplify the abstractions to their core components. We'll also identify some challenges associated with advanced RAG pipelines.</p>
<h3 tabindex="-1" id="user-content-the-setup" dir="auto"><a href="#the-setup">The setup</a></h3>
<p dir="auto">A data warehouse is a collection of data sources (e.g., documents, tables etc.) that contain information relevant to the question answering task.</p>
<p dir="auto">In this example, we'll use a simple data warehouse containing multiple Wikipedia articles for different popular cities, inspired by LlamaIndex's <a href="https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary.html" rel="nofollow">illustrative use-case</a>. Each city's wiki is a separate data source. Note that for simplicity, we limit each document's size to fit within the LLM context limit.</p>
<p dir="auto">Our goal is to build a system that can answer questions like:</p>
<ol dir="auto">
<li><em>"What is the population of Chicago?"</em></li>
<li><em>"Give me a summary of the positive aspects of Atlanta."</em></li>
<li><em>"Which city has the highest population?"</em></li>
</ol>
<p dir="auto">As you can see, the questions can be simple factoid/summarization questions over a single data source (Q1/Q2) or complex factoid/summarization questions over multiple data sources (Q3).</p>
<p dir="auto">We have the following <em>retrieval methods</em> at our disposal:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>vector retrieval</strong> - Given a question and a data source, generate an LLM response using the top-K most similar data chunks to the question from the data source as the context. We use the off-the-shelf FAISS vector index from <a href="https://github.com/georgia-tech-db/evadb">EvaDB</a> for vector retrieval. However, the concepts are applicable to any vector index.</p>
</li>
<li>
<p dir="auto"><strong>summary retrieval</strong> - Given a summary question and a data source, generate an LLM response using the entire data source as context.</p>
</li>
</ol>
<h3 tabindex="-1" id="user-content-the-secret-sauce" dir="auto"><a href="#the-secret-sauce">The secret sauce</a></h3>
<p dir="auto">Our key insight is that each component in an advanced RAG pipeline is powered by a single LLM call. The entire pipeline is a series of LLM calls with carefully crafted prompt templates. These prompt templates are the secret sauce that enable advanced RAG pipelines to perform complex tasks.</p>
<p dir="auto">In fact, any advanced RAG pipeline can be broken down into a series of individual LLM calls that follow a universal input pattern:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pchunduri6/rag-demystified/blob/main/images/equation.png"><img src="https://github.com/pchunduri6/rag-demystified/raw/main/images/equation.png" alt="equation"></a></p>

<p dir="auto">where:</p>
<ul dir="auto">
<li><strong>Prompt Template</strong> - A curated prompt template for the specific task (e.g., sub-question generation, summarization)</li>
<li><strong>Context</strong> - The context to use to perform the task (e.g. top-K most similar data chunks)</li>
<li><strong>Question</strong> - The question to answer</li>
</ul>
<p dir="auto">Now, we illustrate this principle by examining the inner workings of the Sub-question Query Engine.</p>
<p dir="auto">The Sub-question Query Engine has to perform three tasks:</p>
<ol dir="auto">
<li><strong>Sub-question generation</strong> - Given a complex question, break it down into a set of sub-questions, while identifying the appropriate data source and retrieval function for each sub-question.</li>
<li><strong>Vector/Summary Retrieval</strong> - For each sub-question, use the chosen retrieval function over the corresponding data source to retrieve the relevant information.</li>
<li><strong>Response Aggregation</strong> - Aggregate the responses from the sub-questions into a final response.</li>
</ol>
<p dir="auto">Let's examine each task in detail.</p>
<h3 tabindex="-1" id="user-content-task-1-sub-question-generation" dir="auto"><a href="#task-1-sub-question-generation">Task 1: Sub-question Generation</a></h3>
<p dir="auto">Our goal is to break down a complex question into a set of sub-questions, while identifying the appropriate data source and retrieval function for each sub-question. For example, the question <em>"Which city has the highest population?"</em> is broken down into five sub-questions, one for each city, of the form <em>"What is the population of {city}?".</em> The data source for each sub-question has to be the corresponding city's wiki, and the retrieval function has to be vector retrieval.</p>
<p dir="auto">At first glance, this seems like a daunting task. Specifically, we need to answer the following questions:</p>
<ol dir="auto">
<li><strong>How do we know which sub-questions to generate?</strong></li>
<li><strong>How do we know which data source to use for each sub-question?</strong></li>
<li><strong>How do we know which retrieval function to use for each sub-question?</strong></li>
</ol>
<p dir="auto">Remarkably, the answer to all three questions is the same - a single LLM call! The entire sub-question query engine is powered by a single LLM call with a carefully crafted prompt template. Let's call this template the <strong>Sub-question Prompt Template</strong>.</p>
<div data-snippet-clipboard-copy-content="-- Sub-question Prompt Template --

&quot;&quot;&quot;
    You are an AI assistant that specializes in breaking down complex questions into simpler, manageable sub-questions.
    When presented with a complex user question, your role is to generate a list of sub-questions that, when answered, will comprehensively address the original question.
    You have at your disposal a pre-defined set of functions and data sources to utilize in answering each sub-question.
    If a user question is straightforward, your task is to return the original question, identifying the appropriate function and data source to use for its solution.
    Please remember that you are limited to the provided functions and data sources, and that each sub-question should be a full question that can be answered using a single function and a single data source.
&quot;&quot;&quot;"><pre><code>-- Sub-question Prompt Template --

"""
    You are an AI assistant that specializes in breaking down complex questions into simpler, manageable sub-questions.
    When presented with a complex user question, your role is to generate a list of sub-questions that, when answered, will comprehensively address the original question.
    You have at your disposal a pre-defined set of functions and data sources to utilize in answering each sub-question.
    If a user question is straightforward, your task is to return the original question, identifying the appropriate function and data source to use for its solution.
    Please remember that you are limited to the provided functions and data sources, and that each sub-question should be a full question that can be answered using a single function and a single data source.
"""
</code></pre></div>
<p dir="auto">The context for the LLM call is the names of the data sources and the functions available to the system. The question is the user question. The LLM outputs a list of sub-questions, each with a function and a data source.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pchunduri6/rag-demystified/blob/main/images/task_1_table.png"><img src="https://github.com/pchunduri6/rag-demystified/raw/main/images/task_1_table.png" alt="task_1_table"></a></p>
<p dir="auto">For the three example questions, the LLM returns the following output:</p>
<details>
  <summary>
    LLM output Table
  </summary>
<table>
<thead>
  <tr>
    <th>Question</th>
    <th>Subquestions</th>
    <th>Retrieval method</th>
    <th>Data Source</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>"What is the population of Chicago?"</td>
    <td>"What is the population of Chicago?"</td>
    <td>vector retrieval</td>
    <td>Chicago</td>
    </tr>
    <tr>
    <td>"Give me a summary of the positive aspects of Atlanta."</td>
    <td>"Give me a summary of the positive aspects of Atlanta."</td>
    <td>summary retrieval</td>
    <td>Atlanta</td>
    </tr>
    <tr>
    <td rowspan="5">"Which city has the highest population?"</td>
    <td>"What is the population of Toronto?"</td>
    <td>vector retrieval</td>
    <td>Toronto</td>
    </tr>
    <tr>
    <td>"What is the population of Chicago?"</td>
    <td>vector retrieval</td>
    <td>Chicago</td>
    </tr>
    <tr>
    <td>"What is the population of Houston?"</td>
    <td>vector retrieval</td>
    <td>Houston</td>
    </tr>
    <tr>
    <td>"What is the population of Boston?"</td>
    <td>vector retrieval</td>
    <td>Boston</td>
    </tr>
    <tr>
    <td>"What is the population of Atlanta?"</td>
    <td>vector retrieval</td>
    <td>Atlanta</td>
    </tr>
</tbody>
</table>
</details>
<h3 tabindex="-1" id="user-content-task-2-vectorsummary-retrieval" dir="auto"><a href="#task-2-vectorsummary-retrieval">Task 2: Vector/Summary Retrieval</a></h3>
<p dir="auto">For each sub-question, we use the chosen retrieval function over the corresponding data source to retrieve the relevant information. For example, for the sub-question <em>"What is the population of Chicago?"</em>, we use vector retrieval over the Chicago data source. Similarly, for the sub-question <em>"Give me a summary of the positive aspects of Atlanta."</em>, we use summary retrieval over the Atlanta data source.</p>
<p dir="auto">For both retrieval methods, we use the same LLM prompt template. In fact, we find that the popular <strong>RAG Prompt</strong> from <a href="https://smith.langchain.com/hub" rel="nofollow">LangchainHub</a> works great out-of-the-box for this step.</p>
<div data-snippet-clipboard-copy-content="-- RAG Prompt Template --

&quot;&quot;&quot;
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question}
Context: {context}
Answer:"><pre><code>-- RAG Prompt Template --

"""
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question}
Context: {context}
Answer:
</code></pre></div>
<p dir="auto">Both the retrieval methods only differ in the context used for the LLM call. For vector retrieval, we use the top K most similar data chunks to the sub-question as context. For summary retrieval, we use the entire data source as context.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pchunduri6/rag-demystified/blob/main/images/task_2_table.png"><img src="https://github.com/pchunduri6/rag-demystified/raw/main/images/task_2_table.png" alt="task_2_table"></a></p>
<h3 tabindex="-1" id="user-content-task-3-response-aggregation" dir="auto"><a href="#task-3-response-aggregation">Task 3: Response Aggregation</a></h3>
<p dir="auto">This is the final step that aggregates the responses from the sub-questions into a final response. For example, for the question <em>"Which city has the highest population?"</em>, the sub-questions retrieve the population of each city and then response aggregation finds and returns the city with the highest population.
The <strong>RAG Prompt</strong> works great for this step as well.</p>
<p dir="auto">The context for the LLM call is the list of responses from the sub-questions. The question is the original user question and the LLM outputs a final response.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pchunduri6/rag-demystified/blob/main/images/task_3_table.png"><img src="https://github.com/pchunduri6/rag-demystified/raw/main/images/task_3_table.png" alt="task_3_table"></a></p>
<h3 tabindex="-1" id="user-content-putting-it-all-together" dir="auto"><a href="#putting-it-all-together">Putting it all together</a></h3>
<p dir="auto">After unraveling the layers of abstraction, we uncovered the secret ingredient powering the sub-question query engine - 4 types of LLM calls each with different prompt template, context, and a question. This fits the universal input pattern that we identified earlier perfectly, and is a far cry from the complex abstractions that we started with.
To summarize:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/pchunduri6/rag-demystified/blob/main/images/equation.png"><img src="https://github.com/pchunduri6/rag-demystified/raw/main/images/equation.png" alt="equation"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/pchunduri6/rag-demystified/blob/main/images/call_types_table.png"><img src="https://github.com/pchunduri6/rag-demystified/raw/main/images/call_types_table.png" alt="call_types_table"></a></p>
<p dir="auto">To see the full pipeline in action, run the following commands:</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt

export OPENAI_API_KEY='yourkey'
python complex_qa.py"><pre><code>pip install -r requirements.txt

export OPENAI_API_KEY='yourkey'
python complex_qa.py
</code></pre></div>
<p dir="auto">Here is an example of the system answering the question <em>"Which city with the highest population?"</em>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pchunduri6/rag-demystified/blob/main/images/simple_rag.png"><img src="https://github.com/pchunduri6/rag-demystified/raw/main/images/simple_rag.png" alt="full_pipeline"></a></p>
<h2 tabindex="-1" id="user-content-challenges" dir="auto"><a href="#challenges">Challenges</a></h2>
<p dir="auto">Now that we've demystified the inner workings of advanced RAG pipelines, let's examine the challenges associated with them.</p>
<ol dir="auto">
<li><strong>Question sensitivity</strong> - The biggest challenge that we observed with these systems is the question sensitivity. The LLMs are extremely sensitive to the user question, and the pipeline fails unexpectedly for several user questions. Here are a few example failure cases that we encountered:
<ul dir="auto">
<li><strong>Incorrect sub-questions</strong> - The LLM sometimes generates incorrect sub-questions. For example, <em>"Which city has the highest number of tech companies?"</em> is broken down into <em>"What are the tech companies in each city?"</em> 5 times (once for each city) instead of <em>"What is the number of tech companies in Toronto?"</em>, <em>"What is the number of tech companies in Chicago?"</em>, etc.</li>
<li><strong>Incorrect retrieval function</strong> - <em>"Summarize the positive aspects of Atlanta and Toronto."</em> results in using the vector retrieval function instead of the summary retrieval method.</li>
</ul>
</li>
</ol>
<p dir="auto">We had to put in significant effort into prompt engineering to get the pipeline to work for each question. This is a significant challenge for building robust systems.</p>
<p dir="auto">To verify this behavior, we <a href="https://github.com/pchunduri6/rag-demystified/blob/main/llama_index_baseline.py">implemented the example</a> using the LlamaIndex Sub-question query engine. Consistent with our observations, the system often generates the wrong sub-questions and also uses the wrong retrieval function for the sub-questions, as shown below.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pchunduri6/rag-demystified/blob/main/images/baseline.png"><img src="https://github.com/pchunduri6/rag-demystified/raw/main/images/baseline.png" alt="llama_index_baseline"></a></p>
<ol start="2" dir="auto">
<li><strong>Cost</strong> - The second challenge is the cost dynamics of advanced RAG pipelines. The issue is two-fold:
<ul dir="auto">
<li><strong>Cost sensitivity</strong> - The final cost of the question is dependent on the number of sub-questions generated, the retrieval function used, and the number of data sources queried. Since the LLMs are sensitive to the prompt, the cost of the question can vary significantly depending on the question and the LLM output. For example, the incorrect model choice in the LlamaIndex baseline example above (<code>summary_tool</code>) results in a 3x higher cost compared to the <code>vector_tool</code> while also generating an incorrect response.</li>
<li><strong>Cost estimation</strong> - Advanced abstractions in RAG frameworks obscure the estimated cost of the question. Setting up a cost monitoring system is challenging since the cost of the question is dependent on the LLM output.</li>
</ul>
</li>
</ol>
<h2 tabindex="-1" id="user-content-conclusion" dir="auto"><a href="#conclusion">Conclusion</a></h2>
<p dir="auto">Advanced RAG pipelines powered by LLMs have revolutionized question-answering systems.
However, as we have seen, these pipelines are not turnkey solutions. Under the hood, they rely on carefully engineered prompt templates and multiple chained LLM calls. As illustrated in this <a href="https://github.com/georgia-tech-db/evadb">EvaDB</a> application, these pipelines can be question-sensitive, brittle, and opaque in their cost dynamics. Understanding these intricacies is key to leveraging their full potential and paving the way for more robust and efficient systems in the future.</p>

</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Cruel Fantasies of Well-Fed People (130 pts)]]></title>
            <link>https://www.monbiot.com/2023/10/04/the-cruel-fantasies-of-well-fed-people/</link>
            <guid>38050236</guid>
            <pubDate>Sat, 28 Oct 2023 14:50:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.monbiot.com/2023/10/04/the-cruel-fantasies-of-well-fed-people/">https://www.monbiot.com/2023/10/04/the-cruel-fantasies-of-well-fed-people/</a>, See on <a href="https://news.ycombinator.com/item?id=38050236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        <article>

    

    <div>
        

<p>The astonishing story of how a movement’s quest for rural simplicity drifted into a formula for mass death</p>



<p>By George Monbiot, published on monbiot.com, 4<sup>th</sup> October 2023</p>



<div><p>Tourism sells to you the story of what it has taken away. It markets the “traditional” and “unchanging” and, in doing so, changes it. As the old joke goes, “come to this beautiful unspoiled island and spoil it.” The clock starts ticking from the moment the first person says “timeless”. Then everything that is celebrated starts to become the shadow of itself.</p><p>Similarly, food and farming, industries now intimately connected in some places with tourism, are caught in the endless tension between their reality and their representation. As soon as a region becomes celebrated for its food, gastrotourism accelerates its gentrification. Soon the old <em>boulangeries</em>, <em>boucheries</em> and <em>fromageries</em> are replaced with boutiques selling home decor and handbags. Then a new wave of specialist food shops appears among them, selling the old produce at boutique prices, prices that double with every adjective and place name.</p></div>



<p>The products are real but the stories that surround them – the autochthonous peasant economy, the weathered subsistence among woods and meadows – are vitiated by their telling. The tourist dollars they attract internationalise the local economy. They convert the barns and sheds where cows were kept, grapes were pressed, honey was strained into Airbnbs or second homes. Pastures on the valley floor become glamping sites or pony paddocks. The money generated by the myth of timelessness draws local people away from the sparse living the labels revere.</p>



<p>Meanwhile, to keep pace with gastronomic demand, farming relentlessly intensifies. In the famous cheesemaking regions of France, you will scarcely see a dairy cow. Instead, vast tracts are cultivated for maize. Passing tourists romantically assume it’s sweetcorn for human consumption – the French must eat a lot of sweetcorn! No, it will be turned into silage to feed the cattle stalled in the vast steel sheds – cow factories – that have sprung up from Brittany to Savoie, a business as brutal and industrial as any other. Milk is trucked across hundreds of kilometres, trade fairs market the cheese from Dubai to Shanghai.</p>



<p>The further the cycle of intensification turns, the longer the teleconnections of the “local” economy, the more bucolic and homely the marketing becomes: close-ups of cracked and dirt-grained hands, chickens clucking through buttercupped meadows, girls in Heidi costumes and all the other autophagous nonsense of the Spectacle.</p>



<h3>The Spectacle of Production</h3>



<p>To seek to reverse such economies of scale is not a relaxation into a “simpler” mode, but a conscious and frantic race against entropy. Fair play to those who succeed! We need, among others, small local producers, ideally using new forms of high-yield agroecology. But it’s not for the faint-hearted. They are running up the down escalator, and it accelerates every year. Land prices, house rents, low farmgate prices, easier and more rewarding opportunities elsewhere all militate against survival in the new-old economy, let alone success. And you are always in danger (a danger, admittedly, that some embrace) of becoming your own trope, the re-created peasant self-marketed to the gluttonous Spectacle.</p>



<p>Of course, we all re-create ourselves to some extent. We are all self-consumers. But self-creation is seldom more intense than in sectors deemed “authentic”.</p>



<p>Bucolic re-creations bear little relation to the things they claim to be. What we now fetishise as “peasant food” is much richer and more diverse than the food peasants would once have eaten, except, perhaps, on feast days. Meat was, for most, a luxury, cheese eaten less often than we imagine, salads, in many places, not at all. Diets were often inadequate and deficient in crucial components, such as protein. This is one of the reasons – alongside other aspects of health – why the rustic ancestors we celebrate were, on average, <a href="https://www.sciencedirect.com/science/article/abs/pii/S1570677X10000225">tiny by comparison</a> to us.&nbsp;</p>



<p>Many of the “traditional” ingredients considered essential to the cuisine – such as tomatoes in Italy or peppers in Spain – were unknown until surprisingly recently to those whose diets we claim to honour. Much of the protein, insufficient as it was, came not from cheese and meat but from what we now call dhal. It had names (to give some English examples) like pease pottage, pease pudding, mushy peas and pea soup. Few of these dishes are celebrated by gastronomes today. As George Crabbe remarked in <em>The Village</em>, written in 1783, such food was</p>



<p>“Homely not wholesome, plain not plenteous, such</p>



<p>&nbsp;As you who envy would disdain to touch.”</p>



<p>But, to the wealthy people spending lavishly on what they fondly imagine to be peasant diets, every day is a feast day.</p>



<h3>Lands of Plenty</h3>



<p>We benefit above all from a different legacy: the marvel of the past 50 years of falling hunger during a time of rising population, a marvel we in the rich world scarcely acknowledge, so comfortable has it made us. This remarkable phenomenon was widely considered, just 60 or 70 years ago, simply impossible.</p>



<p>There are three things upon which I think we can all agree. First, that this marvel came at a great environmental cost. It was delivered through hungry and thirsty new crop varieties, reliant for their survival on lashings of agrochemicals, unsustainable water use and practices that can accelerate soil degradation. Second, that it also involved severe social and political dislocations, including land-grabbing, enclosures and rising corporate power and concentration. Third, that it might now be running out of road: the <a href="https://www.fao.org/3/CC3017EN/online/CC3017EN.html">prevalence of global undernourishment</a> rose from 613 million (median estimate) in 2019 to 735 million in 2022.</p>



<p>The immediate reasons for this partial reversal are the Covid-19 pandemic and Russia’s invasion of Ukraine, but there are also three deeper and increasingly urgent issues: the decline of crucial resources, such as soil and water, environmental shocks hammering farm production, and the global food chain’s <a href="https://www.monbiot.com/2023/03/09/the-hunger-gap/">loss of systemic resilience</a>.</p>



<p>So the question – one of the key questions of our time – is how we can feed a population likely to rise to 9 or 10 billion by the middle of the century before starting to decline, reliably, equitably and at a much lower environmental cost. In other words, how we might feed the world without devouring the planet, the subject of <a href="https://www.penguin.co.uk/books/317018/regenesis-by-monbiot-george/9780241447642">my book <em>Regenesis</em></a>.</p>



<p>There are, as I found, plenty of possible ways forward. But there are no ways backward. If we were to seek to restore the agricultural systems of, say, 60 or 70 years ago, a time, remember, when many people were deeply pessimistic about human nutrition and expected global starvation as the population rose, their grim predictions would materialise. Why? Because productivity was much lower than it is today. In 2023, a world of 8.1 billion people suffers far less hunger and famine than the world of 3.2 billion did in 1963, the year of my birth.</p>



<p>Let’s pause to consider this for a moment, because it is one of the most remarkable (and, bizarrely, least celebrated) transformations of our time.</p>



<p>The numbers who died in famines were especially high in the 1960s, as a result of China’s Great Leap Backwards. An estimated 16.6 million <a href="https://ourworldindata.org/famines#long-term-trends-in-global-famine-mortality">perished during</a> mass starvation events in that decade. This compares to 8.8 million in the 1950s and 3.4 million in the 1970s. But 3.4 million, by comparison to more recent figures, is massive. Between 2010 and 2016, the most recent years in the standardised dataset, 255,000 people died this way, all of them in the famine that afflicted Somalia. Since then, there have been four major famines: in Yemen, South Sudan, Somalia (again) and Tigray, in which, in total, hundreds of thousands died. All four were caused by conflict. Famine is also much less geographically widespread than it used to be: it now tends to be confined to one nation or province at a time, rather than afflicting vast areas.</p>



<p>To grasp just how astonishing this decline in mass death through hunger has been, we need to look at the <a href="https://ourworldindata.org/uploads/2018/03/Famine-death-rate-since-1860s-revised.png">death <em>rate</em> in famines</a> as a proportion of the population. A century ago, the rate stood at 82 per 100,000 people. In the 1930s, it was 56, in the 1940s, 79, the 1950s, 32, the 1960s, 50, the 1970s, 8.4, and on down to the most recent figures: 0.5. At no known point in recorded history has the third horseman wielded less deadly power.</p>



<p>There’s a similar trend in total deaths from malnutrition (in other words not only those that occurred in the mass events known as famines). These fell, <a href="https://ourworldindata.org/grapher/malnutrition-deaths-by-age">on a fairly steady trajectory</a>, from 656,000 in 1990 to 212,000 in 2019.</p>



<p>What lies behind these extraordinary trends? There are several reasons, but let me dwell on two of the crucial ones. One is the much greater availability of food per person. This is also a remarkable phenomenon. <em>Our World in Data</em>, which collates such global figures, <a href="https://ourworldindata.org/yields-vs-land-use-how-has-the-world-produced-enough-food-for-a-growing-population">shows that</a> between 1961 and 2014 the world’s production of cereals rose by 280%. This is twice the increase in the global population during that period (136%). It was achieved almost entirely through higher crop yields per hectare.</p>



<p>Another is the long-distance transport of food, something that many of us have railed against, but which, for all its downsides, makes an essential contribution to falling rates of hunger. The reason is simple: if there is a bad harvest or outright crop failure in one place, food can now be shifted from regions with a crop surplus, either through trade or through aid and famine relief programmes. The extreme globalisation of the food system has introduced new sets of problems. But without long-distance transport, many more would starve.</p>



<p>Returning to earlier modes of subsistence is a formula for global catastrophe on a scale that defies imagination.</p>



<h3>The Great Divide</h3>



<p>To make these obvious statements is to become the sworn enemy of many food and farming writers, influencers and film-makers (who have a lucrative industry of their own to support). It is to commit the modern equivalent of blasphemy, as food nostalgia inspires semi-religious beliefs.</p>



<p>To make these statements with the support of numbers is to multiply the sin. As I’ve discovered since publishing the book, if there is one habit that incites fury more reliably than any others, it is to put numbers on the problem. Hectares, yields, nutrients, calories, inputs, outputs, costs, emissions, hunger, death: any form of quantification is as welcome in this arena as a tambourine in a Bach sonata.</p>



<p>Why? Because the romantic story of how food “should” be produced is entirely qualitative. It’s an aesthetic reverie. It’s about pictures, poetry, gut feeling – understandable when it comes to food but, literally, lethal when it comes to ensuring everyone has it. It is the great indulgence of those who never miss a meal to celebrate the times and modes in which people missed plenty.</p>



<p>There are two entirely different questions here: “what production systems do certain well-nourished food writers in the rich world want to see?”, and “how might everyone on Earth be fed?”. But, though often leading to very different conclusions, they are endlessly and callously confused with each other.</p>



<p>Fantasising about a food system in which the third horseman would ride victorious again is among the more perverse habits of comfortable people. The anger and passion with which some of them defend their formula for starvation is a wonder to behold. They privilege their aesthetics – their arcadian fantasies – above the wellbeing of 8 billion people.</p>



<p>Perhaps we shouldn’t be surprised: nostalgia is among the most powerful and dangerous of all social forces. Woe betide the person who seeks to disabuse another of their fantasies about the past!</p>



<p>Well, I thought I had seen it all: the full gamut of cruel fantasies which privilege bucolic comfort zones above global necessities. But this was before I read the new book by Chris Smaje, a small farmer and writer with an academic background, called <em><a href="https://www.chelseagreen.com/product/saying-no-to-a-farm-free-future/">Saying No to a Farm-Free Future</a></em>. The book has been praised by a number of prominent food, farming and environmental writers and campaigners, most of whom subscribe to the worldview I have just described. It is becoming something of a bible for their movement. It promotes what appears to be a recipe for mass global starvation.</p>



<p>Before I go any further, I want to emphasise that Chris has every right to write and publish this book. He has been criticised online for fomenting internecine war within left/green circles (as have I). I don’t see it that way. The argument in which he participates is a crucial one, the divisions are real and the debate needs to be had. I believe this is in fact the greatest of all rifts within environmental movements, and we do ourselves no favours by pretending it does not exist. Though his book is framed as an attack on my book <em>Regenesis</em> and, more broadly, on me, I’m glad he has written it. It is highly instructive.</p>



<h3>Mark of the Beast</h3>



<p>Chris launches his attack with labels. Apparently I’m both an “ecomodernist” and an “urbanist”. He fails to define what he means by an ecomodernist. His use of the term would capture anyone who favours new or newish green technologies: solar panels, wind turbines, electrified railways, GIS mapping, induction hobs ….</p>



<p>I <em>do</em> have a definition of ecomodernism:</p>



<p>a movement that treats green technology as a <em>substitute</em> for political and economic change.</p>



<p>I see it in the vision of people like Bill Gates and Ted Nordhaus, a vision <a href="https://www.theguardian.com/commentisfree/2022/may/13/optimism-climate-predictions-techno-polluters">I strongly oppose</a>. I believe technology is just one of the components of the change we need: necessary but not sufficient. I’ve spent my working life pressing for political and economic change, seeking to dethrone the oligarchs (including Bill Gates) and corporations whose economic and political power impede both democracy and human flourishing. &nbsp;</p>



<p>As for the charge of being an “urbanist”, the only evidence he advances is that a Dutch website calls for 90% of the population to live in cities by 2100. I had nothing to do with this, and the idea appals me as much as it appals Chris. But the way he writes this passage makes it look as if it’s my view: a shocking elision. I was not surprised to see a commentary on his book (since changed at my request) <a href="https://www.frontporchrepublic.com/2023/09/hope-for-a-humane-agricultural-future-a-review-of-saying-no-to-a-farm-free-future/">reporting this terrible idea</a> as “one of Monbiot’s proposals”.</p>



<p>On these fictitious grounds, Chris states that I want a “depopulated countryside”, an “un-peopled” nature, to “eliminate” ruralism, to “keep as many people as possible out of garden-sized or small farm-sized patches in the countryside” and to “concentrat[e] people in the cities as helpless consumers”. I want none of these things. In fact, I strongly oppose them all. I will state my position once more, but with no confidence that he or others will hear it: <em>I do not want to see any depopulation of the countryside.</em></p>



<p>But in one respect, though I have no great enthusiasm for cities, I guess you could call me an urbanist. Why? Because I believe urban populations are a reality that cannot be wished away and, crucially, that they should have food. These, if I’m reading his book correctly, appear to be the two great dividing lines between my vision and Chris’s: I don’t want anyone to have to leave their homes; and I believe urban people need to be fed.</p>



<p>The “ecomodernist” and “urbanist” labels could be seen as the usual cut and thrust of debate: by attaching an alienating definition to someone, you might induce people to stop listening to what they’re saying, and to dismiss their evidence and arguments out of hand. It’s the way certain politicians turn complex and difficult questions into culture war fodder. But as the book goes on, his labelling takes a much darker turn.</p>



<h3>Priced Out</h3>



<p>Chris devotes an entire chapter to inveighing against people (I am, apparently, the archetype), who believe that sustaining high crop yields is a good idea. Apparently, this is “agricultural improver ideology”, which is in turn an “urban-industrial articulation of class power against rural and agrarian people”. Sorry, what?</p>



<p>He explains this contention as follows: “The narrative of agricultural improvement has always had this class element to it – a concern with class improvement for unmanaged farmers as well as agricultural improvement for unmanaged farming.” Yup, that’s the real agenda: a cunning plot to improve farmers’ table manners. My interest in high yields couldn’t possibly be because I worry about how, without them, 8 billion people might be fed.</p>



<p>But this is by no means the end of it. If you believe that enough food should be grown to feed &nbsp;everyone, you are also guilty of “productivism”, “consumerism”, even “colonialism”.</p>



<p>This brings us to the issue he carefully swerves throughout the book: that a certain number of people requires a certain amount of food, and this food has both to be produced and to reach those who need it. If there’s not enough food, or it’s not accessible and affordable to everyone, people will starve. It seems extraordinary to have to point this out.</p>



<p>One of the reasons why high yields ensure that more people can be fed is that more supply reduces the price of food, making it more accessible to the poor. Chris flatly rejects this reasoning. He asserts that “Low food prices, high yields and overproduction are absolutely at the root of food system problems, including global poverty and hunger.” He then goes on to make two statements that left my jaw on the floor:</p>



<p>Lower food prices are “the last thing the global poor need. The result is usually more poverty, more hunger”.</p>



<p>and</p>



<p>“Higher food prices might alleviate hunger globally”.</p>



<p>You might have imagined that such astonishing statements would be carefully explained and evidenced. But they are asserted without justification. The closest he gets is to point out that, in countries like the UK, people spend more on housing and energy than they do on food (which is true) and that the cheapness of this food helps the owners of housing and energy to generate more profit, which might be true, but would need some unpacking.</p>



<p>But this says nothing about the situation of the <em>global</em> poor, the subject of those two astonishing statements. So let’s just take a moment.</p>



<p>The <a href="https://www.fao.org/newsroom/detail/global-indicators-on-the-costs-of-healthy-diets-and-how-many-people-can-t-afford-them/en">global definition</a> of an affordable diet is one that costs 52% or less of average household expenditure. Using this definition, 3 billion people – over one-third of the global population – <a href="https://ourworldindata.org/diet-affordability">cannot afford a healthy diet</a>. In other words, buying adequate food would mean spending more on it than on housing, energy, education, health, transport, clothing and all other items put together.</p>



<p>Importantly, the 3 billion below the line include not just urban people and rural people working in the non-agricultural economy, but also many subsistence farmers, some of whom cannot produce enough food, and of sufficient diversity, to meet their nutritional needs.</p>



<p>In some countries, a healthy diet costs more than the median income: even if people spent <em>all</em> their money trying to purchase one, they still couldn’t afford it. Yes, the problem is poverty: a gross maldistribution of wealth. Yes, this maldistribution urgently needs to be addressed, which is why we need political and economic change, not just new technologies. But while I have seen no evidence (and Chris provides none) that higher food prices alleviate global hunger, there is a <a href="https://theconversation.com/further-food-price-rises-could-cause-up-to-1-million-additional-deaths-in-2023-199120">wealth of evidence</a> that they exacerbate it.</p>



<p>Is it really possible that you can write a book on food and farming and fail to grasp this basic fact? Yes, it seems it is. <em>Saying No to a Farm-Free Future</em> is a powerful lesson in how motivated reasoning can lead you to an utterly perverse and ludicrous position.</p>



<h3>Let Them Eat Nothing</h3>



<p>So how does Chris Smaje believe people should be fed? After launching such a ferocious attack on evil bastards like me, you might expect him to produce a clear alternative. But another remarkable aspect of this book, and of the movement it speaks to, is how vague it becomes on such trifling matters as producing sufficient food for 8 billion people. Here are the most specific phrases I could find, while trying to decipher how he proposes that everyone on Earth should be fed.</p>



<p>“Predominantly local self-provisioning of food, fibre and other material requisites of life”</p>



<p>We should “gain autonomy and feed ourselves”</p>



<p>People should “spread themselves out in the landscape and make low energy livelihoods there”</p>



<p>“Repeasantisation, where commercial farmers step off the productivity treadmill and …. orient themselves instead to more autonomous local agricultures geared to local needs”</p>



<p>“We could boost urban food provision by increasing the number of allotments, community gardens, market gardens and truck farms on brownfield sites”</p>



<p>So the question which arises – and please forgive this ecomodernist, urbanist, productivist, consumerist, colonialist framing – is who, in this world of “self-provisioning” and “repeasantised” commercial farmers, will feed those who do not feed themselves?</p>



<p>Most of the places where large numbers of people live do not have sufficient fertile land nearby to support them. A paper in the journal <em>Nature Food</em> <a href="https://www.nature.com/articles/s43016-020-0060-7">found that</a> only a quarter of the world’s people could be fed with staple grain crops grown within 100 kilometres of where they live. The average minimum distance at which the world’s people can be supplied with staple foods, it found, is 2,200 kilometres. Much of the world’s food is grown in vast, lightly-habited lands (US plains, Canadian prairies, Russian steppes etc) and shipped to tight, densely-populated places.</p>



<p>These are the numbers to which people of Chris’s persuasion most furiously object, even though they have no answer to them. Why? Because the numbers are incompatible with their worldview. They show that, while agrarian localism might be great as far as it goes, it simply cannot, by itself, meet the challenge of feeding the world.</p>



<p>Cities can grow only a tiny fraction of their food, as Chris acknowledges elsewhere in his book. Again, it’s not hard to work out why. Urban areas occupy only 1% of the planet’s land, and this land is in high demand for other uses. Allotments, community gardens, market gardens and truck farms are wonderful things to have, enhancing urban life, but they can produce only a very small proportion of a city’s demand for fruit and vegetables, and close to none of its staple foods.</p>



<p>While this is predominantly an urban issue, it’s not just big cities which rely on non-local production. There are many areas dominated by smaller settlements which simply do not have the agricultural capacity to feed themselves. Even regions which are blessed with sufficient agricultural land and water could, as has happened many times in the past, see their production wiped out by local harvest failure, ensuring that a world of agrarian localism would, once more, become a world in which famine is ever-present and widespread.</p>



<p>So how will the 4.5 billion people who live in cities – over 60% of the global population – and many others living where there is little fertile land be fed? Answer comes there none. Seriously: in 159 pages there is no explanation of how they would survive. If you’re not an agrarian localist either producing your own food or buying from local growers, you’re stuffed – or rather, starved. If Chris has a plan for feeding you, he hasn’t mentioned it.</p>



<p>Discussing his own, proudly low-yield production of wheat and potatoes, Chris states:</p>



<p>“there’s no point labouring for next to nothing on someone else’s behalf when you’ve already grown enough to eat for yourself.”</p>



<p>This is why farmers who do not share his worldview pursue higher yields: these yields make it economically worthwhile to produce staple foods that can be sold to other people. We should thank our lucky stars for such people.</p>



<h3>“Human Feedlots”</h3>



<p>How did he get to this point? I can’t see into his mind, but part of the reason might be his hatred of cities. He rails against them like an Old Testament prophet denouncing Sodom and Gomorrah.</p>



<p>He describes them as “human feedlots”, a term I find grotesque and dehumanising. They “consume everything around them and then themselves”. They are “built on cheap and abundant energy and models of globalised trade that aren’t destined to endure.”</p>



<p>It is true that cities rely on unsustainable and exploitative models of extraction, consumption and dumping. But this applies to the economy as a whole, urban or otherwise. The answer, I believe, is not to rain curses on them and their people, but to replace the destructive economic models with systems in which everyone’s needs are met without breaching planetary boundaries. This is what Kate Raworth’s <a href="https://www.theguardian.com/commentisfree/2017/apr/12/doughnut-growth-economics-book-economic-model"><em>Doughnut Economics</em></a> seeks. I believe we can move towards her vision with the help of what I call “<a href="https://centerforneweconomics.org/publications/private-sufficiency-public-luxury-land-is-the-key-to-the-transformation-of-society/">private sufficiency, public luxury</a>”. None of this, of course, magics away the need to produce sufficient food.</p>



<p>If I interpret his airy euphemisms correctly, the question of how urban people should be fed is not worth answering, because cities are soon going to collapse, and their people will have no choice but to “spread themselves out in the landscape”, growing their own “food and fibre, building shelter, producing a modest livelihood from the local ecological base.” (Never mind that in many places the “local ecological base” could support only a small fraction of the region’s people).</p>



<p>When your solution is societal collapse, you should ask yourself some hard questions about what you are trying to solve.</p>



<h3>The Great Cruelty</h3>



<p>I guess there’s a small consolation here: that Chris might have given up on the idea that his <em>xià xiā</em><em>ng</em> – the mass migration to the countryside he envisages – will happen voluntarily. Perhaps he has at last realised that most people have no particular desire to have to grow their own food and fibre, make their own clothes and build their own shelters. Instead, he now appears to believe that urban people will be forced by catastrophe to leave the cities and succumb to “re-ruralisation” and “repeasantisation”. It might be worth noting that the Old Testament prophets also foresaw the imminent collapse of urban life, two and a half thousand years ago.</p>



<p>How mild and gentle he makes it sound! Refugees from the cities “spreading themselves out in the landscape”, “producing a modest livelihood from the local ecological base.” When the fugitives disperse into the countryside, the inhabitants will doubtless greet them with open arms, saying, “Welcome sister! Welcome brother! Here, have some fertile land. Oh, and some water, knowledge, skills, tools, traction and all the other means to grow your own food and live happy lives as re-peasants in our agrarian wonderland.”</p>



<p>If history is any guide, this is not quite how it’s likely to pan out. The more probable outcomes of societal collapse include warlordism or full-scale war, coercion, fascism, slavery, disease, starvation and mass death.</p>



<p>Moreover, if a cataclysm is sufficient to bring the cities down, it is likely also to destroy the basis of much of rural life. After all, the distinctions between the two are not nearly as crisp as Chris would have us believe. In fact, and horrifyingly, it’s likely that, as a result of environmental disaster, rural life in many parts of the world will collapse <em>before</em> urban life does, as suggested by a <a href="https://www.nature.com/articles/s41893-023-01132-6">highly disturbing recent paper</a> in <em>Nature</em>, showing how and where the “human climate niche” is likely to shrink. If anything, we are likely to become more reliant on long-distance transport to deliver our food – a prospect no one, myself included, relishes.</p>



<p>If a catastrophe of the kind Chris envisages – and sometimes appears to yearn – were to materialise, people everywhere are likely to become more desperate. The remaining fertile land and water would be even more valued and fiercely defended than they are today.</p>



<p>To me, Chris’s long-standing plan – to move the people to the food, rather than the food to the people – is a further instance of the Great Cruelty of the past two centuries. The Great Cruelty is common to colonialism, capitalism, communism, Nazism, neoliberalism and all the other conquering and interconnected forces that have dominated thought and action during this period. It can be summarised as follows:</p>



<p>People are counters, to be moved in their millions, as interests or ideology dictate, across the board game called Planet Earth.</p>



<p>It’s consistent with the kind of thinking that characterises cities as “human feedlots”.</p>



<h3>Mysteries and Passions</h3>



<p>But never mind. Whether there is enough food and everyone can afford to eat it is, Chris says, a “secondary goal”. This is because it doesn’t “speak to the mysteries and passions of what animates human (or non-human) life”, which Chris describes as our “primary goals”. In obsessing about “productivity, numbers, yields, costs and so forth”, we “risk missing what makes for the flourishing of humans and other organisms”.</p>



<p>Well, call me old-fashioned, or urbanist, or whatever label you choose to apply to me, but I would say that having enough food is pretty damn primary. In any hierarchy of human needs, it features close to the top. I don’t for a moment deny that mysteries and passions are important to us, or that we need meaning and purpose to lead fulfilling lives, but their pursuit can be somewhat hampered if you are starving to death.</p>



<p>This is the heart of the matter. The particular “mysteries and passions” that appeal to people of Chris’s persuasion come first, and the physical requirements of other humans are secondary: they must either fit in somehow, or fall aside.</p>



<p>There is now a wide movement, some of whose leading figures are quoted on or in Chris’s book, that prioritises its mysteries and passions above other people’s survival to the point at which it promotes the idea of <a href="https://orionmagazine.org/article/confessions-of-a-recovering-environmentalist/">“withdrawing” and “walking away”</a> from the effort to prevent Earth systems collapse. On behalf of the rest of the world, such people grandly proclaim that it’s futile to try to stop the slide. We should give up and “adapt to”, even embrace, whatever awaits.</p>



<p>But there is no “away” to walk to. Ecological and social collapse will find us, wherever we go. What some people <em>can</em> escape is the shared responsibility for facing our multiple crises, and their duty of care towards others.</p>



<p>The acceptance of – sometimes apparent longing for – collapse is among the greatest self-indulgences in human history. It is peculiar to people who are either relatively wealthy and insulated, or have the land and means to grow their own food (or both). It is a variation of the prepper mentality, whose props in this case are not bunkers, bitcoin, tinned food and AR-15s, but hoes, scythes and leather jerkins. (Though these “repeasantised” folk might discover that if the calamity does occur, they’ll also need some heavy weaponry to defend their land and crops).</p>



<p>While the Old Testament prophets had to rely on God’s wrath being visited upon the human feedlots, today the curses have more temporal means of realisation. All we now need do is nothing: let the corporations, the oligarchs and the rising consumer demand that are breaking Earth systems have their way, and some form of collapse is likely to occur, with or without God’s wrath. By deliberately stepping back from the struggle to contain these forces, and even seeking in some cases to dissuade others from participating, they make this possibility more likely.</p>



<p>My belief is that we have no right to grant ourselves this indulgence. Given that rich nations and wealthy people are primarily responsible for the planetary dysbiosis we all face, including the massive burdens the food system imposes on the living world, we all have a duty to engage. Engaging means valuing the lives of others as we value our own. Living on this planet, especially as a member of a privileged society, our lives are intimately bound with the lives of others, including those who live thousands of miles away. We cannot excuse ourselves from the responsibilities we owe to each other. Our aim should be not to use societal collapse as a tool to shape the world to our tastes, but to seek to avert societal collapse.</p>



<h3>Discordant Notes</h3>



<p>There are no perfect solutions in an imperfect world. Everything we might propose, including all the ways forward I suggest in <em>Regenesis</em>, has downsides. We are working in a very tight space, one in which 8 billion people and more need to be fed, within an Earth system whose planetary boundaries have already been breached, to a large extent as a result of food production.</p>



<p>There are no remaining comfort zones. There is no longer – if there ever was – scope for ideological congruence, for solutions that fit snugly into any one worldview. We will find ourselves in disconcerting places. We will be assailed by cognitive dissonance.</p>



<p>In seeking to address our great predicaments, we should be, as much as is humanly possible, open-minded, open-hearted, receptive to evidence, argument and persuasion. The answers, contradictory, incomplete and inadequate as they will always be, will be social, political, economic, organisational <em>and</em> technological. We might not like some of our own conclusions. But it’s not just about us.</p>



<p>When some writers and campaigners, prioritising their own mysteries and passions, appear to treat billions of people as disposable, it should tell us something important: we need to check ourselves. We need to ask what impulses we are following, whether we are really seeking the best outcomes for humanity and the living planet, or simply avoiding cognitive pain. We need, as much as we are able, to set our passions aside.</p>



<p><em>Please feel free to republish this essay, without asking my permission, as long as the original source is linked and credited.</em></p>



<p>www.monbiot.com</p>
    </div>

            
    
</article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MagicaVoxel – A free voxel art editor and interactive path tracing renderer (243 pts)]]></title>
            <link>https://ephtracy.github.io/</link>
            <guid>38050106</guid>
            <pubDate>Sat, 28 Oct 2023 14:32:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ephtracy.github.io/">https://ephtracy.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38050106">Hacker News</a></p>
<div id="readability-page-1" class="page">

    <div>
           <p>

                <a href="#">
                    E
                </a>
            </p>


            <div id="mv_nav">
                <ul>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_main" id="mv_main" mv_content_ref="mv_main.html">MagicaVoxel</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=magicacsg" id="magicacsg" mv_content_ref="magicacsg.html">MagicaCSG</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=aerialod" id="aerialod" mv_content_ref="aerialod.html">Aerialod</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_renderer" id="mv_renderer" mv_content_ref="mv_renderer.html">Viewer</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_controls" id="mv_controls" mv_content_ref="mv_controls.html">Shortcuts</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_commands" id="mv_commands" mv_content_ref="mv_commands.html">Commands</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_resource" id="mv_resource" mv_content_ref="mv_resource.html">Resources</a></li>
                    <li><a href="https://ephtracy.github.io/index.html?page=mv_contact" id="mv_contact" mv_content_ref="mv_contact.html">Contact</a></li>
                </ul>
            </div>
        </div>

    

    



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The UK's Controversial Online Safety Act Is Now Law (129 pts)]]></title>
            <link>https://www.wired.com/story/the-uks-controversial-online-safety-act-is-now-law/</link>
            <guid>38048811</guid>
            <pubDate>Sat, 28 Oct 2023 10:56:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/the-uks-controversial-online-safety-act-is-now-law/">https://www.wired.com/story/the-uks-controversial-online-safety-act-is-now-law/</a>, See on <a href="https://news.ycombinator.com/item?id=38048811">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Jeremy Wright was the first of five UK ministers charged with pushing through the British government’s landmark legislation on regulating the internet, the Online Safety Bill. The current UK government likes to brand its initiatives as “<a data-offer-url="https://bylinetimes.com/2023/04/28/the-uks-world-beating-rhetoric-a-distraction-from-reality/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://bylinetimes.com/2023/04/28/the-uks-world-beating-rhetoric-a-distraction-from-reality/&quot;}" href="https://bylinetimes.com/2023/04/28/the-uks-world-beating-rhetoric-a-distraction-from-reality/" rel="nofollow noopener" target="_blank">world-beating</a>,” but for a brief period in 2019 that might have been right. Back then, three prime ministers ago, the bill—or at least the white paper that would form its basis—outlined an approach that recognized that social media platforms were already de facto arbiters of what was acceptable speech on large parts of the internet, but that this was a responsibility they didn’t necessarily want and weren’t always capable of discharging. Tech companies were pilloried for things that they missed, but also, by free speech advocates, for those they took down. “There was a sort of emerging realization that self-regulation wasn’t going to be viable for very much longer,” Wright says. “And therefore, governments needed to be involved.”</p><p>The bill set out to define a way to handle “legal but harmful” content—material that wasn’t explicitly against the law but which, individually or in aggregate, posed a risk, such as health care disinformation, posts encouraging suicide or eating disorders, or political disinformation with the potential to undermine democracy or create panic. The bill had its critics—notably, those who worried it gave Big Tech too much power. But it was widely praised as a thoughtful attempt to deal with a problem that was growing and evolving faster than politics and society were able to adapt. Of his 17 years in parliament, Wright says, “I’m not sure I’ve seen anything by way of potential legislation that’s had as broadly based a political consensus behind it.”</p><p>Having passed, eventually, through the UK’s two houses of Parliament, the bill received royal assent today. It is no longer world-beating—the European Union’s competing <a href="https://www.theverge.com/23845672/eu-digital-services-act-explained" target="_blank">Digital Services Act</a> came into force in August. And the Online Safety Act enters into law as a broader, more controversial piece of legislation than the one that Wright championed. The act’s more than 200 clauses cover a wide spectrum of illegal content that platforms will be required to address and give platforms a “duty of care” over what their users—particularly children—see online. Some of the more nuanced principles around the harms caused by legal but harmful content have been watered down, and added in is a highly divisive requirement for messaging platforms to scan users’ messages for illegal material, such as child sexual abuse material, which tech companies and privacy campaigners say is an unwarranted attack on encryption.</p><p>Companies, from Big Tech down to smaller platforms and messaging apps, will need to comply with a long list of new requirements, starting with age verification for their users. (Wikipedia, the eighth-most-visited website in the UK, has said it <a href="https://www.bbc.co.uk/news/technology-65388255">won’t be able to comply</a> with the rule because it violates the Wikimedia Foundation’s principles on collecting data about its users.) Platforms will have to prevent younger users from seeing age-inappropriate content, such as pornography, cyberbullying, and harassment; release risk assessments on potential dangers to children on their services; and give parents easy pathways to report concerns. Sending threats of violence, including rape, online will now be illegal, as will assisting or encouraging self-harm online or transmitting deepfake pornography, and companies will need to quickly act to remove them from their platforms, along with scam adverts.</p><p>In a statement, UK Technology Secretary Michelle Donelan said: “The Bill protects free speech, empowers adults and will ensure that platforms remove illegal content. At the heart of this Bill, however, is the protection of children. I would like to thank the campaigners, parliamentarians, survivors of abuse and charities that have worked tirelessly, not only to get this Act over the finishing line, but to ensure that it will make the UK the safest place to be online in the world.”</p><p>Enforcement of the act will be left to the UK’s telecommunications regulator, Ofcom, which <a data-offer-url="https://www.ofcom.org.uk/online-safety/information-for-industry/roadmap-to-regulation/0623-update" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ofcom.org.uk/online-safety/information-for-industry/roadmap-to-regulation/0623-update&quot;}" href="https://www.ofcom.org.uk/online-safety/information-for-industry/roadmap-to-regulation/0623-update" rel="nofollow noopener" target="_blank">said in June</a> that it would begin consultations with industry after royal assent was granted. It’s unlikely that enforcement will begin immediately, but the law will apply to any platform with a significant number of users in the UK. Companies that fail to comply with the new rules face fines of up to £18 million ($21.9 million) or 10 percent of their annual revenue, whichever is larger.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Some of the controversy around the act is less about what is in it and more about what isn’t. The long passage of the legislation means that its development straddled the Covid-19 pandemic, giving legislators a live view of the social impact of mis- and disinformation. The spread of anti-vaccination and anti-lockdown messages became an impediment to public health initiatives. After the worst of the pandemic was over, those same falsehoods fed into <a href="https://www.wired.com/story/15-minute-cities-conspiracy-climate-denier/">other conspiracy theories</a> that continue to disrupt society. The original white paper that was the bill’s foundation included proposals for compelling platforms to tackle this kind of content—which individually might not be illegal but which en masse creates dangers. That’s not in the final legislation, although the act does create a new offense of “false communications,” criminalizing deliberately causing harm by communicating something the sender knows to be untrue.</p><p>“One of the most important things was tackling harms that happen at scale. And because it’s focused so much on individual pieces of content, it’s missed that,” says Ellen Judson, head of the digital research hub at the think tank Demos. The act includes strict rules forcing platforms to move swiftly to remove any illegal post—such as terrorist content or child sexual abuse material—but not on disinformation campaigns comprised of a drip-drip of misleading content, failing to understand that “when that turns into things going viral and spreading, then the harm can occur cumulatively.”</p><p>Wright says that the exclusion of disinformation and misinformation from the bill was partly due to confusion between the remits of different departments. The Department of Culture, Media and Sport “was told that the Cabinet Office would be taking care of all of this. ‘Don’t you worry your pretty little heads about it, it’ll be done elsewhere in something called the <a data-offer-url="https://hansard.parliament.uk/commons/2019-07-22/debates/19072238000019/DefendingDemocracyProgramme" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://hansard.parliament.uk/commons/2019-07-22/debates/19072238000019/DefendingDemocracyProgramme&quot;}" href="https://hansard.parliament.uk/commons/2019-07-22/debates/19072238000019/DefendingDemocracyProgramme" rel="nofollow noopener" target="_blank">Defending Democracy agenda</a>,’” he says. “And then I think, subsequently, it wasn’t really. So I think … there still is a gap there.”</p><p>Under the Act, bigger platforms will be expected to police potentially harmful, but not illegal, content by applying their own standards more consistently than they currently do—something that free-speech campaigners have decried as giving private companies control over what’s acceptable discourse online, but which some experts on dis- and misinformation say is a cop-out that means Big Tech will be less accountable for spreading falsehoods. Legal experts, however, say compliance with the law will require platforms to be more transparent and proactive. “They have to put all of those processes in place as to how their decisions will be made, or they risk actually being seen as a platform that is controlling all kinds of free speech,” says Emma Wright, technology lead at the law firm Harbottle &amp; Lewis. That’s likely to become quite a significant burden. “It’s the new <a href="https://www.wired.com/story/gdpr-2022/">GDPR</a>,” she says.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>By far the most divisive clause out of the more than 300 pages of the Online Safety Act is Section 122, which has been widely interpreted as compelling companies to scan users’ messages to make sure that they aren’t transmitting illegal material. That would be incredibly difficult—perhaps even impossible—to do without breaking the end-to-end encryption on platforms such as WhatsApp and Signal. End-to-end encryption means that the sender and recipient of a message can see its content but the owner of the platform that it’s sent on cannot. The only way to comply with the law, experts say, would be to put so-called client-side scanning software on users’ devices to examine messages before they’re sent, which would make the encryption largely useless. The government said during the bill’s development that companies could find a technical solution to scan messages without undermining encryption; companies and experts countered that that technology doesn’t, and may never, exist.</p><p>“That gives Ofcom, as a regulator, the ability to obligate people like us to go and put third-party content monitoring [on our products] that unilaterally scans everything going through the apps,” Matthew Hodgson, CEO of encrypted messaging company Element, told WIRED before the bill passed. “That’s undermining the encryption and providing a mechanism where bad actors of any kind could compromise the scanning system in order to steal the data flying around the place.”</p><p>Companies whose products depend on end-to-end encryption threatened to leave the country, including Signal. Meta said it may pull WhatsApp from the UK if the bill were to pass. That cliff edge has come and gone, and both services are still available—albeit after an 11th-hour restatement by the government that it wouldn’t force platforms to adopt nonexistent technology to scan users’ messages—which was seen by some as a climbdown.</p><p>However, the clause remains in the act, which worries privacy and free-speech activists, who see it as part of a spectrum of threats against encryption. If the Online Safety Act means companies have to remove encryption or circumvent it using client-side scanning, “it then potentially opens [data] up to being scooped up into the broader surveillance apparatus,” according to Nik Williams, policy and campaigns officer at the campaign group Index on Censorship.</p><p>The Online Safety Act has concerning overlaps with another piece of legislation, the Investigatory Powers Act, which allows the government to compel platforms to remove encryption. Williams says the overlap between the two pieces of legislation creates “a surveillance gateway between the OSB and the IPA in that this can give the security services, such as MI5, MI6, and GCHQ, access to data they previously could not access … I would say it’s probably an unprecedented expansion of surveillance powers.”</p><p>The morning after the Online Safety Bill passed through the House of Lords, the UK Home Office <a href="https://www.bbc.co.uk/news/technology-66854622">launched a new campaign</a> against encrypted messaging, specifically targeting Facebook Messenger.</p><p>Former minister Jeremy Wright says that the question over encryption “is frankly not resolved. I think the government has sort of dodged around giving a concluded view on what it means for encryption.” However, he says, the answer is unlikely to be as absolute as the act’s opponents are making out. Encryption won’t be banned, he says, but platforms will have to explain how their policies around it balance safety with their users’ right to privacy. “If you can meet those [safety] duties by using encryption or with encryption as part of the service, you’re fine,” he says. If not, “you have a problem … it can’t be true, surely, that a platform is entitled to say, ‘Well, I operate encryption, so that’s a get-out-of-jail-free card for me on the safety duties.’”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google agrees to invest up to $2B in OpenAI rival Anthropic (379 pts)]]></title>
            <link>https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/</link>
            <guid>38048155</guid>
            <pubDate>Sat, 28 Oct 2023 08:24:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/">https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/</a>, See on <a href="https://news.ycombinator.com/item?id=38048155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="primary-image" role="figure" aria-describedby="primary-image-caption"><figure><div data-testid="Image"><p><img src="https://cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg" srcset="https://www.reuters.com/resizer/R4Z2uz8RcluFRA-5t_A4vBPD5ZQ=/480x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 480w,https://www.reuters.com/resizer/0JGEgq7x2OviOX_SI0PEgvGT6O4=/960x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 960w,https://www.reuters.com/resizer/lzEztbC-1tYEiIpLH_S6m41FrYQ=/1080x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 1080w,https://www.reuters.com/resizer/Ht-xjY7xrUmaianFeFUDU9ssJ_U=/1200x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 1200w" sizes="(min-width: 1024px) 560px, (min-width: 1440px) 700px, 100vw" width="5342" height="3027" alt="An illuminated Google logo is seen inside an office building in Zurich"></p></div><p data-testid="Body"><span>An illuminated Google logo is seen inside an office building in Zurich, Switzerland December 5, 2018. REUTERS/Arnd Wiegmann/File Photo <a data-testid="Link" href="https://www.reutersagency.com/en/licensereuterscontent/?utm_medium=rcom-article-media&amp;utm_campaign=rcom-rcp-lead" target="_blank" referrerpolicy="no-referrer-when-downgrade"> Acquire Licensing Rights</a></span></p></figure></div><div><p data-testid="paragraph-0">Oct 27 (Reuters) - Alphabet's <a data-testid="Link" href="https://www.reuters.com/markets/companies/GOOGL.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(GOOGL.O)</a> Google has agreed to invest up to $2 billion in the artificial intelligence company Anthropic, a spokesperson for the startup said on Friday.</p><p data-testid="paragraph-1">The company has invested $500 million upfront into the OpenAI rival and agreed to add $1.5 billion more over time, the spokesperson said.</p><p data-testid="paragraph-2">Google is already an investor in Anthropic, and the fresh investment would underscore a ramp-up in its efforts to better compete with Microsoft <a data-testid="Link" href="https://www.reuters.com/markets/companies/MSFT.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(MSFT.O)</a>, a major backer of ChatGPT creator OpenAI, as Big Tech companies race to infuse AI into their applications.</p><p data-testid="paragraph-3">Amazon.com <a data-testid="Link" href="https://www.reuters.com/markets/companies/AMZN.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(AMZN.O)</a> also said <a data-testid="Link" href="https://www.reuters.com/markets/deals/amazon-steps-up-ai-race-with-up-4-billion-deal-invest-anthropic-2023-09-25/" referrerpolicy="no-referrer-when-downgrade">last month</a> it would invest up to $4 billion in Anthropic to compete with growing cloud rivals on AI.</p><p data-testid="paragraph-4">In Amazon's quarterly report to the U.S. Securities and Exchange Commission this week, the online retailer detailed it had invested in a $1.25 billion note from Anthropic that can convert to equity, while its ability to invest up to $2.75 billion in a second note expires in the first quarter of 2024.</p><p data-testid="paragraph-5">Google declined to comment, and Amazon did not immediately respond to a Reuters request for comment.</p><p data-testid="paragraph-6">The Wall Street Journal earlier reported the news of Google's latest agreement with Anthropic.</p><p data-testid="paragraph-7">The rising number of investments shows ongoing maneuvering by cloud companies to secure ties with the AI startups that are reshaping their industry.</p><p data-testid="paragraph-8">Anthropic, which was co-founded by former OpenAI executives and siblings Dario and Daniela Amodei, has shown efforts to secure the resources and deep-pocketed backers needed to compete with OpenAI and be leaders in the technology sector.</p><p data-testid="Body">Reporting by Krystal Hu in New York and Chavi Mehta in Bengaluru; Additional reporting by Jeffrey Dastin; Editing by Anil D'Silva, Devika Syamnath and Chris Reese</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank" referrerpolicy="no-referrer-when-downgrade">The Thomson Reuters Trust Principles.</a></p><div><address><p data-testid="Body">Krystal reports on venture capital and startups for Reuters. She covers Silicon Valley and beyond through the lens of money and characters, with a focus on growth-stage startups, tech investments and AI. She has previously covered M&amp;A for Reuters, breaking stories on Trump's SPAC and Elon Musk's Twitter financing. Previously, she reported on Amazon for Yahoo Finance, and her investigation of the company's retail practice was cited by lawmakers in Congress. Krystal started a career in journalism by writing about tech and politics in China. She has a master's degree from New York University, and enjoys a scoop of Matcha ice cream as much as getting a scoop at work. </p></address></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WinterJS (134 pts)]]></title>
            <link>https://wasmer.io/posts/announcing-winterjs-service-workers</link>
            <guid>38047872</guid>
            <pubDate>Sat, 28 Oct 2023 07:19:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wasmer.io/posts/announcing-winterjs-service-workers">https://wasmer.io/posts/announcing-winterjs-service-workers</a>, See on <a href="https://news.ycombinator.com/item?id=38047872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today we are incredibly excited to announce <a href="https://github.com/wasmerio/winterjs">WinterJS</a> (<a href="https://wasmer.io/wasmer/winterjs"><code>wasmer/winterjs</code> package</a>).</p>
<p>WinterJS is a JavaScript Service Workers server written in Rust, that uses the SpiderMonkey runtime to execute JavaScript (the same runtime that Firefox uses). We chose to follow the <a href="https://wintercg.org/">WinterCG</a> specification to aim for maximum compatibility with other services such as Cloudflare Workers, Deno Deploy and Vercel (hence the name <em>WinterJS</em>).</p>
<p>WinterJS is not only <em>blazing fast™️</em> but can also be compiled to WebAssembly <a href="https://wasix.org/">thanks to WASIX</a> and thus also run fully with Wasmer.</p>
<p>Let's see how it works. We'll start by creating a simple <code>serviceworker.js</code> file that just returns a simple "hello world";</p>
<pre tabindex="0"><code><span><span>addEventListener</span><span>(</span><span>'fetch'</span><span>, (</span><span>req</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>  req.</span><span>respondWith</span><span>(</span><span>`hello world from ${</span><span>req</span><span>.</span><span>request</span><span>.</span><span>url</span><span>.</span><span>href</span><span>}`</span><span>);</span></span>
<span><span>});</span></span></code></pre>
<p>Running it with WinterJS is as simple as this:</p>
<pre tabindex="0"><code><span><span>$</span><span> wasmer run wasmer</span><span>/</span><span>winterjs </span><span>--</span><span>net </span><span>--</span><span>mapdir </span><span>/</span><span>app</span><span>:.</span><span> </span><span>/</span><span>app</span><span>/</span><span>serviceworker</span><span>.</span><span>js</span></span></code></pre>
<blockquote>
<p>WinterJS can also be run natively with Rust (<code>cargo install --git https://github.com/wasmerio/winterjs &amp;&amp; winterjs serviceworker.js</code>).
You can find the source code of WinterJS in the GitHub repo: <a href="https://github.com/wasmerio/winterjs">https://github.com/wasmerio/winterjs</a></p>
</blockquote>
<p>Thanks to the WASIX capabilities of WinterJS, the JavaScript service worker can also be deployed to <a href="https://wasmer.io/products/edge">Wasmer Edge</a>.
Check the working demo here: <a href="https://js-service-worker-demo.wasmer.app/">https://js-service-worker-demo.wasmer.app/</a></p>
<hr>
<p>And now that you have seen a sneak peak on how to use WinterJS, lets do a deep dive on our journey building it.</p>
<h2>Choosing the JS engine</h2>
<p>Before starting on the quest of creating a JavaScript Service Workers server, we analyzed the Javacript runtimes that we could use.</p>
<p>Here are the main requirements we have for such JavaScript runtime:</p>
<ul>
<li><strong>Speed</strong>: It should be fast to run</li>
<li><strong>Wasm-compatible</strong>: It should be able to run without restrictions in a Wasm environment (such as Wasmer)</li>
<li><strong>Development time</strong>: We should be able to iterate fast on it</li>
</ul>
<p>And here are the JS runtimes that we analyzed:</p>
<ul>
<li><strong>QuickJS.</strong> Challenges:
<ul>
<li>We will need to implement all the JS apis diff (<code>peformance.now()</code>, <code>addEventListener</code>, …)</li>
<li>We need to implement the serviceWorker API entirely in C</li>
</ul>
</li>
<li><strong>Static Hermes.</strong> Challenges:
<ul>
<li>Node.js polyfills not available in static mode</li>
<li>Not a lot of functions (such as http calls) are available in the polyfill</li>
<li>Had to make it compile to Wasm with WASIX</li>
</ul>
</li>
<li><strong>Bun</strong> (JavascriptCore). Challenges:
<ul>
<li>Zig not fully supporting WASIX</li>
<li>Compiling JavascriptCore to WASIX is possible (was done before), but not trivial</li>
</ul>
</li>
<li><strong>MozJS</strong> (SpiderMonkey). Challenges:
<ul>
<li>We will need to implement all the JS apis diff (<code>peformance.now()</code>, <code>addEventListener</code>, …)</li>
<li>We need to implement the serviceWorker API (in Rust)</li>
<li>We will need to plug the service worker with a WASIX http server</li>
</ul>
</li>
<li><strong>Node.js</strong> (v8). Challenges:
<ul>
<li>Compile v8 in jitless mode to Wasm is an unknown-unknown</li>
</ul>
</li>
</ul>
<h2>Using SpiderMonkey with mozjs</h2>
<p>After a few runtime trials we set on SpiderMonkey as the most reasonable approach that fitted our tight timeline.</p>
<p>So we begin porting. We started with a fork of mozjs that supported a <strong><a href="https://cfallin.org/blog/2023/10/11/spidermonkey-pbl/">new compilation tier called PBI</a></strong> (Portable Baseline Interpreter).</p>
<p>After some work on the mozjs build system to target WASIX, we were able to bypass most of the issues, except one: the bindings generation.</p>
<p>The bindings that allow using the SpiderMonkey C++ API from Rust were automatically generated using c-bindgen. Plugging those bindings onto WASIX was a challenge so we simply decided to target a 32 bit system and modify them by hand (a 32,000 file!) to target <a href="https://wasix.org/">WASIX</a>.</p>
<p>And voilá… everything worked!</p>
<p>However, after adding a few missing resources to JS, we realized that perhaps mozjs didn’t have the easiest API to use:</p>
<pre tabindex="0"><code><span><span>unsafe</span><span> </span><span>extern</span><span> </span><span>"C"</span><span> </span><span>fn</span><span> </span><span>base64_encode</span><span>(cx</span><span>:</span><span> </span><span>*mut</span><span> </span><span>JSContext</span><span>, argc</span><span>:</span><span> </span><span>u32</span><span>, vp</span><span>:</span><span> </span><span>*mut</span><span> </span><span>Value</span><span>) </span><span>-&gt;</span><span> </span><span>bool</span><span> {</span></span>
<span><span>    </span><span>let</span><span> args </span><span>=</span><span> </span><span>CallArgs</span><span>::</span><span>from_vp</span><span>(vp, argc);</span></span>
<span></span>
<span><span>    </span><span>if</span><span> args</span><span>.</span><span>argc_ </span><span>&lt;</span><span> </span><span>1</span><span> {</span></span>
<span><span>        </span><span>return</span><span> </span><span>false</span><span>;</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    </span><span>let</span><span> source </span><span>=</span><span> </span><span>js_try!</span><span>(cx, </span><span>raw_handle_to_string</span><span>(cx, args</span><span>.</span><span>get</span><span>(</span><span>0</span><span>)));</span></span>
<span><span>    </span><span>let</span><span> result </span><span>=</span><span> </span><span>::</span><span>base64</span><span>::</span><span>engine</span><span>::</span><span>general_purpose</span><span>::</span><span>STANDARD</span><span>.</span><span>encode</span><span>(bytes);</span></span>
<span></span>
<span><span>    </span><span>rooted!</span><span>(</span><span>in</span><span>(cx) </span><span>let</span><span> </span><span>mut</span><span> rval </span><span>=</span><span> </span><span>UndefinedValue</span><span>());</span></span>
<span><span>    result</span><span>.</span><span>to_jsval</span><span>(cx, rval</span><span>.</span><span>handle_mut</span><span>());</span></span>
<span></span>
<span><span>    args</span><span>.</span><span>rval</span><span>()</span><span>.</span><span>set</span><span>(rval</span><span>.</span><span>get</span><span>());</span></span>
<span></span>
<span><span>    </span><span>true</span></span>
<span><span>}</span></span></code></pre>
<h2>Using SpiderMokey with spiderfire</h2>
<p>Thankfully, the <a href="https://github.com/Redfire75369/spiderfire/">spiderfire</a> project had been working on improving the API surface for using SpiderMonkey from Rust.</p>
<p>So the example laid out before now looks way simpler and easier to read/maintain:</p>
<pre tabindex="0"><code><span><span>#[js_fn]</span></span>
<span><span>fn</span><span> </span><span>btoa</span><span>&lt;'</span><span>cx</span><span>&gt;(val</span><span>:</span><span> </span><span>String</span><span>) </span><span>-&gt;</span><span> </span><span>String</span><span> {</span></span>
<span><span>    </span><span>let</span><span> bytes </span><span>=</span><span> val</span><span>.</span><span>as_bytes</span><span>();</span></span>
<span><span>    </span><span>::</span><span>base64</span><span>::</span><span>engine</span><span>::</span><span>general_purpose</span><span>::</span><span>STANDARD</span><span>.</span><span>encode</span><span>(bytes)</span></span>
<span><span>}</span></span></code></pre>
<h2>Deploying to Wasmer Edge</h2>
<p>Compiling WinterJS to WASIX was challenging, but completely worth it. Thanks to its WASIX capabilities we can now run any Javascript Service Workers workloads in <a href="https://wasmer.io/products/edge">Wasmer Edge</a>.</p>
<p>We have put together an <strong>in depth tutorial on how to use Javascript Service Workers in Wasmer Edge</strong>... please check it out!</p>
<p><a href="https://docs.wasmer.io/edge/quickstart/js-wintercg">https://docs.wasmer.io/edge/quickstart/js-wintercg</a></p>
<hr>
<p>We believe WinterJS will enable many new use cases. For example, running Service Workers natively in your IoT device (where Node is too heavy to run), or even in your browser.</p>
<p>At Wasmer we are incredibly excited to see how you will use WinterJS.</p>
<p>WinterJS on GitHub: <a href="https://github.com/wasmerio/winterjs">https://github.com/wasmerio/winterjs</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The fun factor of the video game Uplink (335 pts)]]></title>
            <link>https://vertette.github.io/post/funfactoruplink</link>
            <guid>38047861</guid>
            <pubDate>Sat, 28 Oct 2023 07:16:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vertette.github.io/post/funfactoruplink">https://vertette.github.io/post/funfactoruplink</a>, See on <a href="https://news.ycombinator.com/item?id=38047861">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Remember family computers? Before we had tablets, middle class families would buy overpriced computers from dodgy computer stores that the whole family got to share. As they were mostly used by children and adults who didn't have the slightest understanding of technology, it didn't take long for the family computer to down to a crawl, plagued by dodgy Kazaa downloads and suspicious Internet Explorer toolbars. Between a lack of money to buy a better computer, no easy way to buy digital games in Europe for the longest time and no nearby stores that sold computer games, it took until my brother moved out of the home before I really started getting into PC gaming.</p>
<p>My mom bought a gaming PC for my 14th birthday, and while it was incredibly overpriced for what it was, that didn't matter to me; it was mine and I could do whatever I wanted with it. As the only computer games I was familiar with were <a href="https://www.newgrounds.com/games">free Flash games on Newgrounds</a>, I asked around on various communities what kind of games I could play on a slightly outdated hunk of junk like mine. I forgot who exactly was responsible, but someone recommended I try this old hacking game called <a href="https://store.steampowered.com/app/1510/Uplink/"><em>Uplink</em></a>, and so I found a torrent on ThePirateBay and tried it. I make the following statement with zero hyperbole - that single throwaway recommendation changed my life. Various hours later, I fell in love with it so hard that I made a Steam account just so I could buy the game and support the developers properly.</p>
<p><img src="https://vertette.github.io/img/uplink_hardware.png" alt="A screenshot of the Uplink hardware upgrade screen">
<em>Uplink's interface might be outdated and slightly janky, but dammit, it still looks very cool.</em></p>
<p>If you're unfamiliar with it, here's how it works: <em>Uplink</em> is a hacking simulator reminiscent of old hacking movies like <a href="https://www.imdb.com/title/tt0105435/"><em>Sneakers</em></a> and <a href="https://www.imdb.com/title/tt0113243/"><em>Hackers</em></a>, where the portrayal is less about realism and more about flashiness. You play as a hacker who does various odd jobs like changing people's identities or destroying valuable data. At the beginning of the game, hacking is as easy as using the password breaker on a password screen and finishing up in less than five minutes to avoid getting caught, but the game quickly starts bombarding you with new concepts - deleting logs to avoid being tracked down, shutting down security systems that get in your way and travelling through local area networks. The game is never outright unfair, as most information you need to get through the game can be found in the in-game help section, but certain concepts require a bit of trial and error before you truly get how they work and that can result in you getting caught by the authorities, which results in an instant game over. Thankfully, starting over and getting back to where you were before isn't as daunting as it seems due to <em>Uplink</em>'s fairly open structure. While the game is a bit on the short side, there's enough depth to its mechanics to feel satisfying to master, and the realization that a game that gave you so much trouble at first has turned into a total cakewalk can't be matched.</p>
<p>Before <em>Uplink</em>, I only really played games like <em>Mario</em>, <em>Grand Theft Auto</em> and <em>Alien Hominid</em>; games that might or might not feature mature content, but were decidedly arcadey in nature. They didn't care that much about immersion or emotional engagement. <em>Uplink</em> was a different beast: it pulled me right in with its <a href="https://www.youtube.com/watch?v=QliQ0livbeQ">beautiful ambient soundtrack</a>, retrofuturistic visuals and gameplay that was unlike anything I had ever experienced. Sure, it might not have any fancy 3D models and complex shaders, but I still felt absorbed in a way no other game had done before. Its gameplay was highly addictive and its presentation deceptively brilliant, with a story that would've been deemed too ambitious for an AAA game even at the time. I became an obsessed man, looking up everything that I could find about the game and its developers, buying their newer games <a href="https://store.steampowered.com/app/1500/Darwinia/"><em>Darwinia</em></a> and <a href="https://store.steampowered.com/app/1520/DEFCON/"><em>DEFCON</em></a> and reading the <em>Uplink</em> design documents on the Bonus Disk religiously. Before <em>Uplink</em>, I never gave game design much consideration. I never thought about all the possibilities games have to tell unique stories or how certain game mechanics can make you feel certain emotions. So what is it about the gameplay that makes it so engaging, so immersive and so much fun? Well, the answer might not be what you'd expect: even though there's plenty to praise about <em>Uplink</em>'s design, it manages to be so engaging and immersive because it isn't actually that much fun.</p>
<p><img src="https://vertette.github.io/img/uplink_hack.png" alt="A screenshot of an Uplink hack in progress">
<em>Fun fact: the Trace Tracker's beeps were a last minute addition. There is a world map upgrade that shows you exactly how far the administrator's trace is, but nobody buys it because it doesn't beep.</em></p>
<p>That might make it sound like yet another pretentious indie game that sacrificies good gameplay in service of a Very Important Message™, but that's not actually the case. The anticipation of planning your next attack, the tension as the trace tracker's beeps become quicker and quicker as the system administrator starts closing in on you, the euphoria of a successful job that gets quickly swallowed up by the creeping paranoia of whether you properly correctly cleaned up after yourself or not - <em>Uplink</em> is a hurricane of emotions, but a lot of the emotions it invokes aren't exactly what you'd call positive ones. In that sense, while the game <em>can</em> be fun, it can also feel very tense, obtuse and frustrating, and that's important. Without that, the experience would not nearly be as effective at making you feel like a real hacker as it is, even if the moment-to-moment gameplay pretty much boils down to a script kiddie simulator. The way it goes about it elevates it to something much grander, something truly innovative and memorable, and in that sense "fun" is simply too limiting a term to describe <em>Uplink</em>'s design.</p>
<p>That might sound silly to a lot of players, because "if the game's not fun, why bother", right? But there is an actual precedence for this claim, for example horror games. Most people play horror games not to feel amused but to feel spooked, and those two emotions are almost directly on the opposite end of the emotion wheel. If a horror game is fun to you, then it's doing a very bad job. Another good example is <em>Pathologic</em>, a game that deliberately goes out of its way to be an unpleasant experience to sell the setting of a plague-ridden town in a very effective and memorable way. Even though I find it hard to recommend it, I also find it hard to dismiss it as not being worth your time. And I don't want to ruin people's laughs from back when <a href="https://www.gamesradar.com/we-dont-use-the-word-fun-says-the-last-of-us-2-director-neil-druckmann/&amp;utm_campaign=buffer_grtw/">Neill Druckmann infamously claimed they didn't use the word "fun" during development of <em>The Last of Us 2</em></a>, but that <em>is</em> a valid way to design your game. The way he phrased it made it come across as more pretentious than he meant it to, but the gameplay of <em>TLOU2</em> invokes a lot of the same emotions that <em>Uplink</em> does: tension, paranoia, and euphoria. If games really are an art form, then limiting the design to what's fun is ignoring so many other emotional reactions your game can inspire in others.</p>
<p>That doesn't mean that designing a fun game isn't valuable, but it <em>does</em> mean that it's worth exploring emotions through your game that aren't directly adjacent to fun. With all the opportunities the medium of video games has over others, it would be a waste not to.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cortex X2: ARM aims high (177 pts)]]></title>
            <link>https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/</link>
            <guid>38047743</guid>
            <pubDate>Sat, 28 Oct 2023 06:51:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/">https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/</a>, See on <a href="https://news.ycombinator.com/item?id=38047743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Arm has traditionally targeted the low end of the power and performance curve, but just as Intel has been looking to expand into the low power market, ARM is looking to expand into higher power and performance segments. The Cortex X series is at the forefront of this effort.</p>
<blockquote>
<p>Delivers ultimate peak performance within an expanded envelope for power and area.</p>
<cite><a href="https://www.arm.com/products/cortex-x">Cortex-X Custom CPU Program</a>, Arm</cite></blockquote>
<p>Here, we’ll be looking at the Cortex X2 as implemented in the Snapdragon 8+ Gen 1. This SoC features a single X2 core, alongside four Cortex A510 and three Cortex A710 cores. The Cortex X2 in this SoC typically runs at 2.8 GHz, although lscpu indicates its clock speed can range from 787.2 MHz to 3.187 GHz. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?ssl=1"><img data-attachment-id="20270" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_boost/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?fit=971%2C476&amp;ssl=1" data-orig-size="971,476" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_boost" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?fit=971%2C476&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?fit=688%2C337&amp;ssl=1" decoding="async" width="688" height="337" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?resize=688%2C337&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?w=971&amp;ssl=1 971w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?resize=768%2C376&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Tested on an Asus Zenfone 9</figcaption></figure></div>
<p>When placed under load, the Cortex X2 quickly boosts to an intermediate clock speed of 2.56 GHz. After 55 ms, it reaches 2.8 GHz. No higher clock speeds were observed when testing over a longer duration.</p>
<h2>Core Overview</h2>
<p>Cortex X2 is similar to its 7-series cousin, the Cortex A710, but is substantially larger. X2 has more reordering capacity, a wider pipeline, and more execution units. Despite these changes, X2 has a 10-stage pipeline just like A710.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?ssl=1"><img data-attachment-id="23077" data-permalink="https://chipsandcheese.com/cortex_x2-drawio-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?fit=1392%2C1188&amp;ssl=1" data-orig-size="1392,1188" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?fit=1392%2C1188&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?fit=688%2C587&amp;ssl=1" decoding="async" width="688" height="587" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=688%2C587&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?w=1392&amp;ssl=1 1392w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=768%2C655&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=1200%2C1024&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=1320%2C1127&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<h2>Branch Prediction</h2>
<p>Branch prediction is important for any CPU because wasted work from mispredicts will hurt both performance and power efficiency. Cortex X2 gets more area and power budget than other ARM cores, and therefore gets a more capable branch predictor. It can recognize somewhat longer patterns than Cortex A710 and its server cousin, Neoverse N2. Alongside that, it does better when there are a ton of branches in play. </p>
<figure><p><img decoding="async" id="20275" src="https://i1.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_branchhist.png?ssl=1" alt="" width="1162" height="691"><img decoding="async" loading="lazy" id="20169" src="https://i2.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/yitian710_branchhist.png?ssl=1" alt="" width="1154" height="690"></p></figure>
<p>However, ARM’s statement that X2 has an “expanded envelope for power and area” has to be taken in context. X2 still goes into mobile chips even if high end SoCs only feature a single X-series core. Passive smartphone cooling means X2 is still working within a much tighter power budget than desktop CPUs. AMD’s Zen 4 in comparison pulls all the stops to maximize branch prediction accuracy. </p>
<figure><p><img decoding="async" id="20275" src="https://i1.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_branchhist.png?ssl=1" alt="" width="1162" height="691"><img decoding="async" loading="lazy" id="11077" src="https://i2.wp.com/chipsandcheese.com/wp-content/uploads/2022/10/zen4_bpu_pattern.png?ssl=1" alt="" width="1159" height="679"></p></figure>
<p>The branch predictor’s job is to make sure the frontend is well-fed with fetch addresses. Accurately predicting branch direction is one component of this. Another component is delivering those fetch addresses quickly. To do so, the branch predictor keeps a cache of branch destinations, called a branch target buffer (BTB). </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?ssl=1"><img data-attachment-id="20280" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_btb/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?fit=1215%2C557&amp;ssl=1" data-orig-size="1215,557" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_btb" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?fit=1215%2C557&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?fit=688%2C315&amp;ssl=1" decoding="async" loading="lazy" width="688" height="315" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?resize=688%2C315&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?w=1215&amp;ssl=1 1215w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?resize=768%2C352&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?resize=1200%2C550&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Cortex X2’s BTB is mostly unchanged from A710’s. A micro-BTB can handle two taken branches per cycle, and can track up to 64 branches. Then we see about 10K branches tracked with 1-2 penalty cycles. Returns are handled with a 14 entry return stack as well.</p>
<h2>Frontend: Fetch and Decode</h2>
<p>Cortex X2 has an enlarged version of A710’s frontend, and enjoys both increased caching capacity and higher throughput. The micro-op cache grows to 3072 entries, making it larger than Sunny Cove’s. Also, X2 mandates a 64 KB instruction cache, while A710 implementers could pick between 32 KB or 64 KB instruction cache. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?ssl=1"><img data-attachment-id="20286" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/cortex_x2_frontend-drawio-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?fit=1613%2C411&amp;ssl=1" data-orig-size="1613,411" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_frontend.drawio-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?fit=1613%2C411&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?fit=688%2C175&amp;ssl=1" decoding="async" loading="lazy" width="688" height="175" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=688%2C175&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?w=1613&amp;ssl=1 1613w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=768%2C196&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1536%2C391&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1200%2C306&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1600%2C408&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1320%2C336&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Compared to AMD’s Zen 4, X2’s micro-op cache is smaller, but its larger instruction cache is a notable advantage for larger code footprints. If code footprints exceed 32 KB (but not 64 KB), and have a lot of unpredictable branches, Zen 4 will suffer from L2 latency and see a lot of frontend bubbles.</p>
<p>In terms of throughput, X2’s micro-op cache can provide 8 operations per cycle, which is more than enough to feed the 6-wide renamer downstream. The 5-wide decoder can provide generous instruction throughput for larger code footprints and compares favorably to the 4-wide decoders found on Zen 4 and A710. X2 can sustain more than four instructions per cycle even when running code from L2.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?ssl=1"><img data-attachment-id="20289" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_ifetch/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?fit=1086%2C465&amp;ssl=1" data-orig-size="1086,465" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_ifetch" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?fit=1086%2C465&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?fit=688%2C295&amp;ssl=1" decoding="async" loading="lazy" width="688" height="295" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?resize=688%2C295&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?w=1086&amp;ssl=1 1086w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?resize=768%2C329&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>However once you get past L2, Zen 4 pulls ahead again. Thanks to AMD’s very high performance L3 and an aggressive branch predictor, Zen 4 can sustain over 3 IPC when running code from L3. Cortex X2 doesn’t do badly and can still average 1.66 IPC in that case.</p>
<h2>Out of Order Execution</h2>
<p>After micro-ops from the frontend have been renamed, out-of-order execution tracks and executes them as their data dependencies become available. X2 has a much larger OoO engine than A710 while enjoying similar instruction fusion optimizations. ROB size increased to 288 entries with other structure sizes scaled up to match.</p>
<figure><table><tbody><tr><td>Structure</td><td>Entry required if instruction…</td><td>Cortex X2 Capacity</td><td>A710 Capacity</td><td>Zen 4 Capacity</td></tr><tr><td>Reorder Buffer</td><td>exists</td><td>288</td><td>160</td><td>320</td></tr><tr><td>Integer Register File</td><td>writes to an integer register</td><td>~213</td><td>~147</td><td>224</td></tr><tr><td>FP/Vector Register File</td><td>writes to a FP/vector register</td><td>~156x 128-bit</td><td>~124x 128-bit</td><td>192x 512-bit</td></tr><tr><td>Flags Register File</td><td>sets condition flags</td><td>70</td><td>46</td><td>108 documented<br>238 measured</td></tr><tr><td>Load Queue</td><td>reads from memory</td><td>174</td><td>64</td><td>88 documented<br>136 measured</td></tr><tr><td>Store Queue</td><td>writes to memory</td><td>72</td><td>36</td><td>64</td></tr><tr><td>Branch Order Buffer</td><td>potentially affects control flow (NT branches tested here)</td><td>68</td><td>44</td><td>118</td></tr></tbody></table></figure>
<p>X2 ends up getting close to Zen 4 in most areas, and even exceeds it in a few. ARM’s core can keep a staggering number of loads in flight. Instruction fusion allows it to track 249 FP operations pending retirement, while Zen 4 can only track 154. However, Zen 4 does better if 512-bit vectors are used because its large AVX-512 register file lets it keep a lot more explicitly parallel work in flight.</p>
<p>A710 had an overbuilt scheduler considering its ROB capacity and other structure sizes. Cortex X2 brings things back into balance. Integer scheduler capacity is surprisingly similar to Zen 4’s, with four 24 entry queues. Zen 4 shares those scheduler queues with the AGUs, while Cortex X2 has separate AGU schedulers.</p>
<h3>FP/Vector Execution</h3>
<p>Arm’s Cortex 7 series cores had weak vector execution thanks to tight area and power constraints. Cortex X2 uses its larger power and area budget to implement a quad-pipe FP and vector setup. All four pipes can handle common math operations and enjoy the same low floating point execution latency that Cortex A710 does. Cortex X2 is therefore a very strong contender for scalar or 128-bit vector operations. </p>
<p>I wasn’t able to fully utilize all four pipes even with instructions that should have been able to do so (according to the optimization guide), but even so, throughput is very good.</p>
<figure><table><tbody><tr><td></td><td>Cortex X2</td><td>Cortex A710</td><td>Zen 4</td></tr><tr><td>FP32 Add</td><td>2.53 per cycle<br>2 cycle latency</td><td>2 per cycle<br>2 cycle latency</td><td>2 per cycle<br>3 cycle latency</td></tr><tr><td>FP fused multiply-add</td><td>2.53 per cycle<br>4 cycle latency</td><td>2 per cycle<br>4 cycle latency</td><td>2 per cycle<br>4 cycle latency</td></tr><tr><td>128-bit vector INT32 add</td><td>2.53 per cycle<br>2 cycle latency</td><td>2 per cycle<br>2 cycle latency</td><td>4 per cycle<br>1 cycle latency</td></tr><tr><td>128-bit vector INT32 multiply</td><td>1.26 per cycle<br>4 cycle latency</td><td>1 per cycle<br>4 cycle latency</td><td>2 per cycle<br>3 cycle latency</td></tr></tbody></table><figcaption>Latency and throughput is identical for vector versions of those FP operations</figcaption></figure>
<p>Zen 4 still has an advantage with longer vector lengths and lower latency for vector integer operations. But even if Zen 4 uses 256-bit vectors, Cortex X2 can put up a decent fight because it has identical theoretical throughput (per cycle) for common operations. For example, Zen 4 can do two 256-bit FMAs per cycle. Cortex X2 can match that by doing four 128-bit FMAs. AMD’s core also enjoys better scheduling capacity. X2 seems to have a pair of 23 entry schedulers. I couldn’t find any operations that only go to one of the ADDV pipes, so I can’t tell if it’s a single 23 entry queue, or a 11+12 entry setup. I think a pair of dual port schedulers is more likely. AMD’s Zen 4 uses a pair of 32 entry schedulers, giving it 64 FP scheduling entries compared to Cortex X2’s 46.</p>
<figure><img data-attachment-id="20349" data-permalink="https://chipsandcheese.com/x2_fp_exec/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?fit=888%2C275&amp;ssl=1" data-orig-size="888,275" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_fp_exec" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?fit=888%2C275&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?fit=688%2C213&amp;ssl=1" decoding="async" loading="lazy" width="688" height="213" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?resize=688%2C213&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?w=888&amp;ssl=1 888w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?resize=768%2C238&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></figure>
<p>Like Zen 4, X2 has a non-scheduling queue (NSQ) in front of the FP schedulers, which lets the core track more incomplete operations without using a larger scheduler. An NSQ can contain a lot more entries than a scheduling queue, because it doesn’t have to check each entry each cycle to see if it’s ready for execution. With its 29 entry NSQ, Cortex X2 can keep a total of 75 incomplete FP operations in flight. X2 is an improvement over A710, but AMD prioritizes FP execution more. Zen 4 uses a larger 64 entry non-scheduling queue and can keep a total of 128 incomplete FP operations in flight.</p>
<h3>Memory Execution</h3>
<p>Cortex X2 handles memory accesses with three address generation units (AGUs), with some similarities to A710 and Zen 4. The memory subsystem can handle three memory accesses per cycle, of which three can be loads and two can be stores. Its scheduling setup appears similar to the one on <a href="https://chipsandcheese.com/2023/09/11/hot-chips-2023-arms-neoverse-v2/">Neoverse V2</a>, but with slightly smaller scheduling queues and tiny non-scheduling queues in front of them.</p>
<p>After addresses are calculated, the load/store unit has to ensure they appear to execute in program order. Loads might have to get their data from prior in-flight stores. Ideally, data from the store gets sent to a dependent load with minimal delay. But detecting dependencies can be complicated because loads and stores can overlap without matching addresses.</p>
<p>Cortex X2 acts a lot like prior ARM cores starting from Neoverse N1. The load/store unit can forward either half of a 64-bit load to a dependent 32-bit store, but can’t handle any other cases. Fast-path store forwarding has a latency of five cycles, while the slow path incurs a 10-11 cycle penalty.</p>

<p>Zen 4 has a far more robust mechanism for resolving memory dependencies. Any load contained within a prior store can have its data forwarded, and exact address matches can be handled with zero latency. ARM is falling a bit behind here with essentially pre-2010s forwarding capability on a core design going for ultimate performance. However, the slow fallback path on Zen 4 is more expensive at 19-20 cycles, likely indicating Zen 4 has more pipeline stages between between address calculation and store retirement.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?ssl=1"><img data-attachment-id="20370" data-permalink="https://chipsandcheese.com/zen4_stlf-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?fit=2718%2C1189&amp;ssl=1" data-orig-size="2718,1189" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_stlf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?fit=2560%2C1120&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?fit=688%2C301&amp;ssl=1" decoding="async" loading="lazy" width="688" height="301" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=688%2C301&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?w=2718&amp;ssl=1 2718w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1536%2C672&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=2048%2C896&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1200%2C525&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1600%2C700&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1320%2C577&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Same test on Zen 4</figcaption></figure></div>
<p>Cortex X2 does better with avoiding misalignment penalties. Zen 4’s data cache has 32B store alignment, so stores that cross a 32B aligned boundary have a throughput of one per two cycles. X2 doesn’t see any penalty unless accesses cross a 64B cacheline boundary.</p>
<p>Henry Wong <a href="https://blog.stuffedcow.net/2014/01/x86-memory-disambiguation/" data-type="link" data-id="https://blog.stuffedcow.net/2014/01/x86-memory-disambiguation/">experimented</a> with smaller load and store sizes and didn’t see a significant difference. However, vector loads do behave differently on on some CPUs. Cortex X2 can again forward either 64-bit half of a 128-bit store, but curiously can also forward the low 32 bits and merge that with another 32 bits from the data cache to quickly complete a partially overlapping 64-bit load.</p>
<figure><img data-attachment-id="20355" data-permalink="https://chipsandcheese.com/x2_128_stlf/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?fit=2738%2C1321&amp;ssl=1" data-orig-size="2738,1321" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_128_stlf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?fit=2560%2C1235&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?fit=688%2C332&amp;ssl=1" decoding="async" loading="lazy" width="688" height="332" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=688%2C332&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?w=2738&amp;ssl=1 2738w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=768%2C371&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1536%2C741&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=2048%2C988&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1200%2C579&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1600%2C772&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1320%2C637&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>Using str q,[x] and ldr d, [x]</figcaption></figure>
<p>Zen 4’s vector side acts a lot like the scalar integer side, but with a couple cycles of additional latency. AMD can impressively handle misaligned loads with no cost, but again is more prone to hitting misaligned store penalties than Cortex X2. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?ssl=1"><img data-attachment-id="20371" data-permalink="https://chipsandcheese.com/zen4_128_stlf/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?fit=2718%2C1171&amp;ssl=1" data-orig-size="2718,1171" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_128_stlf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?fit=2560%2C1103&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?fit=688%2C296&amp;ssl=1" decoding="async" loading="lazy" width="688" height="296" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=688%2C296&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?w=2718&amp;ssl=1 2718w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=768%2C331&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1536%2C662&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=2048%2C882&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1200%2C517&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1600%2C689&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1320%2C569&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Using movups store and movsd load</figcaption></figure></div>
<h3>Address Translation</h3>
<p>User programs don’t directly address locations in DRAM. Instead, they use virtual addresses, and the operating system sets up a map of virtual address to physical addresses for each process. This allows cool things like swapping to disk when physical memory runs low. However, hardware has to translate addresses on the fly while maintaining high performance. Translation lookaside buffers (TLBs) cache virtual to physical address mappings. TLB hits let the CPU avoid traversing the operating system’s paging structures, which would turn one memory access into several dependent ones.</p>
<p>Cortex X2 has a two-level TLB setup. The first TLB level has 48 entries and is fully associative. It’s a welcome size increase over the 32 entries in A710, but is still smaller than Zen 4’s 72 entry DTLB.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?ssl=1"><img data-attachment-id="20377" data-permalink="https://chipsandcheese.com/x2_tlb-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?fit=1355%2C402&amp;ssl=1" data-orig-size="1355,402" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_tlb.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?fit=1355%2C402&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?fit=688%2C204&amp;ssl=1" decoding="async" loading="lazy" width="688" height="204" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=688%2C204&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?w=1355&amp;ssl=1 1355w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=768%2C228&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=1200%2C356&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=1320%2C392&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L1 DTLB misses can be caught by Cortex X2’s 2048 entry L2 TLB, with a cost of 5 extra cycles. This is a welcome improvement over the Cortex A710’s 1024 entry TLB, and Neoverse N2’s 1280 entries. Cortex X2’s improved TLB sizes let it incur less address translation latency for programs with larger data footprints. It’s still a step behind Zen 4’s 3072 entry L2 TLB, but it matches Zen 2.</p>
<h2>Cache and Memory</h2>
<p>Caching is an important component of a CPU’s performance. In the Snapdragon 8+ Gen 1, Cortex X2 gets a triple level cache hierarchy. The large 64 KB L1D has 4 cycle latency. It’s not the best for a CPU clocked below 3 GHz, considering the old AMD Athlon and Phenom CPUs achieved 3 cycle L1D latency years ago. As a consolation, indexed addressing doesn’t cost an extra cycle like on recent AMD and Intel CPUs.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23091"><img data-attachment-id="23091" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/cortex_x2_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?fit=1239%2C604&amp;ssl=1" data-orig-size="1239,604" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?fit=1239%2C604&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?fit=688%2C335&amp;ssl=1" decoding="async" loading="lazy" width="688" height="335" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?resize=688%2C335&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?w=1239&amp;ssl=1 1239w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?resize=768%2C374&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?resize=1200%2C585&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Arm mandates a 64 KB L1D on Cortex X2, but lets implementers configure the L2 with 512 KB or 1 MB of capacity. The L2 is inclusive of the L1D, so Arm is making a good decision in not offering smaller L2 options. Both L2 configurations have 8-way associativity, so Arm is changing capacity by increasing the number of sets. Qualcomm picked the 1 MB option on the Snapdragon 8+ Gen 1. L2 hits have 11 cycle latency, which comes out to just under 4 nanoseconds. Cortex X2 can’t clock as high as Zen 4, but the short L2 pipeline helps close some of the gap. Just like the L1D, the L2 is always ECC protected. I’m glad Arm isn’t making ECC protection optional.</p>
<p>The L2 has a 256-bit bus to the DSU-110, which connects cores to the rest of the system. Arm lets implementers configure the DSU-110 with up to 16 MB of L3 cache. The L3 is 16-way set associative with power of two capacities, or 12-way set associative if capacity is divisible by 3. Qualcomm in their infinite wisdom has chosen 6 MB of L3 cache, so the Snapdragon 8+ Gen 1’s L3 is 12-way set associative.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23114"><img data-attachment-id="23114" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/dsu-110_layout/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?fit=632%2C334&amp;ssl=1" data-orig-size="632,334" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dsu-110_layout" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?fit=632%2C334&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?fit=632%2C334&amp;ssl=1" decoding="async" loading="lazy" width="632" height="334" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?resize=632%2C334&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>Figure from Arm’s DSU-110 Technical Reference Manual</figcaption></figure></div>
<p>The L3 is arranged into slices, and is filled by victims from core private caches. Cortex X2 suffers higher L3 latency than Zen 4. At the 4 MB test size, its 18.18 ns result is similar to the 17.41 ns seen by the Intel Core i9-12900K’s E-cores. A small 6 MB cache should make up for its lack of capacity by at least being fast, but I suppose that would be asking too much from a mobile SoC. At least it’s reasonable from the ~51 core cycle latency.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23124"><img data-attachment-id="23124" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/dsu-110_l3_configurable_cycles/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?fit=623%2C229&amp;ssl=1" data-orig-size="623,229" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dsu-110_l3_configurable_cycles" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?fit=623%2C229&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?fit=623%2C229&amp;ssl=1" decoding="async" loading="lazy" width="623" height="229" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?resize=623%2C229&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>Part of the L3 pipeline is configurable. Image from Arm’s DSU-110 Technical Reference Manual</figcaption></figure></div>
<p>Arm’s Technical Reference manual suggests five to seven cycles are spent accessing L3 data storage, so the remaining cycles are spent checking tags, traversing the interconnect, and at upper level caches. Program-visible L3 latency includes time spent accessing the L2 TLB, since the L1 TLB is not large enough to cover the L3 cache.</p>
<p>At the 1 GB test size, we see 202 ns of DRAM latency. L2 TLB misses and page walks add potentially heavy address translation latency on top, but separating that from DRAM latency is difficult because there’s no way to use huge pages on Android. It’s not too bad for a cell phone SoC, but is a world apart from desktop or laptop CPUs. It’s also worse than Apple’s M1, which should worry Qualcomm because Apple shares designs across phones and tablets.</p>
<div>
<figure><a href="https://chipsandcheese.com/x2_m1_latency/"><img data-attachment-id="23150" data-permalink="https://chipsandcheese.com/x2_m1_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?fit=1374%2C714&amp;ssl=1" data-orig-size="1374,714" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_m1_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?fit=1374%2C714&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?fit=688%2C358&amp;ssl=1" decoding="async" loading="lazy" width="688" height="358" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=688%2C358&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?w=1374&amp;ssl=1 1374w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=768%2C399&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=1200%2C624&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=1320%2C686&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Apple’s 12 MB shared L2 serves the same role as the Snapdragon 8+ Gen 1’s 6 MB L3, but has both higher capacity and lower latency. I wonder how Cortex X2 would do if it were better fed.</p>
<h3> Bandwidth</h3>
<p>Cortex X2’s three AGUs and triple port data cache allow it to service three 128-bit accesses per cycle. The core therefore can get the same per-cycle L1D bandwidth as A710 and Apple’s M1, and beats older Arm cores like the Neoverse N1 by a large margin. Apple’s M1 still gets an absolute bandwidth lead thanks to higher clocks. Compared to recent x86 cores, X2’s L1D bandwidth is still low due to lower clocks and lack of wider vector support.</p>
<div>
<figure><a href="https://chipsandcheese.com/cortex_x2_read_vs_arm/"><img data-attachment-id="23128" data-permalink="https://chipsandcheese.com/cortex_x2_read_vs_arm/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?fit=1243%2C581&amp;ssl=1" data-orig-size="1243,581" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_read_vs_arm" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?fit=1243%2C581&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?fit=688%2C322&amp;ssl=1" decoding="async" loading="lazy" width="688" height="322" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?resize=688%2C322&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?w=1243&amp;ssl=1 1243w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?resize=768%2C359&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?resize=1200%2C561&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L2 bandwidth is decent at 28 bytes per cycle. It’s close to Apple M1’s L2 bandwidth. Zen 4 and Skylake again enjoy a large L2 bandwidth lead over Cortex X2 thanks to higher clock speeds.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23092"><img data-attachment-id="23092" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/cortex_x2_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?fit=1248%2C578&amp;ssl=1" data-orig-size="1248,578" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?fit=1248%2C578&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?fit=688%2C319&amp;ssl=1" decoding="async" loading="lazy" width="688" height="319" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?resize=688%2C319&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?w=1248&amp;ssl=1 1248w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?resize=768%2C356&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?resize=1200%2C556&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L2 misses go into a transaction queue with size configurable from 72 to 96 entries. The large transaction queue helps the core cope with high L3 latency, so X2’s L3 bandwidth is on par with Skylake. DRAM bandwidth from the single Cortex X2 core is decent at 32.5 GB/s, hinting at the L3’s ability to track a lot of pending misses. The DSU-110’s CHI (Coherent Hub Interface) can track up to 128 reads per master port. If Qualcomm is using that to connect memory controllers, it would explain the decent memory bandwidth in the face of high latency.</p>
<h3>Write Bandwidth</h3>
<p>We can examine bandwidth without latency restrictions by testing writes instead of reads. Normally, writes have much lower bandwidth because a write access involves a read-for-ownership first to fill the line into cache. However, Cortex X2 detects when entire cachelines are being overwritten without any of the data getting read. If that happens to enough consecutive lines, the core’s bus interface switches into write streaming mode. In write streaming mode, cache misses don’t cause fills and simply write out the data. Thus, writes won’t be held back by read latency and RFO bandwidth won’t compete with writebacks.</p>
<div>
<figure><a href="https://chipsandcheese.com/x2_write/"><img data-attachment-id="23133" data-permalink="https://chipsandcheese.com/x2_write/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?fit=1244%2C579&amp;ssl=1" data-orig-size="1244,579" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="X2_write" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?fit=1244%2C579&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?fit=688%2C320&amp;ssl=1" decoding="async" loading="lazy" width="688" height="320" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?resize=688%2C320&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?w=1244&amp;ssl=1 1244w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?resize=768%2C357&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?resize=1200%2C559&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Bandwidth from L1D is lower because only two AGUs can handle writes. But every lower level in the cache hierarchy benefits. L2 bandwidth goes up to 30 bytes per cycle, while L3 bandwidth reaches 67 GB/s. Finally, DRAM bandwidth sits around 41.2 GB/s. I suspect that’s a better reflection of what the memory controller can deliver using its 64bit LPDDR5-6400 interface.</p>
<h2>Final Words</h2>
<p>Arm’s Cortex X line reaches for higher performance with an increased power and area budget. Cortex X2 is the second member of that line. It apparently has an <a href="https://www.eetasia.com/express/mediatek-delivers-efficient-cortex-x2/">area of about 2.1 mm<sup>2</sup></a>, making it just slightly smaller than Zen 4c. While Arm tries to move up the performance ladder, Intel and AMD are trying to move down to hit lower power and area targets. Arm’s efforts to move up the performance ladder mean it’s starting to overlap with AMD and Intel as those x86 companies try to move down into lower power and area targets.</p>
<p>AMD, Arm, and Intel share another commonality. They all have to maintain multiple cores to broaden their coverage of performance targets. AMD has the most modest and cost efficient approach. Zen 4c uses a different physical implementation of the Zen 4 architecture to reduce core area at the cost of clock speed. Intel goes all the way for maximum flexibility. Gracemont is a completely different core than Golden Cove, so Intel is splitting engineering effort between two core lines. Arm lands in the middle. Cortex X2 is a scaled up A710. The two cores have similar scheduler layouts and instruction fusion optimizations, so they’re really siblings rather than completely different designs. Some of Arm’s engineering effort can be shared across both cores, but additional time has to be spent tuning and validating A710 and X2.</p>
<p>To build Cortex X2, Arm took everything in A710 and moved the sliders up. Out-of-order structures enjoy increased capacity. L1, L2, and micro-op cache sizes get larger. X2 gets a quad pipe FPU, giving it a welcome upgrade over A710’s dual pipe one. Floating point units are area hungry because FP operations involve several basic operations under the hood, so X2’s larger area budget is getting put to good use. The L2 TLB is another good use of extra area. A710’s 1024 entry L2 TLB was small by modern standards, so X2’s 2048 entry one is great to see.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23142"><img data-attachment-id="23142" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_arm_slide/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?fit=2264%2C1272&amp;ssl=1" data-orig-size="2264,1272" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_arm_slide" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?fit=2264%2C1272&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?fit=688%2C387&amp;ssl=1" decoding="async" loading="lazy" width="688" height="387" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?w=2264&amp;ssl=1 2264w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1536%2C863&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=2048%2C1151&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1200%2C674&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1600%2C899&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1320%2C742&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Arm’s slide. Labels for some core components added in red</figcaption></figure></div>
<p>Cortex X2 is therefore a cool showing of what Arm’s out-of-order architecture can do when allowed to stretch its legs. Arm’s engineers have made good use of their increased area and power budget to patch up A710’s weakest areas. Newer Cortex X cores carry forward X2’s strengths while using increased transistor budgets to continue patching weaknesses.</p>
<figure><table><tbody><tr><td></td><td>Cortex X2</td><td>Cortex X4</td></tr><tr><td>L1 TLBs</td><td>48 entry iTLB<br>48 entry dTLB</td><td>128 entry iTLB<br>96 entry dTLB</td></tr><tr><td>L2 Cache</td><td>512 KB or 1 MB</td><td>512 KB, 1 MB, or 2 MB</td></tr></tbody></table><figcaption>From looking through the Cortex X4 TRM</figcaption></figure>
<p>I like where Cortex X is going and can see Arm putting pressure on AMD and Intel to keep up the pace. But when your core is stuffed into a SoC with a slow L3 and horrible DRAM latency, it’s going to suffer even when core width and structure sizes look competitive. I hope future implementations will better showcase Cortex X’s potential. </p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80" loading="lazy" decoding="async"> </p>

</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Study Says Maybe Helicopter Parenting Is Making Kids Depressed (135 pts)]]></title>
            <link>https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/</link>
            <guid>38046910</guid>
            <pubDate>Sat, 28 Oct 2023 03:40:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/">https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/</a>, See on <a href="https://news.ycombinator.com/item?id=38046910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-423910">


<h3>from the <i>correlation-and-causation</i> dept</h3>

<p>We’ve been covering, at great length, the moral panic around the claims that social media is what’s making kids depressed. The problem with this narrative is that there’s basically no real evidence to support it. As the American Psychological Association found when it reviewed all the literature, despite many, many dozens of studies done on the impact of social media on kids, <a target="_blank" rel="noreferrer noopener" href="https://www.techdirt.com/2023/05/12/apa-report-says-that-media-politicians-are-simply-wrong-about-kids-social-media-media-then-lies-about-report/">no one was able to establish a causal relationship</a>.</p>
<p>As that report noted, the research seemed to show no inherent benefit or harm for most kids. For some, it showed a real benefit (often around kids being able to find like-minded people online to communicate with). For a very small percentage, it appeared to potentially exacerbate existing issues. And those are really the cases that we should be focused on.</p>
<p>But, instead, the narrative that continues to make the rounds is that social media is inherently bad for kids. That leads to various bills around age verification and age gating to keep kids off of social media.</p>
<p>Supporters of these bills will point to charts like this one, regarding teen suicide rates, noting the uptick correlates with the rise of social media.</p>

<p>Of course, they seem to cherry pick the start date of that chart, because if you go back further, you realize that while the uptick is a concern, it’s still way below what it had been in the 1990s (pre-social media).</p>

<p>In case that embed isn’t working, here’s an image of it:</p>
<figure><img decoding="async" src="https://i0.wp.com/lex-img-p.s3.us-west-2.amazonaws.com/img/bd036c6a-0422-4e33-9c37-427f9f9ddc81-RackMultipart20231025-142-jzngu7.png?ssl=1" alt="Image" data-recalc-dims="1"></figure>
<p>Obviously, the increase in suicides is a concern. But, considering that every single study that tries to link it to social media ends up failing to do so, that suggests that there might be some other factor at play here.</p>
<p>A recent study in the Journal of Pediatrics suggests a compelling alternative. It’s not social media, but the rise of helicopter parenting, in which kids no longer have spaces to just hang out with each other and be kids. It’s titled: <a target="_blank" rel="noreferrer noopener" href="https://doi.org/10.1016/j.jpeds.2023.02.004">Decline in Independent Activity as a Cause of Decline in Children’s Mental Well-being: Summary of the Evidence</a>. If you can’t see the full version, there’s <a target="_blank" rel="noreferrer noopener" href="https://cdn2.psychologytoday.com/assets/2023-02/Children's%20Independence%20IN%20PRESS%20.pdf">a preprint version</a> here.</p>
<p>The research summarizes the decline in “independent mobility” for kids over the last few decades:</p>
<blockquote>
<p><em>Considerable research, mostly in Europe, has focused on children’s independent mobility (CIM), defined as children’s freedom to travel in their neighborhood or city without adult accompaniment. That research has revealed significant declines in CIM, especially between 1970 and 1990, but also some large national differences. For example, surveys regarding the “licenses” (permissions) parents grant to their elementary school children revealed that in England, license to walk home alone from school dropped from 86% in 1971 to 35% in 1990 and 25% in 2010; and license to use public buses alone dropped from 48% in 1971 to 15% in 1990 to 12% in 2010.11 In another study, comparing CIM in 16 different countries (US not included), conducted from 2010 to 2012, Finland stood out as allowing children the greatest freedom of movement. The authors wrote: “At age 7, a majority of Finnish children can already travel to places within walking distance or cycle to places alone; by age 8 a majority can cross main roads, travel home from school and go out after dark alone, by age 9 a majority can cycle on main roads alone, and by age 10 a majority can travel on local buses alone.” Although we have found no similar studies of parental permissions for US children, other data indicate that the US is more like the UK concerning children’s independent mobility than like Finland. For example, National Personal Transportation Surveys revealed that only 12.7% walked or biked to school in 2009 compared with 47.7% in 1969.</em></p>
</blockquote>
<p>And then it notes the general decline in mental health as well, which they highlight started long before social media existed:</p>
<blockquote>
<p><em>Perhaps the most compelling and disturbing evidence comes from studies of suicide and suicidal thoughts. Data compiled by the CDC indicate that the rate of suicide among children under age 15 rose 3.5-fold between 1950 and 2005 and by another 2.4-fold between 2005 and 2020. No other age group showed increases nearly this large. By 2019, suicide was the second leading cause of death for children from age 10 through 15, behind only unintentional injury. Moreover, the 2019 YRBS survey revealed that during the previous year 18.8% of US high school students seriously considered attempting suicide, 15.7% made a suicide plan, 8.9% attempted suicide one or more times, and 2.5% made a suicide attempt requiring medical treatment. We are clearly experiencing an epidemic of psychopathology among young people.</em></p>
</blockquote>
<p>But, unlike those who assume correlation is causation with regards to social media, the researchers here admit there needs to be more. And they bring the goods, pointing to multiple studies that suggest a pretty clear causal relationship, rather than just correlation.</p>
<blockquote>
<p><em>Several studies have examined relationships between the amount of time young children have for self-directed activities at home and psychological characteristics predictive of future wellbeing. These have revealed significant positive correlations between amount of self-structured time (largely involving free play) and (a) scores on two different measures of executive functioning; (b) indices of emotional control and social ability; and (c) scores, two years later, on a measure of self-regulation. There is also evidence that risky play, where children deliberately put themselves in moderately frightening situations (such as climbing high into a tree) helps protect against the development of phobias and reduces future anxiety by increasing the person’s confidence that they can deal effectively with emergencies.</em></p>
<p><em>Studies with adults involving retrospections about their childhood experiences provide another avenue of support for the idea that early independent activity promotes later wellbeing. In one such study, those who reported much free and adventurous play in their elementary school years were assessed as having more social success, higher self-esteem, and better overall psychological and physical health in adulthood than those who reported less such play. In another very similar study, amount of reported free play in childhood correlated positively with measures of social success and goal flexibility (ability to adapt successfully to changes in life conditions) in adulthood. Also relevant here are studies in which adults (usually college students) rated the degree to which their parents were overprotective and overcontrolling (a style that would reduce opportunity for independent activity) and were also assessed for their current levels of anxiety and depression. A systematic review of such studies revealed, overall, positive correlations between the controlling, overprotective parenting style and the measures of anxiety and depression.</em></p>
</blockquote>
<p>They also note that they are not claiming (of course) that this is the sole reason for the declines in mental health. Just that there is strong evidence that it is a key component. They explore a few other options that may contribute, including increased pressure at schools and societal changes. They also consider the impact of social media and digital technologies and note (as we have many times) that there just is no real evidence to support the claims:</p>
<blockquote>
<p><em>Much recent discussion of young people’s mental health has focused on the role of increased use of digital technologies, especially involvement with social media. However, systematic reviews of research into this have provided little support for the contention that either total screen time or time involved with social media is a major cause of, or even correlate of, declining mental health. One systematic review concluded that research on links between digital technology use and teens’ mental health “has generated a mix of often conflicting small positive, negative and null associations” (Odgers &amp; Jensen, 2020). Another, a “review of reviews” concluded that “the association between digital technology use, or social media use in particular, and psychological well-being is, on average, negative but very small” and noted some evidence, from longitudinal research, that negative correlations may result from declining mental health leading to more social media use rather than the reverse (Orben, 2020)</em></p>
</blockquote>
<p>Indeed, if this theory is true, that the lack of spaces for kids to explore and play and experiment without adult supervision <em>is</em> a leading cause of mental health decline, you could easily see how those who are depressed are more likely to <em>seek out</em> those private spaces, and turn to social media, given the lack of any such spaces they can go to physically.</p>
<p>And, if that’s the case, then all of these efforts to ban social media for kids, or to make social media <a rel="noreferrer noopener" href="https://www.techdirt.com/2022/09/20/the-internet-is-not-disneyland-people-should-stop-demanding-it-become-disneyland/" target="_blank">more like Disneyland</a>, could likely end up doing <strong>a lot more harm than good</strong> by cutting off one of the last remaining places where kids can communicate with their peers without adults watching over their every move. Indeed, the various proposals to give parents more access to what their kids are doing online could worsen the problem as well, taking away yet another independent space for kids.</p>
<p>Over the last few years, there’s been a push to <a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2019/05/10/well/family/adventure-playgrounds-junk-playgrounds.html">bring back more “dangerous” play for kids</a>, as people have begun to realize that things may have gone too far in the other direction. Perhaps it’s time we realize that social media fits into that category as well.</p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/age-appropriate-design/" rel="tag">age appropriate design</a>, <a href="https://www.techdirt.com/tag/age-verification/" rel="tag">age verification</a>, <a href="https://www.techdirt.com/tag/depression/" rel="tag">depression</a>, <a href="https://www.techdirt.com/tag/independent-spaces/" rel="tag">independent spaces</a>, <a href="https://www.techdirt.com/tag/mental-health/" rel="tag">mental health</a>, <a href="https://www.techdirt.com/tag/social-media/" rel="tag">social media</a>, <a href="https://www.techdirt.com/tag/studies/" rel="tag">studies</a>, <a href="https://www.techdirt.com/tag/suicide/" rel="tag">suicide</a>, <a href="https://www.techdirt.com/tag/teens/" rel="tag">teens</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A small warning about UDP based protocols (176 pts)]]></title>
            <link>https://boston.conman.org/2023/10/25.1</link>
            <guid>38046448</guid>
            <pubDate>Sat, 28 Oct 2023 02:13:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boston.conman.org/2023/10/25.1">https://boston.conman.org/2023/10/25.1</a>, See on <a href="https://news.ycombinator.com/item?id=38046448">Hacker News</a></p>
Couldn't get https://boston.conman.org/2023/10/25.1: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Staring at a Wall: Embracing Deliberate Boredom (135 pts)]]></title>
            <link>https://www.ch3ngl0rd.com/staring-at-a-wall/</link>
            <guid>38046396</guid>
            <pubDate>Sat, 28 Oct 2023 02:05:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ch3ngl0rd.com/staring-at-a-wall/">https://www.ch3ngl0rd.com/staring-at-a-wall/</a>, See on <a href="https://news.ycombinator.com/item?id=38046396">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    







<p>
    <i>
        <time datetime="2023-10-28">
            28 Oct, 2023
        </time>
    </i>
</p>

<p>You should spend more time being bored.</p>
<p>I spent twenty minutes staring at a wall. Was it worth my time? Yes. Did I look a little bit crazy doing it? Maybe a little.</p>
<p>My friend <a href="https://www.joshshipton.com/">Josh Shipton</a> recently talked about the <a href="https://www.joshshipton.com/boredom.html">Power of Embracing Boredom</a>, and how boredom is needed for your mind to process your thoughts. One exercise he recommends is to sit down, and stare at a wall. I had my doubts, but after one session of staring at a wall, I found it extremely rewarding.</p>
<p>The exercise is quite simple to do:</p>
<ol>
<li>Set up a timer for ten to thirty minutes</li>
<li>Stare at a wall</li>
</ol>
<p>I found the exercise to be most effective with twenty minutes and a white wall.</p>
<h2 id="unexpected-insights">Unexpected Insights</h2>
<p><img alt="thoughts by ch3ngl0rd ᕦʕ •ᴥ•ʔᕤ" src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/ch3ngl0rd-1698456991-0.jpg"></p>
<p>During my walling session, a scene from The Lego Movie unexpectedly came to mind: the moment when the old wizard and the emo girl discover Emmet's profoundly empty mind. While they initially mock him for this and crush the idea of a double-decker couch, their laughter is cut short upon witnessing the vision of 'The Man Upstairs'.</p>
<blockquote>
<p>Master Builders spend years training themselves to clear their minds enough to have even a fleeting glimpse of The Man Upstairs.</p>
<p>- The Old Wizard Guy</p>
</blockquote>
<p>This exercise taps into the same power that many meditation practices aim for - an uncluttered mind. When our minds are clear, they become fertile grounds for introspection and fresh ideas. In just twenty minutes, I generated more insights and processed more unfinished thoughts than if I had simply tried to write them down at a desk.</p>
<p>One aspect I appreciate is how simple this exercise is. In the past, I've tried other meditation techniques, only to feel lost as if I was somehow meditating incorrectly. But sitting in front of a wall? There's some unexpected beauty to it. The sheer emptiness seems to prompt the mind better than just closing my eyes and thinking. And the slight discomfort of staring at a wall for twenty minutes provides just enough sensation to anchor you to the present. It's the perfect nudge to keep you grounded in the moment.</p>
<h2 id="don-t-get-lost-in-the-sauce">Don't get Lost in the Sauce</h2>
<p>During my "walling" session, I reflected about the importance of <strong>Taking a Step Back</strong> - we should schedule times in the future to take a step back to make sure that we're not losing sight of the bigger picture or becoming too obsessed with minor details - essentially, not getting "lost in the sauce".</p>
<p>Lately, I've become interested in entrepreneurship and how startups work - reading books like <em>The Lean Startup, Zero to One and The Mom Test</em>. While they've offered invaluable insights, I find myself at a crossroads. Should I continue grokking knowledge through reading, or is it time to take the leap and start building a startup?</p>
<p>Take "The Lean Startup" for instance. It emphasises that startups operate under <strong>conditions of extreme uncertainty</strong>. The key to succeeding is to write down the riskiest assumptions and validate or invalidate them with MVPs (Minimal Viable Products). This insight alone has probably saved me a huge mistake in the future. Yet I'm left wondering: Would diving into more books be as beneficial as actually building and bringing my startup idea to life? Are there insights in the next chapter that could supercharge my performance? It's hard to say.</p>
<p>While staring at the wall, I realised that just as it's easy to get lost in thought, it's also easy to get lost in endless reading and preparation. Sometimes, we need to step back, reflect on what we've learned, and take action. Reading about startups is helpful, but at some point, we need to start building one.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>I think you should stare at a wall at least once. If that doesn't suit you, go on a walk and deliberately plan to be bored. Leave your phone at home. Don't bring your headphones. Simply immerse yourself in your surroundings, free from distractions. Lose track of time and appreciate the world around you.</p>
<p><em>You should spend more time being bored.</em></p>
<center><b>ch3ngl0rd out.</b></center>
<p><a href="https://news.ycombinator.com/item?id=38046396">Discuss</a> or <a href="https://www.linkedin.com/in/zachary-cheng-19a395212/">Get in Touch</a>.</p>



    

    
    



    



  </div></div>]]></description>
        </item>
    </channel>
</rss>