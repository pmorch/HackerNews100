<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 27 Jul 2024 03:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Courts Close the Loophole Letting the Feds Search Your Phone at the Border (113 pts)]]></title>
            <link>https://reason.com/2024/07/26/courts-close-the-loophole-letting-the-feds-search-your-phone-at-the-border/</link>
            <guid>41083286</guid>
            <pubDate>Fri, 26 Jul 2024 23:36:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reason.com/2024/07/26/courts-close-the-loophole-letting-the-feds-search-your-phone-at-the-border/">https://reason.com/2024/07/26/courts-close-the-loophole-letting-the-feds-search-your-phone-at-the-border/</a>, See on <a href="https://news.ycombinator.com/item?id=41083286">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							<p><span>The Fourth Amendment still applies at the border, despite the feds' insistence that it doesn't. </span></p> <p><span>For years, courts have ruled that the government has the right to conduct routine, warrantless searches for contraband at the border. Customs and Border Protection (CBP) has taken advantage of that loophole in the Fourth Amendment's protection against unreasonable searches and seizures to force travelers to hand over data from their phones and laptops.</span></p> <p><span>But on Wednesday, Judge Nina Morrison in the Eastern District of New York </span><a href="https://knightcolumbia.org/documents/ymerpfopdw"><span>ruled</span></a><span> that cellphone searches are a "nonroutine" search, more akin to a strip search than scanning a suitcase or passing a traveler through a metal detector.</span></p> <p>Although the interests of stopping contraband are "undoubtedly served when the government searches the luggage or pockets of a person crossing the border carrying objects that can only be introduced to this country by being physically moved across its borders, the extent to which those interests are served when the government searches data stored on a person's cell phone is far less clear," the judge declared.</p> <p><span>Morrison noted that "reviewing the information in a person's cell phone is the best approximation government officials have for mindreading," so searching through cellphone data has an even heavier privacy impact than rummaging through physical possessions. Therefore, the court ruled, a cellphone search at the border requires both probable cause </span><i><span>and</span></i><span> a warrant. Morrison did not distinguish between scanning a phone's contents with <a href="https://reason.com/2024/06/27/baltimore-brings-back-controversial-cellphone-hacking-system/">special software</a> and manually flipping through it.</span></p> <p><span>And in a victory for journalists, the judge specifically acknowledged the First Amendment implications of cellphone searches too. She cited reporting by </span><a href="https://theintercept.com/2019/02/08/us-mexico-border-journalists-harassment/"><i><span>The Intercept</span></i></a><span> and </span><em><a href="https://www.vice.com/en/article/78ke9q/wsj-reporter-homeland-security-tried-to-take-my-phones-at-the-border"><span>VICE</span></a></em><span> about CPB searching journalists' cellphones "based on these journalists' ongoing coverage of politically sensitive issues" and warned that those phone searches could put confidential sources at risk.</span></p> <p><span>Wednesday's ruling adds to a stream of cases restricting the feds' ability to search travelers' electronics. The 4th and 9th Circuits, which cover the mid-Atlantic and Western states, have ruled that border police need at least "</span><a href="https://crsreports.congress.gov/product/pdf/LSB/LSB10387#:~:text=Under%20the%20border%20search%20exception%2C%20the%20government%20may%20conduct%20routine,manual%20searches%20of%20electronic%20devices."><span>reasonable suspicion</span></a><span>" of a crime to search cellphones. Last year, a judge in the Southern District of New York </span><a href="https://s3.documentcloud.org/documents/23813619/us-v-smith.pdf"><span>also ruled</span></a><span> that the government "may not copy and search an American citizen's cell phone at the border without a warrant absent exigent circumstances."</span></p> <p><span>Wednesday's ruling involves defending the rights of an unsympathetic character. U.S. citizen Kurbonali Sultanov allegedly downloaded a sketchy Russian porn trove, including several images of child sex abuse, which landed him on a government watch list. When Sultanov was on the way back from visiting his family in Uzbekistan, agents from the Department of Homeland Security pulled him aside at the airport and searched his phone, finding the images.</span></p> <p><span>Morrison suppressed the evidence from the phone search but not Sultanov's "spontaneous" statement admitting to downloading the videos. And her order would not have prevented the police from getting Sultanov's phone the old-fashioned way. Sultanov had allegedly downloaded the porn while in the United States, and his name popped up on the watch list two months before his return flight. And, in fact, the feds did obtain a court order to search Sultanov's spare phone.</span></p> <p><span>The Southern District of New York ruling last year also involved an unsympathetic character. Jatiek Smith, a member of the Bloods gang, was being investigated for a "</span><a href="https://www.justice.gov/usao-sdny/pr/bloods-gang-member-convicted-trial-violent-and-extortionate-takeover-new-york-city"><span>violent and extortionate takeover</span></a><span>" of New York's fire mitigation industry. When Smith flew home from a vacation in Jamaica, the FBI took advantage of the opportunity to search Smith's phone at the border.</span></p> <p><span>A judge suppressed the evidence from the phone search, but Smith was </span><a href="https://www.justice.gov/usao-sdny/pr/bloods-gang-member-convicted-trial-violent-and-extortionate-takeover-new-york-city"><span>convicted anyway</span></a><span>. In both cases, the feds could have gotten a warrant for the suspects' phones; they saw the border loophole as a way to skip that step.</span></p> <p><span>In fact, CBP Officer Marves Pichardo admitted that these searches are often warrantless fishing expeditions. CBP searches U.S. citizen's phones if they're coming from "countries that have political difficulties at this point in time and that we're currently looking at for intelligence and stuff like that," Pichardo testified during an evidence suppression hearing. He asserted that CBP agents can "look at pretty much anything that's stored on the phone" and that passengers are usually "very compliant."</span></p> <p><span>Because of the powers the government was claiming, civil libertarians intervened in the Sultanov case. The Knight First Amendment Institute at Columbia University and the Reporters Committee for Freedom of the Press filed an </span><a href="https://knightcolumbia.org/documents/9uoisv1izu"><span><em>amicus</em> brief</span></a><span> in October 2023 arguing that warrantless phone searches are a "grave threat to the Fourth Amendment right to privacy as well as the First Amendment freedoms of the press, speech, and association." Morrison heavily cited that brief in her ruling.</span></p> <p><span>"As the court recognized, letting border agents freely rifle through journalists' work product and communications whenever they cross the border would pose an intolerable risk to press freedom," Grayson Clary, staff attorney at the Reporters Committee for Freedom of the Press, said in a statement sent to reporters. "This thorough opinion provides powerful guidance for other courts grappling with this issue, and makes clear that the Constitution would require a warrant before searching a reporter's electronic devices."</span></p>						</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Europe is in danger of regulating its tech market out of existence (200 pts)]]></title>
            <link>https://foreignpolicy.com/2024/07/26/europe-tech-regulation-apple-meta-google-competition/</link>
            <guid>41081238</guid>
            <pubDate>Fri, 26 Jul 2024 18:55:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://foreignpolicy.com/2024/07/26/europe-tech-regulation-apple-meta-google-competition/">https://foreignpolicy.com/2024/07/26/europe-tech-regulation-apple-meta-google-competition/</a>, See on <a href="https://news.ycombinator.com/item?id=41081238">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>In June, Apple announced a new product called <a href="https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/">Apple Intelligence</a>. It’s being sold as a new suite of features for the iPhone, iPad, and Mac that will use artificial intelligence to help you write and edit emails, create new pictures and emojis, and generally accomplish all kinds of tasks. There’s just one problem if you’re a European user eager to get your hands on it: Apple won’t be releasing it in Europe.</p><div>
						<p>In June, Apple announced a new product called <a href="https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/">Apple Intelligence</a>. It’s being sold as a new suite of features for the iPhone, iPad, and Mac that will use artificial intelligence to help you write and edit emails, create new pictures and emojis, and generally accomplish all kinds of tasks. There’s just one problem if you’re a European user eager to get your hands on it: Apple won’t be releasing it in Europe.</p>
<p>The company said in a statement that an entire suite of new products and features including Apple Intelligence, SharePlay screen sharing, and iPhone screen mirroring <a href="https://www.cnbc.com/2024/06/21/apple-ai-europe-dma-macos.html">would not be released</a> in European Union countries because of the regulatory requirements imposed by the EU’s Digital Markets Act (DMA). European Commission Executive Vice President Margrethe Vestager <a href="https://www.computerworld.com/article/2510200/eu-commissioner-slams-apple-intelligence-delay.html">called the decision</a> a “stunning declaration” of anti-competitive behavior.</p>
<p>Vestager’s statement is ridiculous on its face: A tech giant choosing not to release a product invites more competition, not less, and more importantly, this is exactly what you’d expect to happen given Europe’s regulatory stance.</p>
<p>The economist Albert Hirschman once <a href="https://www.hup.harvard.edu/books/9780674276604">described</a> the two options in an unfavorable environment as “voice” and “exit.” The most common option is voice—attempt to negotiate, repair the situation, and communicate toward better conditions. But the more drastic option is exit—choosing to leave the unfavorable environment entirely. That’s more common for people or political movements, but it’s growing increasingly relevant to technology in Europe.</p>
<p>Apple’s decision isn’t the first time that poorly designed regulations have pushed tech companies to block features or services in specific countries. Last year, Facebook <a href="https://www.voanews.com/a/facebook-news-ban-in-canada-leaves-small-outlets-struggling-/7489564.html">removed all news content</a> in Canada in response to the country’s Online News Act, which resulted in smaller news outlets losing business. In 2014, Google News <a href="https://www.theguardian.com/technology/2014/dec/11/google-news-spain-to-close-in-response-to-tax-on-story-links">withdrew from Spain</a> over a “link tax,” causing <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2837553">lower traffic</a> for Spanish news sites, returning only when the law was changed. Numerous technology firms have left China due to the power the Chinese Communist Party exerts over foreign corporations.</p>
<p>Adult sites are blocking users in a <a href="https://www.tallahassee.com/story/news/local/state/2024/03/18/pornhub-blocks-states-with-new-age-verification-laws-is-florida-next/73014384007/">variety of U.S. states</a> over age verification laws. Meta delayed the EU rollout of its Twitter (now X) competitor Threads over regulatory concerns, though it did eventually <a href="https://www.bbc.com/news/technology-67695643">launch there</a>. The firm, in a move that mirrors the Apple Intelligence decision, has also <a href="https://www.theverge.com/2024/7/18/24201041/meta-multimodal-llama-ai-model-launch-eu-regulations">declined to release its cutting-edge Llama AI models</a> in the EU, citing “regulatory uncertainty.” Technology companies have traditionally invested large amounts of money in voice strategies, lobbying officials and trying to improve poorly written laws. But they are increasingly aware of their ability to exit, especially in the European context. And Europe’s regulatory approach risks creating a balkanized “splinternet,” where international tech giants may choose to withdraw from the European continent.</p>
<p>If that seems far-fetched, consider other recent cases. Europe recently charged Meta with <a href="https://www.reuters.com/technology/meta-charged-with-failing-comply-with-eu-tech-rules-2024-07-01/">breaching EU regulations</a> over its “pay or consent” plan. Meta’s business is built around personalized ads, which are worth far more than non-personalized ads. EU regulators required that Meta provide an option that did not involve tracking user data, so Meta created a paid model that would allow users to pay a fee for an ad-free service.</p>
<p>This was already a significant concession—personalized ads are so valuable that one analyst estimated paid users would bring in <a href="https://mobiledevmemo.com/metas-next-tactic-in-the-eu-pay-or-okay/">60 percent less revenue</a>. But EU regulators are now insisting this model also breaches the rules, saying that Meta fails to provide a less personalized but equivalent version of Meta’s social networks. They’re demanding that Meta provide free full services without personalized ads or a monthly fee for users. In a very real sense, the EU has ruled that Meta’s core business model is illegal. Non-personalized ads cannot economically sustain Meta’s services, but it’s the only solution EU regulators want to accept.</p>
<p>Or consider the recent charges the EU levied against X. Under Elon Musk’s ownership, anyone can now purchase a blue check with a paid subscription, whereas blue checks were previously reserved for notable figures. EU regulators singled out the new system for blue checks as a <a href="https://www.rte.ie/news/business/2024/0712/1459534-musk-x-brussels/">deceptive business practice</a> that violates the bloc’s Digital Services Act.</p>
<p>These charges are absurd. For one, the change in the blue check system was widely advertised and dominated <a href="https://www.vox.com/recode/2022/11/4/23438917/twitter-verifications-blue-check-elon-musk">headlines</a> for months—as well as dominating discussion on the site itself. The idea that users have been deceived by one of the loudest and most discussed product changes in the site’s history is silly. And beyond that, the EU’s position is essentially “X cannot change the meaning of the blue check feature—it is permanently bound to the EU’s interpretation of what a blue check should mean.” This goes far beyond competition or privacy concerns; this is the EU straightforwardly making product decisions on behalf of a company.</p>
<p>A final example comes from France, where regulators are preparing to charge Nvidia with <a href="https://www.reuters.com/technology/french-antitrust-regulators-preparing-nvidia-charges-sources-say-2024-07-01/">anti-competitive practices</a> related to its <a href="https://blogs.nvidia.com/blog/what-is-cuda-2/">CUDA software</a>. CUDA is a free software system developed by Nvidia to run on its chips that allows other programs to more efficiently utilize GPUs in calculations. It’s one of the main reasons Nvidia has been so successful—the software makes its chips more powerful, and no competitor has developed comparable technology. It’s exactly the kind of innovative research that should be rewarded, but French regulators seem to view Nvidia’s decades-long investment in CUDA as a crime.</p>
<p>These examples all share a few key features. They’re all actions aimed at successful foreign tech companies—not surprising since the EU’s rules all but ensure there are <a href="https://foreignpolicy.com/2023/10/23/metaverse-europe-uk-us-big-tech-regulation-innovation/">no comparably successful European companies</a>. They’re all instances of regulatory overreach, where the EU is trying to dictate product decisions or rule entire business strategies illegal. And crucially, the sizes of the possible fines in play are so large that they may end up scaring companies off the continent.</p>
<p>EU policy allows for fines of up to <a href="https://competition-policy.ec.europa.eu/index/fines_en">10 percent of global revenue</a>. Analyst Ben Thompson <a href="https://stratechery.com/2024/the-e-u-goes-too-far/">reports</a> that Meta only gets 10 percent of its revenue from the EU and Apple only 7 percent. Nvidia does not provide exact regional numbers, but it’s <a href="https://en.macromicro.me/charts/81141/nvda-revenue-region-us-tw-cn">likely</a> that the EU provides less than 10 percent of its revenue as well. And this is revenue, not profit. A single fine of that magnitude would be more profit than these companies make in the EU in several years and destroy the economic rationale for operating there. With global-sized punishments for inane local issues, Europe is much closer than it realizes to simply driving tech companies away.</p>
<p>Europe’s regulators may insist that if companies simply followed the rules, they’d be able to make their profits without the threat of fines. This is patently untrue in the case of Meta, where the EU has ruled out every practical business strategy for funding its operations. But it’s also impossible writ large because the EU often doesn’t write clear rules in advance. Instead, the DMA requires businesses to meet abstract goals, and regulators decide afterward <a href="https://cepa.org/article/demystifying-europes-digital-markets-act/">whether the company is in compliance or not</a>. The burden does not exist on the EU to write concrete rules with specific requirements but on the companies to read the regulatory tea leaves and determine what steps to take. It’s an arbitrary and poorly designed system, and companies can hardly be blamed for looking to the exit.</p>
<p>Ultimately, Europe needs to figure out what it wants from the world’s technology industry. At times, it seems as if Europe has given up on trying to innovate or succeed in the tech sector. The continent takes more pride in being a leader in regulation than a leader in innovation, and its tech industry is a rounding error compared with that in the United States or China.</p>
<p>What few success stories it has, such as France’s Mistral, risk being strangled by regulatory actions. How would Mistral, a leading AI firm, survive if Nvidia exits the French market due to regulatory concerns? There is no substitute for Nvidia’s cutting-edge chips.</p>
<p>Europeans could end up living in an online backwater with out-of-date phones, cut off from the rest of the world’s search engines and social media sites, unable to even access high-performance computer chips.</p>
<p>As a sovereign body, the EU is within its rights to legislate tech as arbitrarily and harshly as it would like. But politicians such as Vestager don’t get to then act shocked and outraged when tech companies choose to leave. Right now, most tech companies are still attempting to work within the system and make Europe’s regulations more rational. But if voice fails over and over, exit is all that’s left. And in Europe, it’s an increasingly rational choice.</p>

											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zen 5's 2-ahead branch predictor: how a 30 year old idea allows for new tricks (149 pts)]]></title>
            <link>https://chipsandcheese.com/2024/07/26/zen-5s-2-ahead-branch-predictor-unit-how-30-year-old-idea-allows-for-new-tricks/</link>
            <guid>41081021</guid>
            <pubDate>Fri, 26 Jul 2024 18:32:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2024/07/26/zen-5s-2-ahead-branch-predictor-unit-how-30-year-old-idea-allows-for-new-tricks/">https://chipsandcheese.com/2024/07/26/zen-5s-2-ahead-branch-predictor-unit-how-30-year-old-idea-allows-for-new-tricks/</a>, See on <a href="https://news.ycombinator.com/item?id=41081021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a href="https://chipsandcheese.com/2024/07/15/a-video-interview-with-mike-clark-chief-architect-of-zen-at-amd/">When I recently interviewed Mike Clark</a>, he told me, “…you’ll see the actual foundational lift play out in the future on Zen 6, even though it was really Zen 5 that set the table for that.” And at that same Zen 5 architecture event,&nbsp; AMD’s Chief Technology Officer Mark Papermaster said, “Zen 5 is a ground-up redesign of the Zen architecture,” which has brought numerous and impactful changes to the design of the core.</p>
<p>The most substantial of these changes may well be the brand-new 2-Ahead Branch Predictor Unit, an architectural enhancement with roots in papers from three decades ago. But before diving into this both old yet new idea, let’s briefly revisit what branch predictors do and why they’re so critical in modern microprocessor cores.</p>
<p>Ever since computers began operating on programs stored in programmable, randomly accessible memory, architectures have been split into a front end that fetches instructions and a back end responsible for performing those operations. A front end must also support arbitrarily moving the point of current program execution to allow basic functionality like conditional evaluation, looping, and subroutines.</p>
<p>If a processor could simply perform the entire task of fetching an instruction, executing it, and selecting the next instruction location in unison, there would be little else to discuss here. However, incessant demands for performance have dictated that processors perform more operations in the same unit time with the same amount of circuitry, taking us from 5 kHz with ENIAC to the 5+ GHz of some contemporary CPUs like Zen 5, and this has necessitated pipelined logic. A processor must actually maintain in parallel the incrementally completed partial states of logically chronologically distinct operations.</p>
<p>Keeping this pipeline filled is immediately challenged by the existence of conditional jumping within a program. How can the front end know what instructions to begin fetching, decoding, and dispatching when a jump’s condition might be a substantial number of clock cycles away from finishing evaluation? Even unconditional jumps with a statically known target address present a problem when fetching and decoding an instruction needs more than a single pipeline stage.</p>
<p>The two ultimate responses of this problem are to either simply wait when the need is detected or to make a best effort guess at what to do next and be able to unwind discovered mistakes. Unwinding bad guesses must be done by flushing the pipeline of work contingent on the bad guess and restarting at the last known good point. A stall taken on a branch condition is effectively unmitigable and proportional in size to the number of stages between the instruction fetch and the branch condition evaluation completion in the pipeline. Given this and the competitive pressures to not waste throughput, processors have little choice but to attempt guessing program instruction sequences as accurately as possible.</p>
<p>Imagine for a moment that you are a delivery driver without a map or GPS who must listen to on-the-fly navigation from colleagues in the back of the truck. Now further imagine that your windows are completely blacked out and that your buddies only tell you when you were supposed to turn 45 seconds past the intersection you couldn’t even see. You can start to empathize and begin to understand the struggles of the instruction fetcher in a pipelined processor. The art of branch prediction is the universe of strategies that are available to reduce the rate that this woefully afflicted driver has to stop and back up.</p>
<p>Naive strategies like always taking short backwards jumps (turning on to a circular drive) can and historically did provide substantial benefit over always fetching the next largest instruction memory address (just keep driving straight). However, if some small amount of state is allowed to be maintained, much better results in real programs can be achieved. If the blinded truck analogy hasn’t worn too thin yet, imagine the driver keeping a small set of notes of recent turns taken or skipped and hand-drawn scribbles of how roads driven in the last few minutes were arranged and what intersections were passed. These are equivalent to things like branch history and address records, and structures in the 10s of kilobytes have yielded branch prediction percentages in the upper 90s. This article will not attempt to cover the enormous space of research and commercial solutions here, but understanding at least the beginnings of the motivations here is valuable.</p>
<figure><img decoding="async" width="688" height="330" data-attachment-id="30385" data-permalink="https://chipsandcheese.com/2024/07/26/zen-5s-2-ahead-branch-predictor-unit-how-30-year-old-idea-allows-for-new-tricks/image-89/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-5-1.jpg?fit=787%2C378&amp;ssl=1" data-orig-size="787,378" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-5-1.jpg?fit=787%2C378&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-5-1.jpg?fit=688%2C330&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-5-1.jpg?resize=688%2C330&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-5-1.jpg?w=787&amp;ssl=1 787w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-5-1.jpg?resize=768%2C369&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>Annotated floorplan from AMD’s 2023 ISSCC presentation: <em>“Zen 4” – The AMD 5nm 5.7 GHz x86-64 Microprocessor Core</em>. The front end takes half of all total L1 cache and integer logic area, and the branch predictor logic and state received the largest area budget of any front end subsystem.</figcaption></figure>
<p>Enter stage right, Zen 5’s 2-Ahead Branch Predictor.</p>
<div>
<figure><img decoding="async" width="688" height="390" data-attachment-id="30355" data-permalink="https://chipsandcheese.com/2024/07/26/zen-5s-2-ahead-branch-predictor-unit-how-30-year-old-idea-allows-for-new-tricks/zen5-front-end-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/Zen5-front-end.drawio.png?fit=1061%2C601&amp;ssl=1" data-orig-size="1061,601" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Zen5-front-end.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/Zen5-front-end.drawio.png?fit=1061%2C601&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/Zen5-front-end.drawio.png?fit=688%2C390&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/Zen5-front-end.drawio.png?resize=688%2C390&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/Zen5-front-end.drawio.png?w=1061&amp;ssl=1 1061w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/Zen5-front-end.drawio.png?resize=768%2C435&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>Diagram of Zen 5’s Front End</figcaption></figure></div>
<p>The 2-Ahead Branch Predictor is a proposal that dates back to the early ’90s. Even back then the challenge of scaling out architectural widths of 8 or more was being talked about and a 2-Ahead Branch Predictor was one of the methods that academia put forth in order to continue squeezing more and more performance out of a single core.&nbsp;</p>
<p>But as commercial vendors moved from a single core CPU to multi-core CPUs, the size of each individual core started to become a bigger and bigger factor in CPU core design so academia started focusing on more area efficient methods to increase performance with the biggest development being the TAGE predictor. The TAGE predictor is much more area efficient compared to older branch predicting methods so again academia focused on improving TAGE predictors.</p>
<p>But with logic nodes allowing for more and more transistors in a similar area along with moving from dual and quad core CPUs to CPUs with hundreds of out of order CPUs, we have started to focus more and more on single core performance rather than just scaling further and further up. So while some of these ideas are quite old, older than I in fact, they are starting to resurface as companies try and figure out ways to increase the performance of a single core.</p>
<p>It is worth addressing an aspect of x86 that allows it to benefit disproportionately more from 2-ahead branch prediction than some other ISAs might. Architectures with fixed-length instructions, like 64-bit Arm, can trivially decode arbitrary subsets of an instruction cache line in parallel by simply replicating decoder logic and slicing up the input data along guaranteed instruction byte boundaries. On the far opposite end of the spectrum sits x86, which requires parsing instruction bytes linearly to determine where each subsequent instruction boundary lies. Pipelining (usually partially decoding length-determining prefixes first) makes a parallelization of some degree tractable, if not cheap, which resulted in 4-wide decoding being commonplace in performance-oriented x86 cores for numerous years.</p>
<p>While increasing logic density with newer fab nodes has eventually made solutions like Golden Cove’s 6-wide decoding commercially viable, the area and power costs of monolithic parallel x86 decoding are most definitely super-linear with width, and there is not anything resembling an easy path forward with continued expansions here. It is perhaps merciful for Intel and AMD that typical application integer code has a substantial branch density, on the order of one every five to six instructions, which diminishes the motivation to pursue parallelized decoders much wider than that.</p>
<p>The escape valve that x86 front ends need more than anything is for the inherently non-parallelizable portion of decoding, i.e., the determination of the instruction boundaries. If only there was some way to easily skip ahead in the decoding and be magically guaranteed you landed on an even instruction boundary…</p>
<h3>Back to the Future!…. of the 1990s</h3>
<p>Starting with the paper titled <a href="https://dl.acm.org/doi/10.1145/237090.237169">“Multiple-block ahead branch predictors” by Seznec et al.</a>, it lays out the why and how of the reasoning and implementation needed to make a 2-Ahead Branch Predictor.</p>
<div>
<figure><img decoding="async" width="688" height="422" data-attachment-id="30356" data-permalink="https://chipsandcheese.com/2024/07/26/zen-5s-2-ahead-branch-predictor-unit-how-30-year-old-idea-allows-for-new-tricks/image-85/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-3.png?fit=997%2C611&amp;ssl=1" data-orig-size="997,611" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-3.png?fit=997%2C611&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-3.png?fit=688%2C422&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-3.png?resize=688%2C422&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-3.png?w=997&amp;ssl=1 997w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-3.png?resize=768%2C471&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>Figure 4a from Seznac et al. showing how a 2-Ahead Branch Predictor with a double instruction fetch pipeline would be able to predict 2 basic blocks</figcaption></figure></div>
<p>Looking into the paper, you’ll see that implementing a branch predictor that can deal with multiple taken branches per cycle is not as simple as just having a branch predictor that can deal with multiple taken branches. To be able to use a 2-Ahead Branch Predictor to its fullest, without exploding area requirements, Seznac et al. recommended dual-porting the instruction fetch.</p>
<p>When we look at Zen 5, we see that dual porting the instruction fetch and the op cache is exactly what AMD has done. AMD now has two 32 Byte per cycle fetch pipes from the 32KB L1 instruction cache, each feeding its own 4-wide decode cluster. The Op Cache is now a dual-ported 6 wide design which can feed up to 12 operands to the Op Queue.</p>
<p>Now, Seznac et al. also recommends dual porting the Branch Target Buffer (BTB). A dual-ported L1 BTB could explain the massive 16K entries that the L1 BTB has access to. As for the L2 BTB, it’s not quite as big as the L1 BTB at only 8K entries but AMD is using it in a manner similar to how a victim cache would be used. So entries that get evicted out of the L1 BTB, end up in the L2 BTB.</p>
<p>With all these changes, Zen 5 can now deal with 2 taken branches per cycle across a non-contiguous block of instructions.&nbsp;</p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="143" data-attachment-id="30372" data-permalink="https://chipsandcheese.com/2024/07/26/zen-5s-2-ahead-branch-predictor-unit-how-30-year-old-idea-allows-for-new-tricks/image-87/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.png?fit=885%2C184&amp;ssl=1" data-orig-size="885,184" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.png?fit=885%2C184&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.png?fit=688%2C143&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.png?resize=688%2C143&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.png?w=885&amp;ssl=1 885w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.png?resize=768%2C160&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>Figure 3 from Seznac et al. showing the branch information for a 2-Ahead predictor</figcaption></figure></div>
<p>This should reduce the hit to fetch bandwidth when Zen 5 hits a taken branch as well as allowing AMD to predict past the 2 taken branches.</p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="387" data-attachment-id="30371" data-permalink="https://chipsandcheese.com/2024/07/26/zen-5s-2-ahead-branch-predictor-unit-how-30-year-old-idea-allows-for-new-tricks/image-86/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?fit=2096%2C1179&amp;ssl=1" data-orig-size="2096,1179" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?fit=2096%2C1179&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?fit=688%2C387&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?w=2096&amp;ssl=1 2096w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?resize=1536%2C864&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?resize=2048%2C1152&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?resize=1200%2C675&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?resize=1600%2C900&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?resize=1320%2C743&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/image-4.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>Slide 6 from AMD’s Deep Dive brief into the Zen 5 architecture</figcaption></figure></div>
<p>Zen 5 can look farther forward in the instruction stream beyond the 2nd taken branch and as a result Zen 5 can have 3 prediction windows where all 3 windows are useful in producing instructions for decoding. The way that this works is that a 5 bit length field is attached to the 2nd prediction window which prevents the over subscription of the decode or op cache resources. This 5 bit length field while smaller than a pointer does give you the start of the 3rd prediction window. One benefit of this is that if the 3rd window crosses a cache line boundary, the prediction lookup index doesn’t need to store extra state for the next cycle. However a drawback is that if the 3rd prediction window is in the same cache line as the 1st or 2nd prediction window, that partial 3rd window isn’t as effective as having a 3rd full prediction window.</p>
<p>Now when Zen 5 has two threads active, the decode clusters and the accompanying fetch pipes are statically partitioned. This means that to act like a dual fetch core, Zen 5 will have to fetch out of both the L1 instruction cache as well as out of the Op Cache. This maybe the reason why AMD dual-ported the op cache so that they can better insure that they can keep the dual fetch pipeline going.</p>
<h3>Final Words</h3>
<p>In the end, this new 2-Ahead Branch Predictor is a major shift for the Zen family of CPU architectures moving forward and is going to give new branch prediction capabilities that will likely serve the future developments of the Zen core in good stead as they refine and improve this branch predictor.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>
<h3>Endnotes</h3>
<p>If you want to learn more about how multiple fetch processors work then I would highly recommend the papers below as they helped with my understanding of how this whole system works:</p>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/237090.237169">“Multiple-block ahead branch predictors” by Seznec et al. – ASPLOS 1996</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/223982.224444">“Optimization of Instruction Fetch Mechanisms for High Issue Rates” by Conte et al. – ISCA 1995</a></li>
<li><a href="https://dl.acm.org/doi/10.1145/165939.165956">“Increasing the instruction fetch rate via multiple branch prediction and a branch address cache” by Yeh et al. – ICS 1993</a></li>
<li><a href="https://pages.cs.wisc.edu/~param/papers/oberoip_sequencers.pdf">“Out-of-Order Instruction Fetch using Multiple Sequencers” by Oberoi and Sohi – ICPP’02</a></li>
<li><a href="https://pages.cs.wisc.edu/~param/papers/isca03.pdf">“Parallelism in the Front-End” by Oberoi and Sohi – ISCA 2003</a></li>
</ul>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">
<p><span>
<ul>
<li>
<div>
<p><img alt="Cheese" src="https://secure.gravatar.com/avatar/eb262496276a5c8c0a375be578f81db9?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/eb262496276a5c8c0a375be578f81db9?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>
</div>

</li>
<li>
<div>
<p><img alt="Camacho" src="https://secure.gravatar.com/avatar/?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>
</div>

</li>
</ul>
</span>
</p></div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The New Internet (125 pts)]]></title>
            <link>https://tailscale.com/blog/new-internet</link>
            <guid>41080991</guid>
            <pubDate>Fri, 26 Jul 2024 18:29:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tailscale.com/blog/new-internet">https://tailscale.com/blog/new-internet</a>, See on <a href="https://news.ycombinator.com/item?id=41080991">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><em>Avery Pennarun is the CEO and co-founder of Tailscale. A version of this post was originally presented at a company all-hands.</em></p><p>We don’t talk a lot in public about the big vision for Tailscale, why we’re really here. Usually I prefer to focus on what exists right now, and what we’re going to do in the next few months. The future can be distracting.</p><p>But increasingly, I’ve found companies are starting to buy Tailscale not just for what it does now, but for the big things they expect it’ll do in the future. They’re right! Let’s look at the biggest of big pictures for a change.</p><p>But first, let’s go back to where we started.</p><h4 id="old"><span>Old</span></h4><p>David Crawshaw’s first post that laid out what we were doing, long long ago in the late twenty-teens, was called<a target="" rel="noreferrer" href="https://tailscale.com/blog/remembering-the-lan/"> Remembering the LAN</a>, about his experience doing networking back in the 1990s.</p><p>I have bad news: if you remember doing LANs back in the 1990s, you are probably old. Quite a few of us here at Tailscale remember doing LANs in the 1990s. That’s an age gap compared to a lot of other startups. That age gap makes Tailscale unusual.</p><p>Anything unusual about a startup can be an advantage or a disadvantage, depending what you do with it.</p><h4 id="mature"><span>Mature</span></h4><p>Here’s another word for “old” but with different connotations.</p><p>I’m a person that likes looking on the bright side. There are disadvantages to being old, like I maybe can’t do a 40-hour coding binge like I used to when I wrote my first VPN, called Tunnel Vision, in 1997. But there are advantages, like maybe we have enough experience to do things right the first time, in fewer hours. Sometimes. If we’re lucky.</p><p>And maybe, you know, if you’re old enough, you’ve seen the tech cycle go round a few times and you’re starting to see a few patterns.</p><p>That was us, me and the Davids, when we started Tailscale. What we saw was, a lot of things have gotten better since the 1990s. Computers are literally millions of times faster. 100x as many people can be programmers now because they aren’t stuck with just C++ and assembly language, and many, many, many more people now have some kind of computer. Plus app stores, payment systems, graphics. All good stuff.</p><p>But, also things have gotten worse. A lot of day-to-day things that used to be easy for developers, are now hard. That was unexpected. I didn’t expect that. I expected I’d be out of a job by now because programming would be so easy.</p><p>Instead, the tech industry has evolved into an absolute mess. And it’s getting worse instead of better! Our tower of complexity is now so tall that we seriously consider slathering LLMs on top to write the incomprehensible code in the incomprehensible frameworks so we don’t have to.</p><p>And you know, we old people are the ones who have the context to see that.</p><p>It’s all fixable. It doesn’t have to be this way.</p><p>Before I can tell you a vision for the future I have to tell you what I think went wrong.</p><h4 id="not-scaling"><span>Not scaling</span></h4><p>Programmers today are impatient for success. They start planning for a billion users before they write their first line of code. In fact, nowadays, we train them to do this without even knowing they’re doing it. Everything they’ve ever been taught revolves around scaling.</p><p>We’ve been falling into this trap all the way back to when computer scientists started teaching big-O notation. In big-O notation, if you use it wrong, a hash table is supposedly faster than an array, for virtually anything you want to do. But in reality, that’s not always true. When you have a billion entries, maybe a hash table is faster. But when you have 10 entries, it almost never is.</p><p>People have a hard time with this idea. They keep picking the algorithms and architectures that can scale up, even when if you don’t scale up, a different thing would be thousands of times faster, and also easier to build and run.</p><p>Even I can barely believe I just said thousands of times easier and I wasn’t exaggerating.</p><p>I read a post recently where someone bragged about using kubernetes to scale all the way up to 500,000 page views per month. But that’s 0.2 requests per second. I could serve that from my phone, on battery power, and it would spend most of its time asleep.</p><p>In modern computing, we tolerate long builds, and then docker builds, and uploading to container stores, and multi-minute deploy times before the program runs, and even longer times before the log output gets uploaded to somewhere you can see it, all because we’ve been tricked into this idea that everything has to scale. People get excited about deploying to the latest upstart container hosting service because it only takes tens of seconds to roll out, instead of minutes. But on my slow computer in the 1990s, I could run a perl or python program that started in milliseconds and served way more than 0.2 requests per second, and printed logs to stderr right away so I could edit-run-debug over and over again, multiple times per minute.</p><p>How did we get here?</p><p>We got here because sometimes, someone really does need to write a program that has to scale to thousands or millions of backends, so it needs all that… stuff. And wishful thinking makes people imagine even the lowliest dashboard could be that popular one day.</p><p>The truth is, most things don’t scale, and never need to. We made Tailscale for those things, so you can spend your time scaling the things that really need it. The long tail of jobs that are 90% of what every developer spends their time on. Even developers at companies that make stuff that scales to billions of users, spend most of their time on stuff that doesn’t, like dashboards and meme generators.</p><p>As an industry, we’ve spent all our time making the hard things possible, and none of our time <a target="" rel="noreferrer" href="https://www.amazon.com/Learning-Perl-Making-Things-Possible/dp/1491954329">making the easy things easy</a>.</p><p>Programmers are all stuck in the mud. Just listen to any professional developer, and ask what percentage of their time is spent actually solving the problem they set out to work on, and how much is spent on junky overhead.</p><p>It’s true here too. Our developer experience at Tailscale is better than average. But even we have largely the same experience. Modern software development is mostly junky overhead.</p><h4 id="the-internet"><span>The Internet</span></h4><p>In fact, we didn’t found Tailscale to be a networking company. Networking didn’t come into it much at all at first.</p><p>What really happened was, me and the Davids got together and we said, look. The problem is developers keep scaling things they don’t need to scale, and their lives suck as a result. (For most programmers you can imagine the “wiping your tears with a handful of dollar bills” meme here.) We need to fix that. But how?</p><p>We looked at a lot of options, and talked to a lot of people, and there was an underlying cause for all the problems. The Internet. Things used to be simple. Remember the LAN? But then we connected our LANs to the Internet, and there’s been more and more firewalls and attackers everywhere, and things have slowly been degrading ever since.</p><p>When we explore the world of over-complexity, most of it has what we might call, no essential complexity. That is, the problems can be solved without complexity, but for some reason the solutions we use are complicated anyway. For example, logging systems. They just stream text from one place to another, but somehow it takes 5 minutes to show up. Or orchestration systems: they’re programs whose only job is to run other programs, which Unix kernels have done just fine, within milliseconds, for decades. People layer on piles of goop. But the goop can be removed.</p><p>Except networking.</p><p>You can’t build modern software without networking. But the Internet makes everything hard. Is it because networking has essential complexity?</p><p>Well, maybe. But maybe it’s only complex when you built it on top of the wrong assumptions, that result in the wrong problems, that you then have to paper over. That’s the Old Internet.</p><p>Instead of adding more layers at the very top of the OSI stack to try to hide the problems, Tailscale is building a new OSI layer 3 — a New Internet — on top of new assumptions that avoid the problems in the first place.</p><h4 id="dominoes"><span>Dominoes</span></h4><p>If we fix the Internet, a whole chain of dominoes can come falling down, and we reach the next stage of technology evolution.</p><p>If you want to know the bottleneck in any particular economic system, look for who gets to charge rent. In the tech world, that’s AWS. Sure, Apple’s there selling popular laptops, but you could buy a different laptop or a different phone. And Microsoft was the gatekeeper for everything, once, but you don’t have Windows lock-in anymore, unless you choose to. All those “the web is the new operating system” people of the early 2000s finally won, we just forgot to celebrate.</p><p>But the liberation didn’t last long. If you deploy software, you probably pay rent to AWS.</p><p>Why is that? Compute, right? AWS provides scalable computing resources.</p><p>Well, you’d think so. But lots of people sell computing resources way cheaper. Even a mid-range Macbook can do 10x or 100x more transactions per second on its SSD than a supposedly fast cloud local disk, because cloud providers sell that disk to 10 or 100 people at once while charging you full price. Why would you pay exorbitant fees instead of hosting your mission-critical website on your super fast Macbook?</p><p>We all know why:</p><h4 id="ipv4"><span>IPv4</span></h4><p>Location, location, location. You pay exorbitant rents to cloud providers for their computing power because your own computer isn’t in the right place to be a decent server.</p><p>It’s behind a firewall and a NAT and a dynamic IP address and probably an asymmetric network link that drops out just often enough to make you nervous.</p><p>You could fix the network link. You could reconfigure the firewall, and port forward through the NAT, I guess, and if you’re lucky you could pay your ISP an exorbitant rate for a static IP, and maybe get a redundant Internet link, and I know some of my coworkers actually did do all that stuff on a rack in their garage. But it’s all a lot of work, and requires expertise, and it’s far away from building the stupid dashboard or blog or cat video website you wanted to build in the first place. It’s so much easier to just pay a hosting provider who has all the IP addresses and network bandwidth money can buy.</p><p>And then, if you’re going to pay someone, and you’re a serious company, you’d better buy it from someone serious, because now you have to host your stuff on their equipment which means they have access to… everything, so you need to trust them not to misuse that access.</p><p>You know what, nobody ever got fired for buying AWS.</p><p>That’s an IBM analogy. We used to say, nobody ever got fired for buying IBM. I doubt that’s true anymore. Why not?</p><h4 id="pendulums"><span>Pendulums</span></h4><p>I refuse to say pendula.</p><p>IBM mainframes still exist, and they probably always will, but IBM used to be able to charge rent on every aspect of business computing, and now they can’t. They started losing influence when Microsoft arrived, stealing fire from the gods of centralized computing and bringing it back to individuals using comparatively tiny underpowered PCs on every desk, in every home, running Microsoft software.</p><p>I credit Microsoft with building the first widespread distributed computing systems, even though all the early networks were some variant of sneakernet.</p><p>I think we can agree that we’re now in a post-Microsoft, web-first world. Neat. Is this world a centralized one like IBM, or a distributed one like Microsoft?</p><p>[When I did this as a talk, I took a poll: it was about 50/50]</p><p>So, bad news. The pendulum has swung back the other way. IBM was centralized, then Microsoft was distributed, and now the cloud+phone world is centralized again.</p><p>We’ve built a giant centralized computer system, with a few megaproviders in the middle, and a bunch of dumb terminals on our desks and in our pockets. The dumb terminals, even our smart watches, are all supercomputers by the standards of 20 years ago, if we used them that way. But they’re not much better than a VT100. Turn off AWS, and they’re all bricks.</p><p>It’s easy to fool ourselves into thinking the overall system is distributed. Yes, we build fancy distributed consensus systems and our servers have multiple instances. But all that runs centrally on cloud providers.</p><p>This isn’t new. IBM was doing multi-core computing and virtual machines back in the 1960s. It’s the same thing over again now, just with 50 years of Moore’s Law on top. We still have a big monopoly that gets to charge everyone rent because they’re the gatekeeper over the only thing that really matters.</p><h4 id="operating-systems"><span>Operating Systems</span></h4><p>Sorry, just kidding.</p><p>Connectivity.</p><p>Everyone’s attitude is still stuck in the 1990s, when operating systems mattered. That’s how Microsoft stole the fire from IBM and ruled the world, because writing portable software was so hard that if you wanted to… interconnect… one program to another, if you wanted things to be compatible at all, you had to run them on the same computer, which meant you had to standardize the operating system, and that operating system was DOS, and then Windows.</p><p>The web undid that monopoly. Now javascript matters more than all the operating systems put together, and there’s a new element that controls whether two programs can talk to each other: HTTPS. If you can HTTPS from one thing to another, you can interconnect. If you can’t, forget it.</p><h4 id="certificates"><span>Certificates</span></h4><p>And HTTPS is fundamentally a centralized system. It has a client, and a server. A dumb terminal, and a thing that does the work. The server has a static IP address, a DNS name, a TLS certificate, and an open port. A client has none of those things. A server can keep doing whatever it wants if all the clients go away, but if the servers go away, a client does nothing.</p><p>We didn’t get here on purpose, mostly. It was just path dependence. We had security problems and an IPv4 address shortage, so we added firewalls and NATs, so connections became one way from client machines to server machines, and so there was no point putting certificates on clients, and nowadays there are 10 different reasons a client can’t be a server, and everyone is used to it, so we design everything around it. Dumb terminals and centralized servers.</p><p>Once that happened, of course some company popped up to own the center of the hub-and-spoke network. AWS does that center better than everyone else, fair and square. Someone had to. They won.</p><h4 id="taildrop"><span>Taildrop</span></h4><p>Okay, fast forward. We’ve spent the last 5 years making Tailscale the solution to that problem. Every device gets a cert. Every device gets an IP address and a DNS name and end-to-end encryption and an identity, and safely bypasses firewalls. Every device can be a peer. And we do it all without adding any latency or overhead.</p><p>That’s the New Internet. We built it! It’s the future, it’s just unevenly distributed, so far. For people with Tailscale, we’ve already sliced out 10 layers of nonsense. That’s why developers react so viscerally once they get it. Tailscale makes the Internet work how you thought the Internet worked, before you learned how the Internet works.</p><p>I like to use <a target="" rel="noreferrer" href="https://tailscale.com/blog/2021-06-taildrop-was-easy">Taildrop as an example of what that makes possible</a>. Taildrop is a little feature we spent a few months on back when we were tiny. We should spend more time polishing to make it even easier to use. But at its core, it’s a demo app. As long as you have Tailscale already, Taildrop is just one HTTP PUT operation. The sender makes an HTTP request to the receiver, says “here’s a file named X”, and sends the file. That’s it. It’s the most obvious thing in the world. Why would you do it any other way?</p><p>Well, before Tailscale, you didn’t have a choice. The receiver is another client device, not a server. So it was behind a firewall, with no open ports and no identity. Your only option was to upload the file to the cloud and then download it again, even if the sender and receiver are side by side on the same wifi. But that means you pay cloud fees for network egress, and storage, and the CPU time for running whatever server program is managing all that stuff. And if you upload the file and nobody downloads it, you need a rule for when to delete it from storage. And also you pay fees just in case to keep the server online, even when you’re not using it at all. Also, cloud employees can theoretically access the file unless you encrypt it. But you can’t encrypt it without exchanging encryption keys somehow between sender and recipient. And how does the receiver even know a file is there waiting to be received in the first place? Do we need a push notification system? For every client platform? And so on. Layers, and layers, and layers of gunk.</p><p>And all that gunk means rent to cloud providers. Transferring files — one of the first things people did on the Internet, for no extra charge, via FTP — now has to cost money, because somebody has got to pay that rent.</p><p>With Taildrop, it doesn’t cost money. Not because we’re generously draining our bank accounts to make file transfers free. It’s because the <a target="" rel="noreferrer" href="https://tailscale.com/blog/free-plan">cost overhead is gone altogether</a>, because it’s not built on the same devolved Internet everyone else has been using.</p><h4 id="the-new-internet"><span>The New Internet</span></h4><p>Taildrop is just an example, a trivial one, but it’s an existence proof for a whole class of programs that can be 10x easier just because Tailscale exists.</p><p>The chain of dominoes starts with connectivity. Lack of connectivity is why we get centralization, and centralization is why we pay rent for every tiny little program we want to run and why everything is slow and tedious and complicated and hard to debug like an IBM batch job. And we’re about to start those dominoes falling.</p><p>The glimpse at these possibilities is why our users get excited about Tailscale, more than they’ve ever been excited about some VPN or proxy, because there’s something underneath our kind of VPN that you can’t get anywhere else. We’re removing layers, and layers, and layers of complexity, and making it easier to work on what you wanted to work on in the first place. Not everybody sees it yet, but they will. And when they do, they’re going to be able to invent things we could never imagine in the old centralized world, just like the Windows era of distributed computing made things possible that were unthinkable on a mainframe.</p><p>But there’s one catch. If we’re going to untangle the hairball of connectivity, that connectivity has to apply to…</p><h4 id="everyone"><span>Everyone</span></h4><p>There’s going to be a new world of haves and have-nots. Where in 1970 you had or didn’t have a mainframe, and in 1995 you had or didn’t have the Internet, and today you have or don’t have a TLS cert, tomorrow you’ll have or not have Tailscale. And if you don’t, you won’t be able to run apps that only work in a post-Tailscale world.</p><p>And if not enough people have Tailscale, nobody will build those apps. That’s called a <a target="" rel="noreferrer" href="https://apenwarr.ca/log/20201227">chicken-and-egg problem</a>.</p><p>This is why our company strategy sounds so odd at first glance. It's why we spend so much effort giving Tailscale away for free, but also so much effort getting people to bring it to work, and so much effort doing tangential enterprise features so executives can easily roll it out to whole Fortune 500 companies.</p><p>The Internet is for everyone. You know, there were internetworks (lowercase) before the Internet (capitalized). They all lost, because the Internet was the most diverse and inclusive of all. To the people building the Internet, nothing mattered but getting everyone connected. Adoption was slow at first, then fast, then really fast, and today, if I buy a wristwatch and it doesn’t have an Internet link, it’s broken.</p><p>We won’t have built a New Internet if nerds at home can’t play with it. Or nerds at universities. Or employees at enterprises. Or, you know, eventually every person everywhere.</p><h4 id="vision"><span>Vision</span></h4><p>There remain a lot of steps between here and there. But, let’s save those details for another time. Meanwhile, how are we doing?</p><p>Well, about 1 in 20,000 people in the world uses the New Internet (that’s Tailscale). We’re not going to stop until it’s all of them.</p><p>I’m old enough to remember when people made fun of Microsoft for their thing about putting a computer on every desk. Or when TCP/IP was an optional add-on you had to buy from a third party.</p><p>You know, all that was less than 30 years ago. I’m old, but come to think of it, I’m not that old. The tech world changes fast. It can change for the better. We’re just getting started.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why does the chromaticity diagram look like that? (157 pts)]]></title>
            <link>https://jlongster.com/why-chromaticity-shape</link>
            <guid>41080904</guid>
            <pubDate>Fri, 26 Jul 2024 18:19:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jlongster.com/why-chromaticity-shape">https://jlongster.com/why-chromaticity-shape</a>, See on <a href="https://news.ycombinator.com/item?id=41080904">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><a id="block-30b920"></a><p>I've always wanted to understand color theory, so I started reading about the XYZ color space which looked like it was the mother of all color spaces. I had no idea what that meant, but it was created in 1931 so studying 93-year old research seemed like a good place to start.</p><p>When reading about the XYZ color space, this cursed image keeps popping up:</p><figure>
<img src="https://static.jlongster.com/20240103/CIE1931xy_blank.svg">
<figcaption>
By BenRG - Own work based on: CIExy1931.svg, Public Domain, https://commons.wikimedia.org/w/index.php?curid=7889658
</figcaption>
</figure><p>I say "cursed" because I have no idea what that means. <strong>What the heck is that shape??</strong></p><p>I couldn't find any reasonably clear answer to my question. It's obviously not a formula like <code>x = func(y)</code>. Why is it that shape, and where did the colors come from? Obviously the edges are wavelengths which have a specific color, but how did the image above compute every pixel?</p><p>I became obsessed with this question. Below is the path I took to try to answer it.</p><p>I'll spoil the answer but it might not make sense until you read this article: the shape comes from how our eyes perceive red, green, and blue relative to each other. Skip to the <a href="#more-shape-explorations">last section</a> if you want to see some direct examples.</p><p>The fill colors inside the shape are another story, but a simple explanation is there is some math to calculate the mixture of colors and we can draw the above by sampling millions of points in the space and rendering them onto the 2d image.</p><p>Let's dig in more.</p><a id="block-f0af28"></a><a id="block-82e29b"></a><h2 id="color-matching-functions"><a href="#color-matching-functions">Color matching functions</a></h2><p>The first place to start is color matching functions. These functions determine the strength of specific wavelengths (color) to contribute so that our eyes perceive a target wavelength (color). We have 3 color matching functions for red, green, and blue (at wavelengths 700, 546, and 435 respectively), and these functions specify how to mix RGB to so that we visually see a <a href="https://en.wikipedia.org/wiki/Spectral_color">spectral color</a>.</p><p>More simply put: imagine that you have red, green, and blue light sources. What is the intensity of each one so that the resulting light matches a specific color on the spectrum?</p><p>Note that these are <em>spectral</em> colors: monochromatic light with a single wavelength. Think of colors on the rainbow. Many colors are not spectral, and are a mix of many spectral colors.</p><p>The CIE 1931 color space defines these RGB color matching functions. The red, green, and blue lines represent the intensity of each RGB light source:</p><a id="block-4f5638"></a><p><em>Note: this plot uses the table from the original study. This raw data must not be used anymore because I couldn't find it anywhere. I had to extract it myself from an appendix in the <a href="https://ia802802.us.archive.org/23/items/gov.law.cie.15.2004/cie.15.2004.pdf">original report</a>.</em></p><p>Given a wavelength on the X axis, you can see how to "mix" the RGB wavelengths to produce the target color.</p><p>How did they come up with these? They scientifically studied how our eyes mix RGB colors by sitting people down in a room with multiple light sources. One light source was the target color, and the other side had red, green, and blue light sources. People had to adjust the strength of the RGB sources until it matched the target color. They literally had people manually adjust lights and recorded the values! There's a <a href="https://medium.com/hipster-color-science/a-beginners-guide-to-colorimetry-401f1830b65a">great article</a> that explains the experiments in more detail.</p><p>There's a <strong>big problem</strong> with the above functions. Can you see it? What do you think a <em>negative</em> red light source means?</p><p>It's nonsense! That means with this model, given pure RGB lights, there are certain spectral colors that are impossible to recreate. However, this data is still incredibly useful and we can transform it into something meaningful.</p><p>Introducing the XYZ color matching functions. The XYZ color space is simply the RGB color space, but multiplied with a matrix to transform it a bit. The important part is this is a linear transform: it's literally the same thing, just reshaped a little.</p><p>I found a raw table for the XYZ color matching functions <a href="https://files.cie.co.at/CIE_xyz_1931_2deg.csv">here</a> and this is what it looks like. The CIE 1931 XYZ color matching functions:</p><a id="block-2dad83"></a><p><a href="https://en.wikipedia.org/wiki/CIE_1931_color_space#Construction_of_the_CIE_XYZ_color_space_from_the_Wright%E2%80%93Guild_data">Wikipedia</a> defines the RGB matrix transform as this:</p><pre><code>matrix = [
  <span>2.364613</span>,  -<span>0.89654</span>, -<span>0.468073</span>,
 -<span>0.515166</span>,  <span>1.426408</span>, <span>0.088758</span>,
  <span>0.005203</span>, -<span>0.014408</span>, <span>1.009204</span>
]

[R, G, B] = matrix * [X, Y, Z]</code></pre><p>We can take the XYZ table and transform it with the above matrix, and doing so produces this graph. Look familiar? This is exactly what the RGB graph above looks like (plotted directly from the data table)!</p><a id="block-6de61f"></a><p>Wikipedia also documents an <a href="https://en.wikipedia.org/wiki/CIE_1931_color_space#Analytical_approximation">analytical approximation</a> of this data, which means we can use mathematical functions to generate the data instead of using tables. Press "view source" to see the algorithm:</p><a id="block-bcc10d"></a><h3 id="how-is-this-useful"><a href="#how-is-this-useful">How is this useful?</a></h3><p>Ok, so we have these color matching functions. When displaying these colors with RGB lights though, we can't even show all of the spectral colors. Transforming it into XYZ space, where everything is positive, fixes the numbers but what's the point if we still can't physically show them?</p><p>The XYZ space <em>describes</em> all colors, even colors that are impossible to display. It's become a standard space to encode colors in a device-independent way, and it's up to a specific device to interpret them into a space that it can physically produce. This is nice because we have a standard way to encode color information without restricting the possibilities of the future -- as devices become better at displaying more and more colors, they can automatically start displaying them without requiring any infrastructure changes.</p><h2 id="chromaticity"><a href="#chromaticity">Chromaticity</a></h2><p>Now let's get back to that cursed shape. That's actually a <a href="https://en.wikipedia.org/wiki/Chromaticity">chromaticity</a> diagram, which is "objective specification of the quality of a&nbsp;color regardless of its&nbsp;luminance".</p><p>We can derive the chromaticity for a color by taking the XYZ values for it dividing each by the total:</p><pre><code><span>const</span> x = X / (X + Y + Z)
<span>const</span> y = Y / (X + Y + Z)
<span>const</span> z = Z / (X + Y + Z) = <span>1</span> - x - y</code></pre><p>We don't actually need <code>z</code> because we can derive it given <code>x</code> and <code>y</code>. Hence we have the "xy chromaticity diagram". Remember how I said it's a 3d curve projected onto a 2d space? We've done that by just dropping <code>z</code>.</p><p>If we want to go back to XYZ from <code>xy</code>, we need the <code>Y</code> value. This is called the <code>xyY</code> color space and is another way to encode colors.</p><p>Alright, let's try this out. Let's take the RGB table we rendered above, and plot the chromaticity. We do this by using the above functions, and plotting the <code>x</code> and <code>y</code> points (the colors are a basic estimation):</p><a id="block-58d0f7"></a><p>Hey! Look at that! That looks familiar. Why is it so <em>slanted</em> though? If you look at the x axis, it actually goes into negative! That's because the RGB data is representing impossible colors.</p><p>Let's use an RGB to XYZ matrix to transform it into XYZ space (the opposite of what we did before, where we transformed XYZ into RGB) space. If we render the same data but transformed, it looks like this:</p><a id="block-d7d4da"></a><p>Now that's looking <em>really</em> familiar!</p><p>Just to double-check, let's render the chromaticity of the XYZ table data. Note that we have more granular data here, so there are more points, but it matches:</p><a id="block-c7a085"></a><p>Ok, so what about colors? How do we fill the middle part with all the colors? Note: this is where I really start to get out my league, but here's my best attempt.</p><p>What if we iterate over every single pixel in the canvas and try to plot a color for it? The question is given x and y, how do we get a color?</p><p>Here are some steps:</p><p>We scale each x and y point in the canvas to a value between 0 and 1</p><p>Remember above I said we need the <code>Y</code> value to transform back into XYZ space? Turns out that the XYZ space intentionally made <code>Y</code> map to the luminance value of a color, so that means we can... make it up?</p><p>What if we just try to use a luminance value of 1?</p><p>That lets us generate XYZ values, which we then translate into sRGB space (don't worry about the <code>s</code> there, it's just RGB space with some gamma correction)</p><p>One immediate problem you hit this produces many invalid colors. We also want to experiment with different values of <code>Y</code>. The demo below has controls to customize its behavior: change <code>Y</code> from 0 to 1, and hide colors with elements below 0 or 255.</p><a id="block-31f373"></a><a id="block-fba881"></a><a id="block-4e7753"></a><inspect-code disabled="false" data-block-id="4e7753"><div>
<p>Y:  <span id="y-slider-value">1</span></p>
<p> <label for="clip-low">Clip colors low (less than 0)</label></p>
<p> <label for="clip-high">Clip colors max (greater than 255)</label></p>
</div></inspect-code><p>That's neat! We're getting somewhere, and are obviously constrained by the RGB space. By default, it clips colors with negative values and that produces this triangle. Feels like the dots are starting to connect: the above image is clearly showing connections between XYZ/RGB and limitations of representable colors.</p><p>Even more interesting is if you turn on "clip colors max". You only see a small slice of color, and you need to move the <code>Y</code> slider morph the shape to "fill" the triangle. Almost like we're moving through 3d space.</p><p>For each point, there must be a different <code>Y</code> value that is the most optimal representation of that color. For example, blues are rich when <code>Y</code> is low, but greens are only rich when <code>Y</code> is higher.</p><h2 id="taking-a-break-spectrums"><a href="#taking-a-break-spectrums">Taking a break: spectrums</a></h2><p>I'm still confused how to fill that space within the chromaticity diagram, so let's take a break.</p><p>Let's create a spectrum. Take the original color matching function. Since that is telling us the XYZ values needed to create a spectral color, shouldn't we be able to iterate over the wavelengths of visible colors (400-720), get the XYZ values for each one, and convert them to RGB and render a spectrum?</p><a id="block-3222ac"></a><a id="block-0ffafa"></a><p>This looks pretty bad, but why? I found a <a href="https://aty.sdsu.edu/explain/optics/rendering.html">nice article</a> about rendering spectra which seems like another deep hole. My problems aren't even close to that kind of accuracy; the above isn't remotely close.</p><p>Turn out I need to convert XYZ to sRGB because that's what the <code>rgb()</code> color function is assuming when rendering to canvas. The main difference is <em>gamma correction</em> which is another topic.</p><a id="block-5cabf2"></a><p>We've learned that sRGB can only render a subset of all colors, and turns out there are other color spaces we can use to tell browsers to render more colors. The <a href="https://en.wikipedia.org/wiki/DCI-P3">p3</a> wide gamut color space is larger than sRGB, and many browsers and displays support it now, so let's test it.</p><p>You specify this color space by using the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/color_value/color"><code>color</code></a> function in CSS, for example: <code>color(display-p3 r, g, b)</code>. I ran into the same problems where the colors were all wrong, which was surprising because everything I read implied it was linear. Turns out the p3 color space in browsers has the same gamma correction as sRGB, so I needed to include that to get it to work:</p><a id="block-808408"></a><p>If you are seeing this on a wide gamut compatible browser and display, you will see more intense colors. I love that this is a thing, and the idea that so many users are using apps that could be more richly displayed if they supported p3.</p><p>I started having an existential crisis around this point. What are my eyes actually seeing? How do displays... actually work? Looking at the wide gamut spectrum above, what happens if I take a screenshot of it in macOS and send it to a user using a display that doesn't support p3?</p><p>To test this I started a zoom chat with a friend and shared my screen and showed them the wide gamut spectrum and asked if they could see a difference (the top and bottom should look different). Turns out they could! I have no idea if macOS, zoom, or something else is translating it into sRGB (thus "downgrading" the colors) or actually transmitting p3. (Also, PNG supports p3, but what do monitors that don't support it do?)</p><p>The sheer complexity of abstractions between my eyes and pixels is overwhelming. There are so many layers which handle reading and writing the individual pixels on my screen, and making it all work across zoom chats, screenshots, and everything is making my mind melt.</p><p>Let's move on.</p><p><em><strong>A little question:</strong> why does printing use the CMY color system with the primaries of cyan, magenta, and yellow, while digital displays build pixels with the primaries of reg, green, and blue? If cyan, magenta, and yellow allow a wider range of colors via mixing why is RGB better digitally? Answer: because RGB is an additive color system and CMY is a subtractive color system. Materials absorb light, while digital displays emit light.</em></p><h2 id="back-to-the-grind"><a href="#back-to-the-grind">Back to the grind</a></h2><p>We're not giving up on figuring out the colors of the chromaticity diagram yet.</p><p>I found <a href="https://clarkvision.com/articles/color-cie-chromaticity-and-perception/">this incredible article</a> about how to populate chromaticity diagrams. I still have no idea if this is how the original ones were generated. After all, the colors shown are just an approximation (your screen can't actually display the true colors near the edges), so maybe there's some other kind of formula.</p><p>So that I can get back to my daily life and be present with my family, I'm accepting that this is how those images are generated. Let's try do it ourselves.</p><p>There's no way to go from an <code>x, y</code> point in the canvas to a color. There's no formula tells us if it's a valid point in space or how to approximate a color for it.</p><p>We need to do the opposite: start with an value in the XYZ color space, compute an approximate color, and plot it at the right point by converting it into xy space. But how do we even find valid XYZ values? Not all points are valid inside that space (between 0 and 1 on all three axes). To do <em>that</em> we have to take another step back.</p><p>I got this technique from the <a href="https://clarkvision.com/articles/color-cie-chromaticity-and-perception/">incredible article</a> linked above. What we're trying to is <strong>render all colors in existence</strong>. Obviously we can't actually do that, so we need an approximation. Here's the approach we'll take:</p><p>First, we need to generate an arbitrary color. The only way to do this is to generate a <a href="https://en.wikipedia.org/wiki/Spectral_line_shape">spectral line shape</a>. Basically it's a line across all wavelengths (the X axis) that defines how much each wavelength contributes to the color.</p><p>To get the xy, coordinate on the canvas, we need to get the XYZ values for the color. To do that, we multiply the XYZ color matching functions with the spectral line, and then take the integral of each line to get the final XYZ values.</p><p>We do the same for the RGB color. We multiply the RGB color matching functions with the spectral line and take the integral of each one for the final RGB color. (We'll talk about the colors more later)</p><p>I don't know if that made any sense, but here's a demo which might help. The graph in the bottom left is the spectral line we are generating. This represents a specific color, which is shown in the top left. Finally, on the right we plot the color on the chromaticity diagram by summing up the area of the spectral line multiplied by the XYZ color matching functions.</p><p>We generated the spectral line graph with two simple sine curves with a specific width and offset. You can change the offset of each curve with the sliders below. You can see that moving those curves, which generates a different spectral line (and thus color) which plots different points on the diagram.</p><p><strong>By adjusting the sliders, you are basically painting the chromaticity diagram!</strong></p><a id="block-60d1c4"></a><inspect-code disabled="false" data-block-id="60d1c4"></inspect-code><a id="block-ec88e9"></a><a id="block-73d5bc"></a><p>You can see how all of this works but pressing "view source" to see the code.</p><p>Obviously this is a very poor representation of the chromaticity diagram. It's difficult to cover the whole area; adjusting the offset of the curves only allows you to walk through a subset of the entire space. We would need to change how we are generating spectral lines to fully walk through the space.</p><p>Here's a demo which attempts to automate this. It's using the same code as above, except it's changing both offset and width of the curves and walking through the space better:</p><a id="block-554867"></a><a id="block-bba249"></a><p>I created an <a href="https://codepen.io/jlongster/pen/QWoEwXr?editors=0010">isolated codepen</a> if you want to play with this yourself. If you let this run for a while, you'll end up with a shape like this:</p><figure>
<img src="https://static.jlongster.com/20240105/ScreenShot2024-01-05e.png">
<figcaption>Our rendering of the chromaticity diagram. I'll take it?
</figcaption></figure><p>We're still not walking through the full space, but it's not bad! It at least... vaguely resembles the original diagram?</p><figure>
<img src="https://static.jlongster.com/20240103/CIE1931xy_blank.svg">
<figcaption>By BenRG - Own work based on: CIExy1931.svg, Public Domain, https://commons.wikimedia.org/w/index.php?curid=7889658</figcaption>
</figure><h3 id="what-s-going-on-with-the-colors"><a href="#what-s-going-on-with-the-colors">What's going on with the colors?</a></h3><p>Our coloring isn't quite right. It's missing the white spot in the middle and it's too dark in certain places. Let me explain a little more how we generated these colors.</p><p>After all, didn't we generate RGB colors? If so, why weren't they clipped and showing a triangle like before? Or at least we should see more "maxing out" of colors near the edges.</p><p>My first attempts at the above <em>did</em> show this. Here's a picture where I only took the integral to find the XYZ values, and then took those values and used <code>XYZ_to_sRGB</code> to transform them into RGB colors:</p><figure>
<img src="https://static.jlongster.com/20240105/ScreenShot2024-01-05.png">
<figcaption>Way too many "maxed out" colors</figcaption>
</figure><p>We do get more of the bright white spot in the middle, but the colors are far too saturated. It's clear that many of these colors are actually invalid (they are not in between 0 and 255).</p><p>Another technique I learned from the <a href="https://clarkvision.com/articles/color-cie-chromaticity-and-perception/">incredible article</a> is to avoid using the XYZ points to find the color, and instead do the same integration over the RGB color mapping functions. So we take our spectral line, multiple by each of the RGB functions, and then take the sum of each result to find the individual RGB values.</p><p>Even though this still produces invalid colors, intuitively I can see how it more directly maps onto the RGB space and provides a better interpolation.</p><p>That's about as far as I got. I wish I had a better answer for how to generate the colors here, and maybe you know? If so, <a href="https://twitter.com/jlongster">give me a shout!</a> I'm satisfied with how far I got, and I bet the final answer uses slightly different color matching functions or something, but it doesn't feel far off.</p><p>If you have ideas to improve this, please do so <a href="https://codepen.io/jlongster/pen/QWoEwXr?editors=0010">in this demo</a>! I'd love to see any improvements.</p><p>I want to drive home that my above implementation <em>is</em> still generating invalid colors. For example, if I add clipping and avoid rendering any colors with elements outside of the 0-255 range, I get the familiar sRGB triangle:</p><figure>
<img src="https://static.jlongster.com/20240105/ScreenShot2024-01-05d.png">
<figcaption>Same image above but only showing valid RGB colors</figcaption>
</figure><p>It turns out that even though colors outside the triangle aren't rendering accurately, we're still able to represent a change of color because only 1 or 2 of the RGB channels have maxed out. If green maxes out, changes in the red and blue channels will still show up.</p><h3 id="more-shape-explorations"><a href="#more-shape-explorations">More shape explorations</a></h3><p>But really, <em>why</em> that specific shape? I know it derives from how we perceive red, green, and blue relative to each other. Let's look at the XYZ color matching functions again:</p><a id="block-140cd2"></a><p>The shape is derived from <em>these</em> shapes. To render chromaticity, you walk through each wavelength above and calculate the percentage of each XYZ value of the total. So there's a direct relationship.</p><p>Let's drive this home by generating our own random color matching functions. We generate them with some simple sine waves (view source to see the code):</p><a id="block-7d34e3"></a><p>Now let's render the chromaticity according to our nonsensical color matching functions:</p><a id="block-6bf276"></a><p>The shape is very different! So that's it: the shape is due to the XYZ color matching functions, which were derived from experiments that studied how our eyes perceive red, green, and blue light. That's why the chromaticity diagram represents something meaningful: it's how our eyes perceive color.</p><p>Resources:</p><ul>
<li><a href="https://en.wikipedia.org/wiki/CIE_1931_color_space#CIE_RGB_color_space">The CIE RGB color space</a></li>
<li><a href="https://medium.com/hipster-color-science/a-beginners-guide-to-colorimetry-401f1830b65a">A Beginner’s Guide to (CIE) Colorimetry</a></li>
<li><a href="https://clarkvision.com/articles/color-cie-chromaticity-and-perception/">CIE Chromaticity and Perception</a></li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Repulsive Shells [video] (101 pts)]]></title>
            <link>https://www.youtube.com/watch?v=qDM6bCt1Gic</link>
            <guid>41080642</guid>
            <pubDate>Fri, 26 Jul 2024 17:50:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=qDM6bCt1Gic">https://www.youtube.com/watch?v=qDM6bCt1Gic</a>, See on <a href="https://news.ycombinator.com/item?id=41080642">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Call the compiler, fax it your code [video] (139 pts)]]></title>
            <link>https://www.youtube.com/watch?v=pJ-25-pRhpY</link>
            <guid>41080599</guid>
            <pubDate>Fri, 26 Jul 2024 17:45:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=pJ-25-pRhpY">https://www.youtube.com/watch?v=pJ-25-pRhpY</a>, See on <a href="https://news.ycombinator.com/item?id=41080599">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[A simple procedural animation technique [video] (102 pts)]]></title>
            <link>https://www.youtube.com/watch?v=qlfh_rv6khY</link>
            <guid>41080570</guid>
            <pubDate>Fri, 26 Jul 2024 17:40:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=qlfh_rv6khY">https://www.youtube.com/watch?v=qlfh_rv6khY</a>, See on <a href="https://news.ycombinator.com/item?id=41080570">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Bayesian Statistics: The three cultures (210 pts)]]></title>
            <link>https://statmodeling.stat.columbia.edu/2024/07/10/three-cultures-bayes-subjective-objective-pragmatic/</link>
            <guid>41080373</guid>
            <pubDate>Fri, 26 Jul 2024 17:15:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://statmodeling.stat.columbia.edu/2024/07/10/three-cultures-bayes-subjective-objective-pragmatic/">https://statmodeling.stat.columbia.edu/2024/07/10/three-cultures-bayes-subjective-objective-pragmatic/</a>, See on <a href="https://news.ycombinator.com/item?id=41080373">Hacker News</a></p>
Couldn't get https://statmodeling.stat.columbia.edu/2024/07/10/three-cultures-bayes-subjective-objective-pragmatic/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Monumental proof settles geometric Langlands conjecture (132 pts)]]></title>
            <link>https://www.quantamagazine.org/monumental-proof-settles-geometric-langlands-conjecture-20240719/</link>
            <guid>41080366</guid>
            <pubDate>Fri, 26 Jul 2024 17:14:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/monumental-proof-settles-geometric-langlands-conjecture-20240719/">https://www.quantamagazine.org/monumental-proof-settles-geometric-langlands-conjecture-20240719/</a>, See on <a href="https://news.ycombinator.com/item?id=41080366">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In classical signal processing, sound waves get built up out of sine waves whose frequencies correspond to the pitches contained in the sound. It’s not enough to know which pitches the sound contains — you must also know how loud each pitch is. That information allows you to write your sound as a combination of sine waves: just start with the sine waves of amplitude 1, then multiply each sine wave by an appropriate loudness factor before adding the sine waves together. The sum of all the different amplitude-1 sine waves is what we commonly refer to as white noise.</p>
<p>In the world of the geometric Langlands program, eigensheaves are supposed to play the role of sine waves. Gaitsgory and his collaborators had identified something called the Poincaré sheaf that seemed to be serving the role of white noise. But the researchers didn’t know whether each eigensheaf is even represented in the Poincaré sheaf, let alone whether they all have the same amplitude.</p>
<p>In the spring of 2022, Raskin, together with his graduate student Joakim Færgeman, showed how to use the ideas in the six-author paper to <a href="https://arxiv.org/abs/2207.02955">prove</a> that each eigensheaf does contribute to the Poincaré sheaf. “After Sam’s and Joakim’s paper, I was certain we’ll do it within a short period&nbsp;of time,” Gaitsgory said of proving the geometric Langlands conjecture.</p>

<p>The researchers needed to show that all the eigensheaves make equal contributions to the Poincaré sheaf, and that the fundamental-group representations label the frequencies of these eigensheaves. The trickiest part, they came to realize, was handling representations of the fundamental group called irreducible representations.</p>
<p>The solution for these irreducible representations came to Raskin at a moment when his personal life was filled with chaos. A few weeks after he and Færgeman posted their paper online, Raskin had to rush his pregnant wife to the hospital, then return home to take his son to his first day of kindergarten. Raskin’s wife remained in the hospital until the birth of their second child six weeks later, and during this time Raskin’s life revolved around keeping life normal for his son and driving in endless loops between home, his son’s school and the hospital. “My whole life was the car and taking care of people,” he said.</p>
<p>He took to calling Gaitsgory on his drives to talk math. By the end of the first of those weeks, Raskin had realized that he could reduce the problem of irreducible representations to proving three facts that were all within reach. “For me it was this amazing period,” he said. His personal life was “filled with anxiety and dread about the future. For me, math is always this very grounding and meditative thing that takes me out of that kind of anxiety.”</p>

<p>By early 2023, Gaitsgory and Raskin, together with Arinkin, Rozenblyum, Færgeman and four other researchers, had a complete proof of Beilinson and Drinfeld’s “best hope,” as modified by Gaitsgory and Arinkin. (The other researchers are <a href="https://profiles.ucl.ac.uk/77921-dario-beraldo">Dario Beraldo</a> of University College London, <a href="https://windshower.github.io/linchen/">Lin Chen</a> of Tsinghua University in Beijing, and <a href="https://mathematics.uchicago.edu/people/profile/justin-campbell/">Justin Campbell</a> and <a href="https://mathematics.uchicago.edu/people/profile/kevin-lin/">Kevin Lin</a> of the University of Chicago.) It would take the team another year to write up the proof, which they posted online in February. While the papers follow aspects of the outline Gaitsgory developed back in 2013, they both simplify his approach and go beyond it in many ways. “Very bright people contributed a lot of new ideas to this crowning achievement,” Lafforgue said.</p>
<p>“It wasn’t just that they went and proved it,” Ben-Zvi said. “They developed whole worlds around it.”</p>
<h2><strong>Further Shores</strong></h2>
<p>For Gaitsgory, the fulfillment of his decades-long dream is far from the end of the story. A host of further challenges await mathematicians — exploring the connection to quantum physics more deeply, extending the result to Riemann surfaces with punctures, and figuring out the implications for the other columns of the Rosetta stone. “It feels (at least to me) more like that one piece of a big rock has been chipped off, but we are still far from the core,” Gaitsgory wrote in an email.</p>
<p>Researchers working in the other two columns are now eager to translate what they can. “The fact that one of the major pieces has fallen should have major repercussions throughout the Langlands correspondence,” Ben-Zvi said.</p>

<p>Not everything can carry over — for instance, in the number theory and function field settings, there is no counterpart to the conformal field theory ideas that enabled researchers to construct special eigensheaves in the geometric setting. Much of the proof will need serious adjustment before it can be made to work in the other two columns, warned <a href="https://math.berkeley.edu/~fengt/">Tony Feng</a> of Berkeley. It remains to be seen, he said, whether “we can even transport the ideas to a different context where it was not designed to work.”</p>
<p>But many researchers are optimistic that the rising sea of ideas will eventually reach these other domains. “It’s going to seep through all the barriers between subjects,” Ben-Zvi said.</p>
<p>In the past decade, researchers have started turning up unexpected connections between the geometric column and the other two. “If [the geometric Langlands conjecture] had been proved 10 years ago, then the results would be very different,” Feng said. “It wouldn’t have been appreciated that it could potentially have ramifications outside [the geometric Langlands] community.”</p>
<p>Gaitsgory, Raskin and their collaborators have already made progress on translating their geometric Langlands proof to the function field column. (Some of the discoveries Gaitsgory and Raskin made on the latter’s long car drives are “still to come,” Raskin hinted.) If successful, this translation will prove a much more precise version of function field Langlands than mathematicians knew or even&nbsp;conjectured before now.</p>
<p>Most translations from the geometry column to the number theory column pass through function fields along the way. But in 2021, Laurent Fargues, of the Mathematics Institute of Jussieu in Paris, and Scholze devised what Scholze called a <a href="https://www.quantamagazine.org/with-a-new-shape-mathematicians-link-geometry-and-numbers-20210719/">wormhole</a> that carries ideas from the geometric column directly over to a part of the number theory Langlands program.</p>

<p>“I’m definitely one of the people who are now trying to translate all this geometric Langlands stuff,” Scholze said. With the rising sea having spilled over into thousands of pages of text, that is no easy matter. “I’m currently a few papers behind,” Scholze said, “trying to read what they did in around 2010.”</p>
<p>Now that the geometric Langlands researchers finally have their lengthy proof down on paper, Caraiani hopes they will have more time to talk to researchers on the number theory side. “It’s people who have very different ways of thinking about things, and there’s always a benefit if they manage to slow down and talk to each other and see the other’s perspective,” she said. It’s only a matter of time, she predicted, before the ideas from the new work permeate number theory.</p>
<p>As Ben-Zvi put it, “These results are so robust that once you get started, it’s hard to stop.”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stripe acquires Lemon Squeezy (205 pts)]]></title>
            <link>https://www.lemonsqueezy.com/blog/stripe-acquires-lemon-squeezy</link>
            <guid>41080018</guid>
            <pubDate>Fri, 26 Jul 2024 16:39:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lemonsqueezy.com/blog/stripe-acquires-lemon-squeezy">https://www.lemonsqueezy.com/blog/stripe-acquires-lemon-squeezy</a>, See on <a href="https://news.ycombinator.com/item?id=41080018">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Woohoo! We're excited and humbled to announce that<a href="https://twitter.com/stripe"> @stripe</a> has acquired<a href="https://twitter.com/lmsqueezy"> @lmsqueezy</a>.</p><figure><p><img src="https://cdn.prod.website-files.com/6347244ba8d63461aa51c0af/66a2a7f5c6f750743a4429b4_66a11e827944b93c86a509ff_Lemon-Squeezy%252BStripe-3.jpeg" loading="lazy" alt=""></p></figure><p>In 2020, when the world gave us lemons, we decided to make lemonade. We imagined a world where selling digital products would be as simple as opening a lemonade stand. We dreamed of a platform that would take the pain out of selling globally.</p><p>Tax headaches, fraud prevention, handling chargebacks, license key management, and file delivery, among other things, are complicated.</p><p>We believed it should be simple.</p><p>We believed it should be easy-peasy.</p><p>As founders, we've spent a decade selling digital products, and so we created a solution that met our own needs. But what started as an idea to solve the day-to-day problems of selling digital products evolved into something much bigger. Nine months after our public launch in 2021, we surpassed $1M in ARR and never looked back.</p><p>We worked tirelessly through growing pains while also celebrating major milestones along the way. Each step reinforced that we were onto something remarkable.</p><p>Along the way, we received many acquisition offers and (Series A) term sheets from investors. But despite the allure of these opportunities, we knew that what we had built was truly special and needed the right partner to take it to the next level.</p><p>We're proud to say that we've found that partner in Stripe and have gone from idea to acquisition in under three years.</p><h2>A perfect blend: Lemon Squeezy x Stripe</h2><p>Stripe continues to set the bar in the payments industry with its world-class developer experience, API standards, and dedication to beauty and craft. It's no secret that we (like many) have always admired Stripe.</p><p>When we began discussions about a potential acquisition, it was immediately apparent that our values and mission were perfectly aligned.</p><p>Lemon Squeezy and Stripe share a deep love for our customers and a commitment to making selling effortless.</p><p>Now imagine combining everything you love about Lemon Squeezy and Stripe — we believe it's a match made in heaven.</p><p>Lemon Squeezy is now packed with 1,000% more juice.</p><h2>Looking ahead: The next chapter</h2><p>Lemon Squeezy has been processing payments on Stripe since our inception. This acquisition marks the culmination of years of effort and celebrates our close partnership with Stripe and our shared sense of purpose.&nbsp;</p><p>Going forward, our mission remains the same: make selling digital products easy-peasy.</p><p>With Stripe’s help, we’ll continue to improve the merchant of record offering, bolstering billing support, building an even more intuitive customer experience, and more.</p><p>We're incredibly excited about the possibilities that lie ahead with the Lemon Squeezy and Stripe teams joining forces. The future is bright.</p><h2>What does this mean for Lemon Squeezy customers?</h2><p>Rest assured, we’ll continue delivering the same fantastic product and reliability you’ve come to trust. We’ll be in touch as we work through this process with any updates as they come along. We’re excited about finding the best ways to combine Lemon Squeezy and Stripe.</p><p>At Lemon Squeezy, you (our wonderful customers) are at the heart of everything we do. We pride ourselves on creating intuitive, customer-focused products backed by top-notch customer service.</p><p>We remain as committed as ever.</p><h2>A big thank you</h2><p>Over the years, our community has grown exponentially. This growth is a testament to the trust and support you've shown us, and we couldn't be more grateful.</p><p>We owe a huge thank you to our team, community, and supporters. Thousands of companies continue to choose to sell globally through Lemon Squeezy, and we'll never take that for granted.</p><p>Thank you for being part of our journey. We look forward to all the fantastic things we will achieve together with Stripe.</p><p>Stay fresh,<br>Lemon Squeezy Team</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[There is no fix for Intel's crashing 13th/14th Gen CPUs – damage is permanent (141 pts)]]></title>
            <link>https://www.theverge.com/2024/7/26/24206529/intel-13th-14th-gen-crashing-instability-cpu-voltage-q-a</link>
            <guid>41079901</guid>
            <pubDate>Fri, 26 Jul 2024 16:22:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/7/26/24206529/intel-13th-14th-gen-crashing-instability-cpu-voltage-q-a">https://www.theverge.com/2024/7/26/24206529/intel-13th-14th-gen-crashing-instability-cpu-voltage-q-a</a>, See on <a href="https://news.ycombinator.com/item?id=41079901">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>On Monday, it initially seemed like the <a href="https://www.theverge.com/2024/7/22/24203959/intel-core-13th-14th-gen-cpu-crash-update-patch">beginning of the end for Intel’s desktop CPU instability woes</a> — the company confirmed a patch is coming in mid-August that should address the “root cause” of exposure to elevated voltage. But if your 13th or 14th Gen Intel Core processor is already crashing, that patch apparently won’t fix it. </p><p>Citing unnamed sources,<em> </em><a href="https://www.tomshardware.com/pc-components/cpus/intel-finally-announces-a-solution-for-cpu-crashing-errors-claims-elevated-voltages-are-the-root-cause-fix-coming-by-mid-august"><em>Tom’s Hardware </em>reports</a> that any degradation of the processor is irreversible, and an Intel spokesperson did not deny that when we asked. Intel is “confident” the patch will keep it from happening in the first place. (As another preventative measure, you should update your motherboard BIOS ASAP.) But if your defective CPU has been damaged, your best option is to replace it instead of tweaking BIOS settings to try and alleviate the problems. </p><p>And, Intel confirms, too-high voltages aren’t the only reason some of these chips are failing. Intel spokesperson Thomas Hannaford confirms it’s <em>a</em> primary cause, but the company is still investigating. Intel community manager Lex Hoyos also revealed some instability reports can be <a href="https://www.reddit.com/r/intel/comments/1e9mf04/comment/lefz09c/">traced back to an oxidization manufacturing issue</a> that was fixed at an unspecified date last year.</p><p>This raises lots of questions. Will Intel recall these chips? Extend their warranty? Replace them no questions asked? Pause sales <a href="https://www.theverge.com/2024/7/24/24205416/amd-zen-5-ryzen-9000-desktop-delay">like AMD just did</a> with its Ryzen 9000? Identify faulty batches with the manufacturing defect?   </p><p>We asked Intel these questions, and I’m not sure you’re going to like the answers. </p><div><p>Why are these still on sale without so much as an extended warranty?</p></div><p>Intel has not halted sales or clawed back any inventory. It will not do a recall, period. The company is not currently commenting on whether or how it might extend its warranty. It would not share estimates with <em>The Verge</em> of how many chips are likely to be irreversibly impacted, and it did not explain why it’s continuing to sell these chips ahead of any fix. </p><p>Intel’s not yet telling us how warranty replacements will work beyond trying customer support again if you’ve previously been rejected. It did not explain how it will contact customers with these chips to warn them about the issue.</p><p>But Intel <em>does</em> tell us it’s “confident” that you don’t need to worry about invisible degradation. If you’re not currently experiencing issues, the patch “will be an effective preventative solution for processors already in service.” (If you don’t know if you’re experiencing issues, Intel currently suggests <a href="https://youtu.be/wkrOYfmXhIc?si=XzCym6UjLcF3_yW2&amp;t=497">the Robeytech test</a>.)</p><p>And, perhaps for the first time, Intel has confirmed just how broad this issue could possibly be. The elevated voltages could potentially affect any 13th or 14th Gen desktop processor that consumes 65W or more power, not just the highest i9-series chips that <a href="https://www.theverge.com/2024/4/9/24125036/intel-game-crash-13900k-14900k-fortnite-unreal-engine-investigation">initially seemed to be experiencing the issue</a>.</p><p>Here are the questions we asked Intel and the answers we’ve received by email from Intel’s Hannaford:</p><p><strong>How many chips does&nbsp;Intel&nbsp;estimate are likely to be irreversibly impacted by these issues?</strong></p><p>Intel&nbsp;Core 13th and 14th Generation desktop processors with 65W or higher base power – including K/KF/KS and 65W non-K variants – could be affected by the elevated voltages issue. However, this does not mean that all processors listed are (or will be) impacted by the elevated voltages issue.</p><p>Intel&nbsp;continues validation to ensure that scenarios of instability reported to&nbsp;Intel&nbsp;regarding its Core 13th and 14th Gen desktop processors are addressed.</p><p>For customers who are or have been experiencing instability symptoms on their 13th and/or 14th Gen desktop processors,&nbsp;Intel&nbsp;continues advising them to reach out to&nbsp;Intel&nbsp;Customer Support for further assistance. Additionally, if customers have experienced these instability symptoms on their 13th and/or 14th Gen desktop processors but had RMA [return merchandise authorization] requests rejected we ask that they reach out to&nbsp;Intel&nbsp;Customer Support for further assistance and remediation.</p><p><strong>Will&nbsp;Intel&nbsp;issue a recall?</strong></p><p>No.</p><p><strong>Will Intel proactively warn buyers of these chips about the warning signs or that this update is required? If so, how will it warn them?</strong></p><p>Intel&nbsp;targets to release a production microcode update to OEM/ODM customers by mid-August or sooner and will share additional details on the microcode patch at that time.</p><p>Intel&nbsp;is investigating options to easily identify affected processors on end user systems. In the interim, as a general best practice&nbsp;Intel&nbsp;recommends that users adhere to&nbsp;Intel&nbsp;Default Settings on their desktop processors, along with ensuring their BIOS is up to date.</p><p><strong>Has&nbsp;Intel&nbsp;halted sales and / or performed any channel inventory recalls while it validates the update?</strong></p><p>No.</p><p><strong>Does&nbsp;Intel&nbsp;anticipate the fix will be effective for chips that have already been in service but are not yet experiencing symptoms (i.e., invisible degradation)? Are those CPUs just living on borrowed time?</strong></p><p>Intel&nbsp;is confident that the microcode patch will be an effective preventative solution for processors already in service, though validation continues to ensure that scenarios of instability reported to&nbsp;Intel&nbsp;regarding its Core 13th/14th Gen desktop processors are addressed.</p><p>Intel&nbsp;is investigating options to easily identify affected or at-risk processors on end user systems.</p><p>It is&nbsp;<em>possible</em>&nbsp;the patch will provide some instability improvements to currently impacted processors; however customers experiencing instability on their 13th or 14th Generation desktop processor-based systems should contact&nbsp;Intel&nbsp;customer support for further assistance.</p><p><strong>Will&nbsp;Intel&nbsp;extend its warranty on these 13th Gen and 14th Gen parts, and for how long?&nbsp;</strong></p><p>[No answer yet.]</p><p><strong>Given how difficult this issue was for&nbsp;Intel&nbsp;to pin down, what&nbsp;proof will customers need to share to obtain an RMA?&nbsp;(How lenient will&nbsp;Intel&nbsp;be?)&nbsp;&nbsp;</strong></p><p>[No answer yet.]</p><p><strong>What will&nbsp;Intel&nbsp;do for 13th Gen buyers after supply of 13th Gen parts runs out? Final shipments were set to end last month,&nbsp;</strong><a href="https://www.tomshardware.com/pc-components/cpus/intel-axes-13th-gen-core-i5-i7-i9-k-series-cpus-lineup-will-be-discontinued-by-may-24th-2024"><strong>I’m reading</strong></a><strong>.</strong></p><p>Intel&nbsp;is committed to making sure all customers who have or are currently experiencing instability symptoms on their 13th and/or 14th Gen desktop processors are supported in the exchange process. This includes working with&nbsp;Intel’s retail and channel customers to ensure end users are taken care of regarding instability symptoms with their&nbsp;Intel&nbsp;Core 13th and/or 14th Gen desktop processors.</p><p><strong>What will&nbsp;Intel&nbsp;do for 14th Gen buyers after supply of 14th Gen parts run out?&nbsp;</strong></p><p>Same as above.</p><p><strong>Will replacement / RMA’d chips ship with the microcode update preapplied beginning in August? Is&nbsp;Intel&nbsp;still shipping replacement chips ahead of that update?</strong></p><p>Intel&nbsp;will be applying to microcode to 13th/14th Gen desktop processors that are not yet shipped once the production patch is released to OEM/ODM partners (targeting mid-August or sooner). For 13th /14th Gen desktop processors already in service, users will need to apply the patch via BIOS update once available.</p><p><strong>What, if anything, can customers do to slow or stop degradation&nbsp;ahead of the microcode update?</strong></p><p>Intel&nbsp;recommends that users adhere to&nbsp;Intel&nbsp;Default Settings on their desktop processors, along with ensuring their BIOS is up to date. Once the microcode patch is released to&nbsp;Intel&nbsp;partners, we advise users check for the relevant BIOS updates.</p><p><strong>Will&nbsp;Intel&nbsp;share specific manufacturing dates and serial number ranges for the oxidized processors so mission-critical businesses can selectively rip and replace?&nbsp;</strong></p><p>Intel&nbsp;will continue working with its customers on Via Oxidation-related reports and ensure that they are fully supported in the exchange process.</p><div><p><strong>Why does&nbsp;Intel&nbsp;believe the instability issues </strong><a href="https://www.reddit.com/r/intel/comments/1e9mf04/comment/lekoj61/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button"><strong>do not affect mobile laptop chips</strong></a><strong>?&nbsp;</strong></p></div><p>Intel&nbsp;is continuing its investigation to ensure that reported instability scenarios on&nbsp;Intel&nbsp;Core 13th/14th Gen processors are properly addressed.</p><p>This includes ongoing analysis to confirm the primary factors preventing 13th / 14th Gen mobile processor exposure to the same instability issue as the 13th/14th Gen desktop processors.&nbsp;&nbsp;</p><p>That’s all we’ve heard from Intel so far, though Hannaford assured us more answers are on the way and that the company is working on remedies. </p><p>Again, if your CPU is already damaged, you need to get Intel to replace it, and if Intel won’t do so, please let us know. In the meanwhile, you’ll want to update your BIOS as soon as possible because your processor could potentially be invisibly damaging itself — and if you know your way around a BIOS, you may want to adjust your motherboard to Intel’s default performance profiles, too.</p><p>Lastly, here is that Robeytech video that Intel is recommending to Redditors to potentially help them identify if their chip has an issue. Intel says it’s looking into other ways to identify that, too. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scaling One Million Checkboxes to 650M checks (174 pts)]]></title>
            <link>https://eieio.games/essays/scaling-one-million-checkboxes/</link>
            <guid>41079814</guid>
            <pubDate>Fri, 26 Jul 2024 16:14:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eieio.games/essays/scaling-one-million-checkboxes/">https://eieio.games/essays/scaling-one-million-checkboxes/</a>, See on <a href="https://news.ycombinator.com/item?id=41079814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>On June 26th 2024 I <a href="https://eieio.games/nonsense/game-14-one-million-checkboxes/">launched a website</a> called One Million Checkboxes (OMCB). It had one million <em>global</em> checkboxes on it - checking a box checked it for everyone on the site, immediately.</p>

<div>
    
    
    <video playsinline="" controls="" muted="" poster="https://eieio.games/assets/images/scaling-omcb/30mins-activity-firstframe.png">
    
        <source src="https://eieio.games/assets/images/scaling-omcb/30mins-activity.mp4" type="video/mp4">
    </video>
    
    <p> The site, 30 minutes after launch </p>
    
</div>

<p>I built the site in 2 days. I thought I’d get a few hundred users, max. That is <em>not</em> what happened.</p>

<p>Instead, within hours of launching, tens of thousands of users checked millions of boxes. They piled in from <a href="https://news.ycombinator.com/item?id=40975509">Hacker News</a>, <a href="https://www.reddit.com/r/InternetIsBeautiful/comments/1dp4t7y/onemillioncheckboxescom_a_webpage_with_one/">/r/InternetIsBeautiful</a>, <a href="https://mastodon.gamedev.place/@eieio/112683641983412025">Mastodon</a> and <a href="https://x.com/itseieio/status/1805986839058079896">Twitter</a>. A few days later OMCB appeared in the <a href="https://www.washingtonpost.com/technology/2024/07/02/one-million-checkboxes-pointless-fun/">Washington Post</a> and the <a href="https://www.nytimes.com/2024/07/03/style/one-million-checkboxes-game.html">New York Times</a>.</p>

<p>Here’s what activity looked like on the first day (I launched at 11:30 AM EST).</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/first-day-hourly-unique-visitors.png" alt="boxes checked hourly during OMCB's first day. The first few hours are missing.">
    <img src="https://eieio.games/assets/images/scaling-omcb/first-day-hourly-checked-boxes.png" alt="boxes checked hourly during OMCB's first day. The first few hours are missing.">
</p>
<p>I don't have logs for checked boxes from the first few hours because I originally only kept the latest 1 million logs for a given day(!)</p>

<p>I wasn’t prepared for this level of activity. The site crashed a lot. But by day 2 I started to stabilize things and people checked over 50 million boxes. We passed 650 million before I sunset the site 2 weeks later.</p>

<p>Let’s talk about how I kept the site (mostly) online!
<!-- excerpt-end --></p>
<h2 id="the-original-architecture">The original architecture</h2>
<p>Here’s the gist of the original architecture:</p>
<p><img src="https://eieio.games/assets/images/scaling-omcb/simple-original-arch.png" alt="The original architecture of OMCB. The client maintains a bitset tracking which boxes are checked and renders checkboxes lazily. The server has 1 million bits of state - one for each checkbox. The server flips bit 3 when box 3 is checked or unchecked.">
</p>
<p>Our checkbox state is just one million bits (125KB). A bit is “1” if the corresponding checkbox is checked and “0” otherwise.</p>

<p>Clients store the bits in a <a href="https://en.wikipedia.org/wiki/Bit_array">bitset</a> (an array of bytes that makes it easy to store, access, and flip raw bits) and reference that bitset when rendering checkboxes. Clients tell the server when they check a box; the server flips the relevant bit and broadcasts that fact to all connected clients.</p>

<p>To avoid throwing a million elements into the DOM, clients only render the checkboxes in view (plus a small buffer) using <a href="https://www.npmjs.com/package/react-window">react-window</a>.</p>

<p>I could have done this with a single process,I wanted an architecture that I <em>could</em> scale (and an excuse to use Redis for the first time in years). So the actual server setup looked like this:</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/original-arch-reverse-proxy.png" alt="The original architecture of OMCB. A client connects to nginx, which reverse proxies the client to a a Flask instance. Flask interfaces with Redis.">
</p>

<p>Clients hit nginx for static content, and then make a GET for the bitset state and a websocket connection (for updates); nginx (acting as a reverse proxy) forwards those requests to one of two Flask servers (run via gunicorn).</p>

<p>State is stored in Redis, which has good primitives for flipping individual bits. Clients tell Flask when they check a box; Flask updates the bits in Redis and writes an event to a pubsub (message queue). Both Flask servers read from that pubsub and notify connected clients when checkboxes are checked/unchecked.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/original-arch-request.png" alt="The original architecture of OMCB. A client connects to nginx, which reverse proxies the client to a a Flask instance. Flask interfaces with Redis.">
</p>
<p>We need the pubsub because we've got two Flask instances; a Flask instance can't just broadcast "box 2 was checked" to its own clients.</p>

<p>Finally, the Flask servers do simple rate-limiting (on requests per session and new sessions per IP<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> - foolishly stored in Redis!) and regularly send full state snapshots to connected clients (in case a client missed an update because, say, the tab was backgrounded).</p>

<details>
    <summary>Here's an (abbreviated) implementation of the Flask side.</summary>
    
<figure><pre><code data-lang="python"><span># This is long, so I've skipped Redis initialization, 
# my rate limiting and logging implementation, etc.
</span>
<span># You can run lua inside Redis. The lua runs atomically.
# It's pretty sweet.
</span><span>set_bit_script</span> <span>=</span> <span>"""
local key = KEYS[1]
local index = tonumber(ARGV[1])
local value = tonumber(ARGV[2])
local current = redis.call('getbit', key, index)
local diff = value - current
redis.call('setbit', key, index, value)
redis.call('incrby', 'count', diff)
return diff"""</span>
<span>set_bit_sha</span> <span>=</span> <span>redis_client</span><span>.</span><span>script_load</span><span>(</span><span>set_bit_script</span><span>)</span>

<span>def</span> <span>get_bit</span><span>(</span><span>index</span><span>):</span>
    <span>return</span> <span>bool</span><span>(</span><span>redis_client</span><span>.</span><span>getbit</span><span>(</span><span>'bitset'</span><span>,</span> <span>index</span><span>))</span>

<span>def</span> <span>set_bit</span><span>(</span><span>index</span><span>,</span> <span>value</span><span>):</span>
    <span>diff</span> <span>=</span> <span>redis_client</span><span>.</span><span>evalsha</span><span>(</span><span>set_bit_sha</span><span>,</span> <span>1</span><span>,</span> <span>'bitset'</span><span>,</span> <span>index</span><span>,</span> <span>int</span><span>(</span><span>value</span><span>))</span>
    <span>return</span> <span>diff</span> <span>!=</span> <span>0</span>

<span>def</span> <span>get_full_state</span><span>():</span>
    <span>raw_data</span> <span>=</span> <span>redis_client</span><span>.</span><span>get</span><span>(</span><span>"bitset"</span><span>)</span>
    <span>return</span> <span>base64</span><span>.</span><span>b64encode</span><span>(</span><span>raw_data</span><span>).</span><span>decode</span><span>(</span><span>'utf-8'</span><span>)</span>

<span>def</span> <span>get_count</span><span>():</span>
    <span>return</span> <span>int</span><span>(</span><span>redis_client</span><span>.</span><span>get</span><span>(</span><span>'count'</span><span>)</span> <span>or</span> <span>0</span><span>)</span>

<span>def</span> <span>publish_toggle_to_redis</span><span>(</span><span>index</span><span>,</span> <span>new_value</span><span>):</span>
    <span>redis_client</span><span>.</span><span>publish</span><span>(</span><span>'bit_toggle_channel'</span><span>,</span> <span>json</span><span>.</span><span>dumps</span><span>({</span><span>'index'</span><span>:</span> <span>index</span><span>,</span> <span>'value'</span><span>:</span> <span>new_value</span><span>}))</span>

<span>def</span> <span>state_snapshot</span><span>():</span>
    <span>full_state</span> <span>=</span> <span>get_full_state</span><span>()</span>
    <span>count</span> <span>=</span> <span>get_count</span><span>()</span>
    <span>return</span> <span>{</span><span>'full_state'</span><span>:</span> <span>full_state</span><span>,</span> <span>'count'</span><span>:</span> <span>count</span><span>}</span>

<span>@</span><span>app</span><span>.</span><span>route</span><span>(</span><span>'/api/initial-state'</span><span>)</span>
<span>def</span> <span>get_initial_state</span><span>():</span>
    <span>return</span> <span>jsonify</span><span>(</span><span>state_snapshot</span><span>())</span>

<span>def</span> <span>emit_full_state</span><span>():</span>
    <span>socketio</span><span>.</span><span>emit</span><span>(</span><span>'full_state'</span><span>,</span> <span>state_snapshot</span><span>())</span>

<span>@</span><span>socketio</span><span>.</span><span>on</span><span>(</span><span>'toggle_bit'</span><span>)</span>
<span>def</span> <span>handle_toggle</span><span>(</span><span>data</span><span>):</span>
    <span>if</span> <span>not</span> <span>allow_toggle</span><span>(</span><span>request</span><span>.</span><span>sid</span><span>):</span>
        <span>print</span><span>(</span><span>f</span><span>"Rate limiting toggle request for </span><span>{</span><span>request</span><span>.</span><span>sid</span><span>}</span><span>"</span><span>)</span>
        <span>return</span> <span>False</span>
    <span># There's a race here. It bit me pretty early on; I fixed it by 
</span>    <span># moving this logic into the lua script.
</span>    <span>count</span> <span>=</span> <span>get_count</span><span>()</span>
    <span>if</span> <span>count</span> <span>&gt;=</span> <span>1_000_000</span><span>:</span>
        <span>print</span><span>(</span><span>"DISABLED TOGGLE EXCEEDED MAX"</span><span>)</span>
        <span>return</span> <span>False</span>
    <span>index</span> <span>=</span> <span>data</span><span>[</span><span>'index'</span><span>]</span>
    <span>current_value</span> <span>=</span> <span>get_bit</span><span>(</span><span>index</span><span>)</span>
    <span>new_value</span> <span>=</span> <span>not</span> <span>current_value</span>
    <span>print</span><span>(</span><span>f</span><span>"Setting bit </span><span>{</span><span>index</span><span>}</span><span> to </span><span>{</span><span>new_value</span><span>}</span><span> from </span><span>{</span><span>current_value</span><span>}</span><span>"</span><span>)</span>
    <span>set_bit</span><span>(</span><span>index</span><span>,</span> <span>new_value</span><span>)</span>
    <span>forwarded_for</span> <span>=</span> <span>request</span><span>.</span><span>headers</span><span>.</span><span>get</span><span>(</span><span>'X-Forwarded-For'</span><span>)</span> <span>or</span> <span>"UNKNOWN_IP"</span>
    <span># This only keeps the most recent 1 million logs for the day
</span>    <span>log_checkbox_toggle</span><span>(</span><span>forwarded_for</span><span>,</span> <span>index</span><span>,</span> <span>new_value</span><span>)</span>
    <span>publish_toggle_to_redis</span><span>(</span><span>index</span><span>,</span> <span>new_value</span><span>)</span>

<span>def</span> <span>emit_state_updates</span><span>():</span>
    <span>scheduler</span><span>.</span><span>add_job</span><span>(</span><span>emit_full_state</span><span>,</span> <span>'interval'</span><span>,</span> <span>seconds</span><span>=</span><span>30</span><span>)</span>
    <span>scheduler</span><span>.</span><span>start</span><span>()</span>

<span>emit_state_updates</span><span>()</span>

<span>def</span> <span>handle_redis_messages</span><span>():</span>
    <span>message_count</span> <span>=</span> <span>0</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>message</span> <span>=</span> <span>pubsub</span><span>.</span><span>get_message</span><span>(</span><span>timeout</span><span>=</span><span>0.01</span><span>)</span>
        <span>if</span> <span>message</span> <span>is</span> <span>None</span><span>:</span>
            <span>break</span>

        <span>if</span> <span>message</span><span>[</span><span>'type'</span><span>]</span> <span>==</span> <span>'message'</span><span>:</span>
            <span>try</span><span>:</span>
                <span>data</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>message</span><span>[</span><span>'data'</span><span>])</span>
                <span>socketio</span><span>.</span><span>emit</span><span>(</span><span>'bit_toggled'</span><span>,</span> <span>data</span><span>)</span>
                <span>message_count</span> <span>+=</span> <span>1</span>
            <span>except</span> <span>json</span><span>.</span><span>JSONDecodeError</span><span>:</span>
                <span>print</span><span>(</span><span>f</span><span>"Failed to decode message: </span><span>{</span><span>message</span><span>[</span><span>'data'</span><span>]</span><span>}</span><span>"</span><span>)</span>

        <span>if</span> <span>message_count</span> <span>&gt;=</span> <span>100</span><span>:</span>
            <span>break</span>

    <span>if</span> <span>message_count</span> <span>&gt;</span> <span>0</span><span>:</span>
        <span>print</span><span>(</span><span>f</span><span>"Processed </span><span>{</span><span>message_count</span><span>}</span><span> messages"</span><span>)</span>

<span>def</span> <span>setup_redis_listener</span><span>():</span>
    <span>scheduler</span><span>.</span><span>add_job</span><span>(</span><span>handle_redis_messages</span><span>,</span> <span>'interval'</span><span>,</span> <span>seconds</span><span>=</span><span>0.1</span><span>)</span>

<span>setup_redis_listener</span><span>()</span>
    </code></pre></figure>

</details>

<p>This code isn’t great! It’s not even async. I haven’t shipped production Python in like 8 years! But I was fine with that. I didn’t think the project would be popular. This was good enough.</p>

<p>I changed a lot of OMCB but the basic architecture - nginx reverse proxy, API workers, Redis for state and message queues - remained.</p>

<h2 id="principles-for-scaling">Principles for scaling</h2>
<p>Before I talk about what changed, let’s look at the principles I had in mind while scaling.</p>

<h4 id="bound-my-costs">Bound my costs</h4>
<p>I needed to be able to math out an upper bound on my costs. I aimed to <a href="https://x.com/jonchurch/status/1809307525524631621">let things break when they broke my expectations</a> instead of going serverless and scaling into bankruptcy.</p>

<h4 id="embrace-the-short-term">Embrace the short-term</h4>
<p>I assumed the site’s popularity was fleeting. I took on technical debt and aimed for ok solutions that I could hack out in hours over great solutions that would take me days or weeks.</p>

<h4 id="use-simple-self-hosted-tech">Use simple, self-hosted tech</h4>
<p>I’m used to running my own servers. I like to log into boxes and run commands. I tried to only add dependencies that I could run and debug on my own.</p>

<h4 id="have-fun">Have fun</h4>
<p>I optimized for fun, not money. Scaling the site my way was fun. So was saying no to advertisers.</p>

<h4 id="keep-it-global">Keep it global</h4>
<p>The magic of the site was jumping <em>anywhere</em> and seeing immediate changes. So I didn’t want to scale by, for example, sending clients a view of only the checkboxes they were looking at<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup>.</p>

<h2 id="day-1-the-pop">Day 1: the pop</h2>

<p>Within <a href="https://x.com/itseieio/status/1805994700760830066">30 minutes of launch</a>, activity looked like this:</p>


<p>The site was still up, but I knew it wouldn’t tolerate the load for much longer.</p>

<p>The most obvious improvement was more servers. Fortunately this was easy - nginx could easily reverse-proxy to Flask instances on another VM, and my state was already in Redis. I started spinning up more boxes.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/load-second-server.jpeg" alt="Load on the second OMCB server is 100%">
</p>
<p>I spun up the second server around 12:30 PM. Load immediately hit 100%</p>

<p>I originally assumed another server or two would be sufficient. Instead traffic grew as I scaled. I hit #1 on Hacker News; activity on my tweet skyrocketed. I looked for bigger optimizations.</p>

<p>My Flask servers were struggling. Redis was running out of connections (did you notice I wasn’t using a connection pool?). My best idea was to batch updates - I hacked something in that looked like this:</p>

<details>
<summary>Batching logic</summary>

<figure><pre><code data-lang="python"><span>def</span> <span>handle_Redis_messages</span><span>():</span>
    <span>message_count</span> <span>=</span> <span>0</span>
    <span>updates</span> <span>=</span> <span>[]</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>message</span> <span>=</span> <span>pubsub</span><span>.</span><span>get_message</span><span>(</span><span>timeout</span><span>=</span><span>0.01</span><span>)</span>
        <span>if</span> <span>message</span> <span>is</span> <span>None</span><span>:</span>
            <span># No more messages available
</span>            <span>break</span>

        <span>if</span> <span>message</span><span>[</span><span>'type'</span><span>]</span> <span>==</span> <span>'message'</span><span>:</span>
            <span>try</span><span>:</span>
                <span>data</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>message</span><span>[</span><span>'data'</span><span>])</span>
                <span>updates</span><span>.</span><span>append</span><span>(</span><span>data</span><span>)</span>
                <span>message_count</span> <span>+=</span> <span>1</span>
            <span>except</span> <span>json</span><span>.</span><span>JSONDecodeError</span><span>:</span>
                <span>print</span><span>(</span><span>f</span><span>"Failed to decode message: </span><span>{</span><span>message</span><span>[</span><span>'data'</span><span>]</span><span>}</span><span>"</span><span>)</span>

        <span>if</span> <span>message_count</span> <span>&gt;=</span> <span>100</span><span>:</span>
            <span>break</span>

    <span>if</span> <span>message_count</span> <span>&gt;</span> <span>0</span><span>:</span>
        <span>socketio</span><span>.</span><span>emit</span><span>(</span><span>'batched_bit_toggles'</span><span>,</span> <span>updates</span><span>)</span>
        <span>print</span><span>(</span><span>f</span><span>"Processed </span><span>{</span><span>message_count</span><span>}</span><span> messages"</span><span>)</span></code></pre></figure>

</details>

<p>I didn’t bother with backwards compatibility. I figured folks were used to the site breaking and would just refresh.</p>

<p>I also added a connection pool. This <em>definitely</em> did not play nicely with gunicorn and Flask<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup>, but it did seem to reduce the number of connections to Redis<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup>.</p>

<p>I also beefed up my Redis box - easy to do since I was using Digital Ocean’s managed Redis - from a tiny (1 shared CPU; 2 GB RAM) instance to a box with 4 dedicated CPUs and 32 GB of RAM (I did this after Redis mysteriously went down). The resizing took about 30 minutes; the server came back up.</p>

<p>And then things got trickier.</p>

<h3 id="it-was-not-a-good-night-to-have-plans">It was not a good night to have plans</h3>

<p>At around 4:30 PM I accepted it: <em>I had plans.</em> I had spent June at <a href="https://tisch.nyu.edu/itp/camp">a camp at ITP</a> - a school at NYU<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">5</a></sup>. And the night of the 26th was our final show. I had signed up to display a <a href="https://www.instagram.com/p/C8dJ_ZXP3yr/">face-controlled Pacman game</a> and invited some friends - I had to go!</p>

<p>I brought an iPad and put OMCB on it. I spun up servers while my friend <a href="https://x.com/UriBram">Uri</a> and my girlfriend Emma kindly stepped in to explain what I was doing to strangers when they came by my booth.</p>

<p>I had no automation for spinning up servers (oops) so my naming conventions evolved as I worked.</p>
<p><img src="https://eieio.games/assets/images/scaling-omcb/dropletnames.png" alt="Droplet names for OMCB. The names become increasingly terse.">
</p>
<p>My servers. I ended up with 8 worker VMs</p>

<p>I got home from the show around midnight. I was tired. But there was still more work to do, like:</p>
<ul>
  <li>Reducing the number of Flask processes on each box (I originally had more workers than the number of cores on a box; this didn’t work well)</li>
  <li>Increasing the batch size of my updates - I found that doubling the batch size substantially reduced load. I tried doubling it again. This appeared to help even more. I don’t know how to pick a principled number here.</li>
</ul>

<h3 id="bandwidth">Bandwidth</h3>
<p>I pushed the updates. I was feeling good! And then I got a text from my friend <a href="https://greg.technology/">Greg Technology</a>.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/gregtext.png" alt="A screenshot of a text message. It says 'I AM SO PROUD. R u gonna get hit w a server bill for bandwidth or is the banw not cray cray cray cray.'">
</p>
<p>gulp</p>

<p>I realized <em>I hadn’t thought hard enough about bandwidth.</em>  Digital Ocean’s bandwidth pricing is pretty sane ($0.01/GB after a pretty generous per-server compounding free allowance). I had a TB of free bandwidth from <a href="https://eieio.games/nonsense/game-12-stranger-video/">past work</a> and (pre-launch) didn’t think OMCB would make a dent.</p>

<p>I did back of the envelope math. I send state snapshots (1 million bits; 1 Mbit) every 30 seconds. With 1,000 clients that’s already 2GB a minute! Or 120GB an hour. And we’re probably gonna have more clients than that. And we haven’t even started to think about updates.</p>

<p>It was 2 AM. I was very tired. I did some bad math - maybe I confused GB/hour with GB/minute? - and  <em>freaked out</em>. I thought I was already on the hook for thousands of dollars!</p>

<p>So I did a couple of things:</p>
<ul>
  <li>Frantically texted Greg, who helped me realize that my math was way off.</li>
  <li>Ran <code>ip -s link show dev eth0</code> on my nginx box to see how many bytes I had sent, confirming that my math was way off<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">6</a></sup>.</li>
  <li>Started thinking about how to reduce bandwidth - and how to cap my costs.</li>
</ul>

<p>I immediately reduced the frequency of my state snapshots, and then (with some help from Greg) pared down the size of the incremental updates I sent to clients.</p>

<div><pre><code><span>// Original batch implementation</span>
<span>// (I just took the individual updates I was sending to clients and put them in a list)</span>
<span>[{</span> <span>"</span><span>index</span><span>"</span><span>:</span> <span>123</span><span>,</span> <span>"</span><span>value</span><span>"</span><span>:</span> <span>true</span> <span>},</span> <span>{</span> <span>"</span><span>index</span><span>"</span><span>:</span> <span>124</span><span>,</span> <span>"</span><span>value</span><span>"</span><span>:</span> <span>false</span> <span>},</span> <span>{</span><span>"</span><span>index</span><span>"</span><span>:</span> <span>125</span><span>,</span> <span>"</span><span>value</span><span>"</span><span>:</span> <span>true</span><span>}]</span>
<span>// Final implementation - array of true values, array of false values</span>
<span>[[</span><span>123</span><span>,</span> <span>125</span><span>],</span> <span>[</span><span>124</span><span>]]</span>
</code></pre></div>
<p>I moved from stuffing a bunch of dicts into a list to sending two arrays of indices with <code>true</code> and <code>false</code> implied. This was <em>five times shorter</em> than my original implementation!</p>

<p>And then I used linux’s <a href="https://en.wikipedia.org/wiki/Tc_(Linux)"><code>tc</code></a> utility to slam a hard cap on the amount of data I could send per second. <code>tc</code> is famously hard to use<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" rel="footnote">7</a></sup>, so I wrote my configuration script with Claude’s help.</p>

<details>
<summary>the tc script</summary>

<figure><pre><code data-lang="bash"><span>INTERFACE</span><span>=</span>eth0
<span>LIMIT</span><span>=</span>250mbit
tc qdisc del dev <span>$INTERFACE</span> root
tc qdisc add dev <span>$INTERFACE</span> root handle 1: htb default 10
tc class add dev <span>$INTERFACE</span> parent 1: classid 1:1 htb rate <span>$LIMIT</span>
tc class add dev <span>$INTERFACE</span> parent 1:1 classid 1:10 htb rate <span>$LIMIT</span>
tc filter add dev <span>$INTERFACE</span> protocol ip parent 1:0 prio 1 u32 match ip dst 0.0.0.0/0 flowid 1:10</code></pre></figure>

</details>

<p>This just limits traffic flowing over eth0 (my public interface) to 250Mbit a second. That’s a lot of bandwidth - ~2GB/min, or just under 3 TB a day. But it let me reason about my costs, and at $0.01/GB I knew I wouldn’t go bankrupt overnight.</p>

<p>At around 3:30 AM I got in bed.</p>

<p>My server was pegged at my 250 Mb/s limit for much of the night. I originally thought I was lucky to add limits when I did; I now realize someone probably saw <a href="https://x.com/itseieio/status/1806206039945060364">my tweet </a> about reducing bandwidth and tried to give me a huge bill.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/nginx-bandwidth-cropped.png" alt="Bandwidth usage from nginx. The server is pegged at the bandwidth lmiit for several hours">
    <img src="https://eieio.games/assets/images/scaling-omcb/bandwidth-tweet.png" alt="A tweet. It says 'doing this again, sorry! I am significantly reducing message size to keep things snappier and avoid bankrupting myself on bandwidth costs lol'">
</p>
<p>Blue is traffic from my workers to nginx, purple is nginx out to the world. The timing is suspicious</p>

<h2 id="day-2-its-still-growing">Day 2: it’s still growing</h2>

<p>I woke up a few hours later. The site was down. I hadn’t been validating input properly.</p>

<p>The site didn’t prevent folks from checking boxes above 1 million. Someone had checked boxes in the hundred million range! This let them push the count of checked boxes to 1 million, tricking the site into thinking things were over<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" rel="footnote">8</a></sup>.</p>

<p>Redis had also added millions of 0s (between bit one million and bit one hundred million), which 100x’d the data I was sending to clients.</p>

<p>This was embarrassing - I’m new to building for the web but like…I know you should validate your inputs! But it was a quick fix. I stopped nginx, copied the first million bits of my old bitset to a new truncated bitset (I wanted to keep the old one for debugging), taught my code to reference the new bitset, and added proper validation.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/redis-truncated.png" alt="The size of the old bitset and new bitset in redis. The old bitset has 12500000 bytes; the new one has 125000">
</p>
<p>much better</p>

<p>Not too bad! I brought the site back up.</p>

<h3 id="adding-a-replica-and-portscanning-my-vpc">Adding a replica and portscanning my VPC</h3>

<p>The site was <em>slow</em>. The number of checked boxes per hour quickly exceeded the day 1 peak.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/hourly-unique-players-day-2.png" alt="Unique visitors to OMCB on day 2. The site peaks with over 20,000 unique visitors in an hour.">
    <img src="https://eieio.games/assets/images/scaling-omcb/hourly-checked-boxes-day-2.png" alt="Boxes checked during OMCB's second day. Millions of boxes are being checked an hour.">
</p>

<p>The biggest problem was the initial page load. This made sense - we had to hit Redis, which was under a lot of load (and we were making too many connections to it due to bugs in my connection pooling).</p>

<p>I was tired and didn’t feel equipped to debug my connection pool issues. So I embraced the short term and spun up a Redis replica to take load off the primary and spread my connections out.</p>

<p>But there was a problem - after spinning up the replica, I couldn’t find its private IP!</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/omcb-connection-details.gif" alt="Obfuscated connection details for the OMCB redis instance. Selecting 'VPC network' gives a DNS entry with the 'private' prefix">
</p>
<p>I got my Redis instance's private IP by prepending "private-" to its DNS entry</p>

<p>To connect to my primary, I used a DNS record - there were records for its public and private IPs. Digital Ocean told me to prepend <code>replica-</code> to those records to get my replica IP. This worked for the public one, but didn’t exist for the private DNS record! And I really wanted the private IP.</p>

<p>I thought sending traffic to a public IP would risk traversing the public internet, which would mean being billed for way more bandwidth<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" rel="footnote">9</a></sup>.</p>

<p>Since I couldn’t figure out how to find the replica’s private IP in an official way (I’m sure you can! Tell me how!), I took a different approach and starting making connections to private IPs close to the IPs of my Redis primary and my other servers. This worked on the third or fourth try.</p>

<p>Then I hardcoded that IP as my replica IP!</p>

<h3 id="stabilizing">Stabilizing</h3>

<p>My Flask processes kept crashing, requiring me to babysit the site. The crashes seemed to be from running out of Redis connections. I’m wincing as I type this now, but I still didn’t want to debug what was going on there - it was late and the problem was fuzzy.</p>

<p>So I wrote a script that looked at the number of running Flask processes and bounced my systemd unit if too many were down<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" rel="footnote">10</a></sup>.</p>

<details>
    <summary>My bash restart script</summary>
    
<figure><pre><code data-lang="bash"><span>#!/bin/bash</span>

<span># We have 4 running processes. This script only restarts them if fewer than 3 are running;</span>
<span># I figured this might help us reach a stable equilibirum.</span>

<span># jitter</span>
<span>x</span><span>=</span><span>$((</span>RANDOM <span>%</span> <span>300</span><span>))</span>
<span>echo</span> <span>"sleeping for </span><span>$x</span><span>"</span>
<span>sleep</span> <span>"</span><span>$x</span><span>"</span>

count_running_servers<span>()</span> <span>{</span>
    ps aux | <span>grep </span>gunicorn | <span>grep</span> <span>-oP</span> <span>'bind \K0\.0\.0\.0:\d+'</span> | <span>sort</span> | <span>uniq</span> | <span>wc</span> <span>-l</span>
<span>}</span>

<span>running_servers</span><span>=</span><span>$(</span>count_running_servers<span>)</span>

<span>if</span> <span>[</span> <span>$running_servers</span> <span>-lt</span> 3 <span>]</span><span>;</span> <span>then
    </span><span>echo</span> <span>"Less than 3 servers running. Restarting all servers..."</span>

    <span>sudo </span>systemctl stop one-million-checkboxes.service

    <span>sleep </span>10

    <span>sudo </span>systemctl start one-million-checkboxes.service

    <span>echo</span> <span>"Servers restarted."</span>
<span>else
    </span><span>echo</span> <span>"At least 3 servers are running. No action needed."</span>
<span>fi</span></code></pre></figure>

</details>

<p>I threw that into the crontab on my boxes and updated my nginx config to briefly take servers out of rotation if they were down (I should have done this sooner!). This appeared to work pretty well. The site stabilized.</p>

<h3 id="stale-updates">Stale updates</h3>

<p>At around 12:30 AM I posted some stats on Twitter and got ready to go to bed. And then a user reported an issue:</p>
<p><img src="https://eieio.games/assets/images/scaling-omcb/ohgod.png" alt="A user reports a bug around the site changing surprisingly quickly">
</p>
<p>ahhhhhhhhhhhh</p>

<p>I had written a classic bug.</p>

<p>To keep client checkbox state synchronized, I did two things:</p>
<ul>
  <li>Sent clients incremental updates when checkboxes were checked or unchecked</li>
  <li>Sent clients occasional full-state snapshots in case they missed an update</li>
</ul>

<p>These updates didn’t have timestamps. A client could receive a new full-state snapshot and then apply an old incremental update - resulting in them having a totally wrong view of the world until the next full-state snapshot<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" rel="footnote">11</a></sup>.</p>

<p>I was embarrassed by this - I’ve written a whole lot of state machine code and know better. It was almost 1 AM and I had barely slept the night before; it was a struggle to write code that I (ironically) thought I could write in my sleep. But I:</p>
<ul>
  <li>Timestamped each full state snapshot</li>
  <li>Timestamped each update written to my Redis pubsub</li>
  <li>Added the <em>max</em> timestamp of each incremental update in the batches I sent to clients</li>
  <li>Taught clients to drop update batches if their timestamp was behind the timestamp of the last full-state snapshot</li>
</ul>

<p>This isn’t perfect (clients can apply a batch of mostly-stale updates as long as one update is new) but it’s substantially better.</p>

<p><img src="https://eieio.games/assets/images/scaling-omcb/claude-im-tired.png" alt="The author asking Claude to double check their work. They say they're very tired.">
</p>
<p>me to claude, 1 AM</p>

<p>I ran my changes by Claude before shipping to prod. Claude’s suggestions weren’t actually super helpful, but talking through why they were wrong gave me more confidence.</p>

<h2 id="rewrite-in-go">Rewrite in go</h2>

<p>I woke up the next morning and the site was still up! Hackily restarting your servers is great.  This was great timing - the site was attracting more mainstream media attention (I woke up to an email from the Washington Post).</p>

<p>I moved my attention from keeping the site up to thinking about how to wind it down. I was still confident folks wouldn’t be interested in the site forever, and I wanted to provide a real ending before <em>everyone</em> moved on<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" rel="footnote">12</a></sup>.</p>

<p>I came up with a plan - I’d make checked boxes freeze if they weren’t unchecked quickly. I wasn’t sure that my current setup could handle this - it might result in a spike of activity plus I’d be asking my servers to do more work.</p>

<p>So (after taking a break for a day) I got brunch with my friend <a href="https://github.com/EliotHedeman">Eliot</a> - a super talented performance engineer - and asked if he was down to give me a hand. He was, and from around 2 PM to 2 AM on Sunday we discussed implementations of my sunsetting plan and then rewrote the whole backend in go!</p>

<p>The go rewrite was straightforward; we ported without many changes. Lots of our sticking points were things like “finding a go <code>socketio</code> library that supports the latest version of the protocol.”</p>

<p>The speedup was staggering.</p>
<p><img src="https://eieio.games/assets/images/scaling-omcb/cropped-cpu-go-deployment.png" alt="CPU usage on a worker VM. It drops dramatically after the go deployment">
</p>
<p>CPU usage on a worker VM</p>

<p>Things were actually so much faster that we ended up needing to add better rate-limiting; originally we scaled too well and bots on the site were able to push absurd amounts of traffic through the site.</p>

<p>The site <em>was</em> DDOS’d on Sunday night, but addressing this was pretty simple - I just threw the site behind CloudFlare and updated my nginx configs a bit.</p>

<h2 id="sunsetting-the-site">Sunsetting the site</h2>

<p>The site was rock-solid after the go rewrite. I spent the next week doing interviews, enjoying the attention, and trying to relax.</p>

<p>And then I got to work on sunsetting. Checked boxes would freeze if they weren’t unchecked quickly, which would eventually leave the site totally frozen. The architecture here ended up being pretty simple - mostly some more state in Redis:</p>
<p><img src="https://eieio.games/assets/images/scaling-omcb/sunset-arch.png" alt="Architecture diagram. It shows a hashtable that maps box # to the last time it was checked, a frozen bitset that tracks which cells are disabled, and a time to freeze variable that represents how long before a box should be frozen">
</p>
<p>god i love making diagrams in tldraw</p>

<p>I added a hashtable that tracked the last time that a box was checked (this would be too much state to pass to clients, but was fine to keep in Redis), along with a “time to freeze” value. When trying to uncheck a box, we’d first check whether <code>now - last_checked &gt; time_to_freeze</code> - if it is, we don’t uncheck the box and instead update <code>frozen_bitset</code> to note that the relevant checkbox is now frozen.</p>

<p>I distributed <code>frozen_bitset</code> state to clients the same way that I distributed which boxes were checked, and taught clients to disable a checkbox if it was in the frozen bitset. And I added a job to periodically search for bits that <em>should</em> be frozen (but weren’t yet because nobody had tried to uncheck them) and freeze those.</p>

<p>Redis made it soooo easy to avoid race conditions with this implementation - I put all the relevant logic into a Lua script, meaning that it all ran atomically! Redis is great.</p>

<details>
    <summary>my lua script</summary>
    
<figure><pre><code data-lang="lua"><span>local</span> <span>bitset_key</span> <span>=</span> <span>KEYS</span><span>[</span><span>1</span><span>]</span>
<span>local</span> <span>count_key</span> <span>=</span> <span>KEYS</span><span>[</span><span>2</span><span>]</span>
<span>local</span> <span>frozen_bitset_key</span> <span>=</span> <span>KEYS</span><span>[</span><span>3</span><span>]</span>
<span>local</span> <span>frozen_count_key</span> <span>=</span> <span>KEYS</span><span>[</span><span>4</span><span>]</span>
<span>local</span> <span>freeze_time_key</span> <span>=</span> <span>KEYS</span><span>[</span><span>5</span><span>]</span>
<span>local</span> <span>index</span> <span>=</span> <span>tonumber</span><span>(</span><span>ARGV</span><span>[</span><span>1</span><span>])</span>
<span>local</span> <span>max_count</span> <span>=</span> <span>tonumber</span><span>(</span><span>ARGV</span><span>[</span><span>2</span><span>])</span>

<span>local</span> <span>UNCHECKED_SENTINEL</span> <span>=</span> <span>0</span>
<span>local</span> <span>redis_time</span> <span>=</span> <span>redis</span><span>.</span><span>call</span><span>(</span><span>'TIME'</span><span>)</span>
<span>local</span> <span>current_time</span> <span>=</span> <span>tonumber</span><span>(</span><span>redis_time</span><span>[</span><span>1</span><span>])</span> <span>*</span> <span>1000</span> <span>+</span> <span>math.floor</span><span>(</span><span>tonumber</span><span>(</span><span>redis_time</span><span>[</span><span>2</span><span>])</span> <span>/</span> <span>1000</span><span>)</span>
<span>local</span> <span>freeze_time</span> <span>=</span> <span>tonumber</span><span>(</span><span>redis</span><span>.</span><span>call</span><span>(</span><span>'get'</span><span>,</span> <span>freeze_time_key</span><span>)</span> <span>or</span> <span>"0"</span><span>)</span>

<span>local</span> <span>current_count</span> <span>=</span> <span>tonumber</span><span>(</span><span>redis</span><span>.</span><span>call</span><span>(</span><span>'get'</span><span>,</span> <span>count_key</span><span>)</span> <span>or</span> <span>"0"</span><span>)</span>
<span>local</span> <span>current_bit</span> <span>=</span> <span>redis</span><span>.</span><span>call</span><span>(</span><span>'getbit'</span><span>,</span> <span>bitset_key</span><span>,</span> <span>index</span><span>)</span>
<span>local</span> <span>frozen_bit</span> <span>=</span> <span>redis</span><span>.</span><span>call</span><span>(</span><span>'getbit'</span><span>,</span> <span>frozen_bitset_key</span><span>,</span> <span>index</span><span>)</span>

<span>if</span> <span>frozen_bit</span> <span>==</span> <span>1</span> <span>then</span>
    <span>-- Return current bit value, 0 for no change, and 0 to indicate not newly frozen</span>
    <span>return</span> <span>{</span><span>current_bit</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>}</span>
<span>end</span>

<span>if</span> <span>current_count</span> <span>&gt;=</span> <span>max_count</span> <span>then</span>
    <span>return</span> <span>{</span><span>current_bit</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>}</span>
<span>end</span>

<span>local</span> <span>new_bit</span> <span>=</span> <span>1</span> <span>-</span> <span>current_bit</span>
<span>local</span> <span>diff</span> <span>=</span> <span>new_bit</span> <span>-</span> <span>current_bit</span>

<span>-- If we're unchecking (new_bit == 0), check the freeze_time</span>
<span>if</span> <span>new_bit</span> <span>==</span> <span>0</span> <span>then</span>
    <span>local</span> <span>last_checked</span> <span>=</span> <span>tonumber</span><span>(</span><span>redis</span><span>.</span><span>call</span><span>(</span><span>'hget'</span><span>,</span> <span>'last_checked'</span><span>,</span> <span>index</span><span>)</span> <span>or</span> <span>UNCHECKED_SENTINEL</span><span>)</span>
    <span>if</span> <span>last_checked</span> <span>~=</span> <span>UNCHECKED_SENTINEL</span> <span>and</span> <span>current_time</span> <span>-</span> <span>last_checked</span> <span>&gt;=</span> <span>freeze_time</span> <span>then</span>
        <span>-- Box is frozen, update frozen bitset and count</span>
        <span>redis</span><span>.</span><span>call</span><span>(</span><span>'setbit'</span><span>,</span> <span>frozen_bitset_key</span><span>,</span> <span>index</span><span>,</span> <span>1</span><span>)</span>
        <span>redis</span><span>.</span><span>call</span><span>(</span><span>'incr'</span><span>,</span> <span>frozen_count_key</span><span>)</span>
        <span>-- Return 1 (checked), 0 for no change, and 1 to indicate newly frozen</span>
        <span>return</span> <span>{</span><span>1</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>}</span>
    <span>else</span>
        <span>redis</span><span>.</span><span>call</span><span>(</span><span>'hset'</span><span>,</span> <span>'last_checked'</span><span>,</span> <span>index</span><span>,</span> <span>UNCHECKED_SENTINEL</span><span>)</span>
    <span>end</span>
<span>else</span>
    <span>-- We're checking the box, update last_checked time</span>
    <span>redis</span><span>.</span><span>call</span><span>(</span><span>'hset'</span><span>,</span> <span>'last_checked'</span><span>,</span> <span>index</span><span>,</span> <span>current_time</span><span>)</span>
<span>end</span>

<span>redis</span><span>.</span><span>call</span><span>(</span><span>'setbit'</span><span>,</span> <span>bitset_key</span><span>,</span> <span>index</span><span>,</span> <span>new_bit</span><span>)</span>
<span>local</span> <span>new_count</span> <span>=</span> <span>current_count</span> <span>+</span> <span>diff</span>
<span>redis</span><span>.</span><span>call</span><span>(</span><span>'set'</span><span>,</span> <span>count_key</span><span>,</span> <span>new_count</span><span>)</span>

<span>-- new bit value, the change (-1, 0, or 1), and 0 to indicate not newly frozen</span>
<span>return</span> <span>{</span><span>new_bit</span><span>,</span> <span>diff</span><span>,</span> <span>0</span><span>}</span>  </code></pre></figure>

</details>

<p>I rolled the sunsetting changes 2 weeks and 1 day after I launched OMCB. Box 491915 was checked at 4:35 PM Eastern on July 11th, closing out the site.</p>

<h2 id="so-whatd-i-learn">So what’d I learn?</h2>
<p>Well, a lot. This was the second time that I’d put a server with a ‘real’ backend on the public internet, and <a href="https://eieio.games/nonsense/game-12-stranger-video/">the last one</a> barely counted. Learning in a high-intensity but low-stakes environment is great<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" rel="footnote">13</a></sup>.</p>

<p>Building the site in two days with little regard for scale was a good choice. It’s so hard to know what will do well on the internet - nobody I explained the site to seemed that excited about it - and I doubt I would have launched at all if I spent weeks thinking about scale. Having a bunch of eyes on the site energized me to keep it up and helped me focus on what mattered.</p>

<p>I’m happy with the tech that I used. Redis and nginx are incredible. Running things myself made debugging and fixing things so much easier (it was a little painful to not have full control of my Redis instance). The site cost me something like $850 to run - <a href="https://buymeacoffee.com/eieio">donations</a> came pretty close to that, so I’m not in the hole for too much.</p>

<p>In the future I might spend some time trying to decode the pricing pages for <a href="https://www.convex.dev/pricing">convex</a> or <a href="https://developers.cloudflare.com/durable-objects/platform/pricing/">durable objects</a>. But I’m proud of scaling it on my own terms, and I think it’s worth noting that things worked out ok.</p>

<p>This also validated my belief that people are hungry for constrained anonymous interactions with strangers. I love building sites like this (and I was gonna build more regardless!) but I’m more confident than ever that it’s a good idea.</p>

<h2 id="wrapping-up">Wrapping up</h2>

<p>This was an absolute blast.</p>

<p>I’ve got one more story to tell about the site. It’s about teens doing cool things. This blog is too long to tell it right now, so stay tuned (I’ll update my <a href="https://eieio.substack.com/">newsletter</a>, <a href="https://twitter.com/itseieio">twitter</a>, and <a href="https://eieio.games/whats-my-deal/">various other platforms</a>) when it’s live.</p>

<p>And in the meantime - build more stupid websites! The internet can still be fun :)</p>



    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[France high-speed rail traffic disrupted by 'malicious acts' on Olympic ceremony (169 pts)]]></title>
            <link>https://www.lemonde.fr/en/france/article/2024/07/26/france-s-high-speed-trains-struck-by-malicious-acts_6699471_7.html</link>
            <guid>41076831</guid>
            <pubDate>Fri, 26 Jul 2024 08:23:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lemonde.fr/en/france/article/2024/07/26/france-s-high-speed-trains-struck-by-malicious-acts_6699471_7.html">https://www.lemonde.fr/en/france/article/2024/07/26/france-s-high-speed-trains-struck-by-malicious-acts_6699471_7.html</a>, See on <a href="https://news.ycombinator.com/item?id=41076831">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="habillagepub">            <header> <div>     <ul>       <li>  <a href="https://www.lemonde.fr/en/france/">  France </a>  </li>  <li> <a href="https://www.lemonde.fr/en/transport/"> Transport </a> </li>    </ul>     <div>   <p>    The TGV high-speed network has been disrupted by acts including arson attacks, affecting some 800,000 passengers and causing delays ahead of the Paris 2024 opening ceremony. Eurostar trains between Paris and London were also affected. </p>  </div>            </div> </header>       <section> <article>                        <figure> <picture> <source srcset=" https://img.lemde.fr/2024/07/26/0/0/5420/3613/556/0/75/0/3ff6f39_5176049-01-06.jpg 556w, https://img.lemde.fr/2024/07/26/0/0/5420/3613/600/0/75/0/3ff6f39_5176049-01-06.jpg 600w, https://img.lemde.fr/2024/07/26/0/0/5420/3613/664/0/75/0/3ff6f39_5176049-01-06.jpg 664w, https://img.lemde.fr/2024/07/26/0/0/5420/3613/700/0/75/0/3ff6f39_5176049-01-06.jpg 700w, https://img.lemde.fr/2024/07/26/0/0/5420/3613/800/0/75/0/3ff6f39_5176049-01-06.jpg 800w" sizes="(min-width: 1024px) 556px, 100vw" alt="A French gendarme walks on a deserted platform near a high-speed TGV train at the Gare Montparnasse train station in Paris on July 26, 2024." width="664" height="443"> <img src="https://img.lemde.fr/2024/07/26/0/0/5420/3613/664/0/75/0/3ff6f39_5176049-01-06.jpg" alt="A French gendarme walks on a deserted platform near a high-speed TGV train at the Gare Montparnasse train station in Paris on July 26, 2024." sizes="(min-width: 1024px) 556px, 100vw" width="664" height="443"> </picture>     </figure>             <p>France's high-speed rail network was hit by "malicious acts" including arson attacks that have disrupted the transport system, national rail operator SNCF said on Friday, July 26, hours before the opening ceremony of the Paris Olympics.</p>            <p>"This is a massive attack on a large scale to paralyze the TGV network," SNCF told Agence France-Presse (AFP), adding that many routes will have to be canceled and the situation would last "at least all weekend while repairs are conducted."</p>                 <p>"SNCF was the victim of several simultaneous malicious acts overnight," the national train operator said, adding that the attacks affected its Atlantic, northern and eastern lines, at a time of particularly heavy traffic for summer holiday travel. The southeastern line was not affected as "a malicious act was foiled." "Arson attacks were started to damage our facilities," it said, adding that traffic on the affected lines was "heavily disrupted."</p>                         <p>SNCF chief executive Jean-Pierre Farandou said that the attackers had started fires in "conduits carrying multiple (fibre-optic) cables" that carry "safety information for drivers" or control the motors for points. "There's a huge number of bundled cables. We have to repair them one by one, it's a manual operation" requiring "hundreds of workers," he added.</p>            <p>French officials described the attacks on France's high-speed rail network as “criminal actions" and said they were investigating whether they were linked to the Olympic Games.</p>                 <p>The Paris prosecutor's office has opened an investigation into the attacks on the rail network. It said it was investigating into "the deterioration of property that threatens the fundamental interests of the nation." This crime, it added, carries a potential 15-year sentence and fines of €225,000. Further, it said crimes involving "degradation and attempted degradation by dangerous means in an organized group" can carry a 20-year prison sentence and fines of €150,000.</p>            <h2>800,000 passengers affected</h2>            <p>Trains were being diverted to different tracks "but we will have to cancel a large number of them," the statement said. The SNCF has urged passengers to postpone their trips and stay away from train stations. Farandou added that 800,000 passengers were affected.</p>                     <figure> <picture> <source srcset=" https://img.lemde.fr/2024/07/26/0/0/5500/3667/556/0/75/0/7392297_2024-07-26t094212z-164929915-rc2x29ads51v-rtrmadp-3-olympics-2024-railways-vandalism.JPG 556w, https://img.lemde.fr/2024/07/26/0/0/5500/3667/600/0/75/0/7392297_2024-07-26t094212z-164929915-rc2x29ads51v-rtrmadp-3-olympics-2024-railways-vandalism.JPG 600w, https://img.lemde.fr/2024/07/26/0/0/5500/3667/664/0/75/0/7392297_2024-07-26t094212z-164929915-rc2x29ads51v-rtrmadp-3-olympics-2024-railways-vandalism.JPG 664w, https://img.lemde.fr/2024/07/26/0/0/5500/3667/700/0/75/0/7392297_2024-07-26t094212z-164929915-rc2x29ads51v-rtrmadp-3-olympics-2024-railways-vandalism.JPG 700w, https://img.lemde.fr/2024/07/26/0/0/5500/3667/800/0/75/0/7392297_2024-07-26t094212z-164929915-rc2x29ads51v-rtrmadp-3-olympics-2024-railways-vandalism.JPG 800w" sizes="(min-width: 1024px) 556px, 100vw" alt="SNCF railway company workers and police officers work at a site where unknown actors targeted France's high-speed train network, ahead of the Paris 2024 Olympics opening ceremony, in Croisilles, northern France, July 26, 2024." width="664" height="443"> <img src="https://img.lemde.fr/2024/07/26/0/0/5500/3667/664/0/75/0/7392297_2024-07-26t094212z-164929915-rc2x29ads51v-rtrmadp-3-olympics-2024-railways-vandalism.JPG" alt="SNCF railway company workers and police officers work at a site where unknown actors targeted France's high-speed train network, ahead of the Paris 2024 Olympics opening ceremony, in Croisilles, northern France, July 26, 2024." sizes="(min-width: 1024px) 556px, 100vw" width="664" height="443"> </picture>     </figure>                      <figure> <picture> <source srcset=" https://img.lemde.fr/2024/07/26/0/0/5440/3626/556/0/75/0/f7db50f_5175588-01-06.jpg 556w, https://img.lemde.fr/2024/07/26/0/0/5440/3626/600/0/75/0/f7db50f_5175588-01-06.jpg 600w, https://img.lemde.fr/2024/07/26/0/0/5440/3626/664/0/75/0/f7db50f_5175588-01-06.jpg 664w, https://img.lemde.fr/2024/07/26/0/0/5440/3626/700/0/75/0/f7db50f_5175588-01-06.jpg 700w, https://img.lemde.fr/2024/07/26/0/0/5440/3626/800/0/75/0/f7db50f_5175588-01-06.jpg 800w" sizes="(min-width: 1024px) 556px, 100vw" alt="Passengers gather around the departure boards at the Gare Montparnasse train station, in Paris, on July 26, 2024." width="664" height="443"> <img src="https://img.lemde.fr/2024/07/26/0/0/5440/3626/664/0/75/0/f7db50f_5175588-01-06.jpg" alt="Passengers gather around the departure boards at the Gare Montparnasse train station, in Paris, on July 26, 2024." sizes="(min-width: 1024px) 556px, 100vw" width="664" height="443"> </picture>     </figure>             <p>SNCF has announced that there will be no trains at Paris' Gare Montparnasse station, which serves the west of the country, until 1 pm, with some delays and a few cancellations. "All customers affected will receive a 100% refund on all train tickets," it added.</p>       <section> <p> Partner service </p> <a href="https://learn-french.lemonde.fr/?rfextension=LME" target="_blank"> <p>Learn French with Gymglish</p> <p>Thanks to a daily lesson, an original story and a personalized correction, in 15 minutes per day.</p> <p>Try for free</p> </a> </section>        <p>At Montparnasse, passengers were waiting for more information about their trips, with display boards showing delays of more than two hours. "Normal traffic is expected to resume on Monday, July 29," read one of the signs in the departure hall. The station's loudspeakers told passengers that conditions to exchange and refund tickets would be more flexible.</p>            <p>Passenger services chief Christophe Fanichet said there were delays of 90 minutes to two hours on services between Paris and France's north and east. The company also stated that all travelers would be "progressively notified by SMS and email over the next few hours." "We ask you not to come to the station; if we haven't warned you, your train isn't running," said Fanichet.</p>            <h2>Eurostar trains delayed</h2>            <p>Several Eurostar trains between Paris and London were also canceled Friday, the company said. "All high speed trains going to and coming from Paris are being diverted via the classic line today," Eurostar said on its <a href="https://www.eurostar.com/rw-en/travel-info/travel-updates/18844" target="_blank" rel="noopener" title="Nouvelle fenêtre">website</a>, meaning they will take an extra 90 minutes on a route that usually takes around two hours and 20 minutes. Eurostar also told AFP the line between Paris and Brussels was disrupted.</p>                 <p>Around 25% of Eurostar trains between Paris and London were canceled on Friday, the company said. "This will also be the case on Saturday 27 and Sunday 28" July, Eurostar said in a statement. The company expects the disruption to be cleared by Monday morning.</p>                     <figure> <picture> <source srcset=" https://img.lemde.fr/2024/07/26/0/0/5568/3712/556/0/75/0/be7b952_5176541-01-06.jpg 556w, https://img.lemde.fr/2024/07/26/0/0/5568/3712/600/0/75/0/be7b952_5176541-01-06.jpg 600w, https://img.lemde.fr/2024/07/26/0/0/5568/3712/664/0/75/0/be7b952_5176541-01-06.jpg 664w, https://img.lemde.fr/2024/07/26/0/0/5568/3712/700/0/75/0/be7b952_5176541-01-06.jpg 700w, https://img.lemde.fr/2024/07/26/0/0/5568/3712/800/0/75/0/be7b952_5176541-01-06.jpg 800w" sizes="(min-width: 1024px) 556px, 100vw" alt="Passengers wait for their train departures at the Bordeaux-Saint-Jean train station in Bordeaux, western France on July 26, 2024." width="664" height="443"> <img src="https://img.lemde.fr/2024/07/26/0/0/5568/3712/664/0/75/0/be7b952_5176541-01-06.jpg" alt="Passengers wait for their train departures at the Bordeaux-Saint-Jean train station in Bordeaux, western France on July 26, 2024." sizes="(min-width: 1024px) 556px, 100vw" width="664" height="443"> </picture>     </figure>             <p>"Early this morning, coordinated and prepared acts of sabotage were perpetrated against installations of the SNCF," Prime Minister Gabriel Attal said. "There are huge and serious consequences for the rail network," he added, while security services are hunting the culprits. In a later statement, Attal said the sabotage acts had "a clear objective: blocking the high-speed train network."</p>            <p>The "massive attack" against the high-speed rail network was an "outrageous criminal act," Transport Minister Patrice Vergriete said Friday. There would be "very serious consequences" for rail traffic throughout the weekend with connections towards northern, eastern and northwestern France halved, Vergriete said.</p>                 <p>The attacks were launched as Paris prepares for the Olympic Games opening ceremony, with 7,500 athletes, 300,000 spectators and an audience of VIPs. International Olympic Committee president Thomas Bach said he had "full confidence" in the French authorities despite the sabotaging of the train system hours before the Paris Games opening ceremony. "I don't have concerns," the German told the media at the Olympic Athletes' Village.</p>            <p>The sabotage on the rail network will have "no impact on the ceremony," Paris Mayor Anne Hidalgo said.</p>            <p>But British Prime Minister Keir Starmer changed his travel plans to get to the Olympics on Friday after Eurostar trains were disrupted, Downing Street said. Starmer was meant to be traveling on the cross-Channel rail service from London to Paris for the Olympics opening ceremony. A  spokeswoman said he flew instead due to the delays and cancellations.</p>                 <p>However, two German athletes in showjumping, who were on a train to Paris to take part in the opening ceremony, had to turn back in Belgium because of lengthy delays. They will now miss the ceremony, German news agency dpa reported. "It’s a real shame but we would have arrived too late," rider Philipp Weishaupt, who was traveling with teammate Christian Kukuk, told dpa. "There was no longer a chance of making it on time."</p>                                              <section>  <p>  <span>Le Monde with AP and AFP</span>  </p>   </section>    <section>     <a href="https://www.lemonde.fr/en/syndication/" title="Reuse this content" target="_blank">Reuse this content</a>   </section>   </article>   </section>                     </section><section id="js-capping-old-article" data-full="0" data-mini="0"> <section id="js-capping-old-article-header"> <span></span> <p>Lecture restreinte</p> </section> <section id="js-capping-old-article-content"> <p>Votre abonnement n’autorise pas la lecture de cet article</p>  <p>Pour plus d’informations, merci de contacter notre service commercial.</p> </section> </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A hash table by any other name (105 pts)]]></title>
            <link>https://lwn.net/Articles/972580/</link>
            <guid>41076375</guid>
            <pubDate>Fri, 26 Jul 2024 06:53:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/972580/">https://lwn.net/Articles/972580/</a>, See on <a href="https://news.ycombinator.com/item?id=41076375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<b>LWN.net needs you!</b>
<p>
Without subscribers, LWN would simply not exist.  Please consider
       <a href="https://lwn.net/subscribe/">signing up for a subscription</a> and helping
       to keep LWN publishing.
</p></blockquote>

<p>
On June 25, Matthew Wilcox posted
<a href="https://lwn.net/ml/all/20240625211803.2750563-1-willy@infradead.org/">
a second version of a patch set</a>
introducing a new
data structure called rosebush, which
"<q>is a resizing, scalable, cache-aware, RCU optimised hash
table.</q>" The kernel already has generic hash tables, though, including
<a href="https://lwn.net/Articles/751374/">rhashtable</a>. Wilcox believes that the design of
rhashtable is not the best choice for performance, and has written rosebush as
an alternative for use in the
<a href="https://www.kernel.org/doc/html/latest/filesystems/vfs.html#directory-entry-cache-dcache">
directory-entry cache</a> (dcache) — the filesystem cache used to speed up
file-name lookup.
</p>

<p>
Rosebush is intended to present roughly the same API as rhashtable, with the
main difference for users being the performance of the two hash tables —
something which is critically important to the dcache, since it is referenced
during almost all filesystem operations.
All hash tables have the same basic structure,
but the details can be quite important for performance.
The key of the value being looked up (a file name, for example)
is hashed, which is used to select a bucket
from a top-level array of buckets. Multiple keys can have the same hash,
however, so the hash table must either store multiple keys per bucket (the
approach taken by rosebush and rhashtable), or use a more complicated
bucket-selection algorithm (such as linear probing or cuckoo hashing).
Rosebush uses
arrays as fixed-size buckets, whereas rhashtable instead uses linked lists.
While there are other differences between the two data structures, it is the
avoidance of linked lists that Wilcox thinks will make the biggest difference.
</p>

<p>
To look up a key in a rosebush, the key is hashed. In the normal
case, the rosebush has a two-level structure; the hash is first used to select a
bucket from an array of buckets, and then looked up within that bucket using a
linear scan. The array has a number of elements that is a power of two, so the
last <tt>N</tt> bits of the hash can be used to efficiently select an entry.
In order to adjust to the number of items stored in the rosebush,
the top-level array is resizeable. This does mean that the occasional insertion
into the table will need to allocate a new top-level array and rehash the keys
into new buckets, but the <em>average</em> insertion only has to do a constant
amount of work. If few enough items have been added to the
rosebush (the exact number depends on the architecture), the rosebush will just
skip the top-level array and fall back to using a single bucket. This saves some
memory for rosebushes that are not actually being used.
Buckets are stored with the
hashes first, followed by the associated values. This ensures fewer cache lines
need to be fetched to look up an element in a bucket. Finally, the value
associated with the key is returned.
</p>

<p>
In order to efficiently support concurrent access, rosebush uses
read-copy-update (RCU) to protect each bucket.
There is a whole-hash-table spinlock that is
taken during resizing operations, but the vast majority of accesses should only
need to use RCU on the specific bucket being read from or updated. Since
reading under RCU protection is approximately free, this means that the hash table
has excellent concurrent read performance — a necessity for the dcache.
</p>

<p>
The lists rhashtable uses are
<a href="https://www.data-structures-in-practice.com/intrusive-linked-lists/">
intrusive, singly-linked lists</a>.
The kernel makes extensive use of embedded linked list structures.
Intrusive pointers do have downsides, however. For one thing, a structure with
an embedded linked list entry in it cannot be stored in multiple lists. It also
uses extra memory when the structure is not being stored in a list.
The fact that rosebush does not use intrusive pointers allows
an item to be stored in multiple rosebushes, or
the same rosebush under different keys; the benefit is a side effect of the
performance-motivated design decision to avoid linked lists.
</p>

<!-- middle-ad -->

<p>
The main problem with linked lists in this context is pointer-chasing:
linked lists require the CPU to access several
potentially unrelated parts of memory in order to iterate through the bucket.
Rosebush avoids this by using an array to hold bucket entries,
reducing the number of cache misses incurred by iterating over a bucket. Wilcox
explained the motivation in his cover letter:
</p>

<blockquote>
Where I expect rosebush to shine is on dependent cache misses.
I've assumed an average chain length of 10 for rhashtable in the above
memory calculations.  That means on average a lookup would take five cache
misses that can't be speculated.  Rosebush does a linear walk of 4-byte
hashes looking for matches, so the CPU can usefully speculate the entire
array of hash values (indeed, we tell it exactly how many we're going to
look at) and then take a single cache miss fetching the correct pointer.
Add that to the cache miss to fetch the bucket and that's just two cache
misses rather than five.
</blockquote>

<p>
In response to
<a href="https://lwn.net/ml/linux-kernel/20240222203726.1101861-1-willy@infradead.org/">
the first version</a> of the patch set (from February),
Herbert Xu
<a href="https://lwn.net/ml/linux-kernel/Zdk2YgIoAGOEvcJi@gondor.apana.org.au/">
disagreed</a>, pointing out that an rhashtable is resized
whenever the table reaches 75% capacity, so the average bucket contains only one
item. Therefore uses of rhashtable will incur many fewer cache misses than
Wilcox had predicted.
</p>

<p>
David Laight
<a href="https://lwn.net/ml/linux-kernel/4a1416fcb3c547eb9612ce07da6a77ed@AcuMS.aculab.com/">
thought</a> that was considering the wrong metric, however. Since the kernel
never has any reason to look up an empty bucket, it is more interesting to
consider the average length of non-empty buckets. Xu
<a href="https://lwn.net/ml/linux-kernel/ZdqO3G6Fb4wYhVEj@gondor.apana.org.au/">
acknowledged</a> the point, but didn't think it changed the ultimate analysis,
since the average chain length should still be nowhere near 10 items.
</p>

<p>
Reviewers were, as ever, skeptical in the face of assertions about performance
without corresponding measurements. Peng Zhang
<a href="https://lwn.net/ml/linux-kernel/9c0aad2c-548a-4287-b3d5-c7932f40c96f@bytedance.com/">
asked</a> "<q>how
much performance improvement is expected if it is applied to dcache?</q>"
Wilcox didn't reply to Zhang's question.
The other reviewers did seem fairly happy with the second version of the patchset,
mainly suggesting small changes to the documentation and test suite.
</p><br clear="all"><hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bcachefs, an Introduction/Exploration (146 pts)]]></title>
            <link>http://blog.asleson.org/2024/07/24/bcachefs-an-introduction/exploration/</link>
            <guid>41076190</guid>
            <pubDate>Fri, 26 Jul 2024 06:14:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://blog.asleson.org/2024/07/24/bcachefs-an-introduction/exploration/">http://blog.asleson.org/2024/07/24/bcachefs-an-introduction/exploration/</a>, See on <a href="https://news.ycombinator.com/item?id=41076190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
    <article>
      
      
      
      
      <hr>
      <ul>
        <li><time>July 24, 2024</time></li>
        <li>
          <a href="http://blog.asleson.org/tags/fedora/">
            <i></i>
            Fedora
          </a>&nbsp;
        </li>
      </ul>
      

      <h2 id="introduction--background-information">Introduction &amp; background information</h2>
<p><em><strong>NOTE: This content is from an internal talk I gave, thus the reason it may read like a presentation</strong></em></p>
<h3 id="so-what-is-bcachefs">So what is bcachefs?</h3>
<p>bcachefs is a next-generation copy-on-write (COW) filesystem (FS) that aims to provide features similar to Btrfs and ZFS, written by Kent Overstreet</p>
<ul>
<li>Copy-on-write (COW), with goal of performance being better than other COW filesystems</li>
<li>Full checksums on everything</li>
<li>Mult-device, replication , RAID, Caching, Compression, Encryption</li>
<li>Sub-volumes, snapshots</li>
<li>Scalable, 50+ TB tested</li>
</ul>
<h3 id="why-the-need-for-another-fs">Why the need for another FS?</h3>
<p>According to Kent<a href="https://www.patreon.com/bcachefs/about?l=en">[1]</a>, paraphrased here</p>
<ul>
<li>ext4, functional but outdated, intimidates developers due to its codebase. Despite its flaws, it
surprisingly functions, with its fsck being its standout feature.</li>
<li>XFS, reliable and robust, follows a traditional update-in-place design, not COW. The developers,
especially Dave Chinner, impress with the code’s rigor, surpassing other filesystems. However,
lacking COW, XFS can’t support some desirable features.</li>
<li>Btrfs[2], envisioned as Linux’s next-gen COW filesystem akin to ZFS, suffered from rushed development,
resulting in numerous design flaws and a sprawling codebase larger than XFS. Its prolonged stabilization process
tarnished its reputation, leading to distrust among users and prompting migrations to other FS.</li>
<li>ZFS, pioneering COW filesystem, won’t be a first class citizen on Linux (copyright).
While commendable, its block-based design diverges from modern extent-based systems due to complexities in
implementing extents with snapshots.</li>
</ul>
<ol>
<li><a href="https://www.patreon.com/bcachefs/about?l=en">https://www.patreon.com/bcachefs/about?l=en</a></li>
<li>Current Fedora workstation default filesystem</li>
</ol>
<h3 id="why-bcachefs1httpswwwpatreoncombcachefsaboutlen">Why bcachefs<a href="https://www.patreon.com/bcachefs/about?l=en">[1]</a></h3>
<ul>
<li>Stability: Key in filesystem development. Building a POSIX filesystem involves tackling a vast array of functionalities, making incremental development challenging. However, bcachefs benefits from bcache’s stable foundation, extensive testing.</li>
<li>Fast: bcachefs excels in speed, with noticeable performance improvements since earlier benchmarks. It prioritizes low tail latency, avoiding extensive delays seen in other filesystems like ext4.</li>
<li>Small: bcachefs boasts a compact codebase, rivaling ext4’s size yet offering btrfs features. Constant refactorings ensure clarity, aiming for longevity by prioritizing developer accessibility and comprehension.</li>
<li>Features: It has, either finished or in progress, features we all need, especially data checksumming, which alone is sufficient reason for a good COW FS.</li>
</ul>
<ol>
<li>Paraphrased <a href="https://www.patreon.com/bcachefs/about?l=en">https://www.patreon.com/bcachefs/about?l=en</a></li>
</ol>
<h3 id="code-size-comparison">Code size comparison</h3>
<h4 id="lines-of-kernel-code-tokei-utility">lines of kernel code (tokei utility)</h4>
<p>Is bcachefs really that small?  It’s actually pretty good, but some features are not fully complete.
</p><figure><img src="http://blog.asleson.org/pictures/code-size-comparison.svg">
</figure>

<h3 id="feature-comparison">Feature Comparison</h3>
<h4 id="fs-residing-directly-on-hardware">(FS residing directly on hardware)</h4>
<table>
<thead>
<tr>
<th>FS</th>
<th>RAID</th>
<th>Encryption</th>
<th>Thin provision</th>
<th>De-dupe</th>
<th>Caching</th>
<th>Compression</th>
<th>Snapshots</th>
<th>Subvolumes</th>
<th>Send/Receive</th>
<th>Full checksum</th>
<th>Reflink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bcachefs</td>
<td>Y</td>
<td>Y</td>
<td>N</td>
<td>N</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>P</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>ZFS</td>
<td>Y</td>
<td>Y</td>
<td>Y(sparse volumes)</td>
<td>N</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>btrfs</td>
<td>Y</td>
<td>Y</td>
<td>N</td>
<td>N</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>XFS</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>Y (if newer)</td>
</tr>
<tr>
<td>ext3/4</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
</tr>
<tr>
<td>Most non-COW</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>N</td>
<td>Varies</td>
</tr>
</tbody>
</table>
<p><strong>P</strong> = Planned</p>
<h3 id="feature-comparison-layered">Feature Comparison (layered)</h3>
<table>
<thead>
<tr>
<th>FS</th>
<th>RAID</th>
<th>Encryption</th>
<th>Thin provision</th>
<th>De-dupe</th>
<th>Caching</th>
<th>Compression</th>
<th>Snapshots</th>
<th>Subvolumes</th>
<th>Send/Receive</th>
<th>Full checksum</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bcachefs</td>
<td>Y</td>
<td>Y-native</td>
<td>N</td>
<td>N</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>P</td>
<td>Y</td>
</tr>
<tr>
<td>ZFS</td>
<td>Y</td>
<td>Y-native</td>
<td>Y(sparse volumes)</td>
<td>?</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>btrfs</td>
<td>Y</td>
<td>Y-native</td>
<td>L</td>
<td>L</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>Stratis</td>
<td>P</td>
<td>Y(dm-crypt)</td>
<td>Y(dm-thin)</td>
<td>P(vdo)</td>
<td>Y</td>
<td>P(vdo)</td>
<td>Y(dm)</td>
<td>N</td>
<td>N</td>
<td>N(dm-integrity)</td>
</tr>
<tr>
<td>non-COW using LVM</td>
<td>Y</td>
<td>Y(dm-crypt)</td>
<td>Y(dm-thin)</td>
<td>Y(vdo)</td>
<td>Y</td>
<td>Y(vdo)</td>
<td>Y(LVM)</td>
<td>N</td>
<td>P(blk-archive)</td>
<td>N(dm-integrity)</td>
</tr>
</tbody>
</table>
<p><strong>P</strong> = Planned
<strong>L</strong> = layer on LVM/VDO
<strong>?</strong> = Should work layered on VDO, untested</p>
<h3 id="full-checksums">Full checksums</h3>
<ul>
<li>You can detect and optionally correct data before it gets returned to userspace
<ul>
<li>dm-integrity only protects blocks</li>
<li>FS checksum protects 1-2 additional layers depending on stacking</li>
<li>FS checksum doesn’t protect networked FS</li>
</ul>
</li>
<li>Bit-rot is real, I have personal experience with it, an excerpt from an email (circa 2016) on the subject
<pre tabindex="0"><code>...
My home file server (ECC memory, SATA disks) runs CentOS6 and uses md
for a simple mirrored setup (ext4) with off-site backups run each night
to crashplan for newly created files (they do file versioning).  A while
back I read an article on bit rot and I ran the jpeginfo tool to check
my family photos that go back ~16 years.  I found ~100 images that are
corrupt out of ~50k.  I don't know when or how they got trashed, but
they are forever lost at this point (backups and all versions of backups are corrupt).
I have no idea what other files may be trashed as many of them don't have built in
file level checksums or tools to automate checking.  I would like to scrub
my data once a month, to verify correctness and address any issues that come up.
...
</code></pre></li>
<li>Disclaimer, my personal data is stored on ZFS, 10T mirror with monthly scrubs (ECC memory, enterprise SATA disks)</li>
<li>I would welcome a COW FS which is in kernel tree that has full checksums, error detection and correction, and won’t eat my data</li>
</ul>
<h2 id="general-observations">General observations</h2>
<h3 id="how-helpful-is-help">How helpful is help?</h3>
<h4 id="-help--man-pages">(-help &amp; man pages)</h4>
<ul>
<li>In general, documentation details are a bit lacking
<ul>
<li>Example: What does this all mean?
<ul>
<li>metadata_replicas:               2</li>
<li>data_replicas:                   2</li>
<li>metadata_replicas_required:      1</li>
<li>data_replicas_required:          1</li>
</ul>
</li>
</ul>
</li>
<li>Much of what is scattered around the internet appears to be incorrect
<ul>
<li>Current understanding in a number of online resources about *<strong>_required</strong>
<ul>
<li>The <strong>_required</strong> versions means sync() won’t return until that many replicas are written; the others are the targets for the rebalance thread.</li>
<li>But actual IRC message from Kent is: <strong>can run in degraded mode if we’re only able to do this many writes</strong> (translation, the FS stays operational in RW mode)</li>
</ul>
</li>
</ul>
</li>
<li>There are good details in design documentation on implementation</li>
</ul>
<h3 id="what-are-user-space-errors-like">What are user space errors like?</h3>
<ul>
<li>Some could be better, an example FS mount
<pre tabindex="0"><code># bcachefs mount /dev/sdb:/dev/sdc /mnt/bcachefs
ERROR - bcachefs::commands::cmd_mount: Fatal error: Input/output error
</code></pre></li>
<li>If you get something cryptic, check the journal, you may find more information
<pre tabindex="0"><code> kernel: bcachefs (8a4283ce-0388-44ac-99d6-d5849da5ce6a): mounting version 1.3: rebalance_work opts=metadata_replicas=2,data_replicas
 kernel: bcachefs (8a4283ce-0388-44ac-99d6-d5849da5ce6a): recovering from clean shutdown, journal seq 18
 kernel: bcachefs (8a4283ce-0388-44ac-99d6-d5849da5ce6a): alloc_read... done
 kernel: bcachefs (8a4283ce-0388-44ac-99d6-d5849da5ce6a): stripes_read... done
 kernel: bcachefs (8a4283ce-0388-44ac-99d6-d5849da5ce6a): snapshots_read... done
 kernel: bucket 1:0 gen 0 different types of data in same bucket: need_discard, sb
 kernel: while marking sb, exiting
 kernel: bcachefs (8a4283ce-0388-44ac-99d6-d5849da5ce6a): Unable to continue, halting
 kernel: bcachefs (8a4283ce-0388-44ac-99d6-d5849da5ce6a): bch2_trans_mark_dev_sb(): error EIO
 kernel: bcachefs (8a4283ce-0388-44ac-99d6-d5849da5ce6a): bch2_fs_recovery(): error EIO
 kernel: bcachefs (8a4283ce-0388-44ac-99d6-d5849da5ce6a): bch2_fs_start(): error starting filesystem EIO
</code></pre></li>
</ul>
<h2 id="some-use-cases-and-common-questions">Some use cases and common questions</h2>
<h3 id="create-fs-mount-at-boot">Create FS, mount at boot</h3>
<ul>
<li>Single device with FS works with entry in fstab by FS uuid</li>
<li>Multi-device with FS works with entry in fstab by FS uuid with PR I submitted and was merged (thanks Kent!)</li>
<li>There is still an issue where if all the block devices are not present at boot, the mount will fail even though in some cases you can bring the FS up with a subset of the disks in degraded mode, solutions to this are still in progress
<ul>
<li><a href="https://gist.github.com/holmanb/ebf26121194b787afeb713b3d95ce83e#support-multiple-device-degraded-mount-with-systemd">https://gist.github.com/holmanb/ebf26121194b787afeb713b3d95ce83e#support-multiple-device-degraded-mount-with-systemd</a></li>
<li>Investigation on how btrfs accomplishes this would be good</li>
</ul>
</li>
</ul>
<h3 id="mount-option-for-degraded">Mount option for degraded</h3>
<ul>
<li>You can do a standard mount, which won’t mount if your FS is degraded
<ul>
<li><code>mount /dev/sda /mnt</code></li>
</ul>
</li>
<li>Mount in degraded mode, but your data is fully intact
<ul>
<li><code>mount -o degraded /dev/sda /mnt</code></li>
</ul>
</li>
<li>Mount in very degraded mode, where data is likely missing, and you are trying to recover as much as you can
<ul>
<li><code>mount -o very_degraded /dev/sda /mnt</code></li>
</ul>
</li>
</ul>
<h3 id="automation">Automation</h3>
<ul>
<li>Currently much of the implementation is available in user space, you can even run bcachefs as FUSE (not tested),
however there currently isn’t a high level API which provides functionality
<ul>
<li>When asked about adding something like this as a rust crate, Kent responded, <strong>“a rust API would be wonderful”</strong></li>
</ul>
</li>
<li>The bcachefs command line is a combination of rust and C (lower levels)</li>
<li>IOCTL interface</li>
<li>sysfs, debugfs interfaces</li>
<li>Today, automation would mainly revolve around fork &amp; exec command line tool or sysfs interface</li>
</ul>
<h3 id="sysfs-interface">sysfs interface</h3>
<ul>
<li>Many options can be changed after a FS has been formatted.  These configuration options are available in
sysfs and will be persistently changed in the superblock, see: <code>/sys/fs/bcachefs/UUID/options/</code></li>
<li>Time statistics, tracks latency and frequency of various things:</li>
<li>Other values for
<ul>
<li>Internals/data structure values</li>
<li>Tests, unit and performance of low level btree operations (optionally compiled in)</li>
</ul>
</li>
<li>From an automation standpoint, it might be easier to use some of these instead of forking &amp; execing the command line to make simple FS changes after it has been created</li>
</ul>
<h3 id="debugfs-interface">debugfs interface</h3>
<p><code>/sys/kernel/debug/bcachefs/&lt;FS UUID&gt;</code></p>
<p>Many things under this directory, I haven’t found complete documentation for them yet, would likely need to examine code.</p>
<h3 id="information-about-a-fs-root-privs-not-needed">Information about a FS (root privs. not needed)</h3>
<div><pre tabindex="0"><code data-lang="bash"><span><span>$ bcachefs fs usage /mnt/bcachefs/
</span></span><span><span>Filesystem: 801d8d4e-5fb7-4697-a203-a2f7895f66aa
</span></span><span><span>Size:                 	<span>1975685120</span>
</span></span><span><span>Used:                 	<span>1975685120</span>
</span></span><span><span>Online reserved:           <span>843776</span>
</span></span><span><span>
</span></span><span><span>Data type   	Required/total  Durability	Devices
</span></span><span><span>btree:      	1/1         	<span>1</span>         	<span>[</span>sdh<span>]</span>              <span>9437184</span>
</span></span><span><span>user:       	1/1         	<span>1</span>         	<span>[</span>sdh<span>]</span>           <span>1945206784</span>
</span></span><span><span>
</span></span><span><span><span>(</span>no label<span>)</span> <span>(</span>device 0<span>)</span>:       	sdh          	rw
</span></span><span><span>                            	data     	buckets	fragmented
</span></span><span><span>  free:                    <span>172490752</span>         <span>658</span>
</span></span><span><span>  sb:                        <span>3149824</span>          <span>13</span>    	<span>258048</span>
</span></span><span><span>  journal:                  <span>16777216</span>          <span>64</span>
</span></span><span><span>  btree:                     <span>9437184</span>          <span>36</span>
</span></span><span><span>  user:                   <span>1945206784</span>        <span>7421</span>    	<span>163840</span>
</span></span><span><span>  cached:                          <span>0</span>           <span>0</span>
</span></span><span><span>  parity:                      	   <span>0</span>           <span>0</span>
</span></span><span><span>  stripe:                      	   <span>0</span>           <span>0</span>
</span></span><span><span>  need_gc_gens:                	   <span>0</span>           <span>0</span>
</span></span><span><span>  need_discard:                    <span>0</span>           <span>0</span>
</span></span><span><span>  capacity:               <span>2147483648</span>        <span>8192</span>
</span></span></code></pre></div><h3 id="fill-a-fs-100-and-then-enable-compression">Fill a FS 100% and then enable compression</h3>
<ul>
<li>Simple test running as root &amp; unprivileged user with highly compressible data
<ul>
<li>In both cases after enabling compression it worked, 1.8G -&gt; 27M</li>
</ul>
</li>
<li>Test of filling FS with pure random, no space savings as expected, and no errors generated from background kernel threads</li>
<li>Basic integrity testing detected no errors</li>
</ul>
<h3 id="use-different-physical-sector-sizes">Use different physical sector sizes</h3>
<h4 id="cant-always-do-this-not-allowed-with-lvm--stratis">can’t always do this, not allowed with LVM &amp; Stratis</h4>
<ul>
<li>Tested configuration with 512 &amp; 4096 physical sector size devices.  FS created and mounted with no errors</li>
<li>Filled &amp; verified with no issues</li>
<li>Asked Kent on IRC about mixing physical sector sizes his response
<em><strong>yes we can, we’re COW (by default any ways)</strong></em></li>
</ul>
<h3 id="stack-bcachefs-on-thin-lv-or-vdo">Stack bcachefs on thin LV or vdo</h3>
<h4 id="no-manual-fstrim-support">(no manual fstrim support)</h4>
<ul>
<li>Yes, Important part is to enable discard in bcachefs so that you don’t fill up the pool, the default is not to issue discards
<ul>
<li>I repeatedly open/modified/closed same file, with discard enabled in bcachefs, the LV stayed empty as expected</li>
</ul>
</li>
<li>You can’t use fstrim directly (you should be able), bcachefs issues discards
<ul>
<li>Discussion with Kent, he understands that some older devices may have bad implementation of discard, so this should be supported</li>
</ul>
</li>
<li>vdo specific notes
<ul>
<li>discard support on vdo has known performance issues</li>
<li>Filling the FS with highly compressible data -&gt; all ok</li>
<li>Filling the FS with random data files -&gt; bcachefs kernel errors and read only FS (expected), no errors from vdo preceded the event (unexpected)</li>
</ul>
</li>
</ul>
<h3 id="can-i-use-bcachefs-on-smr--raw-flash">Can I use bcachefs on SMR &amp; raw flash?</h3>
<ul>
<li>Not yet, discussed in the design documents, see them for details</li>
</ul>
<h2 id="raid">RAID</h2>
<h3 id="what-about-raid-01056-etc">What about RAID 0,10,5,6 etc.</h3>
<ul>
<li>bcachefs doesn’t have switches to create some of the common RAID levels, this is a planned addition,  Kent on IRC..
<strong>that will come later with ways of restricting/defining the geometry further</strong></li>
<li>You can specify how many copies of metadata and data are desired</li>
<li>You can specify how many copies of data and metadata that are required
<ul>
<li>required == can run in degraded mode if we’re only able to do this many writes (translation, the FS stays operational in RW mode)</li>
</ul>
</li>
<li>Erasure coding support for RAID5/6 like functionality is experimental</li>
<li><strong>bcachefs with –replicas=N will tolerate N-1 disk failures without loss of data</strong>
<ul>
<li>Max N is limited to 3, so currently you can’t create a bcachefs that will tolerate &gt; 2 disk simultaneous failure.</li>
</ul>
</li>
<li>You can instruct bcachefs that specific block devices have custom durability, e.g. they’re a hardware RAID backed block device</li>
</ul>
<h3 id="raid0-experiment">RAID0 experiment</h3>
<ul>
<li>RAID0 behavior is default when using multiple disks
<ul>
<li><code>bcachefs format /dev/sdg /dev/sdh</code></li>
</ul>
</li>
<li>If missing a disk you may be able to mount with data loss
<ul>
<li><code>bcachefs mount -o very_degraded /dev/sdg /mnt/bcachefs</code></li>
</ul>
</li>
<li>To ensure you can mount and possibly recover data not on missing disk you need to ensure metadata is on multiple disks
<ul>
<li><code>bcachefs format /dev/sdg /dev/sdh --metadata_replicas=2</code></li>
<li>However, in testing only 3 out of 929 files were available, the rest got EIO when reading as expected.
This is because most of the contents of each file are spread across each of the disks during the write which is expected</li>
</ul>
</li>
</ul>
<h3 id="raid1">RAID1</h3>
<ul>
<li>Two disks, format with bcachefs format /dev/sdg /dev/sdh –replicas=2</li>
<li><code>—replicas=2</code> is same as:  <code>--data_replicas=2 --metadata_replicas=2</code></li>
<li>Tested with, mount, fill, umount, zero 1 of the disks, remounted -o degraded
<ul>
<li>No file loss or corruption</li>
</ul>
</li>
</ul>
<h3 id="raid10-cannot-do">RAID10 (cannot do)</h3>
<ul>
<li>You can’t right now, 4 block devices with —replicas=2 is mentioned on archlinux wiki which is incorrect
<pre tabindex="0"><code># bcachefs format /dev/sde /dev/sdf /dev/sdg /dev/sdh --replicas=2`
bcachefs fs usage /mnt/bcachefs
Data type   	Required/total  Durability	Devices
btree:      	1/2         	2         	[sdg sdh]        	7864320
btree:      	1/2         	2         	[sde sdf]       35127296
user:       	1/2         	2         	[sde sdh]        	2621440
user:       	1/2         	2         	[sdf sdh]       	1572864
user:       	1/2         	2         	[sde sdg]        	1572864
user:       	1/2         	2         	[sdf sdg]        	2621440
user:       	1/2         	2         	[sdg sdh]     3886860288
user:       	1/2         	2         	[sde sdf]     3865051136
</code></pre></li>
<li>We can only tolerate a 1 disk failure in this configuration, which was tested and found true.
In an actual RAID10 you can lose 1 disk in each leg of the mirror.</li>
</ul>
<h3 id="raid-56-experimental">RAID 5/6 (experimental)</h3>
<ul>
<li>This is referred to as erasure coding and is listed as  <strong>“DO NOT USE YET”</strong>, experimental in kconfig
<ul>
<li><code>bcachefs format --erasure_code --metadata_replicas=3 –data_replicas=2 /dev/sdf /dev/sdg /dev/sdh</code></li>
</ul>
</li>
<li>data_replicas is configuration knob used by the erasure code</li>
<li>Not available for metadata use</li>
<li>Not compiled in with default fedora kernel module</li>
<li>On disk format has not be formalized</li>
<li>Does not suffer from RAID5/6 <strong>write hole</strong>.  This is accomplished by writing copy to multiple disks and when enough data
to fill a full stripe is present, it then writes the stripe.  Fully protected, but at a cost of more writes, which in my
opinion is good!</li>
</ul>
<h3 id="fs-usage-with-erasure-coding">fs usage with erasure coding</h3>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span># bcachefs fs usage /mnt/ec</span>
</span></span><span><span>Filesystem: 8f8b9a5e-afea-49c7-a245-86e132263bd4
</span></span><span><span>Size:                 <span>53343494144</span>
</span></span><span><span>Used:                  	<span>896942080</span>
</span></span><span><span>Online reserved:               	<span>0</span>
</span></span><span><span>
</span></span><span><span>Data type   	Required/total  Durability	Devices
</span></span><span><span>btree:      	1/3         	<span>3</span>         	<span>[</span>sdf sdg sdh<span>]</span>   	<span>15728640</span>
</span></span><span><span>user:       	1/2         	<span>2</span>         	<span>[</span>sdf sdh<span>]</span>         	  <span>524288</span>
</span></span><span><span>user:       	1/2         	<span>2</span>         	<span>[</span>sdf sdg<span>]</span>        	 <span>1458176</span>
</span></span><span><span>user:       	2/3         	<span>3</span>         	<span>[</span>sdf sdg sdh<span>]</span>  	   <span>277348352</span>
</span></span><span><span>parity:     	2/3         	<span>3</span>         	<span>[</span>sdf sdg sdh<span>]</span>  	   <span>138674176</span>
</span></span></code></pre></div><h2 id="misc-information-things-tried">Misc. information, things tried</h2>
<h3 id="why-is-my-free-space--expected">Why is my free space &lt; expected?</h3>
<ul>
<li>~8% of total space reserved for garbage collection, can be configured
<ul>
<li>see <code>--gc_reserve_percent</code></li>
<li>see <code>--gc_reserve_bytes</code></li>
</ul>
</li>
<li>Also depends on the number of copies of metadata</li>
</ul>
<h3 id="multipath">multipath</h3>
<ul>
<li>The pull request I created which leverages the udev DB which was subsequently merged makes this work</li>
<li>NVMe native multipath should have always worked</li>
</ul>
<h3 id="block-device-management">Block device management</h3>
<ul>
<li>dynamically add/remove devices to a FS in use</li>
<li>evacuate data from a device, then remove it</li>
<li>correct a degraded FS</li>
<li>Take devices online/offline, mark block devices as failed</li>
<li>The documentation is a bit lacking on the exact order of steps, to do when a device fails, but I got it to work :-)</li>
<li>Current limit of 64 devices in a single FS</li>
<li>resize filesystem on a device (your disk capacity increases)</li>
<li>resize journal on a device</li>
</ul>
<h3 id="error-handling-on-crc-read-error">Error handling on CRC read error</h3>
<ul>
<li>1 copy of file, CRC error on read results in EIO
<ul>
<li>Future work item, add a read flag which says give me what you do have despite CRC errors</li>
</ul>
</li>
<li>2 or more copies of file, CRC on error, read other copy, data returned to userspace, does not correct bad copy
<ul>
<li>Correction is a future work item</li>
</ul>
</li>
</ul>
<h3 id="things-not-explored">Things not explored</h3>
<ul>
<li>There are lots of other options and functionality that I didn’t evaluate
<ul>
<li>caching (write through, write back, write around)</li>
<li>no-COW mode</li>
<li>using custom durability settings</li>
<li>quotas</li>
<li>different checksums, compression algorithms</li>
<li>settings e.g. on journaling, targets, garbage collection. data I/O and more</li>
</ul>
</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<h3 id="work-in-progress">Work in progress</h3>
<ul>
<li>bcachefs is not complete</li>
<li>Kent is working very hard to ensure early adopters don’t ever lose data, even when they do some very questionable things</li>
<li>Any assistance/help is appreciated</li>
</ul>
<h3 id="specific-areas-that-could-use-some-help">Specific areas that could use some help</h3>
<ul>
<li>Solution to multi-device FS and correct sequencing at boot which includes the option to mount in degraded mode if acceptable to end user
<ul>
<li>Need a good way to warn/alert a user when a FS is mounted degraded at boot</li>
</ul>
</li>
<li>Change encrypt password entry to better integrate with systemd, e.g. <code>systemd-ask-password</code></li>
</ul>

    </article>

    


    
        


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zulip 9.0: Organized chat for distributed teams (136 pts)]]></title>
            <link>https://blog.zulip.com/2024/07/25/zulip-9-0-released/</link>
            <guid>41075934</guid>
            <pubDate>Fri, 26 Jul 2024 05:14:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.zulip.com/2024/07/25/zulip-9-0-released/">https://blog.zulip.com/2024/07/25/zulip-9-0-released/</a>, See on <a href="https://news.ycombinator.com/item?id=41075934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <article><section> <a href="https://blog.zulip.com/tag/major-releases/">Major releases</a>, <a href="https://blog.zulip.com/tag/release-announcements/">Release announcements</a> </section><section> <a href="https://blog.zulip.com/author/tabbott/"> <img src="https://blog.zulip.com/_astro/tabbott.Y0j5haym_1QpB1I.webp" alt="" width="100" height="100" loading="lazy" decoding="async"> </a>  <span> <time datetime="2024-07-25"> Jul 25, 2024</time> <span>•</span> 15 min read</span> </section><section> <p>We’re excited to announce the release of Zulip Server 9.0, containing hundreds
of new features and bug fixes!</p>
<p><span><img src="https://blog.zulip.com/_astro/channels-and-topics.CBcwcz3r_Z2f19Cz.webp" alt="" width="996" height="950" loading="lazy" decoding="async"></span> <a href="https://zulip.com/">Zulip</a>
is an open-source team chat application designed for seamless remote and hybrid
work. With conversations
<a href="https://zulip.com/help/introduction-to-topics">organized by topic</a>, Zulip is
<a href="https://zulip.com/why-zulip/">ideal</a> for both live and asynchronous
communication. Zulip’s 100% open-source software is available as a
<a href="https://zulip.com/plans/#cloud">cloud service</a> or a
<a href="https://zulip.com/self-hosting/">self-hosted solution</a>, and is used by
thousands of organizations around the world.</p>
<p><span><img src="https://blog.zulip.com/_astro/channels-and-topics.CBcwcz3r_Z2f19Cz.webp" alt="" width="996" height="950" loading="lazy" decoding="async"></span></p>
<p>Zulip Server 9.0 is a major release, with over 5,300 new commits merged across
the project since the 8.0 release in December 2023. Notable new features include
a UI redesign with larger font size and line spacing, major improvements for
composing messages, and much more!</p>
<p>A total of 124 people contributed commits to Zulip since the 8.0 release,
bringing the project to over 1,450 code contributors. Zulip is remarkable for
its number of major contributors, with <a href="https://zulip.com/team">89 people</a>
who’ve contributed 100+ commits.</p>
<p>Huge thanks to everyone who’s
<a href="https://zulip.readthedocs.io/en/latest/overview/contributing.html">contributed</a>
to Zulip over the last several months, whether by writing code and
documentation,
<a href="https://zulip.com/help/contact-support#product-feedback">reporting issues</a>,
<a href="https://zulip.readthedocs.io/en/latest/translating/translating.html">translating</a>,
<a href="https://github.com/sponsors/zulip">supporting us financially</a>, participating in
discussions in the
<a href="https://zulip.com/development-community/">Zulip development community</a>, or just
suggesting ideas! We could not do this without the hundreds of people
<a href="https://zulip.com/help/support-zulip-project">giving back</a> to the Zulip
community.</p>
<h2 id="project-highlights">Project highlights</h2>
<p>Today marks a release of the Zulip server and web application. We’d also like to
share news and updates for the project as a whole since the 8.0 release last
December:</p>
<h2 id="apps">Apps</h2>
<p>We’ve been hard at work on Zulip’s next-generation mobile app for Android and
iOS, built with Flutter. Users can look forward to redesigned apps and a
<a href="https://chat.zulip.org/#narrow/stream/2-general/topic/Flutter/near/1582367">faster, smoother experience</a>.</p>
<p>We published a first
<a href="https://chat.zulip.org/#narrow/stream/2-general/topic/Flutter/near/1708728">community beta release</a>
last December, and have made 9 more releases since then,
<a href="https://chat.zulip.org/#narrow/stream/1-announce/topic/mobile.20beta/near/1716421">packed with features</a>.
There are over 1,700 commits in the latest release, and progress has been
accelerating, with another full-time developer and three Google Summer of Code
participants joining our mobile team this summer.</p>
<p>Later this year, we’ll announce a public beta here on the Zulip blog, so that
you can try it out and see the new smoother experience for yourself. In the
months after that, we’ll roll out the new app for everyone as the next version
of the official Zulip mobile apps on both iOS and Android.</p>
<p>Meanwhile, we’ve
<a href="https://github.com/zulip/zulip-mobile/releases">continued updating the existing mobile apps</a>
for key Zulip Server 9.0 features you’ll read about below, as well as fixing
selected bugs.</p>
<p><a href="https://github.com/zulip/zulip-terminal#readme">Zulip Terminal</a> features new
since last December include easy-to-read themes when using a transparent
terminal, improved in-app help, the ability to compose and edit messages in
external editors, and a prompt to avoid accidentally closing — and losing — the
message you are composing.</p>
<h2 id="reviews">Reviews</h2>
<ul>
<li>An in-depth
<a href="https://www.hostingadvice.com/blog/emerging-open-source-team-chat-app-set-to-rival-slack/">review of Zulip</a>
on <a href="https://www.hostingadvice.com/">HostingAdvice.com</a> described it as <em>“an
excellent solution for teams collaborating across different time zones.”</em></li>
<li>Zulip was recognized as a
<a href="https://www.getapp.com/collaboration-software/team-communication/category-leaders/">GetApp 2024 category leader</a>
for team communication. Huge thanks to our users for sharing their feedback
and helping others find us!</li>
</ul>
<h2 id="customer-stories">Customer stories</h2>
<ul>
<li>
<p>In a <a href="https://zulip.com/case-studies/gut-contact/">new case study</a>, the folks
at <a href="https://gutcontact.de/">GUT contact</a> shared how they use Zulip to manage
1,000 employees spread across 3 countries. They love how easy it’s been to
self-host the software and onboard their team.</p>
<blockquote>
<p>“Zulip was quite easy to set up, and worked instantly. 1,000 people — no
problem.”</p>
<p>— <span>Erik Dittert, Head of IT at GUT contact</span></p>
</blockquote>
</li>
<li>
<p>In another <a href="https://zulip.com/case-studies/semsee/">new case study</a>, the
InsurTech startup <a href="https://semsee.com/">Semsee</a> shared how Zulip has created
breathing room for focused work and different work schedules for their
distributed team.</p>
</li>
</ul>
<blockquote>
<p>“I don’t like going back to Slack now. It’s just not as efficient a way to
organize communication.”</p>
<p>— <span>James van Lommel, Director of Engineering at
Semsee</span></p>
</blockquote>
<ul>
<li>The Lean Prover community has
<a href="https://leanprover-community.github.io/blog/posts/FLT-announcement/">kicked off</a>
a multi-year collaboration on Zulip to formalize a proof of
<a href="https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem">Fermat’s Last Theorem</a>.
We wish them the best of luck!</li>
</ul>
<blockquote>
<p>“The project will be run on the
<a href="https://leanprover.zulipchat.com/#narrow/stream/416277-FLT">FLT channel</a> on the
<a href="https://leanprover.zulipchat.com/">Lean Zulip chat</a>, a high-powered research
forum where mathematicians and computer scientists can collaborate in real time,
effortlessly posting code and mathematics using a thread and channel system
which admirably handles the task of enabling many independent conversations to
happen simultaneously.”</p>
<p>— <span>Kevin Buzzard, announcing The Fermat’s Last
Theorem Project</span></p>
</blockquote>
<p>If using Zulip is making a difference for your organization, we’d love to
<a href="https://zulip.com/help/contact-support#product-feedback">hear about it</a>!</p>
<h2 id="mentorship">Mentorship</h2>
<p>Our community is fully committed to helping bring up the next generation of
open-source contributors. 2024 is the ninth consecutive year that Zulip is
participating in <a href="https://summerofcode.withgoogle.com/">Google Summer of Code</a>.
Our 17 GSoC 2024 participants have contributed to many of the features described
in this post!</p>
<blockquote>
<p>“The mentorship I’ve received here is unparalleled in my life, it’s the best
I’ve ever had.”</p>
<p>— <span>Niloth, Google Summer of Code 2024
participant</span></p>
</blockquote>

<p>We proudly sponsor free <a href="https://zulip.com/plans/">Zulip Cloud Standard hosting</a>
for more than 1,500 <a href="https://zulip.com/for/open-source/">open-source projects</a>,
<a href="https://zulip.com/for/communities/">non-profits</a>,
<a href="https://zulip.com/for/education/">educational institutions</a>, and
<a href="https://zulip.com/for/research/">academic research groups</a>. All eligible
organizations are encouraged to
<a href="https://zulip.com/help/zulip-cloud-billing#apply-for-sponsorship">join this program</a>,
or sign up for the
<a href="https://zulip.com/help/self-hosted-billing#apply-for-community-plan">Community plan</a>
for self-hosted organizations.</p>
<blockquote>
<p>“We are a public university that offers free education to 33,000 students across
13 cities in Brazil. We started using Zulip in early 2020, and it works
perfectly for our needs. Zulip’s interface is simple and intuitive.”</p>
<p>— <span>Rafael Cordeiro, head of IT at
</span><a href="https://www.utfpr.edu.br/">UTFPR</a></p>
</blockquote>
<blockquote>
<p>“As a research consortium spread across 14 locations in Germany, we use Zulip to
communicate with each other in a low-threshold manner, without the overhead of
email. Even with more than 200 users across different institutions, Zulip’s
model of topic-labeled conversations makes it easy for our team members to keep
up-to-date on what’s relevant, and work productively together.”</p>
<p>— <span>Christina Schüttler, IT department Team Lead at
</span><a href="https://www.uk-erlangen.de/en/">University Hospital Erlangen</a></p>
</blockquote>
<h2 id="release-highlights">Release highlights</h2>
<p>This section gives an overview of the major features and improvements in Zulip
Server 9.0.</p>
<p>Alongside the major changes, this release contains hundreds of refinements
beyond what can be described here. We believe that getting the little details
right makes a big difference in a product that’s so central to people’s daily
work. To make Zulip
<a href="https://zulip.com/case-studies/rust/#the-language-team-moves-from-discord-to-zulip-its-better-in-all-the-ways-i-care-about">a joy to use</a>,
we relentlessly fine-tune the user experience, and consider it a priority to
investigate and fix even minor bugs.</p>
<h2 id="streams-renamed-to-channels">“Streams” renamed to “channels”</h2>
<p>To better match terminology familiar from other apps, “streams” have been
renamed to “<a href="https://zulip.com/help/introduction-to-channels">channels</a>” across
the app in this release. The functionality remains exactly the same, and
integrations do not need to be updated for this transition.</p>
<h2 id="in-app-feature-announcements">In-app feature announcements</h2>
<p>To make it easier to discover what’s new in Zulip, important product changes are
<a href="https://zulip.com/help/configure-automated-notices#zulip-update-announcements">now announced</a>
via automated messages. These messages are sent to a “Zulip updates” topic in a
channel selected by organization administrators. Users can read the
announcements when it’s convenient, and chat about them in the same topic.</p>
<p>On self-hosted servers, these
<a href="https://zulip.readthedocs.io/en/latest/overview/changelog.html#upgrade-notes-for-9-0">configurable</a>
notices are shipped with the Zulip server version that includes the new feature.</p>
<h2 id="improved-reading-experience">Improved reading experience</h2>
<ul>
<li>To make reading more comfortable, Zulip has been redesigned with a larger font
size and line spacing. Because personal preferences vary, the previous design
remains available via the <a href="https://zulip.com/help/font-size">compact mode</a>
setting, and we plan to offer flexible controls for font size and line spacing
in the next major release.</li>
<li>For a more focused reading experience, you can now hide the
<a href="https://zulip.com/help/left-sidebar">left</a> and
<a href="https://zulip.com/help/user-list">right</a> sidebars even in a full-size window.
When the left sidebar is hidden,
<a href="https://zulip.com/help/keyboard-shortcuts#navigation">keyboard navigation</a>
lets you jump to the next unread topic or to your
<a href="https://zulip.com/help/configure-home-view">home view</a>. The
<a href="https://zulip.com/help/user-list">right sidebar</a> has been redesigned to
highlight users in the current channel or direct-message conversation.</li>
<li>Messages and topic names now load much faster when you open the web or desktop
app.</li>
</ul>
<p><span><img src="https://blog.zulip.com/_astro/focus-mode.CW8MPwgV_MlQm8.webp" alt="" width="2120" height="760" loading="lazy" decoding="async"></span></p>
<h2 id="improved-experience-for-composing-messages">Improved experience for composing messages</h2>
<ul>
<li>The compose box has been redesigned, with a cleaner, more modern look.</li>
<li>When you start composing, the most recently edited draft for the conversation
you are composing to now automatically appears in the compose box. You can
always
<a href="https://zulip.com/help/view-and-edit-your-message-drafts#save-a-draft-and-start-a-new-message">save a draft</a>
for later and start a new message.</li>
<li>When you paste content into the compose box, Zulip will now do its best to
preserve the formatting, including links, bulleted lists, bold, italics, and
more. If you don’t need the formatting, you can undo it with <code>Ctrl+Z</code>, or
<a href="https://zulip.com/help/keyboard-shortcuts#the-basics">paste as plain text</a>.</li>
<li>Pasted channel and topic
<a href="https://zulip.com/help/link-to-a-message-or-conversation#get-a-link-to-a-specific-topic">link URLs</a>
are now automatically converted into
<a href="https://zulip.com/help/link-to-a-message-or-conversation#link-to-a-channel-or-topic-within-zulip">nicely formatted</a>
links.</li>
<li>To <a href="https://zulip.com/help/quote-and-reply">quote and reply</a> to part of a
message, you can now select the part that you want to quote.</li>
<li>You can now scroll down in all typeahead menus to see more matches for what
you’ve typed so far.</li>
</ul>
<p><span><img src="https://blog.zulip.com/_astro/compose-box.6NRbWRE1_Z28NwSv.webp" alt="" width="1814" height="944" loading="lazy" decoding="async"></span></p>
<h2 id="more-natural-navigation">More natural navigation</h2>
<ul>
<li>For clarity and consistency, three key message views that show multiple
conversations at once are now labeled
“<a href="https://zulip.com/help/combined-feed">Combined feed</a>” (previously “All
messages”), “<a href="https://zulip.com/help/channel-feed">Channel feed</a>”, and
“<a href="https://zulip.com/help/direct-messages#access-all-dms">Direct message feed</a>”.</li>
<li>To make it easier to read your messages topic-by-topic, channel links in the
<a href="https://zulip.com/help/left-sidebar">left sidebar</a> now take you directly to
the top topic in the channel. You can
<a href="https://zulip.com/help/channel-feed#configure-where-channel-links-in-the-left-sidebar-go">configure</a>
them to go to the channel feed instead (previously the only option).</li>
<li>When you send a message to a conversation that you’re not viewing, Zulip now
jumps you to that conversation, unless
<a href="https://zulip.com/help/mastering-the-compose-box#automatically-go-to-conversation-where-you-sent-a-message">configured</a>
otherwise.</li>
</ul>
<h2 id="new-and-improved-ways-to-find-messages">New and improved ways to find messages</h2>
<ul>
<li>The main <a href="https://zulip.com/help/search-for-messages">search</a> has been
redesigned with pills for search filters, making it easier to use.
<span><img src="https://blog.zulip.com/_astro/search-pills.gJyJpot5_GPHpu.webp" alt="" width="1012" height="59" loading="lazy" decoding="async"></span></li>
<li>You can use the new
<a href="https://zulip.com/help/emoji-reactions#view-your-messages-with-reactions">Reactions view</a>
to see how others have responded to your messages with emoji reactions. You
can also
<a href="https://zulip.com/help/search-for-messages#search-by-message-status">search</a>
all messages with reactions.</li>
<li>You can now search for messages
<a href="https://zulip.com/help/follow-a-topic#search-for-messages-in-followed-topics">in topics you follow</a>,
or see a feed of all messages (or just unread messages) in your followed
topics.</li>
<li>You can now
<a href="https://zulip.com/help/direct-messages#find-a-direct-message-conversation">filter</a>
direct message conversations in the left sidebar to show conversations that
include a specific person.</li>
</ul>
<h2 id="easier-user-onboarding">Easier user onboarding</h2>
<p>Zulip’s onboarding experience has been redesigned to introduce core Zulip
concepts when they first become relevant.</p>
<p><span><img src="https://blog.zulip.com/_astro/inbox-intro-modal.FV_4wDKS_Z1JlzHx.webp" alt="" width="1496" height="1080" loading="lazy" decoding="async"></span></p>
<ul>
<li>One-time modals now introduce the <a href="https://zulip.com/help/inbox">Inbox</a> and
<a href="https://zulip.com/help/recent-conversations">Recent conversations</a> views.
Permanently dismissable banners explain complex situations, such as composing
a message
<a href="https://zulip.com/help/mastering-the-compose-box#composing-to-a-different-conversation">while viewing a different conversation</a>,
when they come up. <span><img src="https://blog.zulip.com/_astro/inbox-intro-modal.FV_4wDKS_Z1JlzHx.webp" alt="" width="1496" height="1080" loading="lazy" decoding="async"></span> <span><img src="https://blog.zulip.com/_astro/onboarding-banner.Cf42Bcvo_Z1MTT6q.webp" alt="" width="1306" height="170" loading="lazy" decoding="async"></span></li>
<li>To introduce <a href="https://zulip.com/help/star-a-message">starred messages</a>,
<a href="https://zulip.com/help/view-your-mentions">your mentions</a>, and other views,
we added descriptions in the bar at the top, and to the message area when the
view is empty.</li>
<li>Introductory messages have been rewritten to better explain how
<a href="https://zulip.com/help/introduction-to-topics">conversations labeled with topics</a>
work in Zulip, including how to
<a href="https://zulip.com/help/replying-to-messages">reply</a> to an existing thread,
<a href="https://zulip.com/help/introduction-to-topics#how-to-start-a-new-topic">start</a>
a new conversation, and
<a href="https://zulip.com/help/move-content-to-another-topic">move messages</a> if they
are out of place. To make intro messages easier to find, they are now starred
and marked as unread for new users.</li>
</ul>
<h2 id="design-and-usability-improvements">Design and usability improvements</h2>
<p><span><img src="https://blog.zulip.com/_astro/user-card.B8MNmu0b_wNnBe.webp" alt="" width="691" height="1089" loading="lazy" decoding="async"></span></p>
<ul>
<li>All the menus have been restyled, with a cleaner look, better icons, and
improved screen reader accessibility. Also, you can now
<a href="https://zulip.com/help/dark-theme">switch themes</a> directly from your personal
menu. <span><img src="https://blog.zulip.com/_astro/user-card.B8MNmu0b_wNnBe.webp" alt="" width="691" height="1089" loading="lazy" decoding="async"></span></li>
<li>You can now configure how
<a href="https://zulip.com/help/allow-image-link-previews">previews</a> of animated
images show the animation (always, when you hover over them, or never).</li>
<li>To better show key information,
<a href="https://zulip.com/help/desktop-notifications">desktop</a> and
<a href="https://zulip.com/help/mobile-notifications">mobile</a> notifications now
abbreviate quoted content.</li>
<li>The message
<a href="https://zulip.com/help/view-a-messages-edit-history">edit history</a> view has
been redesigned for visual consistency and ease of use.</li>
<li>If you’d prefer not to see
<a href="https://zulip.com/help/typing-notifications">notifications when others type</a>,
you can now disable them.</li>
</ul>
<h2 id="keyboard-shortcuts">Keyboard shortcuts</h2>
<ul>
<li>A new <code>Alt+P</code> keyboard shortcut lets you toggle between edit mode and
<a href="https://zulip.com/help/preview-your-message-before-sending">preview mode</a>
when composing or editing a message.</li>
<li>A new <code>Shift+V</code> keyboard shortcut offers quick access to
<a href="https://zulip.com/help/read-receipts">read receipts</a>.</li>
</ul>
<h2 id="improved-user-management">Improved user management</h2>
<ul>
<li>
<p>A new combined user settings panel lets you manage
<a href="https://zulip.com/help/manage-a-user">users</a>,
<a href="https://zulip.com/help/deactivate-or-reactivate-a-user">deactivated users</a>,
and <a href="https://zulip.com/help/invite-new-users">invitations</a> in one place. You
can now
<a href="https://zulip.com/help/roles-and-permissions#view-users-by-role">filter</a> the
list of users in your organization by role.</p>
</li>
<li>
<p>Profile pictures are now shown alongside user names in settings, making it
easier to identify people at a glance. You can click on a name to open the
user’s <a href="https://zulip.com/help/user-cards">card</a>.</p>
<p><span><img src="https://blog.zulip.com/_astro/user-settings.BoX-g8Jg_Z9fg9B.webp" alt="" width="2296" height="770" loading="lazy" decoding="async"></span></p>
</li>
<li>
<p>You now have a lot of flexibility for configuring direct message permissions
in your organization, with
<a href="https://zulip.com/help/restrict-direct-messages#restrict-direct-messages">new settings</a>
for who can <strong>authorize</strong> and <strong>start</strong> direct message conversations. These
settings are designed so that users can always respond to a direct message
they’ve received.</p>
</li>
<li>
<p>You can now mark a custom profile field as
<a href="https://zulip.com/help/custom-profile-fields#make-a-custom-profile-field-required">required</a>,
to prompt everyone to fill it out.</p>
</li>
<li>
<p>You can
<a href="https://zulip.com/help/restrict-name-and-email-changes#require-unique-names">require</a>
users to choose unique names in your organization.</p>
</li>
<li>
<p>When inviting users, you can now
<a href="https://zulip.com/help/invite-new-users#send-email-invitations">decide</a>
whether to receive a direct message when the invited user joins your
organization. An updated design makes the UI for
<a href="https://zulip.com/help/invite-new-users">inviting</a> new users easier to use.</p>
</li>
<li>
<p>You can now link to a user’s
<a href="https://zulip.com/help/view-someones-profile">profile</a>, to share it with
someone else.</p>
</li>
</ul>
<h2 id="improved-channel-administration">Improved channel administration</h2>
<ul>
<li><a href="https://zulip.com/help/create-a-channel">Creating a channel</a> is now a
smoother experience:
<ul>
<li>Adding subscribers is now a separate step from configuring channel settings.
Advanced settings are collapsed by default.</li>
<li>When you create a channel, you are now taken directly there if you are a
subscriber.</li>
</ul>
</li>
<li>Channel settings now show additional details, including when the channel was
created.</li>
</ul>
<h2 id="integrations">Integrations</h2>
<ul>
<li>There is now a convenient UI for
<a href="https://zulip.com/help/generate-integration-url">configuring custom event filters</a>
for an integration. Documentation for over 60 integrations has been updated
with simplified instructions.</li>
<li>There are new integrations for
<a href="https://zulip.com/integrations/doc/patreon">Patreon</a> and
<a href="https://zulip.com/integrations/doc/githubsponsors">GitHub Sponsors</a>, and the
<a href="https://zulip.com/integrations/doc/github">GitHub</a> and
<a href="https://zulip.com/integrations/doc/grafana">Grafana</a> integrations have been
improved.</li>
<li>We made extensive additions to Zulip’s API documentation, including
<a href="https://zulip.com/api/get-events#events-by-type">handy navigation</a> for
<a href="https://zulip.com/api/real-time-events">real-time push</a> event formats,
articles on important concepts like thumbnailing, and new documentation for
about a dozen previously documented API endpoints.</li>
</ul>
<h2 id="server">Server</h2>
<ul>
<li>This release adds support for Ubuntu 24.04, and drops support for Ubuntu
20.04.</li>
<li>The Zulip server now serves thumbnails of uploaded images for previews in the
message feed, greatly reducing bandwidth usage for most Zulip installations.
Images now load much faster for users in all Zulip apps.</li>
<li>The Zulip server has always scaled well, but this release comes with several
further efficiency improvements, which will make a big difference for
organizations with thousands of user accounts.</li>
</ul>
<h2 id="internationalization">Internationalization</h2>
<p>Our community translators have been hard at work keeping up with the product as
it evolves. This release features translations that cover the majority of
non-error strings in 25 languages, including Gujarati for the first time.</p>
<h2 id="upgrading">Upgrading</h2>
<p>We
<a href="https://zulip.readthedocs.io/en/stable/overview/release-lifecycle.html#compatibility-and-upgrading">highly recommend</a>
upgrading to Zulip Server 9.0 to take advantage of the hundreds of improvements
in this release and future security updates. We work hard to ensure that
upgrades are smooth; you can upgrade by following the straightforward
<a href="https://zulip.readthedocs.io/en/stable/production/upgrade-or-modify.html#upgrading-to-a-release">upgrade instructions</a>.
The
<a href="https://zulip.readthedocs.io/en/stable/changelog.html#upgrade-notes">upgrade notes section of the changelog</a>
details changes you’ll want to understand before upgrading.</p>
<p>Commercial support for server upgrades is available for installations that
purchase a Business or Enterprise <a href="https://zulip.com/plans#self-hosted">plan</a>.
For community support, everyone is welcome to drop by the
<a href="https://zulip.com/development-community/">Zulip development community</a>.</p>
<p>Zulip Cloud is always up to date with the latest Zulip improvements.</p>
<h2 id="product-roadmap">Product roadmap</h2>
<p>We’re continuing to focus on improving the design of the Zulip web app, and
shipping next-generation mobile apps for Android and iOS.</p>
<p>We use project boards to publicly track goals for major
<a href="https://github.com/orgs/zulip/projects/9/views/12">server</a> and
<a href="https://github.com/orgs/zulip/projects/5/views/4">mobile</a> releases. These
boards are updated on an ongoing basis as priorities evolve, and many community
improvements integrated into Zulip are never specifically tracked as release
goals.</p>
<h2 id="release-schedule">Release schedule</h2>
<p>This release is coming out about seven months after the
<a href="https://blog.zulip.com/2024/07/15/zulip-8-0-released/">Zulip Server 8.0 release</a>
last December. We are targeting a shorter interval for the next major release,
which is expected in late 2024.</p>

<p>If you appreciate Zulip, please recommend it to someone you know, for work or
any other collaborative endeavor. If they say that Zulip won’t work for them,
<a href="https://zulip.com/help/contact-support#product-feedback">let us know why</a>! You
can email <a href="mailto:feedback@zulip.com">feedback@zulip.com</a>, or post in the
<a href="https://chat.zulip.org/#narrow/stream/137-feedback">#feedback</a> channel in the
<a href="https://zulip.com/development-community/">Zulip development community</a>.
Feedback from current and potential users is a vital part of how we design and
prioritize improvements to Zulip.</p>
<p>Thanks again to the amazing global Zulip development community for making this
possible!</p>
<p>— Tim Abbott, Zulip project leader</p>
<p>What follows is a summary of the commits contributed to Zulip during the 9.0
release cycle.</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>$ ./zulip/tools/total-contributions 8.0 9.0</span></span>
<span><span>616	Anders Kaseorg</span></span>
<span><span>458	Alex Vandiver</span></span>
<span><span>408	Chris Bobbe</span></span>
<span><span>400	Greg Price</span></span>
<span><span>388	Aman Agrawal</span></span>
<span><span>359	Lauryn Menard</span></span>
<span><span>313	Evy Kassirer</span></span>
<span><span>266	Karl Stolley</span></span>
<span><span>209	Sahil Batra</span></span>
<span><span>195	Tim Abbott</span></span>
<span><span>169	Prakhar Pratyush</span></span>
<span><span>166	Alya Abbott</span></span>
<span><span>165	Sayam Samal</span></span>
<span><span>117	Shubham Padia</span></span>
<span><span>111	Mateusz Mandera</span></span>
<span><span>104	Nehal Sharma</span></span>
<span><span>83	Varun Singh</span></span>
<span><span>61	afeefuddin</span></span>
<span><span>57	Kunal Sharma</span></span>
<span><span>56	Niloth P</span></span>
<span><span>47	Rohan Gudimetla</span></span>
<span><span>46	Kenneth Rodrigues</span></span>
<span><span>42	neiljp (Neil Pilgrim)</span></span>
<span><span>35	Rajesh Malviya</span></span>
<span><span>32	Sujal Shah</span></span>
<span><span>26	Kislay Verma</span></span>
<span><span>24	PieterCK</span></span>
<span><span>24	Tanmay Kumar</span></span>
<span><span>24	Sayed Mahmood Sayedi</span></span>
<span><span>24	Shu Chen</span></span>
<span><span>20	David Rosa</span></span>
<span><span>20	Pratik Chanda</span></span>
<span><span>19	Sanchit Sharma</span></span>
<span><span>19	Khader M Khudair</span></span>
<span><span>19	Zixuan James Li</span></span>
<span><span>17	Adnan Shabbir Husain</span></span>
<span><span>15	Sashank Ravipati</span></span>
<span><span>12	Lalit Kumar Singh</span></span>
<span><span>12	Nimish Medatwal</span></span>
<span><span>11	Shashank Singh</span></span>
<span><span>10	Mahhheshh</span></span>
<span><span>9	Bedo Khaled</span></span>
<span><span>9	CIC4DA</span></span>
<span><span>7	Aditya Kumar Kasaudhan</span></span>
<span><span>7	Pratik Solanki</span></span>
<span><span>7	ecxtacy</span></span>
<span><span>7	vighneshbhat9945</span></span>
<span><span>6	Robert Dyer</span></span>
<span><span>6	Temidayo32</span></span>
<span><span>6	codewithnick</span></span>
<span><span>5	Joydeep Bhattacharjee</span></span>
<span><span>5	sayyedarib</span></span>
<span><span>4	Chris Chua</span></span>
<span><span>3	Aditya Bajaj</span></span>
<span><span>3	John Lu</span></span>
<span><span>3	Rein Zustand (rht)</span></span>
<span><span>3	Sohaib-Ahmed21</span></span>
<span><span>3	Ujjawal Modi</span></span>
<span><span>3	swayam0322</span></span>
<span><span>3	Sagnik Mandal</span></span>
<span><span>2	ColeBurch</span></span>
<span><span>2	Dhruv Goyal</span></span>
<span><span>2	Eklavya Sharma</span></span>
<span><span>2	Pedro Almeida</span></span>
<span><span>2	Rinwaoluwa</span></span>
<span><span>2	abelaba</span></span>
<span><span>2	saisumith770</span></span>
<span><span>2	jrijul1201</span></span>
<span><span>2	neruyzo</span></span>
<span><span>1	Aditya Chaudhary</span></span>
<span><span>1	Afonso Azaruja</span></span>
<span><span>1	AfonsoOrmonde</span></span>
<span><span>1	Ajay Singh</span></span>
<span><span>1	Akarsh Jain</span></span>
<span><span>1	Akash Kumar Singh</span></span>
<span><span>1	Andrew</span></span>
<span><span>1	Angelica Ferlin</span></span>
<span><span>1	Anuja Patil</span></span>
<span><span>1	Artur Szcześniak</span></span>
<span><span>1	Charlie Marsh</span></span>
<span><span>1	Eeshan Garg</span></span>
<span><span>1	Emil</span></span>
<span><span>1	Evgenii</span></span>
<span><span>1	Gaurav Pandey</span></span>
<span><span>1	Gyan Dev</span></span>
<span><span>1	Isabela Pereira</span></span>
<span><span>1	Jason</span></span>
<span><span>1	Jerry Yang</span></span>
<span><span>1	Jose Corte</span></span>
<span><span>1	Joy Chen</span></span>
<span><span>1	Laura Hausmann</span></span>
<span><span>1	Niklas Fiekas</span></span>
<span><span>1	Olivier FAURE</span></span>
<span><span>1	Pietro Albini</span></span>
<span><span>1	Prathamesh Kurve</span></span>
<span><span>1	Riken Shah</span></span>
<span><span>1	Roshan Jagadish</span></span>
<span><span>1	Ryan Crisanti</span></span>
<span><span>1	Sahil Singh</span></span>
<span><span>1	Sayyed Arib Hussain</span></span>
<span><span>1	Sharif Naas</span></span>
<span><span>1	Tomas Fonseca</span></span>
<span><span>1	Tudor Stefan Pagu</span></span>
<span><span>1	Varun-Kolanu</span></span>
<span><span>1	Vector73</span></span>
<span><span>1	Vidhi Agrawal</span></span>
<span><span>1	ayush amawate</span></span>
<span><span>1	cherish2003</span></span>
<span><span>1	iks1</span></span>
<span><span>1	joaoafonso07</span></span>
<span><span>1	kota-karthik</span></span>
<span><span>1	luska44</span></span>
<span><span>1	mk0904</span></span>
<span><span>1	opmkumar</span></span>
<span><span>1	qx24680</span></span>
<span><span>1	somudas</span></span>
<span><span>1	Mehul-Kumar-27</span></span>
<span><span>1	Misha Brukman</span></span>
<span><span>1	enesonus</span></span>
<span><span>1	rsashank</span></span>
<span><span>1	LY</span></span>
<span><span>1	Sushmey</span></span>
<span><span>1	sreecharan7</span></span>
<span><span>1	Sean T. Allen</span></span>
<span><span>Commit range 8.0..9.0 corresponds to 2023-12-15 to 2024-07-25</span></span>
<span><span>4288 commits from zulip/zulip: 8.0..9.0</span></span>
<span><span>798 commits from zulip/zulip-flutter: 81d8b8ccab90..c17dd0efd192</span></span>
<span><span>137 commits from zulip/zulip-mobile: 38ca48a7b801..2217c858e207</span></span>
<span><span>91 commits from zulip/zulip-terminal: 585722a206e8..743db7d8d0ef</span></span>
<span><span>19 commits from zulip/zulip-desktop: 47cdd5fa8b3b..92260b0f97b5</span></span>
<span><span>12 commits from zulip/docker-zulip: 8e716c245f80..ef3a379351cf</span></span>
<span><span>11 commits from zulip/python-zulip-api: ad9b0e62a49d..e9d8ef3b272c</span></span>
<span><span>9 commits from zulip/zulipbot: d0bee679bf3c..3655e1f31e69</span></span>
<span><span>4 commits from zulip/github-actions-zulip: b62d5a0e48a4..e4c8f27c732b</span></span>
<span><span>2 commits from zulip/zulint: cd3b1e0586b0..a070f3a349bc</span></span>
<span><span>Excluded 0 commits authored by bots.</span></span>
<span><span>5371 total commits by 124 contributors between 8.0 and 9.0.</span></span>
<span><span></span></span></code></pre> </section></article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Swiss Town Banned Billboards. Zurich, Bern May Soon Follow (586 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-07-26/zurich-bern-consider-billboard-bans-after-vernier-outlaws-visual-pollution</link>
            <guid>41075766</guid>
            <pubDate>Fri, 26 Jul 2024 04:26:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-07-26/zurich-bern-consider-billboard-bans-after-vernier-outlaws-visual-pollution">https://www.bloomberg.com/news/articles/2024-07-26/zurich-bern-consider-billboard-bans-after-vernier-outlaws-visual-pollution</a>, See on <a href="https://news.ycombinator.com/item?id=41075766">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
    </channel>
</rss>