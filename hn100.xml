<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 20 Nov 2023 07:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Emmett Shear becomes interim OpenAI CEO as Altman talks break down (319 pts)]]></title>
            <link>https://www.theverge.com/2023/11/20/23967515/sam-altman-openai-board-fired-new-ceo</link>
            <guid>38342643</guid>
            <pubDate>Mon, 20 Nov 2023 05:19:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/11/20/23967515/sam-altman-openai-board-fired-new-ceo">https://www.theverge.com/2023/11/20/23967515/sam-altman-openai-board-fired-new-ceo</a>, See on <a href="https://news.ycombinator.com/item?id=38342643">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>After a weekend of negotiations to bring back Sam Altman as OpenAI CEO following his shock firing, the company’s nonprofit board has gone another way entirely and named former Twitch CEO Emmett Shear as interim CEO. Shear will take over as interim OpenAI CEO from Mira Murati, who was publicly aligned with Altman. The news was first reported by <a href="https://www.theinformation.com/articles/breaking-sam-altman-will-not-return-as-ceo-of-openai?utm_source=ti_app&amp;rc=k5vrz1"><em>The Information</em></a>. </p><p>That’s three CEOs in three days, if you’re keeping track.</p><p>The hiring of Shear appears to close the door on Altman’s exit after he was fired Friday when the board said he had not been “consistently candid in his communications.” <a href="https://www.theverge.com/2023/11/18/23967199/breaking-openai-board-in-discussions-with-sam-altman-to-return-as-ceo">As we reported Saturday</a>, the board quickly began discussing the CEO’s return under pressure from investors and the threat of a mass employee walkout. </p><p>On Saturday night, sources told <em>The Verge</em> the remaining board members had missed a 5PM PT deadline to resign and reinstate Altman and fellow co-founder Greg Brockman or face a slew of staff resignations. After the deadline passed, droves of OpenAI employees started <a href="https://www.theverge.com/2023/11/19/23968027/why-openai-employees-were-posting-all-those-heart-emojis">posting their support</a> for Altman on social media.</p><p>Altman returned to OpenAI’s office this morning saying it would be the “<a href="https://www.theverge.com/2023/11/19/23968162/its-the-endgame-for-sam-altmans-potential-return-to-openai">first and last time</a>” he wore a guest badge — implying he would either return as CEO or never return at all. It appears that after hours of negotiation, it’s the latter.</p><p>OpenAI’s profile has skyrocketed since the launch of ChatGPT, which quickly became one of the fastest-growing services ever and kicked off a tech gold rush over generative AI. The company’s valuation <a href="https://www.wsj.com/tech/ai/openai-seeks-new-valuation-of-up-to-90-billion-in-sale-of-existing-shares-ed6229e0">reportedly</a> reached $80 to $90 billion in recent weeks. Earlier this month, Altman reported that the service has <a href="https://www.theverge.com/2023/11/6/23948386/chatgpt-active-user-count-openai-developer-conference">over 100 million</a> weekly users.</p><p><em>Developing...</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Emmett Shear Becomes Interim OpenAI CEO as Altman Talks Break Down (156 pts)]]></title>
            <link>https://www.theinformation.com/articles/breaking-sam-altman-will-not-return-as-ceo-of-openai</link>
            <guid>38342554</guid>
            <pubDate>Mon, 20 Nov 2023 05:11:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theinformation.com/articles/breaking-sam-altman-will-not-return-as-ceo-of-openai">https://www.theinformation.com/articles/breaking-sam-altman-will-not-return-as-ceo-of-openai</a>, See on <a href="https://news.ycombinator.com/item?id=38342554">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="pre-article">
<div id="byline-and-published-time"><p>
By </p> <p><span>and</span></p><p>
&nbsp;|&nbsp;
Nov. 19, 2023 8:58 PM PST
</p></div>
<p>
Photo: OpenAI co-founders Sam Altman and Ilya Sutskever speak together in Tel Aviv on June 5, 2023.
</p>
</div><div>
<p>Sam Altman won’t return as CEO of OpenAI, despite <a href="https://www.theinformation.com/articles/openai-optimistic-it-can-bring-back-sam-altman-greg-brockman?rc=c48ukx">efforts by the company’s executives to bring him back</a>, according to co-founder and board director Ilya Sutskever. After a weekend of negotiations with the board of directors that fired him Friday, as well as with its remaining leaders and top investors, Altman will not return to the startup he co-founded in 2015, Sutskever told staff. Emmett Shear, co-founder of Amazon-owned video streaming site Twitch, will take over as interim CEO, Sutskever said.</p>
<p>The decision—which flew in the face of comments OpenAI executives shared with staff on&nbsp;Saturday&nbsp;and early Sunday—could deepen a crisis precipitated by the board’s sudden ouster of Altman and its removal of President Greg Brockman from the board Friday. Brockman, a key engineer behind the company's successes, resigned later that day, followed by <a href="https://www.theinformation.com/articles/three-senior-openai-researchers-resign-as-crisis-deepens?rc=c48ukx">three senior researchers</a>, threatening to set off a broader wave of departures to OpenAI’s rivals, including Google, and to a new venture Altman has been plotting in the wake of his firing.</p>
<p>Distraught employees streamed out of OpenAI headquarters in San Francisco a little after 9 p.m., shortly after the decision was announced internally. One of the people who left the office appeared to be research chief Bob McGrew, who was one of the many executives working to bring Altman and Brockman back.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cruise co-founder and CEO Kyle Vogt resigns (245 pts)]]></title>
            <link>https://techcrunch.com/2023/11/19/cruise-co-founder-and-ceo-kyle-vogt-resigns/</link>
            <guid>38341466</guid>
            <pubDate>Mon, 20 Nov 2023 02:30:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/11/19/cruise-co-founder-and-ceo-kyle-vogt-resigns/">https://techcrunch.com/2023/11/19/cruise-co-founder-and-ceo-kyle-vogt-resigns/</a>, See on <a href="https://news.ycombinator.com/item?id=38341466">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Kyle Vogt, the serial entrepreneur who co-founded and led Cruise from a startup in a garage through its <a href="https://techcrunch.com/2016/03/11/gm-buys-self-driving-tech-startup-cruise-as-part-of-a-plan-to-make-driverless-cars/" target="_blank" rel="noopener">acquisition</a> and ownership by General Motors, has resigned, according to an email sent to employees Sunday evening that TechCrunch has viewed.</p>
<p>In a separate internal email, also viewed by TechCrunch, GM Chair and CEO Mary Barra announced that Mo Elshenawy, who is executive vice president of engineering at Cruise, will serve as president and CTO for Cruise. Craig Glidden, a Cruise board member and GM’s EVP of legal and policy who was <a href="https://techcrunch.com/2023/11/14/gm-inserts-exec-at-cruise-as-safety-review-expands-manual-self-driving-paused/" target="_blank" rel="noopener">recently put in charge</a> of as chief administrative officer at Cruise, will continue in that role. Jon McNeill, a member of GM’s board, has been appointed vice chairman of the Cruise board. McNeill, who joined the Cruise board recently and was previously chief operating officer at Lyft and president of Tesla, will now serve alongside Cruise Board Chair Mary Barra. A statement from a Cruise spokesperson confirms Barra’s email.</p>
<p>As of Sunday, no one had been named to the CEO spot.</p>
<p><span>The executive shakeup comes a less than a month after the California Department of Motor Vehicles </span><a href="https://techcrunch.com/2023/10/24/dmv-immediately-suspends-cruises-robotaxi-permit-in-california/"><span>suspended Cruise’s permits</span></a><span> to operate self-driving vehicles on public roads after an </span><a href="https://techcrunch.com/2023/10/03/in-latest-cruise-incident-video-shows-pedestrian-struck-by-human-driven-car-then-run-over-by-robotaxi/"><span>October 2 incident</span></a><span> that saw a pedestrian – who had been initially hit by a human-driven car and landed in the path of a Cruise robotaxi – run over and dragged 20 feet by the AV. A video, which TechCrunch viewed a day after the incident, showed the robotaxi braking aggressively and coming to a stop over the woman. The DMV’s order of suspension stated that Cruise withheld about seven seconds of video footage, which showed the robotaxi then attempting to pull over and subsequently dragging the woman 20 feet.&nbsp;</span></p>
<p>Vogt’s email sent to all employees — and viewed by TechCrunch — reads:</p>
<blockquote><p>I have resigned from my position as CEO of Cruise.</p>
<p>The last 10 years have been amazing, and I’m grateful to everyone who helpeds Cruise along the way. The startup I launched in my garage has given over 250,000 driverless rides across several cities, with each ride inspiring people with a small taste of the future.</p>
<p>Cruise is still just getting started, and I believe it has a great future ahead. You all are brilliant, driven and resilient. I’m deeply saddened I won’t be working next to you anymore. However, I know you’re executing against a very strong, multi-year technology roadmap and exciting product vision, and I’m thrilled to see what Cruise has in store in its next chapter!</p>
<p>Cruisers, you’ve got this! Regardless of what originally brought you to work on AVs, remember why this work matters. The status quo on our roads sucks, but together we’ve proven there is something far better around the corner.</p></blockquote>
<p>Vogt also posted a message Sunday evening on the social media site X that used similar language as the internal email. He ended the social media thread <a href="https://x.com/kvogt/status/1726428105873764714?s=20" target="_blank" rel="noopener">with this message</a> “As for what’s next for me, I plan to spend time with my family and explore some new ideas. Thanks for the great ride!”</p>
<p>Barra’s internal email, which was sent about 15 minutes after Vogt sent his, thanked him for this “tremendous vision, passion and dedication over the past decade.” The emailed continued:</p>
<blockquote><p>“The Cruise Board understands and respects his decision to resign as CEO, and we wish him well in his next chapter. We continue to believe strongly in Cruise’s mission and the potential of its transformative technology as we look to make transportation safer, cleaner and more accessible.”</p></blockquote>
<p>Barra later emphasized that “the board and I also want you to know that we are intensely focused on setting Cruise up for long-term success. Public trust is essential to this. As we work to rebuild that trust, safety, transparency and accountability will be our north stars.”</p>
<p><span>Morale at Cruise has been low since the October 2 incident, with employees pointing the finger at poor management that didn’t prioritize safety at the company. Without commercial permits to operate in San Francisco and an internal decision to <a href="https://techcrunch.com/2023/10/26/cruise-pauses-all-driverless-robotaxi-operations-to-rebuild-public-trust/" target="_blank" rel="noopener">pause its driverless fleets</a> in other states, the company <a href="https://techcrunch.com/2023/11/09/cruise-begins-layoffs-starting-with-workers-who-supported-driverless-operations/#:~:text=Cruise%20co%2Dfounder%20and%20CEO,many%20contingent%20workers%20it%20employed." target="_blank" rel="noopener">laid off contract workers</a>, further deepening the malaise.&nbsp;</span></p>
<p>The initial layoffs included contract workers who had jobs cleaning, charging and maintaining the vehicles as well as answering customer support inquiries. Not all contingent workers, who are employed by a third party, were laid off. However, more layoffs are expected at the company that employs&nbsp;about 4,000 full-time employees.</p>
<p><span>Employee discontent was further inflamed last week when </span><a href="https://techcrunch.com/2023/11/16/cruise-suspends-employee-stock-program-corp-bonuses-moved-up/"><span>Cruise suspended its employee share-selling program </span></a><span>for the fourth quarter. Sources who spoke to TechCrunch on the condition of anonymity said they could lose upwards of tens of thousands of dollars as a result of this decision.</span></p>
<p><span>Over the weekend, Cruise backtracked on that move. Vogt sent out an email Saturday saying that certain employees could sell a limited number of shares in a one-time opportunity. Vogt didn’t provide many details but said the company was developing a plan to conduct a new tender offer to provide restricted stock unit liquidity to mitigate potential tax implications. TechCrunch has viewed the email.</span></p>
<p><span>Vogt went on to offer his staff a blanket apology for “the situation Cruise is in today.”</span></p>
<p><span>Vogt and Cruise’s chief product officer Dan Kan founded the autonomous vehicle company in 2013. Initially, the pair had focused on kits that could retrofit a vehicle and turn it into a self-driving car. The startup soon pivoted to a different business model. GM took interest and <a href="https://fortune.com/2016/03/11/gm-buying-self-driving-tech-startup-for-more-than-1-billion/" target="_blank" rel="noopener">acquired the company</a> in March 2016 in a deal of cash and stock valued at more than $1 billion.&nbsp;</span></p>
<p><span>Previously, Vogt had co-founded Justin.tv, a website that allowed anyone to broadcast video online, Twitch, a live-streaming platform, and Socialcam, a mobile social video app.&nbsp;</span><span>Twitch was acquired by Amazon in 2014 for $970 million, and Socialcam by Autodesk for $60 million in 2012.</span></p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inside The Chaos at OpenAI (105 pts)]]></title>
            <link>https://www.theatlantic.com/technology/archive/2023/11/sam-altman-open-ai-chatgpt-chaos/676050/</link>
            <guid>38341399</guid>
            <pubDate>Mon, 20 Nov 2023 02:23:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/technology/archive/2023/11/sam-altman-open-ai-chatgpt-chaos/676050/">https://www.theatlantic.com/technology/archive/2023/11/sam-altman-open-ai-chatgpt-chaos/676050/</a>, See on <a href="https://news.ycombinator.com/item?id=38341399">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header data-event-module="hero"><div><div><p>Sam Altman’s weekend of shock and drama began a year ago, with the release of ChatGPT.</p></div><div><figure><div data-flatplan-lead_figure_media="true"><picture><source media="(prefers-reduced-motion)" srcset="https://cdn.theatlantic.com/thumbor/kf59s-eip-xTuqGFrIQLPQ3eKoY=/0x0:960x540/750x422/filters:still()/media/img/mt/2023/11/chat_gpt_md_1/original.gif 750w, https://cdn.theatlantic.com/thumbor/5hWkpOcbcG780y1KWCsjsgLeU3Y=/0x0:960x540/828x466/filters:still()/media/img/mt/2023/11/chat_gpt_md_1/original.gif 828w, https://cdn.theatlantic.com/thumbor/RCuxjlwQY8gyfnO2TRnT9fr3rtY=/0x0:960x540/960x540/filters:still()/media/img/mt/2023/11/chat_gpt_md_1/original.gif 960w" sizes="(min-width: 976px) 976px, 100vw"><img alt="OpenAI's logo" sizes="(min-width: 976px) 976px, 100vw" srcset="https://cdn.theatlantic.com/thumbor/nzd2x8hkV_cJ-qrjKVy575D5mg8=/0x0:960x540/750x422/media/img/mt/2023/11/chat_gpt_md_1/original.gif 750w, https://cdn.theatlantic.com/thumbor/6M1PJ9iYk0wGSxTf_x2VbrKTyDw=/0x0:960x540/828x466/media/img/mt/2023/11/chat_gpt_md_1/original.gif 828w, https://cdn.theatlantic.com/thumbor/tEkKh5_1HK6S13Za_9LQ0wgHFu4=/0x0:960x540/960x540/media/img/mt/2023/11/chat_gpt_md_1/original.gif 960w" src="https://cdn.theatlantic.com/thumbor/tEkKh5_1HK6S13Za_9LQ0wgHFu4=/0x0:960x540/960x540/media/img/mt/2023/11/chat_gpt_md_1/original.gif" width="960" height="540"></picture></div><figcaption data-flatplan-lead_figure_caption="true">Illustration by Joanne Imperio / The Atlantic</figcaption></figure></div></div><div><p><time datetime="2023-11-20T01:58:18Z" data-flatplan-timestamp="true">November 19, 2023, 8:58 PM ET</time></p></div><gpt-ad format="injector" sizes-at-0="mobile-wide" targeting-pos="injector-article-start" sizes-at-976="desktop-wide"></gpt-ad></header><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true"><small><em>Updated at 12:32 a.m. ET on November 20, 2023</em></small></p><p data-flatplan-paragraph="true">To truly understand the events of the past 48 hours—the shocking, sudden ousting of OpenAI’s CEO, Sam Altman, arguably the figurehead of the generative-AI revolution, followed by reports that the company was in talks to bring him back, and then yet another shocking revelation that he in fact would not return—one must understand that OpenAI is not a technology company. At least, not like other epochal companies of the internet age, such as Meta, Google, and Microsoft.</p><p data-flatplan-paragraph="true">OpenAI was deliberately structured to resist the values that drive much of the tech industry—a relentless pursuit of scale, a build-first-ask-questions-later approach to launching consumer products. It was founded in 2015 as a nonprofit dedicated to the creation of artificial general intelligence, or AGI, that should benefit “humanity as a whole.” (AGI, in the company’s telling, would be advanced enough to outperform any person at “most economically valuable work”—just the kind of cataclysmically powerful tech that demands a responsible steward.) In this conception, OpenAI would operate more like a research facility or a think tank. The company’s charter bluntly <a data-event-element="inline link" href="https://openai.com/charter">states</a> that OpenAI’s “primary fiduciary duty is to humanity,” not to investors or even employees.</p><p data-flatplan-paragraph="true">That model didn’t exactly last. In 2019, OpenAI launched a subsidiary with a “capped profit” model that could raise money, attract top talent, and inevitably build commercial products. But the nonprofit board maintained total control. This corporate minutiae is central to the story of OpenAI’s meteoric rise and Altman’s shocking fall. Altman’s <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2023/11/sam-altman-open-ai-fallout/676046/">dismissal</a> by OpenAI’s board on Friday was the culmination of a power struggle between the company’s two ideological extremes—one group born from Silicon Valley techno optimism, energized by rapid commercialization; the other steeped in fears that AI represents an existential risk to humanity and must be controlled with extreme caution. For years, the two sides managed to coexist, with some <a data-event-element="inline link" href="https://aibusiness.com/verticals/eleven-openai-employees-break-off-to-establish-anthropic-raise-124m">bumps along the way</a>.</p><p data-flatplan-paragraph="true">This tenuous equilibrium broke one year ago almost to the day, according to current and former employees, thanks to the release of the very thing that brought OpenAI to global prominence: ChatGPT. From the outside, ChatGPT looked like one of the most successful product launches of all time. It <a data-event-element="inline link" href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/">grew faster</a> than any other consumer app in history, and it seemed to single-handedly redefine how millions of people understood the threat—and promise—of automation. But it sent OpenAI in polar-opposite directions, widening and worsening the already present ideological rifts. ChatGPT supercharged the race to create products for profit as it simultaneously heaped unprecedented pressure on the company’s infrastructure and on the employees focused on assessing and mitigating the technology’s risks. This strained the already tense relationship between OpenAI’s factions—which Altman referred to, in a 2019 staff email, as “tribes.”</p><p data-flatplan-paragraph="true">In conversations between <em>The Atlantic</em> and 10 current and former employees at OpenAI, a picture emerged of a transformation at the company that created an unsustainable division among leadership. (We agreed not to name any of the employees—all told us they fear repercussions for speaking candidly to the press about OpenAI’s inner workings.) Together, their accounts illustrate how the pressure on the for-profit arm to commercialize grew by the day, and clashed with the company’s stated mission, until everything came to a head with ChatGPT and other product launches that rapidly followed. “After ChatGPT, there was a clear path to revenue and profit,” one source told us. “You could no longer make a case for being an idealistic research lab. There were customers looking to be served here and now.”</p><p data-flatplan-paragraph="true">We still do not know exactly why Altman was fired, nor do we fully understand what his future is. Altman, who visited OpenAI’s headquarters in San Francisco this afternoon to discuss a possible deal, has not responded to our requests for comment. The board announced <a data-event-element="inline link" href="https://openai.com/blog/openai-announces-leadership-transition">on Friday</a> that “a deliberative review process” had found “he was not consistently candid in his communications with the board,” leading it to lose confidence in his ability to be OpenAI’s CEO. An internal memo from the COO to employees, confirmed by an OpenAI spokesperson, subsequently said that the firing had resulted from a "breakdown in communications” between Altman and the board rather than “malfeasance or anything related to our financial, business, safety, or security/privacy practices.” But no concrete, specific details have been given. What we do know is that the past year at OpenAI was chaotic and defined largely by a stark divide in the company’s direction.</p><hr><p data-flatplan-paragraph="true">In the fall of 2022, before the launch of ChatGPT, all hands were on deck at OpenAI to prepare for the release of its most powerful large language model to date, GPT-4. Teams scrambled to refine the technology, which could write fluid prose and code, and describe the content of images. They worked to prepare the necessary infrastructure to support the product and refine policies that would determine which user behaviors OpenAI would and would not tolerate.</p><p data-flatplan-paragraph="true">In the midst of it all, rumors began to spread within OpenAI that its competitors at Anthropic were developing a chatbot of their own. The rivalry was personal: Anthropic had formed after a faction of employees left OpenAI in 2020, reportedly because of concerns over how fast the company was releasing its products. In November, OpenAI leadership told employees that they would need to launch a chatbot in a matter of weeks, according to three people who were at the company. To accomplish this task, they instructed employees to publish an existing model, GPT-3.5, with a chat-based interface. Leadership was careful to frame the effort not as a product launch but as a “low-key research preview.” By putting GPT-3.5 into people’s hands, Altman and other executives said, OpenAI could gather more data on how people would use and interact with AI, which would help the company inform GPT-4’s development. The approach also aligned with the company’s broader deployment strategy, to gradually release technologies into the world for people to get used to them. Some executives, including Altman, started to parrot the same line: OpenAI needed to get the “data flywheel” going.</p><p data-flatplan-paragraph="true">A few employees expressed discomfort about rushing out this new conversational model. The company was already stretched thin by preparation for GPT-4 and ill-equipped to handle a chatbot that could change the risk landscape. Just months before, OpenAI had brought online a new traffic-monitoring tool to track basic user behaviors. It was still in the middle of fleshing out the tool’s capabilities to understand how people were using the company’s products, which would then inform how it approached mitigating the technology’s possible dangers and abuses. Other employees felt that turning GPT-3.5 into a chatbot would likely pose minimal challenges, because the model itself had already been sufficiently tested and refined.</p><p data-flatplan-paragraph="true">The company pressed forward and launched ChatGPT on November 30. It was such a low-key event that many employees who weren’t directly involved, including those in safety functions, didn’t even realize it had happened. Some of those who were aware, according to one employee, had started a betting pool, wagering how many people might use the tool during its first week. The highest guess was 100,000 users. OpenAI’s president <a data-event-element="inline link" href="https://twitter.com/gdb/status/1599683104142430208?lang=en">tweeted</a> that the tool hit 1 million within the first five days. The phrase <em>low-key research preview</em> became an instant meme within OpenAI; employees turned it into laptop stickers.</p><p data-flatplan-paragraph="true">ChatGPT’s runaway success placed extraordinary strain on the company. Computing power from research teams was redirected to handle the flow of traffic. As traffic continued to surge, OpenAI’s servers crashed repeatedly; the traffic-monitoring tool also repeatedly failed. Even when the tool was online, employees struggled with its limited functionality to gain a detailed understanding of user behaviors.</p><p data-flatplan-paragraph="true">Safety teams within the company pushed to slow things down. These teams worked to refine ChatGPT to refuse certain types of abusive requests and to respond to other queries with more appropriate answers. But they struggled to build features such as an automated function that would ban users who repeatedly abused ChatGPT. In contrast, the company’s product side wanted to build on the momentum and double down on commercialization. Hundreds more employees were hired to aggressively grow the company’s offerings. In February, OpenAI released a paid version of ChatGPT; in March, it quickly followed with an API tool, or application programming interface, that would help businesses integrate ChatGPT into their products. Two weeks later, it finally launched GPT-4.</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/technology/archive/2023/03/gpt4-release-rumors-hype-future-iterations/673396/">Read: ChatGPT changed everything. Now its follow-up is here.</a></p><p data-flatplan-paragraph="true">The slew of new products made things worse, according to three employees who were at the company at that time. Functionality on the traffic-monitoring tool continued to lag severely, providing limited visibility into what traffic was coming from which products that ChatGPT and GPT-4 were being integrated into via the new API tool, which made understanding and stopping abuse even more difficult. At the same time, fraud began surging on the API platform as users created accounts at scale, allowing them to cash in on a $20 credit for the pay-as-you-go service that came with each new account. Stopping the fraud became a top priority to stem the loss of revenue and prevent users from evading abuse enforcement by spinning up new accounts: Employees from an already small trust-and-safety staff were reassigned from other abuse areas to focus on this issue. Under the increasing strain, some employees struggled with mental-health issues. Communication was poor. Co-workers would find out that colleagues had been fired only after noticing them disappear on Slack.</p><p data-flatplan-paragraph="true">The release of GPT-4 also frustrated the alignment team, which was focused on further-upstream AI-safety challenges, such as developing various techniques to get the model to follow user instructions and prevent it from spewing toxic speech or “hallucinating”—confidently presenting misinformation as fact. Many members of the team, including a growing contingent fearful of the existential risk of more-advanced AI models, felt uncomfortable with how quickly GPT-4 had been launched and integrated widely into other products. They believed that the AI safety work they had done was insufficient.</p><hr><p data-flatplan-paragraph="true">The tensions boiled over at the top. As Altman and OpenAI President Greg Brockman encouraged more commercialization, the company’s chief scientist, Ilya Sutskever, grew more concerned about whether OpenAI was upholding the governing nonprofit’s mission to create beneficial AGI. Over the past few years, the rapid progress of OpenAI’s large language models had made Sutskever more confident that AGI would arrive soon and thus more focused on preventing its possible dangers, according to Geoffrey Hinton, an AI pioneer who served as Sutskever’s doctoral adviser at the University of Toronto and has remained close with him over the years. (Sutskever did not respond to a request for comment.)</p><p data-flatplan-paragraph="true">Anticipating the arrival of this all-powerful technology, Sutskever began to behave like a spiritual leader, three employees who worked with him told us. His constant, enthusiastic refrain was “feel the AGI,” a reference to the idea that the company was on the cusp of its ultimate goal. At OpenAI’s 2022 holiday party, held at the California Academy of Sciences, Sutskever led employees in a chant: “Feel the AGI! Feel the AGI!” The phrase itself was popular enough that OpenAI employees created a special “Feel the AGI” reaction emoji in Slack.</p><p data-flatplan-paragraph="true">The more confident Sutskever grew about the power of OpenAI’s technology, the more he also allied himself with the existential-risk faction within the company. For a leadership offsite this year, according to two people familiar with the event, Sutskever commissioned a wooden effigy from a local artist that was intended to represent an “unaligned” AI—that is, one that does not meet a human’s objectives. He set it on fire to symbolize OpenAI’s commitment to its founding principles. In July, OpenAI announced the creation of a so-called superalignment team with Sutskever co-leading the research. OpenAI would expand the alignment team’s research to develop more upstream AI-safety techniques with a dedicated 20 percent of the company’s existing computer chips, in preparation for the possibility of AGI arriving in this decade, <a data-event-element="inline link" href="https://openai.com/blog/introducing-superalignment">the company said</a>.</p><p data-flatplan-paragraph="true">Meanwhile, the rest of the company kept pushing out new products. Shortly after the formation of the superalignment team, OpenAI released the powerful image generator DALL-E 3. Then, earlier this month, the company held its first “developer conference,” where Altman launched GPTs, custom versions of ChatGPT that can be built without coding. These once again had major problems: OpenAI experienced a series of outages, including a massive one across ChatGPT and its APIs, according to <a data-event-element="inline link" href="https://status.openai.com/history">company updates</a>. Three days after the developer conference, Microsoft briefly restricted employee access to ChatGPT over security concerns, <a data-event-element="inline link" href="https://www.cnbc.com/2023/11/09/microsoft-restricts-employee-access-to-openais-chatgpt.html">according</a> to CNBC.</p><p data-flatplan-paragraph="true">Through it all, Altman pressed onward. In the days before his firing, he was drumming up hype about OpenAI’s continued advances. The company had begun to work on GPT-5, <a data-event-element="inline link" href="https://www.ft.com/content/dd9ba2f6-f509-42f0-8e97-4271c7b84ded">he told the <em>Financial Times</em></a>, before alluding days later to something incredible in store at <a data-event-element="inline link" href="https://sfstandard.com/2023/11/17/openai-sam-altman-fired-apec-talk/">the APEC summit</a>. “Just in the last couple of weeks, I have gotten to be in the room, when we sort of push the veil of ignorance back and the frontier of discovery forward,” he said. “Getting to do that is a professional honor of a lifetime.” According to <a data-event-element="inline link" href="https://www.bloomberg.com/news/articles/2023-11-19/altman-sought-billions-for-ai-chip-venture-before-openai-ouster?srnd=premium&amp;sref=BGQFqz7X">reports</a>, Altman was also looking to raise billions of dollars from Softbank and Middle Eastern investors to build a chip company to compete with Nvidia and other semiconductor manufacturers, as well as lower costs for OpenAI. In a year, Altman had helped transform OpenAI from a hybrid research company into a Silicon Valley tech company in full-growth mode.</p><hr><p data-flatplan-paragraph="true">In this context, it is easy to understand how tensions boiled over. OpenAI’s charter placed principle ahead of profit, shareholders, and any individual. The company was founded in part by the very contingent that Sutskever now represents—those fearful of AI’s potential, with beliefs at times seemingly rooted in the realm of science fiction—and that also makes up a portion of OpenAI’s current board. But Altman, too, positioned OpenAI’s commercial products and fundraising efforts as a means to the company’s ultimate goal. He told employees that the company’s models were still early enough in development that OpenAI ought to commercialize and generate enough revenue to ensure that it could spend without limits on alignment and safety concerns; ChatGPT is <a data-event-element="inline link" href="https://www.theinformation.com/articles/openais-revenue-crossed-1-3-billion-annualized-rate-ceo-tells-staff">reportedly</a> on pace to generate more than $1 billion a year.</p><p data-flatplan-paragraph="true">Altman’s firing can be seen as a stunning experiment in OpenAI’s unusual structure. It’s possible this experiment is now unraveling the company as we’ve known it, and shaking up the direction of AI along with it. If Altman had returned to the company via pressure from investors and an outcry from current employees, the move would have been a massive consolidation of power. It would have suggested that, despite its charters and lofty credos, OpenAI was just a traditional tech company after all.</p><p data-flatplan-paragraph="true">Even with Altman out, this tumultuous weekend showed just how few people have a say in the progression of what might be the most consequential technology of our age. AI’s future is being determined by an ideological fight between wealthy techno-optimists, zealous doomers, and multibillion-dollar companies. The fate of OpenAI might hang in the balance, but the company’s conceit—the openness it is named after—showed its limits. The future, it seems, will be decided behind closed doors.</p><hr><p data-flatplan-paragraph="true"><small><em>This article previously stated that GPT-4 can create images. It cannot.</em></small></p></section><gpt-ad format="injector" sizes-at-0="mobile-wide,native,house" targeting-pos="injector-most-popular" sizes-at-976="desktop-wide,native,house"></gpt-ad></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Friends don't let friends make bad graphs (283 pts)]]></title>
            <link>https://github.com/cxli233/FriendsDontLetFriends</link>
            <guid>38340226</guid>
            <pubDate>Mon, 20 Nov 2023 00:13:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cxli233/FriendsDontLetFriends">https://github.com/cxli233/FriendsDontLetFriends</a>, See on <a href="https://news.ycombinator.com/item?id=38340226">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Friends Don't Let Friends Make Bad Graphs</h2>
<p dir="auto"><a href="https://doi.org/10.5281/zenodo.7542491" rel="nofollow"><img src="https://camo.githubusercontent.com/fa2b9946e59edc4cb0a548ca370b17facfe6ebea744f655db524dd9b2598cb9e/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e373534323439312e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.7542491.svg"></a></p>
<p dir="auto">Friends don't let friends make certain types of data visualization - What are they and why are they bad.</p>
<ul dir="auto">
<li>Author: Chenxin Li, postdoctoral associate at Center for Applied Genetic Technologies, University of Georgia.</li>
<li>Contact: <a href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Chenxin.Li@uga.edu">Chenxin.Li@uga.edu</a> | <a href="https://twitter.com/ChenxinLi2" rel="nofollow">@ChenxinLi2</a></li>
</ul>
<p dir="auto">This is an <em>opinionated</em> essay about good and bad practices in data visualization.
Examples and explanations are below.</p>
<p dir="auto">The <code>Scripts/</code> directory contains <code>.Rmd</code> files that generate the graphics shown below.
It requires R, RStudio, and the rmarkdown package.</p>
<ul dir="auto">
<li>R: <a href="https://cran.r-project.org/bin/" rel="nofollow">R Download</a></li>
<li>RStudio: <a href="https://www.rstudio.com/products/rstudio/download/" rel="nofollow">RStudio Download</a></li>
<li>rmarkdown can be installed using the install packages interface in RStudio</li>
</ul>
<h2 tabindex="-1" dir="auto">Table of contents</h2>
<ol dir="auto">
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#1-friends-dont-let-friends-make-bar-plots-for-means-separation">Friends Don't Let Friends Make Bar Plots For Mean Separation</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#2-friends-dont-let-friends-make-violin-plots-for-small-sample-sizes">Friends Don't Let Friends Make Violin Plots for Small Sample Sizes</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#3-friends-dont-let-friends-use-bidirectional-color-scales-for-unidirectional-data">Friends Don't Let Friends Use Bidirectional Color Scales for Unidirectional Data</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#4-friends-dont-let-friends-make-bar-plot-meadow">Friends Don't Let Friends Make Bar Plot Meadow</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#5-friends-dont-let-friends-make-heatmap-without-considering-reordering-rows--columns">Friends Don't Let Friends Make Heatmap without Reordering Rows &amp; Columns</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#6-friends-dont-let-friends-make-heatmap-without-checking-outliers">Friends Don't Let Friends Make Heatmap without Checking Outliers</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#7-friends-dont-let-friends-forget-to-check-data-range-at-each-factor-level">Friends Don't Let Friends Forget to Check Data Range at Each Factor Level</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#8-friends-dont-let-friends-make-network-graphs-without-trying-different-layouts">Friends Don't Let Friends Make Network Graphs without Trying Different Layouts</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#9-friends-dont-let-friends-confuse-position-based-visualizations-with-length-based-visualizations">Friends Don't Let Friends Confuse Position and Length Based Visualizations</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#10-friends-dont-let-friends-make-pie-chart">Friends Don't Let Friends Make Pie Charts</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#11-friends-dont-let-friends-make-concentric-donuts">Friends Don't Let Friends Make Concentric Donuts</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends#12-friends-dont-let-friends-use-redgreen-and-rainbow-color-scales">Friends Don't Let Friends Use Red/green and Rainbow for Color Scales</a></li>
<li><a href="https://github.com/cxli233/FriendsDontLetFriends/tree/main#13-friends-dont-let-friends-forget-to-reorder-stacked-bar-plot">Friends Don't Let Friends Forget to Reorder Stacked Bar Plot</a></li>
</ol>
<h2 tabindex="-1" dir="auto">1. Friends Don't Let Friends Make Bar Plots for Means Separation</h2>
<p dir="auto">This has to be the first one.
Means separation plots are some of the most common in scientific publications.
We have two or more groups, which contains multiple observations; they may have different means, variances, and distributions.
The task of the visualization is to show the means and the spread (dispersion) of the data.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/dont_bar_plot.png"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/dont_bar_plot.png" alt="No Bar Plots for Means Separation"></a></p>
<p dir="auto">In this example, two groups have similar means and standard deviations, but quite different distributions. <strong>Are they really "the same"?</strong>
Just don't use bar plot for means separation, or at least check a couple things before settling down on a bar plot.</p>
<p dir="auto">It's worth mentioning that I was inspired by many researchers who have tweeted on the limitation of bar graphs.
Here is a pulication: <a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002128" rel="nofollow">Weissgerber et al., 2015, PLOS Biology</a>.</p>
<h2 tabindex="-1" dir="auto">2. Friends Don't Let Friends Make Violin Plots for Small Sample Sizes</h2>
<p dir="auto">This is quite common in the literature as well, but unfortunately, violin plots (or any sort of smoothed distribution curves) make no sense for small n.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/Beware_of_small_n_box_violin_plot.png"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/Beware_of_small_n_box_violin_plot.png" alt="Beware of Violin Plots for Small Sample Sizes"></a></p>
<p dir="auto">Distributions and quartiles can vary widely with small n, even if the underlying observations are similar.
Distribution and quartiles are only meaningful with large n.
I did an experiment before, where I sampled the <em>same</em> normal distribution several times and computed the quartiles for each sample.
The quartiles only stablize when n gets larger than 50.</p>
<h2 tabindex="-1" dir="auto">3. Friends Don't Let Friends Use Bidirectional Color Scales for Unidirectional Data</h2>
<p dir="auto">Excuse my language, but this is a truly data visualization sin, and again quite common.
I can understand why this error is common, because it appears that many of us have not spent a lot of thoughts on this issue.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/ColorScales.svg"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/ColorScales.svg" alt="Are You Using the Right Color Scale for Your Data?"></a></p>
<p dir="auto">Color scales are pretty, but we have to be extra careful.
When color scales (or color gradients) are used to represent numerical data, the darkest and lightest colors should have special meanings.
You can decide what those special meanings are: e.g., max, min, mean, zero. But they should represent something meaningful.
A data visualization sin for heat maps/color gradients is when the lightest or darkers colors are some arbitrary numbers.
<em>This is as bad as the longest bar in a bar chart not being the largest value.</em> Can you imagine that?</p>
<h2 tabindex="-1" dir="auto">4. Friends Don't Let Friends Make Bar Plot Meadow</h2>
<p dir="auto">We talked about no bar charts for mean separation, but this is a different issue.
It has to do with presenting results of a multi-factorial experiment.
Bar plot meadows are very common in scientific publications and unfortunately also <em>ineffective</em> in communicating the results.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/AvoidBarPlotMeadow.png"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/AvoidBarPlotMeadow.png" alt="Horrendous Giant Bar Plot vs. Better Designed Plot"></a></p>
<p dir="auto">Data from: <a href="https://link.springer.com/article/10.1186/s12870-020-2243-7" rel="nofollow">Matand et al., 2020, BMC Plant Biology</a></p>
<p dir="auto">Bar plot meadows are common because multi-factorial experiments are common.
However, a bar plot meadow is poorly designed for its purpose.
To communicate results of a multi-factorial experiment, it requires thoughtful designs regarding grouping/faceting by factors of interest.</p>
<p dir="auto">In this example, I focus on comparing the effect of <code>Treatment</code> &amp; <code>Explant</code> on <code>Response</code> at the level of each <code>Variety</code>.
However, if the focus is the effect of <code>Treatment</code> &amp; <code>Variety</code> on <code>Response</code> at the level of each <code>Exaplant</code>, then it will require a different layout.</p>
<h2 tabindex="-1" dir="auto">5. Friends Don't Let Friends Make Heatmap without (Considering) Reordering Rows &amp; Columns</h2>
<p dir="auto">Heatmaps are very common in scientific publications, and <em>very very</em> common in omics papers.
However, for heatmaps to be effective, we have to consider the ordering of rows &amp; columns.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/Reorder_rows_and_columns_for_heatmap.png"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/Reorder_rows_and_columns_for_heatmap.png" alt="A Heatmap before and after reordering rows and columns"></a></p>
<p dir="auto">In this example, I have cells as columns and features as rows. Grids are showing z scores.
It is impossible to get anything useful out of the heatmap without reordering rows and columns.
We can reorder rows and columns using clustering, but that is not the only way.
Of course, if the rows and columns are mapping to physical entities (rows and columns of a 96-well plate), then you can't reorder them.
But it is a very good idea to at least consider reordering rows and columns.</p>
<p dir="auto">Data from: <a href="https://www.biorxiv.org/content/10.1101/2022.07.04.498697v1" rel="nofollow">Li et al., 2022, BioRxiv</a></p>
<h2 tabindex="-1" dir="auto">Bonus: heatmaps can be very pretty</h2>
<p dir="auto">...if you are good are reordering rows/columns and choosing color gradients.
Here is an example "abstract aRt" generated from simulated data.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/Abstract_R_2022_11_24.svg"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/Abstract_R_2022_11_24.svg" alt="aRt with Heatmap"></a></p>
<p dir="auto">R code for this aRt piece can be found <a href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Scripts/Abstract_aRt.R">here</a>.</p>
<p dir="auto">For a tutorial on how to reorder rows and columns of a heatmap, see this <a href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Heatmap_tutorial.md">markdown file</a>.</p>
<h2 tabindex="-1" dir="auto">6. Friends Don't Let Friends Make Heatmap without Checking Outliers</h2>
<p dir="auto">Outliers in heatmap can really change how we perceive and interpret the visualization.
This generalizes to all sort of visualizations that use colors to represent numeric data.
Let me show you an example:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/Check_outliers_for_heatmap.svg"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/Check_outliers_for_heatmap.svg" alt="Did you check outliers"></a></p>
<p dir="auto">In this example, I have 2 observations. For each observations, I measured 20 features.
Without checking for outliers, it may appear that the 2 observations are overall similar, except at 2 features.
However, after maxing out the color scale around 95th percentile of the data, it reveals that the two observations are distinct across all features.</p>
<h2 tabindex="-1" dir="auto">7. Friends Don't Let Friends Forget to Check Data Range at Each Factor Level</h2>
<p dir="auto">This is a common issue that many of us have encountered.
In a multifactor experiment, sometimes the range of the response variable changes widely between different factor levels.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/Check_range_at_factor_level.svg"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/Check_range_at_factor_level.svg" alt="Did you check data range at each factor level"></a></p>
<p dir="auto">This hypothetical experiment measured 3 compounds across 2 groups (control vs. treatment).
Without checking data range for each compound, you will likely have missed that the treatment had a strong effect on compound 1.
This is because the concentration of compound 1 has a much narrower range than the other compounds in this experiment.</p>
<h2 tabindex="-1" dir="auto">8. Friends Don't Let Friends Make Network Graphs without Trying Different Layouts</h2>
<p dir="auto">Network graphs are common in scientific publications. They are super useful in presenting relationship data.
However, the apparence (not the topology) of the network can make a huge difference in determing if a network graph is effective.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/TryDifferentLayouts.svg"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/TryDifferentLayouts.svg" alt="Try different network layouts"></a></p>
<p dir="auto">Layouts can drastically change the appearance of networks, making them easier or harder to interpret.
Here are 3 network graphs from the same data. They look very different from each other.
Data from: <a href="https://www.biorxiv.org/content/10.1101/2022.07.04.498697v1" rel="nofollow">Li et al., 2022, BioRxiv</a></p>
<h2 tabindex="-1" dir="auto">9. Friends Don't Let Friends Confuse Position-based Visualizations with Length-based Visualizations</h2>
<p dir="auto">This is always the elephant in the room and the essence of many misleading visualizations.
In this example, I measured a response variable across 3 time points.
Two of the following graphs are fine, but one of them is a data visualization crime. Can you see why?</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/Position_and_length_based_visualizations.svg"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/Position_and_length_based_visualizations.svg" alt="Position vs. length based visualizations"></a></p>
<p dir="auto">In dot and line plots, values are represented by positions along the x and y axis.
The same idea applies to other position based visualizations, such as box plots.
In bar plots, values are represented by the distance from the x axis, and thus the length of the bar.</p>
<p dir="auto">The 3rd graph is not 0-based, which makes the bar length at time point 2 about 3x longer than that at time point 1.
In fact, the true difference in means is closer to 1.6x.
I hope you can see how confusing length and position based visualizations can lead to misleading graphs.</p>
<h2 tabindex="-1" dir="auto">Watch out for bar plots with broken axis</h2>
<p dir="auto">Broken axis may be useful for depicting data across a wide range of numeric values.
(Alternatively, log scaled axis can be used instead.)
Broken axis are fine for position based graphics, because the data are represented by positions along the axis.
However, we must be very careful with bar plots that have broken axis. Here is an example.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/Broken_axis.svg"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/Broken_axis.svg" alt="Broken axis"></a></p>
<p dir="auto">In this example, two graphs (left vs. right) are showing the same data.
However, by changing where the axis is broken, one can make certain bars looks longer or shorter.
In this example, the length of bar "d" can look <em>really</em> different.
The illusion of bar "d" being very short on the right graph boils down to bar plot being a length based graphics, not a position based graphics.</p>
<p dir="auto">Example R code for broken axis can be found <a href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Scripts/Broken_axis.R">here</a>.</p>
<h2 tabindex="-1" dir="auto">10. Friends Don't Let Friends Make Pie Chart</h2>
<p dir="auto">Pie chart is a common type of visualization for fractional data, where fractions add up to 100%.
This is achieved by dividing a circle into sectors, and the sectors add up to a full circle.
Pie charts have been criticized, because human are much worse in reading angles and area than reading lengths.
Here is a <a href="https://www.data-to-viz.com/caveat/pie.html" rel="nofollow">blog post</a> that explores that.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/dont_pie_chart.svg"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/dont_pie_chart.svg" alt="Don't make pie charts"></a></p>
<p dir="auto">In this example, we have two groups, each contains 4 sub-categories.
In classic pie charts, the angles (and thus arc lengths &amp; sector area) represent the data.
The problem is that it is <em>very</em> difficult to compare between groups.
We can visually simplify the pie chart into donut charts, where the data are now represented by arc lengths.
However, if we want to use lengths to represent the data, why don't we just unwrap the donut and make stacked bars?
In stacked bar graphs, bars are shown side-by-side and thus easier to compare across groups.</p>
<p dir="auto">Fun fact: the scripts underlying stacked bars are much simpler than those underlying the pie charts and donut charts.
If you want to produce sub-optimal graph types with ggplot, you actually have to work extra hard.</p>
<h2 tabindex="-1" dir="auto">11. Friends Don't Let Friends Make Concentric Donuts</h2>
<p dir="auto">In this example, we have 3 groups, each of which contains two sub-categories (Type I or Type II).</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/dont_concentric_donuts.svg"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/dont_concentric_donuts.svg" alt="Don't make concentric donuts"></a></p>
<p dir="auto">In concentric donuts, you might be tempted to say the data are represented by the arc lengths, which is in fact <strong>inaccurate</strong>.
The arc lengths on the outer rings are much longer than those in the inner rings.
Group 2 and Group 3 have the same exact values, but the arc lengths of Group 3 are much longer.
In fact the data are represented by the <em>arc angles</em>, which we are bad at reading.</p>
<p dir="auto">Since outer rings are longer, the ordering of the groups (which group goes to which ring) has a big impact on the impression of the plot.
It can lead to the apparent paradox where larger values have shorter arcs.
The better (and simpler!) alternative is just unwrap the donuts and make a good old stacked bar plot.
BTW, this is also my main issue with <a href="http://circos.ca/" rel="nofollow">circos plots</a> and other circular plot layouts.</p>
<h2 tabindex="-1" dir="auto">12. Friends Don't Let Friends Use Red/Green and Rainbow color scales</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/Color_blind_grey_scale_safe_heatmap.svg"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/Color_blind_grey_scale_safe_heatmap.svg" alt="are you making a &quot;safe&quot; heatmap?"></a></p>
<p dir="auto">Deuteranomaly is the most common type of red/green colorblindness, occurring in 1/16 male and 1/256 female.
Any color scales that use shades of red and shades of green in the same time would be a problem for a person with red/green colorblindness (third column of the figure).
In addition, red/green and rainbow do not preserve information well at all when printed on black/white (grey scale, second column in figure).
Many scientific software still use red/green or rainbow as the default color scales, which drives me crazy.
More "modern" color scales, such as <a href="https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html" rel="nofollow">viridis</a> are both colorblind-friendly and grey scale-safe (third row of figure).
And they look nice too.</p>
<h2 tabindex="-1" dir="auto">13. Friends Don't Let Friends Forget to Reorder Stacked Bar Plot</h2>
<p dir="auto">Stacked bar plots are useful for visualizing proportion data.
Stacked bar plots are commonly used to visualize community structure or population structure or admixture analysis.
This kind of visualization boils down to a collection of samples, where each sample contains multiple classes of members.
However, when we have many samples and many classes, stacked bar plots need to be optimized to be effective.
And by "optimize" I mean the grouping and ordering of samples.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Results/Reorder_stacked_bars.png"><img src="https://github.com/cxli233/FriendsDontLetFriends/raw/main/Results/Reorder_stacked_bars.png" alt="Reorder your stacked bars"></a></p>
<p dir="auto">Here we have an example data with 100 samples and 8 classes of member.
Due to the number of samples and classes, it is very hard to discern anything from this graph without optimizing the order of bars. What the heck am I looking at?
After reordering the bars, <strong>wow</strong>, that really made a difference, don't you think?
For a tutorial on how to optimize a stack bar plot, see <a href="https://github.com/cxli233/FriendsDontLetFriends/blob/main/Scripts/stacked_bars_optimization.Rmd">this script</a>.</p>
<h2 tabindex="-1" dir="auto">Conclusion (?)</h2>
<p dir="auto">That's it for now. I will update this when I have the time (and inspirations) to produce more examples.
Not sure what the next one will be, but stay tuned!</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Argentina elects 'shock therapy' libertarian Javier Milei as president (136 pts)]]></title>
            <link>https://www.reuters.com/world/americas/argentina-readies-vote-likely-presidential-election-thriller-2023-11-19/</link>
            <guid>38339845</guid>
            <pubDate>Sun, 19 Nov 2023 23:37:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/world/americas/argentina-readies-vote-likely-presidential-election-thriller-2023-11-19/">https://www.reuters.com/world/americas/argentina-readies-vote-likely-presidential-election-thriller-2023-11-19/</a>, See on <a href="https://news.ycombinator.com/item?id=38339845">Hacker News</a></p>
<div id="readability-page-1" class="page"><div confirmation_email_sent_page_url="/confirmationsent/" legal_marketing_page_url="https://legal.thomsonreuters.com/en/products/reuters-legal-news" sign_in_page_url="/account/sign-in/" sign_up_page_url="/account/register/sign-up/" terms_page_url="/info-pages/terms-of-use/"><p data-testid="paragraph-0">BUENOS AIRES, Nov 19 (Reuters) - Argentina elected right-wing libertarian <a data-testid="Link" href="https://www.reuters.com/world/americas/argentinas-javier-milei-tv-pundit-presidency-2023-11-19/" referrerpolicy="no-referrer-when-downgrade">Javier Milei</a> as its new president on Sunday, rolling the dice on an outsider with radical views to fix an economy battered by triple-digit inflation, a looming recession and rising poverty.</p><p data-testid="paragraph-1">Official results showed Milei with near 56% versus just over 44% for his rival, Peronist Economy Minister <a data-testid="Link" href="https://www.reuters.com/world/americas/sergio-massa-argentina-economy-chief-charms-voters-with-tax-cuts-payouts-2023-11-16/" referrerpolicy="no-referrer-when-downgrade">Sergio Massa</a>, who conceded in a speech. The result once again caught pollsters off guard who had predicted a closer race.</p><p data-testid="paragraph-2">In downtown Buenos Aires hundreds of Milei supporters honked horns and chanted his popular refrain against the political elite - "out with all of them" - as rock music played from speakers. Some people set off fireworks as excitement spread.</p><p data-testid="paragraph-3">"We came to celebrate this historic triumph," said Efrain Viveros, a 21-year-old student from the province of Salta. "I'm honestly ecstatic. Milei represents change, for the better. With Massa we'd have had no future, our future has returned."</p><p data-testid="paragraph-4">Massa in a speech said he had congratulated Milei and said the libertarian now needed to show his readiness to rule. "From tomorrow the responsibility of providing certainty belongs to Milei," he said.</p><p data-testid="paragraph-5">Milei is pledging economic shock therapy. His plans include shutting the central bank, ditching the peso, and slashing spending, potentially painful reforms that resonated with voters angry at the economic malaise.</p><p data-testid="paragraph-6">"Milei is the new thing, he's a bit of an unknown and it is a little scary, but it's time to turn over a new page," said 31-year-old restaurant worker Cristian as he voted on Sunday.</p><p data-testid="paragraph-7">Milei's challenges are enormous. He will have to deal with the empty coffers of the government and central bank, a creaking $44 billion debt program with the International Monetary Fund, inflation nearing 150% and a dizzying array of capital controls.</p><p data-testid="paragraph-8">Some Argentines had characterized the vote as a choice of the "lesser evil": fear of Milei's painful economic medicine versus anger at Massa and his Peronist party for an economic crisis that has left Argentina deeply in debt and unable to tap global credit markets.</p><p data-testid="paragraph-9">Milei has been particularly popular among the young, who have grown up seeing their country lurch from one crisis to another.</p><p data-testid="paragraph-10">"Perhaps not everything Milei says I agree with or can identify with but he is our future," said Irene Sosa, a 20-year-old student celebrating outside his election bunker. "Milei represents a future for young people like me, Massa was everything that is wrong with our country."</p><p data-testid="paragraph-11">Milei's win shakes up Argentina's <a data-testid="Link" href="https://www.reuters.com/world/americas/argentinas-peronists-seek-rebirth-ashes-economic-crisis-2023-11-14/" referrerpolicy="no-referrer-when-downgrade">political landscape</a> and <a data-testid="Link" href="https://www.reuters.com/world/americas/argentina-investors-brace-financial-pain-no-matter-who-wins-presidency-2023-11-16/" referrerpolicy="no-referrer-when-downgrade">economic roadmap</a>, and could impact trade in grains, lithium and hydrocarbons. Milei has criticized China and Brazil, saying he won't deal with "communists," and favors stronger U.S. ties.</p><div data-testid="gallery-0"><p data-testid="Body"><b data-testid="Body">[1/18]</b><span>Supporters of Argentine presidential candidate Javier Milei celebrate the results of Argentina's runoff presidential election, in Buenos Aires, Argentina November 19, 2023. REUTERS/Cristina Sille <a data-testid="Link" href="https://www.reutersagency.com/en/licensereuterscontent/?utm_medium=rcom-article-media&amp;utm_campaign=rcom-rcp-lead" target="_blank" referrerpolicy="no-referrer-when-downgrade"> Acquire Licensing Rights</a></span></p></div><p data-testid="paragraph-12">Despite that, Brazilian President <a data-testid="Link" href="https://www.reuters.com/world/americas/brazils-lula-wishes-luck-success-new-argentine-president-2023-11-20/" referrerpolicy="no-referrer-when-downgrade">Luiz Inacio Lula da Silva</a> wished Milei luck and success after the result was announced, adding that it was important democracy was respected.</p><p data-testid="paragraph-13">Former U.S. President Donald Trump congratulated Milei and said the libertarian would make Argentina great again.</p><p data-testid="paragraph-14">Leftist Colombian President Gustavo Petro, meanwhile, said it was a "sad day" for the region.</p><h2 data-testid="Heading">'PROFOUND RUPTURE'</h2><p data-testid="paragraph-15">The shock rise of Milei, a 53-year-old economist and former TV pundit, has broken the hegemony of the two main political forces on the left and the right - the Peronists and the main Together for Change conservative bloc.</p><p data-testid="paragraph-16">"The election marks a profound rupture in the system of political representation in Argentina," said Julio Burdman, director of the consultancy Observatorio Electoral, ahead of the vote.</p><p data-testid="paragraph-17">Supporters of Massa, 51, an experienced political wheeler-dealer, had sought to appeal to voter fears about Milei's volatile character and plan to cut back the size of the state.</p><p data-testid="paragraph-18">"Milei's policies scare me," teacher Susana Martinez, 42, said on Sunday after she voted for Massa.</p><p data-testid="paragraph-19">Milei is also staunchly anti-abortion, favors looser gun laws and has criticized Argentine Pope Francis. He used to carry a chainsaw in a symbol of his planned cuts but shelved it in recent weeks to help boost his moderate image.</p><p data-testid="paragraph-20">After October's first-round vote, Milei struck an uneasy alliance with the <a data-testid="Link" href="https://www.reuters.com/world/americas/argentinas-conservatives-gamble-survival-odd-couple-milei-macri-2023-11-15/" referrerpolicy="no-referrer-when-downgrade">conservatives</a>, which boosted his support. But he faces a highly fragmented Congress, with no single bloc having a majority, meaning that he will need to get backing from other factions to push through legislation. Milei's coalition also does not have any regional governors or mayors.</p><p data-testid="paragraph-21">That may temper some of his more radical proposals. Long-suffering voters are likely to have little patience, and the threat of social unrest is never far below the surface.</p><p data-testid="paragraph-22">His backers say only he can uproot the political status quo and economic malaise that has dogged South America's second-largest economy for years.</p><p data-testid="paragraph-23">"Milei is the only viable option so we do not end up in misery," said Santiago Neria, a 34-year-old accountant.</p><p data-testid="Body">Reporting by Nicolás Misculin, Lucinda Elliott and Walter Bianchi; Additional reporting by Candelaria Grimberg, Jorge Otaola, and Lucila Sigal; Writing by Adam Jourdan; Editing by Will Dunham and Rosalba O'Brien</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank" referrerpolicy="no-referrer-when-downgrade">The Thomson Reuters Trust Principles.</a></p><div><address><p data-testid="Body">Lucinda reports on the southern part of Latin America from Montevideo, Uruguay. Her beat includes Argentina, Bolivia, Chile, Paraguay, Peru &amp; Uruguay. She was previously a correspondent for the Financial Times in Buenos Aires and has experience chasing down some of the region’s more colorful political characters, securing interviews with several former and current Presidents. She was also based in Brazil and Venezuela as a freelance journalist. Before moving to Latin America in 2017, Lucinda worked from the Financial Times' London office, forming part of their premium Emerging Markets service. </p></address></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation) (241 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms</link>
            <guid>38338635</guid>
            <pubDate>Sun, 19 Nov 2023 21:54:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms">https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms</a>, See on <a href="https://news.ycombinator.com/item?id=38338635">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><a href="https://arxiv.org/abs/2106.09685" rel="">Low-rank adaptation</a><span> (LoRA) is among the most widely used and effective techniques for efficiently training custom LLMs. For those interested in open-source LLMs, it's an essential technique worth familiarizing oneself with.</span></p><p><a href="https://lightning.ai/pages/community/lora-insights/" rel="">Last month, I shared an article with several LoRA experiments</a><span>, based on the open-source </span><a href="https://github.com/Lightning-AI/lit-gpt" rel="">Lit-GPT repository</a><span> that I co-maintain with my colleagues at Lightning AI. This Ahead of AI article aims to discuss the primary lessons I derived from my experiments. Additionally, I'll address some of the frequently asked questions related to the topic.&nbsp; If you are interested in finetuning custom LLMs, I hope these insights will save you some time in "the long run" (no pun intended).</span></p><p>In brief, the main takeaways I am discussing in this article are the following:</p><ol><li><p>Despite the inherent randomness of LLM training (or when training models on GPUs in general), the outcomes remain remarkably consistent across multiple runs.</p></li><li><p>QLoRA presents a trade-off that might be worthwhile if you're constrained by GPU memory. It offers 33% memory savings at the cost of a 39% increase in runtime.</p></li><li><p>When finetuning LLMs, the choice of optimizer shouldn't be a major concern. While SGD on its own is suboptimal, there's minimal variation in outcomes whether you employ AdamW, SGD with a scheduler, or AdamW with a scheduler.</p></li><li><p>While Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn't significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters.</p></li><li><p>For static datasets, iterating multiple times, as done in multi-epoch training, might not be beneficial. It often deteriorates the results, probably due to overfitting.</p></li><li><p>If you're incorporating LoRA, ensure it's applied across all layers, not just to the Key and Value matrices, to maximize model performance.</p></li><li><p>Adjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank's value.</p></li><li><p>7 billion parameter models can be finetuned efficiently within a few hours on a single GPU possessing 14 GB of RAM. With a static dataset, optimizing an LLM to excel across all benchmark tasks is unattainable. Addressing this requires diverse data sources, or perhaps LoRA might not be the ideal tool.</p></li></ol><p>In addition, I will answer ten common questions around LoRA:</p><ul><li><p>Q1: How Important is the Dataset?</p></li><li><p>Q2: Does LoRA Work for Domain Adaptation?</p></li><li><p>Q3: How Do You Select the Best Rank?</p></li><li><p>Q4: Does LoRA Need to Be Enabled for All Layers?</p></li><li><p>Q5: How To Avoid Overfitting?</p></li><li><p>Q6: What about Other Optimizers?</p></li><li><p>Q7: What Other Factors Influence Memory Usage?</p></li><li><p>Q8: How Does it Compare to Full Finetuning and RLHF?</p></li><li><p>Q9: Can LoRA Weights be Combined?</p></li><li><p>Q10: What about Layer-wise Optimal Rank Adaptation?</p></li></ul><p>(In the previous issue of AI, I mentioned that I wanted to write a more general introduction with a from-scratch code implementation of LoRA sometime if there's interest. According to your feedback, there's a lot of interest, and I plan to share another article on LoRA in the future. For now, this article is focused on the broader ideas and takeaways from working with LoRA—a top-down view.)</p><p>Large language models are large, and it can be expensive to update all model weights during training due to GPU memory limitations.&nbsp;</p><p><span>For example, suppose we have an LLM with 7B parameters represented in a weight matrix </span><em>W</em><span>. (In reality, the model parameters are, of course, distributed across different matrices in many layers, but for simplicity, we refer to a single weight matrix here). During backpropagation, we learn a </span><em>ΔW</em><span> matrix, which contains information on how much we want to update the original weights to minimize the loss function during training.</span></p><p>The weight update is then as follows:</p><p><em>W</em><sub>updated</sub><span> = </span><em>W</em><span> + </span><em>ΔW</em></p><p><span>If the weight matrix </span><em>W</em><span> contains 7B parameters, then the weight update matrix </span><em>ΔW</em><span> also contains 7B parameters, and computing the matrix </span><em>ΔW</em><span> can be very compute and memory intensive.</span></p><p><span>The LoRA method proposed by </span><a href="https://arxiv.org/abs/2106.09685" rel="">Hu </a><em><a href="https://arxiv.org/abs/2106.09685" rel="">et al.</a></em><span> replaces to decompose the weight changes, </span><em>ΔW</em><span>, into a lower-rank representation. To be precise, it does not require to explicitly compute </span><em>ΔW</em><span>. Instead, LoRA learns the decomposed representation of </span><em>ΔW</em><span> directly during training which is where the savings are coming from, as shown in the figure below.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png" width="1456" height="612" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:612,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>As illustrated above, the decomposition of </span><em>ΔW</em><span> means that we represent the large matrix </span><em>ΔW</em><span> with two smaller LoRA matrices, </span><em>A</em><span> and </span><em>B</em><span>. If </span><em>A</em><span> has the same number of rows as </span><em>ΔW</em><span> and </span><em>B</em><span> has the same number of columns as </span><em>B</em><span>, we can write the decomposition as </span><em>ΔW = AB</em><span>. (</span><em>AB </em><span>is the matrix multiplication result between matrices </span><em>A</em><span> and </span><em>B</em><span>.)&nbsp;</span></p><p><span>How much memory does this save? It depends on the rank </span><em>r</em><span>, which is a hyperparameter. For example, if </span><em>ΔW</em><span> has 10,000 rows and 20,000 columns, it stores 200,000,000 parameters. If we choose </span><em>A</em><span> and </span><em>B</em><span> with </span><em>r=8</em><span>, then </span><em>A</em><span> has 10,000 rows and 8 columns, and </span><em>B</em><span> has 8 rows and 20,000 columns, that's 10,000×8 + 8×20,000 = 240,000 parameters, which is about 830× less than 200,000,000.</span></p><p><span>Of course, </span><em>A</em><span> and </span><em>B</em><span> can't capture all the information that </span><em>ΔW</em><span> could capture, but this is by design. When using LoRA, we hypothesize that the model requires </span><em>W</em><span> to be a large matrix with full rank to capture all the knowledge in the pretraining dataset. However, when we finetune an LLM, we don't need to update all the weights and capture the core information for the adaptation in a smaller number of weights than </span><em>ΔW</em><span> would; hence, we have the low-rank updates via </span><em>AB</em><span>.</span></p><p>Running multiple experiments with LoRA, I found that the benchmark results are surprisingly consistent across the different runs despite the inherent randomness of LLM training or when training models on GPUs in general. This is a good basis for additional comparison studies.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png" width="1244" height="278" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:278,&quot;width&quot;:1244,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>(Note that the results were obtained with default settings using a small </span><em>r=8</em><span>. The experimental details can be found in my other article </span><a href="https://lightning.ai/pages/community/lora-insights/" rel="">here</a><span>.)</span></p><p><a href="https://arxiv.org/abs/2305.14314" rel="">QLoRA by Dettmers</a><em><a href="https://arxiv.org/abs/2305.14314" rel=""> et al.</a></em><span>, short for quantized LoRA, is a technique that further reduces memory usage during finetuning. During backpropagation, QLoRA quantizes the pretrained weights to 4-bit precision and uses paged optimizers to handle memory spikes.</span></p><p>Indeed, I found that one can save 33% of GPU memory when using LoRA. However, this comes at a 39% increased training runtime caused by the additional quantization and dequantization of the pretrained model weights in QLoRA.</p><p>Default LoRA with 16-bit brain floating point precision:</p><ul><li><p>Training time: 1.85 h</p></li><li><p>Memory used: 21.33 GB</p></li></ul><p><span>QLoRA with 4-bit </span><em>Normal Floats:</em></p><ul><li><p>Training time: 2.79 h</p></li><li><p>Memory used: 14.18 GB</p></li></ul><p>Moreover, I found that the modeling performance was barely affected, which makes QLoRA a feasible alternative to regular LoRA training to work around the common GPU memory bottleneck.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png" width="1240" height="216" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:216,&quot;width&quot;:1240,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Learning rate schedulers lower the learning rate throughout the training to optimize convergence and avoid overshooting the loss minima.&nbsp;</p><p>Cosine annealing is a learning rate scheduler that adjusts the learning rate following a cosine curve. It starts with a high learning rate, which then decreases smoothly, approaching zero in a cosine-like manner. A commonly used variant is the half-cycle variant, where only a half-cosine cycle is completed over the course of training, as shown in the figure below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png" width="441" height="330.75" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ebde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:441,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As part of my experiments, I added a cosine annealing scheduler to the LoRA finetuning scripts and observed that it improved the SGD performance noticeably. However, it has less impact on Adam and AdamW optimizers and makes almost no difference.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png" width="1368" height="286" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:286,&quot;width&quot;:1368,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The potential advantages of SGD over Adam are discussed in the next section.</p><p>Adam and AdamW optimizers remain popular choices in deep learning even though they are very memory-intensive when we are working with large models. The reason is that Adam optimizers maintain two moving averages for each model parameter: the first moment (mean) of the gradients and the second moment (uncentered variance) of the gradients. In other words, Adam optimizers store two additional values for each single model parameter in memory. If we are working with a 7B parameter model, that's an extra 14B parameters to track during training.</p><p>SGD optimizers don't need to track any additional parameters during training, so a question is: what advantage does swapping Adam by SGD have on the peak memory requirements when training LLMs?&nbsp;</p><p><span>In my experiments, training a 7B parameter Llama 2 model trained with AdamW and LoRA defaults (</span><em>r=8</em><span>) required 14.18 GB of GPU memory. Training the same model with SGD instead required 14.15 GB of GPU memory. In other words, the savings (0.03 GB) were minimal.&nbsp;</span></p><p><span>Why are the memory savings so small? That's because with LoRA, we only have a small number of trainable parameters. For instance, if </span><em>r=8</em><span>, we have 4,194,304 trainable LoRA parameters out of all 6,738,415,616 parameters in a 7B Llama 2 model.&nbsp;</span></p><p>If we just look at the bare numbers, 4,194,304 trainable parameters still sound like a lot, but if we do the math, we only have 4,194,304 × 2 × 16 bit = 134.22 megabits = 16.78 megabytes. (We observed a 0.03 Gb = 30 Mb difference since there is an additional overhead in storing and copying optimizer states.) The 2 represents the number of extra parameters that Adam stores, and the 16-bit refers to the default precision for the model weights.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png" width="409" height="289.6756238003839" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:738,&quot;width&quot;:1042,&quot;resizeWidth&quot;:409,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>However, if we increase the LoRA </span><em>r</em><span> to 256, something I've done in later experiments, the difference between Adam and SGD optimizers becomes more noticeable:</span></p><ul><li><p>17.86 GB with AdamW</p></li><li><p>14.46 GB with SGD</p></li></ul><p><span>As a takeaway, swapping Adam optimizers with SGD may not be worthwhile when LoRA's </span><em>r</em><span> is small. However, it may be worthwhile when we are increasing </span><em>r</em><span>.&nbsp;</span><br></p><p>In conventional deep learning, we often iterate over a training set multiple times -- an iteration over the training set is called an epoch. It's common to run hundreds of training epochs when training convolutional neural networks, for example. Is multi-epoch training useful for instruction finetuning as well?</p><p><span>When I increased the number of iterations for the </span><a href="https://github.com/tatsu-lab/stanford_alpaca" rel="">50k-example Alpaca</a><span> instruction finetuning dataset by a factor of two (analogous to 2 training epochs), I noticed a decline in model performance.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png" width="1358" height="186" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:186,&quot;width&quot;:1358,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The takeaway is that multi-epoch training might not benefit instruction finetuning since it can deteriorate the results. I observed the same with the 1k-example LIMA dataset. This performance decline is likely due to increased overfitting, which warrants additional investigation.</p><p>The tables above showed experiments where LoRA was only enabled for select weight matrices, i.e., the Key and Value weight matrices in each transformer layer. In addition, we can also enable LoRA for the Query weight matrices, the projection layers, the other linear layers between the multihead attention blocks, and the linear output layer.&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png" width="309" height="257.11567164179104" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:892,&quot;width&quot;:1072,&quot;resizeWidth&quot;:309,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>If we enable LoRA for all these additional layers, we increase the number of trainable parameters by a factor of 5, from 4,194,304 to 20,277,248, for a 7B Llama 2 model. This also comes with a larger memory requirement (16.62 GB instead of 14.18 GB) but can increase the modeling performance noticeably.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png" width="1364" height="168" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:168,&quot;width&quot;:1364,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:62148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>However, a limitation of my experiment is that I only explored two settings: (1) LoRA for only the query and value weight matrices enabled, and (2) LoRA for all layers enabled. It might be worthwhile exploring the other combinations in future experiments. For example, it would be interesting to know whether activating LoRA for the projection layer is actually beneficial.</p><p><span>As the </span><a href="https://arxiv.org/abs/2106.09685" rel="">original LoRA paper</a><span> outlines, LoRA introduces an additional scaling coefficient for applying the LoRA weights to the pretrained weights during the forward pass. The scaling involves the rank parameter r, which we discussed earlier, as well as another hyperparameter α (alpha) that is applied as follows:</span></p><pre><code>scaling = alpha / r
weight += (lora_B @ lora_A) * scaling </code></pre><p>As we can see in the code formula above, the larger the influence of the LoRA weights.</p><p><span>Previous experiments used </span><em>r=8</em><span> and </span><em>alpha=16</em><span>, which resulted in a 2-fold scaling. Choosing alpha as two times r is a common rule of thumb when using LoRA for LLMs, but I was curious if this still holds for larger r values. In other words, “alpha = 2×rank” really seems to be a sweet spot.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png" width="1366" height="528" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:528,&quot;width&quot;:1366,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:188324,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>(I experimented with </span><em>r=32</em><span>, </span><em>r=64</em><span>, </span><em>r=128</em><span>, and </span><em>r=512</em><span> but omitted the results for clarity as </span><em>r=256</em><span> resulted in the best performance.)</span></p><p><span>Indeed, the choosing alpha as two times as large as </span><em>r</em><span> resulted in the best outcomes.</span></p><p><span>One of the main takeaways is that LoRA allows us to finetune 7B parameter LLMs on a single GPU. In this particular case, using QLoRA with the best setting (</span><em>r=256</em><span> and </span><em>alpha=512</em><span>), this 17.86 GB with AdamW takes about 3 hours (on an A100) for 50k training examples (here, the Alpaca dataset).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png" width="1370" height="174" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d309a2b3-6087-4094-a280-bc60feca149a_1370x174.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:174,&quot;width&quot;:1370,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>In the remaining sections of this article, I am answering additional questions you might have.</span><br></p><p>The dataset can be critical. I used the Alpaca dataset, which contains 50k training examples, for my experiments. I chose this dataset because it's quite popular, and experimenting with different datasets was out of scope due to the already extensive length of the article.</p><p>However, it's worth noting that Alpaca is a synthetic dataset that was generated by querying an old version of ChatGPT and is probably not the best by today's standards.&nbsp;</p><p><span>Data quality can be very important. For example, in June, I discussed the LIMA dataset (</span><a href="https://magazine.sebastianraschka.com/p/ahead-of-ai-9-llm-tuning-and-dataset" rel="">Ahead of AI #9: LLM Tuning &amp; Dataset Perspectives</a><span>), a curated dataset consisting of only 1k examples.</span></p><p><span>According to the </span><a href="https://arxiv.org/abs/2305.11206" rel="">LIMA: Less Is More for Alignment</a><span> paper, a 65B Llama model finetuned on LIMA noticeably outperforms a 65B Llama model finetuned on Alpaca.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png" width="675" height="250.3434065934066" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:540,&quot;width&quot;:1456,&quot;resizeWidth&quot;:675,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Using the best configuration (</span><em>r=256,</em><span> </span><em>alpha=512</em><span>) on LIMA, I got similar, if not better, performance than the 50x larger Alpaca dataset.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png" width="1388" height="270" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bc33e126-6803-4a18-837a-6525226d068b_1388x270.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:270,&quot;width&quot;:1388,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Unfortunately, I don't have a good answer to this question. As a rule of thumb, knowledge is usually absorbed from the pretraining dataset. Instruction finetuning is generally more about helping or guiding the LLM towards following instructions.&nbsp;</p><p>However, it's worth noting that if memory is a concern, LoRA can also be used for further pretraining existing pretrained LLMs on domain-specific datasets.&nbsp;</p><p><span>Note that my experiments also included two arithmetic benchmarks (they are included in </span><a href="https://lightning.ai/pages/community/lora-insights/" rel="">my other more technical write-up</a><span>), on which LoRA-finetuned models performed significantly worse than the pretrained base models. My hypothesis is that the model unlearned arithmetic because the Alpaca dataset did not contain corresponding examples. Whether the model completely lost the knowledge or whether it's because the model can't handle the instructions anymore would require further investigation. However, a takeaway here is that it's probably a good idea to include examples of each task you care about when finetuning LLMs.</span><br></p><p><span>Unfortunately, I don't have any good heuristic for selecting a good </span><em>r </em><span>and think that it's a hyperparameter that needs to be explored for each LLM and each dataset. I suspect that choosing an </span><em>r </em><span>that is too large could result in more overfitting. On the other hand, a small r may not be able to capture diverse tasks in a dataset. In other words, I suspect that the more diverse the tasks in the dataset, the larger the </span><em>r</em><span> should be. For example, if I only want a model that carries out basic 2-digit arithmetic, then a tiny </span><em>r </em><span>might already be sufficient. However, this is only a hypothesis and would require additional investigation.</span></p><p>I only explored two settings: (1) LoRA for only the query and value weight matrices enabled, and (2) LoRA for all layers enabled. It might be worthwhile exploring the other combinations in future experiments. For example, it would be interesting to know whether activating LoRA for the projection layer is actually beneficial.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg" width="271" height="229.9794921875" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:869,&quot;width&quot;:1024,&quot;resizeWidth&quot;:271,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>For instance, if we consider the various settings (</span><code>lora_query</code><span>, </span><code>lora_key</code><span>, </span><code>lora_value</code><span>, </span><code>lora_projection</code><span>, </span><code>lora_mlp</code><span>, and </span><code>lora_head</code><span>), that's </span><em>2^6 = 64</em><span> combinations to explore. This exploration would be an interesting topic for future studies.</span></p><p><span>Generally, a larger </span><em>r</em><span> can lead to more overfitting because it determines the number of trainable parameters. If a model suffers from overfitting, decreasing r or increasing the dataset size are the first candidates to explore. Moreover, you could try to increase the weight decay rate in AdamW or SGD optimizers, and you can consider increasing the dropout value for LoRA layers.&nbsp;</span></p><p><span>The LoRA dropout parameter that I haven't explored in my experiments (I used a fixed dropout rate of 0.05), is an interesting topic for future investigations.</span><br></p><p><span>Other interesting optimizers for LLMs are worth exploring in the future. One such optimizer is </span><a href="https://arxiv.org/abs/2305.14342" rel="">Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training</a><span>, which was published in May.</span></p><p><span>Sophia is a second-order optimization algorithm that promises to be particularly attractive for LLMs where Adam and AdamW are usually the dominant ones. Compared to Adam, Sophia is 2× faster, and models trained with Sophia can achieve better modeling performance, according to the paper. In a nutshell, Sophia normalizes the gradients by gradient curvature instead of gradient variance, as in Adam.</span><br></p><p>Besides precision and quantization settings, the model size, the batch size, and the number of trainable LoRA parameters, the dataset can also influence memory usage.</p><p>Note that Llama 2 has a block size of 4048. For instance, if an LLM has a block size of 4048 tokens, it can process sequences of up to 4048 tokens at once. However, shorter training sequences can result in substantial memory savings due to the masking of future tokens.</p><p>For example, the Alpaca dataset is relatively small, with a maximum length of 1304 tokens.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg" width="539" height="404.25" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:960,&quot;width&quot;:1280,&quot;resizeWidth&quot;:539,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><p><span>When I experimented with other datasets that had lengths of up to 2048 tokens, I noticed that the memory usage went up from 17.86 GB to 26.96 GB.&nbsp;</span></p></div><p><span>I did not run any RLHF experiments (for those who are curious, I covered RLHF </span><a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives" rel="">here</a><span>), but I did consider full finetuning. Full finetuning required at least 2 GPUs and was completed in 3.5 h using 36.66 GB on each GPU. However, the benchmark results were not very good, likely due to overfitting or suboptimal hyperparameters.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png" width="1358" height="218" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:218,&quot;width&quot;:1358,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption></figcaption></figure></div><p>Yes, it's possible to combine multiple sets of LoRA weights. During training, we keep the LoRA weights separate from the pretrained weights and add them during each forward pass.&nbsp;</p><p>However, If you have a real-world application with many sets of LoRA weights, for example, one set for each application customer, it makes sense to store these weights separately to save disk space. However, it's possible to merge the pretrained weights with the LoRA weights after training to create a single model. This way, we don't have to apply the LoRA weights in each forward pass:</p><pre><code>weight += (lora_B @ lora_A) * scaling</code></pre><p>Instead, we apply the weight update as shown above and save the merged (added) weights.</p><p>Similarly, we can keep adding multiple LoRA weight sets:</p><pre><code>weight += (lora_B_set1 @ lora_A_set1) * scaling_set1
weight += (lora_B_set2 @ lora_A_set2) * scaling_set2
weight += (lora_B_set3 @ lora_A_set3) * scaling_set3
...</code></pre><p><span>I have yet to do experiments to evaluate the performance of such an approach, but this is technically already possible via the </span><a href="https://github.com/Lightning-AI/lit-gpt/blob/main/scripts/merge_lora.py" rel="">scripts/merge_lora.py</a><span> script provided in Lit-GPT.</span><br></p><p><span>For simplicity, we usually train deep neural networks with the same learning rate for each layer, and the learning rate is a hyperparameter that we need to optimize. To take it further, we can also choose a different learning rate for each layer (</span><a href="https://kozodoi.me/blog/20220329/discriminative-lr#:~:text=The%20implementation%20of%20layer%2Dwise,with%20the%20corresponding%20learning%20rates." rel="">in PyTorch, this is not too complicated</a><span>). However, it's rarely done in practice because it adds additional overhead, and there are usually already so many knobs to tune when training deep neural networks.</span></p><p><span>Analogous to choosing different learning rates for different layers, we can also choose different LoRA ranks for different layers. I haven't found any experiments on this, but a document that details this approach is </span><a href="https://medium.com/@tom_21755/llm-optimization-layer-wise-optimal-rank-adaptation-lora-1444dfbc8e6a" rel="">Layer-wise Optimal Rank Adaptation</a><span> (also abbreviated LORA). In theory, this sounds like a good idea in practice. However, it also adds an extensive number of choices when optimizing hyperparameters.</span></p><p><span>If you're familiar with the fundamentals of machine learning and deep learning but are looking to bridge some knowledge gaps, the 30 chapters in my new book </span><em><strong>Machine Learning and AI Beyond the Basics</strong></em><span> answer critical questions in the field. </span></p><p><em><strong>Machine Learning and AI Beyond the Basics</strong></em><span> is a fully revised and edited version of </span><em>Machine Learning Q and AI</em><span> and is now available for </span><a href="https://nostarch.com/machine-learning-and-ai-beyond-basics" rel="">pre-order on the No Starch Press website</a><span> and </span><a href="https://www.amazon.com/Machine-Learning-AI-Beyond-Basics/dp/1718503768/ref=sr_1_12?crid=21OZ206J24U3G&amp;keywords=raschka&amp;qid=1699032392&amp;sprefix=raschka%2Caps%2C150&amp;sr=8-12" rel="">Amazon</a><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why I tend not to use content negotiation (184 pts)]]></title>
            <link>https://htmx.org/essays/why-tend-not-to-use-content-negotiation/</link>
            <guid>38338033</guid>
            <pubDate>Sun, 19 Nov 2023 21:07:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmx.org/essays/why-tend-not-to-use-content-negotiation/">https://htmx.org/essays/why-tend-not-to-use-content-negotiation/</a>, See on <a href="https://news.ycombinator.com/item?id=38338033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

  
  
    <address>Carson Gross</address>
    <p><time>November 18, 2023</time></p><p>I have written a lot about Hypermedia APIs vs. Data (JSON) APIs, including <a href="https://htmx.org/essays/hypermedia-apis-vs-data-apis/">the differences between the two</a>,
what <a href="https://htmx.org/essays/how-did-rest-come-to-mean-the-opposite-of-rest/">REST “really” means</a> and why <a href="https://htmx.org/essays/hateoas/">HATEOAS</a>
isn’t so bad as long as your API is interacting with a <a href="https://htmx.org/essays/hypermedia-clients/">Hypermedia Client</a>.</p>
<p>Often when I am engaged in discussions with people coming from the “REST is JSON over HTTP” world (that is, the normal
world) I have to navigate a lot of language and conceptual issues:</p>
<ul>
<li>No, I am not advocating you return HTML as a general purpose API, hypermedia makes for a bad general purpose API </li>
<li>Yes, I am advocating <a href="https://htmx.org/essays/two-approaches-to-decoupling/">tightly coupling</a> your web application to your hypermedia API</li>
<li>No, I do not think that we will ever fix how the industry <a href="https://htmx.org/essays/how-did-rest-come-to-mean-the-opposite-of-rest/">uses the term REST</a></li>
<li>Yes, I am advocating you <a href="https://htmx.org/essays/splitting-your-apis/">split your data API and your hypermedia API up</a></li>
</ul>
<p>The last point often strikes people who are used to a single, general purpose JSON API as dumb: why have two APIs when you
can have a single API that can satisfy any number of types of clients?  I tried to answer that question as best I can in the essay
above, but it is certainly a reasonable one to ask.</p>
<p>It seems like (and it is) extra work in some ways when compared to having one general API.</p>
<p>At this point in a conversation, someone who agrees broadly with my take on REST, <a href="https://htmx.org/essays/hypermedia-driven-applications/">Hypermedia-Driven Applications</a>,
etc. will often jump in and say something like</p>
<blockquote>
<p>“Oh, it’s easy, you just use <em>content negotiation</em>, it’s baked into HTTP!”</p>
</blockquote>
<p>Not being content with alienating only the general purpose JSON API enthusiasts, let me now proceed to also alienate
my erstwhile hypermedia enthusiast allies by saying: </p>
<p><em>I don’t think content negotiation is typically the right approach to
returning both JSON and HTML for most applications.</em></p>
<h2 id="what-is-content-negotiation"><a href="#what-is-content-negotiation" aria-label="Anchor link for: what-is-content-negotiation">#</a>What Is Content Negotiation?</h2>
<p>First things first, what is “content negotiation”?</p>
<p><a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Content_negotiation">Content negotiation</a> is a feature of HTTP that
allows a client to negotiate the content type of the response from a server.  A full treatment of the implementation 
in HTTP is beyond the scope of this essay, but let us consider the most well known mechanism for content negotiation
in HTTP, the <a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Content_negotiation#the_accept_header"><code>Accept</code> Request Header</a>.</p>
<p>The <code>Accept</code> request header allows a client, such as a browser, to indicate the <code>MIME</code> types that it is willing to accept
from the server in a response.</p>
<p>An example value of this header is:</p>
<pre data-lang="http request"><code data-lang="http request"><span>Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8
</span></code></pre>
<p>This <code>Accept</code> header tells the server what formats the client is willing to accept.  Preferences are expressed via the
<code>q</code> weighting factor.  Wildcards are expressed with asterisks <code>*</code>.</p>
<p>In this case, the client is saying:</p>
<blockquote>
<p>I would most like to receive text/html, application/xhtml+xml or image/webp.  Next I would prefer application/xml. Finally, I will accept whatever you give me.</p>
</blockquote>
<p>The server then can take this information and determine the best content type to provide to the client.</p>
<p>This is the act of “content negotiation” and it is certainly an interesting feature of HTTP.</p>
<h2 id="using-content-negotiation-in-apis"><a href="#using-content-negotiation-in-apis" aria-label="Anchor link for: using-content-negotiation-in-apis">#</a>Using Content Negotiation In APIs</h2>
<p>As far as I am aware, it was the <a rel="noopener" target="_blank" href="https://rubyonrails.org/">Ruby On Rails</a> community that first went in in a big way
using content negotiation to provide both HTML and JSON (and other) formats from the same URL.</p>
<p>In Rails, this is accomplished via the <a rel="noopener" target="_blank" href="https://apidock.com/rails/ActionController/MimeResponds/respond_to"><code>respond_to</code></a> helper method available in 
controllers.</p>
<p>Leaving the gory details of Rails aside, you might have a request like an HTTP <code>GET</code> to <code>/contacts</code> that ends up invoking
a function in a <code>ContactsController</code> class that looks like this:</p>
<pre data-lang="ruby"><code data-lang="ruby"><span>def </span><span>index
</span><span>  </span><span>@contacts </span><span>= </span><span>Contacts</span><span>.all
</span><span>
</span><span>  respond_to </span><span>do </span><span>|</span><span>format</span><span>|
</span><span>    </span><span>format</span><span>.html </span><span># default rendering logic
</span><span>    </span><span>format</span><span>.json { render </span><span>json: </span><span>@contacts </span><span>}
</span><span>  </span><span>end
</span><span>end
</span></code></pre>
<p>By making use of the <code>respond_to</code> helper method, if a client makes a request with the <code>Accept</code> header above, the controller
will render an HTML response using the Rails templating systems.</p>
<p>However, if the <code>Accept</code> header from the client has the value <code>application/json</code> instead, Rails will render the contacts 
as a JSON array for the client.</p>
<p>A pretty neat trick: you can keep all your controller logic, like looking up the contacts, the same and just use a 
bit of ruby/Rails magic to render two different response types using content negotiation.  Barely any additional work on 
top of the normal Model/View/Controller logic.</p>
<p>You can see why people like the idea!</p>
<h2 id="so-what-s-the-problem"><a href="#so-what-s-the-problem" aria-label="Anchor link for: so-what-s-the-problem">#</a>So What’s The Problem?</h2>
<p>So why don’t I think this is a good approach to splitting your JSON and HTML APIs up?</p>
<p>It boils down to the <a href="https://htmx.org/essays/why-tend-not-to-use-content-negotiation/hypermedia-apis-vs-data-apis.md">differences between JSON APIs and Hypermedia (HTML) APIs</a> I hinted
at earlier.  In particular:</p>
<ul>
<li>Data APIs should be versioned and should be very stable within a particular version of the API</li>
<li>Data APIs should strive for both regularity and expressiveness due to the arbitrary data needs of consumers</li>
<li>Data APIs typically use some sort of token-based authentication</li>
<li>Data APIs should be rate limited</li>
<li>Hypermedia APIs typically use some sort of session-cookie based authentication</li>
<li>Hypermedia APIs should be driven by the needs of the underlying hypermedia application</li>
</ul>
<p>While all of these differences matter and have an effect on your controller code, pulling it in two different directions,
it is really the first and last items that make me often choose not to use content negotiation in my applications.</p>
<p>Your JSON API needs to be a stable set of endpoint that client code can rely on.</p>
<p>Your hypermedia API, on the other hand, can change dramatically based on the user interface needs of your applications.</p>
<p>These two things don’t mix well.</p>
<p>To give you a concrete example, consider an end point that renders a detail view of a contact, at, say <code>/contacts/:id</code> 
(where <code>:id</code> is a parameter containing the id of the contact to render).  Let’s say that this page has a “related contacts”
section of the UI and, further, computing these related contacts is expensive for some reason.</p>
<p>In this situation you might choose to use the <a rel="noopener" target="_blank" href="https://htmx.org/examples/lazy-load/">Lazy Loading</a> pattern to defer 
loading the related contacts until after the initial contact detail screen has been rendered.  This improves perceived
performance of the page for your users.</p>
<p>If you did this, you might put the lazy loaded content at the end-point <code>/contacts/:id/related</code>.</p>
<p>Now, later on, maybe you are able to optimize the computation of related contacts.  At this point you might choose to 
rip the <code>/contacts/:id/related</code> end-point out and just render the related contacts information in the initial page render.</p>
<p>All of this is fine for your hypermedia API: hypermedia, through <a href="https://htmx.org/essays/hateoas/">the uniform interface &amp; HATEOAS</a>
is <em>designed</em> to handle these sorts of changes.</p>
<p>However, your JSON API… not so much.</p>
<p>Your JSON API should remain stable.  You can’t be adding and removing end-points
willy-nilly.  Yes, you can have <em>some</em> end-points respond with either JSON or HTML and others only respond with HTML, but
it gets messy.  What if you accidentally copy-and-paste in the wrong code somewhere, for example.</p>
<p>Taking all of this into account, as well as things like rate-limiting and so on, I think you can make a strong argument
that there should be a <a rel="noopener" target="_blank" href="https://en.wikipedia.org/wiki/Separation_of_concerns">Separation Of Concerns</a> between the JSON
API and the hypermedia API.</p>
<p>(Yes, I am aware of the irony that the person who coined the term <a href="https://htmx.org/essays/locality-of-behaviour/">Locality of Behaviour</a>
is making a SoC argument.)</p>
<h2 id="so-what-s-the-alternative"><a href="#so-what-s-the-alternative" aria-label="Anchor link for: so-what-s-the-alternative">#</a>So What’s The Alternative?</h2>
<p>The alternative is, as I advocate in <a href="https://htmx.org/essays/splitting-your-apis/">Splitting Your APIs</a>, er, well, splitting your
APIs.  This means providing different paths (or sub-domains, or whatever) for your JSON API and your hypermedia (HTML)
API.</p>
<p>Going back to our contacts API, we might have the following:</p>
<ul>
<li>The JSON API to get all contacts is found at <code>/api/v1/contacts</code></li>
<li>The Hypermedia API to get all contacts is found at <code>/contacts</code></li>
</ul>
<p>This layout implies two different controllers and, I say, that’s a good thing: the JSON API controller can implement the
requirements of a JSON API: rate limiting, stability, maybe an expressive query mechanism like GraphQL.</p>
<p>Meanwhile, your
hypermedia API (really, just your Hypermedia Driven Application endpoints) can change dramatically as your user interface
needs change, with highly tuned database queries, end-points to support special UI needs, etc.</p>
<p>By separating these two concerns, your JSON API can be stable, regular and low-maintenance, and your hypermedia API can
be chaotic, specialized and flexible.  Each gets its own controller environment to thrive in, without conflicting with
one another.</p>
<p>And this is why I prefer to split my JSON and hypermedia APIs up into separate controllers, rather than use HTTP content
negotiation to attempt to reuse controllers for both.</p>

  <p>
    &lt;/&gt;
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI negotiations to reinstate Altman hit snag over board role (261 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2023-11-19/openai-negotiations-to-reinstate-altman-hit-snag-over-board-role</link>
            <guid>38337568</guid>
            <pubDate>Sun, 19 Nov 2023 20:35:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2023-11-19/openai-negotiations-to-reinstate-altman-hit-snag-over-board-role">https://www.bloomberg.com/news/articles/2023-11-19/openai-negotiations-to-reinstate-altman-hit-snag-over-board-role</a>, See on <a href="https://news.ycombinator.com/item?id=38337568">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rosalynn Carter has died (101 pts)]]></title>
            <link>https://www.nbcnews.com/news/obituaries/rosalynn-carter-former-first-lady-dies-rcna62862</link>
            <guid>38337395</guid>
            <pubDate>Sun, 19 Nov 2023 20:20:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/news/obituaries/rosalynn-carter-former-first-lady-dies-rcna62862">https://www.nbcnews.com/news/obituaries/rosalynn-carter-former-first-lady-dies-rcna62862</a>, See on <a href="https://news.ycombinator.com/item?id=38337395">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Rosalynn Carter, the former first lady and humanitarian who championed mental health care, provided constant political counsel to her husband, former President Jimmy Carter, and modeled graceful longevity for the nation, died Sunday at her home in Plains, Georgia, <a href="https://www.cartercenter.org/news/pr/2023/statement-rosalynn-carter-111923.html" target="_blank">according to the Carter Center.</a></p><p>Carter was 96. <a href="https://www.nbcnews.com/news/us-news/former-first-lady-rosalynn-carter-hospice-care-home-jimmy-carter-rcna125770" target="_blank">She had entered hospice care </a><a href="https://www.nbcnews.com/news/us-news/former-first-lady-rosalynn-carter-hospice-care-home-jimmy-carter-rcna125770" target="_blank">in</a><a href="https://www.nbcnews.com/news/us-news/former-first-lady-rosalynn-carter-hospice-care-home-jimmy-carter-rcna125770" target="_blank"> </a><a href="https://www.nbcnews.com/news/us-news/former-first-lady-rosalynn-carter-hospice-care-home-jimmy-carter-rcna125770" target="_blank">her </a><a href="https://www.nbcnews.com/news/us-news/former-first-lady-rosalynn-carter-hospice-care-home-jimmy-carter-rcna125770" target="_blank">home</a> on Friday. </p><p>In a statement, former President Carter said: “Rosalynn was my equal partner in everything I ever accomplished. She gave me wise guidance and encouragement when I needed it. As long as Rosalynn was in the world, I always knew somebody loved and supported me.”</p><p>Rosalynn Carter was widely regarded for her political shrewdness, drawing particular praise for her keen electoral instincts, down-to-earth appeal, and work on behalf of the White House, including serving as an envoy to Latin America.</p><p>She devoted herself to several social causes in the course of her public life, including programs that supported health care resources, human rights, social justice and the needs of elderly people.</p><p>“Twenty-five years ago, we did not dream that people might someday be able actually to recover from mental illnesses,” Carter said at <a href="https://www.cartercenter.org/news/features/h/mental_health/50-years-quote-page.html">a mental health symposium</a> in 2003. “Today it is a very real possibility.”</p><p>“For one who has worked on mental health issues as long as I have,” she added, “this is a miraculous development and an answer to my prayers.”</p><figure><picture data-testid="picture"><source media="(min-width: 1000px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-1979-ac-1158p-9ebb9e.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-1979-ac-1158p-9ebb9e.jpg 1x"><source srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-1979-ac-1158p-9ebb9e.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-1979-ac-1158p-9ebb9e.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-1979-ac-1158p-9ebb9e.jpg" alt="US First Lady Rosalynn Carter shakes one hand and waves the other during an unspecified campaign event, New Hampshire, October 24, 1979. (Photo by Diana Walker/Getty Images)" height="1667" width="2500"></picture><figcaption data-testid="caption"><span data-testid="caption__container">First lady Rosalynn Carter greets people at a campaign event in New Hampshire on Oct. 24, 1979.</span><span data-testid="caption__source">Diana Walker / Getty Images file</span></figcaption></figure><p>In late May, the Carter Center, the couple’s human rights group, announced that she had been diagnosed with dementia. “She continues to live happily at home with her husband, enjoying spring in Plains and visits with loved ones,” the organization said in a statement.</p><p>Bess Truman, the wife of President Harry Truman, is the only first lady to have lived longer, according to the National First Ladies Library. (Bess Truman died in 1982, at 97.) Jimmy and Rosalynn were the longest-married presidential couple in U.S. history.</p><p>The Carters earned admiration for their humanitarian projects after they left the White House. They were closely linked with Habitat for Humanity, <a href="https://www.habitat.org/volunteer/build-events/carter-work-project">considered by the charity</a> to be “tireless advocates, active fundraisers and some of our best hands-on construction volunteers.”</p><p>Eleanor Rosalynn Smith was born in Plains, Georgia, on Aug. 18, 1927, the first of four children reared by Allethea Murray Smith and Wilburn Edgar Smith. Rosalynn’s father died when she was 13, and her mother became a dressmaker to provide for her family.</p><p>The loss of her father at such a young age forced Rosalynn to assume additional responsibilities alongside her mother. But the family unit managed to stay afloat.</p><p>Rosalynn finished high school and enrolled at Georgia Southwestern College. In 1945, following her freshman year, <a href="https://www.nbcnews.com/politics/politics-news/jimmy-rosalynn-carter-mark-75-years-full-partnership-n1273066">she went on a date</a> with Jimmy Carter, a childhood friend of the family who was home from the U.S. Naval Academy.</p><figure><picture data-testid="picture"><source media="(min-width: 1000px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-1978-ac-1158p-07b148.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-1978-ac-1158p-07b148.jpg 1x"><source srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-1978-ac-1158p-07b148.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-1978-ac-1158p-07b148.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-1978-ac-1158p-07b148.jpg" alt="FILE - In this Dec. 13, 1978 file photo, President Jimmy Carter and his wife Rosalynn lead their guests in dancing at the annual Congressional Christmas Ball at the White House in Washington. Jimmy Carter and his wife Rosalynn celebrate their 75th anniversary this week on Thursday, July 7, 2021. (AP Photo/Ira Schwarz, File)" height="1949" width="2923"></picture><figcaption data-testid="caption"><span data-testid="caption__container">President Jimmy Carter and his wife, Rosalynn, dance at the annual Congressional Christmas Ball at the White House on Dec. 13, 1978.</span><span data-testid="caption__source">Ira Schwarz / AP file</span></figcaption></figure><p>“She’s the girl I want to marry,” Jimmy Carter told his mother after their first outing, according to a biography compiled by <a href="https://www.whitehouse.gov/about-the-white-house/first-families/rosalynn-smith-carter/">the White House Historical Association</a>.</p><p>They were married the following year, on July 7, 1946. They relocated to Norfolk, Virginia — Jimmy’s first duty station after graduation. But life as a Navy family meant they had to move frequently.</p><p>Their four children were each born in different states: John William in Virginia, James Earl III in Hawaii, Donnel Jeffrey in Connecticut, and Amy Lynn — their only daughter — in Georgia.</p><p>Jimmy’s father died in 1953, sending the couple back to Plains to run the family peanut business. Rosalynn soon started working for the enterprise full time, assisting with accounting and other front-office functions.</p><p>Jimmy decided to launch a political career in the early 1960s, winning a Georgia state Senate seat in 1962.&nbsp;</p><p>He unsuccessfully sought the governorship in 1966; during that campaign, Rosalynn learned more about the challenges facing people with mental illnesses, as she <a href="https://content.time.com/time/health/article/0,8599,1987385,00.html">recounted to Time magazine in 2010</a>.</p><p>“The more I thought about it and found out about it, the more I thought it was just a terrible situation with no attention,” she said.</p><figure><picture data-testid="picture"><source media="(min-width: 1000px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-2005-ac-1157p-1b0226.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-2005-ac-1157p-1b0226.jpg 1x"><source srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-2005-ac-1157p-1b0226.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-2005-ac-1157p-1b0226.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-2005-ac-1157p-1b0226.jpg" alt="Rosalynn Carter during Habitat for Humanity - 2005 Jimmy Carter Work Project - Day 2 at Benton Harbor in Benton Harbor, Michigan, United States. ***Exclusive*** (Photo by R. Diamond/WireImage)

Habitat for Humanity - 2005 Jimmy Carter Work Project - Day 2" height="1633" width="2500"></picture><figcaption data-testid="caption"><span data-testid="caption__container">Rosalynn Carter works at a Habitat for Humanity event in Benton Harbor, Mich., on June 21, 2005.</span><span data-testid="caption__source">R. Diamond / WireImage file</span></figcaption></figure><p>Rosalynn helped lay the foundation for her husband’s winning bid for the Georgia governorship in 1970 and, six years later, advised her husband’s grassroots presidential campaign. Political reporters took notice of her vivacity on the trail.</p><p>“Rosalynn Carter, 49, the candidate’s wife, campaigns with the untiring race-horse type of energy which has typified Carter’s operation for the past 18 months,” U.S. News &amp; World Report wrote in May 1976.</p><p>“Not only that: Top aides claim Mrs. Carter is her husband’s most influential political adviser,” the author of the article added.</p><p>Rosalynn attracted particular attention for the skillful way she connected with voters, nabbing their support for her husband with down-to-earth warmth. In an unusual move for the era, she traveled across the country on her own, making the case for her husband on her own terms.</p><p>“Mrs. Carter, soft-spoken and low-key, prefers face-to-face meetings with voters,” U.S. News &amp; World Report wrote in June 1976. “In her campaigning in 30 states she has scheduled frequent sessions at plant gates and shopping centers.”</p><p>Jimmy, running as a political outsider and a symbolic break from the disillusioned post-Watergate era, defeated President Gerald Ford in 1976. The press quickly understood that Rosalynn would not be content to remain on the sidelines in Washington.</p><p>“Rosalynn Carter will not be simply an East Wing ornament, a First Lady content to redecorate the White House or preside over soirees,” Newsweek’s Jane Whitmore wrote in January 1977.</p><p>“There’s so much you can do,” Rosalynn told Whitmore, “and there are things I want to do. I want to work on mental health and the problems of the elderly — independently, on my own.”</p><figure><picture data-testid="picture"><source media="(min-width: 1000px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-2017-ac-1156p-c68954.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-2017-ac-1156p-c68954.jpg 1x"><source srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-2017-ac-1156p-c68954.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-2017-ac-1156p-c68954.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-2017-ac-1156p-c68954.jpg" alt="FILE - In this Feb. 8, 2017, file photo former President Jimmy Carter, right, and his wife Rosalynn arrive for a ribbon cutting ceremony for a solar panel project on farmland he owns in their hometown of Plains, Ga. Jimmy Carter and his wife Rosalynn celebrate their 75th anniversary this week on Thursday, July 7, 2021. (AP Photo/David Goldman, File)" height="1667" width="2500"></picture><figcaption data-testid="caption"><span data-testid="caption__container">Rosalynn Carter and former President Jimmy Carter walk on farmland he owns in their hometown, Plains, Ga., on Feb. 8, 2017.</span><span data-testid="caption__source">David Goldman / AP file</span></figcaption></figure><p>“Jimmy’s always talked things over with me, like when he was choosing the Vice President or the Cabinet,” she added. “I’ve always been involved in the meetings. I always tell him what I think even if I disagree — and I’ll continue to do that.”</p><p>Rosalynn established herself as an active part of her husband’s administration.</p><p>She joined Cabinet meetings, attended key briefings, spoke on behalf of the White House at ceremonial gatherings, served as an honorary member on a mental health commission, and traveled to Latin American nations as the president’s personal envoy.</p><p>Jimmy Carter’s presidency itself was judged to be a mixed bag, and many Americans — including some Democrats — believed that he was an ineffective commander in chief, particularly as the Iran hostage crisis dominated headlines in late 1979.</p><p>Rosalynn worked tirelessly in the bid to re-elect her husband to a second term in 1980 — a campaign Jimmy lost to Ronald Reagan, a former Hollywood star and governor of California who represented the ascendant conservative movement.</p><p>She was said to have been gutted by her husband’s loss and the apparent repudiation of his presidency by so many voters. But she made it clear to political reporters that she was trying to look to the future.</p><p>“I think you accept it,” Rosalynn was quoted as saying in a November 1980 article by the longtime UPI reporter Helen Thomas. “When you’ve done all you possibly can do, that’s all you can do. It was out of our hands.”</p><p>She pledged to “speak out” on the issues close to her heart, adding: “You go from one phase of your life to the next phase of life. … I think it’s going to be exciting.”</p><p>The next phase of Rosalynn Carter’s life proved to be fruitful. She wrote several books, including the 1984 memoir “First Lady From Plains” as well as three books about mental health.</p><figure><picture data-testid="picture"><source media="(min-width: 1000px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-2021-ac-1157p-d105c2.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-2021-ac-1157p-d105c2.jpg 1x"><source srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-2021-ac-1157p-d105c2.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-2021-ac-1157p-d105c2.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-jimmy-carter-2021-ac-1157p-d105c2.jpg" alt="Former U.S. President Jimmy Carter and his wife, former first lady Rosalynn Carter" height="1667" width="2500"></picture><figcaption data-testid="caption"><span data-testid="caption__container">Former President Jimmy Carter and his wife, Rosalynn Carter, sit together at a reception to celebrate their 75th wedding anniversary in Plains, Ga., on July 10, 2021.</span><span data-testid="caption__source">John Bazemore / Pool via AP file</span></figcaption></figure><p>The Carters remained committed to bettering the lives of people around the world, winning several awards and honors along the way.&nbsp;</p><p>In 1982, they founded the Carter Center, a nonprofit human rights organization forged in partnership with Emory University in Atlanta. Seven years later, she established the Rosalynn Carter Institute for Caregiving at Georgia Southwestern State University.</p><figure><picture data-testid="picture"><source media="(min-width: 1000px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-1977-cover-ac-1159p-69871d.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-560w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-1977-cover-ac-1159p-69871d.jpg 1x"><source srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:eco,dpr_2.0/rockcms/2022-12/221221-rosalynn-carter-1977-cover-ac-1159p-69871d.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-1977-cover-ac-1159p-69871d.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2022-12/221221-rosalynn-carter-1977-cover-ac-1159p-69871d.jpg" alt="This photo provided by the White House is the official portrait of first lady Rosalynn Carter in the Vermeil Room of the White House, Feb. 18, 1977.

Rosalynn Carter,Eleanor Rosalynn Smith Carter" height="1666" width="2500"></picture><figcaption data-testid="caption"><span data-testid="caption__container">First lady Rosalynn Carter in the Vermeil Room of the White House on Feb. 18, 1977.</span><span data-testid="caption__source">White House via AP file</span></figcaption></figure><p>She held <a href="https://www.cartercenter.org/health/mental_health/rosalynn-carter-mental-health-leadership.html">annual symposia</a> on mental health at the Carter Center for more than three decades, uniting experts and advocates for discussions about mental illness, family coping, financing care services, supporting research and reducing stigma.</p><p>The two were awarded <a href="https://www.youtube.com/watch?v=NIH9rs7F5rc">the Presidential Medal of Freedom</a> by President Bill Clinton in August 1999. Clinton, <a href="https://www.presidency.ucsb.edu/documents/remarks-ceremony-presenting-the-presidential-medal-freedom-former-president-jimmy-carter">speaking at the Carter Center</a>, praised the couple for their humanitarian accomplishments.</p><p>“Rarely do we honor two people who have devoted themselves so effectively to advancing freedom in all those ways,” Clinton said. “Jimmy and Rosalynn Carter have done more good things for more people in more places than any other couple on the face of the Earth.”</p><p>In recent years, the Carters appeared publicly less frequently. But during the 2020 presidential election, they recorded <a href="https://www.youtube.com/watch?v=sFnmIkBsygI">a video tribute to Joe Biden</a> that aired during the televised portion of the Democratic National Convention.</p><p>Tributes to the former first lady poured in Sunday, with many reflecting on her partnership with her husband and devotion to the causes she championed. </p><p>Biden said the Carter family brought "grace" to the office in remarks following the news that Carter had died. </p><p>He said Carter had "integrity" like her husband, adding, "Imagine, they were together for 77 years."</p><p>Former President Donald Trump and former first lady Melania Trump said they are mourning the loss of the “devoted First Lady, a great humanitarian, a champion for mental health, and a beloved wife to her husband for 77 years, President Carter.” </p><p>The couple said Carter “leaves behind a legacy of extraordinary accomplishment and national service.”</p><p>Former first lady Michelle Obama said Carter “used her platform in profoundly meaningful ways,” calling out her work for mental health care, better care for older adults, affordable housing and women’s rights.</p><p>“When our family was in the White House, every so often, Rosalynn would join me for lunch, offering a few words of advice and always — always — a helping hand,” <a href="https://twitter.com/MichelleObama/status/1726378313176727719" target="_blank">Obama wrote. </a>“She reminded me to make the role of First Lady my own, just like she did. I’ll always remain grateful for her support and her generosity.”</p><p>Former President George W. Bush and former first lady Laura Bush said in a statement that Carter was “a woman of dignity and strength.” </p><p>“There was no greater advocate of President Carter, and their partnership set a wonderful example of loyalty and fidelity,” the Bushes said. “She leaves behind an important legacy in her work to destigmatize mental health.”</p><p>Clinton and former Secretary of State Hillary Clinton called Carter "a compassionate and committed champion of human dignity everywhere" <a href="https://twitter.com/BillClinton/status/1726373257090089295" target="_blank">in a post on X</a>. </p><p>"Rosalynn will forever be remembered as the embodiment of a life lived with purpose," they said. "Hillary and I are deeply grateful for her extraordinary service to our nation and the world, and for more than forty years of friendship." </p><p>Rep. Nancy Pelosi, D-Calif., the former House speaker, lauded “the transformational change she brought to mental wellness and to caregiving, inspiring people around the world to work towards a better future for all,” <a href="https://twitter.com/TeamPelosi/status/1726372475221500356" target="_blank">in a post</a> on X. </p><p>A spokesperson for the Secret Service called Carter a "treasure, a beacon of compassion."</p><p>“Your compassion, diplomacy, and penchant to make society better for those less fortunate was an inspiration for an entire generation," the Secret Service said in a statement. "It has been our honor to protect and serve you for all of these years. You were truly a treasure for our nation and our secret service family."</p><p>Ceremonies celebrating Carter's life will be held Monday through Wednesday in Atlanta and Sumter County, Georgia, according to the <a href="https://www.rosalynncartertribute.org/announcements/schedule-of-ceremonies.html" target="_blank">Carter Center</a>. Her funeral will be held Wednesday in Plains.</p></div><div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/daniel-arkin-ncpn1206"><picture data-testid="picture"><source srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:eco,dpr_2.0/newscms/2019_28/2931061/190619-daniel_arkin-byline2285.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_28/2931061/190619-daniel_arkin-byline2285.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_28/2931061/190619-daniel_arkin-byline2285.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/daniel-arkin-ncpn1206">Daniel Arkin</a></span><span><a href="https://twitter.com/d_arkin" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:daniel.arkin@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Daniel Arkin is a national reporter at NBC News.</p></div><div><p>Rebecca Cohen</p><!-- --> <!-- --><p>contributed</p><!-- --><p>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Financial situation of the Matrix.org Foundation (116 pts)]]></title>
            <link>https://github.com/matrix-org/matrix-spec/issues/571</link>
            <guid>38336736</guid>
            <pubDate>Sun, 19 Nov 2023 19:26:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/matrix-org/matrix-spec/issues/571">https://github.com/matrix-org/matrix-spec/issues/571</a>, See on <a href="https://news.ycombinator.com/item?id=38336736">Hacker News</a></p>
<div id="readability-page-1" class="page"><div disabled="" sortable="">
          <p dir="auto">Hi <a data-hovercard-type="user" data-hovercard-url="/users/ara4n/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ara4n">@ara4n</a>. Thanks a lot for your reply.</p>
<blockquote>
<p dir="auto">I've tried to enumerate the functions of the Foundation in the set of bullet points at the beginning of <a href="https://matrix.org/blog/2022/12/25/the-matrix-holiday-update-2022" rel="nofollow">https://matrix.org/blog/2022/12/25/the-matrix-holiday-update-2022</a></p>
</blockquote>
<p dir="auto">Can you please outline how the bullet points described match the <a href="https://github.com/matrix-org/matrix-spec-proposals/blob/main/proposals/1779-open-governance.md#functions-of-the-foundation">Functions of the Foundation as of MSC1779</a>? To me it seems a lot of the items listed are not really in scope of the work of the Matrix Foundation.</p>
<blockquote>
<p dir="auto">Currently the vast majority of the cost of this (~95%) is borne by Element (the trading name for New Vector Ltd, the company set up by the Matrix core team to try to fund Matrix development), where manpower is donated to the Foundation (and so doesn't show up on the books).</p>
</blockquote>
<p dir="auto">I understand that Element is donating large amounts of non-monetary assets to the Foundation. For some of these donations, the transfer is supposed to happen as per <a rel="noopener noreferrer nofollow" href="https://github.com/matrix-org/matrix-spec-proposals/issues/1779">MSC1779</a>, for others this seems to not be the case. In any case, those donations are obviously very generous and highly appreciated.</p>
<p dir="auto">The way I read your messages, it sounds as if the idea is to have the work currently done by Element and then donated to Matrix Foundation be done by the Matrix Foundation in the future (short term probably by paying Element), which seems to not be covered by <a rel="noopener noreferrer nofollow" href="https://github.com/matrix-org/matrix-spec-proposals/issues/1779">MSC1779</a>. It also significantly changes the position of Matrix Foundation from a standards organisation (like IETF or W3C) with some additional tasks (like hosting the matrix.org home server and ensuring the existence of a reference implementation).</p>
<p dir="auto">Would you please explain as to why this approach was chosen rather than</p>
<ul dir="auto">
<li>having the Matrix Foundation continue to act as a mostly standards organisation</li>
<li>put new developments under copyright of Element (instead of Matrix Foundation) which doesn't make a practical difference for users and developers as long as the license remains the Apache License.</li>
<li>allow users to donate to Element to support the development of libraries, SDKs and the element client</li>
</ul>
<p dir="auto">I think such an approach would be much more compatible with <a rel="noopener noreferrer nofollow" href="https://github.com/matrix-org/matrix-spec-proposals/issues/1779">MSC1779</a> and won't persistently raise questions about who to receive how much money from the Matrix Foundation for their developments (e.g. would the independent libQuotient developers be receiving the same funding as the element-hired developers of matrix-js-sdk?).</p>
<blockquote>
<p dir="auto">Hope this provides a bit more clarity</p>
</blockquote>
<p dir="auto">Thanks, It does give some idea on the plans for the future of the Matrix Foundation.</p>
<p dir="auto">However it doesn't really give any insights into previous years and current financials of the Matrix Foundation. Even very small documentation would already be very much appreciated. Could be as easy as:</p>
<table role="table">
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Account balance at beginning of fiscal year:</td>
<td>XXX £</td>
</tr>
<tr>
<td>Income from donations:</td>
<td>+ XXX £</td>
</tr>
<tr>
<td>Income from sponsorships:</td>
<td>+ XXX £</td>
</tr>
<tr>
<td>Other income:</td>
<td>+ XXX £</td>
</tr>
<tr>
<td>Expenses for hosting matrix.org:</td>
<td>- XXX £</td>
</tr>
<tr>
<td>Expenses for promotion:</td>
<td>- XXX £</td>
</tr>
<tr>
<td>Expenses for ensuring existence of reference implementations:</td>
<td>- XXX £</td>
</tr>
<tr>
<td>Expenses for administration and organisation:</td>
<td>- XXX £</td>
</tr>
<tr>
<td>Other expenses:</td>
<td>- XXX £</td>
</tr>
<tr>
<td>Asset re-evaluation (for crypto):</td>
<td>+/- XXX £</td>
</tr>
<tr>
<td>Account balance at end of fiscal year:</td>
<td><strong>XXX £</strong></td>
</tr>
</tbody>
</table>
<p dir="auto">Most of this should probably be in easy reach as they'd likely be needed to create the accounting report legally required.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Awesome Engineering Games (224 pts)]]></title>
            <link>https://github.com/arcataroger/awesome-engineering-games</link>
            <guid>38336688</guid>
            <pubDate>Sun, 19 Nov 2023 19:22:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/arcataroger/awesome-engineering-games">https://github.com/arcataroger/awesome-engineering-games</a>, See on <a href="https://news.ycombinator.com/item?id=38336688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Awesome Engineering Games</h2>
<p dir="auto"><a href="https://awesome.re/" rel="nofollow"><img src="https://camo.githubusercontent.com/3cbfaa1f947ec978205547180a7363399d2aec652c87f91cf4a6f1f332ca610b/68747470733a2f2f617765736f6d652e72652f62616467652d666c61742e737667" alt="Awesome" data-canonical-src="https://awesome.re/badge-flat.svg"></a></p>
<p dir="auto">A curated list of engineering-related video games rated Very Positive or higher on Steam.</p>
<p dir="auto">Games are divided into broad categories based on the type(s) of engineering they're most related to, such as civil engineering, electrical engineering, computer science, etc.</p>
<p dir="auto">Please feel free to suggest new games or update existing titles!</p>
<h2 tabindex="-1" dir="auto">Ratings &amp; Symbols Explained</h2>
<ul dir="auto">
<li>❤️ = "Overwhelmingly Positive" overall reviews on Steam, meaning &gt;= 95% of 500+ reviewers liked it.</li>
<li>👍 = Curator's Choice: Personal recommendations by the maintainers(s) of this list. This is a TOTALLY SUBJECTIVE "best of" for each category.</li>
<li>☁️ = Playable on <a href="https://www.nvidia.com/en-us/geforce-now/" rel="nofollow">GeForce Now</a>. You can stream this game from the cloud without PC gaming hardware (no GPU required). Works in your browser and most OSes.</li>
</ul>
<p dir="auto">All other games have "Very Positive" overall reviews on Steam, meaning &gt;= 80% of 50+ reviewers liked it.</p>
<h2 tabindex="-1" dir="auto">Table of Contents</h2>
<details>
<summary>Click to expand Table of Contents</summary>


<ul dir="auto">
<li><a href="#factory-management--automation">Factory Management &amp; Automation</a>
<ul dir="auto">
<li><a href="#factory-building">Factory Building</a></li>
<li><a href="#puzzle--casual">Puzzle &amp; Casual</a></li>
</ul>
</li>
<li><a href="#vehicle-building">Vehicle Building</a></li>
<li><a href="#city-builders--civil-engineering">City Builders &amp; Civil Engineering</a>
<ul dir="auto">
<li><a href="#modern--near-future">Modern &amp; Near-Future</a></li>
<li><a href="#rail--sails">Rail &amp; Sails</a></li>
<li><a href="#medieval--fantasy">Medieval &amp; Fantasy</a></li>
<li><a href="#sci-fi--far-future">Sci-Fi &amp; Far-Future</a></li>
<li><a href="#theme-parks--dungeons">Theme Parks &amp; Dungeons</a></li>
</ul>
</li>
<li><a href="#transportation--route-management">Transportation &amp; Route Management</a>
<ul dir="auto">
<li><a href="#realistic-route-building">Realistic Route-building</a></li>
<li><a href="#metrotrain-station-management">Metro/Train Station Management</a></li>
<li><a href="#puzzle--casual-1">Puzzle &amp; Casual</a></li>
</ul>
</li>
<li><a href="#survival--settlement-builders-colony-sims">Survival &amp; Settlement Builders, Colony Sims</a></li>
<li><a href="#open-world-survival-sandboxes">Open-World Survival Sandboxes</a></li>
<li><a href="#bridge-building">Bridge-Building</a></li>
<li><a href="#rube-goldberg-machines">Rube Goldberg Machines</a></li>
<li><a href="#hacking-computer-science-logic--automation">Hacking, Computer Science, Logic &amp; Automation</a>
<ul dir="auto">
<li><a href="#electrical-engineering">Electrical Engineering</a></li>
</ul>
</li>
<li><a href="#job-simulators--unique-themes">Job Simulators &amp; Unique Themes</a>
<ul dir="auto">
<li><a href="#ship-crews-space-or-naval">Ship Crews (Space or Naval)</a></li>
<li><a href="#space-operations--orbital-mechanics">Space Operations &amp; Orbital Mechanics</a></li>
<li><a href="#construction--demolition">Construction &amp; Demolition</a></li>
<li><a href="#vehicle-operations--physics">Vehicle Operations &amp; Physics</a></li>
<li><a href="#everything-else">Everything else</a></li>
</ul>
</li>
</ul>

<p dir="auto"><em>This Table of Contents was auto-generated by <a href="https://github.com/thlorenz/doctoc">DocToc</a></em></p>
</details>
<h2 tabindex="-1" dir="auto">Factory Management &amp; Automation</h2>
<p dir="auto">These games emphasize the building of assembly lines that consume input resources in order to produce output goods, often with multiple chains of inputs to intermediary outputs.</p>
<h2 tabindex="-1" dir="auto">Factory Building</h2>
<p dir="auto">This subcategory has games that involve building an actual factory, usually by manually mining resources first and then slowly adding automation.</p>
<ul dir="auto">
<li>(2023) <a href="https://store.steampowered.com/app/1450900/Desynced_Autonomous_Colony_Simulator/" rel="nofollow">Desynced: Autonomous Colony Simulator</a> ☁️</li>
<li>(2021) <a href="https://store.steampowered.com/app/1150090/Learning_Factory/" rel="nofollow">Learning Factory</a></li>
<li>(2021) <a href="https://store.steampowered.com/app/860890/Factory_Town/" rel="nofollow">Factory Town</a></li>
<li>(2021) <a href="https://store.steampowered.com/app/1366540/Dyson_Sphere_Program/" rel="nofollow">Dyson Sphere Program</a> ❤️ 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: Factorio in space, but with much better 3D graphics and the ability to expand across planets and solar systems.</li>
</ul>
</li>
<li>(2020) <a href="https://store.steampowered.com/app/526870/Satisfactory/" rel="nofollow">Satisfactory</a> ❤️ ☁️</li>
<li>(2020) <a href="https://store.steampowered.com/app/427520/Factorio/" rel="nofollow">Factorio</a> ❤️ 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: The game that created the genre. Base game is incredibly deep, with lots to optimize. Mods add even more.</li>
</ul>
</li>
<li>(2019) <a href="https://store.steampowered.com/app/244850/Space_Engineers/" rel="nofollow">Space Engineers</a> ☁️</li>
<li>(2019) <a href="https://store.steampowered.com/app/1127400/Mindustry/" rel="nofollow">Mindustry</a> ❤️</li>
<li>(2015) <a href="https://store.steampowered.com/app/300570/Infinifactory/" rel="nofollow">Infinifactory</a> ❤️ ☁️</li>
</ul>
<h2 tabindex="-1" dir="auto">Puzzle &amp; Casual</h2>
<p dir="auto">These games distill the genre down to its core, stripping away the manual resource management and focusing on routing &amp; automation.</p>
<ul dir="auto">
<li>(2020) <a href="https://store.steampowered.com/app/1318690/shapez/" rel="nofollow">shapez</a> ❤️ 👍
<ul dir="auto">
<li>Curator's Choice: A minimalist Factorio with an emphasis on conveyor belts.</li>
</ul>
</li>
<li>(2019) <a href="https://store.steampowered.com/app/1168880/MOLEKSYNTEZ/" rel="nofollow">MOLEK-SYNTEZ</a></li>
<li>(2017) <a href="https://store.steampowered.com/app/558990/Opus_Magnum/" rel="nofollow">Opus Magnum</a> ❤️ 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: Another minimalist take on the genre, distilling it down into symbols and automated pickers, sorters, and movers on rails.</li>
</ul>
</li>
<li>(2011) <a href="https://store.steampowered.com/app/92800/SpaceChem/" rel="nofollow">SpaceChem</a> ❤️</li>
</ul>
<h2 tabindex="-1" dir="auto">Vehicle Building</h2>
<p dir="auto">These games let you create your own vehicle (whether a car, ship, aircraft, or otherwise) from parts and then simulate its driving behaviors. Some have combat, while others are  peaceful.</p>
<ul dir="auto">
<li>(2023) <a href="https://store.steampowered.com/app/1532200/Mars_First_Logistics/" rel="nofollow">Mars First Logistics</a> ❤️</li>
<li>(2023) <a href="https://store.steampowered.com/app/870200/Juno_New_Origins/" rel="nofollow">Juno: New Origins</a></li>
<li>(2023) <a href="https://store.steampowered.com/app/552100/Brick_Rigs/" rel="nofollow">Brick Rigs</a> ☁️</li>
<li>(2021) <a href="https://store.steampowered.com/app/1674170/Sprocket/" rel="nofollow">Sprocket</a></li>
<li>(2021) <a href="https://store.steampowered.com/app/1078920/Main_Assembly/" rel="nofollow">Main Assembly</a> ☁️</li>
<li>(2020) <a href="https://store.steampowered.com/app/573090/Stormworks_Build_and_Rescue/" rel="nofollow">Stormworks: Build and Rescue</a> ☁️</li>
<li>(2020) <a href="https://store.steampowered.com/app/268650/From_the_Depths/" rel="nofollow">From the Depths</a></li>
<li>(2020) <a href="https://store.steampowered.com/app/346010/Besiege/" rel="nofollow">Besiege</a> ❤️ ☁️</li>
<li>(2019) <a href="https://store.steampowered.com/app/585420/Trailmakers/" rel="nofollow">Trailmakers</a> ☁️</li>
<li>(2016) <a href="https://store.steampowered.com/app/387990/Scrap_Mechanic/" rel="nofollow">Scrap Mechanic</a> ☁️</li>
<li>(2015) <a href="https://store.steampowered.com/app/397340/SimplePlanes/" rel="nofollow">SimplePlanes</a></li>
</ul>
<h2 tabindex="-1" dir="auto">City Builders &amp; Civil Engineering</h2>
<p dir="auto">These games have a primary focus on city building and utilities infrastructure (electricity, sewer, water, garbage, etc.). They are a typically blend of planning and economy management.</p>
<p dir="auto">There is a separate category, Transportation, for games that primarily deal with route-building without the accompanying city and economy management.</p>
<h2 tabindex="-1" dir="auto">Modern &amp; Near-Future</h2>
<p dir="auto">These take place in the modern, post-industrial era with modern vehicles and power grids.</p>
<ul dir="auto">
<li>(2023) <a href="https://store.steampowered.com/app/1593030/Terra_Nil/" rel="nofollow">Terra Nil</a> ☁️</li>
<li>(2022) <a href="https://store.steampowered.com/app/1594320/Captain_of_Industry/" rel="nofollow">Captain of Industry</a> ☁️</li>
<li>(2019) <a href="https://store.steampowered.com/app/492720/Tropico_6/" rel="nofollow">Tropico 6</a> ☁️
<ul dir="auto">
<li>(2011) <a href="https://store.steampowered.com/app/57690/Tropico_4/?curator_clanid=34422534" rel="nofollow">Tropico 4</a> ☁️</li>
</ul>
</li>
<li>(2015) <a href="https://store.steampowered.com/app/255710/Cities_Skylines/" rel="nofollow">Cities: Skylines</a> 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: Probably the most in-depth city builder available today, even compared to its sequel (Cities Skylines 2). A great simulation of zoning, routing, economy management, hydrology, and more. If you play only one game on this entire list, make it this one!</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Rail &amp; Sails</h2>
<p dir="auto">This is the pre-modern era, before open-sea shipping, trucks, and air freight were common. They have a focus on sailing craft, wagons &amp; trains.</p>
<ul dir="auto">
<li>(2019) <a href="https://store.steampowered.com/app/916440/Anno_1800/" rel="nofollow">Anno 1800</a> ☁️</li>
</ul>
<h2 tabindex="-1" dir="auto">Medieval &amp; Fantasy</h2>
<p dir="auto">Before industrialization, or in alternate realities with a typical "old world" fantasy theme.</p>
<ul dir="auto">
<li>(2021) <a href="https://store.steampowered.com/app/1062090/Timberborn/" rel="nofollow">Timberborn</a> 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: If beavers had civil engineering degrees and mastered fluid dynamics, what would they build? Great combo of city builder and fluid physics, plus beaver family narratives that will tug at your engineer heartstrings.</li>
</ul>
</li>
<li>(2020) <a href="https://store.steampowered.com/app/1307890/Kingdoms_Reborn/" rel="nofollow">Kingdoms Reborn</a> ☁️</li>
<li>(2020) <a href="https://store.steampowered.com/app/1281630/Anno_1404__History_Edition/" rel="nofollow">Anno 1404: History Edition</a> ☁️</li>
<li>(2019) <a href="https://store.steampowered.com/app/690830/Foundation/" rel="nofollow">Foundation</a> ☁️</li>
</ul>
<h2 tabindex="-1" dir="auto">Sci-Fi &amp; Far-Future</h2>
<p dir="auto">These take place in the post-modern era, often in space.</p>
<ul dir="auto">
<li>(2018) <a href="https://store.steampowered.com/app/464920/Surviving_Mars/" rel="nofollow">Surviving Mars</a> ☁️</li>
</ul>
<h2 tabindex="-1" dir="auto">Theme Parks &amp; Dungeons</h2>
<p dir="auto">A genre started by RollerCoaster Tycoon back in the 90s, modern games still play homage to it.</p>
<ul dir="auto">
<li>(2021) <a href="https://store.steampowered.com/app/1244460/Jurassic_World_Evolution_2/" rel="nofollow">Jurassic World Evolution 2</a> 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: Build the Jurassic Park of your dreams, complete with genetically-modified supercarnivores, hyperloops, zip lines, and hotels.</li>
<li>(2018) <a href="https://store.steampowered.com/app/648350/Jurassic_World_Evolution/" rel="nofollow">Jurassic World Evolution</a> ☁️</li>
</ul>
</li>
<li>(2020) <a href="https://store.steampowered.com/app/1368820/RollerCoaster_Tycoon_3_Complete_Edition/" rel="nofollow">RollerCoaster Tycoon® 3: Complete Edition</a> ☁️</li>
<li>(2018) <a href="https://store.steampowered.com/app/453090/Parkitect/" rel="nofollow">Parkitect</a> ☁️</li>
</ul>
<h2 tabindex="-1" dir="auto">Transportation &amp; Route Management</h2>
<p dir="auto">These games turn the roads and rails of city builders into its own genre, with a focus on route-building, traffic alleviation, and "traveling salesman" problems (finding the best route between points A, B, C, etc.).</p>
<h2 tabindex="-1" dir="auto">Realistic Route-building</h2>
<ul dir="auto">
<li>(2021) <a href="https://store.steampowered.com/app/1134710/NIMBY_Rails/" rel="nofollow">NIMBY Rails</a></li>
<li>(2019) <a href="https://store.steampowered.com/app/602320/Train_Valley_2/" rel="nofollow">Train Valley 2</a> ☁️
<ul dir="auto">
<li>(2015) <a href="https://store.steampowered.com/app/353640/Train_Valley/" rel="nofollow">Train Valley</a></li>
</ul>
</li>
<li>(2018) <a href="https://store.steampowered.com/app/503940/Railway_Empire/" rel="nofollow">Railway Empire</a> 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: A great track-building and route management simulator, with detailed terrain-track interactions and different kinds of locomotives.</li>
</ul>
</li>
<li>(2004) <a href="https://store.steampowered.com/app/1536610/OpenTTD/" rel="nofollow">OpenTTD</a> ❤️</li>
</ul>
<h2 tabindex="-1" dir="auto">Metro/Train Station Management</h2>
<ul dir="auto">
<li>(2020) <a href="https://store.steampowered.com/app/726110/Overcrowd_A_Commute_Em_Up/" rel="nofollow">STATIONflow</a></li>
<li>(2020) <a href="https://store.steampowered.com/app/726110/Overcrowd_A_Commute_Em_Up/" rel="nofollow">Overcrowd: A Commute 'Em Up</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Puzzle &amp; Casual</h2>
<ul dir="auto">
<li>(2023) <a href="https://store.steampowered.com/app/2272400/Station_to_Station/" rel="nofollow">Station to Station</a> ☁️</li>
<li>(2023) <a href="https://store.steampowered.com/app/1355090/RAILGRADE/" rel="nofollow">RAILGRADE</a></li>
<li>(2022) <a href="https://store.steampowered.com/app/1967510/Railbound/" rel="nofollow">Railbound</a></li>
<li>(2021) <a href="https://store.steampowered.com/app/1127500/Mini_Motorways/" rel="nofollow">Mini Motorways</a> ❤️ 👍
<ul dir="auto">
<li>Curator's Choice: Minimalist but addictive take on transportation networks, with roundabouts, highways, and traffic galore.</li>
</ul>
</li>
<li>(2020) <a href="https://store.steampowered.com/app/1016920/Unrailed/" rel="nofollow">Unrailed!</a></li>
<li>(2016) <a href="https://store.steampowered.com/app/361280/Turmoil/" rel="nofollow">Turmoil</a></li>
<li>(2015) <a href="https://store.steampowered.com/app/287980/Mini_Metro/" rel="nofollow">Mini Metro</a> ❤️</li>
</ul>
<h2 tabindex="-1" dir="auto">Survival &amp; Settlement Builders, Colony Sims</h2>
<p dir="auto">Similar to city builders, but with a stronger focus on surviving harsh conditions and managing limited resources, heat/cold, food, etc. In this genre, individual citizens are often more important and impactful, and the simulation is more about the micro aspects of daily life than big-picture population growth.</p>
<ul dir="auto">
<li>(2023) <a href="https://store.steampowered.com/app/1324130/Stranded_Alien_Dawn/" rel="nofollow">Stranded: Alien Dawn</a> ☁️</li>
<li>(2022) <a href="https://store.steampowered.com/app/1044720/Farthest_Frontier/" rel="nofollow">Farthest Frontier</a> ☁️</li>
<li>(2022) <a href="https://store.steampowered.com/app/975370/Dwarf_Fortress/" rel="nofollow">Dwarf Fortress</a> ☁️</li>
<li>(2022) <a href="https://store.steampowered.com/app/1336490/Against_the_Storm/" rel="nofollow">Against the Storm</a> ❤️ ☁️</li>
<li>(2021) <a href="https://store.steampowered.com/app/1029780/Going_Medieval/" rel="nofollow">Going Medieval</a> ☁️</li>
<li>(2020) <a href="https://store.steampowered.com/app/979110/Space_Haven/" rel="nofollow">Space Haven</a> ☁️</li>
<li>(2019) <a href="https://store.steampowered.com/app/644930/They_Are_Billions/" rel="nofollow">They Are Billions</a> ☁️</li>
<li>(2019) <a href="https://store.steampowered.com/app/457140/Oxygen_Not_Included/" rel="nofollow">Oxygen Not Included</a> ❤️ ☁️</li>
<li>(2018) <a href="https://store.steampowered.com/app/294100/RimWorld/" rel="nofollow">RimWorld</a> ❤️ ☁️</li>
<li>(2018) <a href="https://store.steampowered.com/app/323190/Frostpunk/" rel="nofollow">Frostpunk</a> 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: Tough survival simulator in a frozen wasteland, with enough resource, route, heat and energy management to make it interesting to the engineer-minded.</li>
</ul>
</li>
<li>(2017) <a href="https://store.steampowered.com/app/569480/Kingdoms_and_Castles/" rel="nofollow">Kingdoms and Castles</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Open-World Survival Sandboxes</h2>
<p dir="auto">A sibling genre to colony sims, here you take direct control of one person (or a small group) to try to gather resources, craft, and survive. While they don't all have the strongest engineering focus, most have at least building &amp; resource management aspects.</p>
<ul dir="auto">
<li>(2020) <a href="https://store.steampowered.com/app/383120/Empyrion__Galactic_Survival/" rel="nofollow">Empyrion - Galactic Survival</a> ☁️</li>
<li>(2018) <a href="https://store.steampowered.com/app/264710" rel="nofollow">Subnautica</a> ❤️ 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: Build your own underwater homestead on an alien planet, with lots of exotic flora, fauna, minerals and machines.</li>
<li>(2021) <a href="https://store.steampowered.com/app/848450/Subnautica_Below_Zero/" rel="nofollow">Subnautica: Below Zero</a> ☁️</li>
</ul>
</li>
<li>(2013) <a href="https://store.steampowered.com/app/108600" rel="nofollow">Project Zomboid</a> ☁️</li>
<li>(2013) <a href="https://store.steampowered.com/app/251570/7_Days_to_Die/" rel="nofollow">7 Days to Die</a> ☁️</li>
</ul>
<h2 tabindex="-1" dir="auto">Bridge-Building</h2>
<p dir="auto">These games focus on building semi-realistic (or not) bridges over cliffs, rivers, etc. with physics simulation and load management. Different material strengths and stressors test your ability to build bridges that hold up under heavy use.</p>
<ul dir="auto">
<li>(2023) <a href="https://store.steampowered.com/app/1850160/Poly_Bridge_3/" rel="nofollow">Poly Bridge 3</a> ❤️
<ul dir="auto">
<li>(2020) <a href="https://store.steampowered.com/app/1062160" rel="nofollow">Poly Bridge 2</a> ❤️</li>
<li>(2016) <a href="https://store.steampowered.com/app/367450" rel="nofollow">Poly Bridge</a></li>
</ul>
</li>
<li>(2017) <a href="https://store.steampowered.com/app/684410/Bridge_Constructor_Portal/" rel="nofollow">Bridge Constructor Portal</a>
<ul dir="auto">
<li>(2014) <a href="https://store.steampowered.com/app/319850/Bridge_Constructor_Medieval/" rel="nofollow">Bridge Constructor Medieval</a></li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Rube Goldberg Machines</h2>
<p dir="auto">These games replicate the "Incredible Machine" games of the 90s, where you build crazy contraptions out of everything from lasers to ballons to cats, then use them in humorous ways to complete puzzles.</p>
<ul dir="auto">
<li>(2022) <a href="https://store.steampowered.com/app/681640/World_of_Contraptions/" rel="nofollow">World of Contraptions</a></li>
<li>(2016) <a href="https://store.steampowered.com/app/386940/Ultimate_Chicken_Horse/" rel="nofollow">Ultimate Chicken Horse</a> ❤️ 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: A competitive, multiplayer reimagining of the genre, where your contraptions become traps that foil other players (or yourself, if you're not careful)</li>
</ul>
</li>
<li>(2016) <a href="https://store.steampowered.com/app/351920/Crazy_Machines_3/" rel="nofollow">Crazy Machines 3</a></li>
<li>(2014) <a href="https://store.steampowered.com/app/241240/Contraption_Maker/" rel="nofollow">Contraption Maker</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Hacking, Computer Science, Logic &amp; Automation</h2>
<p dir="auto">These games have a focus on programming and automation. Unlike factory games, they focus on optimizing repeatable sets of actions rather than resource inputs. Some are graphical, while others require some light scripting/coding.</p>
<ul dir="auto">
<li>(2021) <a href="https://store.steampowered.com/app/1812820/Bitburner/" rel="nofollow">Bitburner</a> ❤️</li>
<li>(2019) <a href="https://store.steampowered.com/app/619150/while_True_learn/" rel="nofollow">while True: learn()</a></li>
<li>(2018) <a href="https://store.steampowered.com/app/716490/EXAPUNKS/" rel="nofollow">EXAPUNKS</a> ❤️</li>
<li>(2016) <a href="https://store.steampowered.com/app/370360/TIS100/" rel="nofollow">TIS-100</a> ❤️</li>
<li>(2015) <a href="https://store.steampowered.com/app/365450/Hacknet/" rel="nofollow">Hacknet</a> ☁️</li>
</ul>
<h2 tabindex="-1" dir="auto">Electrical Engineering</h2>
<p dir="auto">Similar to programming games, but at a lower level in the machine. Wire circuits instead of writing code.</p>
<ul dir="auto">
<li>(2021) <a href="https://store.steampowered.com/app/1444480/Turing_Complete/" rel="nofollow">Turing Complete</a> ❤️</li>
<li>(2016) <a href="https://store.steampowered.com/app/504210/SHENZHEN_IO/" rel="nofollow">SHENZHEN I/O</a> ❤️</li>
</ul>
<h2 tabindex="-1" dir="auto">Job Simulators &amp; Unique Themes</h2>
<p dir="auto">This is sort of a catch-all category for games that don't neatly fit anywhere else, usually with a creative take on some mundane (or exciting!) job.</p>
<h2 tabindex="-1" dir="auto">Ship Crews (Space or Naval)</h2>
<ul dir="auto">
<li>(2023) <a href="https://store.steampowered.com/app/1063420/Void_Crew/" rel="nofollow">Void Crew</a> ☁️</li>
<li>(2023) <a href="https://store.steampowered.com/app/602960/Barotrauma/" rel="nofollow">Barotrauma</a> ☁️</li>
<li>(2017) <a href="https://store.steampowered.com/app/527100/Star_Trek_Bridge_Crew/" rel="nofollow">Star Trek: Bridge Crew</a></li>
<li>(2013) <a href="https://store.steampowered.com/app/247350/Artemis_Spaceship_Bridge_Simulator/" rel="nofollow">Artemis Spaceship Bridge Simulator</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Space Operations &amp; Orbital Mechanics</h2>
<ul dir="auto">
<li>(2021) <a href="https://store.steampowered.com/app/846030/V_Rings_of_Saturn/" rel="nofollow">ΔV: Rings of Saturn</a></li>
<li>(2015) <a href="https://store.steampowered.com/app/220200/Kerbal_Space_Program/" rel="nofollow">Kerbal Space Program</a> ❤️</li>
</ul>
<h2 tabindex="-1" dir="auto">Construction &amp; Demolition</h2>
<ul dir="auto">
<li>(2022) <a href="https://store.steampowered.com/app/1428100/Instruments_of_Destruction/" rel="nofollow">Instruments of Destruction</a> ❤️</li>
<li>(2022) <a href="https://store.steampowered.com/app/1273400/Construction_Simulator/" rel="nofollow">Construction Simulator</a> ☁️</li>
</ul>
<h2 tabindex="-1" dir="auto">Vehicle Operations &amp; Physics</h2>
<ul dir="auto">
<li>(2023) <a href="https://store.steampowered.com/app/1422130/SimRail__The_Railway_Simulator/" rel="nofollow">SimRail - The Railway Simulator</a> ☁️</li>
<li>(2017) <a href="https://store.steampowered.com/app/675010/MudRunner/" rel="nofollow">MudRunner</a> 👍 ☁️
<ul dir="auto">
<li>Curator's Choice: Simulated mud physics and deformable terrain make for a unique experience driving large logging trucks through the wilderness.</li>
<li>(2021) <a href="https://store.steampowered.com/app/1465360/SnowRunner/" rel="nofollow">SnowRunner</a> ☁️</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Everything else</h2>
<ul dir="auto">
<li>(2021) <a href="https://store.steampowered.com/app/673610/Airport_CEO/" rel="nofollow">Airport CEO</a> ☁️</li>
<li>(2019) <a href="https://store.steampowered.com/app/621060" rel="nofollow">PC Building Simulator</a> ☁️</li>
<li>(2001) <a href="https://store.steampowered.com/app/1190000" rel="nofollow">Car Mechanic Simulator 2021</a> ❤️ ☁️</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gang crisis shaking Sweden (122 pts)]]></title>
            <link>https://www.ft.com/content/79f0d181-bdae-4c81-a971-861ccd8d512c</link>
            <guid>38336065</guid>
            <pubDate>Sun, 19 Nov 2023 18:37:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/79f0d181-bdae-4c81-a971-861ccd8d512c">https://www.ft.com/content/79f0d181-bdae-4c81-a971-861ccd8d512c</a>, See on <a href="https://news.ycombinator.com/item?id=38336065">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="barrier-page">
<div data-o-grid-colspan="12 L6" data-component="articleBanner" data-component-unique-name="default"><p>Keep abreast of significant corporate, financial and political developments around the world. Stay informed and spot emerging risks and opportunities with independent global reporting, expert commentary and analysis you can trust.</p></div>
<div data-component="unlockBanner" data-component-unique-name="default"><p><img src="https://www.ft.com/__assets/creatives/optimizely/MAR090/key_icon.svg" alt=""><span id="text-unlockBanner-default">Subscribe to unlock this article</span></p></div>
<div data-theme="" data-component="heroOffer" data-component-unique-name="CHE-Print"><div data-o-grid-colspan="12"><p><h3>Try unlimited access</h3>
<h3><strong>Only CHF 1 for 4 weeks</strong></h3></p></div><div><div data-o-grid-colspan="12 M6"><ul>
<li>Then CHF 79 per month</li>
<li>New customers only</li>
<li>Cancel anytime during your trial</li>
</ul></div><div data-o-grid-colspan="12 M6"><p><a id="charge-button-CHE-Print" data-trackable="41218b9e-c8ae-c934-43ad-71b13fcb4465" href="https://www.ft.com/buy/offer/41218b9e-c8ae-c934-43ad-71b13fcb4465/"><span><p>Keep reading for CHF 1</p></span></a></p></div></div></div>
<p data-component="subscriptionOptionsHeader" data-component-unique-name="default"><h4 id="text-subscriptionOptionsHeader-default">Explore our subscriptions</h4></p>
<div data-component="subscriptionOptions" data-component-unique-name="CHE-Print"><div><h5 id="title-CHE-Print">Individual</h5><p>Find the plan that suits you best.</p><ul><li data-text="Digital"><a id="button1-CHE-Print" data-trackable="digital" href="https://subs.ft.com/digital_edit?ft-content-uuid=7da5d7e5-2c4d-4619-9c81-294d9a634ac4">Digital</a></li><li data-text="Print"><a id="button2-CHE-Print" data-trackable="print" href="https://subs.ft.com/spa3_uk3m?segmentId=461cfe95-f454-6e0b-9f7b-0800950bef25&amp;utm_us=JJIBAX&amp;utm_eu=WWIBEAX&amp;utm_ca=JJIBAZ&amp;utm_as=FIBAZ&amp;ft-content-uuid=7da5d7e5-2c4d-4619-9c81-294d9a634ac4">Print</a></li><li data-text="Print + Digital"><a id="button3-CHE-Print" data-trackable="digital-print" href="https://subs.ft.com/bundleoptions?segmentId=de88addc-8125-43ec-21f1-152c9886e67f&amp;utm_us=JJIBAX&amp;utm_eu=WWIBEAX&amp;utm_ca=JJIBAZ&amp;utm_as=FIBAZ">Print + Digital</a></li></ul></div></div>
<div data-component="subscriptionOptions" data-component-unique-name="default"><div><h5 id="title-default">Professional</h5><p>Premium access for businesses and educational institutions.</p><ul><li data-text="Get Started"><a id="button1-default" data-trackable="professional" href="https://professional.ft.com/en-gb/services/professional-subscriptions/?barrierName=anon_barrier&amp;ft-content-uuid=7da5d7e5-2c4d-4619-9c81-294d9a634ac4&amp;segmentId=9fbe4fe1-9315-3d67-cc6d-2bc7650c4aea">Get Started</a></li><li data-text=""><a id="button2-default" data-trackable="" href=""></a></li><li data-text=""><a id="button3-default" data-trackable="" href=""></a></li></ul><p>Check if your <a href="https://www.ft.com/licence-finder?segmentId=a0e9a794-4c6d-bb35-e4dc-8bd409e0f54f" data-trackable="edu-finder">university</a> or <a href="https://enterprise.ft.com/licence-finder?segmentId=9fb23d7d-afe4-12f3-3eaa-ff7a41e9d073" data-trackable="license-finder">organisation</a> offers FT membership to read for free.</p>
</div></div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A map of ATMs designed to scam tourists in Europe (141 pts)]]></title>
            <link>https://twitter.com/faborio/status/1725631676309463137</link>
            <guid>38335864</guid>
            <pubDate>Sun, 19 Nov 2023 18:23:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/faborio/status/1725631676309463137">https://twitter.com/faborio/status/1725631676309463137</a>, See on <a href="https://news.ycombinator.com/item?id=38335864">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don’t fret — let’s give it another shot.</span></p><br></div></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Altman sought billions for AI chip venture before OpenAI ouster (323 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2023-11-19/altman-sought-billions-for-ai-chip-venture-before-openai-ouster</link>
            <guid>38335525</guid>
            <pubDate>Sun, 19 Nov 2023 17:59:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2023-11-19/altman-sought-billions-for-ai-chip-venture-before-openai-ouster">https://www.bloomberg.com/news/articles/2023-11-19/altman-sought-billions-for-ai-chip-venture-before-openai-ouster</a>, See on <a href="https://news.ycombinator.com/item?id=38335525">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[StyleTTS2 – open-source Eleven Labs quality Text To Speech (540 pts)]]></title>
            <link>https://github.com/yl4579/StyleTTS2</link>
            <guid>38335255</guid>
            <pubDate>Sun, 19 Nov 2023 17:40:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yl4579/StyleTTS2">https://github.com/yl4579/StyleTTS2</a>, See on <a href="https://news.ycombinator.com/item?id=38335255">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models</h2>
<h3 tabindex="-1" dir="auto">Yinghao Aaron Li, Cong Han, Vinay S. Raghavan, Gavin Mischler, Nima Mesgarani</h3>
<blockquote>
<p dir="auto">In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS synthesis on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs.</p>
</blockquote>
<p dir="auto">Paper: <a href="https://arxiv.org/abs/2306.07691" rel="nofollow">https://arxiv.org/abs/2306.07691</a></p>
<p dir="auto">Audio samples: <a href="https://styletts2.github.io/" rel="nofollow">https://styletts2.github.io/</a></p>
<p dir="auto"><a href="https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<h2 tabindex="-1" dir="auto">TODO</h2>
<ul>
<li> Training and inference demo code for single-speaker models (LJSpeech)</li>
<li> Test training code for multi-speaker models (VCTK and LibriTTS)</li>
<li> Finish demo code for multispeaker model and upload pre-trained models</li>
<li> Add a finetuning script for new speakers with base pre-trained multispeaker models</li>
<li> Fix DDP (accelerator) for <code>train_second.py</code> <strong>(I have tried everything I could to fix this but had no success, so if you are willing to help, please see <a href="https://github.com/yl4579/StyleTTS2/issues/7" data-hovercard-type="issue" data-hovercard-url="/yl4579/StyleTTS2/issues/7/hovercard">#7</a>)</strong></li>
</ul>
<h2 tabindex="-1" dir="auto">Pre-requisites</h2>
<ol dir="auto">
<li>Python &gt;= 3.7</li>
<li>Clone this repository:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/yl4579/StyleTTS2.git
cd StyleTTS2"><pre>git clone https://github.com/yl4579/StyleTTS2.git
<span>cd</span> StyleTTS2</pre></div>
<ol start="3" dir="auto">
<li>Install python requirements:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<p dir="auto">On Windows add:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -U"><pre>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -U</pre></div>
<p dir="auto">Also install phonemizer and espeak if you want to run the demo:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install phonemizer
sudo apt-get install espeak-ng"><pre>pip install phonemizer
sudo apt-get install espeak-ng</pre></div>
<ol start="4" dir="auto">
<li>Download and extract the <a href="https://keithito.com/LJ-Speech-Dataset/" rel="nofollow">LJSpeech dataset</a>, unzip to the data folder and upsample the data to 24 kHz. The text aligner and pitch extractor are pre-trained on 24 kHz data, but you can easily change the preprocessing and re-train them using your own preprocessing.
For LibriTTS, you will need to combine train-clean-360 with train-clean-100 and rename the folder train-clean-460 (see <a href="https://github.com/yl4579/StyleTTS/blob/main/Data/val_list_libritts.txt">val_list_libritts.txt</a> as an example).</li>
</ol>
<h2 tabindex="-1" dir="auto">Training</h2>
<p dir="auto">First stage training:</p>
<div dir="auto" data-snippet-clipboard-copy-content="accelerate launch train_first.py --config_path ./Configs/config.yml"><pre>accelerate launch train_first.py --config_path ./Configs/config.yml</pre></div>
<p dir="auto">Second stage training <strong>(DDP version not working, so the current version uses DP, again see <a href="https://github.com/yl4579/StyleTTS2/issues/7" data-hovercard-type="issue" data-hovercard-url="/yl4579/StyleTTS2/issues/7/hovercard">#7</a> if you want to help)</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python train_second.py --config_path ./Configs/config.yml"><pre>python train_second.py --config_path ./Configs/config.yml</pre></div>
<p dir="auto">You can run both consecutively and it will train both the first and second stages. The model will be saved in the format "epoch_1st_%05d.pth" and "epoch_2nd_%05d.pth". Checkpoints and Tensorboard logs will be saved at <code>log_dir</code>.</p>
<p dir="auto">The data list format needs to be <code>filename.wav|transcription|speaker</code>, see <a href="https://github.com/yl4579/StyleTTS2/blob/main/Data/val_list.txt">val_list.txt</a> as an example. The speaker labels are needed for multi-speaker models because we need to sample reference audio for style diffusion model training.</p>
<h3 tabindex="-1" dir="auto">Important Configurations</h3>
<p dir="auto">In <a href="https://github.com/yl4579/StyleTTS2/blob/main/Configs/config.yml">config.yml</a>, there are a few important configurations to take care of:</p>
<ul dir="auto">
<li><code>OOD_data</code>: The path for out-of-distribution texts for SLM adversarial training. The format should be <code>text|anything</code>.</li>
<li><code>min_length</code>: Minimum length of OOD texts for training. This is to make sure the synthesized speech has a minimum length.</li>
<li><code>max_len</code>: Maximum length of audio for training. The unit is frame. Since the default hop size is 300, one frame is approximately <code>300 / 24000</code> (0.125) second. Lowering this if you encounter the out-of-memory issue.</li>
<li><code>multispeaker</code>: Set to true if you want to train a multispeaker model. This is needed because the architecture of the denoiser is different for single and multispeaker models.</li>
<li><code>batch_percentage</code>: This is to make sure during SLM adversarial training there are no out-of-memory (OOM) issues. If you encounter OOM problem, please set a lower number for this.</li>
</ul>
<h3 tabindex="-1" dir="auto">Pre-trained modules</h3>
<p dir="auto">In <a href="https://github.com/yl4579/StyleTTS2/tree/main/Utils">Utils</a> folder, there are three pre-trained models:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/yl4579/StyleTTS2/tree/main/Utils/ASR">ASR</a> folder</strong>: It contains the pre-trained text aligner, which was pre-trained on English (LibriTTS), Japanese (JVS), and Chinese (AiShell) corpus. It works well for most other languages without fine-tuning, but you can always train your own text aligner with the code here: <a href="https://github.com/yl4579/AuxiliaryASR">yl4579/AuxiliaryASR</a>.</li>
<li><strong><a href="https://github.com/yl4579/StyleTTS2/tree/main/Utils/JDC">JDC</a> folder</strong>: It contains the pre-trained pitch extractor, which was pre-trained on English (LibriTTS) corpus only. However, it works well for other languages too because F0 is independent of language. If you want to train on singing corpus, it is recommended to train a new pitch extractor with the code here: <a href="https://github.com/yl4579/PitchExtractor">yl4579/PitchExtractor</a>.</li>
<li><strong><a href="https://github.com/yl4579/StyleTTS2/tree/main/Utils/PLBERT">PLBERT</a> folder</strong>: It contains the pre-trained <a href="https://arxiv.org/abs/2301.08810" rel="nofollow">PL-BERT</a> model, which was pre-trained on English (Wikipedia) corpus only. It probably does not work very well on other languages, so you will need to train a different PL-BERT for different languages using the repo here: <a href="https://github.com/yl4579/PL-BERT">yl4579/PL-BERT</a>. You can also replace this module with other phoneme BERT models like <a href="https://arxiv.org/abs/2305.19709" rel="nofollow">XPhoneBERT</a> which is pre-trained on more than 100 languages.</li>
</ul>
<h3 tabindex="-1" dir="auto">Common Issues</h3>
<ul dir="auto">
<li><strong>Loss becomes NaN</strong>: If it is the first stage, please make sure you do not use mixed precision, as it can cause loss becoming NaN for some particular datasets when the batch size is not set properly (need to be more than 16 to work well). For the second stage, please also experiment with different batch sizes, with higher batch sizes being more likely to cause NaN loss values. We recommend the batch size to be 16. You can refer to issues <a href="https://github.com/yl4579/StyleTTS2/issues/10" data-hovercard-type="issue" data-hovercard-url="/yl4579/StyleTTS2/issues/10/hovercard">#10</a> and <a href="https://github.com/yl4579/StyleTTS2/issues/11" data-hovercard-type="issue" data-hovercard-url="/yl4579/StyleTTS2/issues/11/hovercard">#11</a> for more details.</li>
<li><strong>Out of memory</strong>: Please either use lower <code>batch_size</code> or <code>max_len</code>. You may refer to issue <a href="https://github.com/yl4579/StyleTTS2/issues/10" data-hovercard-type="issue" data-hovercard-url="/yl4579/StyleTTS2/issues/10/hovercard">#10</a> for more information.</li>
</ul>
<h2 tabindex="-1" dir="auto">Finetuning</h2>
<p dir="auto">The script is modified from <code>train_second.py</code> which uses DP, as DDP does not work for <code>train_second.py</code>. Please see the bold section above if you are willing to help with this problem.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python train_finetune.py --config_path ./Configs/config_ft.yml"><pre>python train_finetune.py --config_path ./Configs/config_ft.yml</pre></div>
<p dir="auto">Please make sure you have the LibriTTS checkpoint downloaded and unzipped under the folder. The default configuration <code>config_ft.yml</code> finetunes on LJSpeech with 1 hour of speech data (around 1k samples) for 50 epochs. This took about 4 hours to finish on four NVidia A100. The quality is slightly worse (similar to NaturalSpeech on LJSpeech) than LJSpeech model trained from scratch with 24 hours of speech data, which took around 2.5 days to finish on four A100.</p>
<p dir="auto"><a href="https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Finetune_Demo.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<h2 tabindex="-1" dir="auto">Inference</h2>
<p dir="auto">Please refer to <a href="https://github.com/yl4579/StyleTTS2/blob/main/Demo/Inference_LJSpeech.ipynb">Inference_LJSpeech.ipynb</a> (single-speaker) and <a href="https://github.com/yl4579/StyleTTS2/blob/main/Demo/Inference_LibriTTS.ipynb">Inference_LibriTTS.ipynb</a> (multi-speaker) for details. For LibriTTS, you will also need to download <a href="https://drive.google.com/file/d/1YhQO4O4dAsvkMzWZM8nVFMglYyi554YT" rel="nofollow">reference_audio.zip</a> and unzip it under the <code>demo</code> before running the demo.</p>
<ul dir="auto">
<li>
<p dir="auto">The pretrained StyleTTS 2 on LJSpeech corpus in 24 kHz can be downloaded at <a href="https://drive.google.com/file/d/1K3jt1JEbtohBLUA0X75KLw36TW7U1yxq/view?usp=sharing" rel="nofollow">StyleTTS 2 LJSpeech Link</a>.</p>
<p dir="auto"><a href="https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Demo_LJSpeech.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
</li>
<li>
<p dir="auto">The pretrained StyleTTS 2 model on LibriTTS can be downloaded at <a href="https://drive.google.com/file/d/1jK_VV3TnGM9dkrIMsdQ_upov8FrIymr7/view" rel="nofollow">StyleTTS 2 LibriTTS Link</a>.</p>
<p dir="auto"><a href="https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Demo_LibriTTS.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
</li>
</ul>
<p dir="auto"><em><strong>Before using these models, you agree to inform the listeners that the speech samples are synthesized by StyleTTS 2 models, unless you have the permission to use the voice you synthesize. That is, when using it for voice cloning, you also agree to only use voices whose speakers grant the permission to have their voice cloned, either directly or by license before making synthesized voices pubilc.</strong></em></p>
<h3 tabindex="-1" dir="auto">Common Issues</h3>
<ul dir="auto">
<li><strong>High-pitched background noise</strong>: This is caused by numerical float differences in older GPUs. For more details, please refer to issue <a href="https://github.com/yl4579/StyleTTS2/issues/13" data-hovercard-type="issue" data-hovercard-url="/yl4579/StyleTTS2/issues/13/hovercard">#13</a>. Basically, you will need to use more modern GPUs or do inference on CPUs.</li>
</ul>
<h2 tabindex="-1" dir="auto">References</h2>
<ul dir="auto">
<li><a href="https://github.com/archinetai/audio-diffusion-pytorch">archinetai/audio-diffusion-pytorch</a></li>
<li><a href="https://github.com/jik876/hifi-gan">jik876/hifi-gan</a></li>
<li><a href="https://github.com/rishikksh20/iSTFTNet-pytorch">rishikksh20/iSTFTNet-pytorch</a></li>
<li><a href="https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts/tree/master/project/01-nsf">nii-yamagishilab/project-NN-Pytorch-scripts/project/01-nsf</a></li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From airlines to ticket sellers, companies fight U.S. to keep junk fees (211 pts)]]></title>
            <link>https://www.washingtonpost.com/business/2023/11/19/companies-lobbyists-fight-junk-fees/</link>
            <guid>38334126</guid>
            <pubDate>Sun, 19 Nov 2023 16:25:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/business/2023/11/19/companies-lobbyists-fight-junk-fees/">https://www.washingtonpost.com/business/2023/11/19/companies-lobbyists-fight-junk-fees/</a>, See on <a href="https://news.ycombinator.com/item?id=38334126">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/business/2023/11/19/companies-lobbyists-fight-junk-fees/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Terraform Cloud Pricing Changes Sticker Shock (224 pts)]]></title>
            <link>https://shavingtheyak.com/2023/10/28/hashicorps-terraform-cloud-rum-pricing-sticker-shock/</link>
            <guid>38334102</guid>
            <pubDate>Sun, 19 Nov 2023 16:23:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shavingtheyak.com/2023/10/28/hashicorps-terraform-cloud-rum-pricing-sticker-shock/">https://shavingtheyak.com/2023/10/28/hashicorps-terraform-cloud-rum-pricing-sticker-shock/</a>, See on <a href="https://news.ycombinator.com/item?id=38334102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">

<figure><img decoding="async" fetchpriority="high" width="1024" height="585" src="https://shavingtheyak.com/wp-content/uploads/2023/11/tfbridge1-1-1024x585.png" alt="" srcset="https://shavingtheyak.com/wp-content/uploads/2023/11/tfbridge1-1-1024x585.png 1024w, https://shavingtheyak.com/wp-content/uploads/2023/11/tfbridge1-1-300x172.png 300w, https://shavingtheyak.com/wp-content/uploads/2023/11/tfbridge1-1-768x439.png 768w, https://shavingtheyak.com/wp-content/uploads/2023/11/tfbridge1-1-850x486.png 850w, https://shavingtheyak.com/wp-content/uploads/2023/11/tfbridge1-1.png 1165w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
<p>Terraform Cloud is convenient, especially when you have many teams, microservices and projects all needing their own infrastructure as code (IAC) along with SSO regulated access control and the need to run IAC from certain locations inside protected private networks. While Terraform itself can be annoying and difficult at times, it is still the leading tool for many IAC use-cases. Once you get beyond a small amount of IAC, and it starts to become used in a democratized way across your organization, the need to structure and monitor its use becomes important, especially for answering those pesky ‘who has access to X’ questions from compliance and security.</p>
<p>I used to work on a team where we were the only people using Terraform, and we all just ran it on a single server, switching to the same user to run things. Every once in a while we would ping in slack ‘hey who is running an apply – can I kill your lock?’ – This gets old fast, but in a small setup with the state file in a safe place like S3 with versioning, it runs just fine. It is also ‘free’. I put the word free in quotes here as a reminder that almost every open source project you can use has a cost, in time and headaches, that hopefully you aren’t dealing with if you pay a vendor for the same service. In the case of Terraform Cloud however, you still have the same issues you run into when using Terraform for ‘free’, but you get to share those issues easily with everyone in your org by pasting your run link in slack and praying that somehow, someone else has run into the same issues you have and can decipher the cryptic error messages on your screen.</p>
<p>Enter the SAAS business model. Now I’m generally a proponent of managed services that save time, as I personally don’t want to spend all my time troubleshooting the kubernetes control plane, or figuring out what caused a mongo cluster to get hours behind on replication in prod. I like to get things done, help people solve their problems and generally help make things better for the business and better for the people I interact with every day. Sure it can be fun to solve a pesky networking issue that has been plaguing your cluster for months, but there’s something to be said for having a hand in getting something new rolled out, whether it is a feature for the customers or a feature/automation that helps the internal org move faster. One of the biggest parts of being engaged is believing that the work you do ‘matters’. Managed services can help out on this front, to a degree.</p>
<figure><img decoding="async" width="1024" height="277" src="https://shavingtheyak.com/wp-content/uploads/2023/11/eaas1-Copy-1024x277.png" alt="" srcset="https://shavingtheyak.com/wp-content/uploads/2023/11/eaas1-Copy-1024x277.png 1024w, https://shavingtheyak.com/wp-content/uploads/2023/11/eaas1-Copy-300x81.png 300w, https://shavingtheyak.com/wp-content/uploads/2023/11/eaas1-Copy-768x208.png 768w, https://shavingtheyak.com/wp-content/uploads/2023/11/eaas1-Copy-850x230.png 850w, https://shavingtheyak.com/wp-content/uploads/2023/11/eaas1-Copy.png 1117w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
<p>Now Terraform Cloud is generally a solid product. We use it constantly, and according to something I saw recently from our rep we are managing &gt; 35K ‘resources’ with it. This can be a bit misleading, as what that number means in reality is ‘resource blocks’ of Terraform code referenced in corresponding state files. Some physical/real infrastructure resources will use up more than one resource block of code. In the current version of the AWS provider for example, a single S3 bucket can end up being more than 10 different blocks of ‘resources’ due to different aspects of the bucket configuration being defined by each resource type.</p>
<p>The downside of managed services, of course, is possible lock-in, and unexpected changes to pricing or to the product itself. From my understanding Hashicorp is generally full of brilliant folks, and I’m sure that they noticed that a number of companies were using Terraform Cloud (TFC) to manage a ton of resources and had only a few users. In a user-based pricing model this makes sense, but for most companies who ‘need’ to use a product like TFC they need it to manage the IAC runs and access of their developer user base. We currently have ~500 workspaces and hundreds of users in many teams managed automatically through our SSO provider. So TFC for us was already on the ‘expensive’ list. You know the list I’m talking about right? The one that gets passed around every year or every quarter where you hear grumbling from upper management about costs, and you aren’t sure if a vendor contract will be renewed again this time? Generally after a few meetings defending the use of the product and the costs, things settle down again and you wait for the same thing to happen again a year later.</p>
<p>So without any warning we discover that our contract will suddenly go from being the current user-based pricing to their new ‘Resources Under Management’ (RUM) pricing model. I’m sure for some people there isn’t much of a difference, but for for us the costs balloon to over 3x the current costs. This moves our usage of TFC from the ‘expensive but necessary’ list to ‘next year’s migration project’ list. Immediately. Looking around online for other people’s reactions turns up a few amusing things:</p>
<ul>
<li>Reddit thread: One user comment stands out the most to me here: “<em><strong>The pricing on Terraform literally just jumped to almost same as we are paying for the AWS resources it manages.</strong></em>” <a href="https://www.reddit.com/r/Terraform/comments/13jgzc5/terraform_new_pricing/" target="_blank" rel="noreferrer noopener">https://www.reddit.com/r/Terraform/comments/13jgzc5/terraform_new_pricing/</a></li>
</ul>
<ul>
<li>Medium: “<em><strong>The fundamental problem seems to be that the pricing model is okay for small and medium businesses who are just starting off, but it just does not scale.</strong></em>” – <a href="https://medium.com/@DiggerHQ/navigate-terraform-clouds-updated-pricing-strategy-by-moving-to-these-alternatives-84cde063ec3" target="_blank" rel="noreferrer noopener">https://medium.com/@DiggerHQ/navigate-terraform-clouds-updated-pricing-strategy-by-moving-to-these-alternatives-84cde063ec3</a></li>
</ul>
<figure><img decoding="async" width="1024" height="585" src="https://shavingtheyak.com/wp-content/uploads/2023/11/rugpull1-1024x585.png" alt="" srcset="https://shavingtheyak.com/wp-content/uploads/2023/11/rugpull1-1024x585.png 1024w, https://shavingtheyak.com/wp-content/uploads/2023/11/rugpull1-300x171.png 300w, https://shavingtheyak.com/wp-content/uploads/2023/11/rugpull1-768x439.png 768w, https://shavingtheyak.com/wp-content/uploads/2023/11/rugpull1-1536x878.png 1536w, https://shavingtheyak.com/wp-content/uploads/2023/11/rugpull1-850x486.png 850w, https://shavingtheyak.com/wp-content/uploads/2023/11/rugpull1.png 1792w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
<p>Now I’m all for the freedom of companies to price their products in a way that enables them to prosper, but changing pricing models on an existing customer base that has to spend significant money and time to move to other, cheaper products tends to leave a bad taste in my mouth. Remember the Unity game engine fiasco recently? Look it up. Things didn’t go well when some folks realized the new pricing would put them out of business. Not a fair comparison at all to our TFC pricing change here, but it’s an example of something that seems to happen over and over nowadays. Someone starts a business, gets a ton of subscribers and a large chunk of market share and then changes their pricing model once they feel secure enough in doing so. This is the sort of thing I worry about with vendors, especially ones like AWS. I also cannot be the only person who keeps swearing off using Uber and Lyft after discovering their pricing model now makes taxis seem cheap at times.</p>
<p>Another tactic being used in our TFC pricing model change here is the ‘<strong>get them in the door cheap on a pricing model that will never decrease</strong>‘ idea.</p>
<p>Lets unpack this a bit. Why does it matter that the pricing went from user-based to resources based? I’ll tell you why. <strong>You will almost never get rid of more IAC code in a month than you add unless the sky is falling.</strong> Lets say you already have a bunch of infra and you want it all managed with IAC. Each month you will be adding more and more resources to your list of things managed using TFC. Lets say you are a small company just starting out. The first 500 resources are free. You start to add some IAC to your TFC account and pretty soon you are up to thousands of ‘resources’. This in itself isn’t much, but if you continue to grow it gets to be pricey. The hope is that larger companies with many users will get pulled in by the super low starting costs, and after a bunch of users all doing their job start to add a bunch of code individually the pricing will just keep going up and up and up over time.</p>
<p>This won’t happen in a vacuum, however. Companies will be forced to become picky about what they use IAC for, which in some ways goes counter to the reason you started to use IAC in the first place. Remember all those complicated setups in AWS that used to be fully configured via code? Now you will be spending hours on an incident call trying to figure out why one of them stopped working. Or worse, you will start using CloudFormation. (Hey that’s a disturbing idea – I can reduce the number of resources I use in TFC by just deploying CloudFormation using Terraform)</p>
<figure><img decoding="async" loading="lazy" width="896" height="512" src="https://shavingtheyak.com/wp-content/uploads/2023/11/moneyvac1.png" alt="" srcset="https://shavingtheyak.com/wp-content/uploads/2023/11/moneyvac1.png 896w, https://shavingtheyak.com/wp-content/uploads/2023/11/moneyvac1-300x171.png 300w, https://shavingtheyak.com/wp-content/uploads/2023/11/moneyvac1-768x439.png 768w, https://shavingtheyak.com/wp-content/uploads/2023/11/moneyvac1-850x486.png 850w" sizes="(max-width: 896px) 100vw, 896px"></figure>
<p>All jokes aside, this also seems to be eerily similar to the cloud services business model doesn’t it? How many companies started out using cloud services because of the convenience, flexibility and scalability of services, yet discovered years later that they are stuck paying enormous monthly sums to maintain their infrastructure in the cloud? Every little piece of it costs money, and it adds up, like a financial torture device, over time. How many times have we heard the argument about how much cheaper the cloud is due to ‘reasons’ only to discover later that we are seemingly paying the full up-front cost of a real server in 8 months when using one in the cloud? I haven’t seen any stories yet about companies moving from the cloud back to on-prem where they discover, to their surprise, that they have increased their overhead costs. It seems to be a common theme. Start small in the cloud, and if you get big later once your infrastructure footprint becomes stable, realize huge savings moving back to your own servers. Terraform Cloud now seems to work the same way.</p>
<p>The difference here is size, complexity and the fact that Terraform itself is/was open source. With the recent fork to open tofu, and the need for something like Terraform Cloud, I wouldn’t at all be surprised if someone creates a self-hosted clone of TFC and some companies start to use it instead. Only time will tell. In the meantime, we will be evaluating replacement solutions, hoping that in the end whatever we end up using doesn’t also suddenly change underneath us, requiring yet another large and complex migration project. Maybe some of the companies offering replacement services can do the migrations for free to get our business?</p>
<hr>
<p>-AD-</p>
<p>You may know that <a href="http://namecheap.pxf.io/oqJ7q9" data-type="link" data-id="https://namecheap.pxf.io/VmdGm3" target="_blank" rel="noreferrer noopener">Namecheap</a> does domain names, right? Did you also know that they do:</p>
<ul>
<li>Low Cost Server and Site Hosting</li>
<li>CDN</li>
<li>DNS</li>
<li>Managed WordPress Hosting</li>
<li>Marketing</li>
<li>Email Solutions</li>
<li>Anti-Spam Solutions</li>
<li>Personal VPN</li>
<li>Cyber Insurance?</li>
</ul>
<p>The <em>internet isn’t free</em>, so why spend more than you need to on your internet services?</p>
<p>Check out <a rel="noreferrer noopener" href="http://namecheap.pxf.io/oqJ7q9" data-type="link" data-id="https://namecheap.pxf.io/VmdGm3" target="_blank">NameCheap’s</a> Offerings Today!</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I will always prefer to work from home (253 pts)]]></title>
            <link>https://shavingtheyak.com/2023/10/25/wfh-part1/</link>
            <guid>38334084</guid>
            <pubDate>Sun, 19 Nov 2023 16:21:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shavingtheyak.com/2023/10/25/wfh-part1/">https://shavingtheyak.com/2023/10/25/wfh-part1/</a>, See on <a href="https://news.ycombinator.com/item?id=38334084">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">

<figure><img decoding="async" fetchpriority="high" width="896" height="512" src="https://shavingtheyak.com/wp-content/uploads/2023/11/wfh1.png" alt="" srcset="https://shavingtheyak.com/wp-content/uploads/2023/11/wfh1.png 896w, https://shavingtheyak.com/wp-content/uploads/2023/11/wfh1-300x171.png 300w, https://shavingtheyak.com/wp-content/uploads/2023/11/wfh1-768x439.png 768w, https://shavingtheyak.com/wp-content/uploads/2023/11/wfh1-850x486.png 850w" sizes="(max-width: 896px) 100vw, 896px"></figure>
<p>I’ve been working from home since long before the pandemic. It’s been almost 10 years of partial ‘wfh’ and 6 years of full time ‘wfh’ for me, and frankly I don’t see it ever changing on my end, without threats of physical violence or bags of money.</p>
<p>Some folks really missed the office social club when the shutdown happened, and I get that. But being a ‘tech person’ who needs to focus on things in a deep way, and having to share some open plan office space with a hundred other people every day… The two things just don’t mix well. In addition, being in the DevOps/SRE space means that I often need space to bang my head against the wall (figuratively of course) or yell out about the injustice of yet another bug in a Terraform provider that I don’t have the time to upgrade from.</p>
<p>There’s just no way the tradeoff of going back to an office would work for me. Lets look at the two sides here from my perspective and see how it might compare to yours:</p>
<div>
<div>
<figure><p data-align="center"><strong>The Office</strong></p></figure>
<ul>
<li>Noisy, tends to be either a ghost town or crowded, depending on the day.</li>
<li>Some distance away, requiring anywhere from 30-90 minutes a day on average commute. You will never get this time back. Lets not talk about climate change, either.</li>
<li>Vehicle and toll road costs can add up. In my area I would spend $200/mo on tolls just to drive to an office.</li>
<li>Just when I am deep in some twisted train of thought involving a failing JS callback or missing network packets in Kubernetes someone ambles over to my cubicle to ask me some random question about helm that they could have answered in 2 seconds using the search function of their web browser. Train derailed.</li>
<li>Ever have a bad boss that you are certain will be a later source of your soon to happen PTSD? Good luck getting away from this person at the office.</li>
<li>The coffee. Honestly folks, don’t cheap out on the coffee at the office. Sometimes office coffee can be good, but after a round of budget cuts or layoffs it tends to end up as collateral damage.</li>
<li>Child emergency? Parent in the hospital? It’s tough to deal with these at an office. If only there was a way you could just go somewhere else more convenient to get both things done without endangering your next promotion or performance review…</li>
</ul>
</div>
<div>
<figure><p data-align="center"><strong>WFH</strong></p></figure>
<ul>
<li>As noisy or quiet as I darn well want it to be.</li>
<li>My cats may or may not be allowed to attend standup, depending on who is present that day</li>
<li>If I feel like a nice cool Bernie 60 degrees or a sunny so-cal 78, I walk over to the temp control and make it so. Try that at the office and see what happens.</li>
<li>My office at home is a dedicated space to my work. Once you add in the 4 monitors, personal artwork, extra large desk, soft couch, exercise equipment and drink fridge – it’s clear that I would have to be in the C suite to normally afford such luxury at ‘the office’.</li>
<li>That 30-90 minutes of time wasted every day commuting to an office? I exercise, meditate, or do other important things that otherwise would be blocked by excuses or general end of day lethargy. If something breaks in prod – hey I’m right there to fix it anyway, without fuss, after hours if needed.</li>
<li>Why would I take a sick day if I can still sit at my chair and work? No need to wear a mask during standup either, unless I catch some wild new zoom-transmitted virus. If I feel tired – hey look, there’s a nice couch right there and I can still get some work done.</li>
</ul>
</div>
</div>
<figure><img decoding="async" width="717" height="410" src="https://shavingtheyak.com/wp-content/uploads/2023/11/wfh_or_not1.png" alt="" srcset="https://shavingtheyak.com/wp-content/uploads/2023/11/wfh_or_not1.png 717w, https://shavingtheyak.com/wp-content/uploads/2023/11/wfh_or_not1-300x172.png 300w" sizes="(max-width: 717px) 100vw, 717px"></figure>
<p>Management’s arguments against WFH:</p>
<ol>
<li>“<strong>Workers aren’t as productive at home</strong>” – Some workers. Probably the same workers who weren’t productive at the office either, but since there wasn’t anything in place to measure that anyway other than the ‘butt in seat’ metric, now they seem to be AWOL. If you can’t measure your employee’s productivity and hold them accountable to that outside of an office, then what are you managing?</li>
<li>“<strong>People need to collaborate more to be innovative</strong>” – Don’t talk to me about collaboration when I sometimes have to turn off slack to be able to hear myself think. Yes I agree in principle that it may be easier to randomly chat about technical subjects walking around the office but in that case am I getting work done? Could I have just messaged someone instead and gotten the same answer, allowing them to respond when they had the mental bandwidth to do so? Brainstorming also works better from the couch, just saying.</li>
<li>“<strong>People need to be in the office to be a part of the TEAM</strong>” – This may actually be somewhat true. I don’t feel as much a part of the company, the team when I work from my home office. I feel more like a lone agent, fulfilling my own need to have a work ethic and trying to do what ‘s right for our customers. Of course I’m on a team, but I wouldn’t be that upset if a new job showed up and I decided to take it. After all, my office doesn’t even change. I would probably miss the corporate culture of course, if I already like my job, but I certainly won’t feel guilty about getting a better job if I am unhappy. I suspect that many people who aren’t happy with their jobs stay with them out of a sense of attachment to the social nature of the office and the familiarity of the office workplace – and on some level, management knows this.</li>
</ol>
<hr>
<p>This gets at a much deeper issue, however. <strong>Should</strong> we be loyal to companies who would lay us off at the drop of a hat to pad their quarterly earnings? Companies who decide the projects we work on have no merit but can’t be bothered to reassign us to more critical work after spending the time and money to get us in the door? Many of us feel the need to be loyal but what are we getting in return?</p>
<figure><img decoding="async" width="896" height="512" src="https://shavingtheyak.com/wp-content/uploads/2023/11/layoffs1.png" alt="" srcset="https://shavingtheyak.com/wp-content/uploads/2023/11/layoffs1.png 896w, https://shavingtheyak.com/wp-content/uploads/2023/11/layoffs1-300x171.png 300w, https://shavingtheyak.com/wp-content/uploads/2023/11/layoffs1-768x439.png 768w, https://shavingtheyak.com/wp-content/uploads/2023/11/layoffs1-850x486.png 850w" sizes="(max-width: 896px) 100vw, 896px"></figure>
<p>In tech, you can also hurt your career if you stay with a legacy stack for too long, and your skillset no longer matches the needs of the marketplace. Don’t want to get caught out on the next round of layoffs without being good at some new technology? Better start looking now for a company willing to use your skillset to help them move to that new technology from the old one.</p>
<p>When it comes time for a possible raise, how many companies still give them out, and when they do are they enough to mitigate the cost of living increases due to inflation?</p>
<p>In a way, companies in today’s market by their behavior drive employees to look for other jobs to get a raise or improve their skillset. There is little investment in employees or their future beyond the HR skill training website and generally you are considered a ‘cog in the machine’ – I don’t think we should be loyal to that sort of setup or treatment. I don’t really know why or how things got this way, but if you work for a company that is not like this – consider yourself lucky.</p>
<hr>
<p>-AD-</p>
<p>You may know that <a href="https://namecheap.pxf.io/VmdGm3" data-type="link" data-id="https://namecheap.pxf.io/VmdGm3" target="_blank" rel="noreferrer noopener">Namecheap</a> does domain names, right? Did you also know that they do:</p>
<ul>
<li>Low Cost Server and Site Hosting</li>
<li>CDN</li>
<li>DNS</li>
<li>Managed WordPress Hosting</li>
<li>Marketing</li>
<li>Email Solutions</li>
<li>Anti-Spam Solutions</li>
<li>Personal VPN</li>
<li>Cyber Insurance?</li>
</ul>
<p>The <em>internet isn’t free</em>, so why spend more than you need to on your internet services?</p>
<p>Check out <a rel="noreferrer noopener" href="https://namecheap.pxf.io/VmdGm3" data-type="link" data-id="https://namecheap.pxf.io/VmdGm3" target="_blank">NameCheap’s</a> Offerings Today!</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Agency Declares 21 Species Now Extinct L (254 pts)]]></title>
            <link>https://www.pbsnc.org/blogs/science/us-agency-declares-21-species-now-extinct/</link>
            <guid>38333790</guid>
            <pubDate>Sun, 19 Nov 2023 15:56:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pbsnc.org/blogs/science/us-agency-declares-21-species-now-extinct/">https://www.pbsnc.org/blogs/science/us-agency-declares-21-species-now-extinct/</a>, See on <a href="https://news.ycombinator.com/item?id=38333790">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id=""><h2>A Wake-Up Call to Conserve Endangered Wildlife</h2><p>The <a href="https://www.fws.gov/" target="_blank" rel="noopener noreferrer"><u>U.S. Fish and Wildlife Service (FWS)</u></a> has announced that 21 species once deemed “endangered” by the <a href="https://www.fws.gov/law/endangered-species-act" target="_blank" rel="noopener noreferrer"><u>Endangered Species Act (ESA)</u></a> are now extinct. Officially, this process is called “delisting species due to extinction.”&nbsp;</p><p>&nbsp;Unofficially, you could say the species are dead and never to be seen again.&nbsp;</p><p>The agency says it has determined that the species are extinct (based on reviews of the best available science for each species) and should be removed from protection under the ESA.&nbsp;</p><p>“Our determination of whether the best available information indicates that a species is extinct included an analysis of the following criterion: detectability of the species, adequacy of survey efforts and the time since last detection,” the agency said in a statement.&nbsp;&nbsp;</p><p>Most of the species were listed as endangered under the ESA in the 1970s and 1980s; at the time of the listing, the species were in exceptionally small numbers or extinct.&nbsp;</p><p>“My heart breaks over the loss of these 21 species,” said <a href="https://www.biologicaldiversity.org/about/staff/" target="_blank" rel="noopener noreferrer"><u>Noah Greenwald</u></a>, endangered species director at the <a href="https://www.biologicaldiversity.org/" target="_blank" rel="noopener noreferrer"><u>Center for Biological Diversity</u></a>. “These plants and animals can never be brought back. We absolutely must do everything we can to avert the loss of even more threads in our web of life.”&nbsp;</p><h3>One Extinct Songbird Once Lived in NC&nbsp;</h3><p>The extinct species includes eight birds of Hawaii’s honeycreepers, the bridled white-eye bird, the Little Mariana fruit bat (also known as the Guam flying fox), a one-inch-long Texas fish known as the San Marcos gambusia, nine southeastern mussels, the Scioto Madtom (a catfish found only in one river in Ohio) and the Bachman’s warbler, an insect-eating songbird that called the swamps of the Carolinas its home.&nbsp;</p><p>According to Greenwald, “The bird had a ‘buzzy’ song, and the song added to the beauty of the bird, and when combined that added to the magic of North Carolina. We lost a little magic when we lost the species. And what’s really sad is that the Bachman’s warbler was abundant at the turn of the 20th<sup></sup>century, but by 1950 it was noted as one of the rarest birds in North America.”&nbsp;</p><p>Greenwald says the bird’s extinction is largely due to habitat destruction. The swamps where it lived were converted to farmland, and its trees were cut down for logging. Additionally, people used to shoot the birds and collect their feathers for hats.&nbsp;</p><p>According to FWS, there have been no officially accepted, documented observations of the Bachman’s warbler in the United States since 1962. The bird was last spotted in Cuba in the 1980s.&nbsp;</p><h3>The Extinction List Continues to Grow&nbsp;</h3><p>The FWS declared 23 other species as extinct in 2021. Greenwald says there are now 650 species that have gone extinct in the U.S. due to a combination of factors, including habitat loss, climate change, pollution and invasive species.&nbsp;</p><p>Around the world, 902 species have been documented as extinct. However, the actual number is thought to be much higher because some species have never been formally identified. Also, many scientists warn the earth is in an “extinction crisis” with flora and fauna now disappearing at 1,000 times the historical rate.&nbsp;</p><p>While the droughts, wildfires, floods and temperature swings associated with climate change are making species recovery harder, the good news is that it is also changing how scientists are working to save species.&nbsp;</p><p>Greenwald adds the focus is no longer on individual species, such as a particular bird. The broader goal now is to preserve their habitat, which can help species of all types to live there. That new focus, combined with the protections afforded by the Endangered Species Act, may help keep more species off the extinction list.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I cancelled my Replit subscription (170 pts)]]></title>
            <link>https://journal.paoloamoroso.com/why-i-cancelled-my-replit-subscription</link>
            <guid>38333271</guid>
            <pubDate>Sun, 19 Nov 2023 15:06:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journal.paoloamoroso.com/why-i-cancelled-my-replit-subscription">https://journal.paoloamoroso.com/why-i-cancelled-my-replit-subscription</a>, See on <a href="https://news.ycombinator.com/item?id=38333271">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-body"><time datetime="2023-11-19T14:33:22Z" pubdate="" itemprop="datePublished" content="2023-11-19 14:33:22 +0000 UTC">November 19, 2023</time><div><p>I cancelled my annual Replit Hacker plan and I'll let it lapse at the end of December of 2023.</p>

<p><a href="https://replit.com/">Replit</a> is a popular and growing multi language development environment in the cloud. I've been subscribing to the Hacker plan for the past few years and it worked well for <a href="https://journal.paoloamoroso.com/tag:Python">my Python projects</a>, as such an online environment is a good match for my chromeOS lifestyle. But two changes made me reconsider the value of the product.</p>

<p>The first is Replit changed its paid tiers by raising the price of my plan and removing some features. I actually saw it coming when Replit began receiving massive funding by major investors, who are likely pressuring the company to reduce costs and turn profits. Still, this left a sour taste as it felt like a bait and switch.</p>

<p>The other change is <a href="https://journal.paoloamoroso.com/back-to-lisp">my shift of focus back to Lisp</a>, the programming language I love most and know best.</p>

<p>Replit doesn't work well with a highly interactive language like Lisp. It doesn't directly support any Lisp dialects and its default code editor provides little or no integration with a running Lisp image. Moreover, the files created or uploaded outside of the editor are often not preserved across sessions. If I have to install and maintain a full Lisp system in the Linux shell of a Replit workspace, I might as well install it locally on my desktop computer.</p>

<p>Finally, like other development tool vendors, Replit is doubling down on AI coding features which affect the cost of paid plans. But I'm not interested in this as the whole point of my hobby programming is to write all the code myself and learn from the experience.</p>

<p><a href="https://journal.paoloamoroso.com/tag:development"><span>#</span><span>development</span></a> <a href="https://journal.paoloamoroso.com/tag:Python"><span>#</span><span>Python</span></a> <a href="https://journal.paoloamoroso.com/tag:Lisp"><span>#</span><span>Lisp</span></a></p>

<p><a href="https://remark.as/p/journal.paoloamoroso.com/why-i-cancelled-my-replit-subscription">Discuss...</a>
<a href="mailto:info@paoloamoroso.com?subject=Reply%20to%20Paolo%20Amoroso%27s%20Journal">Email</a> | Reply <a href="https://journal.paoloamoroso.com/@/amoroso@fosstodon.org">@<span>amoroso@fosstodon.org</span></a></p>


</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's time for a change: datetime.utcnow() is now deprecated (301 pts)]]></title>
            <link>https://blog.miguelgrinberg.com/post/it-s-time-for-a-change-datetime-utcnow-is-now-deprecated</link>
            <guid>38333116</guid>
            <pubDate>Sun, 19 Nov 2023 14:49:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.miguelgrinberg.com/post/it-s-time-for-a-change-datetime-utcnow-is-now-deprecated">https://blog.miguelgrinberg.com/post/it-s-time-for-a-change-datetime-utcnow-is-now-deprecated</a>, See on <a href="https://news.ycombinator.com/item?id=38333116">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I was going through the <a href="https://docs.python.org/3/whatsnew/3.12.html">release notes</a> of the new Python 3.12 version the other day, and one item caught my attention in the deprecations section:</p>
<blockquote>
<p><code>datetime.datetime</code>’s <code>utcnow()</code> and <code>utcfromtimestamp()</code> are deprecated and will be removed in a future version.</p>
</blockquote>
<p>If you have followed my web development tutorials you must have seen me use <code>utcnow()</code> a lot, so I will clearly need to re-train myself to use an alternative, in preparation for the eventual removal of this function (likely a few years out, so no need to panic!).</p>
<p>In this short article I'll tell you more about why these functions are getting the axe, and what to replace them with.</p>
<h2>What's Wrong with utcnow() and utcfromtimestamp()?</h2>
<p>The problem that the Python maintainers have found comes from the fact that these functions return <a href="https://docs.python.org/3/library/datetime.html#aware-and-naive-objects">"naive" datetime objects</a>. A naive <code>datetime</code> object is one that does not have a timezone, which means that it can only be used in a context where the timezone does not matter or is already known in advance. This is in contrast to "aware" <code>datetime</code> objects, which do have a timezone attached to them explicitly.</p>
<p>If you ask me, I think the names of these functions are misleading. A function that is called <code>utcnow()</code> should be expected to return UTC datetimes, as implied by the name. I would have made it more clear that these functions work with naive time, maybe by calling them <code>naive_utcnow()</code> and <code>naive_utcfromtimestamp()</code>.</p>
<p>But their names are not the problem here. The specific issue is that some Python date and time functions accept naive timestamps and assume that they represent local time, according to the timezone that is configured on the computer running the code. There is a <a href="https://github.com/python/cpython/issues/81669">GitHub issue from 2019</a> that provides some background into this, with the following example:</p>
<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; dt = datetime.utcfromtimestamp(0)
&gt;&gt;&gt; dt
datetime.datetime(1970, 1, 1, 0, 0)
&gt;&gt;&gt; dt.timestamp()
18000
</code></pre>
<p>The example above was executed on a computer that was configured for Eastern Standard Time (EST). First, <code>dt</code> is assigned a naive <code>datetime</code> that is converted from the "zero" time or <a href="https://en.wikipedia.org/wiki/Unix_time">UNIX epoch</a>, which is January 1st, 1970 at midnight.</p>
<p>When this object is converted back to a timestamp, the <code>dt.timestamp()</code> method finds that it does not have a timezone to use in the conversion, so it uses the computer's own timezone, which in this example was EST (note that the EST timezone is 5 hours, or 18,000 seconds behind UTC). So we have a UNIX timestamp that originated as midnight on January 1st, 1970, and after being converted to a <code>datetime</code> and back ends up as 5 am.</p>
<p>If you read the issue linked above, they suggest that this ambiguity did not exist in Python 2 and for that reason this was not a problem for a long time, but it now is and needs to be addressed. This sounded strange, so I had to go and check, and sure enough, the <code>timestamp()</code> method that returns the incorrect UNIX time in the example was introduced in Python 3.3 and nothing similar appears to have existed back in Python 2 times.</p>
<p>So basically, at some point they've added a <code>datetime.timestamp()</code> method (and possibly others as well) that accept both aware and naive datetimes and this was a mistake, because these methods must have a timezone to work.</p>
<p>These methods should have been designed to fail when a naive <code>datetime</code> object is passed to them, but for some strange reason they decided that when a timezone is not provided the timezone from the system should be used. This is really the bug, but instead of fixing the broken implementations of these methods they are now trying to force people to move to aware datetimes by deprecating the two main functions that generate naive ones. They think that because a few functions assume that naive timestamps represent local times, all naive uses that are not in local time should be discouraged.</p>
<p>I may be missing something here, but I don't really follow this logic.</p>
<h2>Do We Need Naive Datetimes Anyway?</h2>
<p>To me it is clear that the Python maintainers behind this deprecation have a problem with naive datetimes and are using this supposed problem as an excuse to cripple them.</p>
<p>So why would you want to work with naive datetimes in the first place?</p>
<p>An application may be designed in such a way that all dates and times are in a single timezone that is known in advance. In this case there is no need for individual <code>datetime</code> instances to carry their own timezones, since this uses more memory and processing power for no benefit, since all these timezones would be the same and it would never be necessary to perform timezone math or conversions.</p>
<p>This is actually very common in web applications or other types of networking servers, which are configured with <a href="https://en.wikipedia.org/wiki/Coordinated_Universal_Time">UTC</a> time and normalize all dates and times to this timezone when they enter the system. It is also a best practice to store naive datetimes representing UTC in databases. The <a href="https://docs.sqlalchemy.org/en/stable/core/type_basics.html#sqlalchemy.types.DateTime">DateTime</a> type in SQLAlchemy represents a naive <code>datetime</code> object by default, for example. This is such a common database pattern that SQLAlchemy provides a <a href="https://docs.sqlalchemy.org/en/20/core/custom_types.html#store-timezone-aware-timestamps-as-timezone-naive-utc">recipe</a> for applications that use aware <code>datetime</code> objects to convert these to and from naive ones on the fly as they are saved to or loaded from the database.</p>
<p>So yes, I expect naive <code>datetime</code> objects will continue to be used, in spite of these deprecations.</p>
<h2>Updating Your Code</h2>
<p>Even though the deprecations are disappointing, it is important to keep in mind that it may take a few years for the functions to actually be removed. The problem is that once you switch to Python 3.12 or newer you will start seeing deprecation messages on your console and your logs, and these can get annoying. Here is an example of what you can expect to see:</p>
<pre><code>$ python
Python 3.12.0 (main, Oct  5 2023, 10:46:39) [GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; datetime.utcnow()
&lt;stdin&gt;:1: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
datetime.datetime(2023, 11, 18, 11, 22, 54, 263206)
</code></pre>
<p>I'm only using Python 3.12 in a small number of projects, and I'm already tired of seeing these warnings. So let's go ahead and look at how these two functions can be replaced.</p>
<p>The advice from the Python maintainers is to switch to aware <code>datetime</code> objects. The deprecation warning provides a hint of what they think we should use, and the deprecation notices included in the documentation are even more specific. Here is what the notice for the <code>utcnow()</code> function says:</p>
<blockquote>
<p>Deprecated since version 3.12: Use <code>datetime.now()</code> with <code>UTC</code> instead.</p>
</blockquote>
<p>Below you can see the one for <code>utcfromtimestamp()</code>:</p>
<blockquote>
<p>Deprecated since version 3.12: Use <code>datetime.fromtimestamp()</code> with <code>UTC</code> instead.</p>
</blockquote>
<p>So this gives us an idea of what can be done. Here are my custom versions of the deprecated functions, with the additional option to choose between aware or naive implementations:</p>
<pre><code>from datetime import datetime, timezone

def aware_utcnow():
    return datetime.now(timezone.utc)

def aware_utcfromtimestamp(timestamp):
    return datetime.fromtimestamp(timestamp, timezone.utc)

def naive_utcnow():
    return aware_utcnow().replace(tzinfo=None)

def naive_utcfromtimestamp(timestamp):
    return aware_utcfromtimestamp(timestamp).replace(tzinfo=None)

print(aware_utcnow())
print(aware_utcfromtimestamp(0))
print(naive_utcnow())
print(naive_utcfromtimestamp(0))
</code></pre>
<p>Note that if you are using Python 3.11 or newer, you can replace <code>datetime.timezone.utc</code> with a shorter <code>datetime.UTC</code>.</p>
<p>Running this script I get the following results:</p>
<pre><code>2023-11-18 11:36:35.137639+00:00
1970-01-01 00:00:00+00:00
2023-11-18 11:36:35.137672
1970-01-01 00:00:00
</code></pre>
<p>You can tell that the first and second lines show aware <code>datetime</code> instances from the <code>+00:00</code> suffix that indicates that the timezone is 00:00 or UTC. The third and fourth lines show abstract timestamps without a timezone, fully compatible with those returned by the deprecated functions.</p>
<p>What I like about these implementations is that they give you the choice to work with or without timezones, removing any ambiguity. Explicit is better than implicit, as <a href="https://peps.python.org/pep-0020/">the old adage</a> says.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Should I replace my 56k modem with a 28.8K Modem? (2001) (155 pts)]]></title>
            <link>https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/</link>
            <guid>38332788</guid>
            <pubDate>Sun, 19 Nov 2023 14:13:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/">https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/</a>, See on <a href="https://news.ycombinator.com/item?id=38332788">Hacker News</a></p>
<div id="readability-page-1" class="page">
		




		



		

		<div id="top">

				
					
	
	



					

					

					
						
						
							<header id="header">
								
							</header>
						
					
					

					
					
					

					
						
						

					

					
	

					
	

					
	
		
	

	

					
	

					
				

				
				

				
	


				<div>
						
						<!--XF:EXTRA_OUTPUT-->

						
	
		
	
		

		
	

	


						
	
		
	
		
			
			
		
	

	


						
	


						
	
		
	
	
	
		
	

	

	
	
	                          
	                        



	


						
	


						
	

						
	<div>You are using an out of date browser. It  may not display this or other websites correctly.<br>You should upgrade or use an <a href="https://www.google.com/chrome/" target="_blank" rel="noopener">alternative browser</a>.</div>



						<div uix_component="MainContainer">
										
	

										
	

										
	

										
	

										
	

										














	
	
	
		
	
	
	


	
	
	
		
	
	
	


	
	
		
	
	
	


	
	












	

	
		
	





















<div data-xf-init="" data-type="post" data-href="/inline-mod/" data-search-target="*">

	<span id="posts"></span>

	
		
	

	

	

	

	
		
	

	

	

	
	
	                          
	                        


<div data-xf-init="lightbox select-to-quote" data-message-selector=".js-post" data-lb-id="thread-437516" data-lb-universal="1">
			
				



					

					
						

	


	

	

	

	
	<article data-author="shawnmos" data-content="post-959696" id="js-post-959696">

		<span id="post-959696"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-959696" data-xf-init="share-tooltip" data-href="/posts/959696/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-959696" rel="nofollow">
						#1
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-959696" data-lb-caption-desc="shawnmos · Jan 11, 2001 at 7:52 PM">

		
			

	

		

		<article>
			
				
			
			
				<div>Should I replace my onboard(well kinda) 56k winmodem with my old US Robotics 28.8k External modem? The reason I would ask such a thing is because the my first phone line (the one my computers are hooked up to) only connects at 24k because the line is split with someone else. They did this to my dads phone line (2nd line) too but he complamed to Sprint for 3 months straight and finally got them to fix <b>only</b> the second line (with a service charge. Motherfu*kers!!!!!! :| ). So I'm stuck with a internet connection that is SLOW AS HELL!!!! When I try to connect to the internet it takes like 5 min. because it tries to connect at 56K and it keeps trying until it finds a speed it can connect at. I can tell because it keeps making the connecting sound over and over. So hopefully the 28.8K modem will connect faster. Also since it's external it does not block a pci slot and frees up an IRQ because it uses the serial port. Also it isn't a winmodem. What do you think?<p>

P.S. Today my dad got a phone bill from Sprint with like a 1000 calls to the 941-999-999 in Fort Myers, FL for $0.25 each. Also there is a call to that number every min. for I don't know how many mins. A LOT!! Who in their right mind would call a non-existant # every min. for like 24hours straight?? Fu*king Sprint and their stupidity!!!!! I hope they rot in HELL!!!!</p></div>
			
			
			
				
			
		</article>

		
			

	

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                            
	                                
	                                  





	                                
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	
	<article data-author="jfall" data-content="post-959779" id="js-post-959779">

		<span id="post-959779"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-959779" data-xf-init="share-tooltip" data-href="/posts/959779/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-959779" rel="nofollow">
						#2
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-959779" data-lb-caption-desc="jfall · Jan 11, 2001 at 8:00 PM">

		

		<article>
			
				
			
			
				<div>I have been doing internet tech support for a long time, and to tell you the truth.. most people with 28.8 &amp;amp; 33.6 modems are able to connect much easier then a 56k v.90 modem, I hate v.90 with a passion, nothing but connection problems, and the long handshake that you are describing is probably being caused by the 56k modem.  They actually came out with the v.91 &amp;amp; v.92 protocols, it will be interesting to see how they work out.</div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	
	<article data-author="etech" data-content="post-959825" id="js-post-959825">

		<span id="post-959825"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-959825" data-xf-init="share-tooltip" data-href="/posts/959825/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-959825" rel="nofollow">
						#3
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-959825" data-lb-caption-desc="etech · Jan 11, 2001 at 8:06 PM">

		

		<article>
			
				
			
			
				<div><a href="http://808hi.com/56k/trouble.htm" target="_blank" data-proxy-href="/proxy.php?link=http%3A%2F%2F808hi.com%2F56k%2Ftrouble.htm&amp;hash=c56d1b69030ff8882e14f3180f9a3881" rel="nofollow ugc noopener">troubleshooting</a><p>

shawnmos, you should be able to put in a code that will limit the speed of your on-board 56K modem.  That should allow it to connect better.  The site above has info to identify your chipset and what codes to use.</p><p>

 <a href="http://808hi.com/56k/x2-linklimit.htm" target="_blank" data-proxy-href="/proxy.php?link=http%3A%2F%2F808hi.com%2F56k%2Fx2-linklimit.htm&amp;hash=a04080b03104c9b80bd7350a0a740cbd" rel="nofollow ugc noopener">limiting your connect speed</a></p><p>

It's still possible that your hardware modem may give you better speeds.  After trying two winmodems, I'm not too impressed with them.</p></div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	
	<article data-author="Cybordolphin" data-content="post-959941" id="js-post-959941">

		<span id="post-959941"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-959941" data-xf-init="share-tooltip" data-href="/posts/959941/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-959941" rel="nofollow">
						#4
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-959941" data-lb-caption-desc="Cybordolphin · Jan 11, 2001 at 8:20 PM">

		

		<article>
			
				
			
			
				<div>Etech..<p>

What brand winmodems were they... and how long ago did you give them a whirl?</p><p>

Thanks.</p></div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	
	<article data-author="etech" data-content="post-960300" id="js-post-960300">

		<span id="post-960300"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-960300" data-xf-init="share-tooltip" data-href="/posts/960300/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-960300" rel="nofollow">
						#5
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-960300" data-lb-caption-desc="etech · Jan 11, 2001 at 9:05 PM">

		

		<article>
			
				
			
			
				<div>The first was a Genitech LT modem.<p>

It would not connect to one ISP at all and the other at only 33K.<br>
I sent it back.</p><p>

The one I have now is a Creative Digicom DI3635, good connects in the 49 to 50K range, but it seems to lag when more then one browser is open.  Downloads are in the 2 to 3k range. It is noticeably slower then my hardware modem.</p><p>

I would of kept my USR ISA hardware modem buy I gave it to my Dad to replace his 28.8.</p><p>

Oh well, maybe I'll splurge and get DSL, it is just now becomming available here.</p></div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	

	
	<article data-author="shawnmos" data-content="post-961944" id="js-post-961944">

		<span id="post-961944"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-961944" data-xf-init="share-tooltip" data-href="/posts/961944/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-961944" rel="nofollow">
						#6
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-961944" data-lb-caption-desc="shawnmos · Jan 12, 2001 at 12:19 AM">

		

		<article>
			
				
			
			
				<div>Crap! My 28.8 modem isn't working!!!! When I plug it in the HS led lights up but insted of the MR led lighting up like it is supposed to the CD led is flashing. No matter what I do it won't stop flashing. I remember this happened a long time ago and that is why I bought a 33.6 modem. What do those leds mean? My modem is a Motorola Lifestyle 28.8. The led lights are MR TR CD RD SD AA OH HS. Also when I would connect it to the computer the TR led would light up. It doesn't do anything now. Please help!</div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	

	
	<article data-author="shawnmos" data-content="post-963594" id="js-post-963594">

		<span id="post-963594"></span>

		
			
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	
	<article data-author="Fardringle" data-content="post-964454" id="js-post-964454">

		<span id="post-964454"></span>

		
			
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	

	
	<article data-author="shawnmos" data-content="post-966395" id="js-post-966395">

		<span id="post-966395"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-966395" data-xf-init="share-tooltip" data-href="/posts/966395/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-966395" rel="nofollow">
						#9
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-966395" data-lb-caption-desc="shawnmos · Jan 12, 2001 at 4:08 PM">

		

		<article>
			
				
			
			
				<div>It is connected but maybe it's not making contact with the phone line. I will check it out but I doubt the is the problem since the modem does not respond when commands are sent to it.</div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	
	<article data-author="H8tank" data-content="post-966538" id="js-post-966538">

		<span id="post-966538"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-966538" data-xf-init="share-tooltip" data-href="/posts/966538/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-966538" rel="nofollow">
						#10
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-966538" data-lb-caption-desc="H8tank · Jan 12, 2001 at 4:25 PM">

		

		<article>
			
				
			
			
				<div><blockquote data-attributes="" data-quote="" data-source="">
	
	<div>
			The reason I would ask such a thing is because the my first phone line (the one my computers are hooked up to) only connects at 24k because the line is split with someone else.
		</div>
</blockquote><br>
I would like to hear this explained...</div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	
	                            
	                              
	                            
							



					

					

				





					

					
						

	


	

	

	
	<article data-author="Topochicho" data-content="post-966751" id="js-post-966751">

		<span id="post-966751"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-966751" data-xf-init="share-tooltip" data-href="/posts/966751/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-966751" rel="nofollow">
						#11
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-966751" data-lb-caption-desc="Topochicho · Jan 12, 2001 at 4:52 PM">

		

		<article>
			
				
			
			
				<div>Well I went with the assumption that he was saying that instead of running new twisted pair for each line, they just wired up to the original set up wires. <p>

Twisted pair comes as 4 or more wires... usually 4. Telephones only use 2 lines for a standard phone connection, so the other 2 wires are just along for the ride.  If an installer is lazy he can just wire up the second pair of wires for the second line, but this will cause carryover(hearing people on the other line) and static.</p><p>

So, due to the line impurities of this badly done install, he is unable to achieve greater than 24k.</p><p>

OR</p><p>

He's a little slow in the melon and thinks 2 phone line = half the speed.</p><p>



KNOCK KNUCK KNUCK... I think this melons a little ripe boy.</p></div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	

	
	<article data-author="shawnmos" data-content="post-966942" id="js-post-966942">

		<span id="post-966942"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-966942" data-xf-init="share-tooltip" data-href="/posts/966942/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-966942" rel="nofollow">
						#12
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-966942" data-lb-caption-desc="shawnmos · Jan 12, 2001 at 5:17 PM">

		

		<article>
			
				
			
			
				<div>All I know is one day the EVIL SPRINT DEMONS came to every house in my neighborhood and fu*ced with their box and now I connect at 24k compared to 48k i had before. My dad called them up and asked them &amp;quot;WHAT THE FU*K DID YOU DO?&amp;quot;. They said that they had run out of lines and they had to split them. Whatever the hell that means!!</div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	

	
	<article data-author="shawnmos" data-content="post-966966" id="js-post-966966">

		<span id="post-966966"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-966966" data-xf-init="share-tooltip" data-href="/posts/966966/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-966966" rel="nofollow">
						#13
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-966966" data-lb-caption-desc="shawnmos · Jan 12, 2001 at 5:20 PM">

		

		<article>
			
				
			
			
				<div>Moral of the story, NEVER move to a city that uses Sprint as their local telephone provider!!!!!!!!!</div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	
	<article data-author="Fardringle" data-content="post-967273" id="js-post-967273">

		<span id="post-967273"></span>

		
			
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	
	<article data-author="fargus" data-content="post-967365" id="js-post-967365">

		<span id="post-967365"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-967365" data-xf-init="share-tooltip" data-href="/posts/967365/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-967365" rel="nofollow">
						#15
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-967365" data-lb-caption-desc="fargus · Jan 12, 2001 at 6:03 PM">

		

		<article>
			
				
			
			
				<div>I wouldn't hold my breath for the the v.91 &amp;amp; v.92 protocols. I'm an engineer for a company that sells equipment to ISP's, and none of the ones I know are talking about implementing it, especially after the pain of upgrading from v.34 to 56k. You can buy the modem, but if the ISP doesn't offer the protocol, fuhgedaboudit.</div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	
	<article data-author="etech" data-content="post-967534" id="js-post-967534">

		<span id="post-967534"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-967534" data-xf-init="share-tooltip" data-href="/posts/967534/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-967534" rel="nofollow">
						#16
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-967534" data-lb-caption-desc="etech · Jan 12, 2001 at 6:26 PM">

		

		<article>
			
				
			
			
				<div><a href="http://www.sonic.net/support/56k/56kprobs.shtml" target="_blank" data-proxy-href="/proxy.php?link=http%3A%2F%2Fwww.sonic.net%2Fsupport%2F56k%2F56kprobs.shtml&amp;hash=3ac1ad198f8eeb37ccdabc3d383024ae" rel="nofollow ugc noopener">sonic.net</a><p>

The phone company also sometimes conserves copper wire by digitally multiplexing calls together, separating them at a junction box in your neighborhood, converting them to analog again and routing them to you. This is called a &amp;quot;line concentrator&amp;quot; or &amp;quot;pair-gain&amp;quot; circuit. While a full discussion of the mechanics of analog-to-digital conversion lies outside the scope of this article, let it suffice to say that the  process is inherently damaging to the signal. In a normal voice-to-voice conversation this is unnoticeable. However, a data connection under 56k protocols can not be achieved if the signal is subjected to more than one conversion. As phone companies are continually having to find new ways to route an increasing volume of call traffic, multiple digital-to-analog conversions are becoming more common. </p><p>


One of the rumors on the net when 56k modems first came out was that if you signed up for Caller ID the phone company would have to give you a seperate line.  I don't know if this is true or not.  If someone can confirm it, then it may be worth looking into to get a seperate line again.</p></div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	
	<article data-author="Cybordolphin" data-content="post-976938" id="js-post-976938">

		<span id="post-976938"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-976938" data-xf-init="share-tooltip" data-href="/posts/976938/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-976938" rel="nofollow">
						#17
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-976938" data-lb-caption-desc="Cybordolphin · Jan 14, 2001 at 1:24 AM">

		

		<article>
			
				
			
			
				<div>Interesting.. <p>

If you ask for Caller ID you might get a seperate NON pair gain line!??</p><p>

That sounds too good to be true!  Anyone else ever heard of that?  I would imagine that my phone company would simply tell me that I cannot get caller ID.  They insist that nothing can be done to get around the pair gain here.<br>
Mf$$kers.   They knew full well that with the future of internet that this would create a major problem.   Try getting DSL products too..... you will be limited to half the performance there as well.  Sure wish satellite would gain faster upload speeds.  Looks like that may happen some day soon.</p><p>

This whole pair gain deal just really pisses me off.</p></div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	

	
	<article data-author="shawnmos" data-content="post-981580" id="js-post-981580">

		<span id="post-981580"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-981580" data-xf-init="share-tooltip" data-href="/posts/981580/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-981580" rel="nofollow">
						#18
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-981580" data-lb-caption-desc="shawnmos · Jan 14, 2001 at 7:52 PM">

		

		<article>
			
				
			
			
				<div>I don't think DSL would be cut in half because it uses a digital line, but I dunno.</div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				





					

					
						

	


	

	

	

	
	<article data-author="shawnmos" data-content="post-982902" id="js-post-982902">

		<span id="post-982902"></span>

		
			<div>

								
									

	<header>
		

		<ul>
			
			<li>
				<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-982902" data-xf-init="share-tooltip" data-href="/posts/982902/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://forums.anandtech.com/threads/should-i-replace-my-56k-modem-with-a-28-8k-modem.437516/post-982902" rel="nofollow">
						#19
					</a>
				</li>
			
		</ul>
	</header>

								

								<div data-lb-id="post-982902" data-lb-caption-desc="shawnmos · Jan 14, 2001 at 10:56 PM">

		

		<article>
			
				
			
			
				<div>Am I right? I hope so cause I might get dsl some day and drop our second phone line.</div>
			
			
			
				
			
		</article>

		

		
	</div>

								

								
									
	

								
							</div>
		
	</article>

	
	

	
	
	                                    
	                        

	
	



					

					

				


			
		</div>

	
		
	

	
	

</div>









	<div data-widget-id="25" data-widget-key="xfes_thread_view_below_quick_reply_similar_threads" data-widget-definition="xfes_similar_threads">
			
				<h3>Similar threads</h3>

				<div data-author="JediKnight">
				
					<a href="https://forums.anandtech.com/members/ch33zw1z.150238/" data-user-id="150238" data-xf-init="member-tooltip">
			<img src="https://forums.anandtech.com/data/avatars/s/150/150238.jpg?1670935009" alt="ch33zw1z" width="48" height="48" loading="lazy"> 
		</a>
				
			</div>
			
		</div>













										
	

									</div>
						
	
		
	
		

	
	
	                          
	                        


		
	
		
	

		

	
	
	                          
	                        


	

	

						
	

					</div>

				
	
		
	



<div>
				<ul>
					<li><a href="https://www.futureplc.com/services/advertising/">Advertising</a></li>
					<li><a href="https://www.futureplc.com/cookies-policy/">Cookies Policies</a></li>
					<li><a href="https://www.futureplc.com/privacy-policy/">Privacy</a></li>
					<li><a href="https://www.futureplc.com/terms-conditions/">Term &amp; Conditions</a></li>
				</ul>
				
			</div>



	


				
					
				
				
				
			</div>

		<div>
			
			
				
	
		
		
		

		<ul data-xf-init="notices" data-type="bottom_fixer" data-scroll-interval="6">

			
				
	<li data-notice-id="-1" data-delay-duration="0" data-display-duration="0" data-auto-dismiss="0" data-visibility="">
		
		<div>
		This site uses cookies to help personalise content, tailor your experience and to keep you logged in if you register.<br>
By continuing to use this site, you are consenting to our use of cookies.
	</div>
	</li>

			
		</ul>
	

			
		</div>

		

		
	
	
	
	
	















	
	

	




	



	



	



	






	



	




	

	

	



	

	

	

	
	
	
	
	

	

	

		
		
			
		

		

		
	
	
		
		
			
		
	


	


	
	
	                          
	                        








</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Victor Mono Typeface (202 pts)]]></title>
            <link>https://rubjo.github.io/victor-mono/</link>
            <guid>38332635</guid>
            <pubDate>Sun, 19 Nov 2023 13:53:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rubjo.github.io/victor-mono/">https://rubjo.github.io/victor-mono/</a>, See on <a href="https://news.ycombinator.com/item?id=38332635">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Revenge Bedtime Procrastination (180 pts)]]></title>
            <link>https://solvingprocrastination.com/revenge-bedtime-procrastination/</link>
            <guid>38332364</guid>
            <pubDate>Sun, 19 Nov 2023 13:16:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://solvingprocrastination.com/revenge-bedtime-procrastination/">https://solvingprocrastination.com/revenge-bedtime-procrastination/</a>, See on <a href="https://news.ycombinator.com/item?id=38332364">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text"><p><em>Revenge bedtime procrastination</em> occurs when people delay going to sleep in order to feel in control of their life and schedule. This phenomenon is particularly associated with people who feel that they have little time for themselves during the day, for example due to their work schedule, so they delay sleep until late at night in order to have leisure time that they’re in control of.</p><p>Revenge bedtime procrastination can lead to various issues, so it’s important to understand it. As such, in the following article you will learn more about this phenomenon, understand its causes, and see what you can do to deal with it effectively.</p><div id="ez-toc-container"><nav><ul><li><a href="#Examples_of_revenge_bedtime_procrastination" title="Examples of revenge bedtime procrastination">Examples of revenge bedtime procrastination</a></li><li><a href="#Origin_history_and_terminology" title="Origin, history, and terminology">Origin, history, and terminology</a></li><li><a href="#Dangers_of_revenge_bedtime_procrastination" title="Dangers of revenge bedtime procrastination">Dangers of revenge bedtime procrastination</a></li><li><a href="#Psychology_and_causes_of_revenge_bedtime_procrastination" title="Psychology and causes of revenge bedtime procrastination">Psychology and causes of revenge bedtime procrastination</a></li><li><a href="#How_to_avoid_revenge_bedtime_procrastination" title="How to avoid revenge bedtime procrastination">How to avoid revenge bedtime procrastination</a></li></ul></nav></div><h2><span id="Examples_of_revenge_bedtime_procrastination"></span>Examples of revenge bedtime procrastination<span></span></h2><p>An example of revenge bedtime procrastination is someone who spends almost all of their days either at work or dealing with errands, so they stay up late at night <a href="https://solvingprocrastination.com/social-media-procrastination/">browsing social media</a> and <a href="https://solvingprocrastination.com/online-procrastination/">watching videos on their phone</a> instead of sleeping, so they can feel that they have some control of their schedule.</p><p>Another example of revenge bedtime procrastination is a teenager who spends almost all of their days doing what their teachers and parents tell them to do, so they delay going to sleep in secret because that’s a way for them to rebel and feel in control.</p><h2><span id="Origin_history_and_terminology"></span>Origin, history, and terminology<span></span></h2><p>The concept of revenge bedtime procrastination was <a href="http://web.archive.org/web/20211017092601/https:/www.bbc.com/worklife/article/20201123-the-psychology-behind-revenge-bedtime-procrastination">popularized</a> in the following tweet, though the term has also <a href="http://web.archive.org/web/20211017092601/https:/www.bbc.com/worklife/article/20201123-the-psychology-behind-revenge-bedtime-procrastination">been used previously</a> by others:</p><blockquote><p>“Learned a very relatable term today: “報復性熬夜” (revenge bedtime procrastination), a phenomenon in which people who don’t have much control over their daytime life refuse to sleep early in order to regain some sense of freedom during late night hours.”</p><p>— Tweet by journalist Daphne K. Lee (June, <a href="http://web.archive.org/web/20211017093350/https:/twitter.com/daphnekylee/status/1277101831693275136">2020</a>)</p></blockquote><p>This concept and similar ones are sometimes referred to using other terms, such as <em>revenge sleep procrastination</em>, <em>sleep revenge</em>, and <em>revenge insomnia</em>.</p><p>The concept of revenge bedtime procrastination itself is currently largely informal, in the sense that there’s no research that investigates it directly. However, research has been conducted on related concepts on which it is based, including, most notably, <a href="https://solvingprocrastination.com/bedtime-procrastination/"><em>bedtime procrastination</em></a>, which occurs when people unnecessarily delay going to bed, especially when they know that doing so is bad for them.</p><p>A distinction is sometimes drawn between bedtime procrastination&nbsp;<em>while-in-bed procrastination</em>, which <a href="https://doi.org/10.3390/ijerph17165892">involves</a> unnecessarily postponing going to sleep after already getting into bed (e.g., by <a href="https://solvingprocrastination.com/online-procrastination/">browsing your phone</a>). However, “bedtime procrastination” is often used to refer to delaying getting into bed while <a href="https://doi.org/10.1016/B978-0-12-802862-9.00005-0">fully prepared</a> to go to sleep (e.g., after setting aside your phone). Furthermore, “bedtime procrastination” is often used synonymously with “<a href="https://solvingprocrastination.com/sleep-procrastination/">sleep procrastination</a>”, to refer to delaying going to sleep in general. Accordingly, “revenge bedtime procrastination” can also be used in a general sense, regardless of whether a person is already in bed.</p><h2><span id="Dangers_of_revenge_bedtime_procrastination"></span>Dangers of revenge bedtime procrastination<span></span></h2><p>Revenge bedtime procrastination can cause the same issues as other types of <a href="https://solvingprocrastination.com/sleep-procrastination/">sleep procrastination</a>, including <a href="https://doi.org/10.1016/j.smrv.2022.101697">primarily</a> lack of sleep, and consequently also tiredness, fatigue, and even exhaustion. This can have <a href="https://solvingprocrastination.com/procrastination-dangers/">various negative effects</a>, like worse mental health, worse physical health, and worse emotional wellbeing (e.g., due to feelings of guilt and shame). This can also cause interpersonal conflicts, for example if it frustrates people who care about the procrastinator’s wellbeing.</p><p>The low energy levels that this behavior causes can also <a href="https://doi.org/10.1002/job.2084">reduce</a> people’s capacity for <a href="https://doi.org/10.1111/joop.12191">self-regulation</a>, which makes them more likely to <a href="https://doi.org/10.3389/fpsyg.2018.02029">procrastinate</a> in other domains, like <a href="https://solvingprocrastination.com/academic-procrastination/">school</a> or <a href="https://solvingprocrastination.com/workplace-procrastination/">work</a>. In addition, this can also make people more likely to procrastinate on going to sleep again, as part of a self-perpetuating <a href="https://solvingprocrastination.com/procrastination-cycle/">sleep-procrastination cycle</a>, especially if going to sleep late <a href="https://solvingprocrastination.com/sleep-procrastination/">interferes</a> with their biological clock.</p><h2><span id="Psychology_and_causes_of_revenge_bedtime_procrastination"></span>Psychology and causes of revenge bedtime procrastination<span></span></h2><p>The main reason why people engage in revenge bedtime procrastination is their desire to feel in control, often by rebelling indirectly against someone who’s perceived as an authority figure. The authority figure can be someone specific, such as a parent, or something more general, such as societal norms.</p><p>This need for control as a primary reason for procrastination is what differentiates revenge bedtime procrastination from other <a href="https://solvingprocrastination.com/bedtime-procrastination/">bedtime procrastination</a>. Though no formal research has been conducted on this phenomenon directly yet, the definition and description of this concept align with several aspects of prior research on procrastination.</p><p>This includes research which shows that <a href="https://doi.org/10.1016/0191-8869(94)90090-6">revenge</a>, <a href="https://doi.org/10.1037/0033-2909.133.1.65">rebellion</a>, and <a href="https://doi.org/10.3200/SOCP.149.2.241-257">resentment</a> can drive people to <a href="https://doi.org/10.1016/S0191-8869(00)00019-2">procrastinate</a>, especially on tasks that are imposed on them by authority figures, and especially when procrastination allows them to <a href="https://doi.org/10.1111/ap.12173">exert</a> control and <a href="https://doi.org/10.1016/bs.adms.2019.01.001">autonomy</a> in situations where they would otherwise <a href="https://doi.org/10.1016/0092-6566(88)90015-3">struggle</a> to do so. As <a href="https://doi.org/10.1016/0191-8869(94)90090-6">one study</a> speculates:</p><blockquote><p>“Procrastination, as a passive-aggressive form of revenge, might occur more frequently in powerless individuals who have fewer outlets for revengeful actions. These individuals cannot run the risk of retaliation and thus would be more likely to engage in subtler forms of behavior than their more powerful counterparts.”</p></blockquote><p>Furthermore, revenge bedtime procrastination is also associated with the <a href="https://doi.org/10.1080/15402002.2018.1491850">concept</a>&nbsp;of&nbsp;<em>deliberate procrastination</em>, which in this context occurs when people intentionally delay going to sleep, because they feel that they deserve some time for themselves.</p><p>In addition, certain issues beyond the need for control, which can make people more likely to engage in <a href="https://solvingprocrastination.com/sleep-procrastination/">sleep procrastination</a> in general, can also make people more likely to engage in revenge bedtime procrastination. This includes poor sleep hygiene (e.g., using bright screens late at night), people’s chronotype (particularly, a preference for being up late at night), and a misaligned biological clock.</p><p>One of the key causes of sleep procrastination in this regard is the desire to <a href="https://doi.org/10.3389/fpsyg.2014.00611">keep engaging</a>&nbsp;with&nbsp;<a href="https://doi.org/10.1080/15205436.2019.1606246">available entertainment</a>, and particularly <a href="https://doi.org/10.1037/apl0000818">digital entertainment</a><em>,</em> for example by <a href="https://doi.org/10.1177/0093650216686877">watching TV</a> or <a href="https://solvingprocrastination.com/social-media-procrastination/">browsing social media</a>. Such entertainment is easily accessible from many places (e.g., people’s bedroom and bed), is stimulating enough to keep people awake in many cases, and often requires relatively little effort to engage with. This <a href="https://doi.org/10.1037/apl0000818">form</a> of <em>cyber leisure</em> is also <a href="https://doi.org/10.1080/15402002.2018.1491850">associated</a> with <em>mindless sleep procrastination</em>, which occurs when people lose track of time because they’re immersed in evening and night activities.</p><p>However, note that the activities that people engage in during revenge bedtime procrastination aren’t necessarily enjoyable, and the procrastinator may engage with them simply out of a desire to feel in control. Furthermore, even in cases where the procrastinator does derive some satisfaction and enjoyment from their behavior, the procrastinator engaging in revenge bedtime procrastination can generally expect to be worse off as a result of delaying going to sleep, for example due to resulting lack of sleep and exhaustion, as with other forms of <a href="https://solvingprocrastination.com/bedtime-procrastination/">bedtime procrastination</a>.</p><h2><span id="How_to_avoid_revenge_bedtime_procrastination"></span>How to avoid revenge bedtime procrastination<span></span></h2><p>If you realize that you’re engaging in revenge bedtime procrastination and want to stop doing it because of the issues it leads to, then you can use the following techniques:</p><ul><li>Think about—and potentially write down—how this behavior harms you.</li><li>Think about—and potentially write down—how minimizing or ending this behavior could benefit you.</li><li>Realize that you’re primarily harming yourself, rather than whoever or whatever you’re rebelling against.</li><li>Find more positive ways to feel in control and have time for yourself.</li><li>Consider working with a relevant professional (e.g., a psychologist) to address the issues that are causing you to procrastinate, especially if issues such as <a href="https://solvingprocrastination.com/anxiety/">anxiety</a> and <a href="https://solvingprocrastination.com/depression/">depression</a> are involved.</li></ul><p>In addition, you can use similar techniques as you would use to deal with general <a href="https://solvingprocrastination.com/bedtime-procrastination/">bedtime procrastination</a>:</p><ul><li><strong>Improve your bedtime habits</strong>, by finishing your obligations as early as reasonably possible before bedtime, developing a consistent and calming bedtime routine, and adding a time delay before you procrastinate (e.g., counting to 30 before indulging your impulse to procrastinate).</li><li><strong>Improve your sleep hygiene</strong>, by minimizing light exposure before bedtime (and especially exposure to bright or blue light), and avoiding stimulating activities, caffeine, tobacco, alcohol, and problematic foods (e.g., heavy meals) in the hours before bedtime.</li><li><strong>Improve your sleep environment</strong>, by removing distractions and temptations (such as digital devices), and making your bedroom and bed feel comfortable.</li><li><strong>Change your sleep habits</strong>, by setting a consistent sleep schedule, waking up earlier, and minimizing napping or avoiding it entirely.</li><li><strong>Change your general habits</strong>, by exercising, getting exposure to light throughout the day, and minimizing the use of your bed and bedroom for things other than sleeping.</li><li><strong>Improve your planning</strong>, by setting concrete goals, having a clear plan for achieving your goals, and figuring out how you will handle obstacles that you might encounter.</li><li><strong>Increase your motivation</strong>, by clearly identifying why you want to go to bed on time, visualizing your future self, reminding yourself that sleep is a top priority for you, and acknowledging and rewarding your progress.</li><li><strong>Change your mindset</strong>, by making sleep something that you look forward to, giving yourself permission to make mistakes, developing&nbsp;<a href="https://solvingprocrastination.com/self-compassion/"><em>self-compassion</em></a> (by being kind to yourself, recognizing that everyone experiences challenges, and accepting your emotions in a non-judgmental manner), and developing&nbsp;<a href="https://solvingprocrastination.com/self-efficacy/"><em>self-efficacy</em></a> (by identifying the strategies that you can use to go to bed on time and thinking about your ability to execute them successfully).</li></ul><p>It helps to start by figuring out <a href="https://solvingprocrastination.com/why-people-procrastinate/">what’s causing your procrastination</a>, so you can pick the most relevant <a href="https://solvingprocrastination.com/how-to-stop-procrastinating/">anti-procrastination techniques</a> to use in your case.</p><p>Note that you can use similar techniques to help someone else stop engaging in revenge bedtime procrastination (e.g., your child if you’re a parent). However, if you do this, make sure to avoid exacerbating the issues that cause revenge bedtime procrastination in the first place (e.g., making the procrastinator feel that they have no control by forcing them to go to bed at a certain time). Instead, <a href="https://doi.org/10.1007/s12144-018-9825-7">focus</a> on helping the procrastinator develop intrinsic and autonomous motivation for going to bed on time, while allowing them to maintain a sense of control, for example by talking to them about this behavior and asking them what they think could help them avoid it.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The architecture of today's LLM applications (144 pts)]]></title>
            <link>https://github.blog/2023-10-30-the-architecture-of-todays-llm-applications/</link>
            <guid>38332137</guid>
            <pubDate>Sun, 19 Nov 2023 12:41:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.blog/2023-10-30-the-architecture-of-todays-llm-applications/">https://github.blog/2023-10-30-the-architecture-of-todays-llm-applications/</a>, See on <a href="https://news.ycombinator.com/item?id=38332137">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    


<main role="main" id="post-74969">
  
<p>We want to empower you to experiment with LLM models, build your own applications, and discover untapped problem spaces. That’s why we sat down with GitHub’s <a href="https://github.com/whatsinfinitum" target="_blank" rel="noopener">Alireza Goudarzi</a>, a senior machine learning researcher, and <a href="https://github.com/wunderalbert" target="_blank" rel="noopener">Albert Ziegler</a>, a principal machine learning engineer, to discuss the emerging architecture of today’s LLMs.</p>
<p>In this post, we’ll cover five major steps to building your own LLM app, the emerging architecture of today’s LLM apps, and problem areas that you can start exploring today.</p>
<h2 id="five-steps-to-building-an-llm-app">Five steps to building an LLM app<a href="#five-steps-to-building-an-llm-app" aria-label="Five steps to building an LLM app"></a></h2>
<p>Building software with LLMs, or any machine learning (ML) model, is <a href="https://karpathy.medium.com/software-2-0-a64152b37c35" target="_blank" rel="noopener">fundamentally different</a> from building software without them. For one, rather than compiling source code into binary to run a series of commands, developers need to navigate datasets, embeddings, and parameter weights to generate consistent and accurate outputs. After all, LLM outputs are probabilistic and don’t produce the same predictable outcomes.</p>
<figure id="attachment_74972"><a href="https://github.blog/wp-content/uploads/2023/10/FivestepstobuildingLLMapp.png?resize=1022%2C537?w=1022" target="_blank" rel="noopener"><img decoding="async" width="1022" height="537" src="https://github.blog/wp-content/uploads/2023/10/FivestepstobuildingLLMapp.png?resize=1022%2C537" alt="Diagram that lists the five steps to building a large language model application. Data source for diagram is detailed here: https://github.blog/?p=74969&amp;preview=true#five-steps-to-building-an-llm-app" loading="lazy" srcset="https://github.blog/wp-content/uploads/2023/10/FivestepstobuildingLLMapp.png?resize=1022%2C537?w=1022 1022w, https://github.blog/wp-content/uploads/2023/10/FivestepstobuildingLLMapp.png?resize=1022%2C537?w=300 300w, https://github.blog/wp-content/uploads/2023/10/FivestepstobuildingLLMapp.png?resize=1022%2C537?w=768 768w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></a><figcaption>Click on diagram to enlarge and save.</figcaption></figure>
<p><strong>Let’s break down, at a high level, the steps to build an LLM app today. <g-emoji fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f447.png?v8" alias="point_down">👇</g-emoji></strong></p>
<p><strong>1. Focus on a single problem, first</strong>. The key? Find a problem that’s the right size: one that’s focused enough so you can quickly iterate and make progress, but also big enough so that the right solution will wow users.</p>
<p>For instance, rather than trying to address all developer problems with AI, the GitHub Copilot team initially focused on one part of the software development lifecycle: <a href="https://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/" target="_blank" rel="noopener">coding functions in the IDE</a>.</p>
<p><strong>2. Choose the right LLM</strong>. You’re saving costs by building an LLM app with a pre-trained model, but how do you pick the right one? Here are some factors to consider:</p>
<ul>
<li><strong>Licensing</strong>. If you hope to eventually sell your LLM app, you’ll need to use a model that has an API licensed for commercial use. To get you started on your search, here’s a community-sourced <a href="https://github.com/eugeneyan/open-llms" target="_blank" rel="noopener">list of open LLMs that are licensed for commercial use</a>.</li>
<li><strong>Model size.</strong> The size of LLMs can range from 7 to 175 billion parameters—and some, like <a href="https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/llm-models" target="_blank" rel="noopener">Ada</a>, are even as small as 350 million parameters. Most LLMs (at the time of writing this post) range in size from 7-13 billion parameters.</li>
</ul>
<p>Conventional wisdom tells us that if a model has more parameters (variables that can be adjusted to improve a model’s output), the better the model is at learning new information and providing predictions. However, the <a href="https://spectrum.ieee.org/large-language-models-size" target="_blank" rel="noopener">improved performance of smaller models</a> is challenging that belief. Smaller models are also usually faster and cheaper, so improvements to the quality of their predictions make them a viable contender compared to big-name models that might be out of scope for many apps.</p>

<ul>
<li><strong>Model performance</strong>. Before you customize your LLM using techniques like fine-tuning and in-context learning (which we’ll cover below), evaluate how well and fast—and how consistently—the model generates your desired output. To measure model performance, you can use <strong>offline evaluations</strong>.</li>
</ul>

<p><strong>3. Customize the LLM</strong>. When you train an LLM, you’re building the scaffolding and neural networks to enable deep learning. When you customize a pre-trained LLM, you’re adapting the LLM to specific tasks, such as generating text around a specific topic or in a particular style. The section below will focus on techniques for the latter. To customize a pre-trained LLM to your specific needs, you can try in-context learning, reinforcement learning from human feedback (RLHF), or fine-tuning.</p>
<ul>
<li><strong>In-context learning,</strong> sometimes referred to as <a href="https://github.blog/2023-06-20-how-to-write-better-prompts-for-github-copilot/" target="_blank" rel="noopener">prompt engineering</a> by end users, is when you provide the model with specific instructions or examples at the time of inference—or the time you’re querying the model—and asking it to infer what you need and generate a contextually relevant output.</li>
</ul>
<p>In-context learning can be done in a variety of ways, like providing examples, rephrasing your queries, and adding a sentence that states your goal at a high-level.</p>
<ul>
<li><strong>RLHF</strong> comprises a reward model for the pre-trained LLM. The reward model is trained to predict if a user will accept or reject the output from the pre-trained LLM. The learnings from the reward model are passed to the pre-trained LLM, which will adjust its outputs based on user acceptance rate.</li>
</ul>
<p>The benefit to RLHF is that it doesn’t require supervised learning and, consequently, expands the criteria for what’s an acceptable output. With enough human feedback, the LLM can learn that if there’s an 80% probability that a user will accept an output, then it’s fine to generate. Want to try it out? Check out these <a href="https://github.com/opendilab/awesome-RLHF" target="_blank" rel="noopener">resources, including codebases, for RLHF</a>.</p>
<ul>
<li><strong>Fine-tuning</strong> is when the model’s generated output is evaluated against an intended or known output. For example, you know that the sentiment behind a statement like this is negative: “The soup is too salty.” To evaluate the LLM, you’d feed this sentence to the model and query it to label the sentiment as positive or negative. If the model labels it as positive, then you’d adjust the model’s parameters and try prompting it again to see if it can classify the sentiment as negative.</li>
</ul>
<p>Fine-tuning can result in a highly customized LLM that excels at a specific task, but it uses supervised learning, which requires time-intensive labeling. In other words, each input sample requires an output that’s labeled with exactly the correct answer. That way, the actual output can be measured against the labeled one and adjustments can be made to the model’s parameters. The advantage of RLHF, as mentioned above, is that you don’t need an exact label.</p>
<p><strong>4. Set up the app’s architecture</strong>. The different components you’ll need to set up your LLM app can be roughly grouped into three categories:</p>
<ul>
<li><strong>User input</strong> which requires a UI, an LLM, and an app hosting platform.</li>
<li><strong>Input enrichment and prompt construction tools.</strong> This includes your data source, embedding model, a vector database, prompt construction and optimization tools, and a data filter.
</li>
<li>
<p><strong>Efficient and responsible AI tooling,</strong> which includes an LLM cache, LLM content classifier or filter, and a telemetry service to evaluate the output of your LLM app.</p>
</li>
</ul>
<p><strong>5. Conduct online evaluations of your app.</strong> These evaluations are considered “online” because they assess the LLM’s performance during user interaction. For example, online evaluations for GitHub Copilot are measured through acceptance rate (how often a developer accepts a completion shown to them), as well as the retention rate (how often and to what extent a developer edits an accepted completion).</p>

<hr>
<h2 id="the-emerging-architecture-of-llm-apps">The emerging architecture of LLM apps<a href="#the-emerging-architecture-of-llm-apps" aria-label="The emerging architecture of LLM apps"></a></h2>
<p>Let’s get started on architecture. We’re going to revisit our friend <a href="https://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms/#building-applications-using-llms" target="_blank" rel="noopener">Dave</a>, whose Wi-Fi went out on the day of his World Cup watch party. Fortunately, Dave was able to get his Wi-Fi running in time for the game, thanks to an LLM-powered assistant.</p>
<p><strong>We’ll use this example and the diagram above to <strong>walk through a user flow with an LLM app, and break down the kinds of tools you’d need to build it. <g-emoji fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f447.png?v8" alias="point_down">👇</g-emoji></strong></strong></p>
<figure id="attachment_74991"><a href="https://github.blog/wp-content/uploads/2023/10/LLMapparchitecturediagram.png?resize=4088%2C2148?w=1536" target="_blank" rel="noopener"><img decoding="async" width="4088" height="2148" src="https://github.blog/wp-content/uploads/2023/10/LLMapparchitecturediagram.png?resize=4088%2C2148" alt="Flow chart that reads from right to left, showing components of a large language model application and how they all work together. Data source for diagram is detailed here: https://github.blog/?p=74969&amp;preview=true#the-emerging-architecture-of-llm-apps" loading="lazy" srcset="https://github.blog/wp-content/uploads/2023/10/LLMapparchitecturediagram.png?resize=4088%2C2148?w=4088 4088w, https://github.blog/wp-content/uploads/2023/10/LLMapparchitecturediagram.png?resize=4088%2C2148?w=300 300w, https://github.blog/wp-content/uploads/2023/10/LLMapparchitecturediagram.png?resize=4088%2C2148?w=768 768w, https://github.blog/wp-content/uploads/2023/10/LLMapparchitecturediagram.png?resize=4088%2C2148?w=1024 1024w, https://github.blog/wp-content/uploads/2023/10/LLMapparchitecturediagram.png?resize=4088%2C2148?w=1536 1536w, https://github.blog/wp-content/uploads/2023/10/LLMapparchitecturediagram.png?resize=4088%2C2148?w=2048 2048w, https://github.blog/wp-content/uploads/2023/10/LLMapparchitecturediagram.png?resize=4088%2C2148?w=3000 3000w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></a><figcaption>Click diagram to enlarge and save.</figcaption></figure>
<h3 id="user-input-tools">User input tools<a href="#user-input-tools" aria-label="User input tools"></a></h3>
<p>When Dave’s Wi-Fi crashes, he calls his internet service provider (ISP) and is directed to an LLM-powered assistant. The assistant asks Dave to explain his emergency, and Dave responds, “My TV was connected to my Wi-Fi, but I bumped the counter, and the Wi-Fi box fell off! Now, we can’t watch the game.”</p>
<p>In order for Dave to interact with the LLM, we need four tools:</p>
<ul>
<li><strong>LLM API and host</strong>: Is the LLM app running on a local machine or in the cloud? In an ISP’s case, it’s probably hosted in the cloud to handle the volume of calls like Dave’s. <a href="https://github.com/vercel" target="_blank" rel="noopener">Vercel</a> and early projects like <a href="https://github.com/jina-ai/rungpt" target="_blank" rel="noopener">jina-ai/rungpt</a> aim to provide a cloud-native solution to deploy and scale LLM apps.</li>
</ul>
<p>But if you want to build an LLM app to tinker, hosting the model on your machine might be more cost effective so that you’re not paying to spin up your cloud environment every time you want to experiment. You can find conversations on GitHub Discussions about hardware requirements for models like LLaMA‚ two of which can be found <a href="https://github.com/facebookresearch/llama/issues/79" target="_blank" rel="noopener">here</a> and <a href="https://github.com/facebookresearch/llama/issues/425" target="_blank" rel="noopener">here</a>.</p>
<ul>
<li><strong>The UI</strong>: Dave’s keypad is essentially the UI, but in order for Dave to use his keypad to switch from the menu of options to the emergency line, the UI needs to include a router tool.</li>
<li><strong>Speech-to-text translation tool</strong>: Dave’s verbal query then needs to be fed through a speech-to-text translation tool that works in the background.</li>
</ul>
<h3 id="input-enrichment-and-prompt-construction-tools">Input enrichment and prompt construction tools<a href="#input-enrichment-and-prompt-construction-tools" aria-label="Input enrichment and prompt construction tools"></a></h3>
<p>Let’s go back to Dave. The LLM can analyze the sequence of words in Dave’s transcript, classify it as an IT complaint, and provide a contextually relevant response. (The LLM’s able to do this because it’s been trained on the internet’s entire corpus, which includes IT support documentation.)</p>
<p><strong>Input enrichment tools</strong> aim to contextualize and package the user’s query in a way that will generate the most useful response from the LLM.</p>
<ul>
<li>A <strong>vector database</strong> is where you can store embeddings, or index high-dimensional vectors. It also increases the probability that the LLM’s response is helpful by providing additional information to further contextualize your user’s query.</li>
</ul>
<p>Let’s say the LLM assistant has access to the company’s complaints search engine, and those complaints and solutions are stored as embeddings in a vector database. Now, the LLM assistant uses information not only from the internet’s IT support documentation, but also from documentation specific to customer problems with the ISP.</p>
<ul>
<li>But in order to retrieve information from the vector database that’s relevant to a user’s query, we need an <strong>embedding model</strong> to translate the query into an embedding. Because the embeddings in the vector database, as well as Dave’s query, are translated into high-dimensional vectors, the vectors will capture both the semantics and intention of the natural language, not just its syntax.</li>
</ul>
<p>Here’s a list of <a href="https://github.com/topics/text-embedding" target="_blank" rel="noopener">open source text embedding models</a>. <a href="https://platform.openai.com/docs/guides/embeddings/embedding-models" target="_blank" rel="noopener">OpenAI</a> and <a href="https://huggingface.co/blog/getting-started-with-embeddings" target="_blank" rel="noopener">Hugging Face</a> also provide embedding models.</p>
<p>Dave’s contextualized query would then read like this:</p>
<pre><code>// pay attention to the the following relevant information.
to the colors and blinking pattern.

// pay attention to the following relevant information.

// The following is an IT complaint from, Dave Anderson, IT support expert.
Answers to Dave's questions should serve as an example of the excellent support
provided by the ISP to its customers.

*Dave: Oh it's awful! This is the big game day. My TV was connected to my
Wi-Fi, but I bumped the counter and the Wi-Fi box fell off and broke! Now we
can't watch the game.
</code></pre>
<p>Not only do these series of prompts contextualize Dave’s issue as an IT complaint, they also pull in context from the company’s complaints search engine. That context includes common internet connectivity issues and solutions.</p>
<p>MongoDB released a public preview of <a href="https://www.mongodb.com/developer/products/atlas/building-generative-ai-applications-vector-search-open-source-models/" target="_blank" rel="noopener">Vector Atlas Search</a>, which indexes high-dimensional vectors within MongoDB. <a href="https://github.com/qdrant" target="_blank" rel="noopener">Qdrant</a>, <a href="https://github.com/pinecone-io" target="_blank" rel="noopener">Pinecone</a>, and <a href="https://github.com/milvus-io" target="_blank" rel="noopener">Milvus</a> also provide free or open source vector databases.</p>

<ul>
<li>A <strong>data filter</strong> will ensure that the LLM isn’t processing unauthorized data, like personal identifiable information. Preliminary projects like <a href="https://github.com/amoffat/HeimdaLLM" target="_blank" rel="noopener">amoffat/HeimdaLLM</a> are working to ensure LLMs access only authorized data.</li>
<li>A <strong>prompt optimization tool</strong> will then help to package the end user’s query with all this context. In other words, the tool will help to prioritize which context embeddings are most relevant, and in which order those embeddings should be organized in order for the LLM to produce the most contextually relevant response. This step is what ML researchers call prompt engineering, where a series of algorithms create a prompt. (A note that this is different from the prompt engineering that end users do, which is also known as in-context learning).</li>
</ul>
<p>Prompt optimization tools like <a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener">langchain-ai/langchain</a> help you to compile prompts for your end users. Otherwise, you’ll need to DIY a series of algorithms that retrieve embeddings from the vector database, grab snippets of the relevant context, and order them. If you go this latter route, you could use <a href="https://github.blog/2023-09-20-github-copilot-chat-beta-now-available-for-all-individuals/" target="_blank" rel="noopener">GitHub Copilot Chat</a> or ChatGPT to assist you.</p>
<div><p>Learn how the GitHub Copilot team uses the <a href="https://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms/" target="_blank" rel="noopener">Jaccard similarity</a> to decide which pieces of context are most relevant to a user’s query &gt;</p></div>
<h3 id="efficient-and-responsible-ai-tooling">Efficient and responsible AI tooling<a href="#efficient-and-responsible-ai-tooling" aria-label="Efficient and responsible AI tooling"></a></h3>
<p>To ensure that Dave doesn’t become even more frustrated by waiting for the LLM assistant to generate a response, the LLM can quickly retrieve an output from a cache. And in the case that Dave does have an outburst, we can use a content classifier to make sure the LLM app doesn’t respond in kind. The telemetry service will also evaluate Dave’s interaction with the UI so that you, the developer, can improve the user experience based on Dave’s behavior.</p>
<ul>
<li>An <strong>LLM cache</strong> stores outputs. This means instead of generating new responses to the same query (because Dave isn’t the first person whose internet has gone down), the LLM can retrieve outputs from the cache that have been used for similar queries. Caching outputs can reduce latency, computational costs, and variability in suggestions.</li>
</ul>
<p>You can experiment with a tool like <a href="https://github.com/zilliztech/GPTCache" target="_blank" rel="noopener">zilliztech/GPTcache</a> to cache your app’s responses.</p>
<ul>
<li>A <strong>content classifier or filter</strong> can prevent your automated assistant from responding with harmful or offensive suggestions (in the case that your end users take their frustration out on your LLM app).</li>
</ul>
<p>Tools like <a href="https://github.com/derwiki/llm-prompt-injection-filtering" target="_blank" rel="noopener">derwiki/llm-prompt-injection-filtering</a> and <a href="https://github.com/laiyer-ai/llm-guard" target="_blank" rel="noopener">laiyer-ai/llm-guard</a> are in their early stages but working toward preventing this problem.</p>
<ul>
<li>A <strong>telemetry service</strong> will allow you to evaluate how well your app is working with actual users. A service that responsibly and transparently monitors user activity (like how often they accept or change a suggestion) can share useful data to help improve your app and make it more useful.</li>
</ul>
<p><a href="https://github.com/open-telemetry" target="_blank" rel="noopener">OpenTelemetry</a>, for example, is an open source framework that gives developers a standardized way to collect, process, and export telemetry data across development, testing, staging, and production environments.</p>
<p>Learn <a href="https://github.blog/2023-10-16-measuring-git-performance-with-opentelemetry/" target="_blank" rel="noopener">how GitHub uses OpenTelemetry</a> to measure Git performance &gt;</p>

<p>Woohoo! 🥳 Your LLM assistant has effectively answered Dave’s many queries. His router is up and working, and he’s ready for his World Cup watch party. Mission accomplished!</p>
<hr>
<h2 id="real-world-impact-of-llms">Real-world impact of LLMs<a href="#real-world-impact-of-llms" aria-label="Real-world impact of LLMs"></a></h2>
<p>Looking for inspiration or a problem space to start exploring? Here’s a list of ongoing projects where LLM apps and models are making real-world impact.</p>
<ul>
<li>NASA and IBM recently open sourced the <a href="https://www.earthdata.nasa.gov/news/impact-ibm-hls-foundation-model" target="_blank" rel="noopener">largest geospatial AI model</a> to increase access to NASA earth science data. The hope is to accelerate discovery and understanding of climate effects.</li>
<li>Read how the Johns Hopkins Applied Physics Laboratory is designing a <a href="https://www.jhuapl.edu/news/news-releases/230817a-cpg-ai-battlefield-medical-assistance" target="_blank" rel="noopener">conversational AI agent</a> that provides, in plain English, medical guidance to untrained soldiers in the field based on established care procedures.</li>
<li>Companies like <a href="https://github.com/customer-stories/duolingo" target="_blank" rel="noopener">Duolingo</a> and <a href="https://github.com/customer-stories/mercado-libre" target="_blank" rel="noopener">Mercado Libre</a> are using <a href="https://github.com/features/copilot" target="_blank" rel="noopener">GitHub Copilot</a> to help more people learn another language (for free) and democratize ecommerce in Latin America, respectively.</li>
</ul>
<hr>
<h3 id="further-reading">Further reading<a href="#further-reading" aria-label="Further reading"></a></h3>
<ul>
<li><a href="https://github.blog/2023-10-05-a-developers-guide-to-open-source-llms-and-generative-ai/#open-source-llms-available-today">A developer’s guide to open source LLMs and generative AI</a></li>
<li><a href="https://github.blog/2023-10-27-demystifying-llms-how-they-can-do-things-they-werent-trained-to-do/">Demystifying LLMs: How they can do things they weren’t trained to do</a></li>
<li><a href="https://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms/">A developer’s guide to prompt engineering and LLMs</a></li>
<li><a href="https://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/">How to build an enterprise LLM application: Lessons from GitHub Copilot</a></li>
</ul>

      
  </main>


  </div><section>
    <h2>
      Explore more from GitHub    </h2>
    <div>
      <div>
          <p><img src="https://github.blog/wp-content/uploads/2022/05/engineering.svg" width="44" height="44" loading="lazy" alt="Engineering"></p><h3>Engineering</h3>
    <p>
      Posts straight from the GitHub engineering team.    </p>

          <p>
        <a href="https://github.blog/category/engineering/">
          Learn more<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" fill="none"><path fill="currentColor" d="M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z"></path><path stroke="currentColor" d="M1.75 8H11" stroke-width="1.5" stroke-linecap="round"></path></svg>
                  </a>
      </p>
      </div>
<div>
          <p><img src="https://github.blog/wp-content/uploads/2023/08/Icon-Circle.svg" width="44" height="44" loading="lazy" alt="GitHub Universe 2023"></p><h3>GitHub Universe 2023</h3>
    <p>
      Get free virtual tickets to the global developer event for AI, security, and DevEx.    </p>

          <p>
        <a href="https://githubuniverse.com/" target="_blank">
          Get free tickets<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
                  </a>
      </p>
      </div>
<div>
          <p><img src="https://github.blog/wp-content/uploads/2022/05/Copilot_Blog_Icon-1.svg" width="44" height="44" loading="lazy" alt="GitHub Copilot"></p><h3>GitHub Copilot</h3>
    <p>
      Don't fly solo. Try 30 days for free.    </p>

          <p>
        <a href="https://github.com/features/copilot?utm_source=blog&amp;utm_medium=bottomnav&amp;utm_campaign=cta&amp;utm_content=copilot" target="_blank">
          Learn more<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
                  </a>
      </p>
      </div>
<div>
          <p><img src="https://github.blog/wp-content/uploads/2022/05/careers.svg" width="44" height="44" loading="lazy" alt="Work at GitHub!"></p><h3>Work at GitHub!</h3>
    <p><span>Check out our current job openings.</span>    </p>

          <p>
        <a href="https://github.com/about/careers" target="_blank">
          Learn more<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
                  </a>
      </p>
      </div>
    </div>
  </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kyutai AI research lab with a $330M budget that will make everything open source (251 pts)]]></title>
            <link>https://techcrunch.com/2023/11/17/kyutai-is-an-french-ai-research-lab-with-a-330-million-budget-that-will-make-everything-open-source/</link>
            <guid>38331751</guid>
            <pubDate>Sun, 19 Nov 2023 11:48:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/11/17/kyutai-is-an-french-ai-research-lab-with-a-330-million-budget-that-will-make-everything-open-source/">https://techcrunch.com/2023/11/17/kyutai-is-an-french-ai-research-lab-with-a-330-million-budget-that-will-make-everything-open-source/</a>, See on <a href="https://news.ycombinator.com/item?id=38331751">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">This morning at Scaleway’s <a href="https://www.ai-pulse.eu/" target="_blank" rel="noopener">ai-PULSE conference</a>, French billionaire and Iliad CEO Xavier Niel gave some extra details about <a href="https://techcrunch.com/2023/09/27/french-billionaire-xavier-niel-pledges-to-invest-up-to-210-million-in-ai/">his plans</a> for an AI research lab based in Paris.</p>
<p>This new lab, called <a href="http://kyutai.org/" target="_blank" rel="noopener">Kyutai</a>, will be a privately funded nonprofit working on artificial general intelligence. It will work with PhD students, postdocs and researchers on research papers and open source projects. When Iliad originally unveiled this research lab, the firm said that Niel was committing €100 million to this project ($109 million at today’s exchange rate).</p>
<p>“Thanks to some amazing friends who are there today, now we are close to €300 million for the financing of this initiative,” Niel said at the conference. Among those “friends” is another French billionaire, Rodolphe Saadé, the CEO of French shipping and logistics giant CMA CGM, who is putting €100 million too. There are other smaller contributors, such as Eric Schmidt’s foundation and some unnamed donators.</p>
<p>This is just a starting point, as Kyutai is open to more donations. “What’s interesting with so many journalists in the room is that the project will potentially interest other investors,” Saadé said at a press conference after the announcement.</p>
<p>As Kyutai will work on foundational models, they will also need some compute power. The good news is that Scaleway, the cloud division of Iliad, recently acquired a thousand Nvidia H100 GPUs. These top-of-the-line GPUs are essential for inference and model training and will be available at cost for Kyutai.</p>
<p>Kyutai has already started hiring for its core scientific team. Six men took the stage this morning to talk about their previous work and what they have in mind for the research lab — Patrick Perez, Edouard Grave, Hervé Jegou, Laurent Mazaré, Neil Zeghidour and Alexandre Defossez. They previously worked for Meta’s AI research team FAIR, Google’s DeepMind division, Inria, etc.</p>
<p>Patrick Perez, who previously worked for Valeo, is going to be the director of the research lab. Kyutai has also put together a team of scientific advisors who are well-known AI researchers — Yejin Choi, Yann LeCun and Bernhard Schölkopf. They will just check everyone’s work once or twice a year and give feedback.</p>
<p>One of the reasons Kyutai thinks it can convince some researchers to join its lab is that researchers will be able to publish research papers.</p>
<p>“Unfortunately, big tech companies tolerate scientific publications less and less. Beyond the ego boost for researchers, it helps to advance research and contribute to the common good,” Niel said during the press conference.</p>
<p>Of course, this isn’t the first open AI research lab. OpenAI, as the name still indicates, started as a nonprofit. But things changed drastically after Sam Altman started working full time on OpenAI in 2019. OpenAI moved to a more traditional corporate structure and raised funding from Microsoft.</p>
<p>Other companies have also been working on open source foundational models, such as Meta with its <a href="https://ai.meta.com/llama/" target="_blank" rel="noopener">Llama model</a> and <a href="https://mistral.ai/" target="_blank" rel="noopener">Mistral AI</a>. Kyutai’s models will be open source too, but the researchers describe their work as open science. They plan to release open source models, but also the training source code and data that explain how they released these models.</p>
<p>“When it comes to the timeline, I don’t think our aim is necessarily to go as fast as Mistral, because our ambition is to provide a scientific purpose, an understanding and a code base to explain the results,” Defossez said at the press conference. But they expect to have something to share within a year.</p>
<p>Mazaré, another researcher from Kyutai’s team, still described <a href="https://techcrunch.com/2023/09/27/mistral-ai-makes-its-first-large-language-model-free-for-everyone/">Mistral AI’s first open source model</a> as a success because many community members have been fine-tuning it and exploring use cases based on the Mistral 7B model.</p>
<p>It’s also going to be interesting to see if a research lab is more efficient at releasing foundational models compared to private companies, and how private companies are going to leverage Kyutai’s work for commercial applications.</p>
<div id="attachment_2630652"><p><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-2630652" src="https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-3.jpg" alt="" width="1024" height="768" srcset="https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-3.jpg 4032w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-3.jpg?resize=150,113 150w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-3.jpg?resize=300,225 300w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-3.jpg?resize=768,576 768w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-3.jpg?resize=680,510 680w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-3.jpg?resize=1536,1152 1536w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-3.jpg?resize=2048,1536 2048w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-3.jpg?resize=1200,900 1200w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-3.jpg?resize=50,38 50w" sizes="(max-width: 1024px) 100vw, 1024px"></p><p id="caption-attachment-2630652"><strong>Image Credits:</strong> Romain Dillet / TechCrunch</p></div>
<p>“I’m also a strong believer in open source, and we need to turn it into a French asset,” French President Emmanuel Macron said in a prerecorded video message at the conference.</p>
<h2>France’s position: Regulating use cases, not models</h2>
<p>Macron also used this opportunity to define and defend France’s position on Europe’s AI Act, saying that use cases should be regulated, not model makers. France has been pushing to <a href="https://techcrunch.com/2023/11/14/eu-ai-act-trilogue-crunch/">water down the AI Act</a> in trilogues (a trilogue is a negotiation between Europe’s three main instances, the Parliament, the Commission and the Council).</p>
<p>“Regulation is not the enemy of innovation, quite the contrary. It’s not a question of defining good models, but we need to ensure that the services made available to our citizens are safe for them, for other economic players and for our democracy,” Macron said.</p>
<p>“With work on the European regulation for artificial intelligence currently in ‘trilogues,’ regulation must be controlled and not punitive, to preserve innovation and regulate usage rather than technology as such,” he added.</p>
<p>Niel basically sided with France’s position on this topic during the press conference. According to him, Europe is lagging behind when it comes to AI innovation, and regulation will slow down European newcomers and diminish the chances of them catching up.</p>
<p>“For the time being we’re more in the innovation part than the regulation part. Creating regulation means it creates barriers to competitors,” Niel said.</p>
<p>Maybe if French AI companies become massively successful, things could change. “I’d love it if one day we could talk about French imperialism in AI,” Niel added later in the conversation.</p>

<div id="attachment_2630664"><p><img decoding="async" aria-describedby="caption-attachment-2630664" src="https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-4.jpg" alt="" width="1024" height="768" srcset="https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-4.jpg 4032w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-4.jpg?resize=150,113 150w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-4.jpg?resize=300,225 300w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-4.jpg?resize=768,576 768w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-4.jpg?resize=680,510 680w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-4.jpg?resize=1536,1152 1536w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-4.jpg?resize=2048,1536 2048w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-4.jpg?resize=1200,900 1200w, https://techcrunch.com/wp-content/uploads/2023/11/Kyutai-4.jpg?resize=50,38 50w" sizes="(max-width: 1024px) 100vw, 1024px"></p><p id="caption-attachment-2630664"><strong>Image Credits:</strong> Romain Dillet / TechCrunch</p></div>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lindenmayer Systems (145 pts)]]></title>
            <link>https://vsekar.me/blog/log_coffee/chapter_0.html</link>
            <guid>38331750</guid>
            <pubDate>Sun, 19 Nov 2023 11:47:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vsekar.me/blog/log_coffee/chapter_0.html">https://vsekar.me/blog/log_coffee/chapter_0.html</a>, See on <a href="https://news.ycombinator.com/item?id=38331750">Hacker News</a></p>
<div id="readability-page-1" class="page">
        <!-- Provide site root to javascript -->
        

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                                
                <div id="menu-bar">
                    

                    <h2>log_coffee!</h2>

                    
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        <h2 id="lindenmayer-systems"><a href="#lindenmayer-systems">Lindenmayer systems</a></h2>
<p>A Lindemayer system or LSystem for short consists of an alphabet of symbols that can be used to make strings, a collection of production that expand each symbol into some larger string of symbols, an initial “axiom” string from which to begin construction, and a mechanism for translating the generated strings into geometric structures.</p>
<p>A few years back, I built a rudimentary website to extrapolate a few known LSystems in pure JS. However, presently I have been getting more comfortable with Rust 🦀 and explored rebuilding it with Rust and wasm-bindgen to get better performance for tougher iterations.</p>
<p>Website - <a href="https://vsekar.me/LSystems/">vsekar.me/LSystems/</a></p>
<p>Repository - <a href="https://github.com/Spockuto/LSystems/">github.com/Spockuto/LSystems/</a></p>
<p>Now let's explore how an axiom, a set of rules, and an angle can be developed into this beautiful fern
<img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/barnsley_fern.png" alt="Barnsley Fern"></p>
<h3 id="step-0---define-the-lsystem"><a href="#step-0---define-the-lsystem">Step 0 - Define the LSystem</a></h3>
<p>For Barnsley Fern, the LSystem is defined as follows</p>
<pre><code>Axiom : X
Rules : 
 - X -&gt; F-[[X]+X]+F[+FX]-X
 - F -&gt; FF
Angle : 22.5
</code></pre>
<p>This translates into the following for our Rust implementation.</p>
<pre><pre><code><span>#![allow(unused)]
</span><span>fn main() {
</span>lazy_static! {
    static ref BARNSLEY_FERN: LSystem = LSystem {
        variables: "XF",
        axiom: "X",
        rules: vec![
            ('X', "F-[[X]+X]+F[+FX]-X"), 
            ('F', "FF"),
        ]
        .into_iter()
        .collect(),
        angle: 22.5,
        max_rounds: 8,
    };
}
<span>}
</span></code></pre></pre>
<ul>
<li><em>variables</em> : These are the identifiers that will be considered during building the sequence. Others will be skipped.</li>
<li><em>axiom</em> : The initial sequence</li>
<li><em>rules</em> : Expansion rules for each identifier</li>
<li><em>angle</em> : Explained later</li>
<li><em>max_rounds</em> - Implementation detail. Maximum number of supported iterations before running out of heap memory.</li>
</ul>
<h3 id="step-1---generate-the-sequence"><a href="#step-1---generate-the-sequence">Step 1 - Generate the sequence</a></h3>
<p>Once the LSystem is defined, we can build the drawing sequence. Iterations define the number of rounds the sequence will be expanded using the given rules. The higher the iteration, the better defined the picture is.</p>
<pre><pre><code><span>#![allow(unused)]
</span><span>fn main() {
</span>/// Generate the Turtle graphics sequence for given iterations
pub fn expand(&amp;self, iterations: u32) -&gt; String {

    // panic if given iterations are greater than the accepted limit.
    if iterations &gt; self.0.max_rounds {
        panic!("Max limit reached");
    }

    let mut sequence = String::new();
    for i in 0..iterations {
        if i == 0 {
            // First iteration is the axiom
            sequence.insert_str(0, self.0.axiom);
        } else {
            let sequence_copy = sequence.to_string();
            let mut insert_index = 0;
            for identifier in sequence_copy.chars() {
                // Skip if the variable in sequence doesn't have a rule
                if !self.0.variables.contains(identifier) {
                    insert_index += 1;
                    continue;
                }
                // Expand the sequence based on the rule
                let rule = self.0.rules.get(&amp;identifier).unwrap();
                sequence.remove(insert_index);
                sequence.insert_str(insert_index, rule);
                insert_index += &amp;rule.len();
            }
        }
        // The current sequence will be used as the generator for the next round
    }
    sequence
}
<span>}
</span></code></pre></pre>
<blockquote>
<p>Note: The function above can also be written recursively but for larger iterations, you would run out of stack depth way before running out of heap memory.</p>
</blockquote>
<p>For Barnsley Fern, the sequence expands as follows for subsequent iterations</p>
<div><table><thead><tr><th>Iterations</th><th>Sequence</th></tr></thead><tbody>
<tr><td>1</td><td>X</td></tr>
<tr><td>2</td><td>F-[[X]+X]+F[+FX]-X</td></tr>
<tr><td>3</td><td>FF-[[F-[[X]+X]+F[+FX]-X]+F-[[X]+X]+F[+FX]-X]+FF[+FFF-[[X]+X]+F[+FX]-X]-F-[[X]+X]+F[+FX]-X</td></tr>
<tr><td>4</td><td>FFFF-[[FF-[[F-[[X]+X]+F[+FX]-X]+F-[[X]+X]+F[+FX]-X]+FF[+FFF-[[X]+X]+F[+FX]-X]-F-[[X]+X]+F[+FX]-X]+FF-[[F-[[X]+X]+F[+FX]-X]+F-[[X]+X]+F[+FX]-X]+FF[+FFF-[[X]+X]+F[+FX]-X]-F-[[X]+X]+F[+FX]-X]+FFFF[+FFFFFF-[[F-[[X]+X]+F[+FX]-X]+F-[[X]+X]+F[+FX]-X]+FF[+FFF-[[X]+X]+F[+FX]-X]-F-[[X]+X]+F[+FX]-X]-FF-[[F-[[X]+X]+F[+FX]-X]+F-[[X]+X]+F[+FX]-X]+FF[+FFF-[[X]+X]+F[+FX]-X]-F-[[X]+X]+F[+FX]-X</td></tr>
</tbody></table>
</div>
<h3 id="step-2---draw-the-canvas-based-on-the-sequence"><a href="#step-2---draw-the-canvas-based-on-the-sequence">Step 2 - Draw the canvas based on the sequence</a></h3>
<p>Now that the sequence is generated, we can start building the canvas. Each character in the sequence defines a particular set of operations on the canvas. This allows us to convert the sequence into a picture.</p>
<pre><pre><code><span>#![allow(unused)]
</span><span>fn main() {
</span>// Define length
// Set the angle to the LSystem angle
let angle_rad = -1.0 * PI * angle / 180.0;
let (mut x, mut y) = (0.0, 0.0);
let mut stack = vec![];

for seq in sequence.chars() {
    // perform operations on the canvas 
    match seq {
        // draw a line to (x,y) at given length and angle
        'F' | 'A' | 'B' =&gt; {
            x += length * angle.cos();
            y += length * angle.sin();
            context.line_to(x, y);
            context.stroke();
        }
        'S' =&gt; {
        // move along a line to (x,y) at given length and angle
            x += length * angle.cos();
            y += length * angle.sin();
            context.move_to(x, y);
        }
        '+' =&gt; {
        // rotate counterclockwise by angle_rad
            angle += angle_rad;
        }
        '-' =&gt; {
        // rotate clockwise by angle rad
            angle -= angle_rad;
        }
        '[' =&gt; {
        // push a point into stack
            stack.push(Line { x, y, angle });
        }
        ']' =&gt; {
        // Pop a point from the stack and move to it.
            let line = stack.pop().unwrap();
            (x, y, angle) = (line.x, line.y, line.angle);
            context.move_to(x, y);
        }
        // For others, skip and continue
        _ =&gt; continue,
    }
}
<span>}
</span></code></pre></pre>

<h3 id="step-3---color-the-canvas-using-a-linear-gradient"><a href="#step-3---color-the-canvas-using-a-linear-gradient">Step 3 - Color the canvas using a linear gradient</a></h3>
<p>The final step in generating our fractal is applying a linear gradient between two colors over our canvas. This can be achieved by applying color interpolation over each dark pixel.</p>
<pre><pre><code><span>#![allow(unused)]
</span><span>fn main() {
</span>let image_data = context
    .get_image_data(0.0, 0.0, width as f64, height as f64)
    .unwrap();

// data contains 4 u8 values per pixel indicating RGBA (red, green, blue, alpha) values
let mut data = image_data.data();

// Set the linear gradient colors : color1 -&gt; color2 

let c1 = Rgb::from_hex_str(&amp;color1).unwrap();
let c2 = Rgb::from_hex_str(&amp;color2).unwrap();

// linear gradient is defined using interpolation
// Assume the fraction grows from 0 to 1
// pixel.r = c1.r + (c2.r - c1.r) * fraction
// pixel.g = c1.g + (c2.g - c1.g) * fraction
// pixel.b = c1.b + (c2.b - c1.b) * fraction

for index in 0..(width * height * 4) as usize {
    // Since the canvas is drawn with black, only alpha will be set initially
    if data[index] &gt; 0 {
    // pixel's alpha is set
    // Update the color using interpolation
        let fraction = index as f32 / (height * width * 4) as f32;
        data[index - 3] = (c1.get_red() + (c2.get_red() - c1.get_red()) * fraction) as u8;
        data[index - 2] = (c1.get_green() + (c2.get_green() - c1.get_green()) * fraction) as u8;
        data[index - 1] = (c1.get_blue() + (c2.get_blue() - c1.get_blue()) * fraction) as u8;
    }
}
let slice_data = Clamped(&amp;data.0[..]);
let image_data = web_sys::ImageData::new_with_u8_clamped_array_and_sh(
    slice_data,
    width as u32,
    height as u32,
)
.unwrap();

// load the data back into the canvas
context.put_image_data(&amp;image_data, 0.0, 0.0).unwrap();
<span>}
</span></code></pre></pre>
<p>With this, a linear gradient between <code>#C6EA8D</code> and <code>#FE90AF</code> gives us
<img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/barnsley_fern.png" alt="Barnsley Fern"></p>
<blockquote>
<p>Note: Colors don't exactly behave well with linear interpolation. For more details check out <a href="https://www.alanzucconi.com/2016/01/06/colour-interpolation/">The Secrets of Colour Interpolation</a></p>
</blockquote>
<h2 id="fractal-tree"><a href="#fractal-tree">Fractal Tree</a></h2>
<pre><code>Axiom : F
Rules : 
 - F -&gt; FF-[-F+F+F]+[+F-F-F]
Angle : 22.5
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/fractal_tree.png" alt="Fractal Tree"></p>
<h2 id="fractal-tree-2"><a href="#fractal-tree-2">Fractal Tree 2</a></h2>
<pre><code>Axiom : VZFFF
Rules : 
 - F -&gt; F
 - V -&gt; [+++W][---W]YV
 - W -&gt; +X[-W]Z
 - X -&gt; -W[+X]Z
 - Y -&gt; YZ
 - Z -&gt; [-FFF][+FFF]F
Angle : 18
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/fractal_tree_2.png" alt="Fractal Tree 2"></p>
<h2 id="dragon-curve"><a href="#dragon-curve">Dragon Curve</a></h2>
<pre><code>Axiom : FX
Rules : 
 - X -&gt; X+YF+
 - Y -&gt; -FX-Y
Angle : 90
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/dragon_curve.png" alt="Dragon Curve"></p>
<h2 id="32-segment-curve"><a href="#32-segment-curve">32 Segment Curve</a></h2>
<pre><code>Axiom : F+F+F+F
Rules : 
 - F -&gt; -F+F-F-F+F+FF-F+F+FF+F-F-FF+FF-FF+F+F-FF-F-F+FF-F-F+F+F-F+
Angle : 90
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/32_segment.png" alt="32 Segment Curve"></p>
<h2 id="peano-gosper-curve"><a href="#peano-gosper-curve">Peano Gosper Curve</a></h2>
<pre><code>Axiom : A
Rules : 
 - A -&gt; A-B--B+A++AA+B-
 - B -&gt; +A-BB--B-A++A+B
Angle : 60
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/peano_gosper.png" alt="Peano Gosper Curve"></p>
<h2 id="koch-snowflake"><a href="#koch-snowflake">Koch Snowflake</a></h2>
<pre><code>Axiom : F++F++F
Rules : 
 - F -&gt; F-F++F-F
Angle : 60
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/koch_snowflake.png" alt="Koch Snowflake"></p>
<h2 id="koch-snowflake-2"><a href="#koch-snowflake-2">Koch Snowflake 2</a></h2>
<pre><code>Axiom : F+F+F+F
Rules : 
 - F -&gt; F-F+F+F-F
Angle : 60
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/koch_snowflake_2.png" alt="Koch Snowflake 2"></p>
<h2 id="koch-snowflake-3"><a href="#koch-snowflake-3">Koch Snowflake 3</a></h2>
<pre><code>Axiom : F
Rules : 
 - F -&gt; F-F+F+F-F
Angle : 85
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/koch_snowflake_3.png" alt="Koch Snowflake 3"></p>
<h2 id="quadratic-koch-island"><a href="#quadratic-koch-island">Quadratic Koch Island</a></h2>
<pre><code>Axiom : F+F+F+F
Rules : 
 - F -&gt; F+F-F-FF+F+F-F
Angle : 90
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/quad_koch_island.png" alt="quad. Koch Island"></p>
<h2 id="quadratic-koch-island-2"><a href="#quadratic-koch-island-2">Quadratic Koch Island 2</a></h2>
<pre><code>Axiom : F+F+F+F
Rules : 
 - F -&gt; F+FF-FF-F-F+F+FF-F-F+F+FF+FF-F
Angle : 90
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/quad_koch_island_2.png" alt="quad. Koch Island 2"></p>
<h2 id="islands"><a href="#islands">Islands</a></h2>
<pre><code>Axiom : F+F+F+F
Rules : 
 - F -&gt; F-SFF+F+FF+F-S-FFF-F+F+F-FFFF
 - S -&gt; SSSSSSSS
Angle : 90
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/islands.png" alt="Islands"></p>
<h2 id="islands-2"><a href="#islands-2">Islands 2</a></h2>
<pre><code>Axiom : F+F+F+F
Rules : 
 - F -&gt; F-SF+FF+F+FF-S-FF+SF-FF-F-FF+S+FFF
 - S -&gt; SSSSSS
Angle : 90
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/islands_2.png" alt="Islands 2"></p>
<h2 id="sierpinski-triangle"><a href="#sierpinski-triangle">Sierpinski Triangle</a></h2>
<pre><code>Axiom : FXF--FF--FF
Rules : 
 - F -&gt; FF
 - X -&gt; --FXF++FXF++FXF--
Angle : 60
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/sierpinski_t.png" alt="Sierpinski Triangle"></p>
<h2 id="sierpinski-square"><a href="#sierpinski-square">Sierpinski Square</a></h2>
<pre><code>Axiom : F+F+F+F
Rules : 
 - F -&gt; FF+F+F+F+FF
Angle : 90
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/sierpinski_s.png" alt="Sierpinski Square"></p>
<h2 id="hilbert-curve"><a href="#hilbert-curve">Hilbert Curve</a></h2>
<pre><code>Axiom : X
Rules : 
 - X -&gt; +YF-XFX-FY+
 - Y -&gt; -XF+YFY+FX-
Angle : 90
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/hilbert.png" alt="Hilbert Curve"></p>
<h2 id="frec-fractal"><a href="#frec-fractal">Frec Fractal</a></h2>
<pre><code>Axiom : XYXYXYX+XYXYXYX+XYXYXYX+XYXYXYX
Rules : 
 - F -&gt; 
 - X -&gt; FX+FX+FXFY-FY-
 - Y -&gt; +FX+FXFY-FY-FY
Angle : 90
</code></pre>
<p><img src="https://raw.githubusercontent.com/Spockuto/blog/master/src/images/frec_fractal.png" alt="Frec Fractal"></p>

                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="https://vsekar.me/blog/log_coffee/chapter_1.html" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i></i>
                            </a>

                            <a rel="next" href="https://vsekar.me/blog/archive.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">
                    <a rel="prev" href="https://vsekar.me/blog/log_coffee/chapter_1.html" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i></i>
                    </a>

                    <a rel="next" href="https://vsekar.me/blog/archive.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div>




        


        
        
        

        
        
        

        <!-- Custom JS scripts -->


    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Comparing Humans, GPT-4, and GPT-4V on Abstraction and Reasoning Tasks (205 pts)]]></title>
            <link>https://arxiv.org/abs/2311.09247</link>
            <guid>38331669</guid>
            <pubDate>Sun, 19 Nov 2023 11:36:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2311.09247">https://arxiv.org/abs/2311.09247</a>, See on <a href="https://news.ycombinator.com/item?id=38331669">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2311.09247.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>We explore the abstract reasoning abilities of text-only and multimodal versions of GPT-4, using the ConceptARC benchmark [10], which is designed to evaluate robust understanding and reasoning with core-knowledge concepts. We extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed, one-shot prompting (rather than simple, zero-shot prompts) with text versions of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4, on zero- and one-shot prompts using image versions of the simplest tasks. Our experimental results support the conclusion that neither version of GPT-4 has developed robust abstraction abilities at humanlike levels.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Melanie Mitchell [<a href="https://arxiv.org/show-email/17a8ecc6/2311.09247">view email</a>]      <br>    <strong>[v1]</strong>
        Tue, 14 Nov 2023 04:33:49 UTC (1,549 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Any comprehensive courses on Auth? (212 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=38331501</link>
            <guid>38331501</guid>
            <pubDate>Sun, 19 Nov 2023 11:06:21 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=38331501">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="38331501">
      <td><span></span></td>      <td><center><a id="up_38331501" href="https://news.ycombinator.com/vote?id=38331501&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=38331501">Ask HN: Any comprehensive courses on Auth?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_38331501">113 points</span> by <a href="https://news.ycombinator.com/user?id=bojangleslover">bojangleslover</a> <span title="2023-11-19T11:06:21"><a href="https://news.ycombinator.com/item?id=38331501">3 hours ago</a></span> <span id="unv_38331501"></span> | <a href="https://news.ycombinator.com/hide?id=38331501&amp;goto=item%3Fid%3D38331501">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Any%20comprehensive%20courses%20on%20Auth%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=38331501&amp;auth=9db2af2bbbf77f7f13cef3cc6cd7ffb2d1e3956d">favorite</a> | <a href="https://news.ycombinator.com/item?id=38331501">39&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><p>I would like cover basic username/password auth, OAuth and Active Directory, security keys and everything in between. Would like to do this in a linear fashion, ie like a coursera course with practice problems.</p></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table><table>
            <tbody><tr id="38331757"><td></td></tr>
                <tr id="38332523"><td></td></tr>
            <tr id="38332002"><td></td></tr>
                <tr id="38332545"><td></td></tr>
                        <tr id="38331628"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38331628" href="https://news.ycombinator.com/vote?id=38331628&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><p><span>What made me understand these things the most, was setting this up just for myself.<p>For example host your own instance of Zitadel, Authentik or whatever you find most appealing.
Tinker a bit around with it. Then use that instance to authenticate yourself somewhere, i.e. another service where you can set up your own oauth provider. Take a look at  the API requests, take a look the code of some OAuth implementation, for example in projects like Gitea, Nextcloud.</p><p>May not be it for everyone, though I really like learning by doing.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38331917"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38331917" href="https://news.ycombinator.com/vote?id=38331917&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><p><span>One thing that might be interesting is SASL has evolved over the years. Most things are RFCs, so well written, short and open specifications. This gives you one larger thing to learn. Should be rather linear if you sort by RFC number.<p>It would head well into advanced user/password schemes.</p><p>The problem is that even advanced mechanism like a SCRAM based authentication with additional 2fa are rather simple to grasp &amp; implement, but really hard to get right / secure.</p><p>A lot of the evolution is rather an evolution of attacks and issues, leading to new schemes. OWASP is thus pretty relevant, too.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38333106"><td></td></tr>
            <tr id="38332562"><td></td></tr>
            <tr id="38332236"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38332236" href="https://news.ycombinator.com/vote?id=38332236&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><p><span>I recently had to learn OIDC which is the standard for auth that most people really mean when they say OAuth now, I think.  I learned by implementing (using Keycloak) and most importantly by reading the OIDC specs.  It may seem intimidating, but the real core of it is not that large.<p>It's a topic I'd be interested in writing more about, and I'm happy to start here if you would find it useful.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38332182"><td></td></tr>
            <tr id="38331599"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38331599" href="https://news.ycombinator.com/vote?id=38331599&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><br><div>
                  <p><span>I'm also interested in this, but specifically something that covers authentication between services and in particular situations where a user authenticates against service a and now service a needs to ask service b to do something on behalf of the user. Not just a handwavy "use OAuth" but more concrete and thorough.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="38332980"><td></td></tr>
            <tr id="38331983"><td></td></tr>
            <tr id="38331792"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38331792" href="https://news.ycombinator.com/vote?id=38331792&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><br><div>
                  <p><span>To be honest, my first instinct was to give a hand wavy "use OAuth". But to elaborate a but further, oauth is made for this and is the industry standard for this kind of thing. There's lots out there on oauth that tells you more than "just use it", it's just a couple searches away. So I don't think I really understand the question.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="38332715"><td></td></tr>
            <tr id="38332916"><td></td></tr>
            <tr id="38332717"><td></td></tr>
            <tr id="38332707"><td></td></tr>
                <tr id="38332953"><td></td></tr>
                  <tr id="38332771"><td></td></tr>
            <tr id="38332111"><td></td></tr>
            <tr id="38331800"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38331800" href="https://news.ycombinator.com/vote?id=38331800&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><p><span>I’ve learned a lot about these things by working on a project using Ory Kratos. The documentation is a bit patchy but it’s open source so you can dive into the gritty details of how a fairly large id provider implements the various aspects of OAuth and so on.
(One nice thing about Azure Active Directory is that it supports OAuth2 integrations so if you understand and can implement OAuth2 then you can also implement AD).<p>I know it’s not a linear learning answer but hope it helps you perhaps later. Good luck!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38331629"><td></td></tr>
            <tr id="38332222"><td></td></tr>
            <tr id="38331834"><td></td></tr>
            <tr id="38331719"><td></td></tr>
            <tr id="38332114"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38332114" href="https://news.ycombinator.com/vote?id=38332114&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><br><div>
                  <p><span>How much are you willing to pay for it so you would get a knowledge base that is not superficial, but thorough and you'll really know the ins and outs of it?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="38332175"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38332175" href="https://news.ycombinator.com/vote?id=38332175&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><p><span>none.<p>but useful stuff:</p><p>certified ethical hacker course can give you a perpetrator's pov on how people get hacked.</p><p>owasp cheatsheet and latacora blog are useful reference also.</p><p>understanding how companies offer these services also helps, e.g. clerk.com, ory.sh, auth0, okta, supertokens, etc.</p><p>understanding how authentication coincides with authorization helps too.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38332895"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38332895" href="https://news.ycombinator.com/vote?id=38332895&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><p><span>I don't know of any, but here's the resources I've found useful as I've worked in the space (disclosure: I work for an auth vendor, FusionAuth).<p>* Solving Identity Management In Modern Applications is a great book offering an overview of the entire identity process, including provisioning (adding users), authentication and more. I read and reference the 2019 edition; don't have the 2023 edition but expect it is just as good: <a href="https://link.springer.com/book/10.1007/978-1-4842-8261-8" rel="nofollow noreferrer">https://link.springer.com/book/10.1007/978-1-4842-8261-8</a></p><p>* OAuth2 In Action walks you through building an OAuth2 server from scratch (in JavaScript). You'll learn about the fundamentals of tokens, clients, registration, and more. Very accessible. <a href="https://www.manning.com/books/oauth-2-in-action" rel="nofollow noreferrer">https://www.manning.com/books/oauth-2-in-action</a></p><p>* The Security Engineering Handbook is great for foundational security knowledge, like 'What does a hash look like, and what makes a good hashing algorithm' as well as a lot of broader security topics: <a href="https://www.cl.cam.ac.uk/~rja14/book.html" rel="nofollow noreferrer">https://www.cl.cam.ac.uk/~rja14/book.html</a></p><p>* FusionAuth's vendor neutral articles: <a href="https://fusionauth.io/articles/" rel="nofollow noreferrer">https://fusionauth.io/articles/</a> . I'd especially call out these two: The Modern Guide to OAuth, which walks through the multiple different ways the OAuth 2 authorization framework can be used: <a href="https://fusionauth.io/learn/expert-advice/oauth/modern-guide-to-oauth" rel="nofollow noreferrer">https://fusionauth.io/learn/expert-advice/oauth/modern-guide...</a> (previous HN discussion: <a href="https://news.ycombinator.com/item?id=29752918">https://news.ycombinator.com/item?id=29752918</a> ), and the Math of Password Hashing: <a href="https://fusionauth.io/learn/expert-advice/security/math-of-password-hashing-algorithms-entropy" rel="nofollow noreferrer">https://fusionauth.io/learn/expert-advice/security/math-of-p...</a></p><p>* The Beer Drinkers Guide to SAML is a great resource for understanding this (still) critical standard, plus just a fun read: <a href="https://duo.com/blog/the-beer-drinkers-guide-to-saml" rel="nofollow noreferrer">https://duo.com/blog/the-beer-drinkers-guide-to-saml</a></p><p>* The RFCs and BCPs (as mentioned). I've also learned a lot by lurking on the OAuth mailing list, which is freely available: <a href="https://mailarchive.ietf.org/arch/browse/oauth/" rel="nofollow noreferrer">https://mailarchive.ietf.org/arch/browse/oauth/</a></p><p>* The Identity Unlocked podcast with  Vittorio Bertocci  (RIP). This is not about the basics at all, but is a deeper dive into the dev focused side of authentication, and will give you great pointers for more reading: <a href="https://identityunlocked.auth0.com/" rel="nofollow noreferrer">https://identityunlocked.auth0.com/</a></p><p>* The OWASP guides are good but specialized. See for example: <a href="https://owasp.org/API-Security/editions/2023/en/0xa2-broken-authentication/" rel="nofollow noreferrer">https://owasp.org/API-Security/editions/2023/en/0xa2-broken-...</a></p><p>* I have a substack where I talk about aspects of customer identity and access management that I think is pretty good :) : <a href="https://ciamweekly.substack.com/" rel="nofollow noreferrer">https://ciamweekly.substack.com/</a></p><p>I think this would be a great linkedin learning, udacity or coursera course, but didn't see anything when I searched there. I've put together courses before and it's a ton of work, but hmmm, maybe it'd be fun to do for this topic.</p><p>Edit: corrected spelling of Vittorio Bertocci's name.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38333032"><td></td></tr>
                <tr id="38333154"><td></td></tr>
                        <tr id="38331735"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38331735" href="https://news.ycombinator.com/vote?id=38331735&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><p><span>Huh, I always forget a lot of programmers weren't around when this stuff was invented. It's all actually pretty simple, and very little complexity. However, there are so many "gotchas" (that can result in zero security) that anyone writing a guide like this would probably have you sign a waiver, then any company you work for sign a waiver, and include your firstborn child.<p>For example, user/pass is pretty simple on the surface:</p><p>1. app sends server user/password.</p><p>2. check if it matches the password in the database.</p><p>3. if so, respond with a token the app can send back that is associated with the user. if not, return with a 401.</p><p>The number of gotchas in this simple 3-step process is insane... here's some off the top of my head (not exhaustive):</p><p>- make sure the login form includes a CSRF token.</p><p>- do not store the password in plaintext in the db. or encrypted, probably. Since an attacker can possibly get the encryption key and then decrypt all your passwords. Use strong, slow hashes.</p><p>- rate limit your logins to prevent brute-forcing (slow hashes work great here)</p><p>- use constant-time comparisons to check if the password matches (e.g., hash_equals() in PHP), RTFM for whatever constant time check you are using or you will open yourself up to timing attacks.</p><p>That's the issue with security stuff, there are so many gotchas that anyone writing a course would open themselves up to getting sued (at least in the US) just for missing a gotcha or someone with Dunning-Kruger thinking they know everything and getting hacked ... it's too risky. You have to just get into the industry and learn it the hard way. At least that's how I learned everything I learned.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38331830"><td></td></tr>
                <tr id="38331900"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_38331900" href="https://news.ycombinator.com/vote?id=38331900&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><br><div>
                  <p><span>I'm having to deal with this stuff right now. Firebase stores their refresh token in local storage and that allows minting new session tokens once they expire, are they wrong? Is there any other way to remain signed in "forever"? (until logout or until token is revoked)</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="38332253"><td></td></tr>
                <tr id="38332490"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_38332490" href="https://news.ycombinator.com/vote?id=38332490&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><br><div>
                  <p><span>That is what I'm using but I have to authenticate again every few days. If I used the client library it would autorefresh the token periodically, but that stores the refresh token in local storage. Since that is something you recommend against, I was wondering why.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="38332759"><td></td></tr>
                                    <tr id="38331839"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38331839" href="https://news.ycombinator.com/vote?id=38331839&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><p><span>Actually, I think we're doing a huge disservice to our profession as programmers when we call stuff like this "an insane number of gotchas". This is no critique of you or your post specifically, mind you, and I know where you're coming from. But it's a critique of a general tendency among programmers to call anything that requires a bit of knowledge and thought beyond the simplest surface level solution "complex" or "insane to implement on your own". It's not. While I know that you're list of gotchas isn't exhaustive, the real list is not so much longer that it's not perfectly reasonable to expect someone to be able to implement it correctly.<p>I say that as someone who was on the "receiving end" of this kind of advice for years btw. I always thought that the things that are "better left to libraries" are really arcane and impossible to understand, which only lead to confusion and an inability to truly assess options. And it's really just a matter of semantics and framing. It would be perfectly reasonable to say "it's not complex as long as you keep this reasonably long list of gotchas in mind".
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38332344"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_38332344" href="https://news.ycombinator.com/vote?id=38332344&amp;how=up&amp;goto=item%3Fid%3D38331501"></a></center>    </td><td><p><span>I don't know why you're getting downvoted (I have no idea why people are on HN if they think this is just Reddit. If you downvote, say why and start a discussion), but you're right. My intent wasn't to imply "just don't do it" or "leave it to libraries." I was trying to say why you can't really find a guide like the post is asking for (at least for free!) and it likely has a lot to do with liability and things like that.<p>I was trying to say exactly what you are saying, and that is just get in there and learn. It isn't that complex to implement this stuff yourself if you need to. I've implemented this stuff myself dozens of times over the years... but I try to use a library before implementing it myself. Interestingly, over the years, I've reviewed libraries and found bugs in them. So, do read the code of the library you're using. Once you've reviewed a few of them (and implemented it yourself a few times), you kinda get an idea of what to look for.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why dumb ideas capture smart and successful people (133 pts)]]></title>
            <link>https://www.robkhenderson.com/p/how-dumb-ideas-capture-smart-and</link>
            <guid>38331493</guid>
            <pubDate>Sun, 19 Nov 2023 11:05:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.robkhenderson.com/p/how-dumb-ideas-capture-smart-and">https://www.robkhenderson.com/p/how-dumb-ideas-capture-smart-and</a>, See on <a href="https://news.ycombinator.com/item?id=38331493">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg" width="451" height="451" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:451,&quot;bytes&quot;:1722125,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2738bcaa-d0b4-4008-93af-9cf9eb817331_1732x1732.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p data-attrs="{&quot;url&quot;:&quot;https://www.robkhenderson.com/p/how-dumb-ideas-capture-smart-and?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.robkhenderson.com/p/how-dumb-ideas-capture-smart-and?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p data-attrs="{&quot;url&quot;:&quot;https://www.robkhenderson.com/subscribe?&amp;gift=true&quot;,&quot;text&quot;:&quot;Give a gift subscription&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.robkhenderson.com/subscribe?&amp;gift=true" rel=""><span>Give a gift subscription</span></a></p><p>Many have discovered an argument hack. They don’t need to argue that something is false. They just need to show that it’s associated with low status. The converse is also true: You don’t need to argue that something is true. You just need to show that it’s associated with high status. And when low status people express the truth, it sometimes becomes high status to lie.</p><p><span>In the 1980s, the psychologists Richard E. Petty and John T. Cacioppo </span><a href="https://www.springer.com/gp/book/9781461293781?ref=quillette.com" rel="">developed</a><span> the “Elaboration Likelihood Model” to describe how persuasion works. “Elaboration” here means the extent to which a person carefully thinks about the information. When people’s motivation and ability to engage in careful thinking is present, the “elaboration likelihood” is high. This means people are likely to pay attention to the relevant information and draw conclusions based on the merits of the arguments or the message. When elaboration likelihood is high, a person is willing to expend their cognitive resources to update their views.</span></p><p>The idea is that there are two paths, or two “routes,” to persuading others. The first type, termed the “central” route, comes from careful and thoughtful consideration of the messages we hear. When the central route is engaged, we actively evaluate the information presented, and try to discern whether or not it’s true.</p><p>When the “peripheral” route is engaged, we pay more attention to cues apart from the actual information or content or the message. For example, we might evaluate someone’s argument based on how attractive they are or where they were educated, without considering the actual merits of their message.</p><p>When we accept a message through the peripheral route, we tend to be more passive than when we accept a message through the central route. Unfortunately, the peripheral route is more prevalent because we are exposed to an increasingly large amount of information.</p><p><span>The renowned psychologists Susan Fiske and Shelley Taylor have characterized humans as “cognitive misers.” They </span><a href="https://archive.org/details/socialcognition0002fisk" rel="">write</a><span>, “People are limited in their capacity to process information, so they take shortcuts whenever they can.”</span></p><p>We are lazy creatures who try to expend as little mental energy as possible.</p><p>And people are typically less motivated to scrutinize a message if the source is considered to be an expert. We interpret the message through the peripheral route.</p><p><span>This is one reason why media outlets often appoint experts who mirror their political values. These experts lend credibility to the views the outlet espouses. Interestingly, though, expertise appears to influence persuasion only if the individual is identified as an expert </span><em>before</em><span> they communicate their message. </span><a href="https://www.scholars.northwestern.edu/en/publications/the-persuasive-effects-of-delaying-identification-of-high-and-low?ref=quillette.com" rel="">Research</a><span> has found that when a person is told the source is an expert </span><em>after</em><span> listening to the message, this new information does not increase the person’s likelihood of believing the message.</span></p><p><span>It works the other way, too. If a person is told that a source is not an expert </span><em>before</em><span> the message, the person tends to be more skeptical of the message. If told the source is not an expert </span><em>after</em><span> the message, this has no effect on a person’s likelihood of believing the message.</span></p><p>This suggests that knowing a source is an expert reduces our motivation to engage in central processing. We let our guards down.</p><p>As motivation and/or ability to process arguments is decreased, peripheral cues become more important for persuasion. Which might not bode well.</p><p>However, when we update our beliefs by weighing the actual merits of an argument (central route), our updated beliefs tend to endure and are more robust against counterpersuasion, compared to when we update our beliefs through peripheral processing. If we come to believe something through careful and thoughtful consideration, that belief is more resilient to change.</p><p>This means we can be more easily manipulated through the peripheral route. If we are convinced of something via the peripheral route, a manipulator will be more successful at using the peripheral route once again to alter our initial belief.</p><p>But why does this matter? Because by understanding how and why we come to hold our beliefs, we can better understand ourselves and guard against manipulation.</p><p>The founders of the elaboration likelihood model wrote that, “Ultimately, we suspect that attitudes are seen as correct or proper to the extent that they are viewed as beneficial for the physical or psychological well-being of the person.”</p><p><span>In his book </span><em>The Social Leap</em><span>, the evolutionary psychologist William von Hippel writes, “a substantial reason we evolved such large brains is to navigate our social world… A great deal of the value that exists in the social world is created by consensus rather than discovered in an objective sense… our cognitive machinery evolved to be only partially constrained by objective reality.” Our social brains process information not only by examining the facts, but also considering the social consequences of what happens to our reputations if we believe something.</span></p><p>Indeed, in his influential theory of social comparison processes, the eminent psychologist Leon Festinger suggested that people evaluate the “correctness” of their opinions by comparing them to the opinions of others. When we see others hold the same beliefs as us, our own confidence in those beliefs increases. Which is one reason why people are more likely to proselytize beliefs that cannot be verified through empirical means.</p><p>In short, people have a mechanism in their minds. It stops them from saying something that could lower their status, even if it’s true. And it propels them to say something that could increase their status, even if it’s false. Sometimes, local norms can push against this tendency. Certain communities (e.g., scientists) can obtain status among their peers for expressing truths. But if the norm is relaxed, people might default to seeking status over truth if status confers the greater reward.</p><p>Furthermore, knowing that we could lose status if we don’t believe in something causes us to be more likely to believe in it to guard against that loss. Considerations of what happens to our own reputation guides our beliefs, leading us to adopt a popular view to preserve or enhance our social positions. We implicitly ask ourselves, “What are the social consequences of holding (or not holding) this belief?”</p><p><span>But our reputation isn’t the only thing that matters when considering what to believe. Equally important is the reputation of others. Returning to the peripheral route of persuasion, we decide whether to believe something not only if </span><em>lots</em><span> of people believe it, but also if the proponent of the belief is a prestigious person. If lots of people believe something, our likelihood of believing it increases. And if a high-status person believes something, we are more prone to believing it, too.</span></p><p><span>This starts when we are children. In her recent book </span><em>Cognitive Gadgets</em><span>, the Oxford psychologist Cecilia Hayes writes, “children show prestige bias; they are more likely to copy a model that adults regard as being higher social status- for example, their head-teacher rather than an equally familiar person of the same age and gender.” Hayes cites a 2013 </span><a href="https://www.sciencedirect.com/science/article/pii/S0022096513000982?casa_token=ZQfNgYFnvxsAAAAA%3AqgqHTEcD5VbklYgiS-byzHdKJ9MF7FvafyL_kysopRyVRQz85WlcU81A4CbpXf1wqV8Dcjvhkg&amp;ref=quillette.com#!" rel="">study</a><span> by Nicola McGuigan who found that five-year-old children are “selective copiers.” Results showed that kids were more likely to imitate their head-teacher rather than an equally familiar person of the same age and gender. Young children are more likely to imitate a person that adults regard as being higher status.</span></p><p>People in general favor mimicking prestigious people compared to ordinary people. This is why elites have an outsized effect on culture, and why it is important to scrutinize their ideas and opinions. As a descriptive observation, the opinions of my friend who works at McDonald’s have less effect on society than the opinions of my friend who works at McKinsey. If you have any kind of prominence, you unavoidably become a model that others, including children, are more likely to emulate.</p><p><span>Indeed, the Canadian anthropologist Jerome Barkow </span><a href="https://link.springer.com/chapter/10.1007/978-1-4939-0867-7_2?ref=quillette.com" rel="">posits</a><span> that people across the world view media figures as more prestigious than respected members of their local communities. People on screen appear to be attractive, wealthy, popular, and powerful. Barkow writes, “All over the world, children are learning not from members of their own community but from media figures whom they perceive as prestigious… local prestige is debased.” As this phenomenon continues to grow, the opinions and actions of the globally-prestigious carry even more influence.</span></p><p>Of course, people don’t copy others with high-status solely because they hope that mimicking them will boost their own status. We tend to believe that prestigious people are more competent; prominence is a heuristic for skill.</p><p><span>In a recent </span><a href="https://www.nature.com/articles/s41599-019-0228-7?ref=quillette.com" rel="">paper</a><span> about prestige-based social learning, researchers Ángel V. Jiménez and Alex Mesoudi wrote that assessing competence directly “may be noisy and costly. Instead, social learners can use short-cuts either by making inferences from the appearance, personality, material possessions, etc. of the models.”</span></p><p>For instance, a military friend of mine used to be a tutor for rich high school students. He himself is not as wealthy as them, and disclosed to me that he paid $200 to replace his old earphones for AirPods. This was so that the kids and their families would believe he is in the same social position as them, and therefore qualified to teach.</p><p>Which brings us to a question: Who is most susceptible to manipulation via peripheral persuasion? It might seem intuitive to believe that people with less education are more manipulable. But research suggests this may not be true.</p><p>High-status people are more preoccupied with how others view them. Which means that educated and/or affluent people may be especially prone to peripheral, as opposed to central, methods of persuasion.</p><p><span>Indeed, the psychology professor Keith Stanovich, discussing his research on “myside bias,” has </span><a href="https://quillette.com/2020/09/26/the-bias-that-divides-us/" rel="">written</a><span>, “if you are a person of high intelligence… you will be less likely than the average person to realize you have derived your beliefs from the social groups you belong to and because they fit with your temperament and your innate psychological propensities.”</span></p><p>Students and graduates of top universities are more prone to myside bias. They are more likely to “evaluate evidence, generate evidence, and test hypotheses in a manner biased toward their own prior beliefs, opinions, and attitudes.”</p><p><span>This is not unique to our own time. William Shirer, the American journalist and author of </span><em>The Rise and Fall of the Third Reich</em><span>, described his experiences as a war correspondent in Nazi Germany. Shirer wrote, “Often in a German home or office or sometimes in a casual conversation with a stranger in a restaurant, beer hall, or café, I would meet with outlandish assertions from seemingly educated and intelligent persons. It was obvious they were parroting nonsense they heard on the radio or read in the newspapers. Sometimes one was tempted to say as much, but one was met with such incredulity, as if one had blasphemed the Almighty.”</span></p><p><span>Likewise, in a fascinating </span><a href="https://www.jstor.org/stable/40870502?seq=1&amp;ref=quillette.com#metadata_info_tab_contents" rel="">study</a><span> on the collapse of the Soviet Union, researchers have found that university-educated people were two to three times more likely than high school graduates to say they supported the Communist Party. White-collar professional workers were likewise two to three times more supportive of communist ideology, relative to farm laborers and semi-skilled workers.</span></p><p><span>Educational divides within the US today are consistent with these historical patterns. The Democratic political analyst David Shor has </span><a href="https://nymag.com/intelligencer/2021/03/david-shor-2020-democrats-autopsy-hispanic-vote-midterms-trump-gop.html?ref=quillette.com" rel="">observed that</a><span>, “Highly educated people tend to have more ideologically coherent and extreme views than working-class ones. We see this in issue polling and ideological self-identification. College-educated voters are way less likely to identify as moderate.”</span></p><p><span>One possibility for this is that regardless of time or place, affluent members of society are more likely to say the right things to either preserve status or gain more of it. A series of studies by researchers at the University of Queensland </span><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/ejsp.2620?ref=quillette.com" rel="">found</a><span> that, “relative to lower-class individuals, upper-class individuals have a greater desire for wealth and status… it is those who have more to start with (i.e., upper-class individuals) who also strive to acquire more wealth and status.”</span></p><p><span>A more recent set of </span><a href="https://journals.sagepub.com/doi/abs/10.1177/0146167220937544?ref=quillette.com" rel="">studies</a><span> led by Cameron Anderson at the University of Berkeley found that social class, measured in terms of education and income, was positively associated with the desire for social status. People who had more education and money were more likely to agree with statements like “I enjoy having influence over other people’s decision making” and “It would please me to have a position of prestige and social standing.”</span></p><p><span>Who feels most in danger of losing their reputations, though? Turns out, those same exact people. A </span><a href="https://www.cato.org/sites/cato.org/files/2020-07/Crosstabs_Political%20Climate_0.pdf?ref=quillette.com" rel="">survey</a><span> by the Cato Institute in collaboration with YouGov asked a nationally representative sample of 2,000 Americans various questions about self-censorship.</span></p><p>They found that highly educated people are the most concerned about losing their jobs or missing out on job opportunities because of their political views. Twenty-five percent of those with a high school education or less are afraid of getting fired or hurting their employment prospects because of their political views, compared with 34 percent of college graduates and an astounding 44 percent of people with a postgraduate degree.</p><p><span>Results from a recent </span><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3647099&amp;ref=quillette.com" rel="">paper</a><span> titled ‘Keeping Your Mouth Shut: Spiraling Self-Censorship in the United States’ by the political scientists James L. Gibson and Joseph L. Sutherland is consistent with the findings from Cato/Yougov. They find that self-censorship has skyrocketed. In the 1950s, at the height of McCarthyism, 13.4 percent of Americans reported that “felt less free to speak their mind than they used to.” In 1987, the figure had reached 20 percent. By 2019, 40 percent of Americans reported that they did not feel free to speak their minds. This isn’t a partisan issue, either. Gibson and Sutherland report that, “The percentage of Democrats who are worried about speaking their mind is just about identical to the percentage of Republicans who self-censor: 39 and 40 percent, respectively.”</span></p><p>The increase is especially pronounced among the educated class. The researchers report, “It is also noteworthy and perhaps unexpected that those who engage in self-censorship are not those with limited political resources… self-censorship is most common among those with the highest levels of education… This finding suggests a social learning process, with those with more education being more cognizant of social norms that discourage the expression of one’s views.”</p><p>Highly-educated people appear to be the most likely to express things they don’t necessarily believe for fear of losing their jobs or their reputation. Within the upper class, the true believers set the pace, and those who are loss-averse about their social positions go along with it.</p><p><span>Interestingly, there is suggestive </span><a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0028756&amp;ref=quillette.com" rel="">evidence</a><span> indicating that education is </span><em>negatively </em><span>associated with one’s sense of power. That is, the more education someone has, the more likely they are to agree with statements like, “Even if I voice them, my views have little sway” and “My ideas and opinions are often ignored.” Granted, the correlation is quite small (r = -.15). Still, the finding is significant and in the opposite direction of what most people would expect.</span></p><p><span>Research by Caitlin Drummond and Baruch Fischhoff at Carnegie Mellon University </span><a href="https://www.pnas.org/content/114/36/9587?ref=quillette.com" rel="">found</a><span> that people with more education, science education, and science literacy are more polarized in their views about scientific issues depending on their political identity. For example, the people who are most concerned about climate change? College-educated Democrats. The people who are least concerned? College-educated Republicans. In contrast, less educated Democrats and Republicans are not so different from one another in their views about climate change.</span></p><p><span>Likewise, in an article titled “Academic and Political Elitism,” the sociologist Musa Al-Gharbi has summarized related research, </span><a href="https://www.insidehighered.com/views/2019/08/27/academe-should-avoid-politicizing-educational-attainment-opinion?ref=quillette.com" rel="">writing</a><span>, “compared to the general public, cognitively sophisticated voters are much more likely to form their positions on issues based on partisan cues of what they are ‘supposed’ to think in virtue of their identity as Democrats, Republicans, etc.”</span></p><p>It’s also useful to understand how highly educated people view others and their social relationships. Consider a paper titled ‘Seeing the Best or Worst in Others: A Measure of Generalized Other-Perceptions’ led by Richard Rau at the University of Münster. Rau and his colleagues were interested in how various factors influence people’s perceptions of others.</p><p>In the study, participants looked at social network profiles of people they did not know. They also viewed short video sequences of unfamiliar people describing a neutral personal experience like traveling to work. Researchers then asked participants to evaluate the people in the social media profiles and videos. Participants were asked how much they agreed with statements like “I like this person,” and “This person is cold-hearted.” Then participants responded to various demographic and personality questions about themselves.</p><p>Some findings weren’t so surprising. The researchers found, for example, that people who scored highly on the personality traits of openness and agreeableness tended to hold more favorable views of others.</p><p><span>More sobering, though, is that higher education was consistently related to </span><em>less positive</em><span> views of other people. In their paper they write, “to understand people’s feelings, behaviors, and social relationships, it is of key importance to know which general view they hold about others… the better people are educated, the less positive their other-perceptions are.”</span></p><p>So affluent people care the most about status, believe they have little power, are afraid of losing their jobs and reputation, and have less favorable views of others.</p><p>In short, opinions can confer status regardless of their truth value. And the individuals most likely to express certain opinions in order to preserve or enhance their status are also those who are already on the upper rungs of the social ladder.</p><p>There may be unpleasant consequences for this misguided use of intellect and time on the part of highly educated and affluent people. If the most fortunate members of society spend more time speaking in hushed tones, or live in fear of expressing themselves, or are more involved in culture wars, that is less time they could spend using their mental and economic resources to solve serious problems.</p><p>Smart people are usually better at finding the truth. But they’re also better at knowing which way the ideological winds are blowing, and thereby producing and accepting absurdities. </p><p><span>A version of this essay was </span><a href="https://quillette.com/2021/04/03/persuasion-and-the-prestige-paradox-are-high-status-people-more-likely-to-lie/" rel="">originally</a><span> published in </span><em>Quillette. </em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zero-k: A libre sci-fi RTS game, with an economy based on metal and energy (263 pts)]]></title>
            <link>https://zero-k.info/</link>
            <guid>38331349</guid>
            <pubDate>Sun, 19 Nov 2023 10:43:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zero-k.info/">https://zero-k.info/</a>, See on <a href="https://news.ycombinator.com/item?id=38331349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="features">
            <div>
                    
                <br>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/pHQkctGTm_A?rel=0" frameborder="0" hd="1" allowfullscreen=""></iframe>
                
                <div>
                    <h3>Commander wanted!</h3> <p>Drive giant robots, build an army of a thousand Fleas, move mountains if needed, bury the enemy at all cost!</p>
                    <ul><li>Real time strategy.</li><li>Physically simulated units and projectiles.</li><li>Terrain manipulation.</li><li>100+ varied units with unique abilities.</li><li>Singleplayer campaign.</li><li>Challenging, non-cheating AI.</li><li>Multiplayer 1v1 - 16v16, FFA, coop.</li><li>Multiplayer online campaign.</li><li>Really free, no in-game currency, no unfair multiplayer.</li></ul>
                    
                    
                    <h3>Fully Utilized Physics</h3>
                    <p>Simulated unit and projectile physics is used to a level rarely found in a strategy game.</p>
                    <p><img src="https://zero-k.info/img/landing/text1.jpg"><br></p>
                    <ul><li>Use small nimble units to dodge slow moving projectiles.</li><li>Hide behind hills that block weapon fire, line of sight and radar.</li><li>Toss units across the map with gravity guns.</li><li>Transport a battleship to a hilltop - for greater views and gun range.</li></ul>
                    
                    
                    <h3>Manipulate the Terrain</h3>
                    <p>The terrain itself is an ever-changing part of the battlefield.</p>
                    <h3><img src="https://zero-k.info/img/landing/text2.jpg"></h3>
                    <ul><li>Wreck the battlefield with craters that bog down enemy tanks.</li><li>Dig canals to bring your navy inland for a submarine-in-a-desert strike.</li><li>Build ramps, bridges, entire fortress if you wish.</li><li>Burn your portrait into continental crust using the planetary energy chisel.</li></ul>
                    
                    
                    <h3>Singleplayer Campaign and Challenging AI</h3>
                    <p>Enjoy many hours of single player and coop fun with our campaign, wide selection of non-cheating AIs and a survival mode against an alien horde.<br></p>
                    <p><img src="https://zero-k.info/img/landing/text3.jpg"></p><ul><li>Explore the galaxy and discover technologies in our singleplayer campaign.</li><li>Face a challenging AI that is neither brain-dead nor a clairvoyant cheater.</li><li>Have some coop fun with friends, surviving waves of chicken-monsters.</li><li>Cloaking? Resurrection? Tough choices customizing your commander.</li></ul>
                    
                    
                    <h3>Casual and Competitive Multiplayer</h3>
                    <p>Zero-K was built for multiplayer from the start, this is where you can end up being hooked for a decade.</p>
                    <h3><img src="https://zero-k.info/img/landing/text4.jpg"></h3>
                    <ul><li>Enjoying epic scale combat? Join our 16v16 team battles!</li><li>Looking for a common goal? Fight AIs or waves of chicken-monsters.</li><li>Prefer dancing on a razor's edge? Play 1v1 in ladder and tournaments.</li><li>Comebacks, betrayals, emotions always running high in FFA.</li><li>Want to fight for a bigger cause? Form a clan and join <strong>PlanetWars</strong>, competitive online campaign with web-game strategic elements, diplomacy and backstabbing.</li></ul>
                    
                    
                    <h3>Power to the People</h3>
                    <p>We are RTS players at heart, we work for nobody. We gave ourselves the tools we always wanted to have in a game.</p>
                    <p><img src="https://zero-k.info/img/landing/text5.jpg"><br></p>
                    <ul><li>Do what you want. No limits to camera, queue or level of control.</li><li>Paint a shape, any shape, and units will move to assume your formation.</li><li>Want to spend more time on tactics? Use construction priorities.</li><li>Want to focus on economy? Order units to kite, strafe or zig zag bullets.</li></ul>
                    
                    
                    <h3>Plenty of Stuff to <del>Explode</del> Explore</h3>
                    <p>Zero-K is a long term project and it shows, millions hours of proper multiplayer testing and dozens of people contributing ever expanding content.</p>
                    <p><img src="https://zero-k.info/img/landing/text6.jpg"><br></p>
                    <ul><li>Learn to use all of our 100+ units and play on hundreds of maps.</li><li>Invent the next mad team-tactics to shock enemies and make allies laugh.</li><li>Combine cloaking, teleports, shields, jumpjets, EMP, napalm, gravity guns, black hole launchers, mind control and self-replication.</li><li>Tiny flea swarm that clings to walls? <br>Jumping "cans" with steam-spike? <br>Buoys that hide under water to ambush ships? <br>Mechs that spew fire and enjoy being tossed from air transports? <br>Carrier with cute helicopters? <br>Jumping Jugglenaut with dual wielding gravity guns? <br>Meet them in Zero-K!<span></span></li></ul>
                </div><br>
                    
                
            </div><br>
    </div><div id="news">
            <h2>Zero-K v1.11.11.1 - Odin's Magpies</h2>
                <p><a href="https://zero-k.info/img/news/3350.png"><img src="https://zero-k.info/img/news/3350_thumb.png"></a></p><div><p>This update introduces two new bombers to fill out the aircraft roster. One is a bit like a light Likho, and another is like a heavy Raven, but there is a lot more to each. Other planes have tweaks and buffs, such as an area reveal ability for Sparrow, and a heavier Phoenix. Some gunships even got in on the action, with the most notable being a sizeable buff for Krow.</p><p>Other balance changes include a bit more health for light assaults, and better spinning for Disco Rave Party. In terms of features, bombers are well-served here as well, with selection icons for whether a bomber needs ammo, as well as smoother landing paths.</p><p>There are a few extra features for modding, such as scaling and tinting, which were technically in the previous hotfix. The campaign has the new bombers, a few features, and a new Records page which displays and aggregates your best stats for each mission. How fast can the campaign be completed, and with how few units lost?</p><h2><a name="New_Bombers"></a>New Bombers</h2><p><strong>Magpie</strong> is a light nimble bomber armed with a pair of missiles. It deals less damage than Raven, but makes up for it with speed, precision, and the ability to shoot from the edge of AA coverage.</p><ul>
<li>
Cost 220</li>
<li>
Health 900</li>
<li>
Speed 252 (between Raven and Likho)</li>
<li>
Rearm time 20s</li>
<li>
Range 550</li>
<li>
Damage 180 x 2</li>
<li>
Can hit small raiders most of the time, and most aircraft.</li>
</ul>
<p><strong>Odin</strong> is a heavily armoured rocket zeppelin that can fire a slow moving disintegrator bomb or a cluster of temporary shields.</p><ul>
<li>
Cost 1500</li>
<li>
Health 5200</li>
<li>
Speed 185 (between Raptor and most gunships)</li>
<li>
Rearm time 35s</li>
<li>
Bomb range 200</li>
<li>
Bomb damage, it's a disintegrator (5-shots Detriment)</li>
<li>
The bomb is very slow, even Detriment can dodge it.</li>
<li>
Special weapon fires seven shields at 550 range.</li>
<li>
Each shield has 3400 charge and a radius of 200.</li>
<li>
Shields decay at 40 charge/s, expire at zero charge, and do not link.</li>
</ul>
<h2><a name="Aircraft_Changes"></a>Aircraft Changes</h2><p><strong>Sparrow</strong> can now boost, and reveals and decloaks everything in an area when it dies.</p><ul>
<li>
Boosts by 5x for 3 seconds.</li>
<li>
Destroys itself after boost.</li>
<li>
Reveals an area of radius 400 for 12 seconds when it dies.</li>
<li>
Revealed area scales up during boost, to a maximum of 640.</li>
</ul>
<p><strong>Phoenix</strong> is heavier and deals more damage over a longer run.</p><ul>
<li>
Cost 360 -&gt; 460</li>
<li>
Health 1060 -&gt; 1450 (90 more than if just scaling by cost)</li>
<li>
Reduced turn rate slightly</li>
<li>
Rearm time 5s -&gt; 8s</li>
<li>
Drops 15 -&gt; 18 bombs over 0.93 -&gt; 1.7 seconds</li>
<li>
Area of Effect 216 -&gt; 320</li>
<li>
Damage per bomb 25 -&gt; 40</li>
<li>
Burn time 10s -&gt; 12s</li>
</ul>
<p><strong>Krow</strong> manoeuvrers a bit better and fires beam lasers with more damage.</p><ul>
<li>
Improved brake rate by 25%.</li>
<li>
Pew pew lasers replaced with burst beam lasers.</li>
<li>
Damage increased by 22%</li>
</ul>
<p><strong>Raptor</strong> is more manoeuvrable and slightly more deadly.</p><ul>
<li>
Increased turn rate while firing, to counteract the effect of slowing down.</li>
<li>
Firing cone angle 90 degrees -&gt; 100 degrees</li>
<li>
Damage increased by 4.2%</li>
</ul>
<p><strong>Locust</strong> is a tiny bit better as raiding, as a great Locust is scary.</p><ul>
<li>
Speed 207 -&gt; 212</li>
<li>
DPS increased by 2.5%</li>
</ul>
<p><strong>Thunderbird</strong> is slightly healthier.</p><ul>
<li>
Health 1120 -&gt; 1200</li>
</ul>
<h2><a name="Balance_Changes"></a>Balance Changes</h2><p><strong>Bolas</strong> has 4.5% more DPS and can 1-shot Flea.</p><ul>
<li>
Reload 0.366 -&gt; 0.433</li>
<li>
Damage 34 -&gt; 42</li>
</ul>
<p><strong>Ravager</strong> is tankier.</p><ul>
<li>
Health 2000 -&gt; 2200</li>
</ul>
<p><strong>Knight</strong> is a bit tankier.</p><ul>
<li>
Health 2400 -&gt; 2500</li>
</ul>
<p><strong>Hermit</strong> is tankier <em>and</em> faster.</p><ul>
<li>
Health 1500 -&gt; 1550</li>
<li>
Speed 51 -&gt; 54</li>
</ul>
<p><strong>Skuttle</strong> can survive a single Jack poke.</p><ul>
<li>
Health 250 -&gt; 380</li>
</ul>
<p><strong>Zephyr</strong> is also tankier, and now has missiles.</p><ul>
<li>
Health 1900 -&gt; 2200</li>
<li>
Replaced its front laser turret with missiles.</li>
<li>
Missile damage 72 x 2</li>
<li>
Reload time 1.6 seconds</li>
<li>
Range 1000 (same as lasers)</li>
<li>
The missiles have 30% less DPS, but fire in a burst.</li>
</ul>
<p><strong>Disco Rave Party</strong> has an interpolation of its May spin rate nerfs.</p><ul>
<li>
Aim speed 4 -&gt; 2.5 -&gt; 3 degrees/s.</li>
<li>
Spin up time 60 -&gt; 120 -&gt; 90 seconds.</li>
<li>
Spin drop while turning is unchanged, but less spin is lost for any given turn due to the increased turn rate.</li>
</ul>
<h2><a name="Campaign"></a>Campaign</h2><ul>
<li>
Magpie is unlocked on Fel Diacia (the Thunderbird mission) and Odin is unlocked on Bavhakya (the Likho mission). Anyone with these missions complete will have their units unlocked.</li>
<li>
Added a Records tab to the Profile menu on the campaign screen. This screen can sort and display your victories with least units lost, in the shortest time, with bonus objectives and difficulties. Unfortunately the data is not retroactive.</li>
<li>
Removed a few Lucifers from easier difficulties on Onsally (the Phoenix mission).</li>
<li>
Worked around the Dominatrix build options bug for the Rover Assembly on Ganong (the Dominatrix mission).</li>
<li>
Clarified Lalata main objective text (the Jugglenaut mission).</li>
</ul>
<h2><a name="Features"></a>Features</h2><ul>
<li>
Added Units Lost to endgame graphs.</li>
<li>
Unit pictures for bombers now have an icon showing whether it is ready to fire.</li>
<li>
Aircraft now smoothly glide and stop when landing on airpads. This is technically a nerf as they take slightly longer to land.</li>
<li>
Improved shotgun visuals, with minor balance implications.</li>
<li>
Grizzly now aims smoothly and holds its gun steady as it fires.</li>
<li>
Tweaked the automatic handicap mode to give lower handicaps for ratings above 2000.</li>
<li>
Moved Raptor to the AA slot (D) of the Airplane Plant and Sparrow to secondary scout/raider (S).</li>
<li>
Taught the opponent AI about Magpie.</li>
</ul>
<h2><a name="Modding"></a>Modding</h2><ul>
<li>
Units can now be tinted, rescaled and made glowing via the customparams model_tint ("R G B"), model_glow ("R G B A"), and model_rescale. Colour values are from 0 to 1.</li>
<li>
Free units (0 metal and energy cost) no longer need to explicitly specify a non-zero buildtime (previous release).</li>
<li>
Added a function to effects/napalm for generating generic fireballs ala Inferno.</li>
<li>
Air pads now call ReammoComplete, if it is a units script, when bombers are rearmed.</li>
<li>
Added command AIR_MANUALFIRE for aircraft, since they cannot use the in-engine type.</li>
</ul>
<h2><a name="Fixes"></a>Fixes</h2><ul>
<li>
Fixed Tremor wheels not spinning.</li>
<li>
Gravity guns push and pull a bit more consistently.</li>
<li>
Jack can no longer pretend to be a missile in enemy Missile Silos.</li>
<li>
Slow damage makes shields charge slower again (it broke at some point)</li>
<li>
Fixed bombers sometimes seeming to ignore a move command.</li>
<li>
Attack Move on bombers is now is now removed after they shoot, unless the bomber is set to repeat. Previously this only worked for some bombers.</li>
<li>
Bombers are now much better at landing on Reef. Much better. Horrific things could happen before.</li>
<li>
Fix sun on Mercurial and Rogues River.</li>
</ul>
<br></div>
            <p><small>
                Posted by <img src="https://zero-k.info/img/flags/AU.png" height="11" width="16" alt="AU"><img src="https://zero-k.info/img/ranks/7_7.png" alt="rank"><a href="https://zero-k.info/Clans/Detail/876" nicetitle="$clan$876"><img src="https://zero-k.info/img/clans/RSN.png" width="16"></a><img src="https://zero-k.info/img/police.png" alt="Admin"><a href="https://zero-k.info/Users/Detail/15114" nicetitle="$user$15114">GoogleFrog</a> 30 hours ago - <a href="https://zero-k.info/Forum/Thread/37015">comment</a>
            </small></p><h2>Zero-K v1.11.10.0</h2>
            <div><p>This is a hotfix release for an engine bug that could be used to place geos anywhere.</p><h2><a name="Modding"></a>Modding</h2><br> <ul>
<li>
Units can now be tinted, rescaled and made glowing via customparams:</li>
</ul>
<pre>model_tint = "1.0  0.5  0.0" -- R G B
model_glow = "0.1  0.2  0.3  1.0" -- R G B A
model_rescale = 1.23,
</pre><br> <ul>
<li>
Free units (0 metal and energy cost) no longer need to explicitly specify a non-zero buildtime.</li>
<li>
Support some recently added engine interfaces, see `engine_compat.lua`. Note that ZK hasn't yet migrated.</li>
</ul>
<br><h2><a name="Fixes"></a>Fixes</h2><br> <ul>
<li>
Fix an exploit to do with geo placement.</li>
<li>
Fix the "Enable Force Fire Command" default state for Lobster.</li>
<li>
Fix Dominatrix breaking when capturing very low completion nanoframes of very expensive units.</li>
<li>
Fix awards for sharing and capture to consider nanoframes at their real value.</li>
<li>
Fix a chat crash if using the "color labels the player colour" option.</li>
<li>
Fix Onyx Cauldron using incorrect boxes in coop comp stomps.</li>
<li>
Fix cloaked constructors revealing themselves by reclaiming if using the automated constructors widget.</li>
<li>
Land units now eventually give up when ordered to move deep into an inaccessible place (such as a large sea).</li>
</ul>
</div>
            <p><small>
                Posted by <img src="https://zero-k.info/img/flags/AU.png" height="11" width="16" alt="AU"><img src="https://zero-k.info/img/ranks/7_7.png" alt="rank"><a href="https://zero-k.info/Clans/Detail/876" nicetitle="$clan$876"><img src="https://zero-k.info/img/clans/RSN.png" width="16"></a><img src="https://zero-k.info/img/police.png" alt="Admin"><a href="https://zero-k.info/Users/Detail/15114" nicetitle="$user$15114">GoogleFrog</a> 33 days ago - <a href="https://zero-k.info/Forum/Thread/36867">comment</a>
            </small></p><h2>Zero-K v1.11.8.1 - Lobster Brained</h2>
                <p><a href="https://zero-k.info/img/news/3348.png"><img src="https://zero-k.info/img/news/3348_thumb.png"></a></p><div><p>Lobsters have been taught to not fire when it would be redundant, making Lobster balls much easier to manage. Overkill prevention in general has a tweak, and some APIs were extended to make modding easier. The Artefact Control game mode which saw some testing about a month ago is now live. In terms of balance, Redback has a bit of a nerf and some of the least used units - Skuttle, Phoenix and Emissary - have small buffs.</p><h2><a name="Balance"></a>Balance</h2><p><strong>Duck </strong>can now be dodged by a Glaive trying as hard as it can to run away.</p><ul>
<li>
Missile fuel time 2s -&gt; 1.5s</li>
</ul>
<p><strong>Redback </strong>is worse at taking map control and assaulting defenses.</p><ul>
<li>
Cost 230 -&gt; 240</li>
<li>
Speed 1.85 -&gt; 1.75</li>
</ul>
<p><strong>Skuttle </strong>can see enemies before they break its cloak.</p><ul>
<li>
Line of sight 280 -&gt; 330</li>
</ul>
<p><strong>Emissary</strong> aims faster and no longer benefits from manually turning.</p><ul>
<li>
Body turn rate reduced 105 -&gt; 84 degrees/second</li>
<li>
Gun aim rate increased 40 -&gt; 70 degrees/second</li>
<li>
Resets its gun between shots (as much as possible) in case it has to move.</li>
</ul>
<p><strong>Phoenix </strong>moves faster and hits its target sooner.</p><ul>
<li>
Speed 8 -&gt; 8.1</li>
<li>
Projectile gravity 0.7 -&gt; 0.72</li>
</ul>
<br><h2><a name="Unit_AI"></a>Unit AI</h2><p>Overkill prevention, the system that prevents ten Scalpels firing at a single Glaive, is now available for Lobster and disabled for units set to hold. It is controlled by a state toggle that is hidden by default because there is very little reason to touch it. The state toggle can be enabled under Settings/Interface/Commands, and per-unit defaults to be set in Settings/Unit Behaviour/Default States, or by holding Space and clicking on a unit, then pressing Edit Behaviour.</p><p><strong>Lobster </strong>is smarter, no longer firing unless there is a visible valid target.</p><ul>
<li>
This prevents it from firing when it would do absolutely nothing, provided there are no invisible enemies nearby.</li>
<li>
As a result, telling a tight clump of Lobsters to fire causes only two to shoot.</li>
<li>
This behaviour can be configured with the hidden Overkill Prevention state.</li>
<li>
Removed the Force Fire command by default. This can be configured via another hidden state toggle.</li>
</ul>
<p><strong>Overkill prevention</strong> is now suspended by default when units are set to Hold Fire.</p><ul>
<li>
Added the "Enable for Fire at Will" and "Enable for auto targeting" states. Previously it just had enabled/disabled.</li>
<li>
Most units default to "Enable for Fire at Will". The theory being that if you set a unit to hold fire you care more about its target dying than about overkill.</li>
<li>
Nothing defaults to "Enable for auto targeting", but it may be useful to try out.</li>
</ul>
<p><strong>Impaler</strong> now has an even stronger preference for targeting structures over units.</p><h2><a name="Interface"></a>Interface</h2><br> <ul>
<li>
Added a minimum wind icon and number to the left column of the wind generator tooltip during placement (thanks Porkchop)</li>
<li>
Cleaned up some inconsistent unit highlighting and selection. Units under interface panels cannot be clicked on by default, tooltips do not appear, and units are not highlighted.</li>
<li>
Selecting through the panel at the bottom of the screen can be configured under Settings/HUD Panels/Selected Units Panel.</li>
<li>
Drag selection can still terminate over a UI panel.</li>
<li>
Add some translations for the commander selector.</li>
<li>
Updated Global Build AI with some fixes and documentation (thanks esainane).</li>
<li>
Added username team colour in chat and removed white outlines for dark names (thanks Birdulon)</li>
<li>
Local widgets are no longer disabled for spectators on rooms with local widgets disabled.</li>
<li>
Added Italian translations for the main menu (thanks fvasco)</li>
</ul>
<br><h2><a name="Campaign"></a>Campaign</h2><br> <ul>
<li>
Tweaked campaign text and replaced some Artefacts with more suitable structures (thanks Thorneel)</li>
</ul>
<br><h2><a name="Maps"></a>Maps</h2><br> <ul>
<li>
Added boxes for Onyx Cauldron 2.0</li>
<li>
Improved lighting on Skulduggery (thanks Shaman)</li>
<li>
Improved water on Cull, Lost v2 and Lowland Crossing Revised v2.</li>
<li>
Fixed an issue with void water with new shaders on some graphics cards.</li>
</ul>
<br><h2><a name="Artefact_Control"></a>Artefact Control</h2><p>Added a control point game mode called Artefact Control under Experimental.</p><ul>
<li>
Several artefacts are spawned on each side of the map.</li>
<li>
Each team controls half at the start of the game.</li>
<li>
Artefacts have 11k health and heal at 100 hp/second.</li>
<li>
Artefacts respawned with switched allegeance when destroyed.</li>
<li>
If a team controlls all the artefacts, they win.</li>
</ul>
<br><h2><a name="Modding"></a>Modding</h2><p>Changed some keys in unit def files to match the lua UnitDefs table. Mods with the old keys are still supported.</p><ul>
<li>
buildCostMetal -&gt; metalCost</li>
<li>
buildCostEnergy -&gt; energyCost</li>
<li>
energyUse -&gt; energyUpkeep</li>
<li>
metalUse -&gt; metalUpkeep (vanilla ZK doesn't use this)</li>
<li>
maxDamage -&gt; health</li>
<li>
maxVelocity -&gt; speed</li>
</ul>
<p>Other changes.</p><ul>
<li>
Unit defs no longer require unitname since it must match the table key, so it is redundant.</li>
<li>
Unit defs are now also read from subfolders of 'units'.</li>
<li>
Set undefined burst rates to 0 (rather than 0.1) as it can interfere with weapon modding.</li>
<li>
The game exits to menu when unit defs fail to load, rather than crashing.</li>
<li>
Added game-side GG.UnitModelRescale(unitID, scale) from Unit Level Ups.</li>
<li>
Added backwards compatibility for Spring.GetUnitIsBeingBuilt.</li>
<li>
Fixed backwards compatibility for Spring.GetPlayerRulesParams.</li>
<li>
Added customparams.buggeroff_angle for factories, in radians.</li>
<li>
Added customparams.metal_extractor_mult to support the creation of higher tier metal extractors.</li>
<li>
Fixed modding away the Sniper reload move penalty breaking the script.</li>
<li>
Cleaned up the build icon generator gadget.</li>
<li>
Napalm effects now contain an example of sin (replaces taylor series).</li>
<li>
Improved modded energy generator tooltips.</li>
<li>
Mexes suport morphing.</li>
</ul>
<br><h2><a name="Fixes"></a>Fixes</h2><br> <ul>
<li>
Fixed commshare sometimes preventing resign or causing a crash when the game ends.</li>
<li>
War music no longer counts morph as violence.</li>
<li>
Fixed missing vote resign button.</li>
<li>
Marginal turret overshoot can no longer be circumvented with command insert.</li>
<li>
Fixed a few build icons incorrectly implying that a floating structure is built underwater.</li>
<li>
Fixed commander selector button image.</li>
<li>
Fixed backwards 'motion blur' on ejected shells.</li>
<li>
Fixed some large structure wreckages having much too large collision volumes.</li>
</ul>
</div>
            <p><small>
                Posted by <img src="https://zero-k.info/img/flags/AU.png" height="11" width="16" alt="AU"><img src="https://zero-k.info/img/ranks/7_7.png" alt="rank"><a href="https://zero-k.info/Clans/Detail/876" nicetitle="$clan$876"><img src="https://zero-k.info/img/clans/RSN.png" width="16"></a><img src="https://zero-k.info/img/police.png" alt="Admin"><a href="https://zero-k.info/Users/Detail/15114" nicetitle="$user$15114">GoogleFrog</a> 2 months ago - <a href="https://zero-k.info/Forum/Thread/36720">comment</a>
            </small></p><h2>Zero-K v1.11.6.5 - Sudden Death</h2>
            <div><p>This is mostly a small fixes update for the previous patch. It has a few balance chanegs and features though. Most notable are a slight Redback nerf and a sudden death mode. The sudden death mode might see use in the upcoming tournament to put a cap on very long games, but we are still waiting to see what people think of it. In any case, it seems like a useful mode to have.</p><h2><a name="Balance"></a>Balance</h2><p><strong>Redback</strong> is worse at dodging projectiles.</p><ul>
<li>
Increased collision shape width by 11% and length by 25%.</li>
<li>
Reduced turn rate by 5%.</li>
</ul>
<p><strong>Ogre</strong> now ignores terrain and wrecks when aiming and firing. It has sufficient AoE and arc for making the attempt to often be beneficial.</p><p><strong>Zeno</strong> now homes onto the actual position of a radar target earlier - early enough to hit it.</p><h2><a name="Features"></a>Features</h2><br> <ul>
<li>
Holding Alt while selecting units now filters out rank 3 (ie army units).</li>
<li>
Added a sudden death mode game option, under Map. It causes the game to end shortly after a specified time via a contracting death circle.</li>
</ul>
<br><h2><a name="Fixes"></a>Fixes</h2><br> <ul>
<li>
Fixed a missile impact indicator error.</li>
<li>
Fixed a pathfinding issue to do with construction orders on terrible terrain.</li>
<li>
Tactical missiles no longer have inconsistent half-prediction of enemy velocity. They now fire exactly where they are aimed.</li>
<li>
Fixed contrast adaptive sharpening scaling with zoom level.</li>
<li>
Fixed moderator tooltip colour in the lobby.</li>
<li>
Improved Pylon collision and selection volumes.</li>
</ul>
</div>
            <p><small>
                Posted by <img src="https://zero-k.info/img/flags/AU.png" height="11" width="16" alt="AU"><img src="https://zero-k.info/img/ranks/7_7.png" alt="rank"><a href="https://zero-k.info/Clans/Detail/876" nicetitle="$clan$876"><img src="https://zero-k.info/img/clans/RSN.png" width="16"></a><img src="https://zero-k.info/img/police.png" alt="Admin"><a href="https://zero-k.info/Users/Detail/15114" nicetitle="$user$15114">GoogleFrog</a> 4 months ago - <a href="https://zero-k.info/Forum/Thread/36547">comment</a>
            </small></p><p>
        <a href="https://zero-k.info/News"><img src="https://zero-k.info/img/rss.png" width="25" alt="rss"> RSS feed</a> | <a href="https://zero-k.info/Forum?categoryID=13">news archive</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep Learning Course (365 pts)]]></title>
            <link>https://fleuret.org/dlc/</link>
            <guid>38331200</guid>
            <pubDate>Sun, 19 Nov 2023 10:19:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fleuret.org/dlc/">https://fleuret.org/dlc/</a>, See on <a href="https://news.ycombinator.com/item?id=38331200">Hacker News</a></p>
<div id="readability-page-1" class="page">

<!-- ************************************************************ -->
<!-- ************************************************************ -->
<!-- ************************************************************ -->

<h2>Deep Learning Course</h2>

<p>You can find here <a href="#lectures">slides, recordings</a>,
and a <a href="#vm">virtual machine</a>
for <a href="https://fleuret.org/francois/">François
Fleuret</a>'s deep-learning
courses <a href="https://wwwi.unige.ch/cursus/programme-des-cours/web/teachings/details/2020-14X050">14x050</a>
of the <a href="https://www.unige.ch/">University of Geneva,</a>
<!-- and <a href="https://edu.epfl.ch/coursebook/en/deep-learning-EE-559">EE-559</a> -->
<!-- of the <a href="https://www.epfl.ch/index.en.html">École -->
<!-- Polytechnique Fédérale de Lausanne,</a> -->
Switzerland.</p>

<p>This course is a thorough introduction to deep-learning, with
examples in the <a href="https://pytorch.org/">PyTorch</a>
framework:</p>

<ul>
<li>machine learning objectives and main challenges,</li>
<li>tensor operations,</li>
<li>automatic differentiation, gradient descent,</li>
<li>deep-learning specific techniques,</li>
<li>generative, recurrent, attention models.</li>
</ul>

<p>You can check the <a href="#information">pre-requisites.</a></p>

<!-- ********************************************************************** -->

<p>This course was developped initialy at
the <a href="https://www.idiap.ch/">Idiap Research Institute</a>
in 2018, and taught as EE-559
at <a href="https://www.epfl.ch/index.en.html">École
Polytechnique Fédérale de Lausanne</a> until 2022. The notes for
the handouts were added with the help
of <a href="https://www.idiap.ch/~ocanevet/">Olivier
Canévet.</a></p>

<p>Thanks to Adam Paszke, Jean-Baptiste Cordonnier, Alexandre
Nanchen, Xavier Glorot, Andreas Steiner, Matus Telgarsky,
Diederik Kingma, Nikolaos Pappas, Soumith Chintala, and Shaojie
Bai for their answers or comments.</p>

<!-- ************************************************************ -->
<!-- ************************************************************ -->
<!-- ************************************************************ -->

<p>In addition to the materials available here, I also wrote and
distribute <a href="https://fleuret.org/francois/lbdl.html">"The Little Book of Deep Learning",</a> a
phone-formatted short introduction to deep learning for readers with a
STEM background.</p>

<h2><a id="lectures"></a>Lecture materials</h2>

<p>The slide pdfs are the ones I use for the lectures. They are in
landscape format with overlays to facilitate the presentation. The
handout pdfs are compiled without these fancy effects in portrait
orientation, with additional notes. The screencasts are available both
as in-browser streaming or downloadable mp4 files.</p>

<p>You can get archives with all the pdf files
(1097 slides):</p>

<ul>
<li><a href="https://fleuret.org/dlc/materials/dlc-handout-all.zip">dlc-handout-all.zip</a> (101.6Mb)</li>
<li><a href="https://fleuret.org/dlc/materials/dlc-slides-all.zip">dlc-slides-all.zip</a> (101.6Mb)</li>
</ul>

<p>and subtitles for the screencasts generated automaticallly
with <a href="https://github.com/openai/whisper">OpenAI's
Whisper</a>:</p>
<ul>
<li> <a href="https://fleuret.org/dlc/materials/dlc-video-subtitles.zip">dlc-video-subtitles.zip</a>
(502.1Kb)</li>
</ul>

<p>or the individual lectures:</p>

<ul>
  <li><a id="lecture-1"></a>1. Introduction. (90 slides, 1h57min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-1.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>1.1.</td><td>From neural networks to deep learning. (18 slides, 26min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-1-1-from-anns-to-deep-learning.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-1-1-from-anns-to-deep-learning.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-1-1-from-anns-to-deep-learning.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-1-1-from-anns-to-deep-learning.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>1.2.</td><td>Current applications and success. (25 slides, 29min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-1-2-current-success.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-1-2-current-success.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-1-2-current-success.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-1-2-current-success.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>1.3.</td><td>What is really happening? (10 slides, 11min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-1-3-what-is-happening.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-1-3-what-is-happening.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-1-3-what-is-happening.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-1-3-what-is-happening.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>1.4.</td><td>Tensor basics and linear regression. (13 slides, 21min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-1-4-tensors-and-linear-regression.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-1-4-tensors-and-linear-regression.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-1-4-tensors-and-linear-regression.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-1-4-tensors-and-linear-regression.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>1.5.</td><td>High dimension tensors. (20 slides, 25min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-1-5-high-dimension-tensors.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-1-5-high-dimension-tensors.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-1-5-high-dimension-tensors.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-1-5-high-dimension-tensors.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>1.6.</td><td>Tensor internals. (4 slides, 6min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-1-6-tensor-internals.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-1-6-tensor-internals.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-1-6-tensor-internals.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-1-6-tensor-internals.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-2"></a>2. Machine learning fundamentals. (72 slides, 1h44min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-2.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>2.1.</td><td>Loss and risk. (12 slides, 20min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-2-1-loss-and-risk.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-2-1-loss-and-risk.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-2-1-loss-and-risk.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-2-1-loss-and-risk.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>2.2.</td><td>Over and under fitting. (25 slides, 36min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-2-2-overfitting.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-2-2-overfitting.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-2-2-overfitting.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-2-2-overfitting.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>2.3.</td><td>Bias-variance dilemma. (10 slides, 18min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-2-3-bias-variance-dilemma.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-2-3-bias-variance-dilemma.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-2-3-bias-variance-dilemma.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-2-3-bias-variance-dilemma.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>2.4.</td><td>Proper evaluation protocols. (6 slides, 11min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-2-4-evaluation-protocols.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-2-4-evaluation-protocols.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-2-4-evaluation-protocols.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-2-4-evaluation-protocols.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>2.5.</td><td>Basic clusterings and embeddings. (19 slides, 19min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-2-5-basic-embeddings.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-2-5-basic-embeddings.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-2-5-basic-embeddings.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-2-5-basic-embeddings.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-3"></a>3. Multi-layer perceptron and back-propagation. (68 slides, 1h54min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-3.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>3.1.</td><td>The perceptron. (16 slides, 28min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-3-1-perceptron.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-3-1-perceptron.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-3-1-perceptron.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-3-1-perceptron.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>3.2.</td><td>Probabilistic view of a linear classifier. (8 slides, 14min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-3-2-LDA.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-3-2-LDA.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-3-2-LDA.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-3-2-LDA.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>3.3.</td><td>Linear separability and feature design. (10 slides, 17min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-3-3-features.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-3-3-features.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-3-3-features.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-3-3-features.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>3.4.</td><td>Multi-Layer Perceptrons. (10 slides, 11min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-3-4-MLP.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-3-4-MLP.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-3-4-MLP.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-3-4-MLP.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>3.5.</td><td>Gradient descent. (13 slides, 24min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-3-5-gradient-descent.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-3-5-gradient-descent.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-3-5-gradient-descent.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-3-5-gradient-descent.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>3.6.</td><td>Back-propagation. (11 slides, 20min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-3-6-backprop.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-3-6-backprop.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-3-6-backprop.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-3-6-backprop.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-4"></a>4. Graphs of operators, autograd, and convolutional layers. (86 slides, 1h36min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-4.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>4.1.</td><td>DAG networks. (11 slides, 21min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-4-1-DAG-networks.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-4-1-DAG-networks.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-4-1-DAG-networks.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-4-1-DAG-networks.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>4.2.</td><td>Autograd. (20 slides, 22min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-4-2-autograd.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-4-2-autograd.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-4-2-autograd.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-4-2-autograd.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>4.3.</td><td>PyTorch modules and batch processing. (15 slides, 15min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-4-3-modules-and-batch-processing.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-4-3-modules-and-batch-processing.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-4-3-modules-and-batch-processing.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-4-3-modules-and-batch-processing.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>4.4.</td><td>Convolutions. (23 slides, 23min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-4-4-convolutions.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-4-4-convolutions.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-4-4-convolutions.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-4-4-convolutions.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>4.5.</td><td>Pooling. (7 slides, 5min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-4-5-pooling.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-4-5-pooling.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-4-5-pooling.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-4-5-pooling.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>4.6.</td><td>Writing a PyTorch module. (10 slides, 10min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-4-6-writing-a-module.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-4-6-writing-a-module.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-4-6-writing-a-module.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-4-6-writing-a-module.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-5"></a>5. Initialization and optimization. (81 slides, 1h42min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-5.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>5.1.</td><td>Cross-entropy loss. (9 slides, 17min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-5-1-cross-entropy-loss.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-5-1-cross-entropy-loss.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-5-1-cross-entropy-loss.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-5-1-cross-entropy-loss.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>5.2.</td><td>Stochastic gradient descent. (17 slides, 26min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-5-2-SGD.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-5-2-SGD.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-5-2-SGD.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-5-2-SGD.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>5.3.</td><td>PyTorch optimizers. (8 slides, 6min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-5-3-optim.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-5-3-optim.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-5-3-optim.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-5-3-optim.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>5.4.</td><td>L<sub><small>2</small></sub> and L<sub><small>1</small></sub> penalties. (11 slides, 13min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-5-4-l2-l1-penalties.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-5-4-l2-l1-penalties.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-5-4-l2-l1-penalties.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-5-4-l2-l1-penalties.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>5.5.</td><td>Parameter initialization. (20 slides, 19min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-5-5-initialization.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-5-5-initialization.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-5-5-initialization.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-5-5-initialization.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>5.6.</td><td>Architecture choice and training protocol. (9 slides, 13min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-5-6-architecture-and-training.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-5-6-architecture-and-training.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-5-6-architecture-and-training.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-5-6-architecture-and-training.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>5.7.</td><td>Writing an autograd function. (7 slides, 8min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-5-7-writing-an-autograd-function.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-5-7-writing-an-autograd-function.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-5-7-writing-an-autograd-function.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-5-7-writing-an-autograd-function.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-6"></a>6. Going deeper. (86 slides, 1h39min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-6.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>6.1.</td><td>Benefits of depth. (12 slides, 24min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-6-1-benefits-of-depth.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-6-1-benefits-of-depth.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-6-1-benefits-of-depth.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-6-1-benefits-of-depth.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>6.2.</td><td>Rectifiers. (7 slides, 4min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-6-2-rectifiers.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-6-2-rectifiers.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-6-2-rectifiers.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-6-2-rectifiers.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>6.3.</td><td>Dropout. (11 slides, 13min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-6-3-dropout.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-6-3-dropout.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-6-3-dropout.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-6-3-dropout.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>6.4.</td><td>Batch normalization. (16 slides, 19min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-6-4-batch-normalization.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-6-4-batch-normalization.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-6-4-batch-normalization.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-6-4-batch-normalization.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>6.5.</td><td>Residual networks. (21 slides, 22min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-6-5-residual-networks.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-6-5-residual-networks.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-6-5-residual-networks.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-6-5-residual-networks.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>6.6.</td><td>Using GPUs. (19 slides, 18min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-6-6-using-GPUs.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-6-6-using-GPUs.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-6-6-using-GPUs.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-6-6-using-GPUs.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-7"></a>7. Autoencoders. (93 slides, 1h22min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-7.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>7.1.</td><td>Transposed convolutions. (14 slides, 14min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-7-1-transposed-convolutions.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-7-1-transposed-convolutions.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-7-1-transposed-convolutions.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-7-1-transposed-convolutions.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>7.2.</td><td>Deep Autoencoders. (26 slides, 16min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-7-2-autoencoders.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-7-2-autoencoders.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-7-2-autoencoders.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-7-2-autoencoders.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>7.3.</td><td>Denoising autoencoders. (38 slides, 33min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-7-3-denoising-autoencoders.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-7-3-denoising-autoencoders.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-7-3-denoising-autoencoders.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-7-3-denoising-autoencoders.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>7.4.</td><td>Variational autoencoders. (15 slides, 19min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-7-4-VAE.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-7-4-VAE.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-7-4-VAE.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-7-4-VAE.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-8"></a>8. Computer vision. (88 slides, 1h49min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-8.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>8.1.</td><td>Computer vision tasks. (14 slides, 20min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-8-1-CV-tasks.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-8-1-CV-tasks.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-8-1-CV-tasks.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-8-1-CV-tasks.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>8.2.</td><td>Networks for image classification. (36 slides, 44min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-8-2-image-classification.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-8-2-image-classification.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-8-2-image-classification.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-8-2-image-classification.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>8.3.</td><td>Networks for object detection. (15 slides, 21min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-8-3-object-detection.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-8-3-object-detection.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-8-3-object-detection.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-8-3-object-detection.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>8.4.</td><td>Networks for semantic segmentation. (10 slides, 11min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-8-4-segmentation.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-8-4-segmentation.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-8-4-segmentation.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-8-4-segmentation.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>8.5.</td><td>DataLoader and neuro-surgery. (13 slides, 13min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-8-5-dataloader-and-surgery.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-8-5-dataloader-and-surgery.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-8-5-dataloader-and-surgery.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-8-5-dataloader-and-surgery.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-9"></a>9. Under the hood. (92 slides, 1h22min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-9.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>9.1.</td><td>Looking at parameters. (13 slides, 10min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-9-1-looking-at-parameters.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-9-1-looking-at-parameters.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-9-1-looking-at-parameters.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-9-1-looking-at-parameters.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>9.2.</td><td>Looking at activations. (20 slides, 23min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-9-2-looking-at-activations.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-9-2-looking-at-activations.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-9-2-looking-at-activations.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-9-2-looking-at-activations.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>9.3.</td><td>Visualizing the processing in the input. (34 slides, 23min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-9-3-visualizing-in-input.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-9-3-visualizing-in-input.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-9-3-visualizing-in-input.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-9-3-visualizing-in-input.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>9.4.</td><td>Optimizing inputs. (25 slides, 25min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-9-4-optimizing-inputs.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-9-4-optimizing-inputs.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-9-4-optimizing-inputs.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-9-4-optimizing-inputs.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-10"></a>10. Autoregression and Normalizing Flows. (84 slides, 1h27min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-10.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>10.1.</td><td>Auto-regression. (25 slides, 28min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-10-1-autoregression.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-10-1-autoregression.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-10-1-autoregression.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-10-1-autoregression.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>10.2.</td><td>Causal convolutions. (25 slides, 22min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-10-2-causal-convolutions.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-10-2-causal-convolutions.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-10-2-causal-convolutions.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-10-2-causal-convolutions.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>10.3.</td><td>Non-volume preserving networks. (34 slides, 37min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-10-3-NVP.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-10-3-NVP.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-10-3-NVP.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-10-3-NVP.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-11"></a>11. Generative Adversarial Networks. (91 slides, 1h22min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-11.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>11.1.</td><td>Generative Adversarial Networks. (33 slides, 30min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-11-1-GAN.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-11-1-GAN.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-11-1-GAN.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-11-1-GAN.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>11.2.</td><td>Wasserstein GAN. (20 slides, 24min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-11-2-Wasserstein-GAN.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-11-2-Wasserstein-GAN.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-11-2-Wasserstein-GAN.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-11-2-Wasserstein-GAN.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>11.3.</td><td>Conditional GAN and image translation. (29 slides, 20min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-11-3-conditional-GAN.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-11-3-conditional-GAN.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-11-3-conditional-GAN.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-11-3-conditional-GAN.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>11.4.</td><td>Model persistence and checkpoints. (9 slides, 8min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-11-4-persistence.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-11-4-persistence.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-11-4-persistence.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-11-4-persistence.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-12"></a>12. Recurrent models and NLP. (73 slides, 1h18min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-12.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>12.1.</td><td>Recurrent Neural Networks. (24 slides, 23min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-12-1-RNN-basics.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-12-1-RNN-basics.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-12-1-RNN-basics.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-12-1-RNN-basics.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>12.2.</td><td>LSTM and GRU. (17 slides, 14min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-12-2-LSTM-and-GRU.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-12-2-LSTM-and-GRU.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-12-2-LSTM-and-GRU.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-12-2-LSTM-and-GRU.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>12.3.</td><td>Word embeddings and translation. (32 slides, 41min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-12-3-word-embeddings-and-translation.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-12-3-word-embeddings-and-translation.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-12-3-word-embeddings-and-translation.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-12-3-word-embeddings-and-translation.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>

  <li><a id="lecture-13"></a>13. Attention models. (the screencasts are not up-to-date, check the slides! – 93 slides, 1h25min videos)
  <img src="https://fleuret.org/dlc/pics/thumb-13.png" alt="Icon made from one of the slides">
    <table>
    <tbody><tr>
      <td>13.1.</td><td>Attention for Memory and Sequence Translation. (21 slides, 21min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-13-1-attention-memory-translation.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-13-1-attention-memory-translation.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-13-1-attention-memory-translation.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-13-1-attention-memory-translation.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>13.2.</td><td>Attention Mechanisms. (30 slides, 30min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-13-2-attention-mechanisms.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-13-2-attention-mechanisms.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-13-2-attention-mechanisms.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-13-2-attention-mechanisms.mp4">mp4</a>).</td>
    </tr>
    <tr>
      <td>13.3.</td><td>Transformer Networks. (42 slides, 34min video)</td>
    </tr>
    <tr>
      <td></td>
      <td><a href="https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf">handout</a>
      (<a href="https://fleuret.org/dlc/materials/dlc-slides-13-3-transformers.pdf">slides</a>),
      <a href="https://fleuret.org/dlc/streaming/dlc-video-13-3-transformers.mp4">stream</a>
      (<a href="https://fleuret.org/dlc/videos/dlc-video-13-3-transformers.mp4">mp4</a>).</td>
    </tr>
    </tbody></table>
  </li>


</ul>

<h2><a id="practicals"></a>Practicals</h2>

<ul>
<li><a href="https://fleuret.org/dlc/materials/dlc-practical-1.pdf">Practical 1</a></li>
<!-- <li><a href="materials/dlc-practical-1.pdf">Practical 1</a> (<a href="src/dlc_practical_1_solution.py">solution</a>)</li> -->
<li><a href="https://fleuret.org/dlc/materials/dlc-practical-2.pdf">Practical 2</a></li>
<!-- <li><a href="materials/dlc-practical-2.pdf">Practical 2</a> (<a href="src/dlc_practical_2_solution.py">solution</a>)</li> -->
<li><a href="https://fleuret.org/dlc/materials/dlc-practical-3.pdf">Practical 3</a></li>
<!-- <li><a href="materials/dlc-practical-3.pdf">Practical 3</a> (<a href="src/dlc_practical_3_solution.py">solution</a>)</li> -->
<li><a href="https://fleuret.org/dlc/materials/dlc-practical-4.pdf">Practical 4</a></li>
<!-- <li><a href="materials/dlc-practical-4.pdf">Practical 4</a> (<a href="src/dlc_practical_4_solution.py">solution</a>)</li> -->
<li><a href="https://fleuret.org/dlc/materials/dlc-practical-5.pdf">Practical 5</a></li>
<!-- <li><a href="materials/dlc-practical-5.pdf">Practical 5</a> (<a href="src/dlc_practical_5_solution.py">solution</a>)</li> -->
<li><a href="https://fleuret.org/dlc/materials/dlc-practical-6.pdf">Practical 6</a></li>
<!-- <li><a href="materials/dlc-practical-6.pdf">Practical 6</a> (<a href="src/dlc_practical_6_solution.py">solution</a>)</li> -->
</ul>

<!-- ************************************************************ -->
<!-- ************************************************************ -->
<!-- ************************************************************ -->

<h2><a id="information"></a>Information</h2>

<h2>Pre-requisites</h2>

<ul>
<li>Linear algebra (vectors, matrices, Euclidean spaces),</li>
<li>differential calculus (Jacobian, Hessian, chain rule),</li>
<li>Python programming,</li>
<li>basics in probabilities and statistics (discrete and continuous
distributions, law of large numbers, conditional probabilities,
Bayes, PCA),</li>
<li>basics in optimization (notion of minima, gradient descent),</li>
<li>basics in algorithmic (computational costs),</li>
<li>basics in signal processing (Fourier transform, wavelets).</li>
</ul>

<h2>Documentation</h2>

<p>You may have to look at the Python, Jupyter notebook, and PyTorch
documentations at</p>

<ul>
<li><a href="https://docs.python.org/">https://docs.python.org/</a></li>
<li><a href="https://jupyter.org/">https://jupyter.org/</a></li>
<li><a href="https://pytorch.org/docs/">https://pytorch.org/docs/</a></li>
</ul>

<!-- ************************************************************ -->

<h2><a id="prologue"></a>Practical session prologue</h2>

<p>Helper Python prologue for the practical
sessions: <a href="https://fleuret.org/dlc/src/dlc_practical_prologue.py">dlc_practical_prologue.py</a></p>

<h3>Argument parsing</h3>

<p>This prologue parses command-line arguments as follows</p>

<pre>usage: dummy.py [-h] [--full] [--tiny] [--seed SEED]
[--cifar] [--data_dir DATA_DIR]

DLC prologue file for practical sessions.

optional arguments:
-h, --help           show this help message and exit
--full               Use the full set, can take ages (default
False)
--tiny               Use a very small set for quick checks
(default False)
--seed SEED          Random seed (default 0, &lt; 0 is no seeding)
--cifar              Use the CIFAR data-set and not MNIST
(default False)
--data_dir DATA_DIR  Where are the PyTorch data located (default
$PYTORCH_DATA_DIR or './data')
</pre>

<h3>Loading data</h3>

<p>The prologue provides the function</p>

<pre>load_data(cifar = None, one_hot_labels = False, normalize = False, flatten = True)
</pre>

<p>which downloads the data when required, reshapes the images to 1d
vectors if <span>flatten</span>
is <span>True</span>, and narrows to a small subset of
samples if <span>--full</span> is not selected.</p>

<p>It returns a tuple of four tensors: <span>train_data</span>,
<span>train_target</span>, <span>test_data</span>, and <span>test_target</span>.</p>

<p>If <span>cifar</span> is <span>True</span>, the data-base used is CIFAR10, if it
is <span>False</span>, MNIST is used, if it is None, the argument
<span>--cifar</span> is taken into account.</p>

<p>If <span>one_hot_labels</span> is <span>True</span>, the targets are converted to 2d
<span>torch.Tensor</span> with as many columns as there are classes, and
-1 everywhere except the coefficients [n, y_n], equal to 1.</p>

<p>If <span>normalize</span> is <span>True</span>, the data tensors are normalized
according to the mean and variance of the training one.</p>

<p>If <span>flatten</span> is <span>True</span>, the data tensors are flattened
into 2d tensors of dimension N × D, discarding the image structure
of the samples. Otherwise they are 4d tensors of dimension N × C
× H × W.</p>

<h3>Minimal example</h3>

<pre>import dlc_practical_prologue as prologue

train_input, train_target, test_input, test_target = prologue.load_data()

print('train_input', train_input.size(), 'train_target', train_target.size())
print('test_input', test_input.size(), 'test_target', test_target.size())
</pre>

<p>prints</p>

<pre>* Using MNIST
** Reduce the data-set (use --full for the full thing)
** Use 1000 train and 1000 test samples
train_input torch.Size([1000, 784]) train_target torch.Size([1000])
test_input torch.Size([1000, 784]) test_target torch.Size([1000])
</pre>

<!-- ************************************************************ -->
<!-- ************************************************************ -->
<!-- ************************************************************ -->

<h2><a id="vm"></a>Virtual Machine</h2>

<p>A Virtual Machine (VM) is a software that simulates a complete
computer. The one we provide here includes a Linux operating
system and all the tools needed to use PyTorch from a web
browser
(<i>e.g.</i> <a href="https://www.mozilla.org/en-US/firefox/new/">Mozilla
Firefox</a> or <a href="https://www.google.com/chrome/">Google
Chrome</a>).</p>

<h3>Installation</h3>

<ol>
<li>Download and install <a href="https://www.virtualbox.org/wiki/Downloads">Oracle's VirtualBox,</a></li>
<li>download the <a href="https://fleuret.org/dlc/files/dlc-vm.ova">virtual machine OVA package</a> (1.68Gb), and</li>
<li>open the latter in VirtualBox with <span>File → Import Appliance</span>.</li>
</ol>

<p>You should now see an entry in the list of VMs. The first time
it starts, it provides a menu to choose the keyboard layout you
want to use (you can force the configuration later by running
the command <span>sudo set-kbd</span>).</p>

<p><b>If the VM does not start and VirtualBox complains that the
VT-x is not enabled, you have to activate the virtualization
capabilities of your CPU in the BIOS of your computer.</b></p>

<h3><a id="using-the-vm"></a>Using the VM</h3>

<p>The VM automatically starts
a <a href="https://jupyter.org/">JupyterLab</a> on port 8888 and
exports that port to the host. This means that you can access this
JupyterLab with a web browser on the machine running VirtualBox at
<a href="http://localhost:8888/">http://localhost:8888/</a>
and use Python notebooks, view files, start terminals, and edit source
files. Typing <span>!bye</span> in a notebook
or <span>bye</span> in a terminal will shutdown the
VM.</p>

<p>You can run a terminal and a text editor from inside the Jupyter
notebook for exercises that require more than the notebook
itself. Source files can be executed by running in a terminal the
Python command with the source file name as argument. Both can be done
from the main Jupyter window with:</p>

<ul>
<li><span>New → Text File</span> to create
the source code, or selecting the file and
clicking <span>Edit</span> to edit an existing
one.</li>
<li><span>New → Terminal</span> to start a
shell from which you can run Python.</li>
</ul>

<!-- <p><b>Files saved in the VM are erased when the VM is -->
<!-- re-installed, which happens for each session on the EPFL -->
<!-- machines. So you should download files you want to keep from -->
<!-- the Jupyter notebook to your account and re-upload them later -->
<!-- when you need them.</b></p> -->

<p>This VM also exports an ssh port to the port 2022 on the host,
which allows to log in with standard ssh clients on Linux and
OSX, and with applications such
as <a href="https://www.putty.org/">PuTTY</a> on Windows. The
default login is <span>'dave'</span> and
password <span>'dummy'</span>, same password
for the root account.</p>

<h3>Remarks</h3>

<p>Note that performance for computation will be very poor compared to
<a href="https://pytorch.org/get-started/locally/">installing
PyTorch</a> natively on your machine. In particular, the VM does
not take advantage of a GPU if you have one.</p>

<p><b>Finally, please also note that this VM is configured in a
convenient but highly non-secured manner, with easy to guess
passwords, including for the root, and network-accessible
non-protected Jupyter notebooks.</b></p>

<p>This VM is built on
a <a href="https://www.linuxfoundation.org/">Linux</a> <a href="https://www.debian.org/">Debian,</a>
with <a href="https://conda.io/miniconda.html">miniconda,</a>
<a href="https://pytorch.org/">PyTorch,</a> <a href="http://yann.lecun.com/exdb/mnist/">MNIST,</a>
<a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10,</a> and many Python utility packages installed.</p>

<!-- ************************************************************ -->
<!-- ************************************************************ -->
<!-- ************************************************************ -->

<h2><a id="license"></a>License of use</h2>

<p>My own materials on this page are licensed under the
<a href="https://fleuret.org/dlc/by-nc-sa-4.0.txt">Creative Commons BY-NC-SA 4.0
International License.</a></p>

<p>More simply: I am okay with this material being used for
regular academic teaching, but definitely not for a book /
youtube loaded with ads / whatever monetization model I am not
aware of.</p>

<!-- ********************************************************************** -->



<!-- ********************************************************************** -->




</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Big TDD Misunderstanding (2022) (102 pts)]]></title>
            <link>https://linkedrecords.com/the-big-tdd-misunderstanding-8e22c2f1fc21?gi=61ce87d573e8</link>
            <guid>38330989</guid>
            <pubDate>Sun, 19 Nov 2023 09:48:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linkedrecords.com/the-big-tdd-misunderstanding-8e22c2f1fc21?gi=61ce87d573e8">https://linkedrecords.com/the-big-tdd-misunderstanding-8e22c2f1fc21?gi=61ce87d573e8</a>, See on <a href="https://news.ycombinator.com/item?id=38330989">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener  ugc nofollow" href="https://linkedrecords.com/?source=post_page-----8e22c2f1fc21--------------------------------"><div aria-hidden="false"><p><img alt="Oliver Wolf" src="https://miro.medium.com/v2/resize:fill:88:88/1*ptf4iR5duKduhso1V-wQQw.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="d87c"><strong>💡</strong>Originally, the term “unit” in “unit test” referred not to the system under test but to the test itself. This implies that the test can be executed as one unit and does not rely on other tests running upfront (see <a href="https://medium.com/r?url=https%3A%2F%2Ftanzu.vmware.com%2Fcontent%2Fblog%2Fwhat-is-a-unit-test-the-answer-might-surprise-you" rel="noopener">here</a> and <a href="https://www.youtube.com/watch?v=HNjlJpuA5kQ" rel="noopener ugc nofollow" target="_blank">here</a>).</p><p id="e365">However, when people started considering parts of the system under test as the “units”, it significantly affected the quality of test suites (in a bad way). The primary consequence was that the majority began writing one “unit test” for every class or method. The second consequence was the isolation of this “unit” from other “units” using test doubles (mocks).</p><p id="183e"><strong>Now, you change a little thing in your code base, and the only thing the testing suite tells you is that you will be busy the rest of the day rewriting false positive test cases.</strong></p><p id="b238">Unfortunately, the status quo of unit testing is perceived very dogmatic and thinking different is kind of a taboo topic. But maybe it helps when you know that there are actually two main schools when it comes to testing: mockist vs. classicist. Here are my tips on how to write “good” tests, mostly <strong>inspired</strong> by the classicist style. But also keep in mind - in software engineering - there is not good and bad, there is only: does if satisfy <strong>your</strong> requirements.</p><p id="99ce"><strong>Tip #1</strong>: Write the tests from outside in. With this, I mean you should write your tests from a realistic user perspective. To have the best quality assurance and refactor resistance, you would write e2e or integration tests. This can lead to tests that take a long time to execute and increase the feedback loop. You can try to solve this by making the tests independent of each other so they can run in parallel. Originally, the test pyramid forbade a lot of end-to-end and integration tests. Instead, the pyramid says we should write a lot of unit tests, and for most people, a unit is a class. This often leads to an inside-out approach, testing the structure of the system rather than its behavior. Challenge the traditional testing pyramid and think about how much end-to-end integration and unit tests make sense in your context. Also consider more recent alternatives to the test pyramid: “<a href="https://www.oreilly.com/library/view/hands-on-microservices/9781789133608/7c9f1260-b0c5-4416-816f-1cad140b56dd.xhtml" rel="noopener ugc nofollow" target="_blank">Honeycomb</a>” and “<a href="https://twitter.com/kentcdodds/status/960723172591992832" rel="noopener ugc nofollow" target="_blank">The Testing Trophy</a>”.</p><p id="0854"><strong>Tip #2: </strong>Do not isolate code when you test it. If you do so, the tests become fragile and will not help you in case you refactor the software. Only isolate your code from truly external services. Have a look at the port and adapter pattern (aka hexagonal architecture), which is a good starting point for decoupling your “main code” from infrastructure code. If you stub, you stub the infrastructure implementations. Also, consider not stub the infrastructure and using a real database. With tools like Docker, it is not that hard or slow anymore. Also, the more you isolate your “unit” under test, the less meaningful the test coverage report becomes. You just don’t know if your system works as a whole, even though each line is tested. This is especially true for dynamically typed languages.</p><p id="37e5"><strong>Tip #3</strong>: Never change your code without having a red test. This is a pretty common practice in TDD. This has two benefits: 1) It is the tests of the tests itself. When it is red, you know it works. 2) It makes sure you test all scenarios. This, of course, does not apply when you refactor your code.</p><p id="b0c0"><strong>Tip #4:</strong> TDD says the process of writing tests first will/should drive the design of your software. I never understood this. Maybe this works for other people but it does not work for me. It is software architecture 101 — Non-functional requirements (NFR) define your architecture. NFR usually do not play a role when writing unit tests.</p><p id="5bf9">To sum up, in my opinion, the most important decision to make when you start writing automated tests is to decide what trade-off to make. <strong>Do you want a high level of quality assurance, refactor resistance, or a fast feedback loop?</strong> Today, it is often possible to make an e2e test or integration test run fast enough.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Timeline of the OpenAI Board (186 pts)]]></title>
            <link>https://loeber.substack.com/p/a-timeline-of-the-openai-board</link>
            <guid>38330158</guid>
            <pubDate>Sun, 19 Nov 2023 07:39:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://loeber.substack.com/p/a-timeline-of-the-openai-board">https://loeber.substack.com/p/a-timeline-of-the-openai-board</a>, See on <a href="https://news.ycombinator.com/item?id=38330158">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h4 translated="">Discover more from Loeber on Substack</h4><p>On interactions between people, markets, and technology.</p> </div><div dir="auto"><p><span>Yesterday, Sam Altman and Greg Brockman were fired from the Board of Directors of OpenAI. Following, all of Tech Twitter was abuzz with one question: wait a moment, who was on the Board? And after they found out, they asked: who on earth are </span><a href="https://www.google.com/search?q=tasha+mccauley" rel="nofollow ugc noopener">Tasha McCauley</a><span> and </span><a href="https://cset.georgetown.edu/staff/helen-toner/" rel="nofollow ugc noopener">Helen Toner</a><span>? It turns out that OpenAI’s Board had undergone numerous changes over the years, especially recently. And that just wasn’t ever the biggest news about OpenAI, so those changes didn’t spark the concerns that maybe they should have. </span></p><p><span>I combed through the Internet Archive and OpenAI’s non-profit filings to try to make sense of OpenAI’s governance. Below, I have attempted to chronicle the composition of OpenAI’s Board over time, point out the conflicts, and you can see how we got to the earthquake yesterday. You can </span><a href="https://loeber.substack.com/i/138968534/summaryperspectives" rel="nofollow ugc noopener">skip to the end</a><span> for my summary perspective.</span></p><p>OpenAI is founded.</p><p><span>Board Directors:</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-138968534" href="https://loeber.substack.com/p/a-timeline-of-the-openai-board#footnote-1-138968534" target="_self" rel="nofollow ugc noopener">1</a></span></p><ul><li><p>Elon Musk (Co-Chair)</p></li><li><p>Sam Altman (Co-Chair)</p></li></ul><p><span>OpenAI’s </span><a href="https://projects.propublica.org/nonprofits/organizations/810861541/201703459349300445/full" rel="nofollow ugc noopener">Form 990 public filings</a><span> for calendar year 2016 show the Board Directors:</span></p><ul><li><p>Elon Musk</p></li><li><p>Sam Altman</p></li><li><p>Chris Clark</p></li><li><p>Jonathan Levy (?)</p></li></ul><p>Chris was the initial COO of OpenAI, and still works there to this day. Jonathan Levy was listed as Secretary/Treasurer, and may have been a trustee rather than a Director. It’s unclear from the filings.</p><p><span>Open Philanthropy </span><a href="https://www.goodventures.org/our-portfolio/grants/openai-general-support/" rel="nofollow ugc noopener">donates</a><span> $30M to OpenAI. Holden Karnofsky, the founder of Open Philanthropy, </span><a href="https://www.openphilanthropy.org/grants/openai-general-support/#5-relationship-disclosures" rel="nofollow ugc noopener">joins</a><span> OpenAI’s Board of Directors.</span></p><p><span>OpenAI’s </span><a href="https://projects.propublica.org/nonprofits/organizations/810861541/201920719349300822/full" rel="nofollow ugc noopener">Form 990 public filings</a><span> for calendar year 2017 show the Board Directors: </span></p><ul><li><p>Elon Musk</p></li><li><p>Sam Altman</p></li><li><p>Chris Clark</p></li><li><p>Holden Karnofsky</p></li><li><p>Greg Brockman</p></li><li><p>Ilya Sutskever</p></li></ul><p><span>Elon Musk is removed from the Board. The </span><a href="https://openai.com/blog/openai-supporters" rel="nofollow ugc noopener">official press release</a><span> proclaims a departure to avoid potential conflicts, but </span><a href="https://www.semafor.com/article/03/24/2023/the-secret-history-of-elon-musk-sam-altman-and-openai" rel="nofollow ugc noopener">journalists report</a><span> leadership disagreements culminating in Elon proposing a takeover and being rebuked.</span></p><p>Board Directors:</p><ul><li><p>Greg Brockman</p></li><li><p>Ilya Sutskever</p></li><li><p>Holden Karnofsky</p></li><li><p>Sam Altman</p></li></ul><p>We don’t know exactly when Chris Clark was removed from the Board. </p><p><span>Reid Hoffman, founder of LinkedIn and General Partner at Greylock, joins the Board. I couldn’t find a press release or official announcement, but Reid’s </span><a href="https://www.linkedin.com/in/reidhoffman/details/experience/" rel="nofollow ugc noopener">LinkedIn profile</a><span> has the dates.</span></p><p><span>Adam D’Angelo, CEO of Quora and former Facebook CTO, </span><a href="https://twitter.com/adamdangelo/status/988859015315701760?lang=en" rel="nofollow ugc noopener">joins</a><span> the Board. This follows the February Board changes, where the OpenAI blog post had noted the intent to add another Director to the Board soon.</span></p><p><span>Sue Yoon joins the Board. Sue’s exact employment at the time was unclear — she was previously an EIR at First Round, and in the coming months would lead robotics projects at Google. Similar to Reid Hoffman, I couldn’t find an official announcement, but her </span><a href="https://www.linkedin.com/in/sue-yoon-8b35a214/" rel="nofollow ugc noopener">LinkedIn profile</a><span> has the dates.</span></p><p><span>OpenAI’s </span><a href="https://projects.propublica.org/nonprofits/organizations/810861541/201943199349318399/full" rel="nofollow ugc noopener">Form 990 public filings</a><span> for calendar year 2018 list the Board Directors: </span></p><ul><li><p>Sam Altman</p></li><li><p>Sue Yoon</p></li><li><p>Holden Karnofsky</p></li><li><p>Greg Brockman</p></li><li><p>Ilya Sutskever</p></li><li><p>Adam D’Angelo</p></li><li><p>Tasha McCauley</p></li></ul><p>I could not find anything in the way of a source on when, or under what circumstances, Tasha McCauley joined the Board. </p><p><span>This gets strange. There’s an OpenAI </span><a href="https://openai.com/blog/openai-lp" rel="nofollow ugc noopener">blog post listing the Board Directors</a><span>:</span></p><ul><li><p>Greg Brockman</p></li><li><p>Ilya Sutskever </p></li><li><p>Sam Altman</p></li><li><p>Adam D’Angelo</p></li><li><p>Holden Karnofsky</p></li><li><p>Reid Hoffman</p></li><li><p>Shivon Zilis</p></li><li><p>Tasha&nbsp;McCauley</p></li></ul><p><span>Note the unannounced elevation of Shivon Zilis (previously an advisor) and the unannounced departure of Sue Yoon. Weirder yet, OpenAI published its </span><a href="https://web.archive.org/web/20190311213355/https://openai.com/about/" rel="nofollow ugc noopener">new homepage</a><span> just that day, still listing Sue Yoon as a Board Director, and not Shivon Zilis.</span></p><p><span>Sue Yoon leaves OpenAI’s Board, according to her LinkedIn. The OpenAI Website </span><a href="https://web.archive.org/web/20191201065651/https://openai.com/about/" rel="nofollow ugc noopener">still lists</a><span> her (and not Shivon Zilis) as a Board Director. </span></p><p><span>Another year, another OpenAI </span><a href="https://projects.propublica.org/nonprofits/organizations/810861541/202003219349325305/full" rel="nofollow ugc noopener">Form 990 public filing</a><span>, listing the Board Directors:</span></p><ul><li><p>Ilya Sutskever</p></li><li><p>Greg Brockman</p></li><li><p>Sam Altman</p></li><li><p>Reid Hoffman</p></li><li><p>Sue Yoon</p></li><li><p>Holden Karnofsky</p></li><li><p>Adam D’Angelo</p></li><li><p>Tasha McCauley</p></li></ul><p>Note that Shivon Zilis still doesn’t appear in the list of Board Directors. Was the March 11, 2019 blog post just wrong? Did someone in marketing make a mistake and no-one caught it? </p><p>Slow news year. The Form 990 public filing for calendar year 2020 lists the Board of Directors, finally including Shivon Zilis and not Sue Yoon:</p><ul><li><p>Ilya Sutskever</p></li><li><p>Greg Brockman</p></li><li><p>Sam Altman</p></li><li><p>Reid Hoffman</p></li><li><p>Shivon Zilis</p></li><li><p>Holden Karnofsky</p></li><li><p>Adam D’Angelo</p></li><li><p>Tasha McCauley</p></li></ul><p>I couldn’t find a public statement on when Shivon actually joined the Board, other than the March 2019 blog post that may have been in error. </p><p><span>Will Hurd, Republican member of the House of Representatives, and former CIA agent, </span><a href="https://openai.com/blog/will-hurd-joins" rel="nofollow ugc noopener">joins</a><span> the Board.</span></p><p><span>Helen Toner, Director at Georgetown’s Center for Security and Emerging Technologies, and formerly of Holden Karnofsky’s Open Philanthropy, </span><a href="https://openai.com/blog/helen-toner-joins" rel="nofollow ugc noopener">joins</a><span> the Board. </span></p><p><span>Holden Karnofsky resigns from the Board, </span><a href="https://www.vox.com/future-perfect/2023/3/18/23645013/openai-gpt4-holden-karnofsky-artificial-intelligence-ai-safety-existential-risk" rel="nofollow ugc noopener">citing</a><span> a potential conflict because his wife, Daniela Amodei, is helping start Anthropic, a major OpenAI competitor, with her brother Dario Amodei. (They all live(d) together.) The exact date of Holden’s resignation is unknown; there was no contemporaneous press release.</span></p><p><span>Between October and November 2021, Holden was quietly removed from the list of Board Directors on the OpenAI website, and Helen was added (</span><a href="https://forum.effectivealtruism.org/posts/fmDFytmxwX9qBgcaX/why-aren-t-you-freaking-out-about-openai-at-what-point-would?commentId=KavuL7Q5qdvxoYSsd" rel="nofollow ugc noopener">Discussion Source</a><span>). Given their connection via Open Philanthropy and the fact that Holden’s Board seat appeared to be permanent, it seems that Helen was picked by Holden to take his seat. </span></p><p><span>OpenAI’s </span><a href="https://projects.propublica.org/nonprofits/organizations/810861541/202243199349314989/full" rel="nofollow ugc noopener">Form 990 public filings</a><span> list the Board Directors of the 2021 calendar year: </span></p><ul><li><p>Ilya Sutskever</p></li><li><p>Shivon Zilis</p></li><li><p>Greg Brockman</p></li><li><p>Will Hurd</p></li><li><p>Sam Altman</p></li><li><p>Reid Hoffman</p></li><li><p>Holden Karnofsky</p></li><li><p>Adam D’Angelo</p></li><li><p>Tasha McCauley</p></li><li><p>Helen Toner</p></li></ul><p>The fact that both Holden and Helen are listed here is not surprising; both of them were Board Directors at points in 2021. (It does not necessarily imply that they were both on the Board at the same time.)</p><p>There did not appear to be any Board events in 2022. The Form 990 does not appear to have been filed as of the time of writing.</p><p><span>Reid Hoffman steps down from the Board, </span><a href="https://www.bloomberg.com/news/articles/2023-03-03/linkedin-co-founder-hoffman-stepping-down-from-openai-board" rel="nofollow ugc noopener">citing</a><span> the need to avoid potential conflicts with his investments. While this was reported in March 2023, according to his LinkedIn profile’s dates it happened in January.</span></p><p><span>Shivon Zilis </span><a href="https://www.theinformation.com/articles/shivon-zilis-musk-associate-leaves-openai-board" rel="nofollow ugc noopener">resigns</a><span> from the Board for reasons unknown. (Commentators speculate that her resignation is over conflicts due to her bearing Elon Musk’s children, but that is ultimately just speculation.)</span></p><p><span>Will Hurd </span><a href="https://www.bloomberg.com/news/articles/2023-07-13/republican-presidential-hopeful-will-hurd-leaves-board-of-openai" rel="nofollow ugc noopener">resigns</a><span> from the Board, citing the need to focus on politics/his 2024 Presidential campaign. (Three months later, in October, he drops out of the race. I don’t know what to make of that.)</span></p><p><span>Sam Altman is fired from OpenAI and the OpenAI Board in a surprise meeting of the Board (except Greg). Minutes later, in a </span><a href="https://twitter.com/gdb/status/1725736242137182594" rel="nofollow ugc noopener">separate surprise Board meeting</a><span>, Greg Brockman is removed from the Board (and as Board Chairman).</span></p><p>Board Directors:</p><ul><li><p>Adam D’Angelo</p></li><li><p>Helen Toner</p></li><li><p>Tasha McCauley</p></li><li><p>Ilya Sutskever</p></li></ul><p>The first thing that sticks out to me is that there have been, for several quarters, two significant conflicts of interest on the Board:</p><ul><li><p><span>Adam D’Angelo founded and appears to be spending all his time on developing </span><a href="https://twitter.com/poe_platform" rel="nofollow ugc noopener">Poe</a><span>, an AI chat platform partially leveraging and partially competing with OpenAI. In my opinion, that’s too close. Reid Hoffman resigned over potential indirect investment conflicts; Adam’s conflicts are more direct. Best practice would’ve been for Adam to resign when he began working on Poe.</span></p></li><li><p><span>Helen Toner and and Tasha McCauley are jointly participating in a highly ideological AI governance organization. As Alex Konrad </span><a href="https://www.forbes.com/sites/alexkonrad/2023/11/17/these-are-the-people-that-fired-openai-ceo-sam-altman/?sh=47da17654ae9" rel="nofollow ugc noopener">noted</a><span>: “McCauley currently sits on the advisory board of British-founded international Center for the Governance of AI (GovAI) alongside fellow OpenAI director Helen Toner.” It turns out that the </span><a href="https://www.governance.ai/people" rel="nofollow ugc noopener">advisory board is six people</a><span>, and beyond Helen and Tasha, the other four include: one who currently works for Open Philanthropy, and another is the founder of GovAI, which was mostly funded by… Open Philanthropy. </span></p><ul><li><p>For OpenAI’s six-person Board, it was inappropriate for two Board Directors to be this strongly associated with an ideological organization and therefore so strongly and predictably aligned in their voting. It calls into question the independence of their votes.</p></li><li><p><span>Due to Open Philanthropy’s link to major OpenAI competitor Anthropic, there’s also a hint of corporate conflict here. If I were on OpenAI’s Board, I would have requested at least for Tasha</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-138968534" href="https://loeber.substack.com/p/a-timeline-of-the-openai-board#footnote-2-138968534" target="_self" rel="nofollow ugc noopener">2</a></span><span> to relinquish her seat for a true independent Director.</span></p></li></ul></li></ul><p>Secondly, what the hell happened in Q1/Q2 2023? </p><ul><li><p>Reid, Shivon, and Will all resigned, and the Board did not line up replacement Directors? By comparison, when Elon resigned in February 2018, Adam joined two months later. </p><ul><li><p>Were these seats just left vacant, with a deadlocked Board unable to agree on new Directors to appoint?</p></li></ul></li><li><p>They all resigned within a few months of one another despite OpenAI looking like the rocketship of the century? Something feels a little odd about that.</p></li></ul><p>It seems less likely that the November firings would have happened if Reid, Shivon, and Will — or even just one of them! — had still been on the Board, or replaced with an appropriate representative. With this view, the outcome was almost predictable given these two facts:</p><ol><li><p>The thinning-out of the Board from 9 to 6 members;</p></li><li><p>Half of those 6 members carrying conflicts in their relationship with OpenAI!</p></li></ol><p>We will find out, in due time, the motivations of the Board in the November firings. Right now they aren’t clear. It isn’t known whether anyone acted inappropriately, and I am not accusing anyone (to be clear, even the Board Directors that I consider conflicted) of having acted subject to conflicts of interest. But the 2023 changes made drama likely, no matter what. A Board is a delicate balance of perspectives and interests. When a Board rapidly changes in size, rarely is the remainder left well-balanced. Potential conflicts only make the balancing act harder.</p><p>Governance can be messy. Time will be the judge of whether this act of governance was wise or not. But you should note that the people involved in this act of corporate governance are roughly the same people trying to position themselves to govern policy on artificial intelligence. </p><p><span>It seems much easier to govern a single-digit number of highly capable people than to “govern” artificial superintelligence. If it turns out that this act of governance was unwise, then it calls into serious question the ability of these people and their organizations (Georgetown’s CSET, Open Philanthropy, etc.) to conduct governance in general, </span><em>especially</em><span> of the most impactful technology of the hundred years to come. Many people are saying we need more governance: maybe it turns out we need less.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HandBrake 1.7.0 – The open source video transcoder (357 pts)]]></title>
            <link>https://forum.handbrake.fr/viewtopic.php?t=43311</link>
            <guid>38329969</guid>
            <pubDate>Sun, 19 Nov 2023 07:03:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forum.handbrake.fr/viewtopic.php?t=43311">https://forum.handbrake.fr/viewtopic.php?t=43311</a>, See on <a href="https://news.ycombinator.com/item?id=38329969">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Upgrade Notice</span></p><p>

Before updating HandBrake, <strong><span>please make sure there are no pending encodes in the queue</span></strong>, and be sure to make a backup of any custom presets and app preferences you have, as they may not be compatible with newer versions.</p><p>

Windows users, please make sure to install [Microsoft .NET Desktop Runtime version 6.0.x](<a href="https://dotnet.microsoft.com/en-us/download/dotnet/6.0">https://dotnet.microsoft.com/en-us/download/dotnet/6.0</a>). Read carefully: you need the **DESKTOP** runtime. You must install .NET 6 even if you have installed .NET 7.</p><p>

<span>Release Notes</span><br>
For a full list of improvements and fixes, please see our <a href="https://github.com/HandBrake/HandBrake/releases/tag/1.7.0">release notes on GitHub.</a></p><p>


<span>Reporting Issues or Providing Feedback</span><br>
If you happen to discover any reproducible bugs, issues, or just want to provide feedback, please tell us on our GitHub issue tracker. You can also get in touch on our IRC community support channel.</p><p>

Please be aware, the HandBrake app is built by a very small team of volunteers in our free time. As such, it may not be possible for an immediate response but we do see your comments and welcome constructive feedback!</p><p>

<span>Thanks and Contributing</span><br>
Some of the features in this release have come from HandBrake users or companies. Translations have come from a vibrant community of volunteers throughout the globe. We'd like to thank everyone who contributed!</p><p>

On that note, if you are interested in contributing but don't today, please take a read of our contributing guide. There are many ways you can contribute and you don't need to be a developer to do so.</p><p>



<span>Release Highlights</span></p><p>

<a href="https://handbrake.fr/downloads.php">Download HandBrake 1.7.0</a></p><p>

<a href="https://handbrake.fr/docs">HandBrake 1.7 Documentation</a></p><p>


<a href="https://forum.handbrake.fr/viewtopic.php?t=43312">[ Comment on this Forum ]</a><br>
<a href="https://github.com/HandBrake/HandBrake/discussions/5505">[ Comment on GitHub Discussions ]</a></p></div></div>]]></description>
        </item>
    </channel>
</rss>