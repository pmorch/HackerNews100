<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 10 Jan 2025 16:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Who Can Understand the Proof? A Window on Formalized Mathematics (120 pts)]]></title>
            <link>https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/</link>
            <guid>42654995</guid>
            <pubDate>Fri, 10 Jan 2025 12:21:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/">https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/</a>, See on <a href="https://news.ycombinator.com/item?id=42654995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
<h2 id="the-simplest-axiom-for-logic">The Simplest Axiom for Logic</h2>



<p><a href="https://www.wolframscience.com/nks/p808--implications-for-mathematics-and-its-foundations/">Theorem <span>(Wolfram with Mathematica, 2000)</span></a>: <br>The single axiom <span>((<em>a</em>•<em>b</em>)•<em>c</em>)•(<em>a</em>•((<em>a</em>•<em>c</em>)•<em>a</em>))<span></span><em>c</em></span> is a complete axiom system for Boolean algebra (and is the simplest possible)</p>
<p>For more than a century <a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#the-history">people had wondered</a> how simple the axioms of logic (Boolean algebra) could be. <a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#a-discovery-about-basic-logic">On January 29, 2000, I found the answer</a>—and made the surprising discovery that they could be about twice as simple as anyone knew. (I also showed that what I found was <a href="https://www.wolframscience.com/nks/notes-12-9--searching-for-logic-axioms/">the simplest possible</a>.) </p>
<p>It was an interesting result—that gave new intuition about just how simple the foundations of things can be, and for example helped inspire my efforts to find a <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">simple underlying theory of physics</a>. </p>
<p>But how did I get the result? Well, I used automated theorem proving (specifically, what’s now <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> in <a href="https://www.wolfram.com/language/">Wolfram Language</a>). Automated theorem proving is something that’s <a href="https://www.wolframscience.com/nks/notes-12-9--automated-theorem-proving/">been around since at least the 1950s</a>, and its core methods haven’t changed in a long time. But in the rare cases it’s been used in mathematics it’s typically been to confirm things that were already believed to be true. And in fact, to my knowledge, my Boolean algebra axiom is actually the only truly unexpected result that’s ever been found for the first time using automated theorem proving.<span id="more-65170"></span></p>
<p>But, OK, so we know it’s true. And that’s interesting. But what about the proof? Does the proof, for example, show us why the result is true? Well, actually, in a quarter of a century, nobody (including me) has ever made much headway at all in understanding the proof (which, at least in the form we currently know it, is long and complicated). So is that basically inevitable—say as a consequence of <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility">computational irreducibility</a>? Or is there some way—perhaps <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">using modern AI</a>—to “humanize” the proof to a point where one can understand it?</p>
<p>It is, I think, an interesting challenge—that gets at the heart of what one can (and can’t) expect to achieve with formalized mathematics. In what follows, I’ll discuss what I’ve been able to figure out—and how it relates to foundational questions about what mathematics is and how it can be done. And while I think I’ve been able to clarify some of the issues, the core problem is still out there—and I’d like to issue it here as a challenge:</p>
<p><span>Challenge:</span> Understand the proof of the Theorem</p>
<p>What do I mean by “understand”? Inevitably, “understand” has to be defined in human terms. Something like “so a human can follow and reproduce it”—and, with luck, feel like saying “aha!” at some point, the kind of way they might on hearing a proof of the Pythagorean theorem (or, in logic, something like de Morgan’s law <tt><a href="http://reference.wolfram.com/language/ref/Not.html">Not</a></tt>[<tt><a href="http://reference.wolfram.com/language/ref/And.html">And</a></tt>[<em>p</em>, <em>q</em>]]<span></span><tt><a href="http://reference.wolfram.com/language/ref/Or.html">Or</a></tt>[<tt><a href="http://reference.wolfram.com/language/ref/Not.html">Not</a></tt>[<em>p</em>], <tt><a href="http://reference.wolfram.com/language/ref/Not.html">Not</a></tt>[<em>q</em>]]). </p>
<p>It should be said that it’s certainly not clear that such an understanding would ever be possible. After all, as we’ll discuss, it’s a basic metamathematical fact that out of all possible theorems almost none have short proofs, at least in terms of any particular way of stating the proofs. But what about an “interesting theorem” like the one we’re considering here? Maybe that’s different. Or maybe, at least, there’s some way of building out a “higher-level mathematical narrative” for a theorem like this that will take one through the proof in human-accessible steps.</p>
<p>In principle one could always imagine a somewhat bizarre scenario in which people would just rote learn chunks of the proof, perhaps giving each chunk some name (a bit like how people learned bArbArA and cElArEnt syllogisms in the Middle Ages). And in terms of these chunks there’d presumably then be a “human way” to talk about the proof. But learning the chunks—other than as some kind of recreational or devotional activity—doesn’t seem to make much sense unless there’s metamathematical structure that somehow connects the chunks to “general concepts” that are widely useful elsewhere. </p>
<p>But of course it’s still conceivable that there might be a “big theory” that would lead us to the theorem in an “understandable way”. And that could be a traditional mathematical theory, built up with precise, if potentially very abstract, constructs. But what about something more like a theory in natural science? In which we might treat our automatically generated proof as an object for empirical study—exploring its characteristics, trying to get intuition about it, and ultimately trying to deduce the analog of “natural laws” that give us a “human-level” way of understanding it. </p>
<p>Of course, for many purposes it doesn’t really matter why the theorem is true. All that matters is that it is true, and that one can deduce things on the basis of it. But as one thinks about the future of mathematics, and the future of doing mathematics, it’s interesting to explore to what extent it might or might not ultimately be possible to understand in a human-accessible way the kind of seemingly alien result that the theorem represents. </p>
<h2 id="the-proof-as-we-know-it">The Proof as We Know It</h2>
<p>I first presented a version of the proof on <a href="https://www.wolframscience.com/nks/p810--implications-for-mathematics-and-its-foundations/">two pages</a> of my 2002 book <em><a href="https://www.wolframscience.com/nks/">A New Kind of Science</a></em>, printing it in 4-point type to make it fit: </p>
<p><a href="https://files.wolframcdn.com/pub/www.wolframscience.com/nks/nks-ch12-sec9.pdf"><img src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg1.png" alt="Axiom proof" title="Axiom proof" width="619" height="373"></a></p>
<p>Today, generating a very similar proof is a one-liner in Wolfram Language (as we’ll discuss below, the · dot here can be thought of as representing the <tt><a href="http://reference.wolfram.com/language/ref/Nand.html">Nand</a></tt> operation):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg2.png" alt="" title="" width="455" height="100"> </p>
</div>
<p>The proof involves 307 (mostly rather elaborate) steps. And here’s one page of it (out of about 30)—presented in the form of a computable Wolfram Language dataset:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg4.png" alt="Example proof steps page" title="Example proof steps page" width="619" height="385"></p>
</div>
<p>What’s the basic idea of this proof? Essentially it’s to perform a sequence of purely structural symbolic operations that go from our axiom to <a href="https://www.wolframscience.com/nks/p808--implications-for-mathematics-and-its-foundations/">known axioms of Boolean algebra</a>. And the proof does this by proving a series of lemmas which can be combined to eventually give what we want: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg5A.png" alt="" title="" width="689" height="1012"> </p>
</div>
<p>The highlighted “targets” here are the standard Sheffer axioms for Boolean algebra from 1913:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg6.png" alt="" title="" width="278" height="60"> </p>
</div>
<p>And, yes, even though these are quite short, the intermediate lemmas involved in the proof get quite long—the longest involving 60 symbols (i.e. having <tt><a href="http://reference.wolfram.com/language/ref/LeafCount.html">LeafCount</a></tt> 60):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg7.png" alt="" title="" width="488" height="40"> </p>
</div>
<p>It’s as if to get to where it’s going, the proof ends up having to go through the wilds of metamathematical space. And indeed one gets a sense of this if one plots the sizes (i.e. <tt><a href="http://reference.wolfram.com/language/ref/LeafCount.html">LeafCount</a></tt>) of successive lemmas:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg8.png" alt="" title="" width="674" height="142"> </p>
</div>
<p>Here’s the distribution of these sizes, showing that while they’re often small, there’s a long tail (note, by the way, that if dot · appears <em>n</em> times in a lemma, the <tt><a href="http://reference.wolfram.com/language/ref/LeafCount.html">LeafCount</a></tt> will be 2<em>n</em> + 3):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg9.png" alt="" title="" width="259" height="122"> </p>
</div>
<p>So how are these lemmas related? Here’s a graph of their interdependence (with the size of each dot being proportional to the size of the lemma it represents):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg10.png" alt="" title="" width="565" height="710"> </p>
</div>
<p>Zooming in on the top we see more detail:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg11.png" alt="" title="" width="595" height="375"> </p>
</div>
<p>We start from our axiom, then derive a whole sequence of lemmas—as we’ll see later, always <a href="https://www.wolframscience.com/metamathematics/proofs-in-accumulative-systems/">combining two lemmas to create a new one</a>. (And, yes, we could equally well call these things theorems—but we generate so many of them it seems more natural to call them “lemmas”.) </p>
<p>So, OK, we’ve got a complicated proof. But how can we check that it’s correct? Well, from the symbolic representation of the proof in the Wolfram Language we can immediately generate a “proof function” that in effect contains executable versions of all the lemmas—implemented using simple structural operations:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofCLOUDAimg11A.png" alt="" title="" width="551" height="453"> </p>
</div>
<p>And when you run this function, it applies all these lemmas and checks that the result comes out right:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg14.png" alt="" title="" width="259" height="43"> </p>
</div>
<p>And, yes, this is basically what one would do in a proof assistant system (like <a href="https://lean-lang.org/" target="_blank" rel="noopener">Lean</a> or <a href="https://us.metamath.org/index.html" target="_blank" rel="noopener">Metamath</a>)—except that here the steps in the proof were generated purely automatically, without any human guidance (or effort). And, by the way, the fact that we can readily translate our symbolic proof representation into a function that we can run provides an operational manifestation of the equivalence between proofs and programs. </p>
<p>But let’s look back at our lemma-interdependence “proof graph”. One notable feature is that we see several nodes with high out-degree—corresponding to what we can think of as “pivotal lemmas” from which many other lemmas end up directly being proved. So here’s a list of the “most pivotal” lemmas in our proof:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg15.png" alt="" title="" width="329" height="178"> </p>
</div>
<p>Or, more graphically, here are the results for all lemmas that occur:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg16.png" alt="" title="" width="344" height="239"> </p>
</div>
<p>So what are the “pivotal lemmas”? <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em> we readily recognize as commutativity. But the others—despite their comparative simplicity—don’t seem to correspond to things that have specifically shown up before in the mathematical literature (or, as we’ll <a href="https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/#llms-to-the-rescue">discuss later</a>, that’s at least what the current generation of LLMs tell us).</p>
<p>But looking at our proof graph something we can conclude is that a large fraction of the “heavy lifting” needed for the whole proof has already happened by the time we can prove <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>. So, for the sake of avoiding at least some of hairy detail in the full proof, in most of what follows, we’ll concentrate on the proof of <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>—which <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> tells us we can accomplish in 104 steps, with a proof graph of the form</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg17.png" alt="" title="" width="666" height="824"> </p>
</div>
<p>with the sizes of successive lemmas (in what is basically a breadth-first traversal of the proof graph) being:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg18A.png" alt="" title="" width="509" height="117"> </p>
</div>
<h2 id="the-machine-code-of-the-proof">The “Machine Code” of the Proof</h2>
<p>It’s already obvious from the previous section that the proof as we currently know it is long, complicated, and fiddly—and in many ways reminiscent of something at a “machine-code” level. But to get a grounded sense of what’s going on in the proof, it’s useful to dive into the details—even if, yes, they can be seriously hard to wrap one’s head around. </p>
<p>At a fundamental level, the way the proof—say of <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>—works is by starting from our axiom, and then progressively deducing new lemmas from pairs of existing lemmas. In the simplest case, that deduction works by <a href="https://www.wolframscience.com/metamathematics/the-metamodeling-of-axiomatic-mathematics/">straightforward symbolic substitution</a>. So, for example, let’s say we have the lemmas</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg1.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>and </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg2.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>Then it turns out that from these lemmas we can deduce:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg3.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>Or, in other words, knowing that the first two lemmas hold for any <em>a</em> gives us enough information about · that the third lemma must inevitably also hold. So how do we derive this?</p>
<p>Our lemmas in effect <a href="https://www.wolframscience.com/metamathematics/rules-applied-to-rules/">define two-way equivalences</a>: their left-hand sides are defined as equal to their right-hand sides, which means that if we see an expression that (structurally) matches one side of a lemma, we can always replace it by the other side of the lemma. And to implement this, we can write our second lemma explicitly as a rule—where to avoid confusion we’re using <em>x</em> rather than <em>a</em>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg4.png" alt="" title="" width="141" height="14"> </p>
</div>
<p>But if we now look at our first lemma, we see that there’s part of it (indicated with a frame) that matches the left-hand side of our rule:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg5.png" alt="" title="" width="132" height="25"> </p>
</div>
<p>If we replace this part (which is at position {2,2}) using our rule we then get</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg6.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>which is precisely the lemma we wanted to deduce. </p>
<p>We can summarize what happened here as a fragment of our proof graph—in which a “substitution event” node takes our first two lemmas as input, and “outputs” our final lemma:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg1.png" alt="" title="" width="277" height="99"> </p>
</div>
<p>As always, the symbolic expressions we’re working with here can be represented as trees:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg8.png" alt="" title="" width="391" height="158"> </p>
</div>
<p>The substitution event then corresponds to a tree rewriting:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg2.png" alt="" title="" width="275" height="242"> </p>
</div>
<p>The <a href="https://www.wolframscience.com/metamathematics/relations-to-automated-theorem-proving/">essence of automated theorem proving</a> is to find a particular sequence of substitutions etc. that get us from whatever axioms or lemmas we’re starting with, to whatever lemmas or theorems we want to reach. Or in effect to find a suitable “path” through the multiway graph of all possible substitutions etc. that can be made. </p>
<p>So, for example, in the particular case we’re considering here, this is the graph that represents all possible transformations that can occur through a single substitution event:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg10.png" alt="" title="" width="582" height="285"> </p>
</div>
<p>The particular transformation (or “path”) we’ve used to prove <em>a</em> · <em>a</em> = <em>a</em> · ((<em>a</em> · <em>a</em>) · <em>a</em>) is highlighted. But as we can see, there are many other possible lemmas that can be generated, or in other words that can be proved from the two lemmas we’ve given as input. Put another way, we can think of our input lemmas as implying or entailing all the other lemmas shown here. And, by analogy to the concept of a light cone in physics, we can view the collection of everything entailed by given lemmas or given events as the (future) “<a href="https://www.wolframscience.com/metamathematics/metamathematical-space/#p-28">entailment cone</a>” of those lemmas or events. A proof that reaches a particular lemma is then effectively a path in this entailment cone—analogous in physics to a world line that reaches a particular spacetime point.</p>
<p>If we continue building out the entailment cone from our original lemmas, then after two (substitution) events we get:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg11.png" alt="" title="" width="701" height="454"> </p>
</div>
<p>There are 49 lemmas generated here. But it turns out that beyond the lemma we already discussed there are only three (highlighted here) that appear in the proof we are studying here:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg12.png" alt="" title="" width="281" height="60"> </p>
</div>
<p>And indeed the <a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#the-mechanics-of-proof">main algorithmic challenge of theorem proving</a> is to figure out which lemmas to generate in order to get a path to the theorem one’s trying to prove. And, yes, as we’ll discuss later, there are typically many paths that will work, and different algorithms will yield different paths and therefore different proofs.</p>
<p>But, OK, seeing how new lemmas can be derived from old by substitution is already quite complicated. But actually there’s something even more complicated we need to discuss: deriving lemmas not only by substitution but also by what we’ve called <a href="https://www.wolframscience.com/metamathematics/beyond-substitution-cosubstitution-and-bisubstitution/">bisubstitution</a>. </p>
<p>We can think of both substitution and bisubstitution as turning one lemma X == Y into a transformation rule (either X <img src="https://content.wolfram.com/uploads/sites/32/2022/10/rightarrow2.png" width="15" height="11"> Y or Y <img src="https://content.wolfram.com/uploads/sites/32/2022/10/rightarrow2.png" width="15" height="11"> X), and then applying this rule to another lemma, to derive a new lemma. In ordinary substitution, the left-hand side of the rule directly matches (in a Wolfram Language pattern-matching sense) a subexpression in the lemma we’re transforming. But the key point is that all the variables that appear in both our lemmas are really “pattern variables” (<tt>x_</tt> etc. in Wolfram Language). So that means there’s another way that one lemma can transform another, in which in effect replacements are made not only in the lemma being transformed, but also in the lemma that’s doing the transforming. </p>
<p>The net effect, though, is still to take two lemmas and derive another, as in:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg3.png" alt="" title="" width="414" height="130"> </p>
</div>
<p>But in tracing through the details of our proof, we need to distinguish “substitution events” (shown yellowish) from “bisubstitution” ones (shown reddish). (In <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> in Wolfram Language, lemmas produced by ordinary substitution are called “substitution lemmas”, while lemmas produced by bisubstitution are called “critical pair lemmas”.)</p>
<p>OK, so how does bisubstitution work? Let’s look at an example. We’re going to be transforming the lemma </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg14.png" alt="" title="" width="235" height="14"> </p>
</div>
<p>using the lemma (which in this case happens to be our original axiom)</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg15.png" alt="" title="" width="183" height="14"> </p>
</div>
<p>to derive the new lemma:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg16.png" alt="" title="" width="235" height="14"> </p>
</div>
<p>We start by creating a rule from the second lemma. In this case, the rule we need happens to be reversed relative to the way we wrote the lemma, and this means that (in the canonical form we’re using) it’s convenient to rename the variables that appear:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg17.png" alt="" title="" width="237" height="14"> </p>
</div>
<p>To do our bisubstitution we’re going to apply this rule to a subterm of our first lemma. We can write that first lemma with explicit pattern variables:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg18.png" alt="" title="" width="311" height="14"> </p>
</div>
<p>As always, the particular names of those variables don’t matter. And to avoid confusion, we’re going to rename them:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg19.png" alt="" title="" width="306" height="14"> </p>
</div>
<p>Now look at this subterm of this lemma (which is part {2,1,1,2} of the expression):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg20.png" alt="" title="" width="102" height="14"> </p>
</div>
<p>It turns out that with appropriate bindings for pattern variables this can be matched (or “unified”) with the left-hand side of our rule. This provides a way to find such bindings:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg21.png" alt="" title="" width="345" height="14"> </p>
</div>
<p>(Note that in these bindings things like c_ stand only for explicit expressions, like c_, not for expressions that the ordinary Wolfram Language pattern <tt>c_</tt> would match.)</p>
<p>Now if we apply the bindings we’ve found to the left-hand side of our rule</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg22.png" alt="" title="" width="277" height="14"> </p>
</div>
<p>and to the subterm we picked out from our lemma</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg23.png" alt="" title="" width="277" height="14"> </p>
</div>
<p>we see that we get the same expression. Which means that with these bindings the subterm matches the left-hand side of our rule, and we can therefore replace this subterm with the right-hand side of the rule. To see all this in operation, we first apply the bindings we’ve found to the lemma we’re going to transform (and, as it happens, the binding for y_ is the only one that matters here):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg24.png" alt="" title="" width="615" height="14"> </p>
</div>
<p>Now we take this form and apply the rule at the position of the subterm we identified:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg25.png" alt="" title="" width="342" height="14"> </p>
</div>
<p>Renaming variables</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg26.png" alt="" title="" width="345" height="14"> </p>
</div>
<p> we now finally get exactly the lemma that we were trying to derive:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg27.png" alt="" title="" width="235" height="14"> </p>
</div>
<p>And, yes, getting here was a pretty complicated process. But with the symbolic character of our lemmas, it’s one that is inevitably possible, and so can be used in our proof. And in the end, out of the 101 lemmas used in the proof, 47 were derived by ordinary substitution, while 54 were derived by bisubstitution.</p>
<p>And indeed the first few steps of the proof turn out to use only bisubstituion. An example is the first step—which effectively applies the original axiom to itself using bisubsitution:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg4.png" alt="" title="" width="243" height="149"> </p>
</div>
<p>And, yes, even this very first step is pretty difficult to follow. </p>
<p>If we start from the original axiom, there are 16 lemmas that can be derived purely by a single ordinary substitution (effectively of the axiom into itself)—resulting in the following entailment cone:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg29.png" alt="" title="" width="693" height="323"> </p>
</div>
<p>As it happens, though, none of the 16 new lemmas here actually get used in our proof. On the other hand, in the bisubstitution entailment cone</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg30.png" alt="" title="" width="693" height="251"> </p>
</div>
<p>there are 27 new lemmas, and 4 of them get used in the proof—as we can see from the first level of the proof graph (here rotated for easier rendering):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg31.png" alt="" title="" width="657" height="101"> </p>
</div>
<p>At the next level of the entailment cone from ordinary substitution, there are 5153 new lemmas—none of which get used in the proof. But of the 23215 new lemmas in the (pure) bisubstitution entailment cone, 5 do get used:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg32.png" alt="" title="" width="663" height="112"> </p>
</div>
<p>At the next level, lemmas generated by ordinary substitution also start to get used:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg33.png" alt="" title="" width="657" height="126"> </p>
</div>
<p>Here’s another rendering of these first few levels of the proof graph:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg34.png" alt="" title="" width="295" height="198"> </p>
</div>
<p>Going to another couple of levels we’re starting to see quite a few independent chains of lemmas developing</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg35.png" alt="" title="" width="454" height="289"> </p>
</div>
<p>which eventually join up when we assemble the whole proof graph:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg36.png" alt="" title="" width="605" height="631"> </p>
</div>
<p>A notable feature of this proof graph is that it has more bisubstitution events at the top, and more ordinary substitution events at the bottom. So why is that? Essentially it seems to be because bisubstitution events tend to produce larger lemmas, and ordinary substitution events tend to produce smaller ones—as we can see if we plot input and output lemma sizes for all events in the proof:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg37.png" alt="" title="" width="451" height="479"> </p>
</div>
<p>So in effect what seems to be happening is that the proof first has to “spread out in <a href="https://www.wolframscience.com/metamathematics/metamathematical-space/">metamathematical space</a>”, using bisubstitution to generate large lemmas “far out in metamathematical space”. Then later the proof has to “corral things back in”, using ordinary substitution to generate smaller lemmas. And for example, at the very end, it’s a substitution event that yields the final theorem we’re trying to prove:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg38.png" alt="" title="" width="277" height="100"> </p>
</div>
<p>And earlier in the graph, there’s a similar “collapse” to a small (and rather pivotal) lemma:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025machineCLOUDimg39.png" alt="" title="" width="406" height="120"> </p>
</div>
<p>As the plot above indicates, ordinary substitution can lead to large lemmas, and indeed bisubstitution can also lead to smaller ones, as in</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025machineCLOUDimg40.png" alt="" title="" width="367" height="87"> </p>
</div>
<p>or slightly more dramatically:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025machineCLOUDimg41.png" alt="" title="" width="630" height="140"> </p>
</div>
<p>But, OK, so this is some of what’s going on at a “machine-code” level inside the proof we have. Of course, given our axiom and the operations of substitution and bisubstitution there are inevitably a huge number of different possible proofs that could be given. The particular proof we’re considering is what the Wolfram Language <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> gives. (In the Appendix, we’ll also look at results from some other automated theorem proving systems. The results will be very comparable, if usually a little lengthier.) </p>
<p>We won’t discuss the detailed (and rather elaborate) algorithms inside <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt>. But fundamentally what they’re doing is to try constructing certain lemmas, then to find sequences of lemmas that eventually form a “path” to what we’re trying to prove. And as some indication of what’s involved in this, here’s a plot of the number of “candidate lemmas” that are being maintained as possible when different lemmas in the proof are generated:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg42.png" alt="" title="" width="354" height="145"> </p>
</div>
<p>And, yes, for a while there’s roughly exponential growth, leveling off at just over a million when we get to the “pulling everything together” stage of the proof.</p>
<h2 id="unrolling-the-proof">Unrolling the Proof</h2>
<p>In what we’ve done so far, we’ve viewed our proof as working by starting from an axiom, then <a href="https://www.wolframscience.com/nks/notes-12-9--proof-structures/">progressively building up lemmas</a>, until eventually we get to the theorem we want. But there’s an alternative view that’s in some ways useful in getting a more direct, “mechanical” intuition about what’s going on in the proof.</p>
<p>Let’s say we’re trying to prove that our axiom implies that <em>p</em> · <em>q</em> = <em>q</em> · <em>p</em>. Well, then there must be some way to start from the expression <em>p</em> · <em>q</em> and just keep on judiciously applying the axiom until eventually we get to the expression <em>q</em> · <em>p</em>. And, yes, the number of axiom application steps required might be very large. But ultimately, if it’s true that the axiom implies <em>p</em> · <em>q</em> = <em>q</em> · <em>p</em> there must be a path that gets from <em>p</em> · <em>q</em> to <em>q</em> · <em>p</em>.</p>
<p>But before considering the case of our full proof, let’s start with something simpler. Let’s assume that we’ve already established the lemmas:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg1.png" alt="" title="" width="121" height="37"> </p>
</div>
<p>Then we can treat them as axioms, and ask a question like whether they imply the lemma</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg2.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>or, in our current approach, whether they can be used to form a path from <em>a</em> · <em>a</em> to <em>a</em> · (<em>a</em> · (<em>a</em> · <em>a</em>)). </p>
<p>Well, it’s not too hard to see that in fact there is such a path. Apply our second lemma to <em>a</em> · <em>a</em> to get:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg3.png" alt="" title="" width="77" height="14"> </p>
</div>
<p>But now this subterm</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg4.png" alt="" title="" width="88" height="25"> </p>
</div>
<p>matches the left-hand of the first lemma, so that it can be replaced by the right-hand side of that lemma (i.e. by <em>a</em> · (<em>a</em> · <em>a</em>)), giving in the end the desired <em>a</em> · (<em>a</em> · (<em>a</em> · <em>a</em>)).</p>
<p>So now we can summarize this process as:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg5.png" alt="" title="" width="252" height="138"> </p>
</div>
<p>In what follows, it’ll be convenient to label lemmas. We’ll call our original axiom A1, we’ll call our successive lemmas generated by ordinary substitution S<em>n</em> and the ones generated by bisubsitution B<em>n:</em></p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg6.png" alt="" title="" width="399" height="621"> </p>
</div>
<p>In our proof we’ll also use <img src="https://content.wolfram.com/sites/43/2025/01/rightgreenarrow.png" width="18" height="25"> and <img src="https://content.wolfram.com/sites/43/2025/01/leftpinkarrow.png" width="18" height="25"> to indicate whether we’re going to use the lemma (say <nobr>X = Y)</nobr> in the “forward direction” X <img src="https://content.wolfram.com/uploads/sites/32/2022/10/rightarrow2.png" width="15" height="11"> Y or the “reverse direction” X <img src="https://content.wolfram.com/uploads/sites/32/2022/10/leftarrow.png" width="15" height="11"> Y. And with this labeling, the proof we just gave (which is for the lemma S23) becomes:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg7.png" alt="" title="" width="168" height="138"> </p>
</div>
<p>Each step here is a pure substitution, and requires no replacement in the rule (i.e. “axiom”) being used. But proofs like this can also be done with bisubstitution, where replacements are applied to the rule to get it in a form where it can directly be applied to transform an expression:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg8.png" alt="" title="" width="529" height="136"> </p>
</div>
<p>OK, so how about the first lemma in our full proof? Here’s a proof that its left-hand side can be transformed to its right-hand side just by judiciously applying the original axiom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg9.png" alt="" title="" width="504" height="140"> </p>
</div>
<p>Here’s a corresponding proof for the second lemma:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg10.png" alt="" title="" width="531" height="136"> </p>
</div>
<p>Both these involve bisubstitution. Here’s a proof of the first lemma derived purely by ordinary substitution:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg11.png" alt="" title="" width="611" height="136"> </p>
</div>
<p>This proof is using not only the original axiom but also the lemma B5. Meanwhile, B5 can be proved using the original axiom together with B2:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg12.png" alt="" title="" width="693" height="181"> </p>
</div>
<p>But now, inserting the proof we just gave above for B2, we can give a proof of B5 just in terms of the original axiom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg13.png" alt="" title="" width="693" height="278"> </p>
</div>
<p>And recursively continuing this unrolling process, we can then prove S1 purely using the original axiom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg14.png" alt="" title="" width="693" height="329"> </p>
</div>
<p>What about the whole proof? Well, at the very end we have:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg15.png" alt="" title="" width="222" height="137"> </p>
</div>
<p>If we “unroll” one step we have</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg16.png" alt="" title="" width="275" height="349"> </p>
</div>
<p>and after 2 steps:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg17.png" alt="" title="" width="435" height="444"> </p>
</div>
<p>In principle we could go on with this unrolling, in effect recursively replacing each rule by the sequence of transformations that represents its proof. Typically this process will, however, generate exponentially longer proof sequences. But say for lemma S5</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg18.png" alt="" title="" width="334" height="14"> </p>
</div>
<p>the result is still very easily manageable:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg19.png" alt="" title="" width="670" height="274"> </p>
</div>
<p>We can summarize this result by in effect plotting the sizes of the intermediate expressions involved—and indicating what part of each expression is replaced at each step (with <img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/01082025redbox.png" alt="" title="" width="15" height="15"> as above indicating “forward” use of the axiom A1 <img src="https://content.wolfram.com/uploads/sites/32/2022/10/rightarrow2.png" width="15" height="11"> and <img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/greenbox.png" '="" title="" width="15" height="15"> “backward” A1 <img src="https://content.wolfram.com/uploads/sites/32/2022/10/leftarrow.png" width="15" height="11">):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg20.png" alt="" title="" width="357" height="160"> </p>
</div>
<p>For lemma B33</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg21.png" alt="" title="" width="681" height="14"> </p>
</div>
<p>the unrolled proof is now 30 steps long</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg22.png" alt="" title="" width="357" height="160"> </p>
</div>
<p>while for lemma S11</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg23.png" alt="" title="" width="467" height="14"> </p>
</div>
<p>the unrolled proof is 88 steps long:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg24.png" alt="" title="" width="412" height="182"> </p>
</div>
<p>But here there is a new subtlety. Doing a direct substitution of the “proof paths” for the lemmas used to prove S11 in our original proof gives a proof of length 104:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg25.png" alt="" title="" width="466" height="177"> </p>
</div>
<p>But this proof turns out to be repetitive, with the whole gray section going from one copy to another of:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg26.png" alt="" title="" width="237" height="14"> </p>
</div>
<p>As an example of a larger proof, we can consider lemma B47:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg27.png" alt="" title="" width="157" height="14"> </p>
</div>
<p>And despite the simplicity of this lemma, our proof for it is 1008 steps long: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg28.png" alt="" title="" width="609" height="204"> </p>
</div>
<p>If we don’t remove repetitive sections, it’s 6805 steps:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg29.png" alt="" title="" width="460" height="157"> </p>
</div>
<p>Can we unroll the whole proof of <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>? We can get closer by considering lemma S36:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg30.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>Its proof is 27105 steps long:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg31A.png" alt="" title="" width="621" height="204"> </p>
</div>
<p>The distribution of expression sizes follows a roughly exponential distribution, with a maximum of 20107:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg32.png" alt="" title="" width="274" height="122"> </p>
</div>
<p>Plotting the expression sizes on a log scale one gets: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg33.png" alt="" title="" width="409" height="142"> </p>
</div>
<p>And what stands out most here is a kind of recursive structure—which is the result of long sequences that basically represent the analog of “subroutine calls” that go back and repeatedly prove lemmas that are needed.</p>
<p>OK, so what about the whole proof of <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>? Yes, it can be unrolled—in terms of 83,314 applications of the original axiom. The sequence of expression sizes is:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg34.png" alt="" title="" width="571" height="187"> </p>
</div>
<p>Or on a log scale:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg35.png" alt="" title="" width="519" height="175"> </p>
</div>
<p>The distribution of expression sizes now shows clear deviation from being exponential:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg36A.png" alt="" title="" width="359" height="160"> </p>
</div>
<p>The maximum is 63245, which occurs just 81 steps after the exact midpoint of the proof. In other words, in the middle, the proof has wandered incredibly far out in metamathematical space (there are altogether <tt><a href="http://reference.wolfram.com/language/ref/CatalanNumber.html">CatalanNumber</a></tt>[63245] ≈ 10<sup>38178</sup> possible expressions of the size it reaches). </p>
<p>The proof returns to small expressions just a few times; here are all the cases in which the size is below 10:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092024JGCLOUDimg8.png" alt="" title="" width="399" height="274"> </p>
</div>
<p>So, yes, it is possible to completely unroll the proof into a sequence of applications of the original axiom. But if one does this, it inevitably involves repeating lots of work. Being able to use intermediate lemmas in effect lets one “share common subparts” in the proof. So that one ends up with just 104 “rule applications”, rather than 63245. Not that it’s easy to understand those 104 steps…</p>
<h2 id="is-there-a-better-notation">Is There a Better Notation?</h2>
<p> Looking at our proof—either in its original “lemma” form, or in its “unrolled” form—the most striking aspect of it is how complicated (and incomprehensible) it seems to be. But one might wonder whether much of that complexity is just the result of not “using the right notation”. In the end, we’ve got a huge number of expressions written in terms of · operations that we can interpret as <tt><a href="http://reference.wolfram.com/language/ref/Nand.html">Nand</a></tt> (or <tt><a href="http://reference.wolfram.com/language/ref/Nor.html">Nor</a></tt>). And maybe it’s a little like seeing the operation of a microprocessor down at the level of individual gates implementing <tt>Nand</tt>s or <tt>Nor</tt>s. And might there perhaps be an analog of a higher-level representation—with higher-level operations (even like arithmetic) that are more accessible to us humans?</p>
<p>It perhaps doesn’t help that <tt>Nand</tt> itself is a rather non-human construct. For example, not a single natural human language seems to have a word for <tt>Nand</tt>. But there are combinations of <tt>Nand</tt>s that have more <a href="https://www.wolframscience.com/nks/p807--implications-for-mathematics-and-its-foundations/">familiar interpretations</a>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025notationimg1.png" alt="" title="" width="228" height="127"> </p>
</div>
<p>But what combinations actually occur in our proof? Here are the most common subexpressions that appear in lemmas in the proof:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092925NLCLOUDimg12.png" alt="" title="" width="290" height="316"> </p>
</div>
<p>And, yes, we could give the most common of these special names. But it wouldn’t really help in “compressing” the proof—or making it easier to understand.</p>
<p>What about “upgrading” our “laws of inference”, i.e. the way that we can derive new lemmas from old? Perhaps instead of substitution and bisubstitution, which both take two lemmas and produce one more, we could set up more elaborate “tactics” that for example take in more input lemmas. We’ve seen that if we completely unroll the proof, it gets much longer. So perhaps there is a “higher-order” setup that for example dramatically shortens the proof. </p>
<p>One way one might identify this is by seeing commonly repeating structures in the subgraphs that lead to lemmas. But in fact these subgraphs are quite diverse:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025notationimg3.png" alt="" title="" width="589" height="130"> </p>
</div>
<h2 id="what-are-the-popular-lemmas">What Are the Popular Lemmas?</h2>
<p>A typical feature of human-written mathematical proofs is that they’re “anchored” by famous theorems or lemmas. They may have fiddly technical pieces. But usually there’s a backbone of “theorems people know”. </p>
<p>We have the impression that the proof we’re discussing here “spends most of its time wandering around the wilds of metamathematical space”. But perhaps it visits waypoints that are somehow recognizable, or at least should be. Or in other words, perhaps out in the metamathematical space of lemmas there are ones that are somehow sufficiently popular that they’re worth giving names to, and learning—and can then be used as “reference points” in terms of which our proof becomes simpler and more human accessible.</p>
<p>It’s a story very much like what happens with human language. There are things out there in the world, but when there’s a certain category of them that are somehow common or important enough, we make a word for them in our language, which we can then use to “compactly” refer to them. (It’s again the same story when it comes to computational language, and in particular the Wolfram Language, except that in that case it’s been my personal responsibility to come up with the appropriate definitions and names for functions to represent “common lumps of computation”.) </p>
<p>But, OK, so what are the “popular lemmas” of <tt><a href="http://reference.wolfram.com/language/ref/Nand.html">Nand</a></tt> proofs? One way to explore this is to enumerate statements that are “true about <tt>Nand</tt>”—then to look at proofs of these statements (say found with <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> from our axiom) and see what lemmas show up frequently in them. </p>
<p><a href="https://www.wolframscience.com/nks/p818--implications-for-mathematics-and-its-foundations/">Enumerating statements “true about </a><tt><a href="https://www.wolframscience.com/nks/p818--implications-for-mathematics-and-its-foundations/">Nand”</a></tt>, starting from the smallest, we get</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg1.png" alt="" title="" width="598" height="222"> </p>
</div>
<p>where we have highlighted statements from this list that appear as lemmas in our proof.</p>
<p>Proving each of these statements from our original axiom, here are the <a href="https://www.wolframscience.com/nks/notes-12-9--proof-lengths-in-logic/">lengths of proofs we find</a> (for all 1341 distinct theorems with up to <tt><a href="http://reference.wolfram.com/language/ref/LeafCount.html">LeafCount</a></tt> 4 on each side):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg2.png" alt="" title="" width="471" height="204"> </p>
</div>
<p>A histogram shows that it’s basically a bimodal distribution</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg3.png" alt="" title="" width="359" height="134"> </p>
</div>
<p>with the smallest “long-proof” theorem being:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg4.png" alt="" title="" width="173" height="14"> </p>
</div>
<p>In aggregate, all these proofs use about 200,000 lemmas. But only about 1200 of these are distinct. And we can plot which lemmas are used in which proofs—and we see that there are indeed many lemmas that are used across wide ranges of proofs, while there are a few others that are “special” to each proof (the diagonal stripe is associated with lemmas close to the statement being proved):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg5.png" alt="" title="" width="428" height="401"> </p>
</div>
<p>If we rank all distinct lemmas from most frequently to least frequently used, we get the following distribution of lemma usage frequencies across all our proofs: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg6.png" alt="" title="" width="355" height="155"> </p>
</div>
<p>It turns out that there is a “common core” of 49 lemmas that are used in every single one of the proofs. So what are these lemmas? Here’s a plot of the usage frequency of lemmas against their size—with the “common ones” populating the top line: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg7.png" alt="" title="" width="438" height="192"> </p>
</div>
<p>And at first this might seem surprising. We might have expected that short lemmas would be the most frequent, but instead we’re seeing long lemmas that always appear, the very longest being:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg8.png" alt="" title="" width="516" height="40"> </p>
</div>
<p>So why is this? Basically it’s that these long lemmas are being used at the beginning of every proof. They’re the result of applying bisubstitution to the original axiom, and in some sense they seem to be laying down a kind of net in metamathematical space that then allows more diverse—and smaller—lemmas to be derived. </p>
<p>But how are these “common core” popular lemmas distributed within proofs? Here are a few examples:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025lemmasCLOUDXimg9.png" alt="" title="" width="566" height="579"> </p>
</div>
<p>And what we see is that while, yes, the common core lemmas are always at the beginning, they don’t seem to have a uniform way of “plugging into” the rest of the proof. And it doesn’t, for example, seem as if there’s just some small set of (perhaps simple) “waypoint” lemmas that one can introduce that will typically shorten these proofs.</p>
<p>If one effectively allows all the common core lemmas to be used as axioms, then inevitably proofs will be shortened; for example, the proof of <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>—which only ends up using 5 of the common core lemmas—is now shortened to 51 lemmas:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025lemmasCLOUDZimg10B.png" alt="" title="" width="282" height="479"> </p>
</div>
<p>It doesn’t seem to become easier to understand, though. And if it’s unrolled, it’s still 5013 steps. </p>
<p>Still, one can ask what happens if one just introduces particular “recognizable” lemmas as additional axioms. For example, if we include “commutativity” <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em> then we find that, yes, we do manage to <a href="https://www.wolframscience.com/nks/notes-12-9--proof-lengths-in-logic/">reduce the lengths of some proofs</a>, but certainly not all:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg11.png" alt="" title="" width="554" height="280"> </p>
</div>
<p>Are there any other “pivotal” lemmas we could add? In particular, what about lemmas that can help with the length-200 or more proofs? It turns out that all of these proofs involve the lemma: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg12.png" alt="" title="" width="130" height="14"> </p>
</div>
<p>So what happens if we add this? Well, it definitely reduces proof lengths:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg13.png" alt="" title="" width="607" height="316"> </p>
</div>
<p>And sometimes it even seems like it brings proofs into “human range”. For example, a proof of</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg14.png" alt="" title="" width="104" height="14"> </p>
</div>
<p>from our original axiom has length 56. Adding in commutativity reduces it to length 18. And adding our third lemma reduces it to just length 9—and makes it not even depend directly on the original axiom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025lemmasCLOUDZimg15.png" alt="" title="" width="272" height="249"> </p>
</div>
<p>But despite the apparent simplicity here, the steps involved—particularly when bisubstitution is used—are remarkably hard to follow. (Note the use of <em>a </em>= <em>a</em> as a kind of “implicit axiom”—something that has actually also appeared, without comment, in many of our other proofs.)</p>
<h2 id="can-we-get-a-shorter-proof">Can We Get a Shorter Proof?</h2>
<p>The proof that we’ve been studying can be seen in some ways as a rather arbitrary artifact. It’s the output of <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt>, with all its specific detailed internal algorithms and choices. In the Appendix, we’ll see that other automated theorem proving systems give very similar results. But we still might wonder whether actually the complexity of the proof as we’ve been studying it is just a consequence of the details of our automated theorem proving—and that in fact there’s a much shorter (and perhaps easier to understand) proof that exists.</p>
<p>One approach we could take—reminiscent of higher category theory—is to think about just simplifying the proof we have, effectively using proof-to-proof transformations. And, yes, this is technically difficult, though it doesn’t seem impossible. But what if there are <a href="https://www.wolframscience.com/metamathematics/the-topology-of-proof-space/">“holes” in proof space</a>? Then a “continuous deformation” of one proof into another will get stuck, and even if there is a much shorter proof, we’re liable to get “topologically stuck” before we find it.</p>
<p>One way to be sure we’re getting the shortest proof of a particular lemma is to explicitly find the first place that lemma appears in the (future) entailment cone of our original axiom. For example, as we saw above, a single substitution event leads to the entailment cone:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg1.png" alt="" title="" width="645" height="285"> </p>
</div>
<p>Every lemma produced here is, by construction, in principle derivable by a proof involving a single substitution event. But if we actually use <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> to prove these lemmas, the proofs we get most involve 2 events (and in one case 4):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025shorterAimg2.png" alt="" title="" width="645" height="75"> </p>
</div>
<p>If we take another step in the entailment cone, we get a total of 5151 lemmas. From the way we generated them, we know that all these lemmas can in principle be reached by proofs of length 2. But if we run <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> on them, we find a distribution of proof lengths:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg3.png" alt="" title="" width="314" height="141"> </p>
</div>
<p>And, yes, there is one lemma (with <tt><a href="http://reference.wolfram.com/language/ref/LeafCount.html">LeafCount</a></tt> 183) that is found only by a proof of length 14. But most often the proof length is 4—or about double what it could be. </p>
<p>If we generate the entailment cone for lemmas using bisubstitution rather than just ordinary substitution, there are slightly more cases where <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> does worse at getting minimal proofs. </p>
<p>For example, the lemma</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg4.png" alt="" title="" width="671" height="14"> </p>
</div>
<p>and 3 others can be generated by a single bisubstitution from the original axiom, but <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> gives only proofs of length 4 for all of these.</p>
<p>What about unrolled proofs, in which one can generate an entailment cone by starting from a particular expression, and then applying the original axiom in all possible ways? For example, let’s say we start with:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg5.png" alt="" title="" width="77" height="14"> </p>
</div>
<p>Then applying bisubstitution with the original axiom once in all possible ways gives:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg6.png" alt="" title="" width="601" height="186"> </p>
</div>
<p>Applying bisubstitution a second time gives a larger entailment cone: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025shorterCimg7.png" alt="" title="" width="451" height="406"> </p>
</div>
<p>But now it turns out that—as indicated—one of the expressions in this cone is: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg8.png" alt="" title="" width="262" height="14"> </p>
</div>
<p>So this shows that the lemma</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg9.png" alt="" title="" width="359" height="14"> </p>
</div>
<p>can in principle be reached with just two steps of “unrolled” proof:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg10.png" alt="" title="" width="441" height="41"> </p>
</div>
<p>And in this particular case, if we use <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> and then unroll the resulting proof we also get a proof of length 3—but it goes through a different intermediate expression:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg11.png" alt="" title="" width="542" height="41"> </p>
</div>
<p>As it happens, this intermediate expression is also reached in the entailment cone that we get by starting from our “output” expression and then applying two bisubsitutions:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025shorterEimg12-min.png" alt="" title="" width="528" height="417"> </p>
</div>
<h2 id="what-actually-is-the--models-and-the-proof">What Actually Is the “·”? Models and the Proof</h2>
<p>We can think of logic (or Boolean algebra) as being associated with a certain collection of theorems. And what our axiom does is to provide something from which all theorems of logic (and nothing but theorems of logic) can be derived. At some level, we can think of it as just being about symbolic expressions. But in our effort to understand what’s going on—say with our proof—it’s sometimes useful to ask how we can “concretely” interpret these expressions.</p>
<p>For example, we might ask what the · operator actually is. And what kinds of things can our symbolic variables be? In effect we’re asking for what in model theory are called <a href="https://www.wolframscience.com/metamathematics/the-model-theoretic-perspective/">“models” of our axiom system</a>. And in aligning with logic the most obvious model to discuss is one in which variables can be <tt><a href="http://reference.wolfram.com/language/ref/True.html">True</a></tt> or <tt><a href="http://reference.wolfram.com/language/ref/False.html">False</a></tt>, and the · represents either the logical operator <tt><a href="http://reference.wolfram.com/language/ref/Nand.html">Nand</a></tt> or the logical operator <tt><a href="http://reference.wolfram.com/language/ref/Nor.html">Nor</a></tt>.</p>
<p>The truth table, say for <tt>Nand</tt>, is:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025actuallyimg1.png" alt="" title="" width="156" height="152"> </p>
</div>
<p>And as expected, with this model for ·, we can confirm that our original axiom holds:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025actuallyimg2.png" alt="" title="" width="379" height="274"> </p>
</div>
<p>In general, though, our original axiom allows two size-2 models (that we can interpret as <tt>Nand</tt> and <tt>Nor</tt>):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025actuallyimg4.png" alt="" title="" width="98" height="52"> </p>
</div>
<p>It allows no size-3 models, and in fact in general <a href="https://www.wolframscience.com/nks/notes-12-9--operators-on-sets/">allows only models of size 2<sup><em>n</em></sup></a>; for example, for size 4 its models are:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025actuallyimg6.png" alt="" title="" width="551" height="210"> </p>
</div>
<p>So what about <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>? What models does it allow? For size 2, it’s all 8 possible models with symmetric “multiplication tables”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025actuallyimg7.png" alt="" title="" width="467" height="52"> </p>
</div>
<p>But the crucial point is that the 2 models for our original axiom system are part of these. In other words, at least for size-2 models, satisfying the original axiom system implies satisfying <nobr><em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>.</nobr></p>
<p>And indeed any lemma derived from our axiom system must allow the models associated with our original axiom system. But it may also allow more—and sometimes many more. So here’s a map of our proof, showing how many models (out of 16 possible) each lemma allows:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg5.png" alt="" title="" width="329" height="674"> </p>
</div>
<p>Here are the results for size-3 models:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg6.png" alt="" title="" width="478" height="972"> </p>
</div>
<p>And, once again, these look complicated. We can think of models as defining—in some sense—<a href="https://www.wolframscience.com/nks/p804--implications-for-mathematics-and-its-foundations/">what lemmas are “about”</a>. So, for example, our original axiom is “about” <tt>Nand</tt> and <tt>Nor</tt>. The lemma <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em> is “about” symmetric functions. And so on. And we might have hoped that we could gain some understanding of our proof by looking at how different lemmas that occur in it “sculpt” what is being talked about. But in fact we just seem to end up with complicated descriptions of sets that don’t seem to have any obvious relationship with each other.</p>
<h2 id="what-about-a-higher-level-abstraction">What about a Higher-Level Abstraction?</h2>
<p>If there’s one thing that stands out about our proof—and the analysis we’ve given of it here—it’s how fiddly and “in the weeds” it seems to be. But is that because we’re missing some big picture? Is there actually a more abstract way of discussing things, that gets to our result without having to go through all the details? </p>
<p>In the history of mathematics many of the most important themes have been precisely about finding such higher-level abstractions. We could start from the <a href="https://www.wolframscience.com/nks/notes-12-9--groups-and-axioms/">explicit symbolic axioms</a></p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025abstractionimg1.png" alt="" title="" width="120" height="59"> </p>
</div>
<p>or even</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025abstractionimg2.png" alt="" title="" width="165" height="22"> </p>
</div>
<p>and start building up theorems much as we’ve done here. Or we could recognize that these are axioms for group theory, and then start using the abstract ideas of group theory to derive our theorems.</p>
<p>So is there some higher-level version of what we’re discussing here? Remember that the issue is not about the overall structure of Boolean algebra; rather it’s about the more metamathematical question of how one can prove that all of Boolean algebra can be generated from the axiom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025abstractionimg3.png" alt="" title="" width="183" height="14"> </p>
</div>
<p>In the last few sections we’ve tried a few semi-empirical approaches to finding higher-level representations. But they haven’t gotten very far. And to get further we’re probably going to need a serious new idea.</p>
<p>And, if history is a guide, we’re going to need to come up with an abstraction that somehow “goes outside of the system” before “coming back”. It’s like trying to figure out the real roots of a cubic equation, and realizing that the best way to do this is to introduce complex numbers, even though the imaginary parts will cancel at the end. </p>
<p>In the direct exploration of our proof, it feels as if the intermediate lemmas we generate “wander off into the wilds of metamathematical space” before coming back to establish our final result. And if we were using a higher-level abstraction, we’d instead be “wandering off” into the space of that abstraction. But what we might hope is that—at least with the concepts we would use in discussing that abstraction—the path that would be involved would be “short enough to be accessible to human understanding”.</p>
<p>Will we be able to find such an abstraction? It’s a subtle question. Because in effect it asks whether we can reduce the computational effort needed for the proof—or, in other words, whether we can find a pocket of computational reducibility in what in general will be a computationally irreducible process. But it’s not a question that can really be answered just for our specific proof on it own. After all, our “abstraction” could in principle just involve introducing a primitive that represents our whole proof or a large part of it. But to make it what we can think of as a real abstraction we need something that spans many different specific examples—and, in our case, likely many axiomatic systems or symbolic proofs.</p>
<p>So is such an abstraction possible? In the history of mathematics the experience has been that after enough time (often measured in centuries) has passed, abstractions tend to be found. But at some level this has been self fulfilling. Because the areas that are considered to have remained “interesting for mathematics” tend to be just those where general abstractions have in fact been found. </p>
<p>In <a href="https://writings.stephenwolfram.com/2021/09/charting-a-course-for-complexity-metamodeling-ruliology-and-more/">ruliology</a>, though, the typical experience has been different. Because there it’s been routine to <a href="https://www.wolframscience.com/nks/">sample the computational universe of possible simple programs</a> and encounter computational irreducibility. In the end it’s still inevitable that among the computational irreducibility there must be pockets of computational reducibility. But the issue is that these pockets of computational reducibility may not involve features of our system that we care about. </p>
<p>So is a proof of the kind we’re discussing here more like ruliology, or more like “typical mathematics”? Insofar as it’s a mathematical-style proof of a mathematical statement it feels more like typical mathematics. But insofar as it’s something found by the computational process of automated theorem proving it perhaps seems more ruliology. </p>
<p>But what might a higher-level abstraction for it look like? Figuring that out is probably tantamount to finding the abstraction. But perhaps one can at least expect that in some ways it will be metamathematical, and more about the structure and character of proofs than about their content. Perhaps it will be something related to the framework of higher category theory, or some form of meta-algebra. But as of now, we really don’t know—and we can’t even say that such an abstraction with any degree of generality is possible.</p>
<h2 id="llms-to-the-rescue">LLMs to the Rescue?</h2>
<p>The unexpected success of LLMs in language generation and related tasks has led to the idea that perhaps eventually <a href="https://writings.stephenwolfram.com/2024/03/can-ai-solve-science/">systems like LLMs will be able to “do everything”</a>—including for example math. We already know—not least thanks to Wolfram Language—that <a href="https://www.wolfram.com/mathematica/">lots of math can be done computationally</a>. But often the computations are hard—and, as in the example of the proof we’re discussing here, incomprehensible to humans. So the question really is: can LLMs “humanize” what has to be done in math, turning everything into a human-accessible narrative? And here our proof seems like an excellent—if challenging—test case. </p>
<p>But what happens if we just ask a current LLM to generate the proof from scratch? It’s not a good picture. Very often the LLM will eagerly generate a proof, but it’ll be completely wrong, often with the same kind of mistakes that a student somewhat out of their depth might make. Here’s a typical response where an LLM simply assumes that the · operator is associative (which it isn’t in Boolean algebra) then produces a proof that on first blush looks at least vaguely plausible, but is in fact completely wrong:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/01/sw01072025rescueimg1.png" alt="Inadequate LLM proof" title="Inadequate LLM proof" width="611" height="489"></p>
<p>Coming up with an explanation for what went wrong is basically an exercise in “LLM psychology”. But in a first approximation one might say the following. LLMs are trained to “fill in what’s typical”, where “typical” is defined by what appears in the training set. But (absent some recent Wolfram Language and <a href="https://www.wolframalpha.com/">Wolfram|Alpha</a> based technology of ours) what’s been available as a training set has been human-generated mathematical texts, where, yes, operators are often associative, and typical proofs are fairly short. And in the “psychology of LLMs” an LLM is much more likely to “do what’s typical” than to “rigorously follow the rules”. </p>
<p>If you press the LLM harder, then it might just “abdicate”, and suggest using the <a href="https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/">Wolfram Language as a tool</a> to generate the proof. So what happens if we do that, then feed the finished proof to the LLM and ask it to explain? Well, typically it just does what LLMs do so well, and writes an essay:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/01/sw01072025rescueimg2.png" alt="LLM proof essay" title="LLM proof essay" width="614" height="516"></p>
<p>So, yes, it does fine in “generally framing the problem”. But not on the details. And if you press it for details, it’ll typically eventually just start parroting what it was given as input. </p>
<p>How else might we try to get the LLM to help? One thing I’ve certainly wondered is how the lemmas in the proof relate to known theorems—perhaps in quite different areas of mathematics. It’s something one might imagine one would be able to answer by searching the literature of mathematics. But, for example, textual search won’t be sufficient: it has to be some form of <a href="https://writings.stephenwolfram.com/2024/07/yet-more-new-ideas-and-new-functions-launching-version-14-1-of-wolfram-language-mathematica/#vector-databases-and-semantic-search">semantic search</a> based on the meaning or symbolic structure of lemmas, not their (fairly arbitrary) textual presentation. A vector database might be all one needs, but one can certainly ask an LLM too:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/01/sw01072025rescueimg3.png" alt="LLM semantic search results" title="LLM semantic search results" width="619" height="485"></p>
<p>It’s not extremely helpful, though, charmingly, it correctly identifies the source of our original axiom. I’ve tried similar queries for our whole set of lemmas across a variety of LLMs, with a variety of RAG systems. Often the LLM will talk about an interpretation for some lemma—but the lemma isn’t actual present in our proof. But occasionally the LLM will mention possible connections (“band theory”; “left self-distributive operations in quandles”; “Moufang loops”)—though so far none have seemed to quite hit the mark.</p>
<p>And perhaps this failure is itself actually a result—telling us that the lemmas that show up in our proof really are, in effect, out in the wilds of metamathematical space, probing places that haven’t ever been seriously visited before by human mathematics.</p>
<p>But beyond LLMs, what about more general machine learning and neural net approaches? Could we imagine <a href="https://writings.stephenwolfram.com/2024/03/can-ai-solve-science/#science-as-narrative">using a neural net as a probe to find “exploitable regularities”</a> in our proof? It’s certainly possible, but I suspect that the systematic algorithmic methods we’ve already discussed for finding optimal notations, popular lemmas, etc. will tend to do better. I suppose it would be one thing if our systematic methods had failed to even find a proof. Then we might have wanted something like neural nets to try to guess the right paths to follow, etc. But as it is, our systematic methods rather efficiently do manage to successfully find a proof. </p>
<p>Of course, there’s still the issue that we’re discussing here that the proof is very “non-human”. And perhaps we could imagine that neural nets, etc.—especially when trained on existing human knowledge—could be used to “form concepts” that would help us humans to understand the proof. </p>
<p>We can get at least a rough analogy for how this might work by looking at <a href="https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/">visual images produced by a generative AI system</a> trained from billions of human-selected images. There’s a concept (like “a cube”) that exists somewhere in the feature space of possible images. But “around” that concept are other things—<a href="https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/#the-notion-of-interconcept-space">“out in interconcept space”</a>—that we don’t (at least yet) explicitly have words for:</p>
<div>
<p><img src="https://content.wolfram.com/sites/43/2025/01/sw01072025rescueimg4.png" alt="Interconcept space" title="Interconcept space" width="494" height="494"></p>
</div>
<p>And it’ll presumably be similar for math, though harder to represent in something like a visual way. There’ll be existing math concepts. But these will be embedded in a vast domain of “mathematical interconcept space” that we humans haven’t yet “colonized”. And what we can imagine is that—perhaps with the help of neural nets, etc.—we can identify a limited number of “points in interconcept space” that we can introduce as new concepts that will, for example, provide useful “waypoints” in understanding our proof.</p>
<h2 id="but-why-is-the-theorem-true">But Why Is the Theorem True?</h2>
<p>It’s a common human urge to think that anything that’s true must be true for a reason. But what about our theorem? Why is it true? Well, we’ve seen a proof. But somehow that doesn’t seem satisfactory. We want “an explanation we can understand”. But we know that in general we can’t always expect to get one.</p>
<p>It’s a fundamental implication of computational irreducibility that things can happen where the only way to “see how they happen” is just to “watch them happen”; there’s no way to “compress the explanation”.</p>
<p>Consider the following patterns. They’re all generated by cellular automata. And all <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/">live exactly 100 steps before dying out</a>. But why?</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025theoremimg1-1.png" alt="" title="" width="597" height="509"> </p>
</div>
<p>In a few cases it seems like we can perhaps at least begin to imagine “narratively describing” a mechanism. But most of the time all we can say is basically that they “live 100 steps because they do”. </p>
<p>It’s a quintessential consequence of computational irreducibility. It might not be what we’d expect, or hope for. But it’s reality in the computational universe. And it seems very likely that our theorem—and its proof—is like this too. The theorem in effect “just happens to be true”—and if you run the steps in the proof (or find the appropriate path in the entailment cone) you’ll find that it is. But there’s no “narrative explanation”. No “understanding of why it’s true”. </p>
<h2 id="intuition-and-automated-theorem-proving">Intuition and Automated Theorem Proving</h2>
<p>We’ve been talking a lot about the proof of our theorem. But where did the theorem to prove come from in the first place? Its immediate origin was an <a href="https://www.wolframscience.com/nks/notes-12-9--searching-for-logic-axioms/">exhaustive search I did of simple axiom systems</a>, filtering for ones that could conceivably generate Boolean algebra, followed by testing each of the candidates using automated theorem proving. </p>
<p>But how did I even get the idea of searching for a simple axiom system for Boolean algebra? Based on the axiom systems for Boolean algebra known before—and the historical difficulty of finding them—one might have concluded that it was quite hopeless to find an axiom system for Boolean algebra by exhaustive search. But by 2000 I had nearly two decades of experience in exploring the computational universe—and I was well used to the <a href="https://www.wolframscience.com/nks/chap-2--the-crucial-experiment/">remarkable phenomenon</a> that even very simple computational rules can lead to behavior of great complexity. So the result was that when I came to think about axiom systems and the foundations of mathematics my intuition led me to imagine that perhaps the simplest axiom system for something like Boolean algebra might be simple enough to exhaustively search for.</p>
<p>And indeed discovering the axiom system we’ve discussed here helped further expand and deepen my intuition about the consequences of simple rules. But what about the proof? What intuition might one get from the proof as we now know it, and as we’ve discussed here?</p>
<p>There’s much intuition to be got from observing the world as it is. But for nearly half a century I’ve had another crucial source of intuition: observing the computational universe—and doing computational experiments. I was recently reflecting on how I came to start developing intuition in this way. And what it might mean for intuition I could now develop from things like automated theorem proving and AI.</p>
<p>Back in the mid-1970s <a href="https://www.stephenwolfram.com/publications/academic/particle-physics">my efforts in particle physics</a> led me to start using computers to do not just numerical, but <a href="https://writings.stephenwolfram.com/2013/06/there-was-a-time-before-mathematica/">also algebraic computations</a>. In numerical computations it was usual to just get a few numbers out, that perhaps one could plot to make a curve. But in algebraic computations one instead got out formulas—and <a href="https://content.wolfram.com/sw-publications/2020/07/effective-coupling-qcd.pdf" target="_blank" rel="noopener">often very ornate ones full of structure and detail</a>. And for me it was routine to get not just one formula, but many. And looking at these formulas I started to develop intuition about them. What functions would they involve? What algebraic form would they take? What kind of numbers would they involve? </p>
<p>I don’t think I ever consciously realized that I was developing a new kind of computationally based intuition. But I soon began to take it for granted. And when—at the beginning of the 1980s—<a href="https://www.wolframscience.com/nks/chap-1--the-foundations-for-a-new-kind-of-science#sect-1-4--the-personal-story-of-the-science-in-this-book">I started to explore the consequences of simple abstract systems</a> like cellular automata it was natural to expect that I would get intuition from just “seeing” how they behaved. And here there was also another important element. Because part of the reason I concentrated on cellular automata was precisely because one could readily visualize their behavior on a computer. </p>
<p>I don’t think I would have learned much if I’d just been printing out “numerical summaries” of what cellular automata do. But as it was, I was seeing their behavior in full detail. And—surprising though what I saw was—I was soon able to start getting an intuition for what could happen. It wasn’t a matter of knowing what the value of every cell would be. But I started doing things like identifying four general classes of cellular automata, and then recognizing the phenomenon of computational irreducibility. </p>
<p>By the 1990s I was much more broadly exploring the computational universe—always trying to see what could happen there. And in almost all cases it was a story of defining simple rules, then running them, and making an explicit step-by-step visualization of what they do—and thereby in effect “seeing computation in action”.</p>
<p>In recent years—spurred by our <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">Physics Project</a>—I’ve increasingly explored not just computational processes, but also <a href="https://writings.stephenwolfram.com/2021/09/multicomputation-a-fourth-paradigm-for-theoretical-science/">multicomputational ones</a>. And although it’s more difficult I’ve made every effort to visualize the behavior of multiway systems—and to get intuition about what they do. </p>
<p>But what about automated theorem proving? In effect, automated theorem proving is about finding a particular path in a multiway system that leads to a theorem we want. We’re not getting to see “complete behavior”; we’re in effect just seeing one particular “solution” for how to prove a theorem. </p>
<p>And after one’s seen many examples, the challenge once again is to develop intuition. And that’s a large part of what I’ve been trying to do here. It’s crucial, I think, to have some way to visualize what’s happening—in effect because visual input is the most efficient way to get information into our brains. And while the visualizations we’ve developed here aren’t as direct and complete as, say, for cellular automaton evolution, I think they begin to give some overall sense of our proof—and other proofs like it.</p>
<p>In studying simple programs like cellular automata, the intuition I developed led me to things like my <a href="https://www.wolframscience.com/nks/chap-6--starting-from-randomness#sect-6-2--four-classes-of-behavior">classification of cellular automaton behavior</a>, as well as to bigger ideas like the <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/">Principle of Computational Equivalence</a> and computational irreducibility. So having now exposed myself to automated theorem proving as I exposed myself to algebraic computation and the running of simple rules in the past, what general principles might I begin to see? And might they, for example, somehow make the fact that our proof works ultimately seem “obvious”?</p>
<p>In some ways yes, but in other ways no. Much as with simple programs, there are axiom systems so simple that, for example, the <a href="https://www.wolframscience.com/metamathematics/axiom-systems-in-the-wild/">multiway systems they generate are highly regular</a>. But beyond a low threshold, it’s common to get very complicated—and in many ways seemingly random—multiway system structures. Typically an infinite number of lemmas are generated, with little or no obvious regularity in their forms.</p>
<p>And one can expect that—following the ideas of universal computation—it’ll typically be possible to encode in any one such multiway system the behavior of any other multiway system. In terms of axioms what one’s saying is that if one sets up the right translation between theorems, one will be able to use any one such axiom system to generate the theorems of any other. But the issue is that the translation will often make major changes to the structure of the theorems, and in effect define not just a “mathematical translation” (like between geometry and algebra) but a <a href="https://www.wolframscience.com/metamathematics/uniformity-and-motion-in-metamathematical-space/#p-146">metamathematical one (as one would need to get from Peano arithmetic to set theory)</a>. </p>
<p>And what this means is that it isn’t surprising that even a very simple axiom system can generate a complicated set of possible lemmas. But knowing this doesn’t immediately tell one whether those lemmas will align with some particular existing theory—like Boolean algebra. And in a sense that’s a much more detailed question.</p>
<p>At some metamathematical level it might not be a natural question. But at a “mathematical level” it is. And it’s what we have to address in connection with the theorem—and proof—we’re discussing here. Many aspects of the overall form and properties of the proof will be quite generic, and won’t depend on the particulars of the axiom system we’re using. But some will. And quite what intuition we may be able to get about these isn’t clear. And perhaps it’ll necessarily be fragmented and specific—in effect responding to the presence of computational irreducibility.</p>
<p>It’s perhaps worth commenting that LLMs—and machine learning in general—represent another potential source of intuition. That intuition may well be more about the general features of us as observers and thinkers. But such intuition is potentially critical in framing just what we can experience, not only in the natural world, but also in the mathematical and metamathematical worlds. And perhaps the apparent impotence of LLMs when faced with the proof we’ve been discussing already tells us something significant about the nature of “mathematical observers” like us.</p>
<h2 id="so-what-does-it-mean-for-the-future-of-mathematics">So What Does It Mean for the Future of Mathematics?</h2>
<p>Let’s say we never manage to “humanize” the proof we’ve been discussing here. Then in effect we’ll end up with a “black-box theorem”—that we can be sure is true—but we’ll never know quite how or why. So what would that mean for mathematics?</p>
<p>Traditionally, mathematics has tended to operate in a “white box” kind of way, trying to build narrative and understanding along with “facts”. And in this respect it’s very different from natural science. Because in natural science much of our knowledge has traditionally been empirical—derived from observing the world or experimenting on it—and without any certainty that we can “understand its origins”. </p>
<p>Automated theorem proving of the kind we’re discussing here—or, for that matter, pretty much any exploratory computational experimentation—aligns mathematics much more with natural science, deriving what’s true without an expectation of having a narrative explanation of why. </p>
<p>Could one imagine practicing mathematics that way? One’s already to some extent following such a path as soon as one introduces axiom systems to base one’s mathematics on. Where do the axiom systems come from? In <a href="https://writings.stephenwolfram.com/2020/09/the-empirical-metamathematics-of-euclid-and-beyond/">the time of Euclid</a> perhaps they were thought of as an idealization of nature. But in more modern times they are realistically much more the result of human choice and human aesthetics.</p>
<p>So let’s say we determine (given a particular axiom system) that some black-box theorem is true. Well, then we can just add it, just as we could another axiom. Maybe one day it’ll be possible to prove <a href="https://www.wolframscience.com/nks/p765--undecidability-and-intractability/">P≠NP</a> or the <a href="https://writings.stephenwolfram.com/2021/03/after-100-years-can-we-finally-crack-posts-problem-of-tag-a-story-of-computational-irreducibility-and-more/#classic-unsolved">Riemann Hypothesis</a> from existing axioms of mathematics (if they don’t in fact turn out to be independent). And—black box or not—we can expect to add them to what we assume in subsequent mathematics we do, much as they’re routinely added right now, even though their status isn’t yet known. </p>
<p>But it’s one thing to add one or two “black-box theorems”. But what happens when black-box theorems—that we can think of as “experimentally determined”—start to dominate the landscape of mathematics? </p>
<p>Well, then mathematics will take on much more of the character of ruliology—or of an experimental science. When it comes to the applications of mathematics, this probably won’t make much difference, except that in effect mathematics will be able to become much more powerful. But the “inner experience” of mathematics will be quite different—and much less “human”.</p>
<p>If one indeed starts from axioms, it’s not at the outset obvious why everything in mathematics should not be mired in the kind of alien-seeming metamathematical complexity that we’ve encountered in the discussion of our proof here. But <a href="https://www.wolframscience.com/metamathematics/mathematics-and-physics-have-the-same-foundations/">what I’ve argued elsewhere</a> is that the fact that in our experience of doing mathematics it’s not is a reflection of how “mathematical observers like us” sample the raw metamathematical structure generated by axioms (or ultimately by the <a href="https://www.wolframscience.com/metamathematics/going-below-axiomatic-mathematics/">subaxiomatic structure of the ruliad</a>). </p>
<p>The physics analogy I’ve used is that we succeed in doing mathematics at a “fluid dynamics level”, far above the detailed “molecular dynamics level” of things like the proof we’ve discussed here. Yes, we can ask questions—like ones about the structure of our proof—that probe the axiomatic “molecular dynamics level”. But it’s an important fact that in doing what we normally think of as mathematics we almost never have to; there’s a coherent way to operate purely at the “fluid dynamics level”.</p>
<p>Is it useful to “dip down” to the molecular dynamics? Definitely yes, because that’s where we can readily do computations—like those in our proof, or in general those going on in the internals of the Wolfram Language. But a key idea in the design of the Wolfram Language is to provide a computational language that can express concepts at a humanized “fluid dynamics” level—in effect bridging between the way humans can think and understand things, and the way raw computation can be done with them.</p>
<p>And it’s notable that while we’ve had great success over the years in defining “human-accessible” high-level representations for what amount to the “inputs” and “outputs” of computations, that’s been much less true of the “ongoing processes” of computation—or, for example, of the innards of proofs. </p>
<p>Is there a good “human-level” way to represent proofs? If the proofs are short, it’s not too difficult (and the <a href="https://www.wolframalpha.com/pro/step-by-step-math-solver">step-by-step solutions technology of Wolfram|Alpha</a> provides a good large-scale example of what can be done). But—as we’ve discussed—computational irreducibility implies that some proofs will inevitably be long. </p>
<p>If they’re not too long, then at least some parts of them might be constructed by human effort, say in a system like a proof assistant. But as soon as there’s much automation (whether with automated theorem proving or with LLMs) it’s basically inevitable that one will end up with things that at least approach what we’ve seen with the proof we’re discussing here. </p>
<p>What can then be done? Well, that’s the challenge. Maybe there is some way to simplify, abstract or otherwise “humanize” the proof we’ve been discussing. But I rather doubt it. I think this is likely one of those cases where we inevitably find ourselves face to face with computational irreducibility. </p>
<p>And, yes, there’s important science (particularly ruliology) to do on the structures we see. But it’s not mathematics as it’s traditionally been practiced. But that’s not to say that the results that come out of things like our proof won’t be useful for mathematics. They will be. But they make mathematics more like an experimental science—where what matters most is in effect the input and output rather than a “publishable” or human-readable derivation in between. And where the key issue in making progress is less in the innards of derivations than in defining clear computational ways to express input and output. Or, in effect, in capturing “human-level mathematics” in the primitives and structure of <a href="https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/">computational language</a>. </p>
<h2 id="appendix-what-about-a-different-theorem-proving-system">Appendix: What about a Different Theorem Proving System?</h2>
<p>The proof we’ve been discussing here was created using <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> in the Wolfram Language. But what if we were to use a different automated theorem proving system? How different would the results be? In the spectrum of things that automated theorem proving systems do, our proof here is on the difficult end. And many existing automated theorem proving systems don’t manage to do it all. But some of the stronger ones do. And in the end—despite their different internal algorithms and heuristics—it’s remarkable how similar the results they give are to those from the Wolfram Language <tt>FindEquationalProof</tt> (differences in the way lemmas vs. inference steps, etc. are identified make detailed quantitative comparisons difficult):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025appendiximg1.png" alt="" title="" width="650" height="433"> </p>
</div>
<h2 id="thanks">Thanks</h2>
<p>Thanks to Nik Murzin of the <a href="https://wolframinstitute.org/" target="_blank" rel="noopener">Wolfram Institute</a> for his extensive help as part of the Wolfram Institute Empirical Metamathematics Project. Also Roger Germundsson, Sergio Sandoval, Adam Strzebonski, Michael Trott, Liubov Tupikina, James Wiles and Carlos Zapata for input. Thanks to Arnim Buch and Thomas Hillenbrand for their work in the 1990s on Waldmeister which is now part of <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> (also to Jonathan Gorard for his 2017 work on the interface for <tt>FindEquationalProof)</tt>. I was first seriously introduced to automated theorem proving in the late 1980s by Dana Scott, and have interacted with many people about it over the years, including Richard Assar, Bruno Buchberger, David Hillman, Norm Megill, Todd Rowland and Matthew Szudzik. (I’ve also interacted with many people about proof assistant, proof presentation and proof verification systems, both recently and in the past.)</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visualizing All ISBNs (248 pts)]]></title>
            <link>https://annas-archive.org/blog/all-isbns.html</link>
            <guid>42652577</guid>
            <pubDate>Fri, 10 Jan 2025 04:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://annas-archive.org/blog/all-isbns.html">https://annas-archive.org/blog/all-isbns.html</a>, See on <a href="https://news.ycombinator.com/item?id=42652577">Hacker News</a></p>
Couldn't get https://annas-archive.org/blog/all-isbns.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Gleam v1.7 (165 pts)]]></title>
            <link>https://gleam.run/news/improved-performance-and-publishing/</link>
            <guid>42652329</guid>
            <pubDate>Fri, 10 Jan 2025 04:04:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gleam.run/news/improved-performance-and-publishing/">https://gleam.run/news/improved-performance-and-publishing/</a>, See on <a href="https://news.ycombinator.com/item?id=42652329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">
  <p>
    Published 05 Jan, 2025 by Louis Pilfold
  </p>
<p>Gleam is a type-safe and scalable language for the Erlang virtual machine and
JavaScript runtimes. Today Gleam <a href="https://github.com/gleam-lang/gleam/releases/tag/v1.7.0">v1.7.0</a> has been published, featuring
an array of wonderful improvements. Let’s take a look!</p>

<h2 id="faster-record-updates">Faster record updates</h2>

<p>Gleam is a language with immutable data, and it has a syntax for creating a new
record from an old one with some updated fields.</p>

<pre><code>/// Create a new version of the user with `admin` set to true.
pub fn make_admin(person: User) {
  User(..person, admin: True)
}
</code></pre>

<p>If you’re familiar with JavaScript this is similar to the object spread update
syntax, and similarly it is fast, only copying the references to the fields,
not the data itself.</p>

<p>The code that the Gleam compiler would generate would also be similar to how
JavaScript’s update works, using a small amount of dynamic code at runtime to
construct the new record with the new fields. This runtime conditional logic
had a small performance cost at runtime.</p>

<p>The compiler now instead <em>monomorphises</em> record updates, meaning it generates
exactly the most efficient code to construct the new record on a case-by-case
basis, removing the runtime conditional logic and its associated cost entirely.</p>

<p>The optimisation is for both the Erlang and the JavaScript targets, has no
additional compile speed cost or increase in code size, so it’s an improvement
in every way!</p>

<p>Another benefit of record update monomorphisation is that you can now change
the parameterised types of a generic record with the update syntax.</p>

<pre><code>pub type Named(element) {
  Named(name: String, value: element)
}

pub fn replace_value(data: Named(a), replacement: b) -&gt; Named(b) {
  Named(..data, value: replacement)
}
</code></pre>

<p>Previously this would not compile as the type parameter changed, and the
compiler wasn’t able to infer it was always done safely. Now it can tell, so
this compiles!</p>

<p>Thank you <a href="https://github.com/joshi-monster">yoshi</a> for this excellent change!</p>

<h2 id="generate-decoder-code-action">Generate decoder code action</h2>

<p>Gleam has a very robust type system, it won’t let you unsafely cast values
between different types. This results in a programming experience where the
compiler and language server can offer lots help to the programmer, especially
in unfamiliar or large codebases.</p>

<p>One drawback of this sound type system is that converting untyped input from
the outside world into data of known types requires some additional code which
would not be required in unsound systems. This decoder code can be unfamiliar
and confusing to those new to Gleam, and in simple cases it can seem a chore.</p>

<p>To aid with this the Gleam language server now includes code action to
generate a dynamic decoder for a custom type. For example, if you have this code:</p>

<pre><code>pub type Person {
  Person(name: String, age: Int)
}
</code></pre>

<p>If you place your cursor on the type header and select the code action in your
editor, then it’ll be updated to this:</p>

<pre><code>import gleam/dynamic/decode

pub type Person {
  Person(name: String, age: Int)
}

fn person_decoder() -&gt; decode.Decoder(Person) {
  use name &lt;- decode.field("name", decode.string)
  use age &lt;- decode.field("age", decode.int)
  decode.success(Person(name:, age:))
}
</code></pre>

<p>Thank you <a href="https://github.com/GearsDatapacks">Surya Rose</a>! I know this will be
a very popular addition.</p>

<h2 id="more-secure-package-manager-credential-handling">More secure package manager credential handling</h2>

<p>Gleam is part of the Erlang ecosystem, so it uses the <a href="https://hex.pm/">Hex package manager</a>.
To publish a package to Hex the build tool needs the credentials for your Hex
account, and you would type them into the command line to supply them.
We make this as secure as possible, but there’s always some risk when typing in
credentials. No amount of in-computer security can save you from someone
sitting behind you, watching your fingers on the keyboard.</p>

<p>Gleam now only asks for your Hex credentials once, and uses that to create a
long-lived API token, which will be stored on your filesystem and encrypted
using a local password of your choosing. For all future interactions with Hex
Gleam will ask for the local password, use that to decrypt the API key, and
then use it to authenticate with the Hex APIs.</p>

<p>With this if someone manages to learn the password you use to Hex they would
not be able to do anything with it unless they can also get access to your
computer and the encrypted file stored on it.</p>

<h2 id="package-namespace-checking">Package namespace checking</h2>

<p>The Erlang virtual machine has a single namespace for modules. It does not have
isolation of modules between different packages, so if two packages define
modules with the same name they can collide and cause a build failure or
other undesired behaviour.</p>

<p>To avoid this packages place their modules within their own namespace. For
example, if I am writing a package named <code>pumpkin</code> I would place my modules
within the <code>src/pumpkin/</code> directory.</p>

<p>Sometimes people from other ecosystems with per-package isolation may not
understand this convention and place all their code in the top-level namespace,
using generic names, which results in problems for any users of their package. To
avoid this the <code>gleam publish</code> command will now check for top-level namespace
pollution, explaining the problem and asking for confirmation if it is present.</p>

<p>Thank you <a href="https://github.com/guria">Aleksei Gurianov</a>!</p>

<h2 id="core-team-package-name-checking">Core team package name checking</h2>

<p>The Hex package manager system doesn’t have namespaces, so we can’t publish
packages maintained by the Gleam core team as <code>@gleam/*</code> or such. Instead Hex
users have to rely on adding a prefix to the names of their packages, and in
the Gleam core team we use the prefix <code>gleam_</code>.</p>

<p>These prefixes are unchecked, so one can use anyone else’s prefix without
issue. This is a problem for us as people occasionally publish packages using
the core team’s prefix, and then other people get confused as to why this
seemingly official package is of a low quality. To try and remedy this Gleam
will ask for confirmation when a package is published with the <code>gleam_</code> prefix.
Unfortunately this was not enough and another unofficial package was
accidentally published, so Gleam now asks for a much longer confirmation to be
typed in, to force the publisher to read the message.</p>

<h2 id="semantic-versioning-encouragement">Semantic versioning encouragement</h2>

<p>Sometimes people like to publish packages that are unfinished or unsuitable for
use by others, publishing them as version 0.*. Other people publish code that
is good to use, but shy away from semantic versioning and publish them as
v0.*. In both of these cases the users of these packages have an inferior
experience, unable to take advantage of the benefits that semantic versioning
is designed to bring, which can lead to irritating build errors.</p>

<p>Gleam will now ask for confirmation if a package is published with a v0.*
version, as it does not respect semantic versioning. The fewer zero-version
packages published the better experience users of the package ecosystem will
have.</p>

<h2 id="variant-deprecation">Variant deprecation</h2>

<p>In Gleam one can deprecate functions and types using the <code>@deprecated</code>
attribute, which causes the compiler to emit a warning if they are used. With
this release it is also possible to deprecate individual custom type variants
too!</p>

<pre><code>pub type HashAlgorithm {
  @deprecated("Please upgrade to another algorithm")
  Md5
  Sha224
  Sha512
}

pub fn hash_password(input: String) -&gt; String {
  hash(input:, algorithm: Md5) // Warning: Deprecated value used
}
</code></pre>

<p>Thank you <a href="https://github.com/wilbert-mad">Iesha</a> for this!</p>

<h2 id="canonical-documentation-links">Canonical documentation links</h2>

<p>When packages are published to Hex Gleam will also generate HTML documentation
and upload it to <a href="https://hexdocs.pm/">HexDocs</a>, the documentation hosting site
for the BEAM ecosystem.</p>

<p>Currently we have a problem where Google is returning documentation for very
old versions of Gleam libraries instead of the latest version, which can result
in confusion as people try to use functions that no longer exist, etc. To
prevent this from happening with future versions Gleam now adds a canonical
link when publishing, which should help search engines return the desired version.
In the near future we will write a tool that will update historical
documentation to add these links too.</p>

<p>Thank you <a href="https://github.com/rockerBOO">Dave Lage</a> for this improvement!</p>

<h2 id="custom-messages-for-pattern-assertions">Custom messages for pattern assertions</h2>

<p>Gleam’s <code>let assert</code> allows you to pattern match with a <em>partial pattern</em>, that
is: one that doesn’t match all possible values a type could be. When the value
does not match the program it crashes the program, which is most often used in
tests or in quick scripts or prototypes where one doesn’t care to implement
proper error handling.</p>

<p>With this version the <code>as</code> syntax can be used to add a custom error message for
the crash, which can be helpful for debugging when the unexpected does occur.</p>

<pre><code>let assert Ok(regex) = regex.compile("ab?c+") as "This regex is always valid"
</code></pre>

<p>Thank you <a href="https://github.com/GearsDatapacks">Surya Rose</a>!</p>

<h2 id="javascript-bit-array-compile-time-evaluation">JavaScript bit array compile time evaluation</h2>

<p>Gleam’s bit array literal syntax is a convenient way to build up and to pattern
match on binary data. When targeting JavaScript the compiler now generates
faster and smaller code for int values in these bit array expressions and
patterns by evaluating them at compile time where possible.</p>

<p>Thank you <a href="https://github.com/richard-viney">Richard Viney</a> for this!</p>

<h2 id="javascript-bit-array-slicing-optimisation">JavaScript bit array slicing optimisation</h2>

<p>Continuing on from his previous bit array optimisation, <a href="https://github.com/richard-viney">Richard Viney</a>
has made taking a sub-slice of a bit array a constant time operation on
JavaScript, to match the behaviour on the Erlang target. This is a significant
improvement to performance.</p>

<p>Thank you Richard! Our bit array magician!</p>

<h2 id="empty-blocks-are-valid">Empty blocks are valid!</h2>

<p>In Gleam one can write an empty function body, and it is considered a
not-yet-implemented function, emitting a warning when compiled. This is useful
for when writing new code, where you want to check some things about your
program but have not yet finished writing it entirely.</p>

<pre><code>pub fn wibble() {} // warning: unimplemented function
</code></pre>

<p>You can now also do the same for blocks, leaving them empty will result in a
compile warning but permit you to compile the rest of your code.</p>

<pre><code>pub fn wibble() {
  let x = {
     // warning: incomplete block
  }
  io.println(x)
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a>!</p>

<h2 id="external-modules-in-subdirectories">External modules in subdirectories</h2>

<p>Gleam has excellent interop with Erlang, Elixir, JavaScript, and other
languages running on its target platforms. Modules in these languages can be
added to your project and imported using Gleam’s <a href="https://tour.gleam.run/advanced-features/externals/">external functions</a>
feature.</p>

<p>Previously these external modules had to be at the top level of the <code>src/</code> or
<code>test/</code> directories, but now they can reside within subdirectories of them too.</p>

<p>Thank you <a href="https://github.com/PgBiel">PgBiel</a> for this long-awaited feature!</p>

<h2 id="installation-hints">Installation hints</h2>

<p>To run Gleam on the BEAM an Erlang installation is required, and to run it on
JavaScript a suitable runtime such as NodeJS is required. To initialise a
repository git is required. To compile Elixir code Elixir must be installed.
You get the idea- to use various external tools they need to be installed.</p>

<p>If there’s a particular recommended way to install a missing component for your
operating system the error message for its absence will now direct you to
install it that way.</p>

<pre><code>error: Shell command failed

The program `elixir` was not found. Is it installed?

You can install Elixir via homebrew: brew install elixir

Documentation for installing Elixir can be viewed here:
https://elixir-lang.org/install.html
</code></pre>

<p>Thank you <a href="https://github.com/enkerewpo">wheatfox</a> for this helpful improvement!</p>

<h2 id="faster-erlang-dependency-compilation">Faster Erlang dependency compilation</h2>

<p>You can add packages written in Erlang or Elixir to your Gleam projects, and
the Gleam build tool will compile them for you. To compile Erlang packages
rebar3, the Erlang build tool, is used.</p>

<p>Gleam now sets the <code>REBAR_SKIP_PROJECT_PLUGINS</code> environment variable
when using rebar3. With future versions of rebar3 this will cause it to skip
project plugins, significantly reducing the amount of code it’ll need to
download and compile, improving compile times.</p>

<p>Thank you to <a href="https://github.com/tsloughter">Tristan Sloughter</a> for this
contribution to both Gleam and rebar3! Elixir’s Mix build tool will also
benefit from this new rebar3 feature.</p>

<h2 id="sugar-and-desugar-use-expressions">Sugar and desugar <code>use</code> expressions</h2>

<p>Gleam’s <code>use</code> expression is a much loved and very useful bit of syntactic
sugar, good for making nested higher-order-functions easy to work with. It is
by-far Gleam’s most unusual feature, so it can take a little time to get a good
understanding of it.</p>

<p>To help with this, and to make refactoring easier, Jak has added two new code
actions to the language server, to convert to and from the <code>use</code> expression
syntax and the equivalent using the regular function call syntax.</p>

<p>Here’s the same code in each syntax, so you can get an idea of what the code
actions will convert to and from for you.</p>

<pre><code>pub fn main() {
  use profile &lt;- result.try(fetch_profile(user))
  render_welcome(user, profile)
}
</code></pre>

<pre><code>pub fn main() {
  result.try(fetch_profile(user), fn(profile) {
    render_welcome(user, profile)
  })
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a> for these!</p>

<h2 id="yet-more-language-server-hover-information">Yet more language server hover information</h2>

<p><a href="https://github.com/GearsDatapacks">Surya Rose</a> has been adding more hover
information to the language server. If you hover over patterns or function
labels in your editor then type and documentation information will be shown.
Thank you Surya!</p>

<h2 id="inexhaustive-let-to-case-code-action">Inexhaustive <code>let</code> to <code>case</code> code action</h2>

<p>Using a partial pattern that does not match all possible values with a <code>let</code>
binding is a compile error in Gleam.</p>

<pre><code>pub fn unwrap_result(result: Result(a, b)) -&gt; a {
  let Ok(inner) = result // error: inexhaustive
  inner
}
</code></pre>

<p>The language server now suggests a code action to convert this <code>let</code> into a
<code>case</code> expression with the missing patterns added, so you can complete the
code.</p>

<pre><code>pub fn unwrap_result(result: Result(a, b)) -&gt; a {
  let inner = case result {
    Ok(inner) -&gt; inner
    Error(_) -&gt; todo
  }
  inner
}
</code></pre>

<p>Thanks again <a href="https://github.com/GearsDatapacks">Surya Rose</a>!</p>



<p>The language server now provides an action to extract a value into a variable.
Given this code:</p>

<pre><code>pub fn main() {
  list.each(["Hello, Mike!", "Hello, Joe!"], io.println)
}
</code></pre>

<p>If you place your cursor on the list and trigger the code action in your editor
the code will be updated to this:</p>

<pre><code>pub fn main() {
  let value = ["Hello, Mike!", "Hello, Joe!"]
  list.each(value, io.println)
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a> for this!</p>

<h2 id="expand-function-capture-code-action">Expand function capture code action</h2>

<p>Gleam has a short-hand syntax for a function that takes a single argument and
passes it to another function, along with some other arguments. Here you can
see it being used with the <code>int.add</code> function to create a function that always
adds 11.</p>

<pre><code>pub fn main() {
  let add_eleven = int.add(_, 11)
  list.map([1, 2, 3], add_eleven)
}
</code></pre>

<p>Triggering the code action results in the function-capture being expanded to the
full anonymous function syntax:</p>

<pre><code>pub fn main() {
  list.map([1, 2, 3], fn(value) { int.add(value, 11) })
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a> for the
final code action of the release!</p>

<h3 id="and-the-rest">And the rest</h3>

<p>And thank you to the bug fixers and error message improvers
<a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a>,
<a href="https://github.com/ivanjermakov">Ivan Ermakov</a>,
<a href="https://github.com/Frank-III">Jiangda Wang</a>,
<a href="https://github.com/jrstrunk">John Strunk</a>,
<a href="https://github.com/PgBiel">PgBiel</a>,
<a href="https://github.com/richard-viney">Richard Viney</a>,
<a href="https://github.com/sbergen">Sakari Bergen</a>,
<a href="https://github.com/GearsDatapacks">Surya Rose</a>, and
<a href="https://github.com/joshi-monster">yoshi</a></p>

<p>For full details of the many fixes and improvements they’ve implemented see <a href="https://github.com/gleam-lang/gleam/blob/main/changelog/v1.7.md">the
changelog</a>.</p>

<h2 id="its-my-birthday-">It’s my birthday 🎁</h2>

<p>Today is my birthday! If you’d like to give me a gift please consider
<a href="https://github.com/sponsors/lpil">supporting Gleam on GitHub Sponsors</a>.</p>

<p>Gleam is not owned by a corporation; instead it is entirely supported by
sponsors, most of which contribute between $5 and $20 USD per month, and Gleam
is my sole source of income.</p>

<p><a href="https://github.com/sponsors/giacomocavalieri">Giacomo Cavalieri</a> is also
deserving of your support. He has been doing amazing work on Gleam without any
pay for nearly two years, but now he has GitHub sponsors, so show him some love!</p>

<p><a href="https://github.com/sponsors/lpil" rel="noopener" target="_blank">
  <img src="https://gleam.run/images/community/github.svg" alt="GitHub Sponsors">
</a></p>

<p>Thank you to all our sponsors, especially our top sponsor: Lambda.</p>



<ul>
  <li><a href="https://github.com/00bpa">00bpa</a></li>
  <li><a href="https://github.com/agundy">Aaron Gunderson</a></li>
  <li><a href="https://github.com/zeroows">Abdulrhman Alkhodiry</a></li>
  <li><a href="https://github.com/abeljim">Abel Jimenez</a></li>
  <li><a href="https://github.com/ad-ops">ad-ops</a></li>
  <li><a href="https://github.com/AdamBrodzinski">Adam Brodzinski</a></li>
  <li><a href="https://github.com/adjohnston">Adam Johnston</a></li>
  <li><a href="https://github.com/adam-wyluda">Adam Wyłuda</a></li>
  <li><a href="https://github.com/thebugcatcher">Adi Iyengar</a></li>
  <li><a href="https://github.com/amouat">Adrian Mouat</a></li>
  <li><a href="https://github.com/JitPackJoyride">Ajit Krishna</a></li>
  <li><a href="https://github.com/Guria">Aleksei Gurianov</a></li>
  <li><a href="https://alembic.com.au/">Alembic</a></li>
  <li><a href="https://github.com/eelmafia">Alex</a></li>
  <li><a href="https://github.com/ahouseago">Alex Houseago</a></li>
  <li><a href="https://github.com/rawhat">Alex Manning</a></li>
  <li><a href="https://github.com/aexvir">Alex Viscreanu</a></li>
  <li><a href="https://github.com/akoutmos">Alexander Koutmos</a></li>
  <li><a href="https://github.com/muonoum">Alexander Stensrud</a></li>
  <li><a href="https://github.com/defgenx">Alexandre Del Vecchio</a></li>
  <li><a href="https://github.com/Acepie">Ameen Radwan</a></li>
  <li><a href="https://github.com/abueide">Andrea Bueide</a></li>
  <li><a href="https://github.com/AndreHogberg">AndreHogberg</a></li>
  <li><a href="https://github.com/antharuu">Antharuu</a></li>
  <li><a href="https://github.com/anthony-khong">Anthony Khong</a></li>
  <li><a href="https://github.com/Illbjorn">Anthony Maxwell</a></li>
  <li><a href="https://github.com/amscotti">Anthony Scotti</a></li>
  <li><a href="https://github.com/aweagel">Arthur Weagel</a></li>
  <li><a href="https://github.com/aryairani">Arya Irani</a></li>
  <li><a href="https://github.com/azureflash">Azure Flash</a></li>
  <li><a href="https://github.com/chiroptical">Barry Moore</a></li>
  <li><a href="https://github.com/bartekgorny">Bartek Górny</a></li>
  <li><a href="https://github.com/requestben">Ben Martin</a></li>
  <li><a href="https://github.com/bgmarx">Ben Marx</a></li>
  <li><a href="https://github.com/benmyles">Ben Myles</a></li>
  <li><a href="https://github.com/bbkane">Benjamin Kane</a></li>
  <li><a href="https://github.com/bcpeinhardt">Benjamin Peinhardt</a></li>
  <li><a href="https://github.com/bentomas">Benjamin Thomas</a></li>
  <li><a href="https://github.com/bgwdotdev">bgw</a></li>
  <li><a href="https://github.com/bigtallbill">Bill Nunney</a></li>
  <li><a href="https://github.com/bjartelund">Bjarte Aarmo Lund</a></li>
  <li><a href="https://github.com/bmehder">Brad Mehder</a></li>
  <li><a href="https://github.com/brettkolodny">brettkolodny</a></li>
  <li><a href="https://github.com/brian-dawn">Brian Dawn</a></li>
  <li><a href="https://github.com/bglusman">Brian Glusman</a></li>
  <li><a href="https://github.com/bruce">Bruce Williams</a></li>
  <li><a href="https://github.com/nono">Bruno Michel</a></li>
  <li><a href="https://github.com/bucsi">bucsi</a></li>
  <li><a href="https://github.com/camray">Cam Ray</a></li>
  <li><a href="https://github.com/cameronpresley">Cameron Presley</a></li>
  <li><a href="https://github.com/carlomunguia">Carlo Munguia</a></li>
  <li><a href="https://github.com/csaltos">Carlos Saltos</a></li>
  <li><a href="https://github.com/chadselph">Chad Selph</a></li>
  <li><a href="https://github.com/ctdio">Charlie Duong</a></li>
  <li><a href="https://github.com/charlie-n01r">Charlie Govea</a></li>
  <li><a href="https://github.com/chazwatkins">Chaz Watkins</a></li>
  <li><a href="https://github.com/choonkeat">Chew Choon Keat</a></li>
  <li><a href="https://github.com/ceedon">Chris Donnelly</a></li>
  <li><a href="https://github.com/Morzaram">Chris King</a></li>
  <li><a href="https://github.com/chrislloyd">Chris Lloyd</a></li>
  <li><a href="https://github.com/utilForever">Chris Ohk</a></li>
  <li><a href="https://github.com/Chriscbr">Chris Rybicki</a></li>
  <li><a href="https://github.com/christophershirk">Christopher David Shirk</a></li>
  <li><a href="https://github.com/devries">Christopher De Vries</a></li>
  <li><a href="https://github.com/cdaringe">Christopher Dieringer</a></li>
  <li><a href="https://github.com/christopherhjung">Christopher Jung</a></li>
  <li><a href="https://github.com/christhekeele">Christopher Keele</a></li>
  <li><a href="https://github.com/specialblend">CJ Salem</a></li>
  <li><a href="https://github.com/clangley">clangley</a></li>
  <li><a href="https://github.com/CliffordAnderson">Clifford Anderson</a></li>
  <li><a href="https://github.com/codecrafters-io">CodeCrafters</a></li>
  <li><a href="https://github.com/coder">Coder</a></li>
  <li><a href="https://github.com/colelawrence">Cole Lawrence</a></li>
  <li><a href="https://github.com/insanitybit">Colin</a></li>
  <li><a href="https://github.com/Comamoca">Comamoca</a></li>
  <li><a href="https://github.com/Lucostus">Constantin (Cleo) Winkler</a></li>
  <li><a href="https://github.com/jcorentin">Corentin J.</a></li>
  <li><a href="https://github.com/ccarvalho-eng">Cristiano Carvalho</a></li>
  <li><a href="https://github.com/sdaigo">Daigo Shitara</a></li>
  <li><a href="https://github.com/dvic">Damir Vandic</a></li>
  <li><a href="https://github.com/ddresselhaus">Dan Dresselhaus</a></li>
  <li><a href="https://github.com/DanielleMaywood">Danielle Maywood</a></li>
  <li><a href="https://github.com/pinnet">Danny Arnold</a></li>
  <li><a href="https://github.com/despairblue">Danny Martini</a></li>
  <li><a href="https://github.com/davydog187">Dave Lucia</a></li>
  <li><a href="https://github.com/dbernheisel">David Bernheisel</a></li>
  <li><a href="https://github.com/davidcornu">David Cornu</a></li>
  <li><a href="https://github.com/davesnx">David Sancho</a></li>
  <li><a href="https://github.com/dangdennis">Dennis Dang</a></li>
  <li><a href="https://github.com/dennistruemper">dennistruemper</a></li>
  <li><a href="https://github.com/diemogebhardt">Diemo Gebhardt</a></li>
  <li><a href="https://github.com/dmmulroy">Dillon Mulroy</a></li>
  <li><a href="https://github.com/gothy">Dima Utkin</a></li>
  <li><a href="https://github.com/poroh">Dmitry Poroh</a></li>
  <li><a href="https://github.com/DoctorCobweb">DoctorCobweb</a></li>
  <li><a href="https://github.com/floodfx">Donnie Flood</a></li>
  <li><a href="https://github.com/ds2600">ds2600</a></li>
  <li><a href="https://github.com/ducdetronquito">ducdetronquito</a></li>
  <li><a href="https://github.com/gdcrisp">Dylan Carlson</a></li>
  <li><a href="https://github.com/edongashi">Edon Gashi</a></li>
  <li><a href="https://github.com/eeeli24">eeeli24</a></li>
  <li><a href="https://github.com/enoonan">Eileen Noonan</a></li>
  <li><a href="https://github.com/dropwhile">eli</a></li>
  <li><a href="https://github.com/Emma-Fuller">Emma</a></li>
  <li><a href="https://github.com/EMRTS">EMR Technical Solutions</a></li>
  <li><a href="https://github.com/yellowsman">Endo Shogo</a></li>
  <li><a href="https://github.com/ekosz">Eric Koslow</a></li>
  <li><a href="https://github.com/eterps">Erik Terpstra</a></li>
  <li><a href="https://liberapay.com/erikareads/">erikareads</a></li>
  <li><a href="https://github.com/ErikML">ErikML</a></li>
  <li><a href="https://github.com/erlend-axelsson">erlend-axelsson</a></li>
  <li><a href="https://github.com/oberernst">Ernesto Malave</a></li>
  <li><a href="https://github.com/EthanOlpin">Ethan Olpin</a></li>
  <li><a href="https://github.com/evaldobratti">Evaldo Bratti</a></li>
  <li><a href="https://github.com/evanj2357">Evan Johnson</a></li>
  <li><a href="https://github.com/evanasse">evanasse</a></li>
  <li><a href="https://github.com/fabridamicelli">Fabrizio Damicelli</a></li>
  <li><a href="https://github.com/fmesteban">Fede Esteban</a></li>
  <li><a href="https://github.com/yerTools">Felix Mayer</a></li>
  <li><a href="https://github.com/nandofarias">Fernando Farias</a></li>
  <li><a href="https://github.com/ffigiel">Filip Figiel</a></li>
  <li><a href="https://github.com/floriank">Florian Kraft</a></li>
  <li><a href="https://github.com/francishamel">Francis Hamel</a></li>
  <li><a href="https://github.com/Frank-III">frankwang</a></li>
  <li><a href="https://github.com/gvrooyen">G-J van Rooyen</a></li>
  <li><a href="https://github.com/gabrielvincent">Gabriel Vincent</a></li>
  <li><a href="https://github.com/janag">Ganesan Janarthanam (Jana)</a></li>
  <li><a href="https://github.com/gahjelle">Geir Arne Hjelle</a></li>
  <li><a href="https://github.com/hagenek">Georg H. Ekeberg</a></li>
  <li><a href="https://github.com/brasilikum">Georg Hartmann</a></li>
  <li><a href="https://github.com/george-grec">George</a></li>
  <li><a href="https://github.com/ggobbe">ggobbe</a></li>
  <li><a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a></li>
  <li><a href="https://github.com/giovannibonetti">Giovanni Kock Bonetti</a></li>
  <li><a href="https://github.com/obmarg">Graeme Coupar</a></li>
  <li><a href="https://github.com/grottohub">grotto</a></li>
  <li><a href="https://github.com/nirev">Guilherme de Maio</a></li>
  <li><a href="https://github.com/guillheu">Guillaume Heu</a></li>
  <li><a href="https://github.com/ghivert">Guillaume Hivert</a></li>
  <li><a href="https://github.com/hammad-r-javed">Hammad Javed</a></li>
  <li><a href="https://github.com/kwando">Hannes Nevalainen</a></li>
  <li><a href="https://github.com/ildorn">Hannes Schnaitter</a></li>
  <li><a href="https://github.com/oderwat">Hans Raaf</a></li>
  <li><a href="https://github.com/jhundman">Hayes Hundman</a></li>
  <li><a href="https://github.com/hayleigh-dot-dev">Hayleigh Thompson</a></li>
  <li><a href="https://github.com/hibachrach">Hazel Bachrach</a></li>
  <li><a href="https://github.com/hdahlheim">Henning Dahlheim</a></li>
  <li><a href="https://github.com/h14h">Henry Firth</a></li>
  <li><a href="https://github.com/henrysdev">Henry Warren</a></li>
  <li><a href="https://github.com/losfair">Heyang Zhou</a></li>
  <li><a href="https://github.com/human154">human154</a></li>
  <li><a href="https://github.com/hpiaia">Humberto Piaia</a></li>
  <li><a href="https://github.com/iainh">Iain H</a></li>
  <li><a href="https://github.com/Ian-GL">Ian González</a></li>
  <li><a href="https://github.com/ianmjones">Ian M. Jones</a></li>
  <li><a href="https://github.com/igordsm">Igor Montagner</a></li>
  <li><a href="https://github.com/irumiha">Igor Rumiha</a></li>
  <li><a href="https://github.com/nilliax">ILLIA NEGOVORA</a></li>
  <li><a href="https://github.com/intarga">Ingrid</a></li>
  <li><a href="https://github.com/inoas">inoas</a></li>
  <li><a href="https://github.com/graphiteisaac">Isaac</a></li>
  <li><a href="https://github.com/isaacharrisholt">Isaac Harris-Holt</a></li>
  <li><a href="https://github.com/imcquee">Isaac McQueen</a></li>
  <li><a href="https://github.com/ismaelga">Ismael Abreu</a></li>
  <li><a href="https://github.com/ivarvong">Ivar Vong</a></li>
  <li><a href="https://github.com/m-rinaldi">J. Rinaldi</a></li>
  <li><a href="https://github.com/jacobdalamb">Jacob Lamb</a></li>
  <li><a href="https://github.com/jakecleary">Jake Cleary</a></li>
  <li><a href="https://github.com/jamesbirtles">James Birtles</a></li>
  <li><a href="https://github.com/jamesmacaulay">James MacAulay</a></li>
  <li><a href="https://github.com/janpieper">Jan Pieper</a></li>
  <li><a href="https://github.com/monzool">Jan Skriver Sørensen</a></li>
  <li><a href="https://github.com/jlgeering">Jean-Luc Geering</a></li>
  <li><a href="https://github.com/okkdev">Jen Stehlik</a></li>
  <li><a href="https://github.com/jiangplus">jiangplus</a></li>
  <li><a href="https://github.com/hunkyjimpjorps">Jimpjorps™</a></li>
  <li><a href="https://github.com/joeykilpatrick">Joey Kilpatrick</a></li>
  <li><a href="https://github.com/joeytrapp">Joey Trapp</a></li>
  <li><a href="https://github.com/johan-st">Johan Strand</a></li>
  <li><a href="https://github.com/JohnBjrk">John Björk</a></li>
  <li><a href="https://github.com/johngallagher">John Gallagher</a></li>
  <li><a href="https://github.com/jmpavlick">John Pavlick</a></li>
  <li><a href="https://github.com/xjojorx">Jojor</a></li>
  <li><a href="https://github.com/jonlambert">Jon Lambert</a></li>
  <li><a href="https://github.com/igern">Jonas E. P</a></li>
  <li><a href="https://github.com/JonasHedEng">Jonas Hedman Engström</a></li>
  <li><a href="https://github.com/jooaf">jooaf</a></li>
  <li><a href="https://github.com/joseph-lozano">Joseph Lozano</a></li>
  <li><a href="https://github.com/joshocalico">Joshua Steele</a></li>
  <li><a href="https://liberapay.com/d2quadra/">Julian Lukwata</a></li>
  <li><a href="https://github.com/schurhammer">Julian Schurhammer</a></li>
  <li><a href="https://github.com/justinlubin">Justin Lubin</a></li>
  <li><a href="https://github.com/Neofox">Jérôme Schaeffer</a></li>
  <li><a href="https://github.com/jkbrinso">Kemp Brinson</a></li>
  <li><a href="https://github.com/keroami">Kero van Gelder</a></li>
  <li><a href="https://github.com/kevinschweikert">Kevin Schweikert</a></li>
  <li><a href="https://github.com/hamptokr">Kramer Hampton</a></li>
  <li><a href="https://github.com/Bearfinn">Kritsada Sunthornwutthikrai</a></li>
  <li><a href="https://github.com/krystofrezac">Kryštof Řezáč</a></li>
  <li><a href="https://github.com/krzysztofgb">Krzysztof G.</a></li>
  <li><a href="https://github.com/leostera">Leandro Ostera</a></li>
  <li><a href="https://github.com/leejarvis">Lee Jarvis</a></li>
  <li><a href="https://github.com/leonqadirie">Leon Qadirie</a></li>
  <li><a href="https://github.com/LeartS">Leonardo Donelli</a></li>
  <li><a href="https://github.com/defp">lidashuang</a></li>
  <li><a href="https://github.com/LighghtEeloo">LighghtEeloo</a></li>
  <li><a href="https://github.com/LilyRose2798">Lily Rose</a></li>
  <li><a href="https://github.com/wowi42">Loïc Tosser</a></li>
  <li><a href="https://github.com/lucaspellegrinelli">Lucas Pellegrinelli</a></li>
  <li><a href="https://github.com/lbjarre">Lukas Bjarre</a></li>
  <li><a href="https://github.com/lukasmeihsner">Lukas Meihsner</a></li>
  <li><a href="https://github.com/lamdor">Luke Amdor</a></li>
  <li><a href="https://github.com/2kool4idkwhat">Luna</a></li>
  <li><a href="https://github.com/manuel-rubio">Manuel Rubio</a></li>
  <li><a href="https://github.com/ideaMarcos">Marcos</a></li>
  <li><a href="https://github.com/marcusandre">marcusandre</a></li>
  <li><a href="https://github.com/AYM1607">Mariano Uvalle</a></li>
  <li><a href="https://github.com/mariuskalvo">Marius Kalvø</a></li>
  <li><a href="https://github.com/markholmes">Mark Holmes</a></li>
  <li><a href="https://github.com/markmark206">Mark Markaryan</a></li>
  <li><a href="https://github.com/datayja">Markéta Lisová</a></li>
  <li><a href="https://github.com/Janiczek">Martin Janiczek</a></li>
  <li><a href="https://github.com/rechsteiner">Martin Rechsteiner </a></li>
  <li><a href="https://github.com/martonkaufmann">martonkaufmann</a></li>
  <li><a href="https://github.com/han-tyumi">Matt Champagne</a></li>
  <li><a href="https://github.com/mhheise">Matt Heise</a></li>
  <li><a href="https://github.com/m">Matt Mullenweg</a></li>
  <li><a href="https://github.com/matthewrobinsondev">Matt Robinson</a></li>
  <li><a href="https://github.com/matt-savvy">Matt Savoia</a></li>
  <li><a href="https://github.com/mattvanhorn">Matt Van Horn</a></li>
  <li><a href="https://github.com/mwhitworth">Matthew Whitworth</a></li>
  <li><a href="https://github.com/maxmcd">Max McDonnell</a></li>
  <li><a href="https://github.com/max-tern">max-tern</a></li>
  <li><a href="https://github.com/metame">metame</a></li>
  <li><a href="https://github.com/metatexx">METATEXX GmbH</a></li>
  <li><a href="https://github.com/amiroff">Metin Emiroğlu</a></li>
  <li><a href="https://github.com/stunthamster">Michael Duffy</a></li>
  <li><a href="https://github.com/michaeljones">Michael Jones</a></li>
  <li><a href="https://github.com/monocursive">Michael Mazurczak</a></li>
  <li><a href="https://github.com/michallepicki">Michał Łępicki</a></li>
  <li><a href="https://github.com/karlsson">Mikael Karlsson</a></li>
  <li><a href="https://liberapay.com/Daybowbow/">Mike</a></li>
  <li><a href="https://github.com/mroach">Mike Roach</a></li>
  <li><a href="https://liberapay.com/mikej/">Mikey J</a></li>
  <li><a href="https://github.com/MoeDevelops">MoeDev</a></li>
  <li><a href="https://github.com/MoritzBoehme">Moritz Böhme</a></li>
  <li><a href="https://github.com/rykawamu">MzRyuKa</a></li>
  <li><a href="https://github.com/n8nio">n8n - Workflow Automation</a></li>
  <li><a href="https://github.com/natanaelsirqueira">Natanael Sirqueira</a></li>
  <li><a href="https://github.com/nathanielknight">Nathaniel Knight</a></li>
  <li><a href="https://github.com/Kuuuuuuuu">Nayuki</a></li>
  <li><a href="https://github.com/NFIBrokerage">NFIBrokerage</a></li>
  <li><a href="https://github.com/arcanemachine">Nicholas Moen</a></li>
  <li><a href="https://github.com/nchapman">Nick Chapman</a></li>
  <li><a href="https://github.com/ndreynolds">Nick Reynolds</a></li>
  <li><a href="https://github.com/NicklasXYZ">Nicklas Sindlev Andersen</a></li>
  <li><a href="https://github.com/NicoVIII">NicoVIII</a></li>
  <li><a href="https://github.com/mrniket">Niket Shah</a></li>
  <li><a href="https://github.com/ninanomenon">Ninaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa</a></li>
  <li><a href="http://www.ninefx.com/">NineFX</a></li>
  <li><a href="https://github.com/nomio">Nomio</a></li>
  <li><a href="https://github.com/oceanlewis">Ocean</a></li>
  <li><a href="https://github.com/osebelin">Olaf Sebelin</a></li>
  <li><a href="https://github.com/OldhamMade">OldhamMade</a></li>
  <li><a href="https://github.com/CanadaHonk">Oliver Medhurst</a></li>
  <li><a href="https://github.com/otosky">Oliver Tosky</a></li>
  <li><a href="https://github.com/optizio">optizio</a></li>
  <li><a href="https://github.com/daslaf">Osman Cea</a></li>
  <li><a href="https://github.com/PastMoments">PastMoments</a></li>
  <li><a href="https://github.com/Davorak">Patrick Wheeler</a></li>
  <li><a href="https://github.com/giddie">Paul Gideon Dann</a></li>
  <li><a href="https://github.com/pguse">Paul Guse</a></li>
  <li><a href="https://github.com/biernacki">Pawel Biernacki</a></li>
  <li><a href="https://github.com/Tulkdan">Pedro Correa</a></li>
  <li><a href="https://github.com/petejodo">Pete Jodo</a></li>
  <li><a href="https://github.com/pvsr">Peter Rice</a></li>
  <li><a href="https://github.com/philpax">Philpax</a></li>
  <li><a href="https://github.com/pierrot-lc">Pierrot</a></li>
  <li><a href="https://github.com/sz-piotr">Piotr Szlachciak</a></li>
  <li><a href="https://github.com/qdentity">Qdentity</a></li>
  <li><a href="https://github.com/raquentin">Race Williams</a></li>
  <li><a href="https://github.com/stoft">Rasmus</a></li>
  <li><a href="https://github.com/ray-delossantos">Ray</a></li>
  <li><a href="https://github.com/chouzar">Raúl Chouza </a></li>
  <li><a href="https://github.com/renatillas">re.natillas</a></li>
  <li><a href="https://github.com/redmar">Redmar Kerkhoff</a></li>
  <li><a href="https://github.com/reillysiemens">Reilly Tucker Siemens</a></li>
  <li><a href="https://github.com/renatomassaro">Renato Massaro</a></li>
  <li><a href="https://github.com/renovatorruler">Renovator</a></li>
  <li><a href="https://github.com/richard-viney">Richard Viney</a></li>
  <li><a href="https://github.com/rico">Rico Leuthold</a></li>
  <li><a href="https://github.com/ripta">Ripta Pasay</a></li>
  <li><a href="https://github.com/robertwayne">Rob</a></li>
  <li><a href="https://github.com/TanklesXL">Robert Attard</a></li>
  <li><a href="https://github.com/rellen">Robert Ellen</a></li>
  <li><a href="https://github.com/malkomalko">Robert Malko</a></li>
  <li><a href="https://github.com/Papipo">Rodrigo Álvarez</a></li>
  <li><a href="https://liberapay.com/Karakunai/">Ronan Harris</a></li>
  <li><a href="https://github.com/rotabull">Rotabull</a></li>
  <li><a href="https://github.com/reinefjord">Rupus Reinefjord</a></li>
  <li><a href="https://github.com/ustitc">Ruslan Ustitc</a></li>
  <li><a href="https://github.com/samaaron">Sam Aaron</a></li>
  <li><a href="https://github.com/metruzanca">Sam Zanca</a></li>
  <li><a href="https://github.com/soulsam480">sambit</a></li>
  <li><a href="https://github.com/samifouad">Sami Fouad</a></li>
  <li><a href="https://github.com/bkspace">Sammy Isseyegh</a></li>
  <li><a href="https://github.com/castletaste">Savva</a></li>
  <li><a href="https://github.com/sasa1977">Saša Jurić</a></li>
  <li><a href="https://github.com/scotttrinh">Scott Trinh</a></li>
  <li><a href="https://github.com/smweber">Scott Weber</a></li>
  <li><a href="https://github.com/scottwey">Scott Wey</a></li>
  <li><a href="https://github.com/seanjensengrey">Sean Jensen-Grey</a></li>
  <li><a href="https://github.com/SeanRoberts">Sean Roberts</a></li>
  <li><a href="https://github.com/sporto">Sebastian Porto</a></li>
  <li><a href="https://github.com/sekunho">sekun</a></li>
  <li><a href="https://github.com/tehprofessor">Seve Salazar</a></li>
  <li><a href="https://github.com/codemonkey76">Shane Poppleton</a></li>
  <li><a href="https://github.com/honsq90">Shuqian Hon</a></li>
  <li><a href="https://github.com/simonewebdesign">Simone Vittori</a></li>
  <li><a href="https://github.com/star-szr">star-szr</a></li>
  <li><a href="https://github.com/bytesource">Stefan</a></li>
  <li><a href="https://github.com/sthagen">Stefan Hagen</a></li>
  <li><a href="https://github.com/Qard">Stephen Belanger</a></li>
  <li><a href="https://github.com/stvpwrs">Steve Powers</a></li>
  <li><a href="https://github.com/Strandinator">Strandinator</a></li>
  <li><a href="https://github.com/threepointone">Sunil Pai</a></li>
  <li><a href="https://github.com/slafs">Sławomir Ehlert</a></li>
  <li><a href="https://github.com/Theosaurus-Rex">Theo Harris</a></li>
  <li><a href="https://github.com/thomaswhyyou">Thomas</a></li>
  <li><a href="https://github.com/tcoopman">Thomas Coopman</a></li>
  <li><a href="https://github.com/ernstla">Thomas Ernst</a></li>
  <li><a href="https://github.com/tmbrwn">Tim Brown</a></li>
  <li><a href="https://github.com/timgluz">Timo Sulg</a></li>
  <li><a href="https://github.com/modellurgist">Tom Calloway</a></li>
  <li><a href="https://github.com/tomjschuster">Tom Schuster</a></li>
  <li><a href="https://github.com/tomekowal">Tomasz Kowal</a></li>
  <li><a href="https://github.com/tommaisey">tommaisey</a></li>
  <li><a href="https://github.com/ThisGuyCodes">Travis Johnson</a></li>
  <li><a href="https://github.com/TristanCacqueray">Tristan de Cacqueray</a></li>
  <li><a href="https://github.com/tsloughter">Tristan Sloughter</a></li>
  <li><a href="https://github.com/tymak">tymak</a></li>
  <li><a href="https://github.com/upsidedownsweetfood">upsidedowncake</a></li>
  <li><a href="https://github.com/vvzen">Valerio Viperino</a></li>
  <li><a href="https://github.com/sandsower">Vic Valenzuela</a></li>
  <li><a href="https://github.com/rodrigues">Victor Rodrigues</a></li>
  <li><a href="https://github.com/PerpetualPossum">Viv Verner</a></li>
  <li><a href="https://github.com/yelps">Volker Rabe</a></li>
  <li><a href="https://github.com/weizhliu">Weizheng Liu</a></li>
  <li><a href="https://github.com/enkerewpo">wheatfox</a></li>
  <li><a href="https://github.com/Willyboar">Willyboar</a></li>
  <li><a href="https://github.com/wilsonsilva">Wilson Silva</a></li>
  <li><a href="https://github.com/HymanZHAN">Xucong Zhan</a></li>
  <li><a href="https://github.com/yamen">Yamen Sader</a></li>
  <li><a href="https://github.com/Yasuo-Higano">Yasuo Higano</a></li>
  <li><a href="https://github.com/joshi-monster">yoshi~ </a></li>
  <li><a href="https://github.com/Yuri2b">Yuriy Baranov</a></li>
  <li><a href="https://github.com/gasparinzsombor">Zsombor Gasparin</a></li>
  <li><a href="https://liberapay.com/~1814730/">~1814730</a></li>
  <li><a href="https://liberapay.com/~1847917/">~1847917</a></li>
  <li><a href="https://liberapay.com/~1867501/">~1867501</a></li>
  <li><a href="https://github.com/eberfreitas">Éber Freitas Dias</a></li>
</ul>

<p>Thanks for reading, I hope you have fun with Gleam! 💜</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A three month review of kagi search and the orion web browser (2024) (106 pts)]]></title>
            <link>https://flatfootfox.com/a-three-month-review-of-kagi-search-the-orion-web-browser/</link>
            <guid>42652125</guid>
            <pubDate>Fri, 10 Jan 2025 03:24:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flatfootfox.com/a-three-month-review-of-kagi-search-the-orion-web-browser/">https://flatfootfox.com/a-three-month-review-of-kagi-search-the-orion-web-browser/</a>, See on <a href="https://news.ycombinator.com/item?id=42652125">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <p>There’s a new web search in town. No, it’s not a re-skin of Bing results. No, it’s not an AI powered tool chasing this particular moment of Large Language Model (LLM) hype. <a href="https://kagi.com/">Kagi</a> is an honest to goodness general purpose search engine with a simple proposal:</p><p><em>Pay us $10 USD a month and we’ll provide you with good search results.</em></p><p>A few of you just got very excited, and some others just closed this tab. Kagi is a very unusual product in 2024. The tool isn’t without its quirks, but it’s the sort of service you can easily write a few thousand words discussing. If you’re interested in technology and the web, it’s worth signing up and kicking their tires.</p><p>Kagi’s been building buzz for the last year, but I’d been dragging my heels on making the switch. It turns out the way to keep New Year’s tech resolutions is to replace your app defaults. Here’s my assorted thoughts after a January, February, and March without Google and Safari.</p><h2 id="paying-for-search">Paying For Search</h2><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiPricing.png" alt="" loading="lazy" width="1256" height="655" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiPricing.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/KagiPricing.png 1000w, https://flatfootfox.com/content/images/2024/04/KagiPricing.png 1256w" sizes="(min-width: 720px) 720px"><figcaption>Kagi's trial isn't time bound. You can start whenever you'd like without any extra pressure.</figcaption></figure><p>It’s hard not to talk about the state of the web as a whole when discussing search engines. The internet’s having a bit of a moment. Longstanding websites like Twitter and Reddit are making antagonistic moves against their user bases. Google and Amazon are dealing with a floor of affiliate links and search engine optimized sludge. Looming on the horizon, LLM outputs threatens to swallow the web whole.</p><p>Out of all of this comes Kagi. On the face of it, they’re <em>yet another</em> subscription looking to monetize some small silver of your daily life. If you take a step back however, some interesting things happen when you pay for search.</p><p>Kagi describes themselves as “user-centric search”. You’re paying them directly for the service they’re rendering. This has two significant and immediate effects: Kagi doesn’t need to show you ads, and their search has to be good.</p><p>Not having ads always makes for an enjoyable browsing experience. As cable TV has proven time and time again, paid services and advertising aren’t completely incompatible which one another. For the time being however Kagi is happy to be one of the increasingly few internet oases that isn’t actively trying to sell you something every time you interact with it.</p><p>This has substantial benefits on the privacy side of things. Since you’re the customer and not the product, Kagi doesn’t need to integrate extensive surveillance and tracking technologies into their tool. They don't log searches, associate searches with your accounts, and they even spell out, in plain English, the intent of the <a href="https://kagi.com/privacy">5 cookies</a> they use.</p><p>The privacy angle is nice, but <a href="https://duckduckgo.com/">Duck Duck Go</a> has also been around for a bit. They offer an anonymized ad-supported privacy-focused search. Building a successful web service at scale purely off of ad revenue has become increasingly tricky however, so Duck Duck Go’s search results are largely a proxy for Microsoft’s middling <a href="https://www.bing.com/">Bing</a> search. (Sorry Bing.)</p><p>The exciting thing about Kagi is that <em>you’re paying them for search</em>. If it’s not good, no one’s going to subscribe for a second month. The company actively pitches the aligned incentives that come from being an ad-free, paid search tool. They get their money from building a good search engine. They don’t benefit from driving traffic to sponsored links, affiliate vendors, or vertically integrated products. Kagi’s search is pretty good as a result.</p><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSearchResults.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiSearchResults.png" alt="" loading="lazy" width="1156" height="1112" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiSearchResults.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSearchResults.png 1000w, https://flatfootfox.com/content/images/2024/04/KagiSearchResults.png 1156w" sizes="(min-width: 720px) 720px"><figcaption>Kagi's search results don't take much getting used to.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>It’s tricky to objectively judge something like the quality of search results. (Although there are some <a href="https://danluu.com/seo-spam/">good attempts to do so</a>.) When search results are good, you hardly notice them. When they’re off, it’s immediate and apparent. Not to harp on Bing again, but everyone’s had that experience of searching on a new work computer or public terminal and struggling to find what they’re looking for before realizing they’re not using Google.</p><p>Web search quality is vibes based. And the vibes are off. People have been whinging for a while now about whether or not <a href="https://www.theatlantic.com/technology/archive/2023/09/google-search-size-usefulness-decline/675409/">Google Search results are getting worse</a>. While drafting this blog post, Google themselves admitted they need to make some <a href="https://www.theverge.com/2024/3/5/24091099/google-search-high-quality-results-spam-ai-content">significant changes</a> to their search results to filter out AI generated spam and SEO reputation laundering.</p><p>All of this is to say that I’ve been satisfied with the search results Kagi provides. They “pass the vibe check” so to speak. While testing Kagi I rarely (if ever) experienced that mental whiplash of viewing “off” search results only to realize I’m “that other search engine”. Kagi’s search is good. It’s not a party trick. It’s not situational. It’s good search. You should <a href="https://kagi.com/pricing">give it a try</a>.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>When searching for a question online, Kagi does a good job of surfacing results from human-centric discussion sites like Reddit or Stack Overflow. When searching for cooking recipes, I notice a slightly more diverse set of food blogs then with Google Search. Comparison shopping or searching for information about a mainstream product does seem to fall down some of the same SEO pitfalls that Google has. This feels more like an artifact of the current state of the web then a particular search engine deficiency however.</p><p>The actual browsing experience with Kagi is pleasant. It sticks with the established Google-style search results page. It delivers links with blurbs, sub-links, and embedded rich content. If you search for something with significant video or image results, Kagi has a familiar strip of thumbnails inviting you to pivot over to one of those dedicated search views. It’s a very familiar search experience you can transition into easily. Kagi’s focus is on the search engine itself, and they’re not trying to be cute with unusual user interface innovations.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>That’s not to say Kagi doesn’t have some unique touches of its own however. Once you’ve gotten familiar with the initial search experience, there’s a lot of additional functionality you can start taking advantage of within Kagi.</p><p>On the search side, Kagi lets you adjust the ranking of pages within your personal search results. If your recipe searches are anything like mine, that might involve bumping up <a href="https://www.seriouseats.com/">SeriousEats.com</a> links. If you’ve been burned by one too many <a href="https://www.allrecipes.com/">AllRecipes.com</a> recipes, you can even go as far as hiding all pages in that domain from your search results.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>If you’re a web search power user, Kagi’s <a href="https://help.kagi.com/kagi/features/lenses.html">Lens</a> functionality may be of particular note to you. If you find yourself frequently searching for programming support for example, you can narrow down Kagi’s view of the web to just the handful of official documentation sites and support forums for the language you’re working with. It’s a bit of a brute force approach to cutting out blog spam, but you were probably scrolling down to the <a href="https://stackoverflow.com/">StackOverflow.com</a> or <a href="https://dev.to/">Dev.to</a> links anyways, right? This tool’s a bit more situational, but it’s nice seeing some thought put into how to push the search experience forward.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>There’s just something comforting about using opinionated software that doesn't feel like it has gone through eight code reviews and six departmental presentations. Kagi’s clearly built by a team who cares about search. When searching for an image, Kagi provides you with links to go to the page the image is on <em>and</em> a direct link to the image itself. No more having to right click and fiddling around for the “Open Image In New Tab” option! Fancy that. These nice touches range from quick shortcuts all the way to letting users upload a <a href="https://help.kagi.com/kagi/features/custom-css.html">custom CSS user theme</a> for the site.</p><p>If you’re hesitant to make the full time switch to Kagi, the “Bang” functionality they borrowed from Duck Duck Go is of particular note. If you prefix a search with <code>!g</code>, Kagi will instead forward that search query over to Google. <code>!r</code> sends you to Reddit’s internal search instead. <code>!yt</code> is a direct link to YouTube search, etc. Duck Duck Go has cataloged <a href="https://duckduckgo.com/bangs">thousands</a> of these integrations.</p><p>I didn’t find myself needing a Google escape hatch that often. When I did, it was mostly when searching for local businesses. Kagi has integration with Apple Maps and Yelp for finding nearby restaurants, but Google Maps remain notably strong in that one area. I use the <code>!g</code> shortcut maybe once a week on the unlimited plan, but they’re instrumental when working with Kagi’s lower priced tiers. Speaking of which;</p><h2 id="don%E2%80%99t-bother-with-kagi%E2%80%99s-5month-search-tier">Don’t Bother With Kagi’s $5/Month Search Tier</h2><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSearches.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiSearches.png" alt="" loading="lazy" width="1060" height="677" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiSearches.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSearches.png 1000w, https://flatfootfox.com/content/images/2024/04/KagiSearches.png 1060w" sizes="(min-width: 720px) 720px"><figcaption>If you're reading this, you use way more than 300 searches a month.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>Kagi’s updated their pricing a few times throughout the life of the product, so go <a href="https://kagi.com/pricing">check</a> and make sure this section is still relevant. Their initial offerings were focused on providing a few different metered tiers. Deciding between 300, 500, and 700 search queries a month harkened back to the days of paying for individual SMS messages. And not in a good way.</p><p>Luckily Kagi’s prices have been coming down. They now offer two main tiers: $5 USD per month for 300 searches, and $10 USD per month for unlimited searches. (There’s also a $25 USD per month tier with early access to upcoming features for super-fans.) Kagi will give you 100 free searches when you sign up that are time unconstrained. If you’re interested in seeing just a few sample search queries, feel free to open a new tab and run some test searches now. You don’t need to set aside a week to give the tool a proper test the moment you sign up.</p><p>Ten dollars per month is a <em>lot</em> to ask for search when an several ad-supported alternative exists. The five dollar tier sounds like a good compromise on paper, but I found it not worth the effort after having used it throughout January.</p><p>It’s hard to overstate just how often technology-minded folks search throughout the day. I apparently do about 45 Kagi searches a day on average. In reality this shakes out to about 30 searches on a weekday, and 60 searches on the weekend when I’m deep diving into side projects. This doesn’t include the Google searches I still perform during the day on my work computer. Just to drive this point home, I managed to perform over 600 searches on an unlimited plan during the month of March while out of the country and completely disconnected from the internet for a week.</p><p>Limiting myself to ten high quality Kagi searches a day in January actually wasn’t <em>too</em> difficult. The <code>!g</code> <a href="https://help.kagi.com/kagi/features/bangs.html">Google shortcut</a> doesn’t count against your Kagi search total. However, there’s a lot of mental overhead in deciding which search engine to use. Looking for programming help? That’s a job for Kagi. Just looking for <code>Local Pizza Brewpub Menu</code>? That’s a job for Google.</p><p>You can <em>technically </em>get through a month on the $5 plan. I ran out of searches a day and a half before next month’s auto-renewed batch of 300 searches. (Kagi doesn’t start billing you per-search. You just get an upsell notice and an option to go back to Google or Bing.) The 300 search plan unfortunately just isn’t a very pleasant experience. I’d find myself wincing any time I accidentally typed a query I already knew the result of &nbsp;like <code>Serious Eats Channa Masala</code> into Kagi’s search during my metered month.</p><p>If you want to experiment with Kagi after your 100 free searches, do yourself a favor and jump straight into the $10 USD per month plan. You’ll get a better feel for Kagi’s search results by the end of your experimentation month if you’re not frequently trying to remember to use the <code>!g</code> shortcut for menial searches throughout the day.</p><p>That’s my big takeaway for this blog post. Hopefully Kagi’s prices continue to come down to the point where they don’t need to offer a metered plan period. This critique may not be accurate anymore by the time you read it. If your spouse is also into tech, the cost may work out a bit better for you on their $14 USD per month two seat “Duo” plan.</p><h2 id="kagi-ai">Kagi &amp; AI</h2><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/2024/04/KagiFastGPT.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiFastGPT.png" alt="" loading="lazy" width="838" height="1029" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiFastGPT.png 600w, https://flatfootfox.com/content/images/2024/04/KagiFastGPT.png 838w" sizes="(min-width: 720px) 720px"><figcaption>It's still surreal seeing myself paraphrased by an AI. For what it's worth, there's a sneaky hallucination in #5. Ergogen doesn't create finalized PCB files, and KiCAD's needed to generate the Gerber files.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>Kagi has been exploring the Large Language Model and AI tools space along with most of Silicon Valley. Their core offering is a model known as FastGPT. It’s a ChatGPT 3.5-style LLM with the ability to perform web searches. The answers it provides are citation-heavy, similar to <a href="https://copilot.microsoft.com/">Microsoft’s Bing Chat / Copilot</a>.</p><p>FastGPT is available as a standard interactive chat experience, but Kagi also exposes it a few other ways. They have a “Summarizer” web app which takes web page URLs and can provide short summaries or a bullet point list of key moments. The Summerizer tool also provides a conversational interface to further interrogate the context of a particular web page.</p><p>Finally, Kagi’s most prominent FastGPT integration is its “Quick Answers” functionality. At the top of every search result page is a “Quick Answers” button you can tap to get a brief 2-3 sentence answer to your query. The response appears inline in a new panel at the top of the search results page. As its name implies, FastGPT is designed to begin typing its response nearly instantaneously.</p><p>Quick Answers is a nice way to begin dabbling in large language models. You don’t need to remember, “Oh right, I can have a conversation with a chatbot about this question.” It’s just always right there at the top of the page if your query seems like one of those things that should be trivial for “the internet” to answer. Kagi will also save you the tap and automatically generate a Quick Answer response for some queries that it detects are in the form of a question.</p><p>Overall FastGPT responds about as well as other consumer LLMs. It does unfortunately produce occasional hallucinations depending on the query. Kagi as a company has a <a href="https://blog.kagi.com/kagi-ai-search">history</a> with AI technology, so these experiments most likely will not be flashes in the pan. Overall the company seems to be taking a balanced approach when using these tools to help users search and interrogate documents further, rather than just using them as search engine replacements. These LLM experiments aren’t as core to the product as, say, the <a href="https://arc.net/">Arc browser</a> or <a href="https://search.brave.com/">Brave Search</a>. All of the AI functionality can be disabled if you don’t want to use LLM-based tools. An unlimited number of interactions are included with Kagi’s $10 USD per month unlimited search plan, so you can think of it as a bonus functionality if you’re still in the experimental phase with AI tools and don’t want to spring for an OpenAI or Microsoft Copilot premium offering.</p><h2 id="the-orion-browser">The Orion Browser</h2><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/size/w1000/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png" alt="" loading="lazy" width="1754" height="893" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png 1000w, https://flatfootfox.com/content/images/size/w1600/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png 1600w, https://flatfootfox.com/content/images/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png 1754w" sizes="(min-width: 720px) 720px"><figcaption>Kagi <em>may</em> have gotten a glance at Apple's homework.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>Speaking of browsers, Kagi has one! Wait wait, don’t close the tab yet! Orion’s not Chromium based!</p><p>All kidding aside, there’s been a trend as of late of companies announcing a “new” browser which is essentially a re-skin of Google Chrome leveraging the same underlying Chromium rendering engine. They’ll sometimes have new functionality which couldn’t fit within the framework of a traditional browser extension, or some novel UI layer justifying the separate product. The core experience is largely still dictated by Chromium’s foundation however.</p><p>The consolidation of rendering technologies and the risk browser monopolies propose to the open web is a bit outside of the scope of this blog post. Rest assured though, Orion’s not Chromium based.</p><p>It’s <a href="https://webkit.org/">Webkit</a> based!</p><p>Orion uses the same rendering engine Apple uses for its Safari web browser. (Incidentally, Orion is only available on macOS and iOS.) Webkit was the basis for Chromium, so this may be splitting hairs a bit. They’re important hairs though.</p><p>For those not familiar with the current state of the browser scene on macOS, there's generally two paths you can take. You can go with Apple's relatively basic but power efficient Safari web browser, or you can use a more feature rich but battery hungry web browser like <a href="https://www.google.com/chrome/">Google's Chrome</a> or <a href="https://www.mozilla.org/en-US/firefox/new/">Mozilla's Firefox</a>. (This isn’t a perfect characterization in all contexts, but it’s a good enough rubric.) I was personally a happy Safari user, but Chrome is useful for a few resource intensive web applications like Google Docs or browser-based video conferencing.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>One of Chrome’s largest selling points over Safari is its extensive 3rd party extension support. Safari technically has 3rd party extensions as well, but they’ve never quite caught on as much as the Chrome or Firefox ecosystems. Safari’s APIs aren’t as robust, and unlike its open source peers you need a paid Apple developer account to publish Safari extensions.</p><p>Put simply, Orion is Safari’s browser technology with Chrome and Firefox’s extension support grafted onto it. That’s not a metaphor. Orion’s developers have added support for approximately 70% of the WebExtensions API onto Orion. The end result is that you can use Chrome or Firefox's UBlock Origin, Bitwarden, and LastPass extensions on a Webkit-based browser. It's an impressive technical feat and it works surprisingly well.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>The rest of Orion is largely what you’d expect. There’s a few UI flourishes such as the option for vertical tabs, but they’ve largely stuck with what worked in Safari. It’s a fast snappy browser that’s not going to drain your battery life. It has all the usual password management, tab syncing, focus reading modes, and other modern touches you’d expect.</p><p>Speaking of syncing, Orion also has a mobile version for iOS. Its development started after the macOS version, and it can occasionally feel like it had less time in the oven so to speak. (It’s the only app in recent memory I’ve had actually crash the iOS Springboard.) It’s still probably the best Safari alternative I’ve tried on iOS... but Safari on iOS is the platonic ideal of mobile web browsing. Any rough edges are going to stick out like a sore thumb.</p><p>The biggest benefit of using both the macOS and iOS versions of Orion is tab syncing and Apple’s Continuity functionality. Using only the desktop app can feel a bit disjointed when suddenly your phone’s trying to open Hand Off links in Safari. Dealing with the occasional rough edge on your phone helps keep the overall experience a bit more seamless. That alone may be a hard sell, but there’s one other piece of functionality both the desktop and mobile apps have which may keep you around.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>Despite supporting the well regarded UBlock Origin, Orion also has ad blocking built in by default. This is mostly pitched from a privacy and performance perspective. Browser ads <em>do</em> have a measurable impact on battery life while on the go, and there are an absurd amount of advertisers looking to track you around the web.</p><p>Online advertising pays for the news sites you read, the social media you browse, and the videos you watch. Still, they’re obtrusive and dangerous enough that <a href="https://techcrunch.com/2022/12/22/fbi-ad-blocker/">even the US’s FBI is recommending ad blockers for your own safety</a>. If you’re part of the one third of all web citizens who use an ad blocker, Orion’s got you covered out of the box. It’s just a little odd to see a company openly brag about blocking YouTube ads and supporting Picture-In-Picture and background audio without a YouTube Red subscription.</p><p>Interestingly for a modern browser, <a href="https://orionfeedback.org/d/3882-open-source-the-browser">Orion isn’t open source</a>. This logistically makes sense given the size of the development team. There are still ways of checking some of their privacy-oriented claims (a packet sniffer can verify that it’s zero telemetry for example), but it’s worth pointing out given their otherwise privacy-focused posture. Safari isn’t fully open source either, so that may not be a sticking point for most macOS and iOS users.</p><h2 id="kagi-other-browsers">Kagi &amp; Other Browsers</h2><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSafari.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiSafari.png" alt="" loading="lazy" width="1176" height="726" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiSafari.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSafari.png 1000w, https://flatfootfox.com/content/images/2024/04/KagiSafari.png 1176w" sizes="(min-width: 720px) 720px"><figcaption>Orion isn't strictly necessary to jump ship to Kagi.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>If you decide to skip Orion, you can still use <a href="https://help.kagi.com/kagi/getting-started/setting-default.html">Kagi as your default search engine</a> in other browsers. In most cases, you just need to dig into the search preferences and set <code>https://kagi.com/search?q=%s</code> as your default search provider. Chrome or Firefox will insert your query in the place of the <code>%s</code> when you enter search terms into your address bar and you’ll be all set from there.</p><p>Frustratingly, Apple didn’t get the memo on user-definable search providers. Safari is hardcoded to Google, Bing, Yahoo (Bing-powered), Duck Duck Go (powered by a variety of search sources including Bing), and Ecosia (a Bing-powered search that uses its ad revenue to plant trees). Hopefully Kagi will get added to Safari’s search engine list if it gets popular enough. Google's currently paying Apple <a href="https://www.theverge.com/2023/10/26/23933206/google-apple-search-deal-safari-18-billion">$18 billion dollars</a> to remain the default search engine in iOS, so we'll see what happens.</p><p>In the meantime, Kagi has a Safari browser extension for both the desktop and mobile clients which allow you to fake Kagi as your default search provider. The extension sees a search sent to one of the Safari default providers, parses the URL, grabs your search term, and then instantly redirects you to the Kagi search page for that term. In my limited testing it appears to perform the redirect imperceptibly fast. The extension doesn’t prevent you from visiting Google if you need to either. You can still use the <code>!g</code> override to get back to Google Search. If you do encounter flakey behavior, you can set the extension to only work on a specific search engine like Duck Duck Go or Ecosia if you run into trouble with accidental redirects. Apparently it was buggier in earlier versions, but it appears to have worked out the issues.</p><h2 id="doing-the-mental-calculus">Doing The Mental Calculus</h2><figure><img src="https://flatfootfox.com/content/images/2024/04/StarTrekHmpf.gif" alt="" loading="lazy" width="245" height="175"><figcaption>Hmpf.</figcaption></figure><p>I’m not quite sure what to make of Kagi after my first three months with it. It’s <em>really</em> good search. Is it $10 USD monthly search? That’s the tougher question.</p><p>There’s some amount of mental calculus involved when deciding to make the switch to a paid service like Kagi. I’m not personally looking for a zero tracking search service or seeking to completely de-Google my life. I care about privacy, but I still own a smart speaker or two. I’ve got an iPhone, but Google still manages my email.</p><p>I’m on the fence about spending a bit more time with Kagi as my default search engine. The gaps in local search aren’t that egregious, but in its current state it still feels like a $50 USD per year indulgence rather than a $100 USD per year splurge. Maybe try to find a friend to go halfsies on with a Duo plan?</p><p>One last factor to consider is Kagi’s recent partnership with Brave Search for use of their search index. Brave is a relatively new browser company helmed by Mozilla’s ousted CEO Brendan Eich. Since his departure from Mozilla, Eich has continued to defend his <a href="https://en.wikipedia.org/wiki/Brendan_Eich#Appointment_to_CEO_and_resignation">homophobia</a>, <a href="https://www.nytimes.com/2020/12/22/business/brave-brendan-eich-covid-19.html">spread Covid misinformation</a>, and generally introduced a slew of scammy crypto functionality within Brave.</p><p>After <a href="https://kagifeedback.org/d/2808-reconsider-your-partnership-with-brave">pushback from their userbase</a> about this partnership, Kagi’s CEO responded that the company is not yet in a position to be able to take moral stands.</p><blockquote>I understand that this has affected many of you in a negative way, creating a sense of betrayal that's against the very ethos of Kagi. I want to address this and be crystal-clear: any semblance of support for discrimination is completely against our principles. The rationale behind our choice was purely based on technological merits and business strategy, including the quality and cost-effectiveness of the service, as well as a critical need for redundancy and diversification in our data sources. The decision was treated the same as getting results from Google or Yandex (to which different groups of users in our userbase object to for various different reasons).</blockquote><blockquote>Kagi is currently not in the position to be fully independent. Searching the web is incredibly hard and Microsoft spent 20 years and billions of dollars building Bing, and it is still, let's say, suboptimal. Definitely not at the level people would pay for it. It is very hard for a small startup with many orders of magnitude less resources to crawl/index/rank the entire web and for it to be so good that people would pay for.</blockquote><blockquote>...</blockquote><blockquote>Choosing to focus on our mission to provide the best search results in the world is the only practical position we can have. This is not because we are ignorant to issues impacting societies across the globe, but because it is impractical for us to deal with them all. We cannot solve all of the world’s problems, human rights issues, conflicts and wars, but we have instead devoted our passion and energy to solving one problem that we believe is within our grasp, and that is the problem of web search, which is what Kagi is known and loved for. We intend to do that to the best of our ability.</blockquote><p>Man, this sucks.</p><p><a href="https://www.404media.co/friendship-ended-with-google-now-kagi-is-my-best-friend/">404 Media</a> also recently reported on Kagi's stance when it comes to manually manipulating search results in their service. The hypothetical discussed in the Kagi forums was whether or not the search engine should present suicide hotline information when someone searches for self harm-related topics. The company pushed back on this proposal saying they do not have any "moral, political, religious, social or any similar kind of bias" they want to explicitly code into their algorithms.</p><p>Arguing that Kagi "does not have any implicit bias built by us that is not search quality based" is a remarkable comment to hear from a CEO in 2023. Particularly when the company is eager to explore the burgeoning AI space. Everyone brings their own biases to the table when building a product. The best anyone can do is be aware of their blindspots and be open and honest about where they're coming from.</p><p>Kagi as a company likes to discuss their moral stances when there's only an upside. They frequently advertise their aligned incentives, and they bragged about going <a href="https://blog.kagi.com/celebrating-20k">"hard mode on"</a> when searching for an ethical t-shirt vendor. When it comes to more nuanced or messy topics however, Kagi's default approach appears to be hoping that not engaging with a topic is the same as not having a stance on the topic. I can't see this position being tenable forever.</p><h2 id="closing-thoughts">Closing Thoughts</h2><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/size/w1000/2024/04/KagiHomepage.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiHomepage.png" alt="" loading="lazy" width="1293" height="878" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiHomepage.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/KagiHomepage.png 1000w, https://flatfootfox.com/content/images/2024/04/KagiHomepage.png 1293w" sizes="(min-width: 720px) 720px"><figcaption>Everything else aside, Kagi's mascot is adorable.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>I’ve been dragging my heels wrapping up this blog post. Originally this was supposed to be a <em>two month</em> evaluation of Kagi. I needed a bit more time to get screenshots for this blog post though, so I decided to give myself an extra month to gather my thoughts on the tool. </p><p>I continue to be impressed by the search experience on Kagi. The results are solid, and the search page is pleasant to browse. Throughout the month I kept stumbling on nice touches. I recently discovered a website I was searching for was down, only to be delighted to find an <a href="https://archive.org/">Archive.org</a> link in the result’s kebab menu. These folks know how people interact with the web and want to make a great search experience.</p><p>The one thing I struggle with Kagi is to identify who its core user base is. For all the talk of Kagi’s aligned incentives with their customers, I can’t shake the feeling that I don’t know who they’re targeting. Duck Duck Go currently handles some amount of general privacy or Google-specific search concerns. So... Tech enthusiasts who would pay for a premium search experience, but aren’t turned off by LLM integrations? Orion is a privacy focused browser for people with strong opinions about Chrome’s Manifest v3 API transition who also don’t mind closed sourced tools?</p><p>There’s an unusual tension running throughout Kagi. They’ve had to hedge their bets throughout the development of the product. Maybe we’re just in a particularly compromising moment in tech. But that pragmatism can cut both ways.</p><p>After this three month experiment, I'm pausing my Kagi subscription. I might hang on to the Orion browser for a little bit longer. $10 USD is a lot to pay per month for search. That's more than the entry tier for most streaming services. I could see myself hopping back onboard once Kagi gets their prices down to $5 USD per month.</p><p>It'll be a pragmatic technology decision though. Tool X has a better experience than Tool Y justifying Z Cost.</p><p>Kagi is the type of service that would really benefit from a clearly defined user base who pays for the service not just because of the functionality of the tool, but because it's the type of search they'd like to see in the world. I'm a sucker for that type of narrative. I’m interested in any company that foregoes traditional venture capital funding and attempts to align their incentives with their customers. Kagi truly does appear to be trying their best, but I feel they've already had a few moments that peel back that early adoptor zeal.</p><p>Maybe it'll resonate more with you. Kagi's technology is good enough that it should be on your radar. So my final takeaway is: <a href="https://kagi.com/">Try it!</a> Kagi gives you a hundred free searches to kick its tires. See how things go. If you do want to give it a proper try after that, please just do yourself a favor and jump straight into the unlimited plan.</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[41% of Employers Worldwide Say They'll Reduce Staff by 2030 Due to AI (124 pts)]]></title>
            <link>https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131</link>
            <guid>42652076</guid>
            <pubDate>Fri, 10 Jan 2025 03:12:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131">https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131</a>, See on <a href="https://news.ycombinator.com/item?id=42652076">Hacker News</a></p>
Couldn't get https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[TikTok tells staff impacted by wildfires to use sick hours if they can't work (173 pts)]]></title>
            <link>https://techcrunch.com/2025/01/09/tiktok-tells-la-staff-impacted-by-wildfires-to-use-personal-sick-hours-if-they-cant-work-from-home/</link>
            <guid>42652056</guid>
            <pubDate>Fri, 10 Jan 2025 03:08:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/01/09/tiktok-tells-la-staff-impacted-by-wildfires-to-use-personal-sick-hours-if-they-cant-work-from-home/">https://techcrunch.com/2025/01/09/tiktok-tells-la-staff-impacted-by-wildfires-to-use-personal-sick-hours-if-they-cant-work-from-home/</a>, See on <a href="https://news.ycombinator.com/item?id=42652056">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Wildfires are currently <a href="https://www.cbsnews.com/live-updates/california-windstorm-fuels-pacific-palisades-wildfire-as-residents-flee-live-updates/" target="_blank" rel="noreferrer noopener nofollow">devastating the greater Los Angeles area</a>, burning over 45 square miles, torching over 1,300 structures, and putting nearly 180,000 people under evacuation orders as of Thursday. And yet, TikTok’s LA-based employees are being told to either continue their work from home or use their personal/sick days if that’s not possible, while the company’s LA office remains closed due to power outages caused by high winds.</p>

<p>Already, the Palisades Fire is close enough to TikTok’s office that smoke can be seen out the windows. But although the Culver City-based office itself is <a href="https://www.culvercity.org/News/LA-Wildfire-Update" target="_blank" rel="noreferrer noopener nofollow">not under mandatory evacuation orders</a> at this time, both it and many employees’ homes are impacted by the fires, windstorms, and related problems.</p>







<p>TikTok’s employees in the region hail from all over the broader LA area — some even commuting hours into work — and many of their homes are currently without power, Wi-Fi, or both, we understand from employee sources at TikTok. Some could even be under evacuation orders of their own (though we have not directly confirmed this at this time).</p>

<p>Unfortunately for staff dealing with this large-scale natural disaster, TikTok is telling them to use their personal or sick time to account for the days they need to take off due to these conditions.</p>

<p>In messages from TikTok leadership to LA staff, the company informed employees that the LA office would be closed on January 8 and would remain closed through Sunday, January 12, as the fires continued to ravage the area and the office itself is without power. The days the office is closed are being made Work From Home days as opposed to days off, however — unless an individual team leader decides otherwise. </p>

<p>In one message, an HR representative shared links to other company resources for those impacted by the fires, including a Mental Wellbeing Portal, a way to sign up for free mental health sessions with <a href="https://www.lyrahealth.com/" target="_blank" rel="noreferrer noopener nofollow">Lyra</a>, and a link to TikTok’s “PSSL” policy. The latter refers to TikTok’s paid sick and safe leave program — essentially, sick time and personal days.</p>

<p>TikTok’s LA employees have 10 paid sick/personal (PSSL) days per year in addition to 15 PTO (paid time off/vacation) days, if they were hired before June. These sick/personal days are highly coveted, too, as TikTok’s strict return-to-office policy requires employees to work from the office a minimum of three days per week. (The days of the week are chosen by the team and can’t be swapped for other days if needed.)</p>


<p>That means if an employee is feeling unwell, like with a simple cold or flu, and they don’t want to spread their illness to coworkers, they do have the option to stay home. But because they’re required to be in the office for three days each week, they would still have to use their PSSL hours and take the day off on those working-from-home-while-sick days (rather than being allowed to work from home with no penalty).</p>

<p>This week, TikTok’s LA staff are being asked to use their personal/sick days if&nbsp;they cannot work from home due to power or Wi-Fi outages, or if they’re under evacuation orders (unless their entire team has been given time off, which is not the case for many impacted by the fires). This leaves them fewer days later in the year to use in case of an actual illness or other personal emergency, like staying home to care for a sick child. If they don’t have enough PSSL hours available, they can either borrow from next year or use their PTO time instead, we understand.</p>

<p>Employees who can work from home still must go into their “My RTO” portal, where they manage their sick time, and change their work-from-home status to “natural disaster” to not be penalized. This won’t subtract from their PSSL hours, though. </p>







<p>Meanwhile, TikTok’s PSSL policy documentation doesn’t specifically state that the time can be used for natural disasters, such as these massive wildfires.</p>

<p>Instead, the policy says employees can use the time for either a physical or mental health condition, to take care of a family member with a health condition, or if the office is closed by the “order of public officials” due to a public health emergency, including exposures to an infectious agent, biological toxin, or hazardous material. (While, arguably, smoke in the area could be “hazardous,” not every TikTok LA employee facing poor air quality is also under an evacuation order enacted by a public official.)</p>

<p>In several internal messages shared with us, employees are reporting their home has no power, or their city overall has no power. (News reports indicate that some <a href="https://ktla.com/news/california/wildfires/millions-without-power-in-southern-california-map-shows-latest-outages/" target="_blank" rel="noreferrer noopener nofollow">4 million people are without power</a> due to the wildfires as of yesterday.) Some employees are worried about how bad their air quality is getting. Others are worried about using up their precious battery power or generator fuel just to work at home, as it’s unclear how long these power outages will last.</p>

<p>Given the pressure TikTok is under due to the <a href="https://techcrunch.com/2024/12/28/trump-asks-supreme-court-to-pause-imminent-tiktok-ban/">upcoming ban in the U.S.</a>, which is probably already impacting U.S. employees’ mental health and stress levels, being told to keep working through a disaster of this scale comes across as a little tone-deaf. In fact, some internal messages reviewed by TechCrunch have very much a “business-as-usual” vibe to them despite the scale of the disaster at hand. One lead, for example, reached out to an employee without power for a status update on some of their work, messages show. </p>

<p>Employees have been told to contact the EAP (Employee Assistance Program) or their HR rep if they are told they need to evacuate. Though there are many messages from leaders stressing that employees should put their own safety and well-being first, asking staff to worry about using personal days if they can’t work from home seems to counter that narrative.</p>

<p>TikTok was asked for comment but didn’t offer a response ahead of publication. After publication, the company issued a statement, shared with TechCrunch. (See below). </p>

<p>TikTok claims that any communications to LA employees telling them to use personal time if they can’t work from home due to fires, power outages or internet issues must be a misunderstanding. (We should note that we’ve seen screenshots of TikTok HR’s communications to staff that contradict these claims. Additionally, after the story was published, TikTok enabled a feature that now alerts everyone in a company-wide Lark channel — <a rel="nofollow" href="https://en.wikipedia.org/wiki/Lark_(software)">a Slack competitor from TikTok parent ByteDance</a> — when a screenshot is taken.)</p>

<p>“The safety and well-being of our employees is our highest priority,” a TikTok spokesperson said. “In light of current circumstances, our offices have been closed since Tuesday and will remain so for as long as necessary. While employees who can work from home safely are encouraged to do so, we also recognize the unique challenges this situation may present and are committed to supporting our team with flexibility if they are unable to work remotely at this time.”</p>







<p><em>Sarah Perez can be reached via email at sarahp@techcrunch.com or @sarahperez.01 on Signal.</em> <em>Note that this article was updated after publication with TikTok’s statement.</em></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Predictions Scorecard, 2025 January 01 (210 pts)]]></title>
            <link>https://rodneybrooks.com/predictions-scorecard-2025-january-01/</link>
            <guid>42651275</guid>
            <pubDate>Fri, 10 Jan 2025 00:27:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rodneybrooks.com/predictions-scorecard-2025-january-01/">https://rodneybrooks.com/predictions-scorecard-2025-january-01/</a>, See on <a href="https://news.ycombinator.com/item?id=42651275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-inner">
		<main id="main" role="main">
			<!-- .page-header -->
		

<article id="post-1698">
	
	<!-- .entry-header -->

	<div>
		<p>[You can follow me on social media: @rodneyabrooks.bsky.social and see my publications etc., at <a href="https://people.csail.mit.edu/brooks" target="_blank" rel="noopener">https://people.csail.mit.edu/brooks</a>]</p>
<p>This is my seventh annual update on how <a href="https://rodneybrooks.com/my-dated-predictions/" target="_blank" rel="noopener">my dated predictions</a> from January 1<sup>st</sup>, 2018&nbsp;concerning (1) <em>self driving cars</em>, (2) <em>robotics, AI , and machine learning</em>, and (3) <em>human space travel</em>, have held up. I promised then to review them at the start of the year every year until 2050 (right after my 95<sup>th</sup> birthday), thirty two years in total. The idea is to hold myself accountable for those predictions. How right or wrong was I?</p>
<p>I have decided to change my rules for myself a little bit after this year, in response to the many many people who have said how much they enjoy seeing my updates.</p>
<p>My predictions were mostly for the first few years, and by next year the density of due dates will be very low. &nbsp;So, on the eight anniversary of my first set of predictions, i.e., a year from today, I will be making a new set of predictions centered on the period January&nbsp;1<sup>st&nbsp;</sup>2026 to January&nbsp;1<sup>st</sup>&nbsp;2036, and that will give a new density of predictions where there will be real meat to see how accurately they turned out.</p>
<p><span><em>What I Want to Achieve and a Changing Hype-driven Landscape</em></span></p>
<p>The level of hype about AI, Machine Learning and Robotics completely distorts people’s understanding of reality. It distorts where VC money goes, always to something that promises impossibly large payoffs–it seems it is better to have an untested idea that would have an enormous payoff than a tested idea which can get to a sustainable business, but does not change the world for ever. It distorts what young researchers work on as they do not want to be seen as old fashioned even when the current hyped topic is sort of dumb–soon the dumbness is forgotten and the heat of the chase becomes all. It distorts what people think they need to get a degree in at college in order to have good career prospects.</p>
<p>I want people to use rational thought processes when they hear about hyped ideas and be able to assess what is really going on, and what is just plain (to use the technical term) bullshit.</p>
<p><span><em>My Color Scheme and Past Analysis</em></span></p>
<p>The acronyms I used for predictions in my original post were as follows.</p>
<p><strong>NET <em>year</em></strong> means it will not happen before that year (No Earlier Than)<br>
<strong>BY <em>year</em></strong> means I predict that it will happen by that year.<br>
<strong>NIML</strong>, Not In My Lifetime, i.e., not before 2050.</p>
<p>As time passes mentioned years I color then as <span>accurate</span>, <span>too pessimistic</span>, or&nbsp;<span>too optimistic</span>.</p>
<p>This year I have added <span>hemming and hawing</span>. This is for when something looks just like what I said would take a lot longer has happened, but the underlying achievement is not what everyone expected, and is not what was delivered. This is mostly for things that were talked about as being likely to happen with no human intervention and it now appears to happen that way, but in reality there are humans in the loop that the companies never disclose. So the technology that was promised to be delivered hasn’t actually been delivered but everyone thinks it has been.</p>
<p>I have not changed any of the text of the first three columns of the prediction tables since their publication on the first day of 2018. I only change the text in the fourth column to say what actually happened. &nbsp;This meant that by two years ago that fourth column was getting very long and skinny, so I removed them and started with fresh comments last year. I have kept last year’s comments and added new ones, with yellow backgrounds, for this year. If you want to see the previous five years of comments you can go back to&nbsp;&nbsp;<a href="https://rodneybrooks.com/predictions-scorecard-2023-january-01/" target="_blank" rel="noopener">the 2023 scorecard</a>.</p>
<h5>Overview of changes this year</h5>
<p>There has been a lot of activity in both self driving cars (the demise of Cruise a big push by Waymo to scale human assisted deployments, and lots of smoke and mirrors from an electric car company) and in AI, where robotics has been pulled into the ultra hyposphere while in generative AI the end of scaling and the introduction of inference mechanisms (!!) have been hotly announced and disputed. &nbsp;The human spaceflight endeavor, as it did last year, has crawled along and again has stretched out dates that were probably too optimistic in the first place.</p>
<h2><span>But First.</span></h2>
<p><span>&lt;rant&gt;</span></p>
<p>We all know about FOMO, Fear Of Missing Out. In late 2023, for a talk on generative AI that I gave at MIT,&nbsp;<a href="https://www.youtube.com/watch?v=pgrzEHJTPPM" target="_blank" rel="noopener">I coined another acronym</a>, &nbsp;FOBAWTPALSL, Fear Of Being A Wimpy Techno-Pessimist And Looking Stupid Later. Perhaps that one is a little bit too much of a mouthful to catch on. These two human insecurities lead people to herd-like behavior in establishing and propagating the zeitgeist on almost any topic.</p>
<p>They lead to people piling on the hype fiestas, rushing to invest (money, effort, or hope) in marginal ideas once they have become a little bit popular, or <a href="https://www.nytimes.com/2024/12/24/nyregion/new-jersey-new-york-drones.html" target="_blank" rel="noopener">believing our airspace is being invaded by foreign drones</a>.</p>
<p>“Mounting evidence, <a title="" href="https://www.nytimes.com/2024/12/19/video/new-jersey-drones-planes-videos.html" target="_blank" rel="noopener">and lack thereof</a>, suggests that perhaps the whole craze has been a sort of communal fever dream fueled by crowd mentality, confirmation bias and a general distrust in all things official.”</p>
<p>That quote is from the drone story linked to above, but it could well as been about the hype that we are moving towards AGI (Artificial General Intelligence).</p>
<p><span>I want to be clear, as there has been for almost seventy years now, there has been significant progress in Artificial Intelligence over the last decade. There are new tools and they are being applied widely in science and technology, and are changing the way we think about ourselves, and how to make further progress.</span></p>
<p>That being said, we are not on the verge of replacing and eliminating humans in either white collar jobs or blue collar jobs. Their tasks may shift in both styles of jobs, but the jobs are not going away. We are not on the verge of a revolution in medicine and the role of human doctors. We are not on the verge of the elimination of coding as a job. We are not on the verge of replacing humans with humanoid robots to do jobs that involve physical interactions in the world. We are not on the verge of replacing human automobile and truck drivers world wide. We are not on the verge of replacing scientists with AI programs.</p>
<p>Breathless predictions such as these have happened for seven decades in a row, and each time people have thought the end is in sight and that it is all over for humans, that we have figured out the secrets of intelligence and it will all just scale. &nbsp;The only difference this time is that these expectations have leaked out into the world at large. I’ll analyze why this continues to happen below in the section on AI and ML.</p>
<p>Here is a list of some of those hype cycles that I, personally, have perceived and lived through, as taken from my presentation at MIT in late 2023 that I referenced above re FOBAWTPALSL.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/AIhypecycles.jpg" alt="" width="2842" height="1590"></p>
<p>Really, was there really hype about all these things? &nbsp;Yes, there was, within the circles that cared. Those circles have gotten wider and wider and when reigning world chess champion Garry Kasparov was beaten by I.B.M.’s Deep Blue computer under tournament conditions in 1997 it was widely reported in the popular press, And it was declared that it was all over for humans.</p>
<p>Back in February 2011 a computer program named Watson <a href="https://www.pbs.org/wgbh/nova/article/watson-and-jeopardy/" target="_blank" rel="noopener">played on the television game show Jeopardy against all time human champions</a>. John Markoff, legendary technology reporter at the New York Times, wrote stories about this <a href="https://www.nytimes.com/2011/02/15/science/15essay.html" target="_blank" rel="noopener">the day before</a> the competition, and <a href="https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html" target="_blank" rel="noopener">the day after</a>, when Watson had indeed beaten the humans, with the same questions (fed as text to it as the same time as the humans heard the questions) all running on a cluster of machines not connected to an outside network. Here are three successive paragraphs from the second of those stories.</p>
<p><span><span>For I.B.M., the future will happen very quickly, company executives said. On Thursday it plans to announce that it will collaborate with Columbia University and the University of Maryland to create a physician’s assistant service that will allow doctors to query a cybernetic assistant. The company also plans to work with Nuance Communications Inc. to add voice recognition to the physician’s assistant, possibly making the service available in as little as 18 months.</span></span></p>
<p><span>“I have been in medical education for 40 years and we’re still a very memory-based curriculum,” said Dr. Herbert Chase, a professor of clinical medicine at Columbia University who is working with I.B.M. on the physician’s assistant. “The power of Watson- like tools will cause us to reconsider what it is we want students to do.”</span></p>
<p><span>I.B.M. executives also said they are in discussions with a major consumer electronics retailer to develop a version of Watson, named after I.B.M.’s founder, Thomas J. Watson, that would be able to interact with consumers on a variety of subjects like buying decisions and technical support.</span></p>
<p>My personal experience at that time was people I did not know, but who had heard about my role at MIT (as director of the MIT AI Lab, and then founding director of MIT CSAIL, the Computer Science and Artificial Intelligence Lab) would come up to me and ask about the future of medicine. The people were variously doctors or health industry executives. I reassured them that medicine as we knew it then would stay much the same and was not about to be rendered obsolete.</p>
<p>And then in 2016 Geoff Hinton, one of the key architects of Deep Learning (which has had undeniable impact on the world) said:</p>
<p>“People should stop training radiologists now. It is just completely obvious that within five years deep learning is going to be better than radiologists.”</p>
<p>More people asking me whether this was true. It wasn’t in five years and it isn’t now. We need more radiologists than ever. And yes they do use deep learning tools to help them see some things they wouldn’t otherwise see. But they also understand anomalies using causal reasoning and we would be in a sorry state if all radiology was done by programs today.</p>
<p>Now look at those plum colored paragraphs above again as you take yourself way back in time to a year or so ago when ChatGPT was just a baby AGI, You can find stories just like this one if you substitute “ChatGPT” for “Watson” and “Microsoft” for “I.B.M.”</p>
<p>The things confidently predicted in 2011 (and in 1979, and in 2016) about the end of doctors didn’t happen then and it is not happening now. Nor are all the other jobs ending.</p>
<p>Today I get asked about humanoid robots taking away people’s jobs. In March 2023 I was at a cocktail party and there was a humanoid robot behind the bar making jokes with people and shakily (in a bad way) mixing drinks. A waiter was standing about 20 feet away silently staring at the robot with mouth hanging open. I went over and told her it was tele-operated. “Thank God” she said. (And I didn’t need to explain what “tele-operated” meant). Humanoids are not going to be taking away jobs anytime soon (and by that I mean not for decades).</p>
<p>You, you people!, are all making fundamental errors in understanding the technologies and where their boundaries lie. Many of them will be useful technologies but their imagined capabilities are just not going to come about in the time frames the majority of the technology and prognosticator class, deeply driven by FOBAWTPALSL, think.</p>
<p>But this time it is different you say. This time it is really going to happen. You just don’t understand how powerful AI is now, you say. All the early predictions were clearly wrong and premature as the AI programs were clearly not as good as now and we had much less computation back then. This time it is all different and it is for sure now.</p>
<p>Yeah, well, I’ve got a <a href="https://en.wikipedia.org/wiki/Predictions_and_claims_for_the_Second_Coming" target="_blank" rel="noopener">Second Coming</a> to sell you…</p>
<p><span>&lt;/rant&gt;</span></p>
<h5>Self Driving Cars</h5>
<p>As with <em>flying cars</em> the definition, or common understanding, of what <em>self driving cars</em>&nbsp;really means has changed since my post on predictions seven years ago. &nbsp;At that time self driving cars meant that the cars would drive themselves to wherever they were told to go with no further human control inputs.</p>
<p>Now self driving cars means that there is no one in the drivers seat, but there may well be, and in all cases so far deployed, <a href="https://www.nytimes.com/2024/09/11/insider/when-self-driving-cars-dont-actually-drive-themselves.html" target="_blank" rel="noopener">humans monitoring those cars from a remote location</a>, and occasionally sending control inputs to the cars. The companies do not advertise this feature out loud too much, but they do acknowledge it, and the reports are that it happens somewhere between every one to two miles traveled. These inputs are not direct control of the normal human mechanism of control the steering wheel, the brakes, and the accelerator. &nbsp;Rather they are advice that overrides some of the algorithms. &nbsp;For instance, “steer out into the next lane and go around this truck” as the human realizes that the truck is just not going to move (see an anecdote below on the first night I took the new Waymo taxis in San Francisco (I had previously last ridden a Waymo in 2012 in Mountain View)).</p>
<p>Why is this difference important? &nbsp;One of the motivations for self driving cars was that the economics of taxis, cars that people hire at any time for a short ride of a few miles from where they are to somewhere else of their choosing, would be radically different as there would be no driver. Systems which do require remote operations assistance to get full reliability cut into that economic advantage and have a higher burden on their ROI calculations to make a business case for their adoption and therefore their time horizon to scaling across geographies.</p>
<p>But wait, you might say, isn’t that electric car company that used to be based in California and is now based in Texas going to roll this out imminently and have a fully digital taxi service. They demoed it on a Hollywood movie studio lot just this year, and the cars were painted gold. Hmm. The location of the demo and the fact that the cars, even down to the tires, were painted gold tells you everything you need to know. Both the cars and the humanoid robots at that event were presented as autonomous but in reality they were all tele-operated directly by people (see below in the humanoid section for more details). And <a href="https://gizmodo.com/tesla-is-looking-to-hire-a-team-to-remotely-control-its-self-driving-robotaxis-2000530600" target="_blank" rel="noopener">that same electric car company is actively hiring people into paying jobs as remote operators</a>.</p>
<p>There was a reasonably balanced appraisal from <a href="https://www.reuters.com/technology/teslas-musk-unveil-robotaxis-amid-fanfare-skepticism-2024-10-10/" target="_blank" rel="noopener">Reuters</a> just after the event, though it does not go into details of the demos. Here is a direct quote from the story:</p>
<p>“We do expect to start fully autonomous unsupervised FSD in Texas and California next year.” Musk said.</p>
<p>The astute reader will note that this is the 11<sup>th</sup> year in a row that the CEO of Tesla has made this prediction of the same milestone happening the next year. We can admire the consistency.</p>
<p><em>Actual</em> <a href="https://www.slashgear.com/1605007/autonomous-driving-plateau-engineer-expert-explains/" target="_blank" rel="noopener">self-driving is now generally accepted to be much harder than every one believed</a>.</p>
<p>The reason that this bait and switch is important to understand is that the promise of inevitable fully self driving technology upended a historical way that new transportation systems have been adopted.</p>
<p>In the past whenever we have introduced new transportation mechanisms there have been large investments in infrastructure and that infrastructure is shared and used by everyone. The Romans built roads so soldiers and traded goods could travel long distances–in Europe those road networks are still the basis of today’s road networks. When steam engine driven trains were the new transportation technology vast networks of rails were built allowing goods to move long distances in mere hours or days. When Ford started mass production of automobiles he built roads and the local governments followed and the the Federal government followed, and those roads are what we use today.</p>
<p>Actual fully self driving cars promised that no infrastructure changes would be needed to revolutionize how vehicles would be controlled. Each individual vehicle would do what was needed all by itself. As sensors and networks got better there was no need for expensive new infrastructure because of this promise.</p>
<p>The promise was false. If government and private partnerships in building smart roads, which was a hot topic in the 1990s. had continued, every one of us would now have smarter safer cars, but still with onboard human drivers taking over in many situations. But we would have had smart freeways where once you were on it your car would be self driving. The road would have had lots of sensors effectively shared across all cars, as that data would have been transmitted to all passing cars. It would have been a fraction of the cost per car compared to the sensing on today’s almost but not really self driving cars like those of Waymo. And we would have had much more accurate congestion data where the root causes of local congestion would have been sensed with semantic understanding rather than just inferring it from the aggregate collection of location data from phones, individual cars, and historical data from roadside sensors.</p>
<p>Instead we now have individual corporate actors using a mixture of partial self driving and remote human supervision. The big question is whether the economics of this works at scale, and whether the fake promises will drive out the human drivers in cheaper services and we’ll all end up paying more. Will the level of hype we saw push our decentralized transportation system into the hands of a few wealthy companies, and in effect make it a centralized system where everybody has to pay private companies to be part of it?</p>
<p>As a reminder of how strong the hype was and the certainty of promises that it was just around the corner here is a snapshot of a whole bunch of predictions by major executives from 2017.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Screenshot-2024-12-21-at-4.02.30%E2%80%AFPM.png" alt="" width="2470" height="1280"></p>
<p>I have shown this many times before but there is one new annotation here for 2024. The years in parentheses are when the predictions were made. The years in blue are the years are the predicted years of achievement. When a blue year is shaded pink it means that it did not come to pass by then. The predictions with orange arrows are those that I had noticed had later been retracted.</p>
<p>The prediction that Jaguar and Land-Rover made that they would have fully autonomous cars by 2024 did not come to pass, so I have shaded it pink,</p>
<p>Note that every single blue year up until now is shaded pink, and that every one that is shaded pink has still not come to pass. None of the predictions that were out there in 2017 for the next few years have happened. &nbsp;None. There are three more for 2025, and I am sure that a year from now they will all be shaded pink also.</p>
<p>One of the big selling points of self driving cars was that they would be safer than cars driven by humans. So far that is not holding up with real data. One electric car maker with self driving software had it disengage when it sensed there would be an accident, supposedly so that the human could take over in a split second. And then the company did not report the incident as the fault of the software as it was no longer controlling the car when the impact occurred. It was reported, and I had this experience myself in my last ride in a Cruise in 2023, that Cruise vehicles would freeze when an accident looked likely, and then not report it as their software’s fault as the car was stationary and was hit by another car. In many reported cases, and in my case, simply continuing to move forward would avert any likely accident (fortunately for me the human driver of the other car slammed on the brakes and did not hit my robot vehicle).</p>
<p>In&nbsp;<a href="https://www.washingtonpost.com/business/2024/05/24/all-major-robotaxi-firms-are-facing-federal-safety-investigations/" target="_blank" rel="noopener">this story from the Washington Post</a>&nbsp;about Federal investigations into the safety incidents with self driving cars, they report that the companies involved claim they have vast amounts of driving on our roads under their belt. Not so.</p>
<p>An industry association says autonomous vehicles have logged a total of 70 million miles, a figure that it compares to 293 trips to the moon and back. But it’s a tiny fraction of the almost 9 billion miles that Americans drive every day. The relatively small number of miles the vehicles have driven makes it difficult to draw broad conclusions about their safety.</p>
<p>To put that into perspective, the total number of miles driven by all autonomous (sort of) vehicles over the last decade is less than 1% of the miles driven by humans every day in the United States. It is a tiny, tiny portion.</p>
<p>Take a look at this embedded video from the Wall Street Journal about investigations of crashes (many of which have been fatal) involving autonomous driving systems.</p>
<p><iframe width="660" height="371" src="https://www.youtube.com/embed/mPUGh0qAqWA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="The Hidden Autopilot Data That Reveals Why Teslas Crash | WSJ"></iframe></p>
<p>From the audio:&nbsp;“The kinds of things that tend to go wrong with these systems are things like it was not trained on, pictures of an overturned double trailer. It just didn’t know what it was. There were some lights there, but the lights were in unusual positions. A person would have clearly said something big is in the middle of the road. But the way machine learning works is it trains it on a bunch of examples and if it encounters something it doesn’t have a bunch of examples for it may have no idea what’s going on.”</p>
<p>[[My own take is that the fetish of end to end learning leads people to leave out well known algorithms that might solve many of &nbsp;these problems (e.g,, the incredibly <a href="https://www.sciencedirect.com/science/article/pii/S0042698911003646" target="_blank" rel="noopener">simple time to collision algorithms based on looming</a>). Yes, end to end learning made speech understanding systems better, but that does not mean it is the appropriate fetish to apply everywhere.]]</p>
<p><strong><span>Pro tip:</span></strong> Think about this history of industry prognostications about fully autonomous driving being just around the corner when you read today’s prognostications about LLMs taking jobs, en masse, in the next couple of years, or humanoid robots being dirt cheap and being able to learn how to do any human manual task real real soon now. You know you have seen this movie before…</p>
<p><span><em>My own experiences with Waymo in 2024</em></span></p>
<p>I have two sorts of experiences with Waymo vehicles. First, as a driver of my own vehicle and sharing road space with them every single time that I drive. And second, as a user of their ride service.</p>
<p>The streets of San Francisco had been thick with Waymo vehicles with no driver in them especially in the second half of 2024. As I drive across the city every morning to head down to my robotics/AI startup half way down the peninsula I see them everywhere until I get on to 101. &nbsp;I see them in front of me and behind me and in adjacent lanes as I drive on multilane one way streets. Sometimes I see four of them in a single block. Twice I’ve seen four of them in a line, in my block and could see four of them in a line in the block ahead of me. &nbsp;When I am at four way intersections with no traffic lights I see them participating in the social ritual of taking your turn to drive through the intersection in the order you stopped, except when a pedestrian is crossing in front of you. They do that pretty well. They do less well when they accidentally get into a line of parents’ cars snaking around a corner for school drop off or pickup.</p>
<p>Over the last few months I have noticed that in general they are getting more aggressive about stretching the rules, just like people do. Otherwise human drivers (including me) take advantage of their politeness. That aggression is not always welcomed. One morning I saw a workman with a group doing some digging on a road, and holding a sign with SLOW on one side and STOP on the other side have to jump in front of a Waymo to get it to do what he was trying to tell it to do with the sign. STOP. It wasn’t stopping for no stinking sign!</p>
<p>The only time I have seen a Waymo go into reverse, ever, was when I was illegally driving the wrong way down a single lane street and we were heading straight at each other.</p>
<p>As a rider I feel they are not quite aggressive enough with human drivers some time, so a ride in a Waymo takes longer than with an Uber or Lyft.</p>
<p>It is hit and miss where they drop me off. Sometimes they take a place to pull over half a block from my house, even when it is raining. There is no way to adjust what they happen to decide that day, even though I know that they will always be able to pull in right in front of my house.</p>
<p>The first time I took a Waymo this year, on the way home it picked me up at a restaurant and then was about to make a right turn. But at that corner there was an 18 wheeler with its lights flashing and surrounded by green cones. It pulled right in behind that truck and waited a long time before it drove forward. I am guessing a remote operator intervened told it to go around because eventually it pulled around it in the lane just to the left. Based on seeing Waymos interact with orange cones I suspect it would have done better if the cones had been orange rather than green. &nbsp;This easily illustrates that the learning that this robot does, and indeed any robot does, is nothing like the learning that people do (see my rant about the seven deadly sins and mistaking performance for competence in the section below on advances in AI and ML).</p>
<p>I mostly feel safe when I am a passenger in a Waymo. &nbsp;Sometimes I don’t feel that my driver of an Uber that I am taking rides with Uber that are not as safe as I would prefer.</p>
<p><span><em>Self Driving Taxi Services</em></span></p>
<p>There have been three self driving taxi services in the US in various stages of play over the last handful of years, though it turns out, as pointed out above that all of them have remote operators. They are Waymo, Cruise, and Zoox.</p>
<p>Waymo and Cruise are similar in that they use conventional cars adorned with lots of sensors. Zoox has purpose built vehicles that have no steering wheel or pedals for brake or accelerator.</p>
<p>Waymo and Cruise went for deployments in large parts of two or more cities and have had ride services callable by apps, just as one can do with Uber or Lyft. Zoox is smaller scale, much more restricted in geography, and really not comparable.</p>
<p>At this time last year Cruise was in trouble has it had suspended all of its San Francisco operations under pressure from regulators after some bad accidents that happened in a way that never would happen for human driven cars. &nbsp;Briefly, their cars were getting hit at night by emergency vehicles with lights flashing as the Cruise cars crossed intersections. Human drivers see the reflections of lights from such vehicles flashing even if they don’t see the vehicles themselves. The Cruise vehicles were only reacting to flashing lights that they could perceive directly. But the accident that tipped the scales was when a pedestrian crossing in front of a human driven vehicle was hit and went flying in the air landing right in front of a Cruise. The Cruise hit the person (who now disappeared from sight) as a human driver would most likely have done. But then it proceeded to drive 20 feet with the human underneath the vehicle being dragged along as it went into a mode where it was supposed to get off the road. A human driver would not have reacted that way to having been in a collision, even if it was not their fault.</p>
<p>The hammer finally fell in December of 2024. General Motors shut down Cruise. The leading paragraphs from this <a href="https://www.wsj.com/business/autos/general-motors-scraps-cruise-robotaxi-program-ea3298a8" target="_blank" rel="noopener">linked story</a>&nbsp;from the Wall Street Journal&nbsp;are:</p>
<div>
<p>General Motors has scrapped its Cruise robotaxi program after nearly a decade and $10 billion in development, citing the time and costs needed to scale the business and rising competition.</p>
<p>GM on Tuesday said it plans to realign its autonomous driving strategy and give priority to development of advanced driver assistance systems, which take over steering and other functions in certain situations and are common on new vehicles today.</p>
<p>The automaker said it would continue to develop fully autonomous technology for personal vehicles, and build on the progress of its Super Cruise system, a hands-off, eyes-on driving feature that the company introduced several years ago.</p>
<p data-type="paragraph">GM said it owns about 90% of Cruise and intends to buy out the remaining investors. It plans to combine the technical teams from Cruise and GM into a single effort to advance autonomous and assisted driving.</p>
<p>“We want to leverage what already has been done as we go forward in this,” Chief Executive Mary Barra told analysts on a call Tuesday.</p>
<p>The Detroit automaker said it expects the restructuring to reduce spending by more than $1 billion annually after the proposed plan is completed, which is expected in the first half of next year.</p>
</div>
<p>While there are 40 companies that have permits to test autonomous driving in California, alone, the demise of Cruise leaves just one company, Waymo, trying to make an actual go of a digital taxi service in the United States. They have an enormous significant lead over anyone else who wants get into this business and have spent billions of dollars (probably very much north of $10 billion) on this endeavor over the last 15 years. In an email they sent me a couple of weeks ago as a user of their services they reported that they provided 4 million customer rides in 2024. That is approximately 4 million more than any other company in the United States.</p>
<p><span><span><i>Waymo</i></span></span></p>
<p>Despite being so far out in front it has not been all smooth sailing for Waymo.</p>
<p>Early in the year the operations center for Waymo somehow neglected to realize it was Chinese New Year in Chinatown in San Francisco. So Waymo vehicles were routed through that area on the biggest night of celebration. Any human driver would have realized that the streets, i.e., the street surfaces where cars usually drive, were completely packed with humans, no doubt some of whom were intoxicated as well as just being out having a good time. Not so the Waymo vehicles. They tried pushing through the very very dense crowds, no doubt annoying many people. And what do people have at Chinese New Year? &nbsp;Fireworks. So some revelers decided to push back on this robot car invading their space. Here are a couple of pictures of the results.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Waymo1.jpg" alt="" width="1924" height="1110"></p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Waymo2.jpg" alt="" width="1916" height="1440"></p>
<p>Not pretty. &nbsp;And an example of how taking away people’s agency is never a good idea for robots (<a href="https://rodneybrooks.com/rodney-brooks-three-laws-of-robotics/" target="_blank" rel="noopener">see my second law of robotics</a>).</p>
<p>Throughout 2024 Waymo has been investigates for various accidents such as those described in <a href="https://www.wsj.com/business/autos/regulators-probe-alphabets-waymo-after-22-self-driving-car-incidents-996fde65" target="_blank" rel="noopener">this Wall Street Journal article</a>. “Reports included collisions with stationary or semistationary objects, such as gates, chains or parked vehicles, according to the regulator.”</p>
<p>In the middle of the summer Waymo added a feature where they would honk their horns at cars in their way. But this backfired when hundreds of Waymos were coming back to their parking lot in the very early hours of the morning, and they started honking at each other and <a href="https://www.designnews.com/automotive-engineering/neighbors-hate-waymo-horn-hassles" target="_blank" rel="noopener">waking up human neighbors</a>. Eventually that got fixed.</p>
<p>In late September a motorcade for Kamala Harris in San Francisco was brought to a halt by a Waymo that <a href="https://sfstandard.com/2024/09/28/waymo-halts-kamala-harris-san-francisco-motorcade/" target="_blank" rel="noopener">stopped in the middle of California Street</a> doing a U-turn in front of it. I’m sure this incident was of great concern to the Secret Service. Eventually a San Francisco police officer got into the car and drove it out of the way–this is shown in a video included with the story above. I do not know how the officer got access to the vehicle and whether Waymo remote operations were cooperating.</p>
<p>More disturbingly humans outside the Waymos started harrassing humans inside them. The most concerning cases come from the realization that if a woman is in a Waymo at night she will be dropped off, outside, on a public road at the end of her journey with no option but to get out of the car where it has stopped. So groups of men have followed Waymos with women in them and then harassing the woman when she gets out. If she was driving her own car she might be heading to an off road parking space or she might choose not to stop if she knows she is being followed. There are no such options in a Waymo so taking a Waymo at night is less safe than other means of transportation–just follow it and eventually the preyed upon woman will have to get out. Here is a <a href="https://www.washingtonpost.com/technology/2024/12/22/waymo-robotaxi-passengers-harassment/" target="_blank" rel="noopener">very recent disturbing story</a>&nbsp;about this practice.</p>
<p>Meanwhile Waymo <a href="https://electrek.co/2024/10/25/waymo-secures-5-6-billion-funding-expanding-robotaxis-new-cities-2025/" target="_blank" rel="noopener">managed to raise $5.6B to expand to new cities in 2025</a>. It already operates in parts of San Francisco, Los Angeles, and Phoenix. The new money will let it expand to Austin and Atlanta in the United States and to start operating in parts of Tokyo in Japan. That is expensive expansion.</p>
<p>Here is the question for the future of watered down remote monitored “autonomous” driving systems (let’s call it “watered down autonomy”), and it is up to Waymo now. Can Waymo expand fast enough in these new markets in 2025 and take enough business from what is left of traditional taxi operators, along with those operating under the Uber and Lyft models, and do it in a way which is in sight of profitability, so that it has a case to raise the stupendous amounts of money needed to operate in all large cities in the US in the next 10 t0 20 years?</p>
<p>If Waymo can not succeed at this in the next two years I think the idea of large scale use of watered down autonomy will be dead for at least a decade or two. Right now full autonomy everywhere is already dead.</p>

<table id="tablepress-30">
<thead>
<tr>
	<th>Prediction<br>
[Self Driving Cars]</th><th>Date</th><th>2018 Comments</th><th>Updates</th>
</tr>
</thead>
<tbody>
<tr>
	<td>A flying car can be purchased by any US resident if they have enough money.</td><td>NET 2036</td><td>There is a real possibility that this will not happen at all by 2050.<br>
</td><td></td>
</tr>
<tr>
	<td>Flying cars reach 0.01% of US total cars.</td><td>NET 2042</td><td>That would be about 26,000 flying cars given today's total.</td><td></td>
</tr>
<tr>
	<td>Flying cars reach 0.1% of US total cars.</td><td>NIML</td><td></td><td></td>
</tr>
<tr>
	<td>First dedicated lane where only cars in truly driverless mode are allowed on a public freeway.<br>
</td><td><p>NET 2021</p></td><td>This is a bit like current day HOV lanes. My bet is the left most lane on 101 between SF and Silicon Valley (currently largely the domain of speeding Teslas in any case). People will have to have their hands on the wheel until the car is in the dedicated lane.</td><td></td>
</tr>
<tr>
	<td>Such a dedicated lane where the cars communicate and drive with reduced spacing at higher speed than people are allowed to drive</td><td><p>NET 2024</p></td><td></td><td><span>20240101</span> <p>This didn't happen in 2023 so I can call it now. But there are no plans anywhere for infrastructure to communicate with cars, though some startups are finally starting to look at this idea--it was investigated and prototyped by academia 20 years ago.</p></td>
</tr>
<tr>
	<td>First driverless "taxi" service in a major US city, with dedicated pick up and drop off points, and restrictions on weather and time of day.</td><td><p>NET 2021</p></td><td>The pick up and drop off points will not be parking spots, but like bus stops they will be marked and restricted for that purpose only.</td><td><span>20240101</span> <p>People may think this happened in San Francisco in 2023, but it didn't. Cruise has now admitted that there were humans in the loop intervening a few percent of the time. THIS IS NOT DRIVERLESS. Without a clear statement from Waymo to the contrary, one must assume the same for them. Smoke and mirrors.</p></td>
</tr>
<tr>
	<td>Such "taxi" services where the cars are also used with drivers at other times and with extended geography, in 10 major US cities</td><td><p>NET 2025</p></td><td>A key predictor here is when the sensors get cheap enough that using the car with a driver and not using those sensors still makes economic sense.</td><td><span>20250101</span> <p>Imminent dual use of personal cars was the carrot that got lots of people to pay cash when buying a Tesla for the software subscription that would allow thei car to operate in this way. Shockingly the CEO of Tesla announced in smoke and mirrors roll out of Cyber Cab in 2024, that the service would use specially built vehicles to be produced at some indeterminate late date. I got suckered by his hype. This is unlikely to happen in the first half of this century.</p></td>
</tr>
<tr>
	<td>Such "taxi" service as above in 50 of the 100 biggest US cities.</td><td>NET 2028</td><td>It will be a very slow start and roll out. The designated pick up and drop off points may be used by multiple vendors, with communication between them in order to schedule cars in and out.<br>
</td><td><span>20250101</span> <p>Even the watered down version of this with remote operators is not gong to happen in 50 cities by 2028. Waymo has it in 3 cities and is currently planning on 2 more in the US in 2025.</p></td>
</tr>
<tr>
	<td>Dedicated driverless package delivery vehicles in very restricted geographies of a major US city.</td><td><p>NET 2023</p></td><td>The geographies will have to be where the roads are wide enough for other drivers to get around stopped vehicles.<br>
</td><td></td>
</tr>
<tr>
	<td>A (profitable) parking garage where certain brands of cars can be left and picked up at the entrance and they will go park themselves in a human free environment.</td><td><p>NET 2023</p></td><td>The economic incentive is much higher parking density, and it will require communication between the cars and the garage infrastructure.</td><td></td>
</tr>
<tr>
	<td>A driverless "taxi" service in a major US city with arbitrary pick and drop off locations, even in a restricted geographical area.<br>
</td><td>NET 2032
<p>NET 2032</p></td><td>This is what Uber, Lyft, and conventional taxi services can do today.</td><td><span>20240101</span> <p>Looked like it was getting close until the dirty laundry came out.</p><span>20250101</span> <p>Waymo now has a service that looks and feels like this in San Francisco, 8 years earlier than I predicted. But it is not what every one was expecting. There are humans in the loop. And for those of us who use it regularly we know it is not as general  case on drop off and pick up as it is with human drivers.</p></td>
</tr>
<tr>
	<td>Driverless taxi services operating on all streets in Cambridgeport, MA, and Greenwich Village, NY. </td><td>NET 2035</td><td>Unless parking and human drivers are banned from those areas before then.</td><td></td>
</tr>
<tr>
	<td>A major city bans parking and cars with drivers from a non-trivial portion of a city so that driverless cars have free reign in that area.</td><td>NET 2027<br>
BY 2031</td><td>This will be the starting point for a turning of the tide towards driverless cars.</td><td></td>
</tr>
<tr>
	<td>The majority of US cities have the majority of their downtown under such rules.</td><td>NET 2045</td><td></td><td></td>
</tr>
<tr>
	<td>Electric cars hit 30% of US car sales.</td><td>NET 2027</td><td></td><td><span>20240101</span> <p>This one looked pessimistic last year, but now looks at risk. There was a considerable slow down in the second derivative of adoption this year in the US.</p><span>20250101</span> <p>Q3 2024 had the rate 8.9% so there is no way it can reach 30% in 2027. I was way too optimistic at a time when EV enthusiasts thought I was horribly pessimistic.</p></td>
</tr>
<tr>
	<td>Electric car sales in the US make up essentially 100% of the sales.</td><td>NET 2038<br>
</td><td></td><td></td>
</tr>
<tr>
	<td>Individually owned cars can go underground onto a pallet and be whisked underground to another location in a city at more than 100mph.</td><td>NIML</td><td>There might be some small demonstration projects, but they will be just that, not real, viable mass market services.<br>
</td><td></td>
</tr>
<tr>
	<td>First time that a car equipped with some version of a solution for the trolley problem is involved in an accident where it is practically invoked.</td><td>NIML</td><td>Recall that a variation of this was a key plot aspect in the movie "I, Robot", where a robot had rescued the Will Smith character after a car accident at the expense of letting a young girl die.</td><td></td>
</tr>
</tbody>
</table>
<!-- #tablepress-30 from cache -->
<p><span><em>Electric Cars</em></span></p>
<p>Last year US manufacturers pulled back on their planned production of EVs. In data from <a href="https://caredge.com/guides/electric-vehicle-market-share-and-sales" target="_blank" rel="noopener">this report</a> we can see that sales dropped at the start of 2024 but have now picked up again.</p>

<table id="tablepress-34">
<tbody>
<tr>
	<td>2022</td><td>2022</td><td>2022</td><td>2022</td><td>2023</td><td>2023</td><td>2023</td><td>2023</td><td>2024</td><td>2024</td><td>2024</td>
</tr>
<tr>
	<td>Q1</td><td>Q2</td><td>Q3</td><td>Q4</td><td>Q1</td><td>Q2</td><td>Q3</td><td>Q4</td><td>Q1</td><td>Q2</td><td>Q3</td>
</tr>
<tr>
	<td>5.3%</td><td>5.6%</td><td>6.1%</td><td>6.5%</td><td>7.3%</td><td>7.2%</td><td>7.9%</td><td>8.1%</td><td>7.3%</td><td>8.0%</td><td>8.9%</td>
</tr>
</tbody>
</table>
<!-- #tablepress-34 from cache -->
<p>There is steady growth in sales but my prediction of 30% of US car sales being electric by 2027 now seems wildly optimistic. We need two doublings to get there in three years and the doubling rate seems more like one doubling in four to five years.</p>
<p>Note that some sources include hybrids and hydrogen powered cars in electric vehicles but I am using the battery electric vehicle (BEV) numbers.</p>
<p>To see how the trends are across brands you can see a breakout for Q2 of 2024 <a href="https://www.coxautoinc.com/wp-content/uploads/2024/07/Q2-2024-Kelley-Blue-Book-Electric-Vehicle-Sales-Report.pdf" target="_blank" rel="noopener">here</a>.</p>
<p>There appear to be two main headwinds for BEV adoption. Firstly, if one doesn’t have on property residential parking it is hard work in the US to find a place to recharge, and it takes hours for the charging to finish. This will stop many city dwellers from adopting. Secondly the increased tire wear adds up to real money. The maintenance requirements for BEVs are much less than for cars with an internal combustion engine. On the other hand tires do not last as long (I have had to buy four new tires in less than two years owning my first BEV), <a href="https://www.telegraph.co.uk//money/consumer-affairs/my-electric-car-heavy-had-change-tyres-after-7500-miles/" target="_blank" rel="noopener">apparently due to the increased weight of the car</a>.</p>
<p><span><em>Flying Cars</em></span></p>
<p>Flying cars are another category where the definitions have changed. Back when I made my predictions it meant a vehicle that could both drive on roads and fly through the air. &nbsp;Now it has come to mean an electric multi-rotor helicopter than can operate like a taxi between various fixed landing locations. Often touted are versions that have no human pilot. These are known as eVTOLs, for “electric vertical take off &amp; landing”.</p>
<p>Large valuations have been given to start ups who make nice videos of their electric air taxis flying about. But on inspection one sees that they don’t have people in them. Often, you might notice, even those flights are completely over water rather than land. I wrote about the <a href="https://rodneybrooks.com/where-are-the-crewed-evtol-videos/" target="_blank" rel="noopener">lack of videos of viable prototypes</a> back in November 2022.</p>
<p>Nevertheless there have been wild predictions. &nbsp;I ended a longer version of this component in <a href="https://rodneybrooks.com/predictions-scorecard-2024-january-01/" target="_blank" rel="noopener">last year’s annual review</a> with:</p>
<p>Also note the size of this vehicle. There are many fossil fuel powered helicopters that are much smaller. This is not going to be a personally owned vehicle for the masses.</p>
<p>Don’t hold your breath. They are not here. They are not coming soon.</p>
<p>Nothing has changed. Billions of dollars have been spent on this fantasy of personal flying cars. &nbsp;It is just that, a fantasy, largely fueled by spending by billionaires.</p>
<h6>Robotics, AI, and Machine Learning</h6>
<p>So what happened in Robotics, AI, and Machine Learning this year?</p>
<p>Many, many, many people got just a little bit over excited. That’s what happened.</p>
<p>There have been a lot of party tricks and it is the researchers who often play the tricks on themselves without realizing it. This is not new, none of it is new. But there are orders of magnitude more people watching it now, and more people are out to make a buck by being hypesters, promising riches to those who will invest in their irrationally overpriced companies.</p>
<p>How could this be?</p>
<p>We are seeing mass sinning, lots and lots of people committing some of the seven deadly sins of predicting the future of AI &nbsp;which I wrote about back in 2017 <a href="https://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/" target="_blank" rel="noopener">here</a> (or <a href="https://www.technologyreview.com/2017/10/06/241837/the-seven-deadly-sins-of-ai-predictions/" target="_blank" rel="noopener">here</a> you can see a professionally edited version of that blog post of mine).</p>
<p>Four of those seven sins seem most relevant to today’s hyped up atmosphere around robotics, AI, and machine learning.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/PerformCompet-1.jpg" alt="" width="314" height="166">&nbsp;<img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Magic-1.jpg" alt="" width="287" height="165">&nbsp;<img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Exponentialism-1.jpg" alt="" width="277" height="173">&nbsp; <img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Deployment-1.jpg" alt="" width="320" height="181"></p>
<p>Here now are short descriptions of these particular four sins, edited down from my earlier much more detailed descriptions. Then I will weave them together to explain how it is still pretty much business as usual, and I mean that in a good way, with steady progress on both the science and engineering of AI.</p>
<p><span><em>Performance versus Competence</em></span></p>
<p>One of the social skills that we all develop is an ability to estimate the capabilities of individual people with whom we interact. We use cues from how a person performs any particular task to estimate how well they might perform some different task. We are able to generalize from observing performance at one task to a guess at competence over a much bigger set of tasks.</p>
<p>These estimators that we have all inherited or learned do not generalize well to other creatures or machines. We are not good at guessing which smart things other species might be able to do, and we are not good at guessing what an AI system can do when we have seen it do a few tasks in a limited domain. We get it wrong all the time.</p>
<p><span><em>Indistinguishable from Magic</em></span></p>
<p>When people cannot explain how something works they cannot know its limits as they do not have any sort of model (nor have they seen enough examples of it before). Arthur C. Clarke said that any sufficiently advanced technology is indistinguishable from magic.</p>
<p>In our minds UFOs can do all sorts of amazing things as we have no way of knowing their limits–they may as well be magic, And that is what they become in speculation about them.</p>
<p>Isaac Newton spent half his working life on alchemy as he did not know that the nucleus of atoms were not subject to mere chemistry. He would have been just as ignorant of the limitations of an iPhone screen (different sort of apple…), despite his own ground breaking work in optics. Remember, he was a really really smart dude. But even he was not able to develop all the theories needed to understand the world around him, despite his successes with calculus and gravity and the makeup of white light. He attributed properties to chemistry that were way beyond its limits.</p>
<p><span><em>Exponentialism</em></span></p>
<p>We have just lived through sixty years of the most phenomenal growth of a technology in the history of humankind. It is the story of silicon-based computation. Everyone has some idea about Moore’s Law, at least as much to sort of know that computers get better and better on a clockwork like schedule.</p>
<p>This reality has trained people to think that probably a lot of other things in tech will change exponentially, especially when that thing has a strong computational component. The sin of exponentialism is to argue that some other process is going to follow a Moore’s-like law when it is unwarranted to so argue.</p>
<p>Moore’s law worked for so long because in the starting technology of the 1960s the currents used to represent digital information were many many orders of magnitude beyond the minimal physical limit needed to determine whether they &nbsp;were present or not, and hence distinguish a 1 from a 0. Those currents could be halved many times without breaking physics limits.</p>
<p><em><span>Speed of Deployment</span></em></p>
<p>New technologies get deployed much more slowly than people imagine. Even software technologies.</p>
<p>The old internet protocol, IPv4, can only address two billion, or&nbsp;2×10<sup>9</sup>, devices, which is way less than the number of people on our planet. A new protocol, IPv6, which can address more than&nbsp;3×10<sup>38</sup>&nbsp;devices was meant to replace it over a two year period of dual use by about 2003. But in 2024 IPv4 was still there and carrying over half the world’s internet traffic despite its inadequacies.</p>
<p>Must functioning businesses that operate in the physical world are very averse to taking up new technology as it dramatically increases existential risk to their business. They must foresee immediate and incredibly high return on investment (ROI) to be tempted to move to new technologies.</p>
<p>Even the military is slow to adopt new technologies. The US Air Force still flies the B-52H variant of the B-52 bomber. This version was introduced in 1961, making it 63 years old. The last one was built in 1963, a mere 61 years ago. Currently these planes are expected to keep flying until at least 2040, and perhaps longer–there is talk of extending their life out to 100 years.</p>
<p><em><span>What does this all mean?</span></em></p>
<p>Right now there is incredible hype for both Large Language Models (LLMs), and all their variations, and for humanoid robots, especially humanoid robots that are going to learn how to do things.</p>
<p>The hype is driven by the four sins above.</p>
<p><em><span>LLMs</span></em></p>
<p>LLMs have proved amazing facile with language. They have been trained on pretty much all the text that is available on the Web and all the digitized historical books that exist. Miraculously LLMs seem to be able to infer a representation of some sort, that is somewhat independent of the particular human language that they read. So they are able to translate between human languages, and when you ask them just about anything they produce text in the language that you asked in, and that text often seems entirely reasonable and informative.</p>
<p>I used the word “miraculously” as we do not really understand why they are able to do what they do. We, of course, know that the architecture for them is built around noticing correlations in vast amounts of text &nbsp;that connect some tens of thousands of tokens which are the components of words in each language that is digested. It is a surprise that they work as well as they they do, and produce coherent sounding language on just about any topic.</p>
<p>Here is the original architectural diagram from the 2017 <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">Attention Is All You Need</a> paper:</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/attention.jpg" alt="" width="1618" height="1600"></p>
<p>Each column from bottom to top is a pure feed forward network, with no search, no iteration, no conventional algorithm at all. There are inputs at the bottom and then layer upon layer of linear neurons that have numbers or weights stored in them that multiply and add their inputs and threshold that sum to provide an output. The detail in the architectural diagram is how the connections between layers are organized.</p>
<p>On the left is an input or question, in a linear string of words, from a user. That gets injected half way up the network on the right and remains constant while a single iteration process runs. The stack on the right outputs a word (or token) and that gets fed back to the bottom of that stack, and a new token pops out the top. All the output tokens that have so far been produced remain in the right bottom input buffer as ordered input.</p>
<p>What the network has been trained to do, is given the user input on the left, and what the network has output so far, choose a very likely next word, given the billions of examples it has seen in training. Some randomness is used to choose among a small number of very likely next words at each stage.</p>
<p>There are hundreds of billions of weights that get learned and stored in the layers of network to act as multipliers for each individual input to each layer.</p>
<p>So now us humans are faced with looking at this system running and our human nature just makes us commit the first two sins from above. &nbsp;It is in our nature and we cannot help ourselves.</p>
<p>First, we see really impressive examples of responses to input questions, and if a human was giving those answers we would estimate that person to be quite clever and able to reason. Often though, because they have so many billions of examples on which they were trained LLMs are essentially looking up the question in the weights. The weight if gained from all of human knowledge that is out there on the network in language form. Invisibly the network is perhaps (but not in any intentional way) merging some similar questions, and then merging the answers which were already in the vast data that it has seen.</p>
<p>But us dumb humans just think the damn thing is really really smart.</p>
<p>Then, since we don’t have a real explanation in our heads for what it is doing we start thinking it is magic, and that there is no real limit to what it is extracting from all that data (that it used a significant portion of the energy budget for many different countries to compute) and how general its capabilities will be. It becomes magic. And then researchers try to show that it can reason, that it has inferred a spatial understanding of the world, that language can be used to do all sorts of things that Moravec’s paradox tells us it can’t. There is a lot of magical thinking that humans do about LLMs.</p>
<p>Of course it can diagnose diseases like a doctor talking about them. Of course it can teach a student as well as a human teacher. Of course it can program as well as a human computer programmer. It is magic after all.</p>
<p>But in reality the fact that it is just picking likely next words means that in fact we can’t trust its output. Some outputs are great. Some are pure confabulations (most people use the word “hallucinations” for this, but I prefer “confabulations”). And we do not know which we will get ahead of time, or more perniciously how much of each we will get, trustworthy pieces of output and confabulated pieces of output all jumbled together.</p>
<p>Not to worry say the proponents, More learning will fix it. Fire up a nuclear power plant (I am not making this up–the tech companies are getting more nuclear power built or activated so that their LLMs can learn what a human learns using just 20 watts powering their brain; I am not confabulating this!!), and we’ll feed it more data and it will become more trustworthy. &nbsp;It is magic after all. But the magic is not going as well as the proponents imagined and promised as this Wall Street Journal <a href="https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693?mod=hp_lead_pos8" target="_blank" rel="noopener">story</a> explains. Their imaginations were definitely encourage by exponentialism, but in fact all they knew was that when the went from smallish to largish networks following the architectural diagram above, the performance got much better. So the inherent reasoning was that if <strong>more</strong> made things better then <em>more</em> <strong>more</strong> would make things <em>more</em> better. Alas for them it appears that this is probably not the case. But rabid exponentialists have not yet given up. Expect a bunch of VCs to adversely affect the growth of pension funds around the world as pension funds are a prime source of capital that VCs spend.</p>
<p>More serious academics are working on boxing in the LLMs with more external mechanism beyond just feeding the output tokens back in as a linear string of input. Many of these mechanisms look a lot like more conventional AI mechanisms, and we will see where these additions prove to be useful, how much of the wheel will be reinvented, and how long (months?, years?, decades?) to get there.</p>
<p>And the answers to those last questions will tell us how much sinning has been done by companies in predicting fast deployments. Back in rant at the beginning of this post I gave the example of I.B.M. and Watson and their completely optimistic predictions of how any problems of applying Watson (which seemed extremely competent based on its performance on live TV) to the real world would be solvable. The areas that it was predicted to be applicable came from magical thinking.</p>
<p>Surely no one today could be as dumb as that big company was back in 2011. Surely not. No, not us smart inhabitants of 2025. Its us. We are nowhere near as dumb as them!!</p>
<p><em><span>Humanoid Robots</span></em></p>
<p>The other thing that has gotten over hyped in 2024 is humanoids robots. &nbsp;The rationale for humanoid robots being a thing is a product of the four sins above and I think way less rooted in reality than the hype about LLMs. In fact I think it is pretty dumb. [[I suspect many people will reason that I cannot have a valid opinion about this precisely because I happen to have built more humanoid robots than anyone else on the planet. So read ahead with caution.]]</p>
<p><a href="https://rodneybrooks.com/rodney-brooks-three-laws-of-robotics/" target="_blank" rel="noopener">My first law of robotics</a> states:</p>
<p><em>The visual appearance of a robot makes a promise about what it can do and how smart it is. It needs to deliver or slightly over deliver on that promise or it will not be accepted.</em></p>
<p>The first sentence describes, I think, what is sucking people into believing that humanoid robots have a big future. It looks like a human, so its performance will be like a human, so it will be competent like a human. &nbsp;It’s the performance/competence sin without even waiting for the performance part!</p>
<p>The second sentence describes how the humanoid fever will break, and how the hundreds of millions of dollars put into many of these companies (billions of dollars overall) will disappear. The puppets will not perform at acceptable levels. It is easy to see this as you hear all the things investors and CEOs of humanoid robots say they will be able to do. They have hardly even got to the lab demonstration phase. &nbsp;My third law of robotics is:</p>
<p><em>Technologies for robots need 10+ years of steady improvement beyond lab demos of the target tasks to mature to low cost and to have their limitations characterized well enough that they can deliver 99.9% of the time. Every 10 more years gets another 9 in reliability.</em></p>
<p>For real work, robots need to operate with four, five, or six nines. We are a long way from that. The zeitgeist is that we will simply teach the robots to do stuff and then they will be able to do it.</p>
<p>BUT, we do not know yet whether that is going to work. In order for it to work you have to both collect the <strong>right sort of data</strong> and then <strong>learn the right things</strong> from that data. It is not at all clear to me that we know the answers to make either of those things true. I think it will be an active place for lots of good research for many years to come.</p>
<p>There is an excellent survey paper of current research state of the art called <a href="https://arxiv.org/abs/2408.03539">Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes</a>. Unfortunately I think the title of the paper is going to confuse many people. “Real-World Successes” to someone like me, who these days deploys robots that people pay for and that provide real ROI, sounds like it is about systems that have been deployed. But on reading the paper it turns out that they mean that it is learning and demonstrations done in a lab setting on physical hardware rather than just in simulations and simulators. &nbsp;And, to me the lab demonstrations are shakier (literally) than I imagined in my third law above.</p>
<p>I think we are a long way off from being able to for-real deploy humanoid robots which have even minimal performance to be useable and even further off from ones that have enough ROI for people want to use them for anything beyond marketing the forward thinking outlook of the buyer.</p>
<p>Despite this, many people have predicted that the cost of humanoid robots will drop exponentially as their numbers grow, and so they will get dirt cheap. I have seen people refer to the cost of integrated circuits having dropped so much over the last few decades as proof. Not so.</p>
<p>They are committing the sin of exponentialism in an obviously dumb way. As I explained above the first integrated circuits were far from working at the limits of physics of representing information. But today’s robots use mechanical components and motors that are not too far at all from physics based limits, about mass, force, and energy. You can’t just halve the size of a motor and have a robot lift the same sized payload. Perhaps you can halve it once to get rid of inefficiencies in current designs. Perhaps. But you certainly can’t do it twice. Physical robots are not ripe for exponential cost reduction by burning wastes in current designs. And it won’t happen just because we start (perhaps) mass producing humanoid robots (oh, but the way, I already did this a decade ago–see my parting shot below). We know that from a century of mass producing automobiles. They did not get exponentially cheaper, except in the computing systems. Engines still have mass and still need the same amount of energy to accelerate good old fashioned mass.</p>
<p><em><span>This Year’s Prediction Update</span></em></p>
<div class="page" title="Page 1">
<p>There is only one new comment in my robotics, AI and ML predictions table this year. There are a bunch of well funded new companies in the home robot space, and perhaps they will come up with new mobility solutions, which in my experience is the big blocker for home robots.</p>
</div>

<table id="tablepress-31">
<thead>
<tr>
	<th>Prediction<br>
[AI and ML]</th><th>Date</th><th>2018 Comments</th><th>Updates</th>
</tr>
</thead>
<tbody>
<tr>
	<td>Academic rumblings about the limits of Deep Learning</td><td><p>BY 2017</p></td><td>Oh, this is already happening... the pace will pick up.</td><td></td>
</tr>
<tr>
	<td>The technical press starts reporting about limits of Deep Learning, and limits of reinforcement learning of game play.</td><td><p>BY 2018</p></td><td></td><td></td>
</tr>
<tr>
	<td>The popular press starts having stories that the era of Deep Learning is over.</td><td><p>BY 2020</p></td><td></td><td></td>
</tr>
<tr>
	<td>VCs figure out that for an investment to pay off there needs to be something more than "X + Deep Learning".</td><td><p>NET 2021</p></td><td>I am being a little cynical here, and of course there will be no way to know when things change exactly.</td><td></td>
</tr>
<tr>
	<td>Emergence of the generally agreed upon "next big thing" in AI beyond deep learning.</td><td><p>NET 2023</p><p>BY 2027</p></td><td>Whatever this turns out to be, it will be something that someone is already working on, and there are already published papers about it. There will be many claims on this title earlier than 2023, but none of them will pan out.</td><td><span>20240101</span> <p>It definitely showed up in 2023. It was in the public mind in December 2022, but was not yet the big thing that it became during 2023. A year ago I thought it would perhaps be neuro-symbolic AI, but clearly it is LLMs, and ChatGPT and its cousins. And, as I predicted in 2018 it was something already being worked on as the "attention is all you need" paper, the key set of ideas, was published in 2017.</p></td>
</tr>
<tr>
	<td>The press, and researchers, generally mature beyond the so-called "Turing Test" and Asimov's three laws as valid measures of progress in AI and ML.</td><td><p>NET 2022</p></td><td>I wish, I really wish.</td><td><span>20230101</span> <p>The Turing Test was missing from all the breathless press coverage of ChatGPT and friends in 2022. Their performance, though not consistent, pushes way past the old comparisons.</p> <span>20240101</span> <p>The Turing Test was largely missing from the press in 2024 also, and there was a <a href="https://www.nature.com/articles/d41586-023-02361-7" rel="noopener">story in Nature</a> commenting on that. So yes, this has now happened.</p> </td>
</tr>
<tr>
	<td>Dexterous robot hands generally available.</td><td>NET 2030<br>
BY 2040 (I hope!)</td><td>Despite some impressive lab demonstrations we have not actually seen any improvement in widely deployed robotic hands or end effectors in the last 40 years.</td><td></td>
</tr>
<tr>
	<td>A robot that can navigate around just about any US home, with its steps, its clutter, its narrow pathways between furniture, etc.</td><td>Lab demo: NET 2026<br>
Expensive product: NET 2030<br>
Affordable product: NET 2035</td><td>What is easy for humans is still very, very hard for robots.</td><td> <span>20250101</span> <p>A bunch of startups in the home robot space got significant funding in 2024. Two of them are run by ex-CEOs of large companies: iRobot and Cruise (and he was also an intern at iRobot after we were already a public company). So this one may be in play for a lab demo in the next few years if they have this as one of their goals..</p></td>
</tr>
<tr>
	<td>A robot that can provide physical assistance to the elderly over multiple tasks (e.g., getting into and out of bed, washing, using the toilet, etc.) rather than just  a point solution.</td><td>NET 2028</td><td>There may be point solution robots before that. But soon the houses of the elderly will be cluttered with too many robots.</td><td></td>
</tr>
<tr>
	<td>A robot that can carry out the last 10 yards of delivery, getting from a vehicle into a house and putting the package inside the front door.</td><td>Lab demo: NET 2025<br>
Deployed systems: NET 2028<br>
</td><td></td><td></td>
</tr>
<tr>
	<td>A conversational agent that both carries long term context, and does not easily fall into recognizable and repeated patterns.</td><td><p>Lab demo: NET 2023</p>Deployed systems: 2025</td><td>Deployment platforms already exist (e.g., Google Home and Amazon Echo) so it will be a fast track from lab demo to wide spread deployment.</td><td><span>20240101</span> <p>One half of this happened this year. ChatGPT has been connected to microphones and speakers so you can now talk to it. and It does not fall into recognizable patterns. BUT the other half is the half it does not have;  it has no updatable memory apart from its token buffer of what it has just said. Long term context may be long term in coming.</p></td>
</tr>
<tr>
	<td>An AI system with an ongoing existence (no day is the repeat of another day as it currently is for all AI systems) at the level of a mouse.</td><td>NET 2030</td><td>I will need a whole new blog post to explain this...</td><td></td>
</tr>
<tr>
	<td>A robot that seems as intelligent, as attentive, and as faithful, as a dog.</td><td>NET 2048</td><td>This is so much harder than most people imagine it to be--many think we are already there; I say we are not at all there.</td><td></td>
</tr>
<tr>
	<td>A robot that has any real idea about its own existence, or the existence of humans in the way that a six year old understands humans.</td><td>NIML</td><td></td><td></td>
</tr>
</tbody>
</table>
<!-- #tablepress-31 from cache -->
<p><span><em>A Parting Shot</em></span></p>
<p>I recently read a research paper on humanoid robots working in built for human environments. It was based on the argument that the best form for a robot that is to operate in human environments is something tallish and skinny-ish, and probably dynamically balancing, with arms that can reach down to table tops etc., and with a sensor system that can look down from above, as that is what our human environments are optimized for. Here is the first paragraph of the paper:</p>
<p>The past decade has seen an explosion of research in humanoid robotics. The stated motivations for this work have varied widely. Many teams have concentrated on bipedal locomotion, some have been interested in human level social interactions, understanding human intelligence, modeling human learning capabilities and others have been more interested in entertainment. Some humanoid robots have had manipulation capabilities on static humanoid platforms and some of that work is aimed at dexterity, plus there has been simple two armed grasping on mobile humanoid platforms. Overall there has been very little work combining dexterous manipulation with humanoid robots, static or mobile–much of that which has appeared, has been concerned with dynamic tasks like pole balancing and juggling rather than manipulation, or has used teleoperated manipulation.</p>
<p>Apart from the weird references to pole balancing and juggling this all sounds pretty reasonable and consistent with what is happening today, and with recent history. &nbsp;In fact this is the very first paragraph of the very first paper in the very first issue of the very first volume of&nbsp;the <a href="https://www.worldscientific.com/worldscinet/IJHR" target="_blank" rel="noopener">International Journal of Humanoid Robotics</a>.</p>
<p>And it was published in 2004, with me as first author. &nbsp;Let me spell that out in case you thought there was a typo in the year. This is from a paper that I and my students and post-docs wrote in the year <em>two thousand and four</em>. Here is the beginning of the contents page for that first issue.</p>
<div class="page" title="Page 1">
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/IJHR.jpg" alt="" width="1663" height="858"></p>
<p>You can download the text of that paper <a href="https://people.csail.mit.edu/brooks/papers/brooks04sensing.pdf" target="_blank" rel="noopener">here</a>. The journal&nbsp;is now in its 21<sup>st</sup> year of operation, an on its 21<sup>st</sup> volume of issues and papers.</p>
<p>By the time this paper was written my research group at MIT had been working on and building humanoid robots for twelve years. This paper, about a robot named Cardea, was probably our sixth or seventh humanoid robot. [[In 2008 I started a company that built and shipped thousands of humanoid robots. The picture at the top of this post was taken in China with a line up of humanoids that we had built in Massachusetts and New Hampshire and sold to people in China (before a US initiated trade war with China <a href="https://www.wired.com/story/a-long-goodbye-to-baxter-a-gentle-giant-among-robots/" target="_blank" rel="noopener">put an end to it in 2018</a>…irony can be personally hard to take at times…).]]</p>
<p>The robot&nbsp;Cardea (Cardea was an ancient Roman goddess of door hinges and handles; these are still a challenge for modern robots…) was a two wheeled dynamically balancing robot &nbsp;that lived in a built-for-humans office environment. Cardea was able to open doors using existing door handles and then make its way through doors it had opened.</p>
<p><strong><span>Pro tip:</span></strong>&nbsp;Just because you heard about a new idea this last year or two doesn’t mean that people haven’t been working on that very same idea for decades. So temper your expectations that it must be about to transform the world. Ideas that transform the world take decades, or centuries of development, and plenty of people long before you have been just as excited about the idea and had thought it was on the verge of taking off. And none of us, including you and me, are likely to be special enough or lucky enough to come along at just the right time to see it all happen.</p>
<p>Like all modern humanoid robots Cardea did not walk in a way that used passive dynamics to store energy, and basically modulate the behavior of a passive mechanism that had only low energy input, which is how all animals walk. So, like all modern mobile humanoid robots (and legged robots in general) when things were going awry its control algorithms tried to recover by pumping in large amounts of energy very quickly and sometimes that didn’t quite work and the energy needed to go somewhere.</p>
<p>Cardea could be a little dangerous in those circumstances, if it fell on you having just increased its kinetic energy. Even the spring based deployment system for its stick-like legs that were engaged when it realized it was going to fall could be dangerous.</p>
<p>This is still a problem with all modern humanoid robots. That is why the tele-operated humanoids that were in the Tesla movie lot theater show a couple of months ago operated in two modes. When they all walked out the human guests were kept away from them. Once they stopped walking and were operating in a very different mode people were allowed to approach them, and then get fooled into thinking they were talking to an AI powered robot when they were really talking to a remote human operator. But the robot was no longer moving its feet, and no longer a source of physical danger as a result.</p>
</div>
<p><strong><span>Another pro tip:</span></strong>&nbsp;Don’t stand anywhere near a walking or balancing wheeled humanoid when they are moving or doing any task. I have had some near misses for myself with my own humanoids twenty years ago and more recently with some of the humanoids from new start ups. And more generally never be below any sort of walking robot, no matter how many legs it has, when it is walking up stairs.</p>
<h5>HUMAN SpaceFLIGHT</h5>
<p>The numbers of flights in 2024 was not much different from those in 2023 (I neglected to include the flights by China last year). &nbsp;It does not feel like a golden age of human spaceflight, though there were other highlights from SpaceX.</p>
<p><span><em>Orbital Crewed Flights</em></span></p>
<p>Three countries put 28 people into orbit in 2024, the United States launched 16 people on five flights and Russia and China launched 6 people each with two launches. So there were nine crewed orbital flights total. Two were private and seven were government flights.</p>
<p><span><em>The United States:</em></span>&nbsp;There were four US flights to the International Space Station, starting with the private Axion-3 mission with a crew of four on January 18<sup>th</sup>. The launch vehicle for this was a SpaceX Falcon 9, and the crew vehicle was a SpaceX Dragon. The remaining US flights to the ISS were paid for by NASA. Two of them were SpaceX flights, with four people on March 4<sup>th</sup>, the Crew-8 mission, and two people on board Crew-9 on October 25<sup>th</sup>. The remaining US flight to the ISS was the inaugural crewed flight of Boeing’s Starliner, launched on June 5<sup>th</sup> atop an Atlas V rocket with two people aboard. They are still stuck in space and will be for a few more months–see the section on Boeing below.</p>
<p>The other US mission was also a SpaceX launch and vehicle flight, this time known as Polaris Dawn. It was the second mission paid for by billionaire Jared Isaacman, with him as commander. There was a former US Air Force fighter pilot as mission pilot and two SpaceX employees as mission specialists, giving a total crew size of four. They stayed aloft for five days, launching on September 10<sup>th</sup>, This mission flew higher above Earth than any mission since Apollo 17, the last lunar landing mission, in 1972. Two of the crew “spacewalked” with their feet inside the Dragon capsule but with their bodies outside. This was the first private spacewalk ever. Now Isaacman has been tapped by the incoming US President to be the administrator of NASA.</p>
<p><span><em>Russia:</em></span>&nbsp;There were two Soyuz launches, each with three people, up and down, but different people coming back. The launch dates were March 23<sup>rd</sup> and September 11<sup>the</sup>. The six people that launched on Soyuz in 2024 were 3 Russian Cosmonauts 2 NASA Astronauts and one Belarusian commercial airline flight attendant who won a national competition with 3,000 applications. She was the only one not set for a long duration mission and was off the ground for slightly less than 14 days. So there were no space tourists per so, but the Belarusian flyer was most likely included as part of Russia’s efforts to keep in good favor with Belarus which has aided it in its war in Ukraine, and was certainly not part of the regular scientific program of the ISS.</p>
<p><span><em>China:</em></span>&nbsp;There were two flights of &nbsp;Shenzhou (a larger more modern version of Soyuz) that were crewed in 2024. &nbsp;Both flights were to the Tiangong Space Station and both took along three Taikonauts, first on April 25<sup>th</sup> and then on October 9<sup>th</sup>. &nbsp;Both crews were assigned long duration missions and now the crews are overlapping previous crews at Tiangong so it is now being continuously occupied. The first handover this year took about five days and the second about three and a half weeks. &nbsp;Both times there were six Taikonauts onboard Tiangong at the same time.</p>
<p><span><em>Suborbital Crewed Flights</em></span></p>
<p>There have been two companies providing space tourism flights on suborbital flights. Blue Origin launches a capsule on top of a reusable rocket, New Shepard, and the capsule lands using a parachute and a brief rocket blast right before hitting the ground (similar to how Soyuz lands). Virgin Galactic has a winged craft which is carried aloft by a bigger a jet engined airplane, it separates at high altitude within the atmosphere and rockets into space. It flies back and lands on a runway.</p>
<p>Both companies are run by billionaires who made their money in other businesses. &nbsp;Both billionaires have flown to space on their own craft.</p>
<p>Both companies have aimed to have regular launches with lots of tourists, but neither has gotten to that scale and so far only a very small number of the many people who have paid a substantial deposit have been able to fly.</p>
<p>Blue Origin had a failure with an uncrewed version of the vehicle in 2022 and only flew one flight in 2023 which was also uncrewed. This year they flew three crewed flights on May 19<sup>th</sup>, August 29<sup>th</sup>, and November 22<sup>nd</sup>, each with six passengers (the system is automated and requires no pilots). In 2021 and 2022 they also had three flights, so there has now been nine crewed flights total. The first two took four passengers and the remaining seven have had six passengers, so altogether they have flown 50 people above the Karman line, 100 kilometers above Earth. &nbsp;This is not yet a regular cadence, nor a large scale tourist business.</p>
<p>In 2024 Virgin Galactic had two flights, each with two crew from the company and four passengers. These flights were on January 26<sup>th</sup> and June 8<sup>th</sup>. Virgin Galactic flights are now on hiatus, awaiting a new bigger and better vehicle in about two years. &nbsp;Virgin Galactic has had a total of twelve flights since December 13th in 2018. &nbsp;Three have had two people on board and nine have had six people on board, for a total of sixty filled seats that have crossed the Karman line. The total number of different people is smaller as the two pilot seats on each flight have been occupied by a small number of people who have flown multiple times.</p>
<p>So, in 2024 thirty people went on suborbital flights, and altogether there have been 110 people on these commercial suborbital flights. Space tourism on suborbital flights has yet to take off in a regular or scaled way.</p>

<table id="tablepress-32">
<thead>
<tr>
	<th>Prediction<br>
[Space]</th><th>Date</th><th>2018 Comments</th><th>Updates</th>
</tr>
</thead>
<tbody>
<tr>
	<td>Next launch of people (test pilots/engineers) on a sub-orbital flight by a private company.</td><td><p>BY 2018</p></td><td></td><td></td>
</tr>
<tr>
	<td>A few handfuls of customers, paying for those flights.</td><td><p>NET 2020</p></td><td></td><td></td>
</tr>
<tr>
	<td>A regular sub weekly cadence of such flights.</td><td><p>NET 2022</p>BY 2026</td><td></td><td><span>20240101</span> <p>There were four flights in 2021, three in 2022, and seven, five with customers on board, in 2023--all of them by Virgin Glactic. Blue Origin did not fly in 2023. At this point 2026 is looking doubtful for regular flights every week.</p><span>20250101</span> <p>Now 2026 is looking impossible given the data from 2023 and 2024, and one of the two companies being on hiatus for all of 2025, and well into 2026.</p></td>
</tr>
<tr>
	<td>Regular paying customer orbital flights.</td><td>NET 2027</td><td>Russia offered paid flights to the ISS, but there were only 8 such flights (7 different tourists). They are now suspended indefinitely. </td><td><span>20240101</span><p>There were three paid flights in 2021, and one each in 2022, and 2023, with the latter being the Axiom 2 mission using SpaceX hardware. So not regular yet, and certainly not common.</p><span>20250101</span> <p>There were two paid flights in 2024.</p></td>
</tr>
<tr>
	<td>Next launch of people into orbit on a US booster.</td><td><p>NET 2019</p><p>BY 2021</p><p>BY 2022 (2 different companies)</p><br>
</td><td>Current schedule says 2018.</td><td><span>20240101</span><p>Both SpaceX and Boeing were scheduled to have crewed flights in 2018. SpaceX pulled it off in 2020, Boeing's Starliner did not fly at all in 2023, but is scheduled to launch with people onboard for the first time in April 2024.</p><span>20250101</span> <p>The second company did finally launch humans into orbit in June 2024, so it has happened three years later than I predicted and six years later than what had been promised when my prediction was made. Of course, everyone implicitly assumed that along with getting humans into space the companies would also be able to bring them back. Not so for Boeing.</p></td>
</tr>
<tr>
	<td>Two paying customers go on a loop around the Moon, launch on Falcon Heavy.</td><td><p>NET 2020</p></td><td>The most recent prediction has been 4th quarter 2018. That is not going to happen.</td><td><span>20240101</span><p>Starship launched twice in 2023 but didn't get to orbit either time. This is going to be well over six years later than the original  prediction by the CEO of SpaceX.</p><span>20250101</span> <p>The billionaire who signed up for this and paid a hefty deposit in 2017 gave up waiting and cancelled the contract in 2024. This fantasy is over, for now at least.</p></td>
</tr>
<tr>
	<td>Land cargo on Mars for humans to use at a later date<br>
</td><td><p>NET 2026</p></td><td>SpaceX has said by 2022. I think 2026 is optimistic but it might be pushed to happen as a statement that it can be done, rather than for an pressing practical reason.</td><td><span>20240101</span><p>I was way too optimistic, and bought into the overoptimistic hype of the CEO of SpaceX even though I added four years, doubling his estimated time frame.</p><span>20250101</span> <p>I can now call this as orbital mechanics and Hohmann transfer windows dictate that the cargo would need to have been launched a few months ago for it to get to Mars in 2025. It has not been launched.</p></td>
</tr>
<tr>
	<td>Humans on Mars make use of cargo previously landed there.</td><td>NET 2032</td><td>Sorry, it is just going to take longer than every one expects.</td><td></td>
</tr>
<tr>
	<td>First "permanent" human colony on Mars.</td><td>NET 2036</td><td>It will be magical for the human race if this happens by then. It will truly inspire us all.<br>
</td><td></td>
</tr>
<tr>
	<td>Point to point transport on Earth in an hour or so (using a BF rocket).</td><td>NIML</td><td>This will not happen without some major new breakthrough of which we currently have no inkling.<br>
</td><td></td>
</tr>
<tr>
	<td>Regular service of Hyperloop between two cities.</td><td><p>NIML</p></td><td>I can't help but be reminded of when Chuck Yeager described the Mercury program as "Spam in a can".<br>
</td><td><span>20240101</span><p>Calling this one 26 years early. As of today no-one is still working on this in an operating company.</p></td>
</tr>
</tbody>
</table>
<!-- #tablepress-32 from cache -->
<p><span><em>Boeing’s Starliner</em></span></p>
<p>First announced in 2010 Boeing’s Starliner was originally scheduled to fly a human crew in 2018. It carried out its second uncrewed flight in May 2022, and finally did make its first crewed flight on June 5<sup>th</sup>. The crew of two docked with the ISS, but there were problems with multiple gas thrusters for fine motion during the docking. The original plan was that the crew would stay on the ISS for about a week and then return to Earth for a touchdown on to hard soil (as all Russian and Chinese crewed missions end along with all Blue Origin sub-orbital flights).</p>
<p>The option of that return was considered, but the thrusters were on a section of the vehicle which is discarded along the way before the landing so there was no possibility of getting a look at the hardware back on Earth. &nbsp;So a program of tests while docked to the ISS was started delaying the crew return.</p>
<p>Eventually it was decided that it was too risky for the crew to return on the craft and so it returned empty on &nbsp;September&nbsp;7<sup>th</sup>, landing in New Mexico. As it happened, although there were more anomalies with the thrusters the crew would have landed safely had they been on board.</p>
<p>Now the crew was stranded in space with no designated ride home. It was decided to remove two crew from the Crew-9 launch and have the Starliner astronauts, Barry Wilmore and Sunita Williams, fly back on that SpaceX Dragon with the other two, which after additional delays is now scheduled to happen some time in March 2025. Their one week visit to the ISS will have stretched out to nine months by then.</p>
<p>Boeing has committed to fixing the problems with Starliner. The boosters that it uses are no longer being built, but there are five existing ones reserved for the five additional contracted flights that Boeing has with NASA. They are supposed to happen once per year.</p>
<p>We do not know at this point, but I think it would not be a huge surprise if Starliner never flies again.</p>
<p><span><span><i>SpaceX Falcon 9&nbsp;</i></span></span></p>
<p>Once again the Falcon 9 launch system has broken all sorts of records for number of launches and reuse.</p>
<p>During 2024 there were 132 single booster launches. &nbsp;For two of those flights no attempt was made to recover the first stage (there is a performance penalty for the primary payload in order to recover the first stage). One attempted recovery failed when the booster (on its 23<sup>rd</sup> flight) caught fire as it landed on the recovery barge. Another booster has since flown a total of 24 times.</p>
<p>In terms of mission success all but one of these flights succeeded; one failed when the second stage failed during re-ignition for adjusting the orbit.</p>
<p>There were also two Falcon Heavy, the three booster version, launches, both of which succeeded. One of the had successful landings for the two side boosters, but there was no attempt to recoer the central booster on that flight and no attempt to recover any of the three boosters on the other Heavy flight.</p>
<p>This brings the total number of launches of the single booster version to 417 along with 11 launches of the three booster Heavy version. &nbsp;These numbers are way beyond the number of launches for any other orbital booster. &nbsp;Additionally it is the only flying orbital system that is reusable at the moment, though &nbsp;Blue Origin and Rocket Lab both plan on joining the club soon.</p>
<p>It is worth, once again, looking at how long it has taken to get to a total (across both single booster and Heavy triple booster versions) of 428 launches, with only three failures to deliver the payload to where it was intended to go.</p>
<p>The first launch occured in June 2010, and there were a total of 4 launches in the first three years. &nbsp;The first successful booster recover happened on the 20th flight, in December 2015, five and a half years in. The first reuse of a booster occured in 2017, in the 8<sup>th</sup> year of the program.</p>
<p>Since 2021 there has been a steady increase in the number of launches per year,</p>

<table id="tablepress-33">
<thead>
<tr>
	<th>Year</th><th># of launches</th>
</tr>
</thead>
<tbody>
<tr>
	<td>2010</td><td>2</td>
</tr>
<tr>
	<td>2011</td><td>0</td>
</tr>
<tr>
	<td>2012</td><td>2</td>
</tr>
<tr>
	<td>2013</td><td>3</td>
</tr>
<tr>
	<td>2014</td><td>6</td>
</tr>
<tr>
	<td>2015</td><td>7</td>
</tr>
<tr>
	<td>2016</td><td>8</td>
</tr>
<tr>
	<td>2017</td><td>18<br>
</td>
</tr>
<tr>
	<td>2018</td><td>21</td>
</tr>
<tr>
	<td>2019</td><td>13</td>
</tr>
<tr>
	<td>2020</td><td>26</td>
</tr>
<tr>
	<td>2021</td><td>31</td>
</tr>
<tr>
	<td>2022</td><td>61</td>
</tr>
<tr>
	<td>2023</td><td>96</td>
</tr>
<tr>
	<td>2024</td><td>134</td>
</tr>
</tbody>
</table>
<!-- #tablepress-33 from cache -->
<p>SpaceX had previously gotten satellites to orbit with its first rocket, the Falcon 1. &nbsp;Falcon 9 has been a spectacular success. &nbsp;But it was not instantaneous. &nbsp;It took time to build from the cadence of launches, about 10 years before the hockey stick curve showed up. &nbsp;Deployment is never sudden but comes after a long build.</p>
<p><em><span>SpaceX Starship</span></em></p>
<p>Starship is SpaceX’s superheavy two stage rocket, designed to put 150 tons of payload into orbit, but also be able to go to the Moon or Mars. There is the booster which is designed only to work in Earth atmosphere with 33 Raptor engines both to get the second stage high enough and fast enough and to let the first stage have a controlled return to the launch site. The second stage, called Starship, is both a booster and the payload. &nbsp;It has three Raptor engines and three Raptor vacuum engines. The Raptor engines are designed to get the Starship into orbit after the first stage drops away, and to guide the Starship as it returns to its Earth launch site. The Raptor vacuum engines are meant for breaking out of Earth orbit and going to the Moon or Mars, and to do soft landings on those two bodies where there is no or almost no atmosphere.</p>
<p>In 2024 SpaceX made steady progress with four launches of the two stages coupled together. &nbsp;The first two launches lead to both stages blowing up.</p>
<p>The third and fourth launches were a big improvement. &nbsp;As with earlier flights they launched from the coast of Texas. In both cases the second stage did a reentry burn on it first orbit and then did a soft landing in a target zone in the Indian Ocean. &nbsp;In the third flight the main booster returned to the launch site and hovered next to the launch tower betweeen two giant arms which then captured it and the engines shot down successfully. It was sifficiently damaged during flight however, that it was not reusable. In the fourth flight there were health anomalies to the first stage was ditched in the Gulf of Mexico.</p>
<p>On the fourth flight there was both less heat shielding and much less damage from heat during reentry. This is definite forward progress. But it is still quite a long way from both being operational and both stages being reusable. And it is even further away from being human rated.</p>
<p>This is the vehicle that the CEO of SpaceX recently said would be launched to Mars and attempt a soft landing there. &nbsp;He also said that if successful the humans would fly to Mars on it in 2030. These are enormously ambitious goals just from a maturity of technology standpoint. The real show stopper however may be human physiology as evidence accumulates that humans would not survive three years (the minimum duration of a Mars mission, due to orbital mechanics) in space with current shielding practices and current lack of gravity on board designs. Those two challenges may take decades, or even centuries to overcome (recall that Leonardo Da Vinci had designs for flying machines that took centuries to be developed…).</p>
<p>The President of SpaceX may be taking a leaf out of the CEO’s always overly optimistic predictions. In November she said <a href="https://www.dailystar.co.uk/news/us-news/spacex-president-says-could-easily-34143695" target="_blank" rel="noopener">“I would not be surprised if we fly 400 Starship launches in the next four years”</a>. Looking at the success of Falcon 9 it is certainly plausible that I may live to see 400 Starship launches in a four year period, but I am quite confident that it will not happen in the&nbsp;<strong>next</strong> four years (2025 through 2028).</p>
<p><em><strong>One more thing.</strong></em> Back when I first made the predictions there had been an announcement by the CEO of SpaceX that in 2018 the company was under contract to send a very rich paying customer in a trip around the moon in 2018, launched on a Falcon Heavy. I was completely skeptical. Over the years the date got pushed back and pushed back, and the proposed flight vehicle was changed to be Starship. As we all know the flight of the Japanese billionaire around the Moon still hasn’t happened. In 2024 Yusaku Maezawa finally gave up waiting and <a href="https://www.space.com/japanese-billionaire-cancels-spacex-starship-moon-dearmoon-flight" target="_blank" rel="noopener">cancelled the contract</a>.</p>
<p><span><em>NASA Artemis</em></span></p>
<p>NASA’s plan is that the second Artemis mission, using the Orion Capsule, Artemis II, will fly to the Moon with four people aboard, the first crewed Artemis flight. An uncrewed flight of Orion around the Moon flew in 2022. &nbsp;The crewed flight was scheduled to launch in May 2024, but it was first delayed by six months and then a little more and in the last year it has slipped another full year. It is now scheduled to fly in April 2026.</p>
<p>Artemis III was scheduled to launch in 2025 with a return to the surface of the Moon. However that relied on using a Starship (itself refueled in LEO by 14 (yes, <em><strong>fourteen</strong></em>!!) other Starship launches) to land there. &nbsp;No one any longer believes that schedule, and willlikely delay a few years, given where Starship is in its development and current capability. &nbsp;The officieal schedule says mid 2027, but that seems unlikely.</p>
<p>You can find the architecture of the Artemis III mission at this <a href="https://www.nasa.gov/missions/artemis/artemis-iii/" target="_blank" rel="noopener">website</a>.</p>
<p><span><em>Blue Origin Orbital BE-4 Engines and New Glenn</em></span></p>
<p>The suborbital tourist flights that Blue Origin operates are not its main business. It has ambitions to compete head to head with SpaceX. Another billionaire vs billionaire competition.</p>
<p>It has developed the BE-4 engine designed to fly 100 times, and to power the first stage of its massive New Glenn rocket (see below). &nbsp;But in the meantime it has started selling the BE-4 to ULA (United Launch Alliance) to power their Vulcan Centaur heavy launch vehicle. It’s first stage uses two BE-4 engines, along with a variable number of solid fuel strap ons.</p>
<p>Vulcan Centaur flew two times in 2024 and the BE-4 engines worked perfectly both times, on January 8<sup>th</sup> and again on October 4<sup>th</sup>. This is a solid validation of the engine’s capabilities.</p>
<p>Blue Origin’s own first orbital class rocket, New Glenn, is massive, and comparable to the Flacon Heavy (three boosters) rather than the Falcon 9 in capability. &nbsp; It has been in development for a long time, but saw its first visits to a launch pad, fully stacked in 2024. The first stage uses seven BE-4 engines, and is intended to land on a barge and be fully reusable. The second stage uses two BE-3U engines, a variant of the single engine used on their New Shepard sub-orbital space tourism vehicle. There is a project underway to make a fully reusable version of the second stage.</p>
<p>Launch seems imminent. &nbsp;Here it is at the launch pad in November 2024.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/image.jpeg" alt="" width="571" height="876"></p>
<p>On Friday December 27<sup>th</sup>, 2024, it was fully fueled in both stages and <a href="https://www.nytimes.com/2024/12/27/science/new-glenn-blue-origin.html" target="_blank" rel="noopener">went through a countdown and fired its seven BE-4 engines for 24 seconds</a>. Now it will leave the pad to have its payload installed. The launch could be as early January 6<sup>th</sup>. &nbsp;The very first launch will be an all up affair, attempting to get something to orbit and land the booster on its first flight. This is a very different development approach to that used by SpaceX.</p>
<p><span><em>Let’s Continue a Noble Tradition!</em></span></p>
<p>The billionaire founders of both Virgin Galactic and Blue Origin had faith in the systems they had created. They both personally flew on the first operational flights of their sub-orbital launch systems. They went way beyond simply talking about how great their technology was, they believed in it, and flew in it.</p>
<p>Let’s hope this tradition continues. Let’s hope the billionaire founder/CEO of SpaceX will be onboard the first crewed flight of Starship to Mars, and that it happens sooner than I expect. We can all cheer for that.</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article><!-- #post-## -->

<!-- .comments-area -->

	
		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Candy Crush, Tinder, MyFitnessPal: Apps hijacked to spy on location (172 pts)]]></title>
            <link>https://www.wired.com/story/gravy-location-data-app-leak-rtb/</link>
            <guid>42651115</guid>
            <pubDate>Fri, 10 Jan 2025 00:00:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/gravy-location-data-app-leak-rtb/">https://www.wired.com/story/gravy-location-data-app-leak-rtb/</a>, See on <a href="https://news.ycombinator.com/item?id=42651115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Some of the world’s most popular apps are likely being co-opted by rogue members of the advertising industry to harvest sensitive location data on a massive scale, with that data ending up with a location data company whose subsidiary has previously sold global location data to US law enforcement.</p><p>The thousands of apps, <a data-offer-url="https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/&quot;}" href="https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/" rel="nofollow noopener" target="_blank">included in hacked files</a> from location data company Gravy Analytics, include everything from games like <em>Candy Crush</em> and dating apps like Tinder to pregnancy tracking and religious prayer apps across both Android and iOS. Because much of the collection is occurring through the advertising ecosystem—not code developed by the app creators themselves—this data collection is likely happening without users’ or even app developers’ knowledge.</p><p>“For the first time publicly, we seem to have proof that one of the largest data brokers selling to both commercial and government clients appears to be acquiring their data from the online advertising ‘bid stream,’” rather than code embedded into the apps themselves, Zach Edwards, senior threat analyst at cybersecurity firm Silent Push and who has followed the location data industry closely, tells 404 Media after reviewing some of the data.</p><p>The data provides a rare glimpse inside the world of real-time bidding (RTB). Historically, location data firms <a data-offer-url="https://www.vice.com/en/article/us-military-location-data-xmode-locate-x/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/us-military-location-data-xmode-locate-x/&quot;}" href="https://www.vice.com/en/article/us-military-location-data-xmode-locate-x/" rel="nofollow noopener" target="_blank">paid app developers</a> to include bundles of code that collected the location data of their users. Many companies have turned instead to <a data-offer-url="https://www.wsj.com/tech/cybersecurity/how-ads-on-your-phone-can-aid-government-surveillance-943bde04?ref=404media.co" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.wsj.com/tech/cybersecurity/how-ads-on-your-phone-can-aid-government-surveillance-943bde04?ref=404media.co&quot;}" href="https://www.wsj.com/tech/cybersecurity/how-ads-on-your-phone-can-aid-government-surveillance-943bde04?ref=404media.co" rel="nofollow noopener" target="_blank">sourcing location information through the advertising ecosystem</a>, where companies bid to place ads inside apps. But a side effect is that data brokers can listen in on that process and harvest the location of peoples’ mobile phones.</p><p>“This is a nightmare scenario for privacy, because not only does this data breach contain data scraped from the RTB systems, but there's some company out there acting like a global honey badger, doing whatever it pleases with every piece of data that comes its way,” Edwards says.</p><p>Included in the hacked Gravy data are tens of millions of mobile phone coordinates of devices inside the US, Russia, and Europe. Some of those files also reference an app next to each piece of location data. 404 Media extracted the app names and built a list of mentioned apps.</p><p>The list includes dating sites Tinder and Grindr; massive games such as <em>Candy Crush</em>, <em>Temple Run</em>, <em>Subway Surfers</em>, and <em>Harry Potter: Puzzles &amp; Spells</em>; transit app Moovit; My Period Calendar &amp; Tracker, a period-tracking app with more than 10 million downloads; popular fitness app MyFitnessPal; social network Tumblr; Yahoo’s email client; Microsoft’s 365 office app; and flight tracker Flightradar24. The list also mentions multiple religious-focused apps such as Muslim prayer and Christian Bible apps, various pregnancy trackers, and many VPN apps, which some users may download, ironically, in an attempt to protect their privacy.</p><p>The full list can be found <a href="https://docs.google.com/spreadsheets/d/1Ukgd0gIWd9gpV6bOx2pcSHsVO6yIUqbjnlM4ewjO6Cs/edit?usp=sharing">here</a>. Multiple security researchers <a data-offer-url="https://pastejustit.com/atnbotturr" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://pastejustit.com/atnbotturr&quot;}" href="https://pastejustit.com/atnbotturr" rel="nofollow noopener" target="_blank">have published</a> <a data-offer-url="https://gist.github.com/fs0c131y/f498b21cba9ee23956fc7d7629262e9d" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://gist.github.com/fs0c131y/f498b21cba9ee23956fc7d7629262e9d&quot;}" href="https://gist.github.com/fs0c131y/f498b21cba9ee23956fc7d7629262e9d" rel="nofollow noopener" target="_blank">other lists</a> of apps included in the data, of varying sizes. Our version is relatively larger because it includes both Android and iOS apps, and we decided to keep duplicate instances of the same app that had slight name variations to make it easier for readers to search for apps they have installed.</p><p>Although this dataset came from an apparent hack of Gravy, it is not clear whether Gravy collected this location data itself or sourced it from another company, or which location company ultimately owns it or is licensed to use it.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Much of the location data attached to these app names does not have a time stamp. But there are indications it dates from 2024. One of the apps listed is <em>Call of Duty: Mobile</em>, and specifically its Season 5 iteration, <a data-offer-url="https://www.callofduty.com/blog/2024/05/call-of-duty-mobile-season-5-digital-dusk-operators-map-challenge-announcement" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.callofduty.com/blog/2024/05/call-of-duty-mobile-season-5-digital-dusk-operators-map-challenge-announcement&quot;}" href="https://www.callofduty.com/blog/2024/05/call-of-duty-mobile-season-5-digital-dusk-operators-map-challenge-announcement" rel="nofollow noopener" target="_blank">which launched in May 2024</a>.</p><p>Gravy is a company that powers much of the rest of the location data industry. It collates mobile phone location data from various sources, then sells that to commercial companies or, through its subsidiary Venntel, to US government agencies. Norwegian outlet NRK and I <a data-offer-url="https://www.vice.com/en/article/ice-dhs-fbi-location-data-venntel-apps/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/ice-dhs-fbi-location-data-venntel-apps/&quot;}" href="https://www.vice.com/en/article/ice-dhs-fbi-location-data-venntel-apps/" rel="nofollow noopener" target="_blank">previously revealed the flow of location data</a> from a handful of ordinary apps to Gravy and then to Venntel. Venntel’s clients have included <a data-offer-url="https://www.wsj.com/articles/federal-agencies-use-cellphone-location-data-for-immigration-enforcement-11581078600" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.wsj.com/articles/federal-agencies-use-cellphone-location-data-for-immigration-enforcement-11581078600&quot;}" href="https://www.wsj.com/articles/federal-agencies-use-cellphone-location-data-for-immigration-enforcement-11581078600" rel="nofollow noopener" target="_blank">Immigration and Customs Enforcement, Customs and Border Protection</a>, <a data-offer-url="https://www.vice.com/en/article/irs-investigation-location-data-no-warrant-venntel/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/irs-investigation-location-data-no-warrant-venntel/&quot;}" href="https://www.vice.com/en/article/irs-investigation-location-data-no-warrant-venntel/" rel="nofollow noopener" target="_blank">the IRS</a>, <a data-offer-url="https://theintercept.com/2020/06/24/fbi-surveillance-social-media-cellphone-dataminr-venntel/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://theintercept.com/2020/06/24/fbi-surveillance-social-media-cellphone-dataminr-venntel/&quot;}" href="https://theintercept.com/2020/06/24/fbi-surveillance-social-media-cellphone-dataminr-venntel/" rel="nofollow noopener" target="_blank">the FBI</a>, <a data-offer-url="https://www.vice.com/en/article/dea-venntel-location-data/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/dea-venntel-location-data/&quot;}" href="https://www.vice.com/en/article/dea-venntel-location-data/" rel="nofollow noopener" target="_blank">and the Drug Enforcement Administration</a>.</p><p>Venntel has also provided the underlying data for another government-bought surveillance tool called Locate X. <a data-offer-url="https://www.404media.co/inside-the-u-s-government-bought-tool-that-can-track-phones-at-abortion-clinics/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/inside-the-u-s-government-bought-tool-that-can-track-phones-at-abortion-clinics/&quot;}" href="https://www.404media.co/inside-the-u-s-government-bought-tool-that-can-track-phones-at-abortion-clinics/" rel="nofollow noopener" target="_blank">404 Media and a group of other outlets</a> showed last year how that tool, made by a company called Babel Street, could be used to monitor visitors to out-of-state abortion clinics.</p><p>But the newly hacked data shows for the first time just how many apps could be part of a location data supply chain, even if their developers are not aware of it. Most app developers and companies included in the list did not respond to a request for comment. Flightradar24 said in an email that it had never heard of Gravy, but that it does display ads, which “help keep Flightradar24 free.”</p><p>Tinder said in an email that “Tinder takes safety and security very seriously. We have no relationship with Gravy Analytics and have no evidence that this data was obtained from the Tinder app” but did not answer questions about ads inside the app.</p><p>Muslim Pro, one of the Muslim prayer apps included in the list, said in an email that it was not aware of Gravy. “Yes, we display ads through several ad networks to support the free version of the app. However, as mentioned above, we do not authorize these networks to collect location data of our users,” the email said. That does not necessarily mean that a member of the advertising ecosystem can’t extract such data, though. (In 2020 <a data-offer-url="https://www.vice.com/en/article/muslim-pro-location-data-military-xmode/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/muslim-pro-location-data-military-xmode/&quot;}" href="https://www.vice.com/en/article/muslim-pro-location-data-military-xmode/" rel="nofollow noopener" target="_blank">I revealed Muslim Pro was selling its users’ location data</a> to a company called X-Mode, whose clients included US military contractors; Muslim Pro stopped the practice after my reporting.)</p><p>A Grindr spokesperson told 404 Media in an email that “Grindr has never worked with or provided data to Gravy Analytics. We do not share data with data aggregators or brokers and have not shared geolocation with ad partners for many years. Transparency is at the core of our privacy program, therefore the third parties and service providers we work with are listed here <a data-offer-url="https://www.grindr.com/privacy-policy/third-parties/en-us" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.grindr.com/privacy-policy/third-parties/en-us&quot;}" href="https://www.grindr.com/privacy-policy/third-parties/en-us" rel="nofollow noopener" target="_blank">on our website</a>.” Grindr <a data-offer-url="https://lamag.com/news/grindr-sold-its-user-data-for-years-revealing-personal-information" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://lamag.com/news/grindr-sold-its-user-data-for-years-revealing-personal-information&quot;}" href="https://lamag.com/news/grindr-sold-its-user-data-for-years-revealing-personal-information" rel="nofollow noopener" target="_blank">was previously found to have allowed data brokers</a> to obtain its users’ location data.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>It’s important that the data appears to be sourced through real-time bidding, because that dictates who is responsible (rogue members of the advertising industry and the tech giants that facilitate that industry), how users can protect themselves (attempting to block ads), and the fact that massive app publishers may not even be aware their users’ data is being harvested and therefore might not know how to stop it. An app developer will know if it implemented location-data-gathering code itself. It might not know that some company, somewhere, is silently listening in on the advertising process and siphoning data from their app.</p><p>Surveillance firms can obtain RTB data by <a data-offer-url="https://www.bloomberg.com/news/articles/2023-05-11/surveillance-company-turns-ad-data-into-government-tracking-tool?ref=404media.co" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.bloomberg.com/news/articles/2023-05-11/surveillance-company-turns-ad-data-into-government-tracking-tool?ref=404media.co&quot;}" href="https://www.bloomberg.com/news/articles/2023-05-11/surveillance-company-turns-ad-data-into-government-tracking-tool?ref=404media.co" rel="nofollow noopener" target="_blank">acquiring ad tech companies</a> and posing as prospective advertisers. The spy-run location data company doesn’t need to successfully place an ad; instead, it is able to gather data on devices by simply being plugged into that industry. Location data in this case can also include a users’ IP address, which is then geolocated to give their coarse location.</p><p>Last January, <a data-offer-url="https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/&quot;}" href="https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/" rel="nofollow noopener" target="_blank">404 Media reported</a> on an Israeli surveillance company called Patternz, which was sourcing masses of location data through the RTB process.</p><p>In an exposed training video, Patternz showed some of the popular apps it got location data from: 9GAG, Kik, sports app FUTBIN, caller ID apps such as CallApp and Truecaller; and various word, sudoku, and solitaire puzzle games. Every one of these apps are also in the Gravy data. This suggests that Gravy, or wherever Gravy got that data from, also sourced it from interacting with the advertising system rather than location-tracking code baked into the apps.</p><p>404 Media shared some of the location data with another security researcher with knowledge of the advertising and location data industries. “It appears that at least some of this data would likely have been sourced from advertising related, real-time bidding,” Krzysztof Franaszek, founder of Adalytics, a digital forensics firm, told 404 Media after reviewing the data. He pointed out some of the user-agents in the file, which show how a user’s device connected to a service, referenced “afma-sdk.” That is a string <a href="https://ads-developers.googleblog.com/2022/07/use-new-google-mobile-ads-sdk.html">used by Google’s Mobile Ads SDK</a> (software development kit). In other words, in some cases, it is Google’s advertising platform that is delivering the ads that are eventually leading to this tracking by outside companies and potentially government contractors.</p><p>Google did not respond to multiple requests for comment for this article. Neither did Apple.</p><p>Franaszek also says that “a significant amount of this geolocation dataset appears to be inferred by IP address to geolocation lookups, meaning the vendor or their source is deriving the user's geolocation by checking their IP address rather than by using GNSS [Global Navigation Satellite System]/GPS data. That would suggest that the data is not being sourced entirely from a location data SDK.”</p><p>“What we’re seeing here in this data appears to me to be a huge diversity of apps,” Edwards says. “That’s not what you see from an SDK ingestion; that’s what you see from bulk RTB ingestions.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In December <a href="https://www.ftc.gov/news-events/news/press-releases/2024/12/ftc-takes-action-against-mobilewalla-collecting-selling-sensitive-location-data">the Federal Trade Commission banned another location data company</a> called Mobilewalla from collecting consumer data “from online advertising auctions for purposes other than participating in those auctions.” In other words, the agency banned Mobilewalla from participating in the RTB process for building datasets on peoples’ devices. The <a data-offer-url="https://www.404media.co/ftc-bans-location-data-company-that-powers-the-surveillance-ecosystem/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/ftc-bans-location-data-company-that-powers-the-surveillance-ecosystem/&quot;}" href="https://www.404media.co/ftc-bans-location-data-company-that-powers-the-surveillance-ecosystem/" rel="nofollow noopener" target="_blank">FTC also said Venntel and Gravy collected data</a> without obtaining user consent, ordered the company to delete historical location data, and banned it from selling data related to sensitive areas like health clinics and places of worship, except in “limited circumstances” involving national security or law enforcement.</p><p>404 Media has verified the hacked Gravy data in various ways. Some files include credentials for Gravy’s Snowflake instances, a data warehousing tool. 404 Media checked that the URLs in the hacked files do correspond to real Snowflake instances. One file called “users” contains a long list of companies. Some of these firms denied having any relationship with Gravy; Cuebiq, another location data firm mentioned in the file, told 404 Media it “routinely evaluates available data in the market to determine if they are an appropriate fit for our business. Most do not make it past the evaluation stage to production, as was the case here. Cuebiq tested a limited data sample, which was never made available to our customers, and the data was deleted at the end of the limited trial.”</p><p>404 Media also sent a section of the data to another data broker called Datonics. “We investigated the matter described in your email, and the segment IDs in those files are those of Gravy, not Datonics,” the company said in an email.</p><p>Unacast, <a data-offer-url="https://www.prnewswire.com/news-releases/gravy-analytics-and-unacast-merge-to-become-leader-in-location-data-and-insights-302000184.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.prnewswire.com/news-releases/gravy-analytics-and-unacast-merge-to-become-leader-in-location-data-and-insights-302000184.html&quot;}" href="https://www.prnewswire.com/news-releases/gravy-analytics-and-unacast-merge-to-become-leader-in-location-data-and-insights-302000184.html" rel="nofollow noopener" target="_blank">which merged with Gravy in 2023</a>, did not respond to multiple requests for comment, both on the hack and whether it or any of its suppliers have derived location data from the real-time bidding process.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disappointed with the TVs at CES 2025 (181 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/</link>
            <guid>42650855</guid>
            <pubDate>Thu, 09 Jan 2025 23:20:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/">https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/</a>, See on <a href="https://news.ycombinator.com/item?id=42650855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2069712">
  
  <header>
  <div>
    <div>
      <div>
        <p><span>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="section-gadgets_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath><clipPath id="section-gadgets_svg__b"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g clip-path="url(#section-gadgets_svg__a)"><g fill="currentColor" clip-path="url(#section-gadgets_svg__b)"><path d="M38 22c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2V4h-4V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2S8 .9 8 2v2H4v4H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v4h4v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h4v-4h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6zm-6 10H8V8h24z"></path><path d="M24.7 17.3 20 12h-7.1c-.6 0-1 .4-1 1s.4 1 1 1h6.3l4.1 4.7L20 22h8v-8z"></path><path d="m15.2 22.7 4.7 5.3H27c.6 0 1-.4 1-1s-.4-1-1-1h-6.3l-4.1-4.7 3.3-3.3h-8v8z"></path></g></g></svg>
  </span>
  <span>
    Won't someone please think of the viewer?
  </span>
</p>
      </div>

      

      <p>
        Op-ed: TVs miss opportunity for real improvement by prioritizing corporate needs.
      </p>

      
    </div>

    <div>
    
    <p>
      The TV industry is hitting users over the head with AI and other questionable gimmicks 

              <span>
          Credit:

          
          Getty

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>If you asked someone what they wanted from <a href="https://arstechnica.com/gadgets/2024/12/buying-a-tv-in-2025-expect-lower-prices-more-ads-and-an-os-war/">TVs released in 2025</a>, I doubt they'd say "more software and AI." Yet, if you look at what TV companies have planned for this year, which is being primarily promoted at the CES technology trade show in Las Vegas this week, software and AI are where much of the focus is.</p>
<p>The trend reveals the implications of TV brands increasingly viewing themselves as software rather than hardware companies, with their products being customer data rather than TV sets. This points to an alarming future for smart TVs, where even premium models sought after for top-end image quality and hardware capabilities are stuffed with unwanted gimmicks.</p>
<h2>LG’s remote regression</h2>
<p>LG has long made some of the best—and most expensive—TVs available. Its OLED lineup, in particular, has appealed to people who use their TVs to watch Blu-rays, enjoy HDR, and the like. However, some features that LG is introducing to high-end TVs this year seem to better serve LG’s business interests than those users' needs.</p>
<p>Take the new remote. Formerly known as the Magic Remote, LG is calling the 2025 edition the AI Remote. That is already likely to dissuade people who are skeptical about AI marketing in products (<a href="https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2368040">research suggests</a> there are many such people). But the more immediately frustrating part is that the new remote doesn’t have a dedicated button for switching input modes, as previous remotes from LG and countless other remotes do.</p>
<figure>
    <p><img width="640" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-640x360.jpg" alt="LG AI remote" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1440x810.jpg 1440w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote.jpg 1920w" sizes="auto, (max-width: 640px) 100vw, 640px">
                  </p>
          <figcaption>
        <div><p>
      LG's AI Remote.

              <span>
          Credit:

                      <a href="https://www.youtube.com/watch?v=z_KexjTLETo" target="_blank">
          
          Tom's Guide/YouTube

                      </a>
                  </span>
          </p></div>
      </figcaption>
      </figure>

<p>To use the AI Remote to change the TV’s input—a common task for people using their sets to play video games, watch Blu-rays or DVDs, connect their PC, et cetera—you have to long-press the Home Hub button. Single-pressing that button brings up a dashboard of webOS (the operating system for LG TVs) apps. That functionality isn't immediately apparent to someone picking up the remote for the first time and detracts from the remote’s convenience.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>By overlooking other obviously helpful controls (play/pause, fast forward/rewind, and numbers) while including buttons dedicated to things like LG's free ad-supported streaming TV (FAST) channels and Amazon Alexa, LG missed an opportunity to update its remote in a way centered on how people frequently use TVs. That said, it feels like user convenience didn't drive this change. Instead, LG seems more focused on getting people to use webOS apps. LG can monetize app usage through, i.e., getting a cut of streaming subscription sign-ups, selling <a href="https://arstechnica.com/gadgets/2024/09/lg-tvs-continue-down-advertising-rabbit-hole-with-new-screensaver-ads/">ads on webOS</a>, and selling and<a href="https://arstechnica.com/gadgets/2024/10/streaming-industry-has-unprecedented-surveillance-manipulation-capabilities/">&nbsp;leveraging user data</a>.</p>
<h2>Moving from hardware provider to software platform</h2>
<p>LG, like many other TV OEMs, has been growing its ads and data business. Deals with data analytics firms like Nielsen give it more incentive to acquire customer data. Declining TV margins and rock-bottom prices from budget brands (like Vizio and Roku, which sometimes lose money on TV hardware sales and make up for the losses through ad sales and data collection) are also pushing LG's software focus. In the case of the AI Remote, software prioritization comes at the cost of an oft-used hardware capability.</p>
<p>Further demonstrating its motives, in September 2023, LG announced intentions to "become a media and entertainment platform company" by offering "services" and a "collection of curated content in products, including LG OLED and LG QNED TVs." At the time, the South Korean firm said it would invest 1 trillion KRW (about $737.7 million) into its webOS business through 2028.</p>
<p>Low TV margins, improved TV durability, market saturation, and broader economic challenges are all serious challenges for an electronics company like LG and have pushed LG to explore alternative ways to make money off of TVs. However, after paying four figures for TV sets, LG customers shouldn't be further burdened to help LG accrue revenue.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<h2>Google TVs gear up for subscription-based features</h2>
<p>There are numerous TV manufacturers, including Sony, TCL, and Philips, relying on Google software to power their TV sets. Numerous TVs announced at CES 2025 will come with what Google calls Gemini Enhanced Google Assistant. The idea that this is something that people using Google TVs have requested is somewhat contradicted by Google Assistant interactions with TVs thus far being “somewhat limited,” per a <a href="https://www.lowpass.cc/p/google-tv-far-field-microphones-ces-gemini">Lowpass</a> report.</p>
<p>Nevertheless, these TVs are adding far-field microphones so that they can hear commands directed at the voice assistant. For the first time, the voice assistant will include Google’s generative AI chatbot, Gemini, this year—another feature that TV users don’t typically ask for. Despite the lack of demand and the privacy concerns associated with microphones that can pick up audio from far away even when the TV is off, companies are still loading 2025 TVs with far-field mics to support Gemini. Notably, these TVs will likely allow the mics to be disabled, like you can with <a href="https://arstechnica.com/gadgets/2022/09/amazons-self-branded-tvs-get-fancier-with-quantum-dots-local-dimming/">other TVs using far-field mics.</a> But I still ponder about features/hardware that could have been implemented instead.</p>
<p>Google is also working toward having people pay a subscription fee to use Gemini on their TVs, <a href="https://www.pcworld.com/article/2568513/google-hopes-youll-pay-for-an-ai-tv-assistant-someday.html">PCWorld</a> reported.</p>
<p>“For us, our biggest goal is to create enough value that yes, you would be willing to pay for [Gemini],” Google TV VP and GM Shalini Govil-Pai told the publication.</p>
<p>The executive pointed to future capabilities for the Gemini-driven Google Assistant on TVs, including asking it to “suggest a movie like <em>Jurassic Park </em>but suitable for young children” or to show “Bollywood movies that are similar to <em>Mission: Impossible</em>.”</p>
<p>She also pointed to future features like showing weather, top news stories, and upcoming calendar events when someone is near the TV, showing AI-generated news briefings, and the ability to respond to questions like “explain the solar system to a third-grader” with text, audio, and YouTube videos.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>But when people have desktops, laptops, tablets, and phones in their homes already, how helpful are these features truly? Govil-Pai admitted to PCWorld that “people are not used to” using their TVs this way “so it will take some time for them to adapt to it.” With this in mind, it seems odd for TV companies to implement new, more powerful microphones to support features that Google acknowledges aren't in demand. I’m not saying that tech companies shouldn’t get ahead of the curve and offer groundbreaking features that users hadn’t considered might benefit them. But already planning to monetize those capabilities—with a subscription, no less—suggests a prioritization of corporate needs.</p>
<h2>Samsung is hungry for AI</h2>
<p>People who want to use their TV for cooking inspiration often turn to cooking shows or online cooking videos. However, Samsung wants people to use its TV software to identify dishes they want to try making.</p>
<p>During CES, Samsung announced Samsung Food for TVs. The feature leverages Samsung TVs’ AI processors to identify food displayed on the screen and recommend relevant recipes. Samsung introduced the capability in 2023 as an iOS and Android app after buying the app Whisk in 2019. As noted by <a href="https://techcrunch.com/2025/01/05/samsungs-new-tvs-can-find-recipes-for-dishes-in-shows/">TechCrunch</a>, though, <a href="https://www.tomsguide.com/ai/i-gave-claude-chatgpt-and-gemini-a-photo-of-some-ingredients-to-see-which-came-up-with-the-best-recipe-heres-the-results">other AI tools</a> for providing recipes based on food images <a href="https://www.cnet.com/tech/services-and-software/tired-of-eating-out-i-tried-this-recipe-generating-ai-tool-to-create-a-restaurant-meal-at-home/">are flawed</a>.</p>
<p>So why bother with such a feature? You can get a taste of Samsung’s motivation from its CES-announced deal with Instacart that lets people order off Instacart from Samsung smart fridges that support the capability. Samsung Food on TVs can show users the progress of food orders placed via the Samsung Food mobile app on their TVs. Samsung Food can also create a shopping list for recipe ingredients based on what it knows (using cameras and AI) is in your (supporting) Samsung fridge. The feature also requires a Samsung account, which allows the company to gather more information on users.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Other software-centric features loaded into Samsung TVs this year include a dedicated AI button on the new TVs’ remotes, the ability to use gestures to control the TV but only if you’re wearing a Samsung Galaxy Watch, and AI Karaoke, which lets people sing karaoke using their TVs by stripping vocals from music playing and using their phone as a mic.</p>
<p>Like LG, Samsung has shown growing interest in ads and data collection. In May, for example, it expanded its automatic content recognition tech to track ad exposure on streaming services viewed on its TVs. It also has an ads analytics partnership with Experian.</p>

<h2>Large language models on TVs</h2>
<p>TVs are mainstream technology in most US homes. Generative AI chatbots, on the other hand, are emerging technology that many people have yet to try. Despite these disparities, LG and Samsung are incorporating Microsoft’s Copilot chatbot into 2025 TVs.</p>
<p>LG claims that Copilot will help its TVs “understand conversational context and uncover subtle user intentions,” adding: “Access to Microsoft Copilot further streamlines the process, allowing users to efficiently find and organize complex information using contextual cues. For an even smoother and more engaging experience, the AI chatbot proactively identifies potential user challenges and offers timely, effective solutions.”</p>
<p>Similarly, Samsung, which is also adding Copilot to some of its smart monitors, said in its announcement that Copilot will help with “personalized content recommendations.” Samsung has also said that Copilot will help its TVs understand strings of commands, like increasing the volume and changing the channel, <a href="https://www.cnet.com/tech/home-entertainment/samsungs-2025-tvs-get-all-the-ai-extras-nobody-asked-for/">CNET</a> noted. Samsung said it intends to work with additional AI partners, namely Google, but it's unclear why it needs multiple AI partners, especially when it hasn’t yet seen how people use large language models on their TVs.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<h2>TV-as-a-platform</h2>
<p>To be clear, this isn't a condemnation against new, unexpected TV features. This also isn't a censure against new TV apps or the usage of AI in TVs.</p>
<p><a href="https://arstechnica.com/gadgets/2024/04/ai-marketing-hype-is-coming-for-your-favorite-gadgets/">AI marketing hype </a>is real and misleading regarding the demand, benefits, and possibilities of AI in consumer gadgets. However, there are some cases when innovative software, including AI, can improve things that TV users not only care about but actually want or need. For example, some TVs use AI for things like trying to optimize sound, color, and/or brightness, including based on current environmental conditions or upscaling. This week, Samsung announced AI Live Translate for TVs. The feature is supposed to be able to translate foreign language closed captions in real time, providing a way for people to watch more international content. It's a feature I didn't ask for but can see being useful and changing how I use my TV.</p>
<p>But a lot of this week's TV announcements underscore an alarming TV-as-a-platform trend where TV sets are sold as a way to infiltrate people's homes so that apps, <a href="https://arstechnica.com/gadgets/2024/12/tcl-tvs-will-use-films-made-with-generative-ai-to-push-targeted-ads/">AI</a>, and <a href="https://arstechnica.com/gadgets/2024/11/an-ad-giant-wants-to-control-your-next-tvs-operating-system/">ads</a> can be pushed onto viewers. Even high-end TVs are moving in this direction and amplifying features with questionable usefulness, effectiveness, and privacy considerations. Again, I can't help but wonder what better innovations could have come out this year if more R&amp;D was directed toward hardware and other improvements that are more immediately rewarding for users than karaoke with AI.</p>
<p>The TV industry is facing economic challenges, and, understandably, TV brands are seeking creative solutions for making money. But for consumers, that means paying for features that you’re likely to ignore. Ultimately, many people just want a TV with amazing image and sound quality. Finding that without having to sift through a bunch of fluff is getting harder.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/scharonharding/"><img src="https://arstechnica.com/wp-content/uploads/2021/09/scharon-harding-headshot.jpg" alt="Photo of Scharon Harding"></a></p>
  </div>

  <div>
    

    <p>
      Scharon is a Senior Technology Reporter at Ars Technica writing news, reviews, and analysis on consumer gadgets and services. She's been reporting on technology for over 10 years, with bylines at Tom’s Hardware, Channelnomics, and CRN UK.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/#comments" title="117 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    117 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/ai/2025/01/how-i-program-with-llms/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/ai-code-buddy-768x432.jpg" alt="Listing image for first story in Most Read: How I program with LLMs" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Soldering the Tek way (178 pts)]]></title>
            <link>https://hackaday.com/2025/01/09/retrotechtacular-soldering-the-tek-way/</link>
            <guid>42650561</guid>
            <pubDate>Thu, 09 Jan 2025 22:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackaday.com/2025/01/09/retrotechtacular-soldering-the-tek-way/">https://hackaday.com/2025/01/09/retrotechtacular-soldering-the-tek-way/</a>, See on <a href="https://news.ycombinator.com/item?id=42650561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        <main id="main" role="main">

        
            
<article itemscope="" itemtype="http://schema.org/Article" id="post-752809">
    <!-- .entry-header -->

    <div itemprop="articleBody">
        <p>For a lot of us, soldering just seems to come naturally. But if we’re being honest, none of us was born with a soldering iron in our hand — ouch! — and if we’re good at soldering now, it’s only thanks to good habits and long practice. But what if you’re a company that lives and dies by the quality of the solder joints your employees produce? How do you get them to embrace the dark art of soldering?</p>
<p>If you’re Tektronix in the late 1970s and early 1980s, the answer is simple: make <a href="https://youtu.be/yZSveVpgmIM" target="_blank">in-depth training videos that teach people to solder the Tek way</a>. The first video below, from 1977, is aimed at workers on the assembly line and as such concentrates mainly on the practical aspects of making solid solder joints on PCBs and mainly with through-hole components. The video does have a bit of theory on soldering chemistry and the difference between eutectic alloys and other tin-lead mixes, as well as a little about the proper use of silver-bearing solders. But most of the time is spent discussing the primary tool of the trade: the iron. Even though the film is dated and looks like a multi-generation dupe from VHS, it still has a lot of valuable tips; we’ve been soldering for decades and somehow never realized that cleaning a tip on a wet sponge is so effective because the sudden temperature change helps release oxides and burned flux. The more you know.</p>
<p><a href="https://youtu.be/jMchFqu3Jx0" target="_blank">The second video below</a> is aimed more at the Tek repair and rework technicians. It reiterates a lot of the material from the first video, but then veers off into repair-specific topics, like effective desoldering. Pro tip: Don’t use the “Heat and Shake” method of desoldering, and wear those safety glasses. There’s also a lot of detail on how to avoid damaging the PCB during repairs, and how to fix them if you do manage to lift a trace. They put a fair amount of emphasis on the importance of making repairs look good, especially with bodge wires, which should be placed on the back of the board so they’re not so obvious. It makes sense; Tek boards from the era are works of art, and you don’t want to mess with that.</p>

<p><iframe title="Tektronix Solder And Its Application In Electrical Assembly 1977" width="800" height="450" src="https://www.youtube.com/embed/yZSveVpgmIM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p><iframe title="Tektronix Making Quality Circuit Board Repairs 1980" width="800" height="450" src="https://www.youtube.com/embed/jMchFqu3Jx0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
	            </div><!-- .entry-content -->
    
    <!-- .entry-footer -->
</article><!-- #post-## -->

            	<!-- .navigation -->
	
            

            
<!-- #comments -->

        
        

        
        

        
        </main><!-- #main -->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: A friend has brain cancer: any bio hacks that worked? (126 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42649996</link>
            <guid>42649996</guid>
            <pubDate>Thu, 09 Jan 2025 21:33:06 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42649996">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42649996: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[How to delete your Facebook account (111 pts)]]></title>
            <link>https://www.theverge.com/22231495/delete-facebook-page-account-how-to</link>
            <guid>42649887</guid>
            <pubDate>Thu, 09 Jan 2025 21:19:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/22231495/delete-facebook-page-account-how-to">https://www.theverge.com/22231495/delete-facebook-page-account-how-to</a>, See on <a href="https://news.ycombinator.com/item?id=42649887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><div><p>You may be wondering how to delete your Facebook account now that <a href="https://www.theverge.com/24339131/meta-content-moderation-fact-check-zuckerberg-texas">fact-checking is no longer considered important</a>, and Meta’s changing its <a href="https://www.theverge.com/2025/1/7/24338471/meta-hate-speech-hateful-conduct-policy-moderation">definition of what constitutes Hateful Conduct</a>. It’s easy to do, and we’ll show you how. But you should download all your stuff first.</p></div><p>The following instructions are for the web version of Facebook, but you can follow pretty much the same sequence on the mobile app.</p><p><h3>Download your archives</h3></p><p>Your Facebook archives contain just about <a href="https://www.facebook.com/help/405183566203254?helpref=faq_content">all of the pertinent information related to your account</a>, including your photos, active sessions, chat history, IP addresses, facial recognition data, and which ads you clicked. That’s personal information you should save.</p><div><ul><li>Click on your personal icon in the upper-right corner. </li><li>Go to <strong>Settings &amp; Privacy</strong> &gt; <strong>Settings</strong>. </li><li>Click on the <strong>Accounts Center</strong> box on the left.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Facebook “Settings and privacy” page showing Account Center on the left." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>The Accounts Center is where you can both download your info and delete your account.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>Go to <strong>Your information and permissions</strong> on the left, and then <strong>Download Your Information &gt;  Download or transfer information.</strong></li><li>You can choose to transfer information from your Facebook or Instagram account (or both). </li><li>You now have another choice. You can select <strong>Available information</strong>, which includes everything but data logs. (Meta defines these as “additional details we collect and store that can be associated with you.”) Or you can select <strong>Specific types of information</strong>, which allows you to determine exactly what you want to download, including those data logs.</li><li>If you choose the latter, you can then select from the variety of data you’ve accumulated, including posts, friends, logged information, saved items and collections, etc. (You can also get the data logs, although Facebook warns it could take 15 days for them to show.) Click on <strong>Select all</strong> — but be aware you have to click it for each category.  When you’re ready, select <strong>Next</strong>.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A “Select information” pop-up box showing a list of types of Facebook data with checkboxes next to them." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>You can download various types of Facebook data, or all of it. The latter will take longer.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>Choose <strong>Download to device</strong> or <strong>Transfer to destination</strong>. According to Meta, the typical download is about 2.5GB.</li><li>You’ll now be able to select the date range of the info you want to download (or you can simply download all of it), the format (usually HTML or JSON), and the media quality (low, medium, or high). Enter an email address for a notification when the download is ready.</li><li>Finally, select <strong>Create files</strong>. You’ll receive an email when your file is ready, and it will be available for a few days. If you’ve been waiting a while and want to know the status of your download (or want to cancel it), go back to the <strong>Download your information</strong> tab.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up headed “Download your information” with a section labeled “In progress” halfway down." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>You’ll be notified when your data is ready for download.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><p><h3>Delete your account</h3></p><p>You’re ready to delete your account once you’ve finished downloading your archive.</p><div><ul><li>When you are ready, go back to the <strong>Accounts Center</strong> and click on <strong>Personal Details &gt; Account ownership and control &gt; Deactivation or deletion</strong>. </li><li>Click&nbsp;<strong>Deactivation and Deletion</strong>.</li><li>If you have both Facebook and Instagram accounts, you will be asked to choose one.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up window headed Deactivating or deleting your Facebook account, with the choice of doing either underneath." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Facebook gives you the option of temporarily deactivating your account — just in case you might change your mind.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>If you only want to deactivate your account temporarily (maybe you hope CEO Mark Zuckerberg <a href="https://www.theverge.com/24339131/meta-content-moderation-fact-check-zuckerberg-texas">will change his mind</a>?), you can choose to do so. Otherwise, select <strong>Delete account</strong> and click <strong>Continue</strong>.</li><li>You’ll be informed of any other accounts you have with Meta and given several options to explain why you’re leaving. Just keep hitting <strong>Continue</strong>.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up with check boxes so people can choose why they’re leaving Facebook." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Why do you want to leave? Choose your reason.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>You’ll see an option to deactivate your account instead or save the posts in your archive, download your info, and review the apps you’re logged into. When you’re ready, hit <strong>Continue</strong>.</li><li>You’ll be asked for your password for confirmation. Enter it. </li><li>Finally ready? Hit <strong>Delete account</strong>.</li><li>Once you click <strong>Delete account</strong>, your account will be marked for termination and inaccessible to others using Facebook. </li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up headed Confirm permanent account deletion with explanatory text beneath and a blue Delete account button." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>It takes a few pages, but you will finally get to the point where you can delete your account. And even after that, you have time to log in again.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><p>Meta <a href="https://www.facebook.com/help/125338004213029?helpref=uf_permalink">notes</a> that it delays termination for a few days after the request has gone through.  The deletion will be canceled if you log back in during that period. So don’t sign on, or you’ll be forced to start the process over again. </p><p>Certain things, like comments you’ve made on a friend’s post, may still appear even after you delete your account. Facebook also says that copies of certain items like log records will remain in its database, but it notes that those are disassociated with personal identifiers. </p><p>If you’re really serious about quitting anything associated with Meta, remember that the company owns several other popular services as well, like Instagram, WhatsApp, and Threads, so you should delete your accounts there, too.</p><p><em><strong>Update January 9th, 2025:</strong> This article was originally published on September 28th, 2018, and has been updated several times to allow for changes in the Facebook interface.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Britain got its first internet connection (2015) (152 pts)]]></title>
            <link>https://theconversation.com/how-britain-got-its-first-internet-connection-by-the-late-pioneer-who-created-the-first-password-on-the-internet-45404</link>
            <guid>42649340</guid>
            <pubDate>Thu, 09 Jan 2025 20:02:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theconversation.com/how-britain-got-its-first-internet-connection-by-the-late-pioneer-who-created-the-first-password-on-the-internet-45404">https://theconversation.com/how-britain-got-its-first-internet-connection-by-the-late-pioneer-who-created-the-first-password-on-the-internet-45404</a>, See on <a href="https://news.ycombinator.com/item?id=42649340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p><em>British computer scientist and <a href="https://www.internethalloffame.org/inductee/peter-kirstein">Internet Hall of Fame inductee</a> Peter Kirstein died in January 2020 at the age of 86, after a nearly 50-year career at UCL. A few years before he died, he was commissioned by then Conversation technology editor Michael Parker (now director of operations) to write an in-depth piece originally intended as part of a special series on the internet. It wasn’t published at the time, as the series was postponed, but now to mark Professor Kirsten’s contributions we are delighted to be able to publish his reflections on the challenges he faced connecting the UK in the early 1970s to the forerunner of what would become the modern internet. The article was edited by Michael with oversight kindly provided by <a href="https://theconversation.com/profiles/jon-crowcroft-143812">Professor Jon Crowcroft</a>, a colleague of Professor Kirstein’s.</em></p>

<p>The internet has become the most prevalent communications technology the world has ever seen. Though there are more fixed and mobile telephone connections, even they use internet technology in their core. For all the many uses the internet allows for today, its origins lie in the cold war and the need for a defence communications network that could survive a nuclear strike. But that defence communications network quickly became used for general communications and within only a few years of the first transmission, traffic on the predecessor to today’s internet was already 75% email.</p>

<h2>In the beginning</h2>

<p><a href="https://www.britannica.com/topic/ARPANET">Arpanet</a> was the vital precursor of today’s internet, commissioned by the US Defence Advanced Research Projects Agency (Darpa) in 1969. In his interesting account of <a href="https://www.computer.org/csdl/magazine/an/2011/03/man2011030004/13rRUxly9fL">why Arpanet came about</a>, Stephen Lukasic, Director of Darpa from 1970-75, wrote that if its true nature and impact had been realised it would never have been permitted under the US government structure of the time. The concept for a decentralised communications technology that would survive a nuclear attack would have placed it outside Darpa’s remit (as defence communications specifically were assigned to a different agency), so the focus changed to how to connect computers together so that major applications could be run on the most appropriate system available.</p>

<p>This was in the era of <a href="https://www.ibm.com/history/time-sharing">time-sharing computers</a>. Today’s familiar world of the ubiquitous “personal computer” on each desk was decades away. Computers of this time were generally very large, filling entire rooms, and comparatively rare. Users working at connected terminals would submit jobs to the computer which would allocate processing time for the job when available. The idea went that if these computers were networked together, an available remote computer could process a job even when the computers closer to the users were full. The resulting network was called Arpanet and the first packets of data traversed the network in September 1969. </p>

<figure>
            <a href="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>A CDC 7600 mainframe computer fills an entire room at Lawrence Livermore National Laboratory, California, mid-1970s.</span>
              <span><a href="https://www.flickr.com/photos/llnl/3094299714/">Lawrence Livermore National Laboratory</a>, <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a></span>
            </figcaption>
          </figure>

<p>At this time the computing industry was dominated by a few large companies, which produced products that would work only with others from the same company. However the Arpanet concept included a vital decision on how the network would function: <a href="https://twobithistory.org/2021/03/08/arpanet-protocols.html">it sharply distinguished and separated</a> the technology and medium that would carry the communications (satellite link, copper cable, fibre optic), the network layer (the software that manages communications between different computers), and applications (the programs that users run over the network to do work) from one another. </p>

<p>This contrasted with the vertical “stove-pipe” philosophy that persisted among computer manufacturers at the time, where any networking that existed worked only in specific situations and for specific computer systems. For example, IBM computers could communicate using IBM’s <a href="https://www.ibm.com/docs/en/zos-basic-skills?topic=implementation-what-is-systems-network-architecture-sna">SNA protocol</a>, but not with non-IBM equipment. The direction Arpanet took was manufacturer-agnostic, where different types of computers could be networked together.</p>

<h2>First footprint in Europe</h2>

<p>In 1970, the leading network research outside the US was a group at the <a href="https://www.npl.co.uk/getattachment/about-us/History/11408-History-of-NPL-May-2023.pdf.aspx?lang=en-GB">National Physical Laboratory</a> (NPL) in London led by <a href="https://www.internethalloffame.org/inductee/donald-davies/">Donald Davies</a>. Davies had built a network with similar concepts to Arpanet, and as one of the inventors of <a href="https://www.npl.co.uk/getattachment/about-us/History/Famous-faces/Donald-Davies/UK-role-in-Packet-Switching-(1).pdf.aspx?lang=en-GB">packet-switching</a> his work had influenced the direction of Arpanet. But despite his plans for a national digital network, he was prevented from extending his project outside the lab by pressure from the British Post Office, which then held a monopoly on telecommunications. </p>

<p>Around this time the director of the Arpanet project, <a href="https://www.nytimes.com/2018/12/30/obituaries/lawrence-g-roberts-dies-at-81.html">Larry Roberts</a>, proposed connecting Arpanet to Davies’ NPL network in the UK. This would be possible because a few years previously a large seismic array in Norway run by Norwegian researchers for Darpa had been connected to Arpanet via a dedicated 2.4 Kbps connection to Washington. Due to the transatlantic technology of the time, this was by satellite link via the only earth station for satellite communications in Europe, in <a href="https://www.bbc.co.uk/news/uk-england-cornwall-62277946">Goonhilly, Cornwall</a>, and thence by cable to Oslo. Larry proposed to interrupt the connection in London, connect the NPL network, and then continue to Norway. </p>

<p>Since the international communications were the main cost, this seemed straightforward. Unfortunately Britain was at this point negotiating to join the Common Market, and the UK government was afraid that closer links with the US would jeopardise the talks. When the government refused NPL permission to participate, as I was doing relevant research at the University of London’s Institute of Computer Science and subsequently at <a href="https://www.ucl.ac.uk/computer-science/about-0">UCL</a>, I was the obvious alternative.  </p>

<h2>Vaulting many non-technical hurdles</h2>

<p>From the beginning I proposed a twin approach. I would connect the large computers at the University of London and the <a href="https://www.chilton-computing.org.uk/acl/pdfs/davies.pdf">Rutherford and Appleton laboratories</a> (RAL) in Oxfordshire, which were hubs for other UK computer networks, and I would provide services to allow UK researchers to use the networks to collaborate with colleagues in the US.</p>

<p>This novel approach would mean the IBM System 360/195 at RAL, then the most powerful computer in the UK, would be made available as a remote host – available to those in the US on the other side of the transatlantic link, without being directly connected to the interface message processor – the equipment which sent and received messages between Arapanet nodes, which would be installed in UCL.</p>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=1242&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=1242&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=1242&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=1561&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=1561&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=1561&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p>
            <figcaption>
              <span>An interface message processor used to connect Arpanet nodes. About the size of a wardrobe, it is the type that would have been impounded by customs.</span>
              <span><a href="https://commons.wikimedia.org/wiki/File:ARPANET_first_router_2.jpg">Steve Jurvetson</a>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<p>Unfortunately there then came <a href="https://www.researchgate.net/publication/3330677_Early_experiences_with_the_Arpanet_and_Internet_in_the_UnitedKingdom">many non-technical hurdles</a>. I attempted to get other universities’ computer science departments to back the project, but this foundered because the Science Research Council did not consider the opportunity worth funding. The UK Department of Industry wanted a statement of interest from industry before funding, but even though I knew executives at ICL, the UK’s principal computer manufacturer, after months of agonising it declined stating that “one would gain more from a two-week visit to the US than from a physical link”. Consequently after a year of back and forth I had nothing.</p>

<p>However by 1973 the project was becoming a reality. By now the Norwegian siesmic array, <a href="https://www.norsar.no/about-us/history/arpanet">Norsar</a>, was connected to Arpanet via a newly opened satellite earth station at Tanum in Sweden, and so there was no longer a link via the UK at all. Now what was required was a link from UCL to Oslo. With a small grant of £5,000 from Donald Davies at the NPL, and the provision by the British Post Office of a 9.6 Kbps link to Oslo without charge for one year, we had the resources to proceed. </p>

<p>Darpa duly shipped its message processor with which to connect the new London node to Arpanet. It was promptly impounded at Heathrow Airport for import duty and the newly introduced Value Added Tax. I managed to avoid paying the duty by declaring it an “instrument on loan”, but it took all my available funds to provide a guarantee that would allow me to get hold of the equipment pending an appeal. With the equipment finally installed, in July 1973 I connected the first computers outside the US to the Arpanet, sending a transmission from London, via Norway, through the Arpanet to the Information Science Institute at the University of Southern California.</p>

<h2>First password on the internet</h2>

<p><a href="https://datatracker.ietf.org/doc/html/rfc588">Within three months</a> my group was able to implement the Arpanet network protocols and translate them to the IBM protocols necessary to communicate with computers at RAL. And so, once connected to the wider network through our gateway at UCL, the IBM computer at RAL became one of the most powerful on the Arpanet. </p>

<figure>
            <a href="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=430&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=430&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=430&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=540&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=540&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=540&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>Arpanet map in 1977. The satellite connection from SDAC to NORSAR and then to London visible bottom right, with the large box bottom right representing the computers available at the Rutherford and Appleton Laboratories, Royal Signals and Radar Establishment and elsewhere.</span>
              <span><a href="https://en.wikipedia.org/wiki/ARPANET#/media/File:Arpanet_logical_map,_march_1977.png">The Computer History Museum</a></span>
            </figcaption>
          </figure>

<p>When I gave a talk stating this fact, RAL staff first did not believe me; they still saw only my small minicomputer, without understanding that it was the gateway to the rest of the Arpanet on the other side of the link. On realising they became very concerned that access to their computer services would be available not only to me, but with my complicity to the whole research community in the US. </p>

<p>However, I had been concerned that I would, in exactly this way, be criticised for improper use of both UK and US facilities. So from the beginning I put password protection on my gateway. This had been done in such a way that even if UK users telephoned directly into the communications computer provided by Darpa in UCL, they would require a password. </p>

<p>In fact this was the first password on Arpanet. It proved invaluable in satisfying authorities on both sides of the Atlantic for the 15 years I ran the service – during which no security breach occurred over my link. I also put in place a system of governance that any UK users had to be approved by a committee which I chaired but which also had UK government and British Post Office representation. </p>

<hr>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=112&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=112&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=112&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=140&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=140&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=140&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p>
            <figcaption>
              <span></span>
              
            </figcaption>
          </figure>

<p><em>The <a href="https://theconversation.com/uk/insights">Insights section</a> is committed to high-quality <a href="https://theconversation.com/insights-the-conversations-long-reads-section-240155">longform journalism</a>. Our editors work with academics from many different backgrounds who are tackling a wide range of societal and scientific challenges.</em></p>

<hr>

<p>The transatlantic connection included terminal services (which connected users to remote computers to run jobs), file access and later email services. It was immediately very popular. Within a couple of years, I was supported by half a dozen government ministries, with leased line links (a dedicated line) to five remote sites – some of which allowed access through their own networks. Other users could telephone into my UCL site, or use the fledgling post office data network to which I also provided access. </p>

<p>Indeed, its profile had become so prominent that when the Queen opened a building at the Ministry of Defence’s Royal Radar Establishment at Malvern in Worcestershire in 1976 (which had taken over funding the leased line to Oslo), this was accompanied by her inaugurating the connection by <a href="https://www.internethalloffame.org/2012/12/31/how-queen-england-beat-everyone-internet/">sending an email</a> – the first to be sent by a head of state.</p>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=396&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=396&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=396&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=498&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=498&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=498&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p>
            <figcaption>
              <span>Her Majesty Queen Elizabeth II sends her first email, and the first email sent by a head of state, at the Royal Radar Establishment in 1976.</span>
              <span><span>Peter Kirstein</span></span>
            </figcaption>
          </figure>

<p>As the UK side of Arpanet continued growing, additional message processors had to be imported, each one racking up additional VAT and duty to be paid, pending the outcome of the appeal. Finally in 1976 the appeal was refused. But a meeting with senior treasury officials subsequently led to an agreement that my research group would be permitted to import equipment free of VAT and duty. The importance of this ruling cannot be overemphasised for ensuring the independence of our operation: over the following decade many government bodies considered trying take it over, and each time would be discouraged by the magnitude of the VAT and duty bill they would incur.</p>

<h2>Agreeing the language of Arpanet</h2>

<p>In their 1975 paper <a href="https://www.internethalloffame.org/inductee/robert-kahn/">Bob Kahn</a> at Darpa and <a href="https://www.internethalloffame.org/inductee/vint-cerf/">Vint Cerf</a> at Stanford University made the next vital contribution towards building the internet of today when they formulated the concept of connecting together different network technologies – such as those defined by different computer manufacturers, or designed for different communications media such as cable, satellite link or radio waves – with a <a href="https://www.cs.princeton.edu/courses/archive/fall06/cos561/papers/cerf74.pdf">common inter-network layer</a>, which would come to be known as TCP/IP. </p>

<p>Transport Control Protocol (TCP) managed the packaging and unpacking of data sent between computers, while Internet Protocol (IP) provided the pathfinding to ensure the data packets reached the intended destination. One of the important aspects of IP was that it allowed <a href="https://www.juniper.net/documentation/us/en/software/junos/interfaces-security-devices/topics/topic-map/security-interface-ipv4-ipv6-protocol.html">scalability</a>: the 8-bit number previously used to identify a computer on the network that allowed just 256 devices suddenly increased to a 32-bit number, which allowed 4 billion devices. </p>

<p>I misjudged how successful TCP/IP would be. In one of the <a href="https://archive.org/details/IssuesInPacketNetworkInterconnection/mode/1up?view=theater">first papers on network interconnection</a> Cerf argued that all computers should adopt TCP/IP, but I felt that this was unrealistic, and that gateways like the interface message processors were needed to “translate” communications between networks. While for the first 15 years my view prevailed, eventually in the long run Cerf’s view was the right one.</p>

<figure>
            <a href="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=388&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=388&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=388&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=488&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=488&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=488&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p></a>
            <figcaption>
              <span>Stanford Research Institute’s Packet Radio Van, used in the first TCP/IP internet experiments. The van drove across the Golden Gate Bridge while transmitting, and the steel girders interrupted the signal. But when it exited the bridge, the transmission picked up where it left off.</span>
              <span><a href="https://en.wikipedia.org/wiki/Packet_Radio_Van#/media/File:SRI_Packet_Radio_Van.jpg">SRI International</a>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<p>At UCL, my group participated in the first independent TCP/IP implementations, connecting in 1977 for the first time networks using a different technology to Arpanet. This saw three different types of network, Arpanet, the satellite network Satnet, and PRNET, a packet-radio network using <a href="https://computerhistory.org/blog/born-in-a-van-happy-40th-birthday-to-the-internet/">radio transmissions from mobile vans</a>, all connected using the same common “language”, TCP/IP. This was in essence the first demonstration of the internet – a network of networks.</p>

<p>Later, we connected the first multi-service heterogeneous network outside the US (<a href="https://www.jisc.ac.uk/janet">Janet</a>, the UK’s academic network connecting universities) to Arpanet, and then to the internet in the early 1980s. Indeed, UCL was the first organisation on Arpanet to adopt TCP/IP as standard.</p>

<figure>
            <a href="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=416&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=416&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=416&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=522&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=522&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=522&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>Schematic of the first internet demonstration, connecting three different networks, PRNET, ARPANET and SATNET, with TCP/IP. This was the first connection that created a ‘network of networks’, as the internet would become.</span>
              <span><a href="https://en.wikipedia.org/wiki/History_of_the_Internet#/media/File:First_Internet_Demonstration,_1977.jpg">Computer History Museum</a></span>
            </figcaption>
          </figure>

<p>During the 1980s the internet approach took over, where computers used TCP/IP to manage their own connections to the network. Darpa provided funding to add TCP/IP into its <a href="https://klarasystems.com/articles/history-of-freebsd-part-4-bsd-and-tcp-ip/">chosen operating system of the time, BSD</a>, and this was later made available to the public. </p>

<p>After the release of the IBM PC microcomputer in 1981 there was a rapid growth of cheap (relatively speaking) personal computers in offices connected to each other by <a href="https://www.wired.com/story/what-is-ethernet/">ethernet</a> networks. And routers (small devices to connect networks) were developed that made the huge, outdated interface message processors used with the original Arpanet obsolete. </p>

<p>The universal adoption of common protocols that provided useful services like virtual terminal (telnet), file transfer (FTP), directory (LDAP) and email (SMTP) made the internet an invaluable tool for researchers. As fibre optic installations became more economical it allowed networks to scale up to very large numbers of interconnected computers. The internet’s most widespread and largest use by volume was still email, but a number of shared data repositories and resources developed. </p>

<p>Then in 1989, with the development of the <a href="https://home.cern/science/computing/birth-web/short-history-web">World Wide Web</a>, Tim Berners-Lee provided the killer application that would make the internet essential to all types of commercial and government use. The simplicity and ease of use of the web and web browsers, together with the internet as the distribution mechanism underpinning it, laid the basis for the universal use of the internet we have today. </p>

<h2>The little black book of the internet</h2>

<p>Back when there were even only a few hundred computers, discovering their addresses and maintaining a directory of them had become impractical. Bob Kahn, then director of the relevant office at Darpa, remedied this problem by commissioning the <a href="https://www.cs.cornell.edu/courses/cs6411/2018sp/papers/mockapetris.pdf">Domain Name Service</a>. This mapped IP addresses to names organised in hierarchical structure. The effect was a sort of directory of internet-connected computers, where top level domains (such as .com, .org, .uk, .fr) lay above second level domains (such as .ac.uk, .co.uk, or microsoft.com, wikipedia.org), which in turn lay above domains below them (such as www.microsoft.com or www.wikipedia.org, where the www. represents a subdomain below the domain). This domain model forms the basis of the URLs that we type into our browser address bars today.</p>

<p>Although four billion addresses seemed near infinite in 1974, by the early 1990s it was already evident that the internet would soon run out of IP (IPv4) addresses, necessary for computers to be connected to the internet. Work on the next generation of IP, IPv6, was to increase the number of routable network addresses from 32-bit (2<sup>32,</sup> or 4 billion) to 128-bit (or 2<sup>128</sup> or 3.4x10<sup>29</sup> billion) addresses. Technical fixes managed to extend the lifetime of IPv4, but over the last few years the need to move to IPv6 has become pressing, and adoption is now happening faster. </p>

<figure>
            <p><iframe data-src="https://www.youtube.com/embed/JLilyBJeYgQ?wmode=transparent&amp;start=0" frameborder="0" allowfullscreen="" width="100%" height="400"></iframe></p>
            
          </figure>

<h2>Growth and change</h2>

<p>Over the last two decades, the emergence of social networks, the increasing availability of internet streaming media and the integration of mobile telephone networks with the internet have hugely increased demand for internet capacity. Such demand will require large investments to meet, but probably without any radical rethink of the internet’s architecture. The number of internet-connected devices is growing significantly, but we can assume that it would increase only to a small multiple of the world’s population. So even if the protocols that govern how devices connect to the internet had to change to cope with demand, this could be achieved within only a few years.</p>

<p>The ability to monitor the activities of people – with or without their knowledge – is one important outcome of so many people so frequently connected to the network. The ability by unauthorised individuals to hack into private systems, to obtain private data or damage operations, are very worrying developments. The advances in computer and network security needed require massive research and development, and new legal and regulatory powers. And an even more disruptive development now looms, <a href="https://direct.mit.edu/daed/article/145/1/33/27105/Edge-Networks-amp-Devices-for-the-Internet-of">the Internet of Things</a>.</p>

<p>Increasingly devices and equipment found in all aspects of our life may incorporate sensors and actuators that can be operated remotely. The estimated numbers of devices to be network-connected is much larger: as many as hundreds of billions within ten years. Cars (for navigation or automated driving), home appliances (for automation, security), devices on the national power grid (monitoring and error correction), smart buildings (temperature or humidity control, security), smart cities (traffic control, services supply, waste management), wearable and implanted medical devices, and so on. </p>

<p>The characteristics of such devices are often quite different from today’s computers on the internet. The data rate may be very low, and often but not necessarily the data may be required only for local networks, rather than full internet availability. The devices or their controllers may have internet interfaces, but they may not obey other internet protocols, and would possibly need to be left in place for years, or decades.</p>

<p>They may not be able to carry out sophisticated security operations themselves, yet ensuring they are secure will be crucial if they are not to become a vast vulnerable network of potential points of entry for hostile actors. It is the Things on the internet of the future, rather than typical computing devices, that may prompt a radical re-think of the ways the internet works. </p>

<p>The impact of the internet on our way of life in its first 40 years has been immeasurable. It has expanded and developed in a way none of us envisaged in 1975. While we may have a better idea of what to expect over the next couple of decades, I am sure most of us will be mistaken.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: How do you backup your Android? (116 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42648597</link>
            <guid>42648597</guid>
            <pubDate>Thu, 09 Jan 2025 18:43:07 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42648597">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42648597: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA["Twelfth Night Till Candlemas" – a 40-year book-quest and its remarkable ending (168 pts)]]></title>
            <link>https://davidallengreen.com/2024/12/twelfth-night-till-candlemas-the-story-of-a-forty-year-book-quest-and-of-its-remarkable-ending/</link>
            <guid>42647633</guid>
            <pubDate>Thu, 09 Jan 2025 17:07:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://davidallengreen.com/2024/12/twelfth-night-till-candlemas-the-story-of-a-forty-year-book-quest-and-of-its-remarkable-ending/">https://davidallengreen.com/2024/12/twelfth-night-till-candlemas-the-story-of-a-forty-year-book-quest-and-of-its-remarkable-ending/</a>, See on <a href="https://news.ycombinator.com/item?id=42647633">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
<article>

<div dir="auto">
<p>This post is about finally finding a book from one’s youth forty years later – and after nearly thirty years of searching.</p>
<p>It is also a tale about goblins and Christmas decorations; about the perils of ChatGPT and Artificial Intelligence; and about the real value of librarians, cataloguers, indexers, and archivists – what should be called the Noble Professions.</p>
<p>And it is an account that ends with not one but two wonderful events.</p>
<p>So if you are sitting comfortably, with a suitable seasonal drink, we will start with a bit of background and with a historical excursion.</p>
<p>*</p>
<p>Once upon a time there was a story.</p>
<p>And the story was in a book – a child’s anthology: the sort of book that one used to get in school bookshops and advertised in the special catalogues that were common in English schools (and elsewhere) in the 1970s and 1980s.</p>
<p>All the books I had at the time got lost – house moves and so on – and since the world wide web made searching for second-hand books easy I have replaced the books one-by-one.</p>
<p>When you re-read such books, sometimes what one thinks are one’s own original ideas and expressions stare back at you and you realise where you got them from.</p>
<p>What the economist J. M. Keynes once said –&nbsp;<em>“Practical men who believe themselves to be quite exempt from any intellectual influence, are usually the slaves of some defunct economist”</em>&nbsp;– has a far wider application.</p>
<p>Many of us are the slaves of what we read when very young.</p>
<p>*</p>
<p>But there was one book what eluded me, every time it was searched for.</p>
<p>What I could remember (or believed I could remember) was as follows:</p>
<p>– it was a story in an anthology;</p>
<p>– the story was about what will happen if you do not take your decorations down by Twelfth Night – for goblins and other ne’er-do-wells will go through your town and hide behind any remaining decorations and cause you mischief all year round;</p>
<p>– but there was a cure to this mischief if a certain thing was done on Candlemas – 2 February – and this was because of an esoteric rule which could be applied surreptitiously by those with special knowledge;</p>
<p>– the book was purple;</p>
<p>– the title or sub-title of the book, or of the story, was <em>“from Michelmas to Candlemas”&nbsp;</em>– the use of “<em>Candlemas</em>” was obvious from the story, and the&nbsp;<em>“Michaelmas”</em>&nbsp;I was certain about because it was a word I would again encounter in my late teens as a student, as it reminded me of the story/book.</p>
<p>(One of these memories, however, was false and another only semi-reliable.)</p>
<p>*</p>
<p>The story was important to me because it led to my passion for lore.</p>
<p>For me as a legal commentator, law (in its technical, black-letter sense) is practically far less important than what people – including lawyers and even judges – believe the law to be.</p>
<p>(Long-term followers may also recall my original blogging name was of&nbsp;<a href="https://en.wikipedia.org/wiki/Jack_o%27_Kent" rel="">a folklore hero</a>&nbsp;who bested the devil by careful attention to what was actually agreed.)</p>
<p>And so this remembered Candlemas story had everything for a lover of lore and law: a predicament, an obscure rule, the skilled application of that rule, and a remedy.</p>
<p>*</p>
<p>How I searched for this story – usually every year in November or December.</p>
<p>At first, I searched the web generally – with text and then, as Google developed, for the book cover.</p>
<p>I searched sites which had pictures of the book catalogues of the time.</p>
<p>I searched the British Library, the Bodleian Library, and every library I could think of.</p>
<p>Nil-return.</p>
<p>*</p>
<p>It was a mini-exercise&nbsp;<a href="https://www.youtube.com/watch?v=r2TilNclT8k" rel="">in being J. R. Hartley</a>&nbsp;year after year.</p>
<p>After a while certain results became familiar – and I probably know more about <a href="https://www.google.com/search?sca_esv=187175ea13837ba9&amp;sxsrf=ADLYWIIVO6N79MGGuSSGU9PrVvM2PfL4mQ:1734603763244&amp;q=Candlemas+books+devotional&amp;udm=2&amp;fbs=AEQNm0Aa4sjWe7Rqy32pFwRj0UkWde_MVagR57NIrd96T8bhwqEHeQ8jxei7F1s0aV23iWdi741Y3wNj5M6YQHR2X6laQ2bG_-lQAEb0rBAh8HRP8QnwCon_R8sFRzqEefr32Ob9swxFwdMawgPopA8MJ3jT9WeCgU-iUIUXZtKdtqKHtMWi_SZ41t31ikJ6wkUr5mZT0z82cmGy5mWpAaSfdIqbKHiMSw&amp;sa=X&amp;ved=2ahUKEwj6ionJzrOKAxUcWkEAHS_TK8IQtKgLegQIFhAB&amp;biw=1920&amp;bih=958&amp;dpr=1" rel="">devotional texts about</a>&nbsp;– and&nbsp;<a href="https://www.google.com/search?q=Candlemas+adventure+stories&amp;sca_esv=187175ea13837ba9&amp;udm=2&amp;biw=1920&amp;bih=958&amp;sxsrf=ADLYWIKkKlYh0g7MhfVOIAghQnYkSQZRFg%3A1734603765270&amp;ei=9fNjZ8ObEJ6VhbIPsMWz6QE&amp;ved=0ahUKEwiD6oTKzrOKAxWeSkEAHbDiLB0Q4dUDCBE&amp;uact=5&amp;oq=Candlemas+adventure+stories&amp;gs_lp=EgNpbWciG0NhbmRsZW1hcyBhZHZlbnR1cmUgc3Rvcmllc0j2LVDqBFiKLHABeACQAQCYAUGgAfAJqgECMjS4AQPIAQD4AQGYAgmgAoAEwgIEECMYJ8ICBBAAGB7CAgYQABgIGB7CAgUQABiABJgDAIgGAZIHATmgB_Iq&amp;sclient=img" rel="">adventure stories set</a>&nbsp;at – Candlemas than many other people.</p>
<p>And it was always a pleasure to renew contact with&nbsp;<a href="https://www.british-history.ac.uk/letters-papers-hen8/vol10/pp1-12" rel="">texts like</a>&nbsp;<em>“[i]t is a very old enactment that no Gascon wines or Toulouse woad be brought into England in strange bottoms, and nothing which has been done affects them but was devised to restrain the folly of English merchants who ventured to Bordeaux at unseasonable times, and the restraint&nbsp;<strong>from Michaelmas to Candlemas</strong>, by avoiding dangerous times, will rather augment the traffic…” (emphasis added.)</em></p>
<p>I bought books of Christmas stories on the off-chance they would reprint the story I was looking for – a disconcerting number of which appear to have been edited by Gyles Brandreth.</p>
<p>Nil-return.</p>
<p>*</p>
<p>When social media came along, I would then appeal from time-to-time for any information.</p>
<p>Those who responded were often very helpful – and so yet more Christmas anthologies were bought, and further lines of enquiry followed.</p>
<p>I made direct contact with experts in folklore and fairy tales, but they were as non-plussed as me.</p>
<p>Still nil-return.</p>
<p>*</p>
<p>Along the way though, I found out a great deal about the lores of the twelve days of Christmas and Candlemas which contextualised what I could remember.</p>
<p>For example, both Twelfth Night and&nbsp;<a href="https://en.wikipedia.org/wiki/Candlemas" rel="">Candlemas</a>&nbsp;have historically been the ends of the Christmas period – the latter being the fortieth day after Christmas.</p>
<p>And I discovered that Candlemas – which is also marked the purification (or what became known in England as ‘<a href="https://en.wikipedia.org/wiki/Churching_of_women" rel="">churching</a>’) of Mary and the presentation of Jesus at the Temple – was once an annual event that was very important in English culture.</p>
<p>Indeed Charles I arranged&nbsp;<a href="https://historyofparliament.com/2023/05/11/1626-coronation-charles-i/" rel="">his coronation to be held on Candlemas</a>.</p>
<p>And royalists made a point of celebrating Candlemas as part of what we would now call&nbsp;<em>“culture wars”</em>&nbsp;of the 1600s.</p>
<p>One once-famous poet, the loyalist clergyman&nbsp;<a href="https://en.wikipedia.org/wiki/Robert_Herrick_(poet)" rel="">Robert Herrick</a>&nbsp;<a href="https://gutenberg.org/cache/epub/22421/pg22421-images.html#id_2.p892" rel="">published three poems</a>&nbsp;about Candlemas, one of which urged the burning of decorations on that day, else bad things would follow:</p>
<p><em>Kindle the Christmas brand, and then</em></p>
<p><em>Till sunset let it burn;</em></p>
<p><em>Which quench’d, then lay it up again</em></p>
<p><em>Till Christmas next return.</em></p>
<p><em>Part must be kept wherewith to teend</em></p>
<p><em>The Christmas log next year,</em></p>
<p><em>And where ’tis safely kept, the fiend</em></p>
<p><em>Can do no mischief there.</em></p>
<p>(This ritual burning of decorations is a tradition that&nbsp;<a href="https://www.dailymail.co.uk/columnists/article-12930745/BORIS-JOHNSON-burn-Christmas-tree.html" rel="">still has echoes today</a>.)</p>
<p>After the culture wars of the 1600s, however, Candlemas became less popular – and soon it was all-but forgotten culturally, outside the annual blessing of candles at certain churches.</p>
<p>(On Candlemas in particular, see chapter 13 of&nbsp;<em>The Stations of the Sun</em>&nbsp;by Ronald Hutton, and on the place of Candlemas in the politics and religion of early modern England generally, see Eamon Duffy’s&nbsp;<em>The Stripping of the Altars: Traditional Religion in England, 1400-1580</em>.)</p>
<p>*</p>
<p>This was all fascinating, but it was not getting me any closer to the book or the story.</p>
<p>A couple of years ago, after the usual social media appeal, someone suggested I try the&nbsp;<a href="https://www.reddit.com/r/whatsthatbook/" rel="">r/whatsthatbook</a>&nbsp;thread on Reddit, where very clever and generous people spend time trying to identify books from the scantiest of details.</p>
<p>So&nbsp;<a href="https://www.reddit.com/r/whatsthatbook/comments/zdbnk3/book_with_candlemas_story/" rel="">I did</a>.</p>
<p>And someone there corresponded with a suggestion which actually covered each of the data points I could recall about the book – and it had the right title, and the book even had a well-known editor.</p>
<p>This was an extraordinary find – how could I have missed this in all the years of searching?</p>
<p>Well.</p>
<p>The reason it had never been uncovered before was because the impressive looking account had been generated – entirely fabricated – by ChatGPT.</p>
<p>This false account has now been deleted, but the correspondent remarked when I said this looked like it had been auto-generated:&nbsp;<em>“You’re right, I’ve tried chatGPT on some descriptions around here and it worked pretty well. However sometimes it has a propensity to spew random bullshit. I forgot because it’s so good in other areas. I’ll check better.”</em></p>
<p>I had never come across ChatGPT before – and so I have distrusted it ever since.</p>
<p>*</p>
<p>So this year – a couple of weeks ago – I did the annual appeal – but this time on BlueSky and Mastodon, and not on Twitter.</p>
<p>And yet again, people were helpful – anthologies were suggested and bought (though no further ones by Gyles Brandreth).</p>
<p>Someone again used ChatGPT, and they came up with:</p>
<p><em>“The book you’re describing sounds like “From Michaelmas to Candlemas” by Ruth Ainsworth. It was published in the 1970s and features seasonal stories aimed at children, including the one about the need to take down Christmas decorations by Candlemas to avoid goblins hiding behind them. The title references the traditional English calendar, marking the time from the feast of Michaelmas (September 29) to Candlemas (February 2). The story you mentioned aligns with themes found in folklore and poetry, including those by Robert Herrick. If this is the book you’re thinking of, it was indeed popular in school book clubs during that era.”</em></p>
<p>Again, like the account offered by the Reddit correspondent, this passage looks authoritative and plausible.</p>
<p>You will even notice how it neatly covers everything I could remember – giving equal weight to each data point and deftly joining them all together.</p>
<p>And again, what ChatGPT here had to offer was utterly – absolutely – false.</p>
<p>Like a fluent and practised (but unwise) liar it had contrived an account that fitted only the available information.</p>
<p>It was fake.</p>
<p>This year looked like another nil-return.</p>
<p>*</p>
<p>And then, something remarkable happened.</p>
<p>The appeal got&nbsp;<a href="https://bsky.app/profile/tambourine.bsky.social/post/3lc25w54a3k2k" rel="">this response</a>:</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" fetchpriority="high" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png?resize=589%2C948&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png 1456w" alt="" width="589" height="948" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/927d3b98-1136-4a4a-8bfb-74c7315bb76b_589x948.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:948,&quot;width&quot;:589,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:505803,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>Wow.</p>
<p>It was the same story, now looking up at me from a computer screen forty years later.</p>
<p>I remember the stylised first letter, the imagery, the pacing, the tone.</p>
<p>It did mention goblins as part of the ne’er-do-wells, but it was about a demon – not a goblin – who hid behind a sprig of holly.</p>
<p>(My insistence that it was a goblin was a semi-unreliable memory.)</p>
<p>And there was (who I now know was) Granny Hawkins being the holder of the all-important esoteric knowledge.</p>
<p>*</p>
<p>What had happened was this: Charlotte was far from a ChatGPT bot but instead a trained and experienced librarian.</p>
<p>(You can and should&nbsp;<a href="https://bsky.app/profile/tambourine.bsky.social" rel="">follow her here</a>&nbsp;– she is a genius and a treasure, and she has found other odd things out for other people.)</p>
<p>She sensibly assumed some of the things I could recall would have more weight – be more reliable – than others.</p>
<p>(The&nbsp;<em>“Michaelmas”</em>&nbsp;was, it turned out, a false memory – and this had undermined my searches.)</p>
<p>She then used various permutations of my memory points until she found a match, and she then found a book which someone had scanned onto internet archive.</p>
<p>You can see the book <a href="https://archive.org/details/ghostsshadows0000unse" rel="">here</a>.</p>
<p>The details there found could then be cross-referenced against&nbsp;<a href="http://philsp.com/" rel="">this truly amazing catalogue of fantasy short stories</a>&nbsp;-and it was indeed in an anthology – alongside the Herrick poem!</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png?resize=660%2C377&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png 1456w" alt="" width="660" height="377" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b02f8041-ef9c-477c-a5b7-13db21d7c341_951x543.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:543,&quot;width&quot;:951,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:209299,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>The story had been found – because of a librarian using critical skills (and thereby not giving equal weight to each factor), an archive, and a catalogue/index.</p>
<p>Verily: librarians, archivists, cataloguers, and indexers are the Noble Professions.</p>
<p>For they organise information in a manner in which humans actually think – unlike ChatGPT and Artificial Intelligence.</p>
<p>They are the holders of the old knowledge and skills.</p>
<p>*</p>
<p>So here is the book:</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png?resize=414%2C659&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png 1456w" alt="" width="414" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/41ff082d-5eb9-4c1a-bfdc-89f1705dd260_414x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:414,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:606327,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>*</p>
<p>My initial reaction was that Charlotte had certainly uncovered the same story – but it was perhaps in a different edition.</p>
<p>The cover of what Charlotte had found was black – and I distinctly remembered the book being purple.</p>
<p>Nonetheless I ordered the book online – so I could read all the story again in physical form (I refused to read it on the archive – I could wait one more week after so many years).</p>
<p>And when the book arrived I noticed something.</p>
<p>The back of the book was purple.</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/527e1691-3aee-49b5-a710-69ddf3dcf5c2_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:482521,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>Never judge a book by its front cover.</p>
<p>*</p>
<p>Before we come to the second wonderful event of this book-quest, here is the story of&nbsp;<em>“Twelfth Night Till Candlemas”</em>&nbsp;in full.</p>
<p>Note as you read how the old knowledge is used and the necessary rule are applied – and how the vicar ensures that a suppressed, secret ceremony can take place – there seems to be a great deal of cultural and religious knowledge behind this simple-looking children’s story.</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/725fbcf9-bac6-4b65-a15c-0e46dc3cccd8_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:335613,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/700745e2-8a85-481f-9f50-9427877fa4b6_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:347152,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d50de177-acdb-4042-91b5-b0365b00a5f6_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:385282,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1c79dc38-f246-4419-bf01-8496f5a7a131_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:378668,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dace6fa3-1a4d-406f-ac71-2eef3aad1005_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:383785,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png?resize=381%2C659&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0736730-8772-48b3-b369-fadafefc7a97_381x659.png 1456w" alt="" width="381" height="659" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a0736730-8772-48b3-b369-fadafefc7a97_381x659.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:381,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:271792,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>*</p>
<p><em>“Of course!”</em>&nbsp;said Granny Hawkins.</p>
<p>Of course.</p>
<p>*</p>
<p>But who was Ruth C. Paine?</p>
<p>It was certainly not this&nbsp;<a href="https://en.wikipedia.org/wiki/Ruth_Paine" rel="">Ruth Paine</a>&nbsp;(which made internet searches very difficult).</p>
<p>The editor&nbsp;<a href="https://en.wikipedia.org/wiki/Dorothy_Edwards_(children%27s_writer)" rel="">Dorothy Edwards</a>&nbsp;was a prolific author and editor – many of her books are still in print – and she was also involved with&nbsp;<em><a href="https://en.wikipedia.org/wiki/Listen_with_Mother" rel="">Listen with Mother</a></em>.</p>
<p>(I said you should be sitting comfortably.)</p>
<p>But Ruth C. Paine was more elusive.</p>
<p>*</p>
<p>What I was then able to find out was that Ruth C. Paine had published stories in a number of Dorothy Edwards’ anthologies.</p>
<p>Here is another example, about the changing of the seasons, with a splendid line from a frog about how to deal with winter</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png?resize=393%2C649&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png 1456w" alt="" width="393" height="649" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23b4dd81-5322-4ef4-b39c-1680bbf3faf5_393x649.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:649,&quot;width&quot;:393,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:408742,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png?resize=660%2C544&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png 1456w" alt="" width="660" height="544" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6f322288-26f8-44f2-9c4e-45e0bd7cc060_794x655.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:655,&quot;width&quot;:794,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:841143,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>*</p>
<p><em>‘I’m just off to the pond. I shall dive to the bottom and cover myself with mud and stay there. That is the only proper way to spend the winter,’ said Frog, and he hopped away.</em></p>
<p>*</p>
<p>I can also recommend the story about old Mother Merriweather in her&nbsp;<em>Cuckoo Fair</em>&nbsp;story, which deals with summer, in&nbsp;<a href="https://www.amazon.co.za/Mists-Magic-Dorothy-Edwards/dp/0718825373" rel="">this other Dorothy Edwards anthology</a>.</p>
<p>*</p>
<p>A bit more research showed that Ruth C. Paine had contributed&nbsp;<a href="https://genome.ch.bbc.co.uk/schedules/service_bbc_radio_fourfm/1971-07-09" rel="">a story for broadcast for&nbsp;</a><em><a href="https://genome.ch.bbc.co.uk/schedules/service_bbc_radio_fourfm/1971-07-09" rel="">Listen with Mother</a></em>&nbsp;(thank you to the kind person who put the&nbsp;<em>Radio Times</em>&nbsp;listings archive online).</p>
<p>But otherwise it was really not surprising that an author from the 1970s and 1980s had so little online trace.</p>
<p>It crossed my mind that Ruth C. Paine could be a pseudonym of Dorothy Edwards – such things are not uncommon with busy editors who need to fill spaces in books and broadcasts.</p>
<p>Yet there was something about the distinctive depth of knowledge behind the Candlemas story which made it unlikely to be a throwaway pseudonym of someone else. And Dorothy Edwards often included her own stories in her anthologies.</p>
<p>Anyway, no matter: I had the book and the story.</p>
<p>That is where I thought this story would end.</p>
<p>*</p>
<p>And then the second remarkable event occurred.</p>
<p>Charlotte and I got&nbsp;<a href="https://bsky.app/profile/vickymackenzie.bsky.social/post/3lclu2ovlpc2h" rel="">this reply</a>&nbsp;from the novelist&nbsp;<a href="https://victoriamackenzie.net/" rel="">Victoria MacKenzie</a>:</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png?resize=596%2C666&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png 1456w" alt="" width="596" height="666" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aba439d8-acc6-48a1-8dd6-17e3d74e1594_596x666.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:666,&quot;width&quot;:596,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:133344,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>And so Ruth C. Paine certainly did exist – and, as the Candlemas story indicated, she did have an extensive knowledge of religion and cultural history.</p>
<p>Her great niece has now kindly provided the following fascinating details:</p>
<p><em>“Ruth Cecilia Paine was my great aunt – my grandfather’s twin sister – and although she passed away in 2001, when I was just twenty-one, she was a big influence on my life. We wrote to each other regularly (I still remember her postcode, all these years later) and she was very encouraging to me about my education; at the start of each school year she sent me a little money for stationery, which I found incredibly exciting!</em></p>
<p><em>“I always knew that she had written stories for children, but that only a handful had been published – mostly in anthologies edited by Dorothy Edwards. As far as I knew, writing was a hobby, but I sensed it was one that meant a great deal to her. She often sent me books as gifts and occasionally I visited her in her flat in Canterbury where she lived with her lifelong companion, Lillian.</em></p>
<p><em>“She was a Christian and a church-goer all her life, latterly giving tours of Canterbury Cathedral. My dad told me that she’d been a missionary in India earlier in her life and I seem to half-remember a story she told me of travelling through a monsoon in a small aeroplane – understandably a terrifying experience!</em></p>
<p><em>“When she returned to in Britain she became a Religious Studies teacher, and her last job was at Hastings High School, before her retirement around the time I was born in 1980. Apparently she was regarded as quite formidable by my dad’s generation, but she was always very kind to me. I wish so much that she could have known her great niece would become a writer!”</em></p>
<p>*</p>
<p>The formidable Ruth C. Paine had indeed been a former missionary in her youth – Birmingham University records attest this.</p>
<p>This is Ruth Cecilia Paine in her teaching days:</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg?resize=268%2C297&amp;ssl=1" sizes="auto, 100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg 1456w" alt="" width="268" height="297" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3af0f479-0818-44ac-a779-c39df7d98d08_268x297.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:297,&quot;width&quot;:268,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:21283,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>And not only did Vicky MacKenzie provide this information and this photo, she also had a box of papers from her late great aunt, and in that box of papers were the original amended proofs of the personally influential story I had spent years looking for!</p>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg?w=317&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg 1456w" alt="" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/21d2c02a-88d8-4502-81a4-c905c60b6d8b_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:317,&quot;bytes&quot;:441348,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg?w=319&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg 1456w" alt="" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7dfd8064-4c5d-4de8-a28f-13b6b05fdda6_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:319,&quot;bytes&quot;:774750,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg?w=319&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg 1456w" alt="" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/12ee94d0-2d2e-4fb1-8fc1-f8d76a40090b_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:319,&quot;bytes&quot;:827104,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg?w=318&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg 1456w" alt="" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/296797d8-bba9-4aea-b7d2-aeecb721b7c4_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:318,&quot;bytes&quot;:827104,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<div>
<figure>
<div>
<picture><source srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 1456w" type="image/webp" sizes="100vw"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg?w=320&amp;ssl=1" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg 1456w" alt="" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eeb5a86a-5616-4ecc-aecc-7fe92ef4704b_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:320,&quot;bytes&quot;:556761,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}"></picture>

</div>
</figure>
</div>
<p>*</p>
<p><em>“Of course!”</em>&nbsp;said Granny Hawkins.</p>
<p>Of course.</p>
<p>*</p>
<p>In a matter of days I has gone from the ritual despair of an annual fruitless, futile search, to not only having the story and the book – but to also seeing the actual manuscript of the story I had spent forty years thinking about and about thirty years searching for.</p>
<p>This was a wonderful, extraordinary turn-of-events.</p>
<p>*</p>
<p>Two things can perhaps be said by way of a conclusion to this story.</p>
<p>The first is that we should be wary of the mischievous demons of our own age – that is ChatGPT and Artificial Intelligence – and to renew our trust in the Noble Professions who hold the old knowledge and skills: librarians, archivists, cataloguers, and indexers.</p>
<p>The second is that nowadays the real problem perhaps is not with Christmas decorations staying up too late, but with them going up too early, and with shops selling Christmas wares and playing Christmas music well before Advent, let alone Christmas.</p>
<p>We need new cautionary tales about when such things should be done and not done.</p>
<p>We are going to need some new goblins.</p>
<p>*******</p>
<p>I am very grateful to the heirs and holders of the literary estate of Ruth C. Paine for their kind permission for me to publish&nbsp;<em>“Twelfth Night Till Candlemas”</em>&nbsp;and&nbsp;<em>“How Nip spent the winter”</em>.</p>
<p>Editors would do well to contact Vicky MacKenzie to arrange permission to put her great aunt’s stories in new anthologies.</p>
<p>I am also grateful to Vicky MacKenzie for her kind permission to publish the unpublished corrected manuscript of&nbsp;<em>“Twelfth Night Till Candlemas”&nbsp;</em>and the unpublished photograph of her aunt..</p>
<p>Many thanks to Charlotte who not only found the story, but dealt with many follow-on queries.</p>
<p>Many thanks also to my friends who listened to previous versions of this post.</p>
<p>This post is dedicated to one of these friends, Nick, who is currently dealing with a challenging time – and who has also listened to me go on about story-telling for over thirty years. Poor sod.</p>
</div>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: TabPFN v2 – A SOTA foundation model for small tabular data (128 pts)]]></title>
            <link>https://www.nature.com/articles/s41586-024-08328-6/link</link>
            <guid>42647343</guid>
            <pubDate>Thu, 09 Jan 2025 16:38:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/s41586-024-08328-6/link">https://www.nature.com/articles/s41586-024-08328-6/link</a>, See on <a href="https://news.ycombinator.com/item?id=42647343">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                        <div id="Sec1-section" data-title="Main"><h2 id="Sec1">Main</h2><div id="Sec1-content"><p>Throughout the history of artificial intelligence, manually created algorithmic components have been replaced with better-performing end-to-end learned ones. Hand-designed features in computer vision, such as SIFT (Scale Invariant Feature Transform)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Lowe, D. G. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis. 60, 91–110 (2004)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR10" id="ref-link-section-d83722773e547">10</a></sup> and HOG (Histogram of Oriented Gradients)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Dalal, N. &amp; Triggs, B. Histograms of oriented gradients for human detection. In Proc. 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) 886–893 (IEEE, 2005)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR11" id="ref-link-section-d83722773e551">11</a></sup>, have been replaced by learned convolutions; grammar-based approaches in natural language processing have been replaced by learned transformers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Vaswani, A. et al. Attention is all you need. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR12" id="ref-link-section-d83722773e555">12</a></sup>; and the design of customized opening and end-game libraries in game playing has been superseded by end-to-end learned strategies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Silver, D. et al. Mastering the game of go with deep neural networks and tree search. Nature 529, 484–489 (2016)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR3" id="ref-link-section-d83722773e559">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Silver, D. et al. Mastering the game of go without human knowledge. Nature 550, 354–359 (2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR13" id="ref-link-section-d83722773e562">13</a></sup>. Here we extend this end-to-end learning to the ubiquitous domain of tabular data.</p><p>The diversity of tabular data&nbsp;sets them apart from unprocessed modalities such as text and images. While in language modelling&nbsp;for example the meaning of a word is consistent across documents, in tabular datasets the same value can mean fundamentally different things. A drug discovery dataset, for example, might record chemical properties, whereas another dataset in materials science might document thermal and electric properties. This specialization leads to a proliferation of smaller, independent datasets and associated models. To illustrate, on the popular tabular benchmarking website openml.org, 76% of the datasets contain less than 10,000 rows at the time of writing.</p><p>Deep learning methods have traditionally struggled with tabular data, because of the heterogeneity between datasets and the heterogeneity of the raw data itself: Tables contain columns, also called features, with various scales and types (Boolean, categorical, ordinal, integer, floating point), imbalanced or missing data, unimportant features, outliers and so on. This made non-deep-learning methods, such as tree-based models, the strongest contender so far<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e572">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e575">15</a></sup>.</p><p>However, these traditional machine learning models have several drawbacks. Without substantial modifications, they yield poor out-of-distribution predictions and poor transfer of knowledge from one dataset to another<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goodfellow, I., Bengio, Y. &amp; Courville, A. Deep Learning (MIT Press, 2016)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR16" id="ref-link-section-d83722773e582">16</a></sup>. Finally, they are hard to combine with neural networks, as they do not propagate gradients.</p><p>As a remedy, we introduce TabPFN, a foundation model for small- to medium-sized tabular data. This new supervised tabular learning method can be applied to any small- to moderate-sized dataset and yields dominant performance for datasets with up to 10,000 samples and 500 features. In a single forward pass, TabPFN significantly outperforms state-of-the-art baselines on our benchmarks, including gradient-boosted decision trees, even when these are allowed 4 h of tuning, a speedup of 5,140× (classification) and 3,000× (regression). Finally, we demonstrate various foundation model characteristics of TabPFN, including fine-tuning, generative abilities and density estimation.</p></div></div><div id="Sec2-section" data-title="Principled in-context learning"><h2 id="Sec2">Principled in-context learning</h2><div id="Sec2-content"><p>TabPFN leverages in-context learning (ICL)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Brown, T. et al. Language models are few-shot learners. In Proc. Advances in Neural Information Processing Systems (eds Larochelle, H. et al.) Vol. 33, 1877–1901 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR17" id="ref-link-section-d83722773e598">17</a></sup>, the same mechanism that led to the astounding performance of large language models, to generate a powerful tabular prediction algorithm that is fully learned. Although ICL was first observed in large language models, recent work has shown that transformers can learn simple algorithms such as logistic regression through ICL<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Garg, S., Tsipras, D., Liang, P. S. &amp; Valiant, G. What can transformers learn in-context? A case study of simple function classes. In Proc. Advances in Neural Information Processing Systems Vol. 35, 30583–30598 (ACM, 2022)." href="#ref-CR18" id="ref-link-section-d83722773e602">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. &amp; Zhou, D. What learning algorithm is in-context learning? Investigations with linear models. In Proc. The Eleventh International Conference on Learning Representations (ICLR, 2023)." href="#ref-CR19" id="ref-link-section-d83722773e602_1">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Von Oswald, J. et al. Transformers learn in-context by gradient descent. In Proc. 40th International Conference on Machine Learning 35151–35174 (PMLR, 2023)." href="#ref-CR20" id="ref-link-section-d83722773e602_2">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Zhou, H. et al. What algorithms can transformers learn? A study in length generalization. In Proc. The Twelfth International Conference on Learning Representations (ICLR, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR21" id="ref-link-section-d83722773e605">21</a></sup>. Prior-data Fitted Networks (PFNs) have shown that even complex algorithms, such as Gaussian Processes and Bayesian Neural Networks, can be approximated with ICL<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e609">22</a></sup>. ICL enables us to learn a wider space of possible algorithms, including cases for which a closed-form solution does not exist.</p><p>We build on a preliminary version of TabPFN<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hollmann, N., Müller, S., Eggensperger, K. &amp; Hutter, F. TabPFN: a transformer that solves small tabular classification problems in a second. In Proc. The Eleventh International Conference on Learning Representations (ICLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR23" id="ref-link-section-d83722773e616">23</a></sup>, which demonstrated the applicability of in-context-learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Brown, T. et al. Language models are few-shot learners. In Proc. Advances in Neural Information Processing Systems (eds Larochelle, H. et al.) Vol. 33, 1877–1901 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR17" id="ref-link-section-d83722773e620">17</a></sup> for tabular data in principle but had many limitations that rendered it inapplicable in most cases. Based on a series of improvements, the new TabPFN scales to 50× larger datasets; supports regression tasks, categorical data and missing values; and is robust to unimportant features and outliers.</p><p>The key idea behind TabPFN is to generate a large corpus of synthetic tabular datasets and then train a transformer-based<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Vaswani, A. et al. Attention is all you need. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR12" id="ref-link-section-d83722773e627">12</a></sup> neural network to learn to solve these synthetic prediction tasks. Although traditional approaches require hand-engineered solutions for data challenges such as missing values, our method autonomously learns effective strategies by solving synthetic tasks&nbsp;that include these challenges. This approach leverages ICL as a framework for exemplar-based declarative programming of algorithms. We design desired algorithmic behaviour by generating diverse synthetic datasets that demonstrate the desired behaviour and then train a model to encode an algorithm that satisfies it. This shifts the algorithm design process from writing explicit instructions to defining input–output examples, opening up possibilities for creating algorithms in various domains. Here, we apply this approach to the high-impact field of tabular learning, generating a powerful tabular prediction algorithm.</p><p>Our ICL approach differs fundamentally from standard supervised deep learning. Usually, models are trained per dataset, updating model parameters on individual samples or batches according to hand-crafted weight-updating algorithms, such as Adam<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Kingma, D. &amp; Ba, J. Adam: A method for stochastic optimization. In Proc. International Conference on Learning Representations (ICLR, 2015)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR24" id="ref-link-section-d83722773e634">24</a></sup>. At inference time, the learned model is applied to test samples. By contrast, our approach is trained across datasets and is applied to entire datasets at inference time rather than individual samples. Before being applied to real-world datasets, the model is once pre-trained on millions of synthetic datasets representing different prediction tasks. At inference time, the model receives an unseen dataset with both labelled training and unlabelled test samples and performs training and prediction on this dataset in a single neural network forward pass.</p><p>Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig1">1</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig2">2</a> outline our approach:</p><ol>
                <li>
                  <span>1.</span>
                  
                    <p>Data generation: we define a generative process (referred to as our prior) to synthesize diverse tabular datasets with varying relationships between features and targets, designed to capture a wide range of potential scenarios that our model might encounter. We sample millions of datasets from the generative process. For each dataset, a subset of samples has their target values masked, simulating a supervised prediction problem. Further details of our prior design are shown in the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec4">Synthetic data based on causal models</a>’.</p>
                  
                </li>
                <li>
                  <span>2.</span>
                  
                    <p>Pre-training: we train a transformer model, our PFN, to predict the masked targets of all synthetic datasets, given the input features and the unmasked samples as context. This step is done only once during model development, learning a generic learning algorithm that can be used to predict any dataset.</p>
                  
                </li>
                <li>
                  <span>3.</span>
                  
                    <p>Real-world prediction: the resulting trained model can now be applied to arbitrary unseen real-world datasets. The training samples are provided as context to the model, which predicts the labels of these unseen datasets through ICL.</p>
                  
                </li>
              </ol><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Overview of the proposed method."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Overview of the proposed method.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="365"></picture></a></div><p><b>a</b>, The high-level overview of TabPFN pre-training and usage. <b>b</b>, The TabPFN architecture. We train a model to solve more than 100 million synthetic tasks. Our architecture is an adaptation of the standard transformer encoder that is adapted for the two-dimensional data encountered in tables.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Our approach also has a theoretical foundation as described in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e714">22</a></sup>. It can be viewed as approximating Bayesian prediction for a prior defined by the synthetic datasets. The trained PFN will approximate the posterior predictive distribution <span>\(p({\widehat{{\bf{y}}}}_{{\rm{test}}}| {{\bf{X}}}_{{\rm{test}}},{{\bf{X}}}_{{\rm{train}}},{{\bf{y}}}_{{\rm{train}}})\)</span>  and thus return a Bayesian prediction for the specified distribution over artificial datasets used during PFN pre-training.</p></div></div><div id="Sec3-section" data-title="An architecture designed for tables"><h2 id="Sec3">An architecture designed for tables</h2><div id="Sec3-content"><p>The transformer architecture is currently the favoured architecture for flexible deep learning and foundation models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Jumper, J. M. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583 – 589 (2021)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR4" id="ref-link-section-d83722773e834">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="OpenAI. GPT-4 Technical Report. Preprint at 
                  https://arxiv.org/abs/2303.08774
                  
                 (2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR5" id="ref-link-section-d83722773e837">5</a></sup>. Transformer models work on sequences and combine information between sequence items using so-called attention mechanisms<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. In Proc. 3rd International Conference on Learning Representations (eds Bengio, Y. &amp; LeCun, Y.) (ICLR, 2015)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR25" id="ref-link-section-d83722773e841">25</a></sup>, allowing them to effectively capture long-range dependencies and learn complex relationships in data. Although transformer-based models can be applied to tabular data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Gorishniy, Y., Rubachev, I., Khrulkov, V. &amp; Babenko, A. Revisiting deep learning models for tabular data. In Proc. Advances in Neural Information Processing Systems 34 (eds Ranzato, M. et al.) 18932–18943 (NeurIPS, 2021)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR26" id="ref-link-section-d83722773e845">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Zhu, B. et al. XTab: cross-table pretraining for tabular transformers. In Proc. 40th International Conference on Machine Learning (eds Krause, A. et al.) 43181–43204 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR27" id="ref-link-section-d83722773e848">27</a></sup>, TabPFN addresses two key limitations inherent to them. First, as transformers are designed for sequences, they treat the input data as a single sequence, not using the tabular structure. Second, machine learning models are often used in a fit-predict model, in which a model is fitted on the training set once and then reused for multiple test datasets. Transformer-based ICL algorithms, however, receive train and test data in a single pass and thus perform training and prediction at once. Thus, when a fitted model is reused, it has to redo computations for the training set.</p><p>To better use the tabular structure, we propose an architecture that assigns a separate representation to each cell in the table, inspired by refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e855">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Lorch, L., Sussex, S., Rothfuss, J., Krause, A. &amp; Schölkopf, B. Amortized inference for causal structure learning. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) Vol. 35, 13104–13118 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR28" id="ref-link-section-d83722773e858">28</a></sup>. Our architecture, visualized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig1">1b</a>, uses a two-way attention mechanism, with each cell attending to the other features in its row (that is, its sample) and then attending to the same feature across its column (that is, all other samples). This design enables the architecture to be invariant to the order of both samples and features and enables more efficient training and extrapolation to larger tables than those encountered during training, in terms of both the number of samples and features.</p><p>To mitigate repeating computations on the training set for each test sample in a fit-predict setting, our model can separate the inference on the training and test samples. This allows us to perform ICL on the training set once, save the resulting state and reuse it for multiple test set inferences. On datasets with 10,000 training samples and 10 features, our optimized train-state caching results in inference speedups of around 300× on CPU (from 32 s to 0.1 s) and 6× on GPU. With 10× more features (100), the speedups increase to 800× on CPU and 30× speedup on GPU. These measurements focus solely on the core inference process, excluding pre-processing and ensembling steps detailed in the section ‘Inference details’. The lower speedups on GPUs are because of an underutilization of their massively parallel architecture.</p><p>We further optimize the memory and compute requirements of the architecture by computing layer norms in half-precision, using flash attention<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Dao, T., Fu, D., Ermon, S., Rudra, A. &amp; Ré, C. Flashattention: fast and memory-efficient exact attention with io-awareness. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) Vol. 35, 16344–16359 (2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR29" id="ref-link-section-d83722773e871">29</a></sup>, activation checkpointing and sequential computation of the state. Our optimizations reduce the memory requirements by a factor of four, resulting in less than 1,000 bytes per cell. This enables the prediction on datasets with up to 50 million cells (for example, 5 million rows × 10 features) on a single H100 GPU.</p><p>For regression tasks, we use a piece-wise constant output distribution, following refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e879">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Torgo, L. &amp; Gama, J. Regression using classification algorithms. Intell. Data Anal. 1, 275–292 (1997)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR30" id="ref-link-section-d83722773e882">30</a></sup>, which allows our models to predict a probability distribution of target values instead of a single value, including, for example, bimodal distributions.</p></div></div><div id="Sec4-section" data-title="Synthetic data based on causal models"><h2 id="Sec4">Synthetic data based on causal models</h2><div id="Sec4-content"><p>The performance of TabPFN relies on generating suitable synthetic training datasets that capture the characteristics and challenges of real-world tabular data. To generate such datasets, we developed an approach based on structural causal models (SCMs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Pearl, J. Causality 2nd edn (Cambridge Univ. Press, 2009)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR31" id="ref-link-section-d83722773e894">31</a></sup>. SCMs provide a formal framework for representing causal relationships and generative processes underlying the data. By relying on synthetic data instead of large collections of public tabular data, we avoid common problems of foundational models, such as privacy and copyright infringements, contaminating our training data with test data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Jiang, M. et al. Investigating Data Contamination for Pre-training Language Models. Preprint at 
                  https://arxiv.org/abs/2401.06059
                  
                 (2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR32" id="ref-link-section-d83722773e898">32</a></sup> or limited data availability.</p><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig2">2</a>, our generative pipeline first samples high-level hyperparameters, such as dataset size, number of features and difficulty level, to govern the overall properties of each synthetic dataset. Guided by these hyperparameters, we construct a directed acyclic graph specifying the causal structure underlying the dataset.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Overview of the TabPFN prior."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: Overview of the TabPFN prior.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="318"></picture></a></div><p><b>a</b>, For each dataset, we first sample high-level hyperparameters. <b>b</b>, Based on these hyperparameters, we construct a structural causal model that encodes the computational function generating the dataset. Each node holds a vector and each edge in the computational graph implements a function according to one of the connection types. In step 1, using random noise variables we generate initialization data, which is fed into the root nodes of the graphs and propagated through the computational graph for each to-be-generated sample. In step 2, we randomly sample feature and target node positions in the graph, labelled F and T, respectively. In step 3, we extract the intermediate data representations at the sampled feature and target node positions. In step 4, we post-process the extracted data. <b>c,</b> We retrieve the final datasets. We plot interactions of feature pairs and the node colour represents the class of the sample.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>To generate each sample within a dataset, we propagate randomly generated noise, called our initialization data, through the root nodes of the causal graph. This initialization data are generated by sampling from a random normal or uniform distribution with varying degrees of non-independence between samples, see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec23">Initialization data sampling</a>’. As these data traverse the edges of the computational graph, we apply a diverse set of computational mappings: small neural networks with linear or nonlinear activations (for example, sigmoid, ReLU (rectified linear unit), modulo, sine), discretization mechanisms for generating categorical features and decision tree structures to encode local, rule-based dependencies. At each edge, we add Gaussian noise, introducing uncertainty into the generated data. We save the intermediate data representations at each node to be retrieved later. See section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec22">Computational edge mappings</a>’ for details.</p><p>After traversing the causal graph, we extract the intermediate representations at the sampled feature and target nodes, yielding a sample consisting of feature values and an associated target value.</p><p>By incorporating various data challenges and complexities into the synthetic datasets, we create a training ground that allows TabPFN to develop strategies for handling similar issues in real-world datasets. For instance, consider the case of missing values, commonly present in tabular data. By exposing TabPFN to synthetic datasets with varying patterns and fractions of missing values in our synthetic data generation process, the model learns effective ways of handling missing values that generalize to real-world datasets. We apply post-processing techniques to further enhance the realism and challenge the robustness of the learned prediction algorithms. This includes warping with the Kumaraswamy distribution<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Kumaraswamy, P. A generalized probability density function for double-bounded random processes. J. Hydrol. 46, 79–88 (1980)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR33" id="ref-link-section-d83722773e952">33</a></sup>, introducing complex nonlinear distortions and quantization mimicking discretized features. See section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec24">Post-processing</a>’ for details.</p><p>Through this generative process, we created a massive corpus of around 100 million synthetic datasets per model training, each with a unique causal structure, feature types and functional characteristics.</p></div></div><div id="Sec5-section" data-title="Qualitative analysis"><h2 id="Sec5">Qualitative analysis</h2><div id="Sec5-content"><p>We first analyse the behaviour of TabPFN on toy problems to build intuition and disentangle the impact of various dataset characteristics. As regression problems are easier to visualize, we focus on these in our qualitative analysis. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig3">3a</a>, we compare TabPFN with a diverse set of standard predictors, with all methods using default settings.</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="The behaviour of TabPFN and a set of baselines on simple functions."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: The behaviour of TabPFN and a set of baselines on simple functions.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="319"></picture></a></div><p>In all plots, we use orange for the ground truth and blue for model predictions. <b>a</b>, Each column represents a different toy function, each having a single feature (along the <i>x</i>-axis) and a target (along the <i>y</i>-axis). TabPFN can model a lot of different functions, including noisy functions. <b>b</b>, TabPFN can model distributions over outputs out of the box, which is exemplified by predicting the light intensity pattern in a double-slit experiment after observing the positions of 1,000 photons.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Linear (ridge) regression can naturally model only linear functions, leading to simple and interpretable predictions but catastrophic failure on many of the toy functions. Multilayer perceptrons (MLPs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Rosenblatt, F. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Report No. 1196-0-8 (Cornell Aeronautical Lab, 1961)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR34" id="ref-link-section-d83722773e1008">34</a></sup> perform worse on datasets with highly non-smooth patterns<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e1012">14</a></sup>. This is especially apparent for the step function. TabPFN, by contrast, models either function type, smooth or non-smooth, out of the box. This includes a good approximation to step functions despite TabPFN being a neural network. CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d83722773e1016">9</a></sup>, representative of tree-based methods, fits only piece-wise constant functions. Although this leads to approximation errors and unintuitive predictions, it avoids catastrophic failures.</p><p>The main advantage of TabPFN over all baselines is its inherent ability to model uncertainty at no extra cost. Whereas classical regression methods output a single real-valued prediction, TabPFN returns a target distribution, capturing the uncertainty of predictions. These uncertainty modelling abilities of TabPFN extend beyond simple distributions and can handle complex, multi-modal distributions. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig3">3b</a> shows this by modelling the density of light reaching a detector screen in a double-slit experiment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Young, T. I. The bakerian lecture. experiments and calculations relative to physical optics. Philos. Trans. R. Soc. Lond. 94, 1–16 (1804)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR35" id="ref-link-section-d83722773e1026">35</a></sup> for different slit distances and widths. In this classic experiment, photons are sent through two slits creating a multi-modal intensity pattern because of the wave-like interference behaviour of light. TabPFN predicts these intricate patterns in just a single forward pass, requiring only 1.2 s. By contrast, traditional methods such as CatBoost require training multiple quantile models at different quantiles and reconstructing the distribution from these predictions. Even after tuning CatBoost specifically for this task, it produced substantially worse predictions compared with TabPFN, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig3">3b</a>. With default settings, CatBoost requires 169.3 s and yields further deteriorated results. Qualitatively, we observe that TabPFN is more accurate in predicting very low densities and has fewer artefacts compared with CatBoost.</p></div></div><div id="Sec6-section" data-title="Quantitative analysis"><h2 id="Sec6">Quantitative analysis</h2><div id="Sec6-content"><p>We quantitatively evaluate TabPFN on two dataset collections: the AutoML Benchmark<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gijsbers, P. et al. AMLB: an AutoML benchmark. J. Mach. Learn. Res. 25, 1–65 (2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR36" id="ref-link-section-d83722773e1042">36</a></sup> and OpenML-CTR23<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Fischer, S. F., Feurer, M. &amp; Bischl, B. OpenML-CTR23 – a curated tabular regression benchmarking suite. In Proc. AutoML Conference 2023 (Workshop) (AutoML, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR37" id="ref-link-section-d83722773e1046">37</a></sup>. These benchmarks comprise diverse real-world tabular datasets, curated for complexity, relevance and domain diversity. From these benchmarks, we use&nbsp;the 29 classification datasets and 28 regression datasets that have up to 10,000 samples, 500 features and 10 classes. We further evaluated additional benchmark suites from refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e1050">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e1053">15</a></sup>, as well as five Kaggle competitions from the Tabular Playground Series.</p><p>We compared TabPFN against state-of-the-art baselines, including tree-based methods (random forest<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Breimann, L. Random forests. Mach. Learn. 45, 5–32 (2001)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR38" id="ref-link-section-d83722773e1060">38</a></sup>, XGBoost (XGB)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR7" id="ref-link-section-d83722773e1064">7</a></sup>, CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d83722773e1068">9</a></sup>, LightGBM<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 3149–3157 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR8" id="ref-link-section-d83722773e1072">8</a></sup>), linear models, support vector machines (SVMs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Cortes, C. &amp; Vapnik, V. Support-vector networks. Mach. Learn. 20, 273–297 (1995)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR39" id="ref-link-section-d83722773e1076">39</a></sup> and MLPs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Rosenblatt, F. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Report No. 1196-0-8 (Cornell Aeronautical Lab, 1961)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR34" id="ref-link-section-d83722773e1081">34</a></sup>.</p><p>Evaluation metrics include ROC AUC (area under the receiver operating characteristic curve; One-vs-Rest) and accuracy for classification, and <i>R</i><sup>2</sup> (coefficient of determination) and negative RMSE (root mean squared error) for regression. Scores were normalized per dataset, with 1.0 representing the best and 0.0 the worst performance with respect to all baselines.</p><p>For each dataset and method, we ran 10 repetitions with different random seeds and train–test splits (90% train, 10% test). We tuned hyperparameters using random search with five-fold cross-validation, with time budgets ranging from 30 s to 4 h. All methods were evaluated using eight CPU cores, with TabPFN additionally using a consumer-grade GPU (RTX 2080 Ti;&nbsp;other methods did not benefit from this, see Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig8">2d</a>). TabPFN was pre-trained once using eight NVIDIA RTX 2080 GPUs over 2 weeks, allowing for ICL on all new datasets in a single forward pass. These modest computational requirements make similar research accessible to academic labs. For details, refer to the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec35">Detailed evaluation protocol</a>’.</p><h3 id="Sec7">Comparison with state-of-the-art baselines</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig4">4a</a> demonstrates the strong out-of-the-box performance of TabPFN compared with tuned and default configurations of XGBoost, CatBoost and a random forest. For classification tasks, TabPFN surpasses CatBoost, the strongest default baseline, by 0.187 (0.939 compared with 0.752) in normalized ROC AUC in the default setting and by 0.13 (0.952 compared with 0.822) in the tuned setting. For regression, TabPFN outperforms CatBoost in normalized RMSE by 0.051 (0.923 compared with 0.872) in the default setting and by 0.093 (0.968 compared with 0.875) in the tuned setting. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig4">4b</a>, we show per-dataset comparisons. Although for some datasets CatBoost outperforms TabPFN, TabPFN wins on most of the datasets.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Comparison of TabPFN on our test benchmarks, containing datasets with up to 10,000 samples and 500 features."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Comparison of TabPFN on our test benchmarks, containing datasets with up to 10,000 samples and 500 features.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="304"></picture></a></div><p>Performance was normalized per dataset before aggregation using all baselines; intervals represent the 95% confidence interval. Wilcoxon <i>P</i> refers to the two-sided Wilcoxon signed-rank test <i>P</i> value<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Wilcoxon, F. in Breakthroughs in Statistics: Methodology and Distribution (eds Kotz, S. &amp; Johnson, N. L.) 196–202 (Springer, 1992)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR54" id="ref-link-section-d83722773e1133">54</a></sup>. <b>a</b>, Average performance of the default as well as the tuned versions of TabPFN and our baselines. All methods are tuned for ROC AUC or RMSE, respectively, thus decreasing the representativeness of the secondary metrics. LGBM, LightGBM; MLP, multilayer perceptron; SVM, support vector machines; RF, random forest; CB, CatBoost; XGB, XGBoost; Lin, logistic regression for classification and ridge regression for regression tasks. Plots on the right-hand side show a magnified analysis of the strongest baselines considered. <b>b</b>, A per-dataset comparison of TabPFN with its strongest baseline, CatBoost. Each dot is the average score on one dataset. <b>c</b>, The impact of hyperparameter tuning for the considered methods. The <i>x</i>-axis shows the average time required to fit and predict with the algorithm.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig4">4c</a> shows how the performance of TabPFN and the baselines improve with more time spent on hyperparameter search. The default of TabPFN, taking 2.8 s on average for classification and 4.8 s for regression, outperforms all baselines, even when tuning them for 4 h—a speedup of 5,140× and 3,000×, respectively. We show comparisons on a larger number of metrics in Extended Data Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab1">1</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab2">2</a>.</p><p>As shown in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig8">2</a>, similar to our primary benchmarks, TabPFN substantially outperformed all baselines on the benchmarks of refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e1176">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e1179">15</a></sup>. The benchmark of ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e1183">14</a></sup> is particularly noteworthy because on this benchmark, tree-based methods were previously found to excel. Moreover, we show in Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab6">6</a> that default TabPFN outperforms default CatBoost on all five Kaggle competitions with less than 10,000 training samples from the latest completed Tabular Playground Series.</p><h3 id="Sec8">Evaluating diverse data attributes</h3><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig5">5a,b</a>, we show the robustness of TabPFN to dataset characteristics that are traditionally hard to handle for neural-network-based approaches<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e1201">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hollmann, N., Müller, S., Eggensperger, K. &amp; Hutter, F. TabPFN: a transformer that solves small tabular classification problems in a second. In Proc. The Eleventh International Conference on Learning Representations (ICLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR23" id="ref-link-section-d83722773e1204">23</a></sup>.</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Robustness across datasets and performance comparison with tuned ensembles."><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Fig. 5: Robustness across datasets and performance comparison with tuned ensembles.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="352"></picture></a></div><p><b>a</b>, A comparison of modified datasets. We can see that TabPFN is not more vulnerable to the modifications compared with baselines. We also see that TabPFN reproduces the accuracy of CatBoost (default) with only half the training samples provided. Here we normalize scores per dataset (sharing one normalization across all modifications of one experiment) to avoid negative outliers. <b>b</b>, We split the test datasets by data characteristics and analyse the performance per subgroup. <b>c</b>, Classification performance. Left, the win rate of TabPFN (PHE) against AutoGluon (with one tie excluded); right, the ROC AUC score over time for tuning each method, with the first marker representing the default configuration for the non-ensembling methods. <b>d</b>, Regression performance presented as in <b>c</b> but using the RMSE metric. Intervals represent the 95% confidence interval and Wilcoxon <i>P</i> refers to the two-sided Wilcoxon signed-rank test <i>P</i> value<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Wilcoxon, F. in Breakthroughs in Statistics: Methodology and Distribution (eds Kotz, S. &amp; Johnson, N. L.) 196–202 (Springer, 1992)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR54" id="ref-link-section-d83722773e1241">54</a></sup>.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig5">5a</a> provides an analysis of the performance of TabPFN across various dataset types. First, we add uninformative features (randomly shuffled features from the original dataset) and outliers (multiply each cell with 2% probability with a random number between 0 and the outlier factor). The results show that TabPFN is very robust to uninformative features and outliers, something typically hard for neural networks, as can be seen with the MLP baseline. Second, although dropping either samples or features hurts the performance of all methods, with half the samples TabPFN still performs as well as the next best method using all samples.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig5">5b</a>, we split our test datasets into subgroups and perform analyses per subgroup. We create subgroups based on the presence of categorical features, missing values, number of samples and number of features in the datasets. The sample- and feature-number subgroups are split such that a third of the datasets fall into each group. We can see that none of these characteristics strongly affect the performance of TabPFN relative to the other methods. However, we note that these results should not be taken as evidence that TabPFN scales well beyond the 10,000 samples and 500 features considered here. We show four further ablations in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig7">1</a>.</p><h3 id="Sec9">Comparison with tuned ensemble methods</h3><p>We compare the performance of TabPFN with AutoGluon 1.0 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d83722773e1276">40</a></sup>), which combines various machine learning models, including our baselines, into a stacked ensemble<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Wolpert, D. Stacked generalization. Neural Netw. 5, 241–259 (1992)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR41" id="ref-link-section-d83722773e1280">41</a></sup>, tunes their hyperparameters and then generates the final predictions using post hoc ensembling (PHE)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Caruana, R., Niculescu-Mizil, A., Crew, G. &amp; Ksikes, A. Ensemble selection from libraries of models. In Proc. 21st International Conference on Machine Learning (ed. Greiner, R.) (Omnipress, 2004)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR42" id="ref-link-section-d83722773e1284">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In Proc. International Conference on Automated Machine Learning Vol. 224 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR43" id="ref-link-section-d83722773e1287">43</a></sup>. It thus represents a different class of methods compared with individual baselines.</p><p>To assess whether TabPFN can also be improved by a tuned ensemble approach, we introduce TabPFN (PHE). TabPFN (PHE) automatically combines only TabPFN models with PHE and tunes their hyperparameters using a random portfolio from our search space. We detail this approach in the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec30">TabPFN (PHE)</a>’.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig5">5c–d</a> compares the performance of TabPFN, TabPFN (PHE), AutoGluon and CatBoost. For TabPFN (PHE) and AutoGluon, we start with a minimal budget of 300 s for tuning because AutoGluon otherwise does not reliably return results. In just 2.8 s, TabPFN (default) outperforms AutoGluon for classification tasks, even if AutoGluon is allowed up to 4 h, a 5.140× speedup. TabPFN (PHE) further improves performance leading to an average normalized ROC AUC score of 0.971, compared with 0.939 for TabPFN (default) and 0.914 for AutoGluon. For regression tasks, tuning hyperparameters is more important. Here, TabPFN (PHE) outperforms AutoGluon (allowed 4 h) after its minimal tuning budget of 300 s, a 48× speedup.</p></div></div><div id="Sec10-section" data-title="Foundation model with interpretability"><h2 id="Sec10">Foundation model with interpretability</h2><div id="Sec10-content"><p>Apart from its strong predictive performance, TabPFN exhibits key foundation model abilities, such as data generation, density estimation, learning reusable embeddings and fine-tuning. We showcase these abilities through proof-of-concept experiments on the German Credit Dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Hofmann, H. Statlog (German Credit Data). UCI Machine Learning Repository 
                  https://doi.org/10.24432/C5NC77
                  
                 (1994)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR44" id="ref-link-section-d83722773e1312">44</a></sup>, which contains credit risk information and the mfeat-factors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Duin, R. Multiple Features. UCI Machine Learning Repository 
                  https://doi.org/10.24432/C5HC70
                  
                 (1998)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR45" id="ref-link-section-d83722773e1316">45</a></sup> dataset classifying handwritten digits based on a tabular representation.</p><p>TabPFN can estimate the probability density function of numerical features, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig6">6a</a>, and the probability mass function of categorical features. Computing the sample densities enables anomaly detection to identify issues such as fraud, equipment failures, medical emergencies or low-quality data.</p><div data-test="figure" data-container-section="figure" id="figure-6" data-title="Showcase of the application of TabPFN as tabular foundation model."><figure><figcaption><b id="Fig6" data-test="figure-caption-text">Fig. 6: Showcase of the application of TabPFN as tabular foundation model.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig6_HTML.png?as=webp"><img aria-describedby="Fig6" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="180"></picture></a></div><p><b>a</b>,<b>b</b>, On the German Credit Dataset, we perform data density estimation (<b>a</b>) and generation of new synthetic samples (<b>b</b>). <b>c</b>, We show our learned embeddings are useful representations of each sample on the handwritten digits dataset (mfeat-factors) with different classes forming different clusters. <b>d</b>, We demonstrate fine-tuning TabPFN for a specific set of tasks. Fine-tuned on a dataset containing various sine curves (top), we see the model makes more accurate predictions on another sine curve dataset.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-08328-6/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>TabPFN also allows synthesizing new tabular data samples that mimic real-world dataset characteristics as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig6">6b</a>. This enables applications such as data augmentation or privacy-preserving data sharing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Rajotte, J.-F. et al. Synthetic data as an enabler for machine learning applications in medicine. iScience 25, 105331 (2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR46" id="ref-link-section-d83722773e1369">46</a></sup>.</p><p>The architecture of TabPFN yields meaningful feature representations that can be reused for downstream tasks such as data imputation and clustering. We extract and visualize learned embeddings from the mfeat-factors dataset in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig6">6c</a>, showing improved class separation compared with the raw data on the first two principal components.</p><p>Furthermore, we demonstrate the ability of TabPFN to improve performance through fine-tuning on related datasets. Unlike tree-based methods, the neural architecture of TabPFN enables fine-tuning on specific dataset classes. We conduct proof-of-concept experiments using sine curve datasets with varying offsets between fine-tuning and test data. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig6">6d</a> shows an example fine-tuning result. Our analysis across 50 runs (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig10">4</a>) shows that TabPFN successfully transfers knowledge even when labels differ significantly between fine-tuning and test tasks, with performance improving as distributions become more similar. This could, for example, enable fine-tuning for a range of datasets from medical studies to obtain an improved general model for medical diagnosis tasks. For details, refer to section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec31">Foundation model abilities</a>’.</p><p>Finally, we have developed a methodology to easily interpret the predictions of TabPFN. Interpretability is crucial for building trust and accountability when deploying models in high-stakes domains. We support the computation of feature importance through SHAP<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Lundberg, S. M. &amp; Lee, S.-I. A unified approach to interpreting model predictions. In Proc. Advances in Neural Information Processing Systems (eds Guyon, I. et al.) Vol. 30, 4765–4774 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR47" id="ref-link-section-d83722773e1395">47</a></sup> (Shapley Additive Explanations), a game-theoretic approach to explain predictions. SHAP values represent the contribution of each feature to the output of the model. Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig9">3</a> compares the feature importance and impact for logistic regression, CatBoost and TabPFN. TabPFN achieves high accuracy while learning simple, interpretable feature relationships. By contrast, logistic regression is interpretable but less accurate, whereas CatBoost is accurate but qualitatively less interpretable because of complex, non-smooth decision boundaries.</p></div></div><div id="Sec11-section" data-title="Conclusion"><h2 id="Sec11">Conclusion</h2><div id="Sec11-content"><p>TabPFN represents a major change in tabular data modelling, leveraging ICL to autonomously discover a highly efficient algorithm that outperforms traditional human-designed approaches on datasets with up to 10,000 samples and 500 features. This shift towards foundation models trained on synthetic data opens up new possibilities for tabular data analysis across various domains.</p><p>Potential future directions include scaling to larger datasets<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Feuer, B. et al. TuneTables: context optimization for scalable prior-data fitted networks. In Proc. 38th Conference on Neural Information Processing Systems (NeurIPS, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR48" id="ref-link-section-d83722773e1413">48</a></sup>, handling data drift<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Helli, K., Schnurr, D., Hollmann, N., Müller, S. &amp; Hutter, F. Drift-resilient tabPFN: In-context learning temporal distribution shifts on tabular data. In Proc. 38th Conference on Neural Information Processing Systems (NeurIPS, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR49" id="ref-link-section-d83722773e1417">49</a></sup>, investigating fine-tuning abilities across related tabular tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Thomas, V. et al. Retrieval &amp; fine-tuning for in-context tabular models. In Proc. 1st Workshop on In-Context Learning at the 41st International Conference on Machine Learning (ICML, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR50" id="ref-link-section-d83722773e1421">50</a></sup> and understanding the theoretical foundations of our approach<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Nagler, T. Statistical foundations of prior-data fitted networks. In Proc. 40th International Conference on Machine Learning (eds Krause, A. et al.) Vol. 202, 25660–25676 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR51" id="ref-link-section-d83722773e1425">51</a></sup>. Future work could also explore creating specialized priors to handle data types such as time series<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Dooley, S., Khurana, G. S., Mohapatra, C., Naidu, S. V. &amp; White, C. ForecastPFN: synthetically-trained zero-shot forecasting. In Proc. 37th Conference on Advances in Neural Information Processing Systems (eds Oh, A. et al.) (NeurIPS, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR52" id="ref-link-section-d83722773e1429">52</a></sup> and multi-modal data, or specialized modalities such as ECG, neuroimaging data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Czolbe, S. &amp; Dalca, A. V. Neuralizer: General neuroimage analysis without re-training. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 6217–6230 (IEEE, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR53" id="ref-link-section-d83722773e1434">53</a></sup> and genetic data. As the field of tabular data modelling continues to evolve, we believe that foundation models, such as TabPFN, will play a key part in empowering researchers. To facilitate the widespread use of TabPFN, in the section ‘User guide’ we discuss how to use it effectively.</p></div></div><div id="Sec12-section" data-title="Methods"><h2 id="Sec12">Methods</h2><div id="Sec12-content"><h3 id="Sec13">User guide</h3><h4 id="Sec14">When to use TabPFN</h4><p>TabPFN excels in handling small- to medium-sized datasets with up to 10,000 samples and 500 features (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig4">4</a> and Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab1">1</a>). For larger datasets and highly non-smooth regression datasets, approaches such as CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d83722773e1460">9</a></sup>, XGB<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR7" id="ref-link-section-d83722773e1464">7</a></sup> or AutoGluon<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d83722773e1468">40</a></sup> are likely to outperform TabPFN.</p><p>Although TabPFN provides a powerful drop-in replacement for traditional tabular data models such as CatBoost, similar to these models, it is intended to be only one component in the toolkit of a data scientist. Achieving top performance on real-world problems often requires domain expertise and the ingenuity of data scientists. As for other modelling approaches, data scientists should continue to apply their skills and insights in feature engineering, data cleaning and problem framing to get the most out of TabPFN. We hope that the training speed of TabPFN will facilitate faster iterations in the data science workflow.</p><h4 id="Sec15">Limitations of TabPFN</h4><p>The limitations of TabPFN are as follows: (1) the inference speed of TabPFN may be slower than highly optimized approaches such as CatBoost; (2) the memory usage of TabPFN scales linearly with dataset size, which can be prohibitive for very large datasets; and (3) our evaluation focused on datasets with up to 10,000 samples and 500 features; scalability to larger datasets requires further study.</p><h4 id="Sec16">Computational and time requirements</h4><p>TabPFN is computationally efficient and can run on consumer hardware for most datasets. However, training on a new dataset is recommended to run on a (consumer) GPU as this speeds it up by one to three orders of magnitude. Although TabPFN is very fast to train, it is not optimized for real-time inference tasks. For a dataset with 10,000 rows and 10 columns, our model requires 0.2 s (0.6 s without GPU) to perform a prediction for one sample, whereas CatBoost (default) can do the same in 0.0002 s. In ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Müller, A., Curino, C. &amp; Ramakrishnan, R. Mothernet: a foundational hypernetwork for tabular classification. Preprint at 
                  https://arxiv.org/abs/2312.08598
                  
                 (2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR55" id="ref-link-section-d83722773e1491">55</a></sup>, further optimizing TabPFN specifically for inference tasks has already been explored, resulting in four times faster inference performance compared with even XGBoost, but so far also reducing predictive quality. Refer to the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec19">Details on the neural architecture</a>’ for details on the memory usage and runtime complexity of TabPFN.</p><h4 id="Sec17">Data preparation</h4><p>TabPFN can handle raw data with minimal pre-processing. If we simply provide the data in a tabular format (NumPy matrix), TabPFN will automatically handle missing values, encode categorical variables and normalize features. Although TabPFN works well out of the box, we can further improve the performance using dataset-specific pre-processing. This can also be partly done automatically with our PHE technique or manually by modifying the default settings. When manually pre-processing data, we should keep in mind that the neural network of TabPFN expects roughly normally distributed features and targets after all pre-processing steps. If we, for example, know that a feature follows a log distribution, it might help to exponentiate it before feeding it to TabPFN. As TabPFN does <i>z</i>-normalization of all inputs, scaling does not affect the predictions. As for all algorithms, however, using domain knowledge to combine or remove features can increase performance.</p><h4 id="Sec18">Hyperparameter tuning</h4><p>TabPFN provides strong performance out of the box without extensive hyperparameter tuning (see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec7">Comparison with state-of-the-art baselines</a>’). If we have additional computational resources, we can further optimize the performance of TabPFN using hyperparameter optimization (HPO) or the PHE technique described in the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec30">TabPFN (PHE)</a>’. Our implementation directly provides HPO with random search and PHE.</p><h3 id="Sec19">Details on the neural architecture</h3><p>Our architecture is a variation of the original transformer encoder<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Vaswani, A. et al. Attention is all you need. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR12" id="ref-link-section-d83722773e1533">12</a></sup> and the original PFN architecture<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e1537">22</a></sup>, but it treats each cell in the table as a separate time position, similar to that in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Lorch, L., Sussex, S., Rothfuss, J., Krause, A. &amp; Schölkopf, B. Amortized inference for causal structure learning. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) Vol. 35, 13104–13118 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR28" id="ref-link-section-d83722773e1541">28</a></sup>. Therefore, it can generalize to more training samples as well as features than seen during training.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig1">1b</a> details our new architecture. All features that go into our architecture are first mapped to floating point values, that is, categoricals are transformed to integers. These values are subjected to <i>z</i>-normalization using the mean and standard deviation for each feature separately across the whole training set. These values are now encoded with simple linear encoders. Each layer first has an attention over features, followed by an attention over samples, both of which operate separately on each column or row, respectively. These two sub-layers are followed by an MLP sublayer. Each sublayer is followed by a residual addition and a half-precision layer norm.</p><p>We found that encoding groups of features can be even more effective compared with encoding one value per representation. For our hyperparameter search space, we selected six architectures for classification and five for regression. In three of the six classification models and four of the five regression models, including the TabPFN default, a transformer position encodes two features of one example; in others, it represents one value.</p><p>Although the inter-feature attention is a classical fully connected attention, our inter-sample attention does not allow the test samples to attend to each other but only to the training data. Therefore, we make sure that the test samples do not influence each other or the training set representations. To allow our model to differentiate features more easily that have the same statistics, for example, two features that have the same entries just in different orders, we use random feature embeddings that we add to all embeddings before the first layer. We generate one embedding per feature by projecting a random vector of one-fourth the size of our embeddings through a learned linear layer and add this to all embeddings representing an instance of that feature.</p><p>As the representations of training samples are not influenced by the test set, we cache the keys and values of the training samples to allow splitting training and inference. We use a special variant of multi-query attention for our inter-sample attention from test samples<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Shazeer, N. Fast transformer decoding: one write-head is all you need. Preprint at 
                  https://arxiv.org/abs/1911.02150
                  
                 (2019)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR56" id="ref-link-section-d83722773e1564">56</a></sup> to save memory when caching representations. In our variant, we use all keys and values for the attention between samples of the training set, but repeatedly use the first key and value for attention from the test samples. This allows caching only one key or value vector pair per cell in the training set that is fed into our inter-sample attention of new test samples.</p><p>The compute requirements of this architecture scale quadratically with the number of samples (<i>n</i>) and the number of features (<i>m</i>), that is <i>O</i>(<i>n</i><sup>2</sup> + <i>m</i><sup>2</sup>), and the memory requirements scale linearly in the dataset size, <i>O</i>(<i>n</i> <span>⋅</span> <i>m</i>).</p><p>Finally, we found that pre-processing inputs can help performance, thus we can perform <i>z</i>-normalization of all inputs across the sample dimension and add an extra input for each cell that indicates whether the input was missing; the input itself is set to 0 in these cases. All inputs are finally linearly encoded into the embedding dimension of TabPFN.</p><h3 id="Sec20">Details on the causal generative process</h3><p>An SCM <span>\({\mathcal{G}}:= (Z,{\epsilon })\)</span> consists of a collection <i>Z</i> <span>≔</span> (<i>z</i><sub>1</sub>, …, <i>z</i><sub><i>k</i></sub>) of structural assignments (called mechanisms): <span>\({z}_{i}={f}_{i}({z}_{{\rm{PA}}{\mathcal{G}}(i)},{{\epsilon }}_{i})\,,\)</span> where <span>\({\rm{PA}}\,{\mathcal{G}}(i)\)</span> is the set of parents of node <i>i</i> (its direct causes) in the underlying directed acyclic graph (DAG) <span>\({\mathcal{G}}\)</span> (the causal graph), <i>f</i><sub><i>i</i></sub> is a (potentially nonlinear) deterministic function and <i>ϵ</i><sub><i>i</i></sub> is a noise variable. Causal relationships in <span>\({\mathcal{G}}\)</span> are represented by edges pointing from causes to effects<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Pearl, J. Causality 2nd edn (Cambridge Univ. Press, 2009)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR31" id="ref-link-section-d83722773e1864">31</a></sup>. As our prior is a sampling procedure, we can make a lot of choices on, for example, the graph size or complexity. By defining a probability distribution over these hyperparameters in the prior, the posterior predictive distribution approximated by TabPFN at inference time implicitly represents a Bayesian ensemble, jointly integrating over a weighted hyperparameter space. The specific hyperparameter ranges and sampling strategies are chosen to cover a diverse set of scenarios that we expect to encounter in real-world tabular data.</p><h4 id="Sec21">Graph structure sampling</h4><p>The structural causal models underlying each dataset are based on a DAG <span>\({\mathcal{G}}\)</span>. We sample these graphs using the growing network with redirection sampling method<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Krapivsky, P. L. &amp; Redner, S. Organization of growing random networks. Phys. Rev. E 63, 066123 (2001)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR57" id="ref-link-section-d83722773e1893">57</a></sup>, a preferential attachment process that generates random scale-free networks. We either sample a single connected component or merge multiple disjoint subgraphs. Disjoint subgraphs lead to features that are marginally independent of the target if they are not connected to the target node, reflecting real-world scenarios with uninformative predictors.</p><p>To control the complexity of the sampled DAGs, we use two hyperparameters: the number of nodes <i>N</i> and the redirection probability <i>P</i>. <i>N</i> is sampled from a log-uniform distribution, <span>\(\log N \sim {\mathcal{U}}(a,b)\)</span>, where <i>a</i> and <i>b</i> are hyperparameters controlling the range of the graph size. The redirection probability <i>P</i> is sampled from a gamma distribution, <i>P</i> ~ <i>Γ</i>(<i>α</i>, <i>β</i>), where <i>α</i> and <i>β</i> are shape and rate parameters, respectively. Larger values of <i>N</i> yield graphs with more nodes, whereas smaller values of <i>P</i> lead to denser graphs with more edges on average<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Krapivsky, P. L. &amp; Redner, S. Organization of growing random networks. Phys. Rev. E 63, 066123 (2001)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR57" id="ref-link-section-d83722773e1987">57</a></sup>.</p><h4 id="Sec22">Computational edge mappings</h4><p>In our implementation, each SCM node and sample is represented as a vector in <span>\({{\mathbb{R}}}^{d}\)</span>. When propagating data through the SCM, the deterministic functions <i>f</i><sub><i>i</i></sub> at each edge map the input vectors to an output vector using four types of computational modules:</p><ol>
                    <li>
                      <span>1.</span>
                      
                        <p>Small neural networks: here we initialize weight matrices <span>\(W\in {{\mathbb{R}}}^{d\times d}\)</span> using Xavier initialization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Glorot, X. &amp; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. 13th International Conference on Artificial Intelligence and Statistics 249–256 (JMLR, 2010)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR58" id="ref-link-section-d83722773e2084">58</a></sup> and apply a linear transformation <i>W</i><i>x</i> + <i>b</i> to the input vectors <span>\(x\in {{\mathbb{R}}}^{d}\)</span>, where <span>\(b\in {{\mathbb{R}}}^{d}\)</span> is a bias vector. After the linear projection, we apply element-wise nonlinear activation functions <span>\(\sigma :{{\mathbb{R}}}^{d}\to {{\mathbb{R}}}^{d}\)</span>, randomly sampled from a set, including identity, logarithm, sigmoid, absolute value, sine, hyperbolic tangent, rank operation, squaring, power functions, smooth ReLU<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Nair, V. &amp; Hinton, G. Rectified linear units improve restricted Boltzmann machines. In Proc. 27th International Conference on Machine Learning (eds Fürnkranz, J. &amp; Joachims, T.) 807–814 (Omnipress, 2010)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR59" id="ref-link-section-d83722773e2221">59</a></sup>, step function and modulo operation.</p>
                      
                    </li>
                    <li>
                      <span>2.</span>
                      
                        <p>Categorical feature discretization: to generate categorical features from the numerical vectors at each node, we map the vector to the index of the nearest neighbour in a set of per node randomly sampled vectors {<i>p</i><sub>1</sub>, …, <i>p</i><sub><i>K</i></sub>} for a feature with <i>K</i> categories. This discrete index will be observed in the feature set as a categorical feature. We sample the number of categories <i>K</i> from a rounded gamma distribution with an offset of 2 to yield a minimum number of classes of 2. To further use these discrete class assignments in the computational graph, they need to be embedded as continuous values. We sample a second set of embedding vectors <span>\(\{{p}_{1}^{{\prime} },\ldots ,{p}_{K}^{{\prime} }\}\)</span> for each class and transform the classes to these embeddings.</p>
                      
                    </li>
                    <li>
                      <span>3.</span>
                      
                        <p>Decision trees: to incorporate structured, rule-based dependencies, we implement decision trees in the SCMs. At certain edges, we select a subset of features and apply decision boundaries on their values to determine the output<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Quinlan, J. R. Induction of decision trees. Mach. Learn. 1, 81–106 (1986)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR60" id="ref-link-section-d83722773e2331">60</a></sup>. The decision tree parameters (feature splits, thresholds) are randomly sampled per edge.</p>
                      
                    </li>
                    <li>
                      <span>4.</span>
                      
                        <p>Noise injection: at each edge, we add random normal noise from the normal distribution <span>\({\mathcal{N}}(0,{\sigma }^{2}I)\)</span>.</p>
                      
                    </li>
                  </ol><h4 id="Sec23">Initialization data sampling</h4><p>For each to-be-generated sample, we randomly generate initialization data <i>ϵ</i> that is inserted at the DAG root nodes and then propagated through the computational graph. The noise variables <i>ϵ</i> are generated according to one of three sampling mechanisms:</p><ol>
                    <li>
                      <span>1.</span>
                      
                        <p>Normal: <span>\({\epsilon } \sim {\mathcal{N}}(0,{\sigma }_{{\epsilon }}^{2})\)</span>, where <span>\({\sigma }_{{\epsilon }}^{2}\)</span> is a hyperparameter.</p>
                      
                    </li>
                    <li>
                      <span>2.</span>
                      
                        <p>Uniform: <span>\({\epsilon } \sim {\mathcal{U}}(-a,a)\)</span>, where <i>a</i> is a hyperparameter.</p>
                      
                    </li>
                    <li>
                      <span>3.</span>
                      
                        <p>Mixed: for each root node, we randomly select either a normal or uniform distribution to sample the initialization noise <i>ϵ</i> from.</p>
                      
                    </li>
                  </ol><p>Furthermore, we sample input data with varying degrees of non-independence for some datasets. Here we first sample a random fraction <i>ρ</i> of samples to serve as prototypes <span>\({x}_{1}^{* },\ldots ,{x}_{M}^{* }\)</span>, where <i>M</i> = <i>ρ</i><i>n</i> and <i>n</i> is the dataset size. Then, for each input vector <i>x</i><sub><i>i</i></sub> to be sampled, we assign weights <i>α</i><sub><i>i</i><i>j</i></sub> to the prototypes and linearly mix the final input as</p><div id="Equ1"><p><span>$${x}_{i}=\mathop{\sum }\limits_{j=1}^{M}{\alpha }_{ij}{x}_{j}^{* },$$</span></p><p>
                    (1)
                </p></div><p>where ∑<sub><i>j</i></sub><i>α</i><sub><i>i</i><i>j</i></sub> = 1. The weights <i>α</i><sub><i>i</i><i>j</i></sub> are sampled from a multinomial distribution, <i>α</i><sub><i>i</i></sub> ~ Multinomial(<i>β</i>), where <i>β</i> is a temperature hyperparameter controlling the degree of non-independence: larger <i>β</i> yields more uniform weights, whereas smaller <i>β</i> concentrates the weights on fewer prototypes per sample.</p><h4 id="Sec24">Post-processing</h4><p>Each dataset is post-processed randomly with one or more of the following post-processings: (1) For some datasets, we use the Kumaraswamy feature warping, introducing nonlinear distortions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Kumaraswamy, P. A generalized probability density function for double-bounded random processes. J. Hydrol. 46, 79–88 (1980)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR33" id="ref-link-section-d83722773e2824">33</a></sup> to features as done in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Müller, S., Feurer, M., Hollmann, N. &amp; Hutter, F. PFNS4BO: in-context learning for Bayesian optimization. In Proc. 40th International Conference on Machine Learning 25444–25470 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR61" id="ref-link-section-d83722773e2828">61</a></sup>. (2) We quantize some continuous features into buckets of randomly sampled cardinality <i>K</i>, mimicking binned or discretized features commonly encountered in datasets. We map a feature value <i>x</i> to the index of the bucket it falls into, determined by <i>K</i> + 1 bin edges sampled from the set of values this feature takes. (3) To introduce scenarios for dynamic imputation and handling of incomplete datasets, a common challenge in data science, we randomly designate a fraction <i>ρ</i><sub>miss</sub> of the data as missing according to the missing completely at random strategy. Each value is masked as missing with probability <i>ρ</i><sub>miss</sub>, independently of the data values.</p><h4 id="Sec25">Target generation</h4><p>To generate target labels for regression tasks, we select a randomly chosen continuous feature without post-processing. For classification labels, we select a random categorical feature that contains up to 10 classes. Thus, natively our method is limited to predicting at most 10 classes. This number can be increased by pre-training on datasets with a larger number of classes or by using approaches such as building a one-vs-one classifier, one-vs-rest classifier or building on approaches such as error-correcting output codes (ECOC)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Dietterich, T. G. &amp; Bakiri, G. Solving multiclass learning problems via error-correcting output codes. J. Artif. Intell. Res. 2, 263–286 (1994)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR62" id="ref-link-section-d83722773e2858">62</a></sup>.</p><h3 id="Sec26">Training details</h3><p>The training loss of any PFN is the cross-entropy between the targets of held-out samples of synthetic datasets and the model prediction. For a test set (<b>X</b><sub>test</sub>, <b><i>y</i></b><sub>test</sub>) = <i>D</i><sub>test</sub>, the training loss is given by <span>\({{\mathcal{L}}}_{{\rm{P}}{\rm{F}}{\rm{N}}}={{\bf{E}}}_{(({{\boldsymbol{X}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}},{{\boldsymbol{y}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}})\cup {D}_{{\rm{t}}{\rm{r}}{\rm{a}}{\rm{i}}{\rm{n}}})\sim p(D)}[-\log {q}_{\theta }({{\boldsymbol{y}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}}|{{\boldsymbol{X}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}},{D}_{{\rm{t}}{\rm{r}}{\rm{a}}{\rm{i}}{\rm{n}}})]\)</span>. By minimizing this loss, the PFN learns to approximate the true Bayesian posterior predictive distribution for a chosen prior over datasets (and potentially their latent variables) <i>D</i>, as shown in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e3210">22</a></sup>.</p><p>We trained our final models for approximately 2,000,000 steps with a batch size of 64 datasets. That means the models used for TabPFN are trained on around 130,000,000 synthetically generated datasets each. One training run requires around 2 weeks on one node with eight Nvidia RTX 2080 Ti GPUs. We sample the number of training samples for each dataset uniformly up to 2,048 and use a fixed validation set size of 128. We sample the number of features using a beta distribution (<i>k</i> = 0.95,&nbsp;<i>b</i> = 8.0) that we linearly scale to the range 1–160. To avoid peaks in memory usage, the total size of each table was restricted to be below 75,000 cells by decreasing the number of samples for large numbers of features.</p><p>We chose the hyperparameters for the prior based on random searches, in which we use only a single GPU per training and evaluate on our development set, see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec6">Quantitative analysis</a>’. We used the Adam optimizer<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Kingma, D. &amp; Ba, J. Adam: A method for stochastic optimization. In Proc. International Conference on Learning Representations (ICLR, 2015)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR24" id="ref-link-section-d83722773e3229">24</a></sup> with linear warmup and cosine annealing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Loshchilov, I. &amp; Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In Proc. 5th International Conference on Learning Representations (ICLR, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR63" id="ref-link-section-d83722773e3233">63</a></sup> and tested a set of learning rates in [0.0001, 0.0005], using the one with the lowest final training loss.</p><h3 id="Sec27">Inference details</h3><p>To get the most performance out of TabPFN, it is crucial to optimize its inference pipeline. We generally always apply TabPFN in a small ensemble, in which we perform pre-processing or post-processing of the data differently for each ensemble member.</p><p>As our models are not fully permutation invariant, for each ensemble member, we shuffle the feature order, approximating order invariance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Murphy, R. L., Srinivasan, B., Rao, V. A. &amp; Ribeiro, B. Janossy pooling: learning deep permutation-invariant functions for variable-size inputs. In Proc. 7th International Conference on Learning Representations (ICLR, 2019)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR64" id="ref-link-section-d83722773e3249">64</a></sup>. For classification tasks, we additionally randomly permute the labels. We also apply a temperature to the softmax distribution of our model outputs for calibration.</p><p>Apart from the above, we use a subset of the following for each of our default ensemble members:</p><ol>
                  <li>
                    <span>1.</span>
                    
                      <p>Quantile + Id: we quantize the inputs to equally spaced values between 0 and 1, but keep a copy of each original feature. This effectively doubles the number of features passed to TabPFN.</p>
                    
                  </li>
                  <li>
                    <span>2.</span>
                    
                      <p>Category shuffling: the labels of categorical features with low cardinality are shuffled.</p>
                    
                  </li>
                  <li>
                    <span>3.</span>
                    
                      <p>SVD: an SVD compression of the features is appended to the features.</p>
                    
                  </li>
                  <li>
                    <span>4.</span>
                    
                      <p>Outlier removal: all outliers, more than 12 standard deviations from the mean, are removed.</p>
                    
                  </li>
                  <li>
                    <span>5.</span>
                    
                      <p>Power transform: each feature (or the label for regression) is transformed using a Yeo–Johnson transformation to stabilize the variance and make the data more normally distributed.</p>
                    
                  </li>
                  <li>
                    <span>6.</span>
                    
                      <p>One-hot encoding: categorical features are encoded using one-hot encoding, in which each category is represented as a binary vector.</p>
                    
                  </li>
                </ol><p>For PHE and hyperparameter tuning of TabPFN, we use a larger set of pre-processing techniques that additionally include a logarithmic, an exponential and a KDI transformation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="McCarter, C. The kernel density integral transformation. Transact. Mach. Learn. Res. 
                  https://openreview.net/pdf?id=6OEcDKZj5j
                  
                 (2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR65" id="ref-link-section-d83722773e3328">65</a></sup>. These transformations help address nonlinear relationships, skewed distributions and varying scales among features.</p><p>To calibrate prediction uncertainty, we apply a softmax temperature (default <i>T</i> = 0.9) by dividing logits before the softmax calculation:</p><div id="Equ2"><p><span>$$P({y}_{i}| x)=\frac{\exp ({z}_{i}/T)}{{\sum }_{j}\exp ({z}_{j}/T)},$$</span></p><p>
                    (2)
                </p></div><p>where <i>z</i><sub><i>i</i></sub> are the logits, <i>T</i> is the temperature and <i>P</i>(<i>y</i><sub><i>i</i></sub><span>∣</span><i>x</i>) is the calibrated probability. We offer the option to generate second-order polynomial features by multiplying up to 50 randomly selected feature pairs:</p><div id="Equ3"><p><span>$${f}_{ij}={x}_{i}\cdot {x}_{j},\quad \,{\rm{for}}\,(i,j)\in {\mathcal{S}},$$</span></p><p>
                    (3)
                </p></div><p>where <span>\({\mathcal{S}}\)</span> is the set of randomly chosen feature pairs. This can capture nonlinear interactions between features. This option is disabled by default. To ensure proper handling of duplicate samples given the sample permutation invariance of our architecture, we add a unique sample identifier feature. This is a random number drawn from a standard normal distribution, ensuring each sample is treated distinctly in the attention mechanism. We also provide an option for subsampling in each estimator, to increase ensemble diversity, which performs random sampling without replacement. This option is disabled by default.</p><h4 id="Sec28">Regression details</h4><p>To enable our model to do classification on a large range of scales and target distributions, we use the following approach. During pre-training, we rescale our regression targets to have zero mean and a standard deviation of 1 (<i>z</i>-score). To decide where the borders between our features lie, we draw a large sample of datasets from our prior and choose the 1/5,000 quantiles from this distribution. At inference time, we bring the real-world data to a similar range by again applying <i>z</i>-score normalization. Furthermore, we allow applying a range of transforms, including a power transform as part of our default. All of the transforms, including the <i>z</i>-score are inverted at prediction time by applying the inverse of the transform to the borders between buckets. This is equivalent to applying the inverse of the transform to the random variable represented by our output distribution but for the half-normals used on the sides for full support<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d83722773e3630">22</a></sup>. This is because all transforms are strictly monotone and the borders represent positions on the cumulative distribution function.</p><h4 id="Sec29">Data grouping based on random forest</h4><p>To perform well on very heterogeneous datasets, we also propose to use random trees to split the training data into smaller more homogeneous datasets. This technique is used only when performing HPO or PHE for TabPFN. It is especially useful for TabPFN as our model performs best on small datasets.</p><p>The pre-processing for a single ensemble member, that is, a single tree, works as follows: we use a standard random tree with feature and sample bootstrapping and Gini impurity loss. For each leaf node of the decision tree, we store the subset of training samples that fall into that node and train a TabPFN on these. To predict the class label for a test sample <i>x</i>, we determine the TabPFN to use by passing <i>x</i> through the decision tree. We set the minimal leaf size to be large (500–2,000) such that the resulting data groups are large enough to train a strong model.</p><h3 id="Sec30">TabPFN (PHE)</h3><p>To further enhance the inference performance of TabPFN, in TabPFN (PHE), we use PHE for a fixed portfolio of TabPFN configurations from our search space detailed in Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab5">5</a>. For TabPFN (PHE), we first use holdout validation to sequentially evaluate models from the portfolio until a time limit is reached. After all models are evaluated once, we repeat holdout validation with new data splits until the time limit is reached. Then, we ensemble all evaluated TabPFN models by aggregating their predictions with a weighted arithmetic mean. We learn the weights using greedy ensemble selection (GES)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Caruana, R., Niculescu-Mizil, A., Crew, G. &amp; Ksikes, A. Ensemble selection from libraries of models. In Proc. 21st International Conference on Machine Learning (ed. Greiner, R.) (Omnipress, 2004)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR42" id="ref-link-section-d83722773e3663">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Caruana, R., Munson, A. &amp; Niculescu-Mizil, A. Getting the most out of ensemble selection. In Proc. 6th IEEE International Conference on Data Mining (eds Clifton, C. et al.) 828–833 (IEEE, 2006)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR66" id="ref-link-section-d83722773e3666">66</a></sup> with 25 iterations on prediction data from holdout validation. Finally, we prune each zero-weighted model, refit all remaining models on all data and return the weighted average of their predictions.</p><p>Following standard practice in AutoML, we use GES because its predictive performance is often superior to the best individual model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In Proc. International Conference on Automated Machine Learning Vol. 224 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR43" id="ref-link-section-d83722773e3673">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Feurer, M. et al. in Automated Machine Learning: Methods, Systems, Challenges (eds Hutter, F. et al.) Ch. 6 (Springer, 2019)." href="#ref-CR67" id="ref-link-section-d83722773e3676">67</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Purucker, L. &amp; Beel, J. Assembled-OpenML: creating efficient benchmarks for ensembles in AutoML with OpenML. In Proc. First International Conference on Automated Machine Learning (AutoML, 2022)." href="#ref-CR68" id="ref-link-section-d83722773e3676_1">68</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Purucker, L. &amp; Beel, J. CMA-ES for post hoc ensembling in AutoML: a great success and salvageable failure. In Proc. International Conference on Automated Machine Learning Vol. 224, 1–23 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR69" id="ref-link-section-d83722773e3679">69</a></sup>. Owing to its ICL, we expect TabPFN to overfit the training data less than predictions of traditionally trained algorithms; thus, we opt for (repeated) holdout validation (as in Auto-Sklearn 1; ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Feurer, M. et al. in Automated Machine Learning: Methods, Systems, Challenges (eds Hutter, F. et al.) Ch. 6 (Springer, 2019)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR67" id="ref-link-section-d83722773e3683">67</a></sup>) instead of (repeated) cross-validation (as in AutoGluon<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d83722773e3687">40</a></sup>). Moreover, as GES usually produces sparse weight vectors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In Proc. International Conference on Automated Machine Learning Vol. 224 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR43" id="ref-link-section-d83722773e3691">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Purucker, L. &amp; Beel, J. CMA-ES for post hoc ensembling in AutoML: a great success and salvageable failure. In Proc. International Conference on Automated Machine Learning Vol. 224, 1–23 (PMLR, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR69" id="ref-link-section-d83722773e3694">69</a></sup>, we expect the final ensemble after pruning each zero-weighted model to consist of a smaller number of models than for other ensembling approaches, such as bagging. Consequently, PHE can also improve the inference efficiency of a TabPFN ensemble compared with other ensembling approaches.</p><h3 id="Sec31">Foundation model abilities</h3><h4 id="Sec32">Density estimation</h4><p>The combination of a regression and a classification TabPFN can be used as a generative model for tabular data, not only modelling targets but features as well. Let <span>\({\mathcal{D}}={\{({{\bf{x}}}_{i},{y}_{i})\}}_{i=1}^{N}\)</span> denote the original dataset, where <span>\({{\bf{x}}}_{i}\in {{\mathbb{R}}}^{d}\)</span> is a <i>d</i>-dimensional feature vector and <i>y</i><sub><i>i</i></sub> is the corresponding target value, and let <i>q</i><sub><i>θ</i></sub> represent our trained TabPFN model, either a regression or classification model depending on the target type. We aim to approximate the joint distribution of a new example and its label <span>\(p({\bf{x}},y| {\mathcal{D}})\)</span>. To do this, we factorize the joint distribution as</p><div id="Equ4"><p><span>$$p({\bf{x}},y| {\mathcal{D}})=\mathop{\prod }\limits_{j=1}^{d}p({x}_{j}| {{\bf{x}}}_{ &lt; j},{\mathcal{D}})\cdot p(\,y| {\bf{x}},{\mathcal{D}})$$</span></p><p>
                    (4)
                </p></div><div id="Equ5"><p><span>$$\approx \mathop{\prod }\limits_{j=1}^{d}{q}_{\theta }({x}_{j}| {{\boldsymbol{x}}}_{ &lt; j},{{\mathcal{D}}}_{:, &lt; j})\cdot {q}_{\theta }(\,y| {\boldsymbol{x}},{\mathcal{D}}),$$</span></p><p>
                    (5)
                </p></div><p>where we only condition on a subset of the features in the training set (<span>\({{\mathcal{D}}}_{:, &lt; j}\)</span>). The feature order of the joint density factorization influences the estimated densities. To reduce variance from this source, we apply a permutation sampling approximation of Janossy Pooling at inference time, in which we average the outputs of <i>N</i><sub><i>j</i></sub> feature permutations, with <i>N</i><sub><i>j</i></sub> = 24 in our experiments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Murphy, R. L., Srinivasan, B., Rao, V. A. &amp; Ribeiro, B. Janossy pooling: learning deep permutation-invariant functions for variable-size inputs. In Proc. 7th International Conference on Learning Representations (ICLR, 2019)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR64" id="ref-link-section-d83722773e4262">64</a></sup>.</p><p>As we cannot condition on an empty feature set for technical reasons, we condition the prediction of the first feature <i>x</i><sub>1</sub>, on a feature with random noise, that is, no information.</p><p>The above factorization of the density of a sample (equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Equ5">5</a>)) is completely tractable and we thus use it to estimate the likelihood for data points. This enables tasks such as anomaly detection and outlier identification.</p><h4 id="Sec33">Synthetic data generation</h4><p>We can leverage the generative abilities of TabPFN (see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec32">Density estimation</a>’) to synthesize new tabular data samples that mimic the characteristics of a given real-world dataset, by simply following the factorization in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Equ5">5</a>) and sampling each feature step by step. The generated synthetic samples (<b>x</b><sup>*</sup>, <i>y</i><sup>*</sup>) can be used for various purposes, such as data augmentation, privacy-preserving data sharing and scenario simulation.</p><h4 id="Sec34">Embeddings</h4><p>TabPFN can be used to retrieve meaningful feature representations or embeddings. Given a dataset <span>\({\mathcal{D}}={\{({{\bf{x}}}_{i},{y}_{i})\}}_{i=1}^{N}\)</span>, the goal is to learn a mapping <span>\({f}_{\theta }:{{\mathbb{R}}}^{d}\to {{\mathbb{R}}}^{k}\)</span> that transforms the original <i>d</i>-dimensional feature vectors <b>x</b><sub><i>i</i></sub> into an embedding space of dimension <i>k</i>. The resulting embeddings <span>\({f}_{\theta }({{\bf{x}}}_{i})\in {{\mathbb{R}}}^{k}\)</span> capture the learned relationships between features and can be used for downstream tasks. To use TabPFN for this problem, we simply use the target-column representations of its final layer as embeddings.</p><h3 id="Sec35">Detailed evaluation protocol</h3><p>To rigorously assess the performance and robustness of TabPFN, we conduct a comprehensive quantitative evaluation on standard tabular dataset benchmarks, comparing against state-of-the-art baselines under a standardized protocol.</p><h4 id="Sec36">Default configuration of TabPFN</h4><p>Unlike traditional algorithms, in-context-learned algorithms do not have hyperparameters that directly control their training procedure. Instead, hyperparameters for inference of TabPFN only control the pre-processing of data and post-processing of predictions (for example, feature scaling or softmax temperature). Our default configuration (TabPFN (default)) for both classification and regression is optimized for accurate predictions with minimal fitting time. Here, we apply the same model multiple times with different pre- and post-processors and take the average over the predictions, yielding a four-way (eight-way for regression) ensemble. The settings for our data processing were obtained through a hyperparameter search optimized on our development datasets. The exact settings chosen are listed in Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab5">5</a>. We emphasize that, as for other foundation models (such as GPT), we trained our TabPFN model once and used the same model to perform ICL in a forward pass on all new datasets.</p><h4 id="Sec37">Baselines</h4><p>We compare with tree-based methods, such as random forests<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Breimann, L. Random forests. Mach. Learn. 45, 5–32 (2001)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR38" id="ref-link-section-d83722773e4575">38</a></sup>, XGBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR7" id="ref-link-section-d83722773e4579">7</a></sup>, CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d83722773e4583">9</a></sup> and LightGBM<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 3149–3157 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR8" id="ref-link-section-d83722773e4587">8</a></sup>, the state of the art for experts to perform predictions on tabular data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e4591">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e4594">15</a></sup>. We also compare with simpler methods, such as ridge regression<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Hoerl, A. E. &amp; Kennard, R. W. Ridge regression: biased estimation for nonorthogonal problems. Technometrics 12, 55–67 (1970)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR70" id="ref-link-section-d83722773e4599">70</a></sup>, logistic regression and SVMs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Cortes, C. &amp; Vapnik, V. Support-vector networks. Mach. Learn. 20, 273–297 (1995)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR39" id="ref-link-section-d83722773e4603">39</a></sup>. Although standard neural networks, which unlike TabPFN do not use ICL, were shown to underperform for small (&lt;10,000 samples) tabular data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Borisov, V. et al. Deep neural networks and tabular data: a survey. IEEE Trans. Neural Netw. Learn. Syst. 35, 7499–7519 (2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR1" id="ref-link-section-d83722773e4607">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e4610">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Shwartz-Ziv, R. &amp; Armon, A. Tabular data: deep learning is not all you need. Inf. Fusion 81, 84–90 (2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR71" id="ref-link-section-d83722773e4613">71</a></sup>, as a point of reference, we still consider a simple neural network, the MLP.</p><h4 id="Sec38">Tabular dataset benchmarks</h4><p>We perform our analysis on two widely used and publicly available benchmark suites: the standard AutoML benchmark<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gijsbers, P. et al. AMLB: an AutoML benchmark. J. Mach. Learn. Res. 25, 1–65 (2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR36" id="ref-link-section-d83722773e4625">36</a></sup> and the recent regression benchmark OpenML-CTR23 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Fischer, S. F., Feurer, M. &amp; Bischl, B. OpenML-CTR23 – a curated tabular regression benchmarking suite. In Proc. AutoML Conference 2023 (Workshop) (AutoML, 2023)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR37" id="ref-link-section-d83722773e4629">37</a></sup>). Both benchmarks comprise a diverse set of real-world tabular datasets, carefully curated to be representative of various domains and data characteristics. The authors of the benchmark suite selected these datasets based on criteria such as sufficient complexity, real-world relevance, absence of free-form text features and diversity of problem domains.</p><p>For our quantitative analysis of TabPFN for classification tasks, we use a set of test datasets comprising all 29 datasets from the AutoML benchmark with up to 10,000 samples, 500 features and 10 classes. For regression tasks, the AutoML benchmark contains only 16 datasets matching these constraints. To increase statistical power, we augmented this set with all datasets matching our constraints from the recent OpenML-CTR23 benchmark, yielding a test set of 28 unique regression datasets in total. Extended Data Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab3">3</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Tab4">4</a> provide full details for our test sets of classification and regression datasets, respectively.</p><p>We further evaluated additional benchmark suites from refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e4645">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e4648">15</a></sup>. In ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d83722773e4652">14</a></sup>, there are 22 tabular classification datasets selected based on criteria such as heterogeneous columns, moderate dimensionality and sufficient difficulty. In ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d83722773e4656">15</a></sup>, there is a collection of 176 classification datasets, representing one of the largest tabular data benchmarks. However, the curation process for these datasets may not be as rigorous or quality controlled as for AutoML Benchmark and OpenML-CTR23. We also evaluated five Kaggle competitions with less than 10,000 training samples from the latest completed Tabular Playground Series.</p><h4 id="Sec39">Development datasets</h4><p>To decide on the hyperparameters of TabPFN, as well as our hyperparameter search spaces, we considered another set of datasets, our development datasets. We carefully selected datasets to be non-overlapping with our test datasets described above. The list of development datasets can be found in Supplementary Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-08328-6#MOESM2">5</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-08328-6#MOESM2">6</a>. We considered the mean of normalized scores (ROC/RMSE) and rank quantiles and chose the best model configurations on these development datasets.</p><h4 id="Sec40">Metrics and cross-validation</h4><p>To obtain scores for classification tasks, we use two widely adopted evaluation metrics: ROC AUC (One-vs-Rest) and accuracy. ROC AUC averages performance over different sensitivity–specificity trade-offs, and&nbsp;accuracy measures the fraction of samples labelled correctly.</p><p>For regression tasks, we use <i>R</i><sup>2</sup> and negative RMSE as evaluation metrics. <i>R</i><sup>2</sup> represents the proportion of variance in the target column that the model can predict. RMSE is the root of the average squared magnitude of the errors between the predicted and actual values. As we use negative RMSE, for all our four metrics higher values indicate a better fit.</p><p>To increase statistical validity, for each dataset and method in our test datasets, we evaluated 10 repetitions, each with a different random seed and train–test split (90% train and 10% test samples; all methods used the same cross-validation splits, defined by OpenML<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Vanschoren, J., van Rijn, J. N., Bischl, B. &amp; Torgo, L. OpenML: networked science in machine learning. SIGKDD Explor. 15, 49–60 (2014)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR72" id="ref-link-section-d83722773e4697">72</a></sup>). We average the scores of all repetitions per dataset. Then, to average scores across datasets, we normalize per dataset following previous benchmarks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gijsbers, P. et al. AMLB: an AutoML benchmark. J. Mach. Learn. Res. 25, 1–65 (2024)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR36" id="ref-link-section-d83722773e4701">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d83722773e4704">40</a></sup>. The absolute scores are linearly scaled such that a score of 1.0 corresponds to the highest value achieved by any method on that dataset, whereas a score of 0 represents the lowest result. This normalization allows for building meaningful averages across datasets with very different score ranges. We provide absolute performance numbers in Supplementary Data Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-08328-6#MOESM1">1</a>–<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-08328-6#MOESM1">2</a>. All confidence intervals shown are 95% confidence intervals.</p><p>We tuned all methods with a random search using five-fold cross-validation with ROC AUC/RMSE up to a given time budget, ranging from half a minute to 4 h. The first candidate in the random search was the default setting supplied in the implementation of the method and was also used if not a single cross-validation run finished before the time budget was consumed. See the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Sec5">Qualitative analysis</a>’ for the used search spaces per method. All methods were evaluated using 8 CPU cores. Moreover, TabPFN makes use of a 5-year-old consumer-grade GPU (RTX 2080 Ti). We also tested GPU acceleration for the baselines. However, as Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig8">2</a> shows, this did not improve performance, probably because of the small dataset sizes.</p></div></div>
                    
                </div><div>
                <div id="data-availability-section" data-title="Data availability"><h2 id="data-availability">Data availability</h2><p>All datasets evaluated are publicly available on <a href="https://openml.org/">openml.org</a> or <a href="https://kaggle.com/">kaggle.com</a>. We have provided scripts in our code repository that automate the process of downloading and evaluating the datasets. These scripts contain dataset identifiers, as well as exact data splitting and processing procedures.</p></div><div id="code-availability-section" data-title="Code availability"><h2 id="code-availability">Code availability</h2><p>Our code is available at <a href="https://priorlabs.ai/tabpfn-nature/">https://priorlabs.ai/tabpfn-nature/</a> (<a href="https://doi.org/10.5281/zenodo.13981285">https://doi.org/10.5281/zenodo.13981285</a>). We also provide an API that allows users to run TabPFN with minimal coding experience or without the availability of specific computing hardware such as a GPU. The code is designed to be modular and easily installable in a standard Python environment. The code to generate synthetic pre-training data has not been released with our models. We aim to enable researchers and practitioners to easily integrate TabPFN into their workflows and apply it to their specific tabular data tasks. We encourage users to provide feedback, report issues, and contribute to the further development of TabPFN. This open release aims to facilitate collaboration and accelerate the adoption and advancement of TabPFN in various research and application domains.</p></div><div id="MagazineFulltextArticleBodySuffix" aria-labelledby="Bib1" data-title="References"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Borisov, V. et al. Deep neural networks and tabular data: a survey. <i>IEEE Trans. Neural Netw. Learn. Syst.</i> <b>35</b>, 7499–7519 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TNNLS.2022.3229161" data-track-item_id="10.1109/TNNLS.2022.3229161" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTNNLS.2022.3229161" aria-label="Article reference 1" data-doi="10.1109/TNNLS.2022.3229161">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37015381" aria-label="PubMed reference 1">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1543.93250" aria-label="MATH reference 1">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20neural%20networks%20and%20tabular%20data%3A%20a%20survey&amp;journal=IEEE%20Trans.%20Neural%20Netw.%20Learn.%20Syst.&amp;doi=10.1109%2FTNNLS.2022.3229161&amp;volume=35&amp;pages=7499-7519&amp;publication_year=2024&amp;author=Borisov%2CV">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">van Breugel, B. &amp; van der Schaar, M. Position: why tabular foundation models should be a research priority. In <i>Proc. 41st International Conference on Machine Learning</i> 48976–48993 (PMLR, 2024).</p></li><li data-counter="3."><p id="ref-CR3">Silver, D. et al. Mastering the game of go with deep neural networks and tree search. <i>Nature</i> <b>529</b>, 484–489 (2016).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nature16961" data-track-item_id="10.1038/nature16961" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature16961" aria-label="Article reference 3" data-doi="10.1038/nature16961">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016Natur.529..484S" aria-label="ADS reference 3">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28Xhs12is7w%3D" aria-label="CAS reference 3">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26819042" aria-label="PubMed reference 3">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0153.49402" aria-label="MATH reference 3">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search&amp;journal=Nature&amp;doi=10.1038%2Fnature16961&amp;volume=529&amp;pages=484-489&amp;publication_year=2016&amp;author=Silver%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="4."><p id="ref-CR4">Jumper, J. M. et al. Highly accurate protein structure prediction with AlphaFold. <i>Nature</i> <b>596</b>, 583 – 589 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-021-03819-2" data-track-item_id="10.1038/s41586-021-03819-2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-021-03819-2" aria-label="Article reference 4" data-doi="10.1038/s41586-021-03819-2">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34265844" aria-label="PubMed reference 4">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8371605" aria-label="PubMed Central reference 4">PubMed Central</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0849.90143" aria-label="MATH reference 4">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Highly%20accurate%20protein%20structure%20prediction%20with%20AlphaFold&amp;journal=Nature&amp;doi=10.1038%2Fs41586-021-03819-2&amp;volume=596&amp;publication_year=2021&amp;author=Jumper%2CJM">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="5."><p id="ref-CR5">OpenAI. GPT-4 Technical Report. Preprint at <a href="https://arxiv.org/abs/2303.08774" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a> (2023).</p></li><li data-counter="6."><p id="ref-CR6">Friedman, J. H. Greedy function approximation: a gradient boosting machine. <i>Ann. Stat</i>. 1189–1232 (2001).</p></li><li data-counter="7."><p id="ref-CR7">Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In <i>Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016).</p></li><li data-counter="8."><p id="ref-CR8">Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In <i>Proc. 30th International Conference on Advances in Neural Information Processing Systems</i> (eds Guyon, I. et al.) 3149–3157 (Curran Associates, 2017).</p></li><li data-counter="9."><p id="ref-CR9">Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In <i>Proc. 30th International Conference on Advances in Neural Information Processing Systems</i> (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018).</p></li><li data-counter="10."><p id="ref-CR10">Lowe, D. G. Distinctive image features from scale-invariant keypoints. <i>Int. J. Comput. Vis.</i> <b>60</b>, 91–110 (2004).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1023/B:VISI.0000029664.99615.94" data-track-item_id="10.1023/B:VISI.0000029664.99615.94" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" aria-label="Article reference 10" data-doi="10.1023/B:VISI.0000029664.99615.94">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1066.68568" aria-label="MATH reference 10">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Distinctive%20image%20features%20from%20scale-invariant%20keypoints&amp;journal=Int.%20J.%20Comput.%20Vis.&amp;doi=10.1023%2FB%3AVISI.0000029664.99615.94&amp;volume=60&amp;pages=91-110&amp;publication_year=2004&amp;author=Lowe%2CDG">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="11."><p id="ref-CR11">Dalal, N. &amp; Triggs, B. Histograms of oriented gradients for human detection. In <i>Proc. 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR</i>’<i>05)</i> 886–893 (IEEE, 2005).</p></li><li data-counter="12."><p id="ref-CR12">Vaswani, A. et al. Attention is all you need. In <i>Proc. 30th International Conference on Advances in Neural Information Processing Systems</i> (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017).</p></li><li data-counter="13."><p id="ref-CR13">Silver, D. et al. Mastering the game of go without human knowledge. <i>Nature</i> <b>550</b>, 354–359 (2017).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nature24270" data-track-item_id="10.1038/nature24270" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature24270" aria-label="Article reference 13" data-doi="10.1038/nature24270">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2017Natur.550..354S" aria-label="ADS reference 13">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXhs12ltLvM" aria-label="CAS reference 13">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29052630" aria-label="PubMed reference 13">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1443.97024" aria-label="MATH reference 13">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Mastering%20the%20game%20of%20go%20without%20human%20knowledge&amp;journal=Nature&amp;doi=10.1038%2Fnature24270&amp;volume=550&amp;pages=354-359&amp;publication_year=2017&amp;author=Silver%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="14."><p id="ref-CR14">Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In <i>Proc. 36th International Conference on Neural Information Processing Systems</i> Vol. 35, 507–520 (ACM, 2022).</p></li><li data-counter="15."><p id="ref-CR15">McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In <i>Proc. 37th International Conference on Neural Information Processing System</i> Vol. 36, 76336–76369 (ACM, 2024).</p></li><li data-counter="16."><p id="ref-CR16">Goodfellow, I., Bengio, Y. &amp; Courville, A. <i>Deep Learning</i> (MIT Press, 2016).</p></li><li data-counter="17."><p id="ref-CR17">Brown, T. et al. Language models are few-shot learners. In <i>Proc. Advances in Neural Information Processing Systems</i> (eds Larochelle, H. et al.) Vol. 33, 1877–1901 (Curran Associates, 2020).</p></li><li data-counter="18."><p id="ref-CR18">Garg, S., Tsipras, D., Liang, P. S. &amp; Valiant, G. What can transformers learn in-context? A case study of simple function classes. In <i>Proc. Advances in Neural Information Processing Systems</i> Vol. 35, 30583–30598 (ACM, 2022).</p></li><li data-counter="19."><p id="ref-CR19">Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. &amp; Zhou, D. What learning algorithm is in-context learning? Investigations with linear models. In <i>Proc. The Eleventh International Conference on Learning Representations</i> (ICLR, 2023).</p></li><li data-counter="20."><p id="ref-CR20">Von Oswald, J. et al. Transformers learn in-context by gradient descent. In <i>Proc. 40th International Conference on Machine Learning</i> 35151–35174 (PMLR, 2023).</p></li><li data-counter="21."><p id="ref-CR21">Zhou, H. et al. What algorithms can transformers learn? A study in length generalization. In <i>Proc. The Twelfth International Conference on Learning Representations</i> (ICLR, 2024).</p></li><li data-counter="22."><p id="ref-CR22">Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In <i>Proc.</i> <i>The Tenth International Conference on Learning Representations</i> (ICLR, 2022).</p></li><li data-counter="23."><p id="ref-CR23">Hollmann, N., Müller, S., Eggensperger, K. &amp; Hutter, F. TabPFN: a transformer that solves small tabular classification problems in a second. In <i>Proc. The Eleventh International Conference on Learning Representations</i> (ICLR, 2023).</p></li><li data-counter="24."><p id="ref-CR24">Kingma, D. &amp; Ba, J. Adam: A method for stochastic optimization. In <i>Proc. International Conference on Learning Representations</i> (ICLR, 2015).</p></li><li data-counter="25."><p id="ref-CR25">Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. In <i>Proc. 3rd International Conference on Learning Representations</i> (eds Bengio, Y. &amp; LeCun, Y.) (ICLR, 2015).</p></li><li data-counter="26."><p id="ref-CR26">Gorishniy, Y., Rubachev, I., Khrulkov, V. &amp; Babenko, A. Revisiting deep learning models for tabular data. In <i>Proc. Advances in Neural Information Processing Systems 34</i> (eds Ranzato, M. et al.) 18932–18943 (NeurIPS, 2021).</p></li><li data-counter="27."><p id="ref-CR27">Zhu, B. et al. XTab: cross-table pretraining for tabular transformers. In <i>Proc. 40th International Conference on Machine Learning</i> (eds Krause, A. et al.) 43181–43204 (PMLR, 2023).</p></li><li data-counter="28."><p id="ref-CR28">Lorch, L., Sussex, S., Rothfuss, J., Krause, A. &amp; Schölkopf, B. Amortized inference for causal structure learning. In <i>Proc. Advances in Neural Information Processing Systems</i> (eds Koyejo, S. et al.) Vol. 35, 13104–13118 (ACM, 2022).</p></li><li data-counter="29."><p id="ref-CR29">Dao, T., Fu, D., Ermon, S., Rudra, A. &amp; Ré, C. Flashattention: fast and memory-efficient exact attention with io-awareness. In <i>Proc. Advances in Neural Information Processing Systems</i> (eds Koyejo, S. et al.) Vol. 35, 16344–16359 (2022).</p></li><li data-counter="30."><p id="ref-CR30">Torgo, L. &amp; Gama, J. Regression using classification algorithms. <i>Intell. Data Anal.</i> <b>1</b>, 275–292 (1997).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3233/IDA-1997-1405" data-track-item_id="10.3233/IDA-1997-1405" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3233%2FIDA-1997-1405" aria-label="Article reference 30" data-doi="10.3233/IDA-1997-1405">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1032.62065" aria-label="MATH reference 30">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Regression%20using%20classification%20algorithms&amp;journal=Intell.%20Data%20Anal.&amp;doi=10.3233%2FIDA-1997-1405&amp;volume=1&amp;pages=275-292&amp;publication_year=1997&amp;author=Torgo%2CL&amp;author=Gama%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="31."><p id="ref-CR31">Pearl, J. <i>Causality</i> 2nd edn (Cambridge Univ. Press, 2009).</p></li><li data-counter="32."><p id="ref-CR32">Jiang, M. et al. Investigating Data Contamination for Pre-training Language Models. Preprint at <a href="https://arxiv.org/abs/2401.06059" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2401.06059">https://arxiv.org/abs/2401.06059</a> (2024).</p></li><li data-counter="33."><p id="ref-CR33">Kumaraswamy, P. A generalized probability density function for double-bounded random processes. <i>J. Hydrol.</i> <b>46</b>, 79–88 (1980).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/0022-1694(80)90036-0" data-track-item_id="10.1016/0022-1694(80)90036-0" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2F0022-1694%2880%2990036-0" aria-label="Article reference 33" data-doi="10.1016/0022-1694(80)90036-0">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=1980JHyd...46...79K" aria-label="ADS reference 33">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0253.73040" aria-label="MATH reference 33">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20generalized%20probability%20density%20function%20for%20double-bounded%20random%20processes&amp;journal=J.%20Hydrol.&amp;doi=10.1016%2F0022-1694%2880%2990036-0&amp;volume=46&amp;pages=79-88&amp;publication_year=1980&amp;author=Kumaraswamy%2CP">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="34."><p id="ref-CR34">Rosenblatt, F. <i>Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms</i>. Report No. 1196-0-8 (Cornell Aeronautical Lab, 1961).</p></li><li data-counter="35."><p id="ref-CR35">Young, T. I. The bakerian lecture. experiments and calculations relative to physical optics. <i>Philos. Trans. R. Soc. Lond.</i> <b>94</b>, 1–16 (1804).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=1804RSPT...94....1Y" aria-label="ADS reference 35">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1032.68554" aria-label="MATH reference 35">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=I.%20The%20bakerian%20lecture.%20experiments%20and%20calculations%20relative%20to%20physical%20optics.&amp;journal=Philos.%20Trans.%20R.%20Soc.%20Lond.&amp;volume=94&amp;pages=1-16&amp;publication_year=1804&amp;author=Young%2CT">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="36."><p id="ref-CR36">Gijsbers, P. et al. AMLB: an AutoML benchmark. <i>J. Mach. Learn. Res.</i> <b>25</b>, 1–65 (2024).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=AMLB%3A%20an%20AutoML%20benchmark&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=25&amp;pages=1-65&amp;publication_year=2024&amp;author=Gijsbers%2CP">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="37."><p id="ref-CR37">Fischer, S. F., Feurer, M. &amp; Bischl, B. OpenML-CTR23 – a curated tabular regression benchmarking suite. In <i>Proc. AutoML Conference 2023 (Workshop)</i> (AutoML, 2023).</p></li><li data-counter="38."><p id="ref-CR38">Breimann, L. Random forests. <i>Mach. Learn.</i> <b>45</b>, 5–32 (2001).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1023/A:1010933404324" data-track-item_id="10.1023/A:1010933404324" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1023%2FA%3A1010933404324" aria-label="Article reference 38" data-doi="10.1023/A:1010933404324">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1007.68152" aria-label="MATH reference 38">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20forests&amp;journal=Mach.%20Learn.&amp;doi=10.1023%2FA%3A1010933404324&amp;volume=45&amp;pages=5-32&amp;publication_year=2001&amp;author=Breimann%2CL">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="39."><p id="ref-CR39">Cortes, C. &amp; Vapnik, V. Support-vector networks. <i>Mach. Learn.</i> <b>20</b>, 273–297 (1995).</p><p><a data-track="click_references" rel="noopener" data-track-label="10.1007/BF00994018" data-track-item_id="10.1007/BF00994018" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/BF00994018" aria-label="Article reference 39" data-doi="10.1007/BF00994018">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0831.68098" aria-label="MATH reference 39">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Support-vector%20networks&amp;journal=Mach.%20Learn.&amp;doi=10.1007%2FBF00994018&amp;volume=20&amp;pages=273-297&amp;publication_year=1995&amp;author=Cortes%2CC&amp;author=Vapnik%2CV">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="40."><p id="ref-CR40">Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at <a href="https://arxiv.org/abs/2003.06505" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2003.06505">https://arxiv.org/abs/2003.06505</a> (2020).</p></li><li data-counter="41."><p id="ref-CR41">Wolpert, D. Stacked generalization. <i>Neural Netw.</i> <b>5</b>, 241–259 (1992).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/S0893-6080(05)80023-1" data-track-item_id="10.1016/S0893-6080(05)80023-1" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2FS0893-6080%2805%2980023-1" aria-label="Article reference 41" data-doi="10.1016/S0893-6080(05)80023-1">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0792.68144" aria-label="MATH reference 41">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Stacked%20generalization&amp;journal=Neural%20Netw.&amp;doi=10.1016%2FS0893-6080%2805%2980023-1&amp;volume=5&amp;pages=241-259&amp;publication_year=1992&amp;author=Wolpert%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="42."><p id="ref-CR42">Caruana, R., Niculescu-Mizil, A., Crew, G. &amp; Ksikes, A. Ensemble selection from libraries of models. In <i>Proc. 21st International Conference on Machine Learning</i> (ed. Greiner, R.) (Omnipress, 2004).</p></li><li data-counter="43."><p id="ref-CR43">Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In <i>Proc. International Conference on Automated Machine Learning</i> Vol. 224 (PMLR, 2023).</p></li><li data-counter="44."><p id="ref-CR44">Hofmann, H. Statlog (German Credit Data). UCI Machine Learning Repository <a href="https://doi.org/10.24432/C5NC77" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.24432/C5NC77">https://doi.org/10.24432/C5NC77</a> (1994).</p></li><li data-counter="45."><p id="ref-CR45">Duin, R. Multiple Features. UCI Machine Learning Repository <a href="https://doi.org/10.24432/C5HC70" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.24432/C5HC70">https://doi.org/10.24432/C5HC70</a> (1998).</p></li><li data-counter="46."><p id="ref-CR46">Rajotte, J.-F. et al. Synthetic data as an enabler for machine learning applications in medicine. <i>iScience</i> <b>25</b>, 105331 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.isci.2022.105331" data-track-item_id="10.1016/j.isci.2022.105331" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.isci.2022.105331" aria-label="Article reference 46" data-doi="10.1016/j.isci.2022.105331">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2022iSci...25j5331R" aria-label="ADS reference 46">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB38XivVSitb%2FM" aria-label="CAS reference 46">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36325058" aria-label="PubMed reference 46">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9619172" aria-label="PubMed Central reference 46">PubMed Central</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1471.91288" aria-label="MATH reference 46">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Synthetic%20data%20as%20an%20enabler%20for%20machine%20learning%20applications%20in%20medicine&amp;journal=iScience&amp;doi=10.1016%2Fj.isci.2022.105331&amp;volume=25&amp;publication_year=2022&amp;author=Rajotte%2CJ-F">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="47."><p id="ref-CR47">Lundberg, S. M. &amp; Lee, S.-I. A unified approach to interpreting model predictions. In <i>Proc.</i> <i>Advances in Neural Information Processing Systems</i> (eds Guyon, I. et al.) Vol. 30, 4765–4774 (Curran Associates, 2017).</p></li><li data-counter="48."><p id="ref-CR48">Feuer, B. et al. TuneTables: context optimization for scalable prior-data fitted networks. In <i>Proc.</i> <i>38th Conference on Neural Information Processing Systems</i> (NeurIPS, 2024).</p></li><li data-counter="49."><p id="ref-CR49">Helli, K., Schnurr, D., Hollmann, N., Müller, S. &amp; Hutter, F. Drift-resilient tabPFN: In-context learning temporal distribution shifts on tabular data. In <i>Proc. 38th Conference on Neural Information Processing Systems</i> (NeurIPS, 2024).</p></li><li data-counter="50."><p id="ref-CR50">Thomas, V. et al. Retrieval &amp; fine-tuning for in-context tabular models. In <i>Proc. 1st Workshop on In-Context Learning at the 41st International Conference on Machine Learning</i> (ICML, 2024).</p></li><li data-counter="51."><p id="ref-CR51">Nagler, T. Statistical foundations of prior-data fitted networks. In <i>Proc. 40th International Conference on Machine Learning</i> (eds Krause, A. et al.) Vol. 202, 25660–25676 (PMLR, 2023).</p></li><li data-counter="52."><p id="ref-CR52">Dooley, S., Khurana, G. S., Mohapatra, C., Naidu, S. V. &amp; White, C. ForecastPFN: synthetically-trained zero-shot forecasting. In <i>Proc. 37th Conference on Advances in Neural Information Processing Systems</i> (eds Oh, A. et al.) (NeurIPS, 2023).</p></li><li data-counter="53."><p id="ref-CR53">Czolbe, S. &amp; Dalca, A. V. Neuralizer: General neuroimage analysis without re-training. In <i>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> 6217–6230 (IEEE, 2023).</p></li><li data-counter="54."><p id="ref-CR54">Wilcoxon, F. in <i>Breakthroughs in Statistics: Methodology and Distribution</i> (eds Kotz, S. &amp; Johnson, N. L.) 196–202 (Springer, 1992).</p></li><li data-counter="55."><p id="ref-CR55">Müller, A., Curino, C. &amp; Ramakrishnan, R. Mothernet: a foundational hypernetwork for tabular classification. Preprint at <a href="https://arxiv.org/abs/2312.08598" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2312.08598">https://arxiv.org/abs/2312.08598</a> (2023).</p></li><li data-counter="56."><p id="ref-CR56">Shazeer, N. Fast transformer decoding: one write-head is all you need. Preprint at <a href="https://arxiv.org/abs/1911.02150" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/1911.02150">https://arxiv.org/abs/1911.02150</a> (2019).</p></li><li data-counter="57."><p id="ref-CR57">Krapivsky, P. L. &amp; Redner, S. Organization of growing random networks. <i>Phys. Rev. E</i> <b>63</b>, 066123 (2001).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1103/PhysRevE.63.066123" data-track-item_id="10.1103/PhysRevE.63.066123" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1103%2FPhysRevE.63.066123" aria-label="Article reference 57" data-doi="10.1103/PhysRevE.63.066123">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2001PhRvE..63f6123K" aria-label="ADS reference 57">ADS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD38%2FhsF2isA%3D%3D" aria-label="CAS reference 57">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1109.92301" aria-label="MATH reference 57">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Organization%20of%20growing%20random%20networks&amp;journal=Phys.%20Rev.%20E&amp;doi=10.1103%2FPhysRevE.63.066123&amp;volume=63&amp;publication_year=2001&amp;author=Krapivsky%2CPL&amp;author=Redner%2CS">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="58."><p id="ref-CR58">Glorot, X. &amp; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In <i>Proc. 13th International Conference on Artificial Intelligence and Statistics</i> 249–256 (JMLR, 2010).</p></li><li data-counter="59."><p id="ref-CR59">Nair, V. &amp; Hinton, G. Rectified linear units improve restricted Boltzmann machines. In <i>Proc. 27th International Conference on Machine Learning</i> (eds Fürnkranz, J. &amp; Joachims, T.) 807–814 (Omnipress, 2010).</p></li><li data-counter="60."><p id="ref-CR60">Quinlan, J. R. Induction of decision trees. <i>Mach. Learn.</i> <b>1</b>, 81–106 (1986).</p><p><a data-track="click_references" rel="noopener" data-track-label="10.1007/BF00116251" data-track-item_id="10.1007/BF00116251" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/BF00116251" aria-label="Article reference 60" data-doi="10.1007/BF00116251">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0887.73077" aria-label="MATH reference 60">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Induction%20of%20decision%20trees&amp;journal=Mach.%20Learn.&amp;doi=10.1007%2FBF00116251&amp;volume=1&amp;pages=81-106&amp;publication_year=1986&amp;author=Quinlan%2CJR">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="61."><p id="ref-CR61">Müller, S., Feurer, M., Hollmann, N. &amp; Hutter, F. PFNS4BO: in-context learning for Bayesian optimization. In <i>Proc. 40th International Conference on Machine Learning</i> 25444–25470 (PMLR, 2023).</p></li><li data-counter="62."><p id="ref-CR62">Dietterich, T. G. &amp; Bakiri, G. Solving multiclass learning problems via error-correcting output codes. <i>J. Artif. Intell. Res.</i> <b>2</b>, 263–286 (1994).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1613/jair.105" data-track-item_id="10.1613/jair.105" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1613%2Fjair.105" aria-label="Article reference 62" data-doi="10.1613/jair.105">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0900.68358" aria-label="MATH reference 62">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Solving%20multiclass%20learning%20problems%20via%20error-correcting%20output%20codes&amp;journal=J.%20Artif.%20Intell.%20Res.&amp;doi=10.1613%2Fjair.105&amp;volume=2&amp;pages=263-286&amp;publication_year=1994&amp;author=Dietterich%2CTG&amp;author=Bakiri%2CG">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="63."><p id="ref-CR63">Loshchilov, I. &amp; Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In <i>Proc. 5th International Conference on Learning Representations</i> (ICLR, 2017).</p></li><li data-counter="64."><p id="ref-CR64">Murphy, R. L., Srinivasan, B., Rao, V. A. &amp; Ribeiro, B. Janossy pooling: learning deep permutation-invariant functions for variable-size inputs. In <i>Proc. 7th International Conference on Learning Representations</i> (ICLR, 2019).</p></li><li data-counter="65."><p id="ref-CR65">McCarter, C. The kernel density integral transformation. <i>Transact. Mach. Learn. Res.</i> <a href="https://openreview.net/pdf?id=6OEcDKZj5j" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/pdf?id=6OEcDKZj5j">https://openreview.net/pdf?id=6OEcDKZj5j</a> (2023).</p></li><li data-counter="66."><p id="ref-CR66">Caruana, R., Munson, A. &amp; Niculescu-Mizil, A. Getting the most out of ensemble selection. In <i>Proc. 6th IEEE International Conference on Data Mining</i> (eds Clifton, C. et al.) 828–833 (IEEE, 2006).</p></li><li data-counter="67."><p id="ref-CR67">Feurer, M. et al. in <i>Automated Machine Learning: Methods, Systems, Challenges</i> (eds Hutter, F. et al.) Ch. 6 (Springer, 2019).</p></li><li data-counter="68."><p id="ref-CR68">Purucker, L. &amp; Beel, J. Assembled-OpenML: creating efficient benchmarks for ensembles in AutoML with OpenML. In <i>Proc. First International Conference on Automated Machine Learning</i> (AutoML, 2022).</p></li><li data-counter="69."><p id="ref-CR69">Purucker, L. &amp; Beel, J. CMA-ES for post hoc ensembling in AutoML: a great success and salvageable failure. In <i>Proc. International Conference on Automated Machine Learning</i> Vol. 224, 1–23 (PMLR, 2023).</p></li><li data-counter="70."><p id="ref-CR70">Hoerl, A. E. &amp; Kennard, R. W. Ridge regression: biased estimation for nonorthogonal problems. <i>Technometrics</i> <b>12</b>, 55–67 (1970).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1080/00401706.1970.10488634" data-track-item_id="10.1080/00401706.1970.10488634" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F00401706.1970.10488634" aria-label="Article reference 70" data-doi="10.1080/00401706.1970.10488634">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0202.17205" aria-label="MATH reference 70">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=Ridge%20regression%3A%20biased%20estimation%20for%20nonorthogonal%20problems&amp;journal=Technometrics&amp;doi=10.1080%2F00401706.1970.10488634&amp;volume=12&amp;pages=55-67&amp;publication_year=1970&amp;author=Hoerl%2CAE&amp;author=Kennard%2CRW">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="71."><p id="ref-CR71">Shwartz-Ziv, R. &amp; Armon, A. Tabular data: deep learning is not all you need. <i>Inf. Fusion</i> <b>81</b>, 84–90 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.inffus.2021.11.011" data-track-item_id="10.1016/j.inffus.2021.11.011" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.inffus.2021.11.011" aria-label="Article reference 71" data-doi="10.1016/j.inffus.2021.11.011">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 71" href="http://scholar.google.com/scholar_lookup?&amp;title=Tabular%20data%3A%20deep%20learning%20is%20not%20all%20you%20need&amp;journal=Inf.%20Fusion&amp;doi=10.1016%2Fj.inffus.2021.11.011&amp;volume=81&amp;pages=84-90&amp;publication_year=2022&amp;author=Shwartz-Ziv%2CR&amp;author=Armon%2CA">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="72."><p id="ref-CR72">Vanschoren, J., van Rijn, J. N., Bischl, B. &amp; Torgo, L. OpenML: networked science in machine learning. <i>SIGKDD Explor.</i> <b>15</b>, 49–60 (2014).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1145/2641190.2641198" data-track-item_id="10.1145/2641190.2641198" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1145%2F2641190.2641198" aria-label="Article reference 72" data-doi="10.1145/2641190.2641198">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1505.62090" aria-label="MATH reference 72">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 72" href="http://scholar.google.com/scholar_lookup?&amp;title=OpenML%3A%20networked%20science%20in%20machine%20learning&amp;journal=SIGKDD%20Explor.&amp;doi=10.1145%2F2641190.2641198&amp;volume=15&amp;pages=49-60&amp;publication_year=2014&amp;author=Vanschoren%2CJ&amp;author=Rijn%2CJN&amp;author=Bischl%2CB&amp;author=Torgo%2CL">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="73."><p id="ref-CR73">Fix, E. &amp; Hodges, J. L. Discriminatory analysis. Nonparametric discrimination: consistency properties. <i>Int. Stat. Rev.</i> <b>57</b>, 238–247 (1989).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.2307/1403797" data-track-item_id="10.2307/1403797" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.2307%2F1403797" aria-label="Article reference 73" data-doi="10.2307/1403797">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0715.62080" aria-label="MATH reference 73">MATH</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 73" href="http://scholar.google.com/scholar_lookup?&amp;title=Discriminatory%20analysis.%20Nonparametric%20discrimination%3A%20consistency%20properties&amp;journal=Int.%20Stat.%20Rev.&amp;doi=10.2307%2F1403797&amp;volume=57&amp;pages=238-247&amp;publication_year=1989&amp;author=Fix%2CE&amp;author=Hodges%2CJL">
                    Google Scholar</a>&nbsp;
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-024-08328-6?format=refman&amp;flavour=references">Download references</a></p></div></div><div id="Ack1-section" data-title="Acknowledgements"><h2 id="Ack1">Acknowledgements</h2><p>We express our gratitude to the following individuals for their valuable contributions and support. We thank E. Bergman for his assistance with the evaluation of TabPFN, for helping implement the random forest pre-processing, and for his efforts in improving the code quality and documentation. His contributions were instrumental in benchmarking TabPFN and ensuring the reproducibility of our results. We thank A. Gupta and D. Otte for their work on the Inference Server, which enables the fast deployment of TabPFN without the need for a local GPU. Their efforts have greatly enhanced the accessibility and usability of TabPFN. We thank L. Schweizer for his work on exploring the random forest pre-processing for TabPFN further. We thank D. Schnurr and K. Helli for their work on visualization, and D. Schnurr for his specific contributions related to handling missing values. We thank S. M. Lundberg for the collection of visualization methods for feature attribution that we adapted for our work. We thank A. Müller for the insightful discussions related to TabPFN training and for his guidance on identifying and mitigating biases in the prior. His expertise has been invaluable in refining the TabPFN methodology. We are very grateful to C. Langenberg and M. Pietzner for providing insights on medical applications, interpreting model results and offering general advice. Their continued support has been instrumental in shaping this work. We thank S. Stäglich for his outstanding maintenance and support with the cluster infrastructure. We thank B. Lake for his general paper writing advice. We are grateful for the computational resources that were available for this research. Specifically, we acknowledge support by the state of Baden-Württemberg through bwHPC and the German Research Foundation (DFG) through grant no INST 39/963-1 FUGG (bwForCluster NEMO), and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant no. 417962828. We acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under SFB 1597 (SmallData), grant no. 499552394, and by the European Union (through ERC Consolidator Grant DeepLearning 2.0, grant no. 101045765). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. F.H. acknowledges the financial support of the Hector Foundation.</p></div><div id="author-information-section" aria-labelledby="author-information" data-title="Author information"><h2 id="author-information">Author information</h2><div id="author-information-content"><p><span id="author-notes">Author notes</span></p><ol><li id="na1"><p>These authors contributed equally: Noah Hollmann, Samuel Müller</p></li></ol><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Machine Learning Lab, University of Freiburg, Freiburg, Germany</p><p>Noah Hollmann,&nbsp;Samuel Müller,&nbsp;Lennart Purucker,&nbsp;Arjun Krishnakumar,&nbsp;Max Körfer,&nbsp;Shi Bin Hoo&nbsp;&amp;&nbsp;Frank Hutter</p></li><li id="Aff2"><p>Computational Medicine, Berlin Institute of Health at Charité, Universitätsmedizin Berlin, Berlin, Germany</p><p>Noah Hollmann</p></li><li id="Aff3"><p>Prior Labs, Freiburg, Germany</p><p>Noah Hollmann&nbsp;&amp;&nbsp;Frank Hutter</p></li><li id="Aff4"><p>Neuromedical AI Lab, Department of Neurosurgery, Medical Center - University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany</p><p>Robin Tibor Schirrmeister</p></li><li id="Aff5"><p>Medical Physics, Department of Diagnostic and Interventional Radiology, Medical Center - University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany</p><p>Robin Tibor Schirrmeister</p></li><li id="Aff6"><p>ELLIS Institute Tübingen, Tübingen, Germany</p><p>Frank Hutter</p></li></ol><div data-test="author-info"><p><span>Authors</span></p><ol><li id="auth-Noah-Hollmann-Aff1-Aff2-Aff3"><span>Noah Hollmann</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Noah%20Hollmann" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Noah%20Hollmann%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Samuel-M_ller-Aff1"><span>Samuel Müller</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Samuel%20M%C3%BCller" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Samuel%20M%C3%BCller%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Lennart-Purucker-Aff1"><span>Lennart Purucker</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Lennart%20Purucker" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Lennart%20Purucker%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Arjun-Krishnakumar-Aff1"><span>Arjun Krishnakumar</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Arjun%20Krishnakumar" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Arjun%20Krishnakumar%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Max-K_rfer-Aff1"><span>Max Körfer</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Max%20K%C3%B6rfer" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Max%20K%C3%B6rfer%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Shi_Bin-Hoo-Aff1"><span>Shi Bin Hoo</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shi%20Bin%20Hoo" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shi%20Bin%20Hoo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Robin_Tibor-Schirrmeister-Aff4-Aff5"><span>Robin Tibor Schirrmeister</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Robin%20Tibor%20Schirrmeister" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Robin%20Tibor%20Schirrmeister%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Frank-Hutter-Aff1-Aff3-Aff6"><span>Frank Hutter</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Frank%20Hutter" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Frank%20Hutter%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li></ol></div><h3 id="contributions">Contributions</h3><p>N.H. improved the prior of the model; added regression support, unsupervised capabilities and inference optimizations; and contributed to the experiments and wrote the paper. S.M. improved the neural network architecture, training and efficiency; added inference optimizations; and contributed to experiments and wrote the paper. L.P. improved the inference interface of the model; contributed to hyperparameter tuning; added post hoc ensembling of TabPFN models; contributed to benchmarking; and wrote the paper. A.K. added inference optimizations and Kaggle experiments. M.K. contributed to inference optimizations. S.B.H. contributed to the usability of our code. R.T.S. contributed to preliminary architectural experiments to speed up inference and helped revise the first draft of the paper. F.H. contributed technical advice and ideas, contributed to the random forest pre-processing, managed collaborations and funding, and wrote the paper.</p><h3 id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:noah@priorlabs.ai">Noah Hollmann</a>, <a id="corresp-c2" href="mailto:samuelgabrielmuller@gmail.com">Samuel Müller</a> or <a id="corresp-c3" href="mailto:fh@cs.uni-freiburg.de">Frank Hutter</a>.</p></div></div><div id="ethics-section" data-title="Ethics declarations"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar2">Competing interests</h3>
                <p>The following patent applications invented by S.M. and F.H. and filed by R. Bosch are related to this work: DE202021105192U1 and DE102021210775A1. The authors do not have any ownership rights to these patent applications. F.H. and N.H. are affiliated with PriorLabs, a company focused on developing tabular foundation models. The authors declare no other competing interests.</p>
              
            </div></div><div id="peer-review-section" data-title="Peer review"><h2 id="peer-review">Peer review</h2><div id="peer-review-content">
              
              
                <h3 id="FPar1">Peer review information</h3>
                <p><i>Nature</i> thanks Duncan McElfresh, Oleksandr Shchur and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p>
              
            </div></div><div id="additional-information-section" data-title="Additional information"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div><div id="Sec42-section" data-title="Extended data figures and tables"><h2 id="Sec42">Extended data figures and tables</h2><div data-test="supplementary-info" id="Sec42-content"><div data-test="supp-item" id="Fig7"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 1 performance comparison across" href="https://www.nature.com/articles/s41586-024-08328-6/figures/7" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig7_ESM.jpg">Extended Data Fig. 1 Performance comparison across additional dataset characteristics, extending Fig. </a><a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-08328-6#Fig5">5</a>.</h3><p>This figure shows the relative performance of different methods when datasets are split based on specific attributes. Error bars represent 95% confidence intervals. While performance differences are generally subtle across these splits, the most notable variation is observed for datasets with outliers in the target variable, though confidence intervals still overlap.</p></div><div data-test="supp-item" id="Fig8"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 2 performance comparisons of ta" href="https://www.nature.com/articles/s41586-024-08328-6/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig8_ESM.jpg">Extended Data Fig. 2 Performance comparisons of TabPFN and baselines on additional benchmark datasets&nbsp;and with GPU support.</a></h3><p>(a) Classification performance on the Grinsztajn medium-sized benchmark with categorical features, across 7 datasets. (b) Classification performance on the Grinsztajn medium-sized benchmark with numerical features, across its 15 datasets. (c) Classification performance on the TabZilla benchmark, consisting of 102 datasets with fewer than 10,000 rows of data, 500 features, and 10 classes. Duplicated datasets and those with fewer than 5 samples per class were removed to enable 5-fold cross-validation. (d) Performance Over Time Comparison with CPU vs. GPU Hardware: The performance over time when running our strongest baselines with eight CPUs (CPU) vs. eight CPUs and on one GPU (+GPU) on our classification test benchmark. AutoGluon automatically decides which models to train with what resources. For CatBoost and XGB, we specified that the models should train with GPU. Intervals represent 95% CI.</p></div><div data-test="supp-item" id="Fig9"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 3 comparing shap (shapley addit" href="https://www.nature.com/articles/s41586-024-08328-6/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig9_ESM.jpg">Extended Data Fig. 3 Comparing SHAP (SHapley Additive exPlanations) summary plots between TabPFN and baselines.</a></h3><p>We compare SHAP feature importance and impact for Logistic Regression, TabPFN, and CatBoost on the “Default of Credit Card Clients” dataset. The top features visualized are credit amount, age, and duration. Each point represents a single instance, with the color indicating the value of the checking status feature (blue for low, red for high), illustrating its interaction with the respective feature on the x-axis. We see that Logistic Regression is most interpretable due to the simple underlying functions. However, Logistic Regression has poor predictive accuracy, and the learned functions are unintuitive when looking at the outer bounds of features. TabPFN has good predictive accuracy and learns simple, interpretable functions. CatBoost is the least interpretable, with unclear patterns and wide variation in SHAP values per sample. This figure is adapted from Lundberg et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Lundberg, S. M. &amp; Lee, S.-I. A unified approach to interpreting model predictions. In Proc. Advances in Neural Information Processing Systems (eds Guyon, I. et al.) Vol. 30, 4765–4774 (Curran Associates, 2017)." href="https://www.nature.com/articles/s41586-024-08328-6#ref-CR47" id="ref-link-section-d83722773e4935">47</a></sup>.</p></div><div data-test="supp-item" id="Fig10"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 4 finetuning tabpfn on 2-dimens" href="https://www.nature.com/articles/s41586-024-08328-6/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig10_ESM.jpg">Extended Data Fig. 4 Finetuning TabPFN on 2-dimensional sine curve datasets.</a></h3><p>(a) Examples of 2D sine curve datasets with different offsets. (b) Finetuning loss curves for 50 runs with random train-test offsets. Colors indicate the offset between train and test. TabPFN shows positive transfer, with better performance for more similar distributions. For a dataset shift of <i>π</i>, the inverse label needs to be predicted in the test set, compared to the finetuning data. However, TabPFN still generalizes when finetuned on this data.</p></div><div data-test="supp-item" data-container-section="table" id="table-1"><figure><figcaption><b id="Tab1" data-test="table-caption">Extended Data Table 1 Aggregated results on the 29 AMLB classification Benchmark datasets</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/1" aria-label="Full size table 1"><span>Full size table</span></a></p></figure></div><div data-test="supp-item" data-container-section="table" id="table-2"><figure><figcaption><b id="Tab2" data-test="table-caption">Extended Data Table 2 Aggregated results on the 28 AMLB and OpenML-CTR23 regression Benchmark datasets</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/2" aria-label="Full size table 2"><span>Full size table</span></a></p></figure></div><div data-test="supp-item" data-container-section="table" id="table-3"><figure><figcaption><b id="Tab3" data-test="table-caption">Extended Data Table 3 List of test datasets used for primary evaluation of classification tasks</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/3" aria-label="Full size table 3"><span>Full size table</span></a></p></figure></div><div data-test="supp-item" data-container-section="table" id="table-4"><figure><figcaption><b id="Tab4" data-test="table-caption">Extended Data Table 4 List of test datasets used for primary evaluation of regression tasks</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/4" aria-label="Full size table 4"><span>Full size table</span></a></p></figure></div><div data-test="supp-item" data-container-section="table" id="table-5"><figure><figcaption><b id="Tab5" data-test="table-caption">Extended Data Table 5 Hyperparameter defaults and search space for TabPFN and our baselines</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/5" aria-label="Full size table 5"><span>Full size table</span></a></p></figure></div><div data-test="supp-item" data-container-section="table" id="table-6"><figure><figcaption><b id="Tab6" data-test="table-caption">Extended Data Table 6 Performance on Kaggle Data Science Challenges</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41586-024-08328-6/tables/6" aria-label="Full size table 6"><span>Full size table</span></a></p></figure></div></div></div><div id="Sec43-section" data-title="Supplementary information"><h2 id="Sec43">Supplementary information</h2><div data-test="supplementary-info" id="Sec43-content"><div data-test="supp-item" id="MOESM1"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary tables 1–4" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Tables 1–4</a></h3><p> Unnormalized per dataset results: per dataset ROC AUC scores for our model and baselines on the four evaluated benchmarks.</p></div><div data-test="supp-item" id="MOESM2"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary tables 5 and 6" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_MOESM2_ESM.pdf" data-supp-info-image="">Supplementary Tables 5 and 6</a></h3><p>Meta-information on development datasets: meta-information of the development dataset is used to validate the performance of our models for regression and classification.</p></div></div></div><div id="rightslink-section" data-title="Rights and permissions"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Accurate%20predictions%20on%20small%20data%20with%20a%20tabular%20foundation%20model&amp;author=Noah%20Hollmann%20et%20al&amp;contentID=10.1038%2Fs41586-024-08328-6&amp;copyright=The%20Author%28s%29&amp;publication=0028-0836&amp;publicationDate=2025-01-08&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div><div id="article-info-section" aria-labelledby="article-info" data-title="About this article"><h2 id="article-info">About this article</h2><div id="article-info-content"><p><a data-crossmark="10.1038/s41586-024-08328-6" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41586-024-08328-6" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></p><div><h3 id="citeas">Cite this article</h3><p>Hollmann, N., Müller, S., Purucker, L. <i>et al.</i> Accurate predictions on small data with a tabular foundation model.
                    <i>Nature</i> <b>637</b>, 319–326 (2025). https://doi.org/10.1038/s41586-024-08328-6</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-024-08328-6?format=refman&amp;flavour=citation">Download citation</a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2024-05-17">17 May 2024</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2024-10-31">31 October 2024</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2025-01-08">08 January 2025</time></span></p></li><li><p>Issue Date<span>: </span><span><time datetime="2025-01-09">09 January 2025</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41586-024-08328-6</span></p></li></ul></div></div></div>
            </div></div>]]></description>
        </item>
    </channel>
</rss>