<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 23 Aug 2025 21:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[What makes Claude Code so damn good (122 pts)]]></title>
            <link>https://minusx.ai/blog/decoding-claude-code/</link>
            <guid>44998295</guid>
            <pubDate>Sat, 23 Aug 2025 19:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minusx.ai/blog/decoding-claude-code/">https://minusx.ai/blog/decoding-claude-code/</a>, See on <a href="https://news.ycombinator.com/item?id=44998295">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Claude Code is the most delightful AI agent/workflow I have used so far. Not only does it make targeted edits or vibe coding throwaway tools less annoying, using Claude Code makes me happy. It has enough autonomy to do interesting things, while not inducing a jarring loss of control like some other tools do. Of course most of the heavy lifting is done by the new Claude 4 model (especially interleaved thinking). But I find Claude Code objectively less annoying to use compared to Cursor, or Github Copilot agents even with the same underlying model! What makes it so damn good? If you're reading this and nodding along, I'm going to try and provide some answers.</p>
<p><strong>Note</strong>: This is not a blogpost with Claude Code's architecture dump (there are some good ones out there). This blogpost is meant to be a guide for building delightful LLM agents, based on my own experience using and tinkering with Claude Code over the last few months (and all the logs we intercepted and analyzed). You can find <a href="#appendix">prompts</a> and <a href="#appendix">tools</a> in the <a href="#appendix">Appendix section</a>. This post is ~2k words long, so strap in! If you're looking for some quick takeaways, the <a href="#how-to-build-a-claude-code-like-agent-tldr">TL;DR</a> section is a good place to start.</p>

<p><img src="https://minusx.ai/images/claude-code/prompts.png" alt="prompts">
</p><p>You can clearly see the different Claude Code updates.</p>

<p>Claude Code (CC) feels great to use, because it <em>just simply works</em>. CC has been crafted with a fundamental understanding of what the LLM is good at and what it is terrible at. Its prompts and tools cover for the model's stupidity and help it shine in its wheelhouse. The control loop is extremely simple to follow and trivial to debug.</p>
<p>We started using CC at MinusX as soon as it launched. To look under the hood, <a href="https://x.com/ppsreejith_">Sreejith</a> wrote a logger that intercepts and logs every network request made. The following analysis is from my extensive use over the last couple of months. <strong>This post attempts to answer the question - "What makes Claude Code so good, and how can you give a CC-like experience in your own chat-based-LLM agent?"</strong> We've incorporated most of these into MinusX already and I'm excited to see you do it too!</p>

<p><img src="https://minusx.ai/images/claude-code/tools.png" alt="prompts">
</p><p>Edit is the most frequent tool, followed by Read and ToDoWrite</p>
<br>
<h2 id="how-to-build-a-claude-code-like-agent-tldr"><a href="#how-to-build-a-claude-code-like-agent-tldr">How to build a Claude Code like agent: TL;DR</a></h2>
<p>If there is one thing to take away from this, it is this - <strong>Keep Things Simple, Dummy</strong>. LLMs are terrible enough to debug and evaluate. Any additional complexity you introduce (multi-agents, agent handoffs or complex RAG search algorithms) only makes debugging 10x harder. If such a fragile system works at all, you'll be terrified of making drastic changes to it later. So, keep everything in one file, avoid excessive boilerplate scaffolding and rip it all out at least a couple of times :)</p>
<p>Here are the main takeaways from Claude Code to implement in your own system.</p>
<h4 id="1-control-loop"><a href="#1-control-loop">1. Control Loop</a></h4>
<ul>
<li>1.1 <a href="#11-keep-one-main-loop">Keep one main loop (with max one branch) and one message history</a></li>
<li>1.2 <a href="#12-use-a-smaller-model-for-everything">Use a smaller model for all sorts of things. All. The. Frickin. Time.</a></li>
</ul>
<h4 id="2-prompts"><a href="#2-prompts">2. Prompts</a></h4>
<ul>
<li>2.1 <a href="#21-use-claudemd-for-collaborating-on-user-context-and-preferences">Use claude.md pattern to collaborate on and remember user preferences</a></li>
<li>2.2 <a href="#22-special-xml-tags-markdown-and-lots-of-examples">Use special XML Tags, Markdown, and lots of examples</a></li>
</ul>
<h4 id="3-tools"><a href="#3-tools">3. Tools</a></h4>
<ul>
<li>3.1 <a href="#31-llm-search---rag-based-search">LLM search &gt;&gt;&gt;  RAG based search</a></li>
<li>3.2 <a href="#32-how-to-design-good-tools-low-level-vs-high-level-tools">How to design good tools? (High vs Low level tools)</a></li>
<li>3.3 <a href="#33-let-the-agent-manage-a-todo-list">Let your agent manage its own todo list</a></li>
</ul>
<h4 id="4-steerability"><a href="#4-steerability">4. Steerability</a></h4>
<ul>
<li>4.1 <a href="#41-tone-and-style">Tone and style</a></li>
<li>4.2 <a href="#42-this-is-important-is-still-state-of-the-art">"<strong>PLEASE THIS IS IMPORTANT</strong>" is unfortunately still state of the art</a></li>
<li>4.3 <a href="#43-write-the-algorithm-with-heuristics-and-examples">Write the algorithm, with heuristics and examples</a></li>
</ul>
<br>
<blockquote>
<p>Claude Code choses architectural simplicity at every juncture - one main loop, simple search, simple todolist, etc. Resist the urge to over-engineer, build good harness for the model let it cook! Is this end-to-end self-driving all over again? Bitter lesson much?</p>
</blockquote>
<hr>
<h2 id="1-control-loop-design"><a href="#1-control-loop-design">1. Control Loop Design</a></h2>
<h3 id="11-keep-one-main-loop"><a href="#11-keep-one-main-loop">1.1 Keep One Main Loop</a></h3>
<p>Debuggability &gt;&gt;&gt; complicated hand-tuned multi-agent lang-chain-graph-node mishmash.</p>
<p>Despite multi agent systems being all the rage, Claude Code has just one main thread. It uses a few different types of prompts periodically to summarize the git history, to clobber up the message history into one message or to come up with some fun UX elements. But apart from that, it maintains a flat list of messages. An interesting way it handles hierarchical tasks is by spawning itself as a sub-agent without the ability to spawn more sub-agents. There is a maximum of one branch, the result of which is added to the main message history as a "tool response".</p>
<p>If the problem is simple enough, the main loop just handles it via iterative tool calling. But if there are one or more tasks that are complex, the main agent creates clones of itself. The combination of the max-1-branch and the todo list makes sure the agent has the ability to break the problem into sub-problems, but also keep the eye on the final desired outcome.</p>
<p>I highly doubt your app needs a multi-agent system. With every layer of abstraction you make your system harder to debug, and more importantly you deviate from the general-model-improvement trajectory.</p>
<p><img src="https://minusx.ai/images/claude-code/control_loop.gif" alt="Control Loop"></p>
<h3 id="12-use-a-smaller-model-for-everything"><a href="#12-use-a-smaller-model-for-everything">1.2 Use a Smaller model for <em>everything</em></a></h3>
<p>Over 50% of all important LLM calls made by CC are to claude-3-5-haiku. It is used to read large files, parse web pages, process git history and summarize long conversations. It is also used to come up with the one-word processing label - literally for every key stroke! The smaller models are 70-80% cheaper than the standard ones (Sonnet 4, GPT-4.1). Use them liberally!</p>
<h2 id="2-prompts-1"><a href="#2-prompts-1">2. Prompts</a></h2>
<p>Claude Code has extremely elaborate prompts filled with heuristics, examples and IMPORTANT (tch-tch) reminders. The system prompt is ~2800 tokens long, with the Tools taking up a whopping 9400 tokens. The user prompt always contains the claude.md file, which can typically be another 1000-2000 tokens. The system prompt contains sections on tone, style, proactiveness, task management, tool usage policy and doing tasks. It also contains the date, current working directory, platform and OS information and recent commits.</p>
<p><a href="#appendix"><strong>Go read the entire prompt</strong></a>!</p>
<h3 id="21-use-claudemd-for-collaborating-on-user-context-and-preferences"><a href="#21-use-claudemd-for-collaborating-on-user-context-and-preferences">2.1 Use claude.md for collaborating on user context and preferences</a></h3>
<p>One of the major patterns most coding agent creators have settled on is the context file (aka Cursor Rules / claude.md / agent.md). The difference in Claude Code's performance with and without claude.md is night and day. It is a great way for the developers to impart context that cannot be inferred from the codebase and to codify all strict preferences. For example, you can force the LLM to skip some folders, or use specific libraries. CC sends the entire contents of the claude.md with every user request</p>
<p>We recently introduced <a href="https://minusx.ai/blog/memory/">minusx.md in MinusX</a> which is fast becoming the de-facto context file for our agents to codify user and team preferences.</p>
<h3 id="22-special-xml-tags-markdown-and-lots-of-examples"><a href="#22-special-xml-tags-markdown-and-lots-of-examples">2.2 Special XML Tags, Markdown, and lots of examples</a></h3>
<p>It is fairly established that XML tags and Markdown are two ways to structure a prompt. CC uses both, extensively. Here are a few notable XML tags in Claude Code:</p>
<ul>
<li><code>&lt;system-reminder&gt;</code>: This is used at the end of many prompt sections to remind the LLM of thing it presumably otherwise forgets. Example:</li>
</ul>
<pre><code>&lt;system-reminder&gt;This is a reminder that your todo list is currently empty. DO NOT mention this to the user explicitly because they are already aware. If you are working on tasks that would benefit from a todo list please use the TodoWrite tool to create one. If not, please feel free to ignore. Again do not mention this message to the user.&lt;/system-reminder&gt;
</code></pre>
<ul>
<li><code>&lt;good-example&gt;</code>, <code>&lt;bad-example&gt;</code>: These are used to codify heuristics. They can be especially useful when there is a fork in the road with multiple seemingly reasonable paths/tool_calls the model can choose. Examples can be used to contrast the cases and make it very clear which path is preferable. Example:</li>
</ul>
<pre><code>Try to maintain your current working directory throughout the session by using absolute paths and avoiding usage of `cd`. You may use `cd` if the User explicitly requests it.
&lt;good-example&gt;
pytest /foo/bar/tests  
&lt;/good-example&gt;
&lt;bad-example&gt;
cd /foo/bar &amp;&amp; pytest tests
&lt;/bad-example&gt;
</code></pre>
<p>CC also uses markdown to demarcate clear sections in the system prompt. Example markdown headings include:</p>
<ul>
<li>Tone and style</li>
<li>Proactiveness</li>
<li>Following conventions</li>
<li>Code style</li>
<li>Task Management</li>
<li>Tool use policy</li>
<li>Doing Tasks</li>
<li>Tools</li>
</ul>
<h2 id="3-tools-1"><a href="#3-tools-1">3. Tools</a></h2>
<p><a href="#appendix"><strong>Go read the entire tools prompt</strong></a> - it is a whopping 9400 tokens long!</p>
<h3 id="31-llm-search---rag-based-search"><a href="#31-llm-search---rag-based-search">3.1 LLM search &gt;&gt;&gt;  RAG based search</a></h3>
<p>One significant way in which CC deviates from other popular coding agents is in its rejection of RAG. Claude Code searches your code base just as you would, with really complex <code>ripgrep</code>, <code>jq</code> and <code>find</code> commands. Since the LLM understands code really well, it can use sophisticated regex to find pretty much any codeblock it deems relevant. Sometimes it ends up reading whole files with a smaller model.</p>
<p>RAG sounds like a good idea in theory, but it introduces new (and more importantly, hidden) failure modes. What is the similarity function to use? What reranker? How do you chunk the code? What do you do with large JSON or log files? With LLM Search, it just looks at 10 lines of the json file to understand its structure. If it wants, it looks at 10 more lines - just like you would. Most importantly, this is RL learnable - something BigLabs are already working on. The model does most of the heavy lifting - as it should, dramatically reducing the number of moving parts in the agent. Also, having two complicated, intelligent systems wired this way is just ugly. I was recently kidding with a friend saying this is the Camera vs Lidar of the LLM era and I'm only half joking.</p>
<h3 id="32-how-to-design-good-tools-low-level-vs-high-level-tools"><a href="#32-how-to-design-good-tools-low-level-vs-high-level-tools">3.2 How to design good tools? (Low level vs High level tools)</a></h3>
<p>This question keeps anyone who is building an LLM agent up at night. Should you give the model generic tasks (like meaningful actions) or should it be low level (like type and click and bash)? The answer is that it depends (and you should use both).</p>
<p>Claude Code has low level (Bash, Read, Write), medium level (Edit, Grep, Glob) and high level tools (Task, WebFetch, exit_plan_mode). CC can use bash, so why give a separate Grep tool? The real trade-off here is in how often you expect your agent to use the tool vs accuracy of the agent in using the tool. CC uses grep and glob so frequently that it makes sense to make separate tools out of them, but at the same time, it can also write generic bash commands for special scenarios.</p>
<p>Similarly, there are even higher level tools like WebFetch or 'mcp__ide__getDiagnostics' that are extremely deterministic in what they do. This saves the LLM from having to do multiple low level clicking and typing and keeps it on track. Help the poor model out, will ya!? Tool descriptions have elaborate prompts with plenty of examples. The system prompt has information about ‘when to use a tool' or how to choose between two tools that can do the same task.</p>
<p><strong>Tools in Claude Code:</strong></p>
<div><div><ul>
<li><a href="#appendix">Task</a></li>
<li><a href="#appendix">Bash</a></li>
<li><a href="#appendix">Glob</a></li>
<li><a href="#appendix">Grep</a></li>
<li><a href="#appendix">LS</a></li>
<li><a href="#appendix">ExitPlanMode</a></li>
<li><a href="#appendix">Read</a></li>
<li><a href="#">Edit</a></li>
</ul></div><div><ul>
<li><a href="#appendix">MultiEdit</a></li>
<li><a href="#appendix">Write</a></li>
<li><a href="#appendix">NotebookEdit</a></li>
<li><a href="#appendix">WebFetch</a></li>
<li><a href="#appendix">TodoWrite</a></li>
<li><a href="#appendix">WebSearch</a></li>
<li><a href="#">mcp__ide__getDiagnostics</a></li>
<li><a href="#">mcp__ide__executeCode</a></li>
</ul></div></div>
<h3 id="33-let-the-agent-manage-a-todo-list"><a href="#33-let-the-agent-manage-a-todo-list">3.3 Let the agent manage a todo list</a></h3>
<p>There are many reasons why this is a good idea. Context rot is a common problem in long-running LLM agents. They enthusiastically start out tackling a difficult problem, but over time lose their way and devolve into garbage. There are a few ways current agent designs tackle this.
Many agents have experimented with explicit todos (one model generates todos, another model implements them) or with Multi-agent handoff + verification (PRD/PM agent -&gt; implementer agent -&gt; QA agent)</p>
<p>We already know multi-agent handoff is not a good idea, for many many reasons. CC uses an explicit todo list, but one that the model maintains. This keeps the LLM on track (it has been heavily prompted to refer to the todo list frequently), while at the same time giving the model the flexibility to course correct mid-way in an implementation. This also effectively leverages the model's interleaved thinking abilities to either reject or insert new todo items on the fly.</p>
<h2 id="4-steerability-1"><a href="#4-steerability-1">4. Steerability</a></h2>
<h3 id="41-tone-and-style"><a href="#41-tone-and-style">4.1 Tone and Style</a></h3>
<p>CC explicitly attempts to control the aesthetic behavior of the agent. There are sections in the system prompt around tone, style and proactiveness - full of instructions and examples. This is  why Claude Code “feels” tasteful in its comments and eagerness. I recommend just copying large sections of this into your app as is.</p>
<pre><code># Some examples of tone and style
- IMPORTANT: You should NOT answer with unnecessary preamble or postamble (such as explaining your code or summarizing your action), unless the user asks you to.
Do not add additional code explanation summary unless requested by the user.

- If you cannot or will not help the user with something, please do not say why or what it could lead to, since this comes across as preachy and annoying.

- Only use emojis if the user explicitly requests it. Avoid using emojis in all communication unless asked.
</code></pre>
<h3 id="42-this-is-important-is-still-state-of-the-art"><a href="#42-this-is-important-is-still-state-of-the-art">4.2 "THIS IS IMPORTANT" is still State of the Art</a></h3>
<p>Unfortunately CC is no better when it comes to asking the model to not do something. IMPORTANT, VERY IMPORTANT, NEVER and ALWAYS seem to be the best way to steer the model away from landmines. I expect the models to get more steerable in the future and avoid this ugliness. But for now, CC uses this liberally, and so should you. Some examples:</p>
<pre><code>- IMPORTANT: DO NOT ADD ***ANY*** COMMENTS unless asked

- VERY IMPORTANT: You MUST avoid using search commands like `find` and `grep`. Instead use Grep, Glob, or Task to search. You MUST avoid read tools like `cat`, `head`, `tail`, and `ls`, and use Read and LS to read files.\n  - If you _still_ need to run `grep`, STOP. ALWAYS USE ripgrep at `rg` first

- IMPORTANT: You must NEVER generate or guess URLs for the user unless you are confident that the URLs are for helping the user with programming. You may use URLs provided by the user in their messages or local files.

</code></pre>
<h3 id="43-write-the-algorithm-with-heuristics-and-examples"><a href="#43-write-the-algorithm-with-heuristics-and-examples">4.3 Write the Algorithm (with heuristics and examples)</a></h3>
<p>It is extremely important to identify the most important task the LLM needs to perform and write out the algorithm for it. Try to role-play as the LLM and work through examples, identify all the decision points and write them explicitly. It helps if this is in the form of a flow-chart. This helps structure the decision making and aids the LLM in following instructions. One thing to definitely avoid is a big soup of Dos and Don'ts. They are harder to keep track, and keep mutually exclusive. If your prompt is several thousand tokens long, you will inadvertently have conflicting Dos and Don'ts. The LLM becomes extremely fragile in this case and it becomes impossible to incorporate new use cases.</p>
<p><code>Task Management</code>, <code>Doing Tasks</code> and <code>Tool Usage Policy</code> sections in Claude Code's system prompt clearly walk through the algorithm to follow. This is also the section to add lots of heuristics and examples of various scenarios the LLM might encounter.</p>
<h2 id="bonus-why-pay-attention-to-biglab-prompts"><a href="#bonus-why-pay-attention-to-biglab-prompts">Bonus: Why pay attention to BigLab prompts?</a></h2>
<p>A lot of the effort in steering LLMs is trying to reverse engineer their post-training / RLHF data distribution. Should you use JSON or XML? Should the tool descriptions be in the system prompt or just in tools? What about your app's current state? It helps to see what they do in their own apps and use it to inform yours. Claude Code design is very opinionated and it helps to use that in forming your own.</p>
<br>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2>
<p>The main takeaway, again, is to keep things simple. Extreme scaffolding frameworks will hurt more than help you. Claude Code really made me believe that an "agent" can be simple and yet extremely powerful. We've incorporated a bunch of these lessons into MinusX, and are continuing to incorporate more.</p>
<p>If you're interested in Claude-Codifying your own LLM agent, I'd love to chat - ping me on <a href="https://x.com/nuwandavek">twitter</a>! If you want trainable Claude Code like data agents for your Metabase, check out <a href="https://minusx.com/">MinusX</a> or set up a demo with me <a href="https://minusx.com/demo">here</a>. Happy (Claude) Coding!</p>

<br>
<hr>
<br>
<h2 id="appendix"><a href="#appendix">Appendix</a></h2>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I hacked Monster Energy (145 pts)]]></title>
            <link>https://bobdahacker.com/blog/monster-energy</link>
            <guid>44997145</guid>
            <pubDate>Sat, 23 Aug 2025 16:42:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bobdahacker.com/blog/monster-energy">https://bobdahacker.com/blog/monster-energy</a>, See on <a href="https://news.ycombinator.com/item?id=44997145">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            <div>
                <h2>The Energy Drink Giant That Forgot to Lock Its Doors</h2>
<p>As a hacker who likes energy drinks, I decided to check out Monster Energy's corporate infrastructure. What I found was completely exposed and making terrible security decisions.</p>
<h3>Monster University: Where Security Goes to Die</h3>
<p>Monster University (<code>mu.monsterenergy.com</code>) is where Monster employees go to learn about their brand. It's also where I learned that changing <code>/login</code> to <code>/register</code> in the URL is apparently Monster's idea of "authentication."</p>
<p>The registration form appeared but wouldn't submit. So I went straight to the JavaScript to find the actual API endpoint. The API helpfully told me exactly which fields were missing from my registration attempt.</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/register_api.png" alt="Monster University Registration API"></p>
<p>Once I called the API directly with the right fields, boom, I was in. Full access to Monster University, complete with all their training materials, including this absolute masterpiece about their target demographic:</p>
<h3>This Is What Monster Thinks You Look Like</h3>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/consumer_demographics.jpg" alt="Monster's " consumer"="" demographics"=""></p>
<p>I'm not kidding. This is from their actual brand training guide. According to Monster Energy, their "Core Brand Family Consumer" is:</p>
<p><em>"Monster Green shoppers are likely younger (Gen-Z/Millennial/Gen-X) male, lower income &amp; Caucasian (skews Hispanic)."</em></p>
<p>And they included this photo of five people in Monster gear looking like they're being held hostage in a marketing photoshoot. This is literally what Monster corporate thinks their average customer looks like. I can't make this stuff up.</p>
<h3>The Irony: Their Own Cybersecurity Training</h3>
<p>The best part? Monster University has a cybersecurity course they bought from a third-party vendor. The same platform with no authentication has lessons teaching employees about phishing and basic security:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/cybersecurity_course.png" alt="Monster Cybersecurity Course"></p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/gamelist.png" alt="Monster Cybersecurity Game List"></p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/amplify_game.png" alt="Amplify Cybersecurity Game"></p>
<p>The irony of hosting a cybersecurity course about phishing on a completely unsecured platform is just <em>chef's kiss</em>.</p>
<h3>Meanwhile in Monster Corporate: Walmart Zoom Calls and "ULTIMATE BEAST" Badges</h3>
<p>While exploring Monster University, I found some gems about their corporate culture. You can view their entire Zoom meeting schedule and even get the join links:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/walmart_meeting.png" alt="Walmart Office Hours Event"></p>
<p>And check out their employee achievement system - you can earn badges for everything from "BEAST" to "ULTIMATE BEAST":</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/monster_badges.png" alt="Monster Achievement Badges"></p>
<h3>Beast Bux: Monster's Internal Currency System</h3>
<p>But wait, it gets better. I found their internal employee rewards system called "Beast Bux." Here's their actual training video explaining it:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/video_section.png" alt="Monster University Video Section"></p>

<p>Essentially, employees get Beast Bux yearly and can give them to other employees to show appreciation. These can be used to buy Monster merch at their internal store: <a target="_blank" rel="noopener noreferrer" href="https://bleedgreenshop.monsterenergy.com/">https://bleedgreenshop.monsterenergy.com/</a></p>
<h3>The Real Treasure: Exposed Corporate File System</h3>
<p>The scariest part wasn't the training portal or the questionable customer profiling. It was finding their OpenText API completely exposed with no authentication required:</p>
<pre><code>https://opentextapi.monsterenergy.com/opentext/search/?page=1&amp;pageSize=1000&amp;searchTerms=
</code></pre>
<p>This endpoint allows anyone to search through Monster's entire file system. No password. No authentication. Nothing.</p>
<p>Want to see internal contracts? Sure, here's one I found:</p>
<pre><code>https://opentextapi.monsterenergy.com/opentext/images/7e02a7602d8cee4aaf5b999850c243df9d0a184b
</code></pre>
<p>(Don't let the "images" in the URL fool you - it serves all file types, not just images)</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/contract.png" alt="Monster Contract Document"></p>
<p>The API returns full document metadata, file paths, and direct download links for everything in their system. Contracts, internal documents, you name it.</p>
<h3>Even Worse: The ClickUp Integration Disaster</h3>
<p>On a subdomain called Kermometer (<code>kermometer.monsterenergy.com</code>), I discovered Monster had integrated ClickUp into their workflow, but they made a critical mistake: they exposed an admin's private account token directly in their website's JavaScript.</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/clickup_token.png" alt="ClickUp Token Exposed in JavaScript"></p>
<p>This token would allow anyone to:</p>
<ul>
<li>Access their entire ClickUp workspace</li>
<li>View all private documents and projects</li>
<li>Invite themselves to the workspace</li>
<li>Potentially modify or delete critical project data</li>
</ul>
<p>To prove it worked, I invited myself to their workspace:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/clickup_invite.png" alt="ClickUp Workspace Invitation"></p>
<p>For the lulz, I wrote a script to share everything with myself using their admin token:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/sharing_script.png" alt="Script Sharing Everything With Me"></p>
<p>Don't worry, I left their workspace immediately after proving the vulnerability. I'm not trying to steal Monster's secret energy drink formulas or anything.</p>
<h3>The Response (Or Lack Thereof)</h3>
<p>I tried contacting Monster Energy directly about these vulnerabilities. No response.</p>
<p>They did fix the Monster University registration issue, but I don't think they even read my emails - they probably just noticed someone had signed up through their broken system and patched it.</p>
<p>Finally, I told ClickUp themselves about the exposed token on Monster's site. They investigated and contacted Monster, getting it fixed in less than a week. </p>
<p>But Monster? They never even acknowledged any of my reports. And as you can see, they left their entire file system API wide open.</p>
<p><strong>The OpenText API is STILL ACTIVE as of writing this post.</strong></p>
<h3>To Monster Energy</h3>
<p>Your energy drinks might "Unleash the Beast," but your security is definitely asleep.</p>
<p>Maybe spend less time creating stereotypical customer profiles and more time securing your infrastructure? Just a thought.</p>
<p>Also, a security contact email would be nice. You know, for next time.</p>
<hr>

            </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RFC 9839 and Bad Unicode (195 pts)]]></title>
            <link>https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839</link>
            <guid>44995640</guid>
            <pubDate>Sat, 23 Aug 2025 12:54:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839">https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839</a>, See on <a href="https://news.ycombinator.com/item?id=44995640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="centercontent">
<p itemprop="description">Unicode is good. If you’re designing a data structure or protocol that has text fields, they should contain
    Unicode characters encoded in UTF-8. There’s another question, though:
    “<em>Which</em> Unicode characters?” The 
    answer is “Not all of them, please exclude some.”</p>

<p>This issue keeps coming up, so Paul Hoffman and I put together an individual-submission draft
    to the IETF and now (where by “now” I mean “two years later”) it’s been published as
    <a href="https://www.rfc-editor.org/rfc/rfc9839.html">RFC 9839</a>. It explains which characters are bad, and why, then offers
    three plausible less-bad subsets that you might want to use.
    Herewith a bit of background, but…</p>

<p id="p-2"><span>Please</span> · 
If you’re actually working on something new that will have text fields, please read the RFC. It’s only ten pages long, and that’s
    with all the IETF boilerplate. It’s written specifically for software and networking people.</p>

<p id="p-3"><span>The smoking gun</span> · 
The badness that 9839 focuses on is “problematic characters”, so let’s start with a painful example of what that means.
    Suppose you’re designing a protocol that uses JSON and one of your constructs has a <code>username</code> field.
    Suppose you get this message (I omit all the non-<code>username</code> fields). It’s 
    a perfectly legal JSON text:</p>

<div><pre><span></span><span>{</span>
  <span>  </span><span>"username"</span><span>:</span><span> </span><span>"\u0000\u0089\uDEAD\uD9BF\uDFFF"</span>
<span>}</span>    </pre></div>
    <p>Unpacking all the JSON escaping gibberish reveals that the value of the <code>username</code> field contains four 
    numeric “code points” identifying Unicode characters:</p>

    <ol>
      <li><p>The first code point is zero, in Unicode jargon <code>U+0000</code>. In human-readable text it
      has no meaning, but it will interfere with the operation of certain programming languages.</p>
</li>
      <li><p>Next is Unicode <code>U+0089</code>, official name “CHARACTER TABULATION WITH JUSTIFICATION”. It’s what Unicode calls a
      <a href="https://en.wikipedia.org/wiki/C0_and_C1_control_codes">C1
      control code</a>, inherited from ISO/IEC 6429:1992, adopted from 
      <a href="https://www.ecma-international.org/wp-content/uploads/ECMA-48_5th_edition_june_1991.pdf">ECMA 48</a> (1991), which calls it
      “HTJ” and says: <i>HTJ causes the contents of the active field (the field in the presentation component that contains the
      active presentation position) to be shifted forward so that it ends at the character position preceding the
      following character tabulation stop. The active presentation position is moved to that following character
      tabulation stop. The character positions which precede the beginning of the shifted string are put into the
      erased state.</i></p>

      <p>Good luck with that.</p>
</li>
      <li><p>The third code point, <code>U+DEAD</code>, in Unicode lingo, is an “unpaired surrogate”.  To understand,
      you’d have to learn how Unicode’s much-detested
      <a href="https://en.wikipedia.org/wiki/UTF-16">UTF-16</a> encoding works.
      I recommend not bothering.</p>

      <p>All you need to know is that surrogates are only meaningful when they come in pairs in UTF-16 encoded text. There is
      effectively no such text on the wire and thus no excuse for tolerating surrogates in your data. In fact, the UTF-8 specification
      says that you mustn’t use UTF-8 to encode surrogates. But the real problem is that different libraries in different
      programming languages don’t always do the same things when they encounter this sort of fœtid interloper.</p>
    </li>
      <li><p>Finally, <code>\uD9BF\uDFFF</code> is JSON for the code point <code>U+7FFFF</code>.
      Unicode has a category called “noncharacter”, containing a few dozen code points that, for a variety of
      reasons, some good, 
      don’t represent anything and must not be interchanged on the wire. <code>U+7FFFF</code> is one of those.</p>
</li>
    </ol>
    <p>The four code points in the example are all clearly problematic. 
    The just-arrived RFC 9839 formalizes the notion of “problematic” and
    offers easy-to-cite language saying which of these problematic types you want to
    exclude from your text fields. Which, if you’re going to use JSON, you should probably do.</p>

    <p id="p-6"><span>Don’t blame Doug</span> · 
    Doug Crockford I mean, the inventor of JSON.  If he (or I or really anyone careful) were inventing JSON now that Unicode is
    mature, he’d have been fussier about its character repertoire. Having said that, we’re stuck with JSON-as-it-is forever, so we
    need a good way to say which of the problematic characters we’re going to exclude even if JSON allows them.</p>

    <p id="p-5"><span>PRECISion</span> · 
    You may find yourself wondering why the IETF waited until 2025 to provide help with Bad Unicode.
    It didn’t; here’s
    <a href="https://www.rfc-editor.org/rfc/rfc8264.html">RFC 8264</a>: <cite>PRECIS Framework: Preparation, Enforcement, and
    Comparison of Internationalized Strings in Application Protocols</cite>; the first PRECIS predecessor was published in 2002.
    8264 is 43 pages long, containing a <em>very</em>
    thorough discussion of many more potential Bad Unicode issues than 9839 does.</p>

    <p>Like 9839, PRECIS specifies subsets of the Unicode character repertoire and goes further, providing a mechanism for defining
    more.</p>

    <p>Having said that, PRECIS doesn’t seem to be very widely used by people who are defining new data structures and protocols. My
    personal opinion is that there are two problems which make it hard to adopt. First, it’s large and 
    complex, with many moving parts, and requires careful study to understand. Developers are (for good reason) lazy.</p>

    <p>Second, using PRECIS ties you to a specific version of Unicode. In particular, it forbids the use of the (nearly a million)
    unassigned code points. Since each release of Unicode includes new code point assignments, that means that a sender and receiver
    need to agree on exactly which version of Unicode they’re both going to use if they want reliably interoperable behavior. This
    makes life difficult for anyone writing a general-purpose code designed to be used in lots of different applications.</p>

    <p>I personally think that the only version of Unicode anybody wants to use is “as recent as possible”, so they can be confident
    of having all the latest emojis.</p>

    <p>Anyhow, 9839 is simpler and dumber than PRECIS. But I think some people will find it useful and now the IETF agrees.</p>

    <p id="p-7"><span>Source code</span> · 
    I’ve written a little Go-language library to validate incoming text fields against each of the three subsets that 9839
    specifies,
    <a href="https://github.com/timbray/RFC9839">here</a>.  I don’t claim it’s optimal, but it is well-tested.</p>

    <p>It doesn’t have a version number or release just yet, I’ll wait till a few folk have had a chance to spot any dumb mistakes I
    probably made.</p>

    <p id="p-9"><span>Details</span> · 
    Here’s a compact summary of the world of problematic Unicode code points and data formats and standards.</p>

    <table>
      <tbody><tr><td></td><th colspan="3">Problematic classes excluded?</th></tr>
      <tr><td></td><th>Surrogates</th><th>Legacy controls</th><th>Noncharacters</th></tr>
      <tr><td>CBOR</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>I-JSON</td><td>yes</td><td>no</td><td>yes</td></tr>
      <tr><td>JSON</td><td>no</td><td>no</td><td>no</td></tr>
      <tr><td>Protobufs</td><td>no</td><td>no</td><td>no</td></tr>
      <tr><td>TOML</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>XML</td><td>yes</td><td>partial [1]</td><td>partial [2]</td></tr>
      <tr><td>YAML</td><td>yes</td><td>mostly [3]</td><td>partial [2]</td></tr>
      <tr><td></td><th colspan="3">RFC 9839 Subsets</th></tr>
      <tr><td>Scalars</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>XML</td><td>yes</td><td>partial</td><td>partial</td></tr>
      <tr><td>Assignables</td><td>yes</td><td>yes</td><td>yes</td></tr>
    </tbody></table>
    <p>Notes:</p>

    <p><b>[1]</b> XML allows C1 controls.</p>

    <p><b>[2]</b> XML and YAML don’t exclude the noncharacters outside the Basic Multilingual Pane.</p>

    <p><b>[3]</b> YAML excludes all the legacy controls except for the mostly-harmless <code>U+0085</code>, another version of
    <code>\n</code> used in IBM mainframe documents.</p>

    <p id="p-8"><span>Thanks!</span> · 
    9839 is not a solo production. It received an extraordinary amount of discussion and improvement from a lot of smart and
    well-informed people 
    and the published version, 15 draft revisions later, is immensely better than my initial draft. My sincere thanks go to my
    co-editor Paul Hoffman and to all those mentioned in the RFC’s “Acknowledgements” section.</p>

    <p id="p-4"><span>On individual submissions</span> · 
    9839 is the second “individual submission” RFC I’ve pushed through the IETF (the other is
    <a href="https://datatracker.ietf.org/doc/html/rfc7725">RFC 7725</a>, which registers the HTTP 451 status code).  While it’s nice
    to decide something is worth standardizing and eventually have that happen, it’s really a lot of work. Some of that work is
    annoying.</p>

    <p>I’ve been involved in
    other efforts as Working-Group member, WG chair, and WG specification editor, and I can report authoritatively that creating an
    RFC the traditional way, through a Working Group, is easier and better.</p>

    <p>I feel discomfort advising others not to follow in my footsteps, but in this case I think it’s the right advice.</p>

  <hr>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing Speed-of-Light Flash Attention for 5090 in CUDA C++ (115 pts)]]></title>
            <link>https://gau-nernst.github.io/fa-5090/</link>
            <guid>44995508</guid>
            <pubDate>Sat, 23 Aug 2025 12:29:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gau-nernst.github.io/fa-5090/">https://gau-nernst.github.io/fa-5090/</a>, See on <a href="https://news.ycombinator.com/item?id=44995508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><header><p><time>Aug 23, 2025</time></p></header><section><p>In this post, I will walkthrough how I learned to implement Flash Attention for 5090 in CUDA C++. The main objective is to learn writing attention in CUDA C++, since many features are not available in <a href="https://triton-lang.org/main/index.html">Triton</a>, such as MXFP8 / NVFP4 MMA for sm120. I also feel this is a natural next step after learning about matmul kernels. Lastly, there are <a href="https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html">many</a> <a href="https://www.spatters.ca/mma-matmul">excellent</a> <a href="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">blogposts</a> on writing fast matmul kernels, but there is none for attention. So I want to take this chance to write up something nicely.</p><p>Readers are highly recommended to be familiar with CUDA C++ and how to use Tensor cores on NVIDIA GPUs. Of course you can still read along and clarify with your favourite LLMs along the way. Or you can check out GPU-MODE series (<a href="https://github.com/gpu-mode/lectures">slides</a>, <a href="https://www.youtube.com/@GPUMODE">YouTube</a>) for basic CUDA C++ knowledge, as well as the excellent matmul blogposts mentioned above, to quickly get up to speed.</p><p>You can find the full implementation discussed in this post here: <a href="https://github.com/gau-nernst/learn-cuda/tree/e83c256/07_attention">https://github.com/gau-nernst/learn-cuda/tree/e83c256/07_attention</a>. For <code>bs=1, num_heads=8, len_query=4096, len_kv = 8192</code>, 5090 @ 400W, compile with CUDA 12.9, I obtained the following benchmark results (theoretical limit of 5090 is 209.5 TFLOPS for BF16)</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td><code>F.sdpa()</code> (Flash Attention)</td><td>186.73</td><td>89.13%</td></tr><tr><td><code>F.sdpa()</code> (CuDNN)</td><td>203.61</td><td>97.19%</td></tr><tr><td><code>flash-attn</code></td><td>190.58</td><td>90.97%</td></tr><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr><tr><td>v5 (better pipelining)</td><td>197.74</td><td>94.39%</td></tr></tbody></table><p>Do note that although I only use Ampere features in these implementations (sm120 supports <code>cp.async.bulk</code> i.e. TMA, but I don’t use it here), my implementations might not run performantly on earlier generations of GPUs. Due to improvements in newer hardware, you might need to use more tricks to reach Speed-of-Light on older GPUs e.g. pipeline shared memory to register memory data movements.</p><h2 id="flash-attention-algorithm">Flash Attention algorithm</h2><p>Let’s start with the reference implementation of attention.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> torch <span>import</span> Tensor
</span></span><span><span>
</span></span><span><span><span>def</span> <span>sdpa</span>(q: Tensor, k: Tensor, v: Tensor):
</span></span><span><span>    <span># q: [B, Lq, DIM]</span>
</span></span><span><span>    <span># k: [B, Lk, DIM]</span>
</span></span><span><span>    <span># v: [B, Lk, DIM]</span>
</span></span><span><span>    D <span>=</span> q<span>.</span>shape[<span>-</span><span>1</span>]
</span></span><span><span>    scale <span>=</span> D <span>**</span> <span>-</span><span>0.5</span>
</span></span><span><span>    attn <span>=</span> (q <span>@</span> k<span>.</span>transpose(<span>-</span><span>1</span>, <span>-</span><span>2</span>)) <span>*</span> scale  <span># [B, Lq, Lk]</span>
</span></span><span><span>    attn <span>=</span> attn<span>.</span>softmax(dim<span>=-</span><span>1</span>)
</span></span><span><span>    out <span>=</span> attn <span>@</span> v  <span># [B, Lq, DIM]</span>
</span></span><span><span>    <span>return</span> out
</span></span></code></pre></div><p>Technically, if the inputs are BF16, some computations should remain in FP32, especially softmax. However, for brevity, we omit them.</p><p>We are implementing the algorithm outlined in the <a href="https://arxiv.org/abs/2307.08691">Flash Attention 2 paper</a>. Each threadblock is responsible for a chunk of Q, and we will iterate along the sequence length of KV. A Python-like outline of the algorithm looks like below (S and P follow Flash Attention notation).</p><div><pre tabindex="0"><code data-lang="python"><span><span>scale <span>=</span> DIM <span>**</span> <span>-</span><span>0.5</span>
</span></span><span><span><span>for</span> b_idx <span>in</span> range(B):
</span></span><span><span>    <span>for</span> tile_Q_idx <span>in</span> range(Lq <span>//</span> BLOCK_Q):
</span></span><span><span>        <span>### start of each threadblock's kernel</span>
</span></span><span><span>        tile_O <span>=</span> torch<span>.</span>zeros(BLOCK_Q, DIM)
</span></span><span><span>        tile_Q <span>=</span> load_Q(b_idx, tile_Q_idx)  <span># [BLOCK_Q, DIM]</span>
</span></span><span><span>
</span></span><span><span>        <span>for</span> tile_KV_idx <span>in</span> range(Lk <span>//</span> BLOCK_KV):
</span></span><span><span>            <span># first MMA: S = Q @ K.T</span>
</span></span><span><span>            <span># (BLOCK_Q, DIM) x (BLOCK_KV, DIM).T -&gt; (BLOCK_Q, BLOCK_KV)</span>
</span></span><span><span>            tile_Q                               <span># (BLOCK_Q, DIM)</span>
</span></span><span><span>            tile_K <span>=</span> load_K(b_idx, tile_KV_idx)  <span># (BLOCK_KV, DIM)</span>
</span></span><span><span>            tile_S <span>=</span> tile_Q <span>@</span> tile_K<span>.</span>T           <span># (BLOCK_Q, BLOCK_KV)</span>
</span></span><span><span>            tile_S <span>=</span> tile_S <span>*</span> scale
</span></span><span><span>
</span></span><span><span>            <span># online softmax and rescale tile_O</span>
</span></span><span><span>            <span>...</span>
</span></span><span><span>
</span></span><span><span>            <span># second MMA: O = P @ V</span>
</span></span><span><span>            <span># (BLOCK_Q, BLOCK_KV) x (BLOCK_KV, DIM) -&gt; (BLOCK_Q, DIM)</span>
</span></span><span><span>            tile_P                               <span># (BLOCK_Q, BLOCK_KV)</span>
</span></span><span><span>            tile_V <span>=</span> load_V(b_idx, tile_KV_idx)  <span># (BLOCK_KV, DIM)</span>
</span></span><span><span>            tile_O <span>+=</span> tile_P <span>@</span> tile_V            <span># (BLOCK_Q, DIM)</span>
</span></span><span><span>
</span></span><span><span>        <span># normalize output and write results</span>
</span></span><span><span>        store_O(b_idx, tile_Q_idx)
</span></span><span><span>        <span>### end of each threadblock's kernel</span>
</span></span></code></pre></div><p>It’s implied <code>DIM</code> is small, so that we can hold <code>tile_Q</code> in register memory throughout the duration of the kernel. This is the reason pretty much all models nowadays use <code>head_dim=128</code>. There are exceptions of course, like <a href="https://arxiv.org/abs/2405.04434">MLA</a>, which uses <code>head_dim=576</code> for Q and K, and <code>head_dim=512</code> for V. Talking about this, I should study <a href="https://github.com/deepseek-ai/FlashMLA">FlashMLA</a> some day.</p><p>Online softmax is quite tricky to explain, so let’s delay the explanation of that part. At the high level, you just need to know that online softmax will transform <code>tile_S</code> to <code>tile_P</code>, and also rescale <code>tile_O</code>.</p><h2 id="version-1---basic-implementation">Version 1 - Basic implementation</h2><p>We will follow the typical MMA flow</p><ul><li>Load a 2D tile of data from global memory to shared memory using <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-cp-async">cp.async</a>. This requires Ampere (sm80 and newer).</li><li>Load data from shared memory to register memory using <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-instructions-ldmatrix">ldmatrix</a>.</li><li>Call <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">mma.m16n8k16</a> for BF16 matrix multiplication (and accumulate).</li></ul><p>I want to focus on implementing the algorithm correctly first, hence I leave out more complicated tricks like shared memory swizzling and pipelining. This reduces the surface area for mistakes, and we will revisit them later for performance optimization.</p><h3 id="global-to-shared-memory-data-transfer">Global to Shared memory data transfer</h3><p>The following templated function does a 2D tile copy from global memory to shared memory.</p><ul><li>Shape of the 2D tile is specified via <code>HEIGHT</code> and <code>WIDTH</code>.</li><li><code>dst</code> is shared memory address, <code>src</code> is global memory address.</li><li>Global memory <code>src</code> is row-major, so <code>src_stride</code> specifies how much to move to the next row.</li><li>Shared memory <code>dst</code> is also row-major, and will be stored as a contiguous block -&gt; <code>dst_stride = WIDTH</code>.</li></ul><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;cuda_bf16.h&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>template</span> <span>&lt;</span><span>int</span> HEIGHT, <span>int</span> WIDTH, <span>int</span> TB_SIZE<span>&gt;</span>
</span></span><span><span>__device__ <span>inline</span>
</span></span><span><span><span>void</span> global_to_shared(<span>uint32_t</span> dst, <span>const</span> nv_bfloat16 <span>*</span>src, <span>int</span> src_stride, <span>int</span> tid) {
</span></span><span><span>  <span>constexpr</span> <span>int</span> num_elems <span>=</span> <span>16</span> <span>/</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>  <span>constexpr</span> <span>int</span> num_iters <span>=</span> HEIGHT <span>*</span> WIDTH <span>/</span> (TB_SIZE <span>*</span> num_elems);
</span></span><span><span>
</span></span><span><span>  <span>for</span> (<span>int</span> iter <span>=</span> <span>0</span>; iter <span>&lt;</span> num_iters; iter<span>++</span>) {
</span></span><span><span>    <span>const</span> <span>int</span> idx <span>=</span> (iter <span>*</span> TB_SIZE <span>+</span> tid) <span>*</span> num_elems;
</span></span><span><span>    <span>const</span> <span>int</span> row <span>=</span> idx <span>/</span> WIDTH;
</span></span><span><span>    <span>const</span> <span>int</span> col <span>=</span> idx <span>%</span> WIDTH;
</span></span><span><span>
</span></span><span><span>    <span>const</span> <span>uint32_t</span> dst_addr <span>=</span> dst <span>+</span> (row <span>*</span> WIDTH <span>+</span> col) <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>    <span>const</span> nv_bfloat16 <span>*</span>src_addr <span>=</span> src <span>+</span> (row <span>*</span> src_stride <span>+</span> col);
</span></span><span><span>    <span>asm</span> <span>volatile</span>(<span>"cp.async.cg.shared.global [%0], [%1], 16;"</span> <span>::</span> <span>"r"</span>(dst_addr), <span>"l"</span>(src_addr));
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><figure><img src="https://gau-nernst.github.io/fa-5090/global_to_shared.svg" alt="Global to Shared data transfer"><figcaption><p>2D tile copy from Global memory to Shared memory.</p></figcaption></figure><p>We will use inline assembly to write <code>cp.async.cg.shared.global</code>. This PTX does 16-byte transfer, or 8 BF16 elements (<code>num_elems = 16 / sizeof(nv_bfloat16)</code>), for each CUDA thread. To ensure coalesced memory access, consecutive threads will be responsible for consecutive groups of 8xBF16.</p><figure><img src="https://gau-nernst.github.io/fa-5090/coalesced.svg" alt="Coalesced memory access"><figcaption><p>Consecutive threads are responsible for consecutive groups of 8xBF16.</p></figcaption></figure><p>Note:</p><ul><li>The loop <code>for (int iter = 0; iter &lt; num_iters; iter++)</code> is written this way so that the compiler (<code>nvcc</code>) can fully unroll the loop. <code>num_iters</code> is known at compile time (guaranteed by <code>constexpr</code>). If we mix <code>tid</code> in the loop, which is a “dynamic” variable to the compiler, the loop can’t be unrolled, even when we know certain constraints about the variable i.e. <code>tid &lt; TB_SIZE</code>.</li><li>Data type of shared memory pointer <code>dst</code> is <code>uint32_t</code>. This is intentional. Pretty much all PTX instructions expect shared memory addresses to be in <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#state-spaces">shared state space</a>. We can convert C++ pointers, which are generic addresses, to shared state space addresses with <code>static_cast&lt;uint32_t&gt;(__cvta_generic_to_shared(ptr))</code>. This is done outside of <code>global_to_shared()</code>.</li></ul><p>To finish using <code>cp.async</code>, we also need to add the following:</p><ul><li><code>cp.async.commit_group</code> (PTX): commit all previously issued <code>cp.async</code> instructions into a <strong><code>cp.async</code> group</strong>. This group will be the unit for synchronization.</li><li><code>cp.async.wait_all</code> (PTX): wait for all committed groups to finish.</li><li><code>__syncthreads()</code>: make sure all threads (in a threadblock) reach here before reading the loaded data in shared memory (because one thread may read data loaded by another thread). More importantly, this broadcasts <strong>visibility</strong> of the new data to all threads in the threadblock. Without <code>__syncthreads()</code>, the compiler is free to optimize away memory accesses.</li></ul><p>As always, refer to <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/">PTX doc</a> for more information about the instructions. Basically we issue multiple <code>cp.async</code> and wait for them to complete immediately right after. <code>commit_group</code> and <code>wait_group</code> provide a mechanism for us to implement pipelining later. But for now, just need to know we have to write it that way to use <code>cp.async</code>.</p><p>Our code snippet would look something like this.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// nv_bfloat16 *Q;
</span></span></span><span><span><span>// uint32_t Q_smem;
</span></span></span><span><span><span>// const int tid = blockIdx.x;
</span></span></span><span><span><span>// constexpr int TB_SIZE = 32 * 4;
</span></span></span><span><span><span>// constexpr int DIM = 128;
</span></span></span><span><span><span></span>
</span></span><span><span>global_to_shared<span>&lt;</span>BLOCK_Q, DIM, TB_SIZE<span>&gt;</span>(Q_smem, Q, DIM, tid);
</span></span><span><span><span>asm</span> <span>volatile</span>(<span>"cp.async.commit_group;"</span>);
</span></span><span><span><span>asm</span> <span>volatile</span>(<span>"cp.async.wait_all;"</span>);
</span></span><span><span>__syncthreads();
</span></span></code></pre></div><h3 id="shared-memory-to-register-memory-data-transfer">Shared memory to Register memory data transfer</h3><p>When doing global-&gt;shared data transfer, we think in terms of threadblock tiles and individual CUDA threads. For shared-&gt;register data transfer, since this is to service later MMA instructions, we think in terms of warp tiles/MMA tiles and warps. Following Flash Attention 2 (section 3.3), we let each warp in a threadblock handle a portion of <code>tile_Q</code>, splitting along the Q sequence length dimension. This means that different warps will index into different chunks of <code>tile_Q</code>, but they all index to the same <code>tile_K</code> and <code>tile_V</code> chunks in the KV-sequence-length loop.</p><figure><img src="https://gau-nernst.github.io/fa-5090/fa_warp_partition.svg" alt="Flash Attention warp partition"><figcaption><p>Warp partition in Flash Attention 2.</p></figcaption></figure><p>Since we are using <code>mma.m16n8k16</code> instruction, each MMA 16x8 output tile (<code>m16n8</code>) requires 16x16 A tile (<code>m16k16</code>) and 8x16 B tile (<code>n8k16</code>). <code>ldmatrix</code> can load one, two, or four 8x8 tile(s) of 16-bit elements. Hence,</p><ul><li>A tile <code>m16k16</code> requires four 8x8 tiles -&gt; <code>ldmatrix.x4</code></li><li>B tile <code>n8k16</code> requires two 8x8 tiles -&gt; <code>ldmatrix.x2</code></li></ul><p>Only Q acts as A in an MMA. Both K and V act as B in their MMAs, though K will require transposed <code>ldmatrix</code> for correct layout (all tensors use row-major layout in global and shared memory).</p><p>To use <code>ldmatrix</code>, each thread supplies address of a row. Threads 0-7 select the 1st 8x8 tile, threads 8-15 select the 2nd 8x8 tile, and so on. The <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">layout of A</a> in the official PTX documentation can look confusing. But it’s easier (at least for me) to focus on the order of 8x8 tiles within an MMA tile.</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix.svg" alt="ldmatrix for MMA layout"><figcaption><p>Order of <code>ldmatrix</code> tiles in <code>mma.m16n8k16</code>.</p></figcaption></figure><p>With the visualisation above, I hope the following snippet makes sense</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>constexpr</span> <span>int</span> MMA_M <span>=</span> <span>16</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_N <span>=</span> <span>8</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_K <span>=</span> <span>16</span>;
</span></span><span><span>
</span></span><span><span><span>uint32_t</span> Q_smem;
</span></span><span><span><span>uint32_t</span> Q_rmem[WARP_Q <span>/</span> MMA_M][DIM <span>/</span> MMA_K][<span>4</span>];
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>  <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>) {
</span></span><span><span>    <span>const</span> <span>int</span> row <span>=</span> (warp_id <span>*</span> WARP_Q) <span>+</span> (mma_id_q <span>*</span> MMA_M) <span>+</span> (lane_id <span>%</span> <span>16</span>);
</span></span><span><span>    <span>const</span> <span>int</span> col <span>=</span> (mma_id_d <span>*</span> MMA_K) <span>+</span> (lane_id <span>/</span> <span>16</span> <span>*</span> <span>8</span>);
</span></span><span><span>    <span>const</span> <span>uint32_t</span> addr <span>=</span> Q_smem <span>+</span> (row <span>*</span> DIM <span>+</span> col) <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>    ldmatrix_x4(Q_rmem[mma_id_q][mma_id_d], addr);
</span></span><span><span>  }
</span></span></code></pre></div><ul><li>The two nested loops tile <code>[MMA_M, MMA_K]</code> (i.e. <code>[16, 16]</code>) over <code>[WARP_Q, DIM]</code> in shared memory.</li><li><code>(warp_id * WARP_Q)</code> selects the warp tile. We don’t need this for K and V.</li><li><code>(mma_id_q * MMA_M)</code> in <code>row</code> and <code>(mma_id_d * MMA_K)</code> in <code>col</code> selects the MMA tile.</li><li><code>(lane_id % 16)</code> in <code>row</code> and <code>(lane_id / 16 * 8)</code> in <code>col</code> select the correct row address for each thread, following the required Multiplicand A layout (see the figure above).</li></ul><p><code>ldmatrix_x4()</code> is a small wrapper around <code>ldmatrix.sync.aligned.m8n8.x4.b16</code> PTX for convenience. You can refer to <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/common.h">common.h</a> for more details.</p><p>K and V can be loaded from shared to register memory similarly. One thing to note is about the row-major / column-major layout when using <code>ldmatrix</code>. Regardless of whether <code>.trans</code> modifier is used, each thread still provides the row address of each row in 8x8 tiles. <code>.trans</code> only changes the <strong>register layout</strong> of <code>ldmatrix</code> results.</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix_kv.svg" alt="ldmatrix for K and V"><figcaption><p>Use transposed version of <code>ldmatrix</code> for V.</p></figcaption></figure><p>One trick to know whether to use the transposed version of <code>ldmatrix</code> is to look at the K-dim or the reduction dimension. The 1st MMA’s K-dim is along <code>DIM</code> dimension, while the 2nd MMA’s K-dim is along the <code>BLOCK_KV</code> dimension.</p><h3 id="draft-version">Draft version</h3><p>We have the high-level tile-based design, and know how to load the data for MMA. Calling MMA is simple - just drop <code>mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32</code> PTX in our code. Our draft version looks like this.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>constexpr</span> <span>int</span> BLOCK_Q <span>=</span> <span>128</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> BLOCK_KV <span>=</span> <span>64</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> DIM <span>=</span> <span>128</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> NUM_WARPS <span>=</span> <span>4</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> TB_SIZE <span>=</span> NUM_WARPS <span>*</span> <span>32</span>;
</span></span><span><span>
</span></span><span><span><span>// mma.m16n8k16
</span></span></span><span><span><span></span><span>constexpr</span> <span>int</span> MMA_M <span>=</span> <span>16</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_N <span>=</span> <span>8</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_K <span>=</span> <span>16</span>;
</span></span><span><span>
</span></span><span><span>__global__
</span></span><span><span><span>void</span> <span>attention_v1_kernel</span>(
</span></span><span><span>  <span>const</span> nv_bfloat16 <span>*</span>Q,  <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>K,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>V,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  nv_bfloat16 <span>*</span>O,        <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>int</span> bs,
</span></span><span><span>  <span>int</span> len_q,
</span></span><span><span>  <span>int</span> len_kv) {
</span></span><span><span>
</span></span><span><span>  <span>// basic setup
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> tid <span>=</span> threadIdx.x;
</span></span><span><span>  <span>const</span> <span>int</span> warp_id <span>=</span> tid <span>/</span> <span>32</span>;
</span></span><span><span>  <span>const</span> <span>int</span> lane_id <span>=</span> tid <span>%</span> <span>32</span>;
</span></span><span><span>
</span></span><span><span>  <span>// increment Q, K, V, O based on blockIdx.x
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// set up shared memory
</span></span></span><span><span><span></span>  <span>// Q_smem is overlapped with (K_smem + V_smem), since we only use Q_smem once
</span></span></span><span><span><span></span>  <span>extern</span> __shared__ <span>uint8_t</span> smem[];
</span></span><span><span>  <span>const</span> <span>uint32_t</span> Q_smem <span>=</span> __cvta_generic_to_shared(smem);
</span></span><span><span>  <span>const</span> <span>uint32_t</span> K_smem <span>=</span> Q_smem;
</span></span><span><span>  <span>const</span> <span>uint32_t</span> V_smem <span>=</span> K_smem <span>+</span> BLOCK_KV <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>
</span></span><span><span>  <span>// FA2: shard BLOCK_Q among warps
</span></span></span><span><span><span></span>  <span>constexpr</span> <span>int</span> WARP_Q <span>=</span> BLOCK_Q <span>/</span> NUM_WARPS;
</span></span><span><span>
</span></span><span><span>  <span>// set up register memory
</span></span></span><span><span><span></span>  <span>uint32_t</span> Q_rmem[WARP_Q <span>/</span> MMA_M][DIM <span>/</span> MMA_K][<span>4</span>];       <span>// act as A in MMA
</span></span></span><span><span><span></span>  <span>uint32_t</span> K_rmem[BLOCK_KV <span>/</span> MMA_N][DIM <span>/</span> MMA_K][<span>2</span>];     <span>// act as B in MMA
</span></span></span><span><span><span></span>  <span>uint32_t</span> P_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_K][<span>4</span>];  <span>// act as A in MMA
</span></span></span><span><span><span></span>  <span>uint32_t</span> V_rmem[BLOCK_KV <span>/</span> MMA_K][DIM <span>/</span> MMA_N][<span>2</span>];     <span>// act as B in MMA
</span></span></span><span><span><span></span>  <span>float</span> O_rmem[WARP_Q <span>/</span> MMA_M][DIM <span>/</span> MMA_N][<span>4</span>];          <span>// act as C/D in MMA
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Q global-&gt;shared [BLOCK_Q, DIM]
</span></span></span><span><span><span></span>  global_to_shared<span>&lt;</span>BLOCK_Q, DIM, TB_SIZE<span>&gt;</span>(Q_smem, Q, DIM, tid);
</span></span><span><span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.commit_group;"</span>);
</span></span><span><span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.wait_all;"</span>);
</span></span><span><span>  __syncthreads();
</span></span><span><span>
</span></span><span><span>  <span>// Q shared-&gt;register. select the correct warp tile
</span></span></span><span><span><span></span>  <span>// Q stays in registers throughout the kernel's lifetime
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>) {
</span></span><span><span>      <span>const</span> <span>int</span> row <span>=</span> warp_id <span>*</span> WARP_Q <span>+</span> mma_id_q <span>*</span> MMA_M <span>+</span> (lane_id <span>%</span> <span>16</span>);
</span></span><span><span>      <span>const</span> <span>int</span> col <span>=</span> mma_id_d <span>*</span> MMA_K <span>+</span> (lane_id <span>/</span> <span>16</span> <span>*</span> <span>8</span>);
</span></span><span><span>      <span>const</span> <span>uint32_t</span> addr <span>=</span> Q_smem <span>+</span> (row <span>*</span> DIM <span>+</span> col) <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>      ldmatrix_x4(Q_rmem[mma_id_q][mma_id_d], addr);
</span></span><span><span>    }
</span></span><span><span>  __syncthreads();
</span></span><span><span>
</span></span><span><span>  <span>// main loop
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> num_kv_iters <span>=</span> len_kv <span>/</span> BLOCK_KV;
</span></span><span><span>  <span>for</span> (<span>int</span> kv_idx <span>=</span> <span>0</span>; kv_idx <span>&lt;</span> num_kv_iters; kv_idx<span>++</span>) {
</span></span><span><span>    <span>// accumulator for the 1st MMA. reset to zeros
</span></span></span><span><span><span></span>    <span>float</span> S_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_N][<span>4</span>] <span>=</span> {};  <span>// act as C/D in MMA
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>// load K global-&gt;shared-&gt;registers [BLOCK_KV, DIM]
</span></span></span><span><span><span></span>    <span>// similar to loading Q, except we use ldmatrix_x2()
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// 1st MMA: S = Q @ K.T
</span></span></span><span><span><span></span>    <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>      <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>        <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>)
</span></span><span><span>          mma_m16n8k16(Q_rmem[mma_id_q][mma_id_d],
</span></span><span><span>                       K_rmem[mma_id_kv][mma_id_d],
</span></span><span><span>                       S_rmem[mma_id_q][mma_id_kv]);
</span></span><span><span>
</span></span><span><span>    <span>// online softmax. we will touch on this later
</span></span></span><span><span><span></span>    <span>// also pack S_rmem to P_rmem for the 2nd MMA
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// load V global-&gt;shared-&gt;registers [BLOCK_KV, DIM]
</span></span></span><span><span><span></span>    <span>// similar to loading K, except we use ldmatrix_x2_trans()
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// 2nd MMA: O += P @ V
</span></span></span><span><span><span></span>    <span>// similar to the 1st MMA
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// increment pointer to the next KV block
</span></span></span><span><span><span></span>    K <span>+=</span> BLOCK_KV <span>*</span> DIM;
</span></span><span><span>    V <span>+=</span> BLOCK_KV <span>*</span> DIM;
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>  <span>// write output
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_N; mma_id_d<span>++</span>) {
</span></span><span><span>      <span>const</span> <span>int</span> row <span>=</span> warp_id <span>*</span> WARP_Q <span>+</span> mma_id_q <span>*</span> MMA_M <span>+</span> (lane_id <span>/</span> <span>4</span>);
</span></span><span><span>      <span>const</span> <span>int</span> col <span>=</span> mma_id_d <span>*</span> MMA_N <span>+</span> (lane_id <span>%</span> <span>4</span>) <span>*</span> <span>2</span>;
</span></span><span><span>
</span></span><span><span>      <span>float</span> <span>*</span>regs <span>=</span> O_rmem[mma_id_q][mma_id_d];
</span></span><span><span>      <span>reinterpret_cast</span><span>&lt;</span>nv_bfloat162 <span>*&gt;</span>(O <span>+</span> (row <span>+</span> <span>0</span>) <span>*</span> DIM <span>+</span> col)[<span>0</span>] <span>=</span> __float22bfloat162_rn({regs[<span>0</span>], regs[<span>1</span>]});
</span></span><span><span>      <span>reinterpret_cast</span><span>&lt;</span>nv_bfloat162 <span>*&gt;</span>(O <span>+</span> (row <span>+</span> <span>8</span>) <span>*</span> DIM <span>+</span> col)[<span>0</span>] <span>=</span> __float22bfloat162_rn({regs[<span>2</span>], regs[<span>3</span>]});
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// kernel launcher
</span></span></span><span><span><span></span><span>void</span> <span>attention_v1</span>(
</span></span><span><span>  <span>const</span> nv_bfloat16 <span>*</span>Q,  <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>K,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>V,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  nv_bfloat16 <span>*</span>O,        <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>int</span> bs,
</span></span><span><span>  <span>int</span> len_q,
</span></span><span><span>  <span>int</span> len_kv) {
</span></span><span><span>
</span></span><span><span>  <span>// 1 threadblock for each BLOCK_Q
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> num_blocks <span>=</span> bs <span>*</span> cdiv(len_q, BLOCK_Q);
</span></span><span><span>
</span></span><span><span>  <span>// Q overlap with K+V.
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> smem_size <span>=</span> max(BLOCK_Q, BLOCK_KV <span>*</span> <span>2</span>) <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>
</span></span><span><span>  <span>// use dynamic shared memory so we can allocate more than 48kb if needed.
</span></span></span><span><span><span></span>  <span>if</span> (smem_size <span>&gt;</span> <span>48'000</span>)
</span></span><span><span>    CUDA_CHECK(cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
</span></span><span><span>
</span></span><span><span>  attention_v1_kernel<span>&lt;&lt;&lt;</span>num_blocks, TB_SIZE, smem_size<span>&gt;&gt;&gt;</span>(Q, K, V, O, bs, len_q, len_kv);
</span></span><span><span>  CUDA_CHECK(cudaGetLastError());
</span></span><span><span>}
</span></span></code></pre></div><p>Now, let’s tackle online softmax.</p><h3 id="online-softmax---theory">Online softmax - Theory</h3><p>For the original explanation, you can refer to <a href="https://arxiv.org/abs/1805.02867">Online normalizer calculation for softmax</a> and Flash Attention 2 paper.</p><p>We have the following mathematical definition of softmax. For each row with length $L_{kv}$</p><p>$$
p_l = \frac{\exp(s_l-m)}{\exp(s_0-m) + \exp(s_1-m) + \dots + \exp(s_{L_{kv}-1}-m)}
$$
$$
l\in[0,L_{kv})
$$
$$
m=\max(s_0,s_1,\dots,s_{L_{kv}-1})
$$</p><p>$-m$ is max subtraction to improve numerical stability ($\exp(\cdot)$ can easily explode if its input is large). Let’s bring out the denominator normalizer and write the whole row as a vector.</p><p>$$
\vec P =
\begin{bmatrix}
p_0 \\
\vdots \\
p_{L_{kv}-1}
\end{bmatrix}
= \frac{1}{\sum_{l\in[0,L_{kv})}\exp(s_l-m)}
\begin{bmatrix}
\exp(s_0-m) \\
\vdots \\
\exp(s_{L_{kv}-1}-m)
\end{bmatrix}
$$</p><p>In our 2nd matmul <code>O += P @ V</code>, each row of P (softmax output) is dot-producted with the corresponding column of V.</p><p>$$
o=\vec P \cdot \vec V = \frac{1}{\sum_{l\in[0,L_{kv})}\exp(s_l-m)} \sum_{l\in[0,L_{kv})}\exp(s_l-m) \cdot v_l
$$</p><p>The extra dot-product is a blessing in disguise - we no longer need individual elements in a row for the final result. This enables Flash Attention to compute attention in one pass. To see it more clearly, let’s consider the iterative process of adding a new element during online computation.</p><p>$$
o_{[0,L)} = \frac{1}{\sum_{l\in[0,L)}\exp(s_l-m_{[0,L)})} \sum_{l\in[0,L)}\exp(s_l-m_{[0,L)}) \cdot v_l
$$
$$
m_{[0,L)}=\max(s_0,s_1,\dots,s_{L-1})
$$</p><p>I’m abusing the notation here, but I hope I get the idea across. When we add a new element $s_{L+1}$</p><p>$$
o_{[0,L+1)} = \frac{1}{\sum_{l\in[0,L+1)}\exp(s_l-m_{[0,L+1)})} \sum_{l\in[0,L+1)}\exp(s_l-m_{[0,L+1)}) \cdot v_l
$$</p><p>Look at the normalizer (denominator)</p><p>$$
\sum_{l\in[0,L+1)}\exp(s_l-m_{[0,L+1)}) = \colorbox{red}{$\displaystyle\exp(m_{[0,L)}-m_{[0,L+1)})$}\colorbox{orange}{$\displaystyle\sum_{l\in[0,L)}\exp(s_l-m_{[0,L)})$} + \colorbox{lime}{$\displaystyle\exp(s_L-m_{[0,L+1)})$}
$$</p><p>The equation means that we only need to $\colorbox{red}{rescale}$ the $\colorbox{orange}{previous normalizer}$ before adding the $\colorbox{lime}{new term}$. The same logic can be applied for the dot product with V (unnormalized output). <strong>This is the key idea of online softmax and Flash Attention</strong>.</p><p>Define <strong>attention state</strong></p><p>$$
\begin{bmatrix}
m \\
\tilde{o} \\
\mathrm{sumexp}
\end{bmatrix}
$$</p><p>where $m$ is the max of elements seen so far, $\tilde{o}$ is the <strong>unnormalized</strong> output, and $\mathrm{sumexp}$ is the normalizer. We need $m$ to compute the rescaling factor as seen above.</p><p>You can convince yourself that updating attention state is an <strong>associative</strong> operation - it does not matter the order in which elements are used to update the attention state.</p><p>$$
\begin{aligned}
\begin{bmatrix}
m_1 \\
\tilde{o}_1 \\
\mathrm{sumexp}_1
\end{bmatrix}
\oplus \begin{bmatrix}
m_2 \\
\tilde{o}_2 \\
\mathrm{sumexp}_2
\end{bmatrix}
&amp;= \begin{bmatrix}
m_3 \\
\tilde{o}_3 \\
\mathrm{sumexp}_3
\end{bmatrix} \\
&amp;= \begin{bmatrix}
\max(m_1,m_2) \\
\exp(m_1-m_3)\tilde{o}_1+\exp(m_2-m_3)\tilde{o}_2 \\
\exp(m_1-m_3)\mathrm{sumexp}_1+\exp(m_2-m_3)\mathrm{sumexp}_2
\end{bmatrix}
\end{aligned}
$$</p><p>This associative property enables things like <a href="https://pytorch.org/blog/flash-decoding/">Flash Decoding</a>, a split-K version of attention.</p><h3 id="online-softmax---implementation">Online softmax - Implementation</h3><p>We can now fill in the gap of online softmax in our high-level Python implementation.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># attention state</span>
</span></span><span><span>m <span>=</span> torch<span>.</span>zeros(BLOCK_Q)
</span></span><span><span>tile_O <span>=</span> torch<span>.</span>zeros(BLOCK_Q, DIM)
</span></span><span><span>sumexp <span>=</span> torch<span>.</span>zeros(BLOCK_Q)
</span></span><span><span>
</span></span><span><span><span>for</span> _ <span>in</span> range(Lk <span>//</span> BLOCK_KV):
</span></span><span><span>  <span># 1st MMA</span>
</span></span><span><span>  tile_S <span>=</span> tile_Q <span>@</span> tile_K<span>.</span>T  <span># [BLOCK_Q, BLOCK_KV]</span>
</span></span><span><span>  tile_S <span>=</span> tile_S <span>*</span> scale
</span></span><span><span>
</span></span><span><span>  <span># online softmax</span>
</span></span><span><span>  tile_max <span>=</span> tile_S<span>.</span>amax(dim<span>=-</span><span>1</span>)  <span># [BLOCK_Q]</span>
</span></span><span><span>  new_m <span>=</span> torch<span>.</span>maximum(m, tile_max)
</span></span><span><span>  tile_P <span>=</span> torch<span>.</span>exp(tile_S <span>-</span> new_m<span>.</span>unsqueeze(<span>-</span><span>1</span>))
</span></span><span><span>
</span></span><span><span>  <span># rescale</span>
</span></span><span><span>  scale <span>=</span> torch<span>.</span>exp(m <span>-</span> new_m)
</span></span><span><span>  tile_O <span>*=</span> scale<span>.</span>unsqueeze(<span>-</span><span>1</span>)
</span></span><span><span>  sumexp <span>=</span> sumexp <span>*</span> scale <span>+</span> tile_P<span>.</span>sum(dim<span>=-</span><span>1</span>)
</span></span><span><span>  m <span>=</span> new_m  <span># save new max</span>
</span></span><span><span>
</span></span><span><span>  <span># 2nd MMA</span>
</span></span><span><span>  tile_O <span>+=</span> tile_P <span>@</span> tile_V  <span># [BLOCK_Q, DIM]</span>
</span></span><span><span>
</span></span><span><span><span># apply normalization</span>
</span></span><span><span>tile_O <span>/=</span> sumexp<span>.</span>unsqueeze(<span>-</span><span>1</span>)
</span></span></code></pre></div><h4 id="row-max">Row max</h4><p>When translating this to CUDA C++, the most tricky part is to wrap our head around MMA layout. Let’s start with <code>tile_S</code>.</p><figure><img src="https://docs.nvidia.com/cuda/parallel-thread-execution/_images/mma-16816-C-f16.png" alt="MMA m16n8k16 output layout"><figcaption><p>Thread and register layout of MMA m16n8k16 output. Source: <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">NVIDIA PTX doc</a>.</p></figcaption></figure><p>Softmax scale applies the same scaling for all elements, so that is trivial. Next, we need to compute row max for the current tile. Remember that we allocate the registers for <code>tile_S</code> this way.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>float</span> S_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_N][<span>4</span>];
</span></span></code></pre></div><p><code>4</code> means <code>c0,c1,c2,c3</code> in the figure above i.e. each thread holds 2 consecutive elements from 2 rows. To do reduction within a row (of an MMA output tile), we do reduction for 2 consecutive elements held by a thread, then reduction within a group of 4 threads i.e. <code>T0-T3</code>, <code>T4-T7</code>, and so on. However, the row reduction is actually within the whole <code>tile_S</code>, hence we also need to loop over <code>BLOCK_KV / MMA_N</code> of <code>S_rmem</code>. This can be combined with thread-level reduction before 4-thread reduction.</p><figure><img src="https://gau-nernst.github.io/fa-5090/row_reduction.svg" alt="Row reduction"><figcaption><p>Perform row reduction on MMA output.</p></figcaption></figure><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// initial attention state
</span></span></span><span><span><span></span><span>float</span> rowmax[WARP_Q <span>/</span> MMA_M][<span>2</span>];
</span></span><span><span><span>float</span> rowsumexp[WARP_Q <span>/</span> MMA_M][<span>2</span>] <span>=</span> {};
</span></span><span><span><span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>) {
</span></span><span><span>  rowmax[mma_id_q][<span>0</span>] <span>=</span> <span>-</span>FLT_MAX;
</span></span><span><span>  rowmax[mma_id_q][<span>1</span>] <span>=</span> <span>-</span>FLT_MAX;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// main loop
</span></span></span><span><span><span></span><span>const</span> <span>int</span> num_kv_iters <span>=</span> len_kv <span>/</span> BLOCK_KV;
</span></span><span><span><span>for</span> (<span>int</span> kv_idx <span>=</span> <span>0</span>; kv_idx <span>&lt;</span> num_kv_iters; kv_idx<span>++</span>) {
</span></span><span><span>  <span>// tile_S = tile_Q @ tile_K.T
</span></span></span><span><span><span></span>  S_rmem[][] <span>=</span> ...
</span></span><span><span>
</span></span><span><span>  <span>// loop over rows
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>) {
</span></span><span><span>    <span>// apply softmax scale
</span></span></span><span><span><span></span>    <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>      <span>for</span> (<span>int</span> reg_id <span>=</span> <span>0</span>; reg_id <span>&lt;</span> <span>4</span>; reg_id<span>++</span>)
</span></span><span><span>        S_rmem[mma_id_q][mma_id_kv][reg_id] <span>*=</span> softmax_scale;
</span></span><span><span>
</span></span><span><span>    <span>// rowmax
</span></span></span><span><span><span></span>    <span>float</span> this_rowmax[<span>2</span>] <span>=</span> {<span>-</span>FLT_MAX, <span>-</span>FLT_MAX};
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>) {
</span></span><span><span>      <span>float</span> <span>*</span>regs <span>=</span> S_rmem[mma_id_q][mma_id_kv];
</span></span><span><span>      this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], max(regs[<span>0</span>], regs[<span>1</span>]));  <span>// c0 and c1
</span></span></span><span><span><span></span>      this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], max(regs[<span>2</span>], regs[<span>3</span>]));  <span>// c2 and c3
</span></span></span><span><span><span></span>    }
</span></span><span><span>
</span></span><span><span>    <span>// butterfly reduction within 4 threads
</span></span></span><span><span><span></span>    this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>0</span>], <span>1</span>));
</span></span><span><span>    this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>0</span>], <span>2</span>));
</span></span><span><span>    this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>1</span>], <span>1</span>));
</span></span><span><span>    this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>1</span>], <span>2</span>));
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>In a typical reduction kernel, when there are only 32 active threads left, we can use warp shuffle <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">__shfl_down_sync()</a> to copy data from higher lanes to lower lanes, and the final result is stored in thread 0. In this case, since we need the max value to be shared among the 4 threads in a group (for max subtraction later), we can use <code>__shfl_xor_sync()</code> to avoid an additional broadcast step.</p><figure><img src="https://gau-nernst.github.io/fa-5090/butterfly_reduction.svg" alt="Butterfly reduction"><figcaption><p>Butterfly reduction within 4 threads using __shfl_xor_sync().</p></figcaption></figure><h4 id="rescaling">Rescaling</h4><p>With row max of the new tile, we can compute rescaling factor for (unnormalized) output as well as normalizer (sumexp of each row).</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// new rowmax
</span></span></span><span><span><span></span>this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], rowmax[mma_id_q][<span>0</span>]);
</span></span><span><span>this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], rowmax[mma_id_q][<span>1</span>]);
</span></span><span><span>
</span></span><span><span><span>// rescale for previous O
</span></span></span><span><span><span></span><span>float</span> rescale[<span>2</span>];
</span></span><span><span>rescale[<span>0</span>] <span>=</span> __expf(rowmax[mma_id_q][<span>0</span>] <span>-</span> this_rowmax[<span>0</span>]);
</span></span><span><span>rescale[<span>1</span>] <span>=</span> __expf(rowmax[mma_id_q][<span>1</span>] <span>-</span> this_rowmax[<span>1</span>]);
</span></span><span><span><span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_N; mma_id_d<span>++</span>) {
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>0</span>] <span>*=</span> rescale[<span>0</span>];
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>1</span>] <span>*=</span> rescale[<span>0</span>];
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>2</span>] <span>*=</span> rescale[<span>1</span>];
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>3</span>] <span>*=</span> rescale[<span>1</span>];
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// save new rowmax
</span></span></span><span><span><span></span>rowmax[mma_id_q][<span>0</span>] <span>=</span> this_rowmax[<span>0</span>];
</span></span><span><span>rowmax[mma_id_q][<span>1</span>] <span>=</span> this_rowmax[<span>1</span>];
</span></span></code></pre></div><p>We don’t rescale <code>rowsumexp</code> here because we want to fuse it with addition of the new sumexp term later i.e. FMA - fused multiply add. We can’t fuse multiplication with MMA, hence we need to do a separate multiplication for <code>O_rmem</code>.</p><h4 id="pack-tile_s-to-tile_p-and-compute-row-sum-exp">Pack <code>tile_S</code> to <code>tile_P</code> (and compute row sum exp)</h4><p>For the next part, we will loop over the row dimension again (<code>BLOCK_KV / MMA_N</code>), to compute and pack <code>tile_P</code> from <code>tile_S</code>, as well as doing reduction for sumexp. Recall that we declare registers for <code>S</code> and <code>P</code> as follows.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>float</span> S_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_N][<span>4</span>]      <span>// m16n8
</span></span></span><span><span><span></span><span>uint32_t</span> P_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_K][<span>4</span>];  <span>// m16k16
</span></span></span></code></pre></div><p>Look up the thread/register layout for MMA multiplicand A and output C/D again in PTX doc. Luckily, the layouts are exactly the same - within an 8x8 tile, the arrangement of elements is identical.</p><figure><img src="https://gau-nernst.github.io/fa-5090/m16n8_to_m16k16.svg" alt="Register layout of MMA m16n8k16"><figcaption><p>The left half of multiplicand A has the same layout as accumulator. Source: <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">NVIDIA PTX doc</a>.</p></figcaption></figure><p>It means that for all threads, every 2 floats in <code>S_rmem</code> can be packed as BF16x2 in a single 32-bit register of <code>P_rmem</code>, exactly how <code>mma.m16n8k16</code> expects for the 2nd MMA. There are no data movements across threads. Note that this is not always true: if we use INT8 or FP8 MMA for the 1st and/or 2nd MMA, we would need to permute data across threads to pack <code>tile_S</code> to <code>tile_P</code>.</p><p>Our code for the last part of online softmax is below.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// rowsumexp
</span></span></span><span><span><span></span><span>float</span> this_rowsumexp[<span>2</span>] <span>=</span> {};
</span></span><span><span><span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>) {
</span></span><span><span>  <span>float</span> <span>*</span>regs <span>=</span> S_rmem[mma_id_q][mma_id_kv];
</span></span><span><span>  regs[<span>0</span>] <span>=</span> __expf(regs[<span>0</span>] <span>-</span> rowmax[mma_id_q][<span>0</span>]);  <span>// c0
</span></span></span><span><span><span></span>  regs[<span>1</span>] <span>=</span> __expf(regs[<span>1</span>] <span>-</span> rowmax[mma_id_q][<span>0</span>]);  <span>// c1
</span></span></span><span><span><span></span>  regs[<span>2</span>] <span>=</span> __expf(regs[<span>2</span>] <span>-</span> rowmax[mma_id_q][<span>1</span>]);  <span>// c2
</span></span></span><span><span><span></span>  regs[<span>3</span>] <span>=</span> __expf(regs[<span>3</span>] <span>-</span> rowmax[mma_id_q][<span>1</span>]);  <span>// c3
</span></span></span><span><span><span></span>
</span></span><span><span>  this_rowsumexp[<span>0</span>] <span>+=</span> regs[<span>0</span>] <span>+</span> regs[<span>1</span>];
</span></span><span><span>  this_rowsumexp[<span>1</span>] <span>+=</span> regs[<span>2</span>] <span>+</span> regs[<span>3</span>];
</span></span><span><span>
</span></span><span><span>  <span>// pack to P registers for next MMA
</span></span></span><span><span><span></span>  <span>// we need to change from m16n8 to m16k16
</span></span></span><span><span><span></span>  <span>// each iteration of this loop packs half of m16k16
</span></span></span><span><span><span></span>  nv_bfloat162 <span>*</span>this_P_rmem <span>=</span> <span>reinterpret_cast</span><span>&lt;</span>nv_bfloat162 <span>*&gt;</span>(P_rmem[mma_id_q][mma_id_kv <span>/</span> <span>2</span>]);
</span></span><span><span>  this_P_rmem[(mma_id_kv <span>%</span> <span>2</span>) <span>*</span> <span>2</span>]     <span>=</span> __float22bfloat162_rn({regs[<span>0</span>], regs[<span>1</span>]});  <span>// top row
</span></span></span><span><span><span></span>  this_P_rmem[(mma_id_kv <span>%</span> <span>2</span>) <span>*</span> <span>2</span> <span>+</span> <span>1</span>] <span>=</span> __float22bfloat162_rn({regs[<span>2</span>], regs[<span>3</span>]});  <span>// bottom row
</span></span></span><span><span><span></span>}
</span></span><span><span>
</span></span><span><span><span>// butterfly reduction on this_rowsumexp[2]
</span></span></span><span><span><span></span>...
</span></span><span><span>
</span></span><span><span><span>// accumulate to total rowsumexp using FMA
</span></span></span><span><span><span></span>rowsumexp[mma_id_q][<span>0</span>] <span>=</span> rowsumexp[mma_id_q][<span>0</span>] <span>*</span> rescale[<span>0</span>] <span>+</span> this_rowsumexp[<span>0</span>];
</span></span><span><span>rowsumexp[mma_id_q][<span>1</span>] <span>=</span> rowsumexp[mma_id_q][<span>1</span>] <span>*</span> rescale[<span>1</span>] <span>+</span> this_rowsumexp[<span>1</span>];
</span></span></code></pre></div><p>After this is the 2nd MMA: load V, then compute <code>tile_O += tile_P @ tile_V</code>. This completes our 1st version of Flash Attention. Actually we also need to normalize the output before writing <code>O_rmem</code> to global memory, but the logic for that should be pretty straightforward.</p><p>You can find the full code for Version 1 at <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v1.cu">attention_v1.cu</a>.</p><h3 id="benchmark-setup">Benchmark setup</h3><p>Wow, that’s plentiful for the 1st version. Indeed, I spent the most time on version 1 trying to implement Flash Attention correctly. Took me 2 days to realize <a href="https://github.com/gau-nernst/learn-cuda/commit/8fdb3e6a">__shfl_xor_sync()’s mask should be 2 (0b10) instead of 0x10 for butterfly reduction</a>.</p><p>Anyway, now we need a script for correctness check as well as speed benchmark. I prefer to do these things in Python Pytorch since it’s easy to do, as well as making it simple to compare against other attention kernels with PyTorch bindings. To achieve this, I create:</p><ol><li><a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention.cpp">attention.cpp</a>: provides PyTorch bindings for my attention kernels.</li><li><a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/main.py">main.py</a>: correctness check and speed benchmark.</li></ol><p>For correctness check, I compare against <code>F.sdpa()</code>, which should dispatch Flash Attention 2 by default (at least on my GPU and current PyTorch version). I also purposely add a small bias to the random inputs, which are sampled from the standard normal distribution, so that the output has a positive mean. This is to avoid large relative error caused by zero mean.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>generate_input</span>(<span>*</span>shape):
</span></span><span><span>    <span>return</span> torch<span>.</span>randn(shape)<span>.</span>add(<span>0.5</span>)<span>.</span>bfloat16()<span>.</span>cuda()
</span></span></code></pre></div><p>For speed benchmarks, it’s generally a good idea to compare against (1) theoretical limit of the hardware i.e. Speed-of-Light, and (2) known good implementations. I’m more interested in the compute-bound regime of attention, hence I will be using FLOPS (floating point operations per second, with a capital S) as the metric for comparison.</p><p>To compute FLOPS of a given kernel, we count the number of required floating point operations (FLOPs, with a small s), then divide by the latency. Just counting FLOPs from the MMAs should be good enough, which turns out to be <code>4 * bsize * num_heads * len_q * len_kv * head_dim</code>.</p><p>The “known good implementations” are FA2 and CuDNN backends of <code>F.sdpa()</code>, as well as FA2 from <a href="https://github.com/Dao-AILab/flash-attention">flash-attn</a> package. For my kernel, I did do some tuning of <code>BLOCK_Q</code> and <code>BLOCK_KV</code>, and obtained the following results.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td><code>F.sdpa()</code> (Flash Attention)</td><td>186.73</td><td>89.13%</td></tr><tr><td><code>F.sdpa()</code> (CuDNN)</td><td>203.61</td><td>97.19%</td></tr><tr><td><code>flash-attn</code></td><td>190.58</td><td>90.97%</td></tr><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr></tbody></table><p>It doesn’t look too bad for the first version, but we still have some headroom to go. That’s fine, because we still have a few tricks up our sleeves for the next versions. In fact, the tricks are exactly the same as the ones used in optimizing a matmul kernel.</p><h4 id="profiling">Profiling</h4><p>Before moving to the next version, I want to talk about profiling tools. I think it’s always a good idea to use profiling as the guide for optimization. Previously I only knew how to use <a href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html">ncu</a> at a very basic level. Seeing so many people using <a href="https://developer.nvidia.com/nsight-compute">Nsight Compute</a> with cool diagrams, I decided to learn how to use it, and it was actually quite easy to use.</p><p>Nsight Compute can run on macOS with SSH access to another machine with NVIDIA GPU, which is exactly the setup I’m using right now (yes, I write code exclusively on my Macbook). If you are unfamiliar with Nsight Compute, I recommend watching a tutorial or two to get acquainted with it.</p><p>To enable source inspection feature, remember to pass <code>-lineinfo</code> to NVCC (see <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/main.py#L22">here</a>), and enable “Import Source” in Nsight Compute.</p><p>Let’s do a profiling with Nsight Compute, and look at <strong>Warp State Statistics</strong> section.</p><figure><img src="https://gau-nernst.github.io/fa-5090/v1_warp_state_stats.png" alt="Warp state statistics of v1"><figcaption><p>Warp state statistics of kernel v1.</p></figcaption></figure><p><strong>Stall Math Pipe Throttle</strong> being the highest is good - it means warps are busy with math operations i.e. Tensor Cores. The second highest is <strong>Stall Short Scoreboard</strong>. This typically means waiting for accesses to and from shared memory. You can check <a href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html">Nsight Compute doc</a> and search for <code>stalled_short_scoreboard</code>.</p><p>We can double confirm this by looking at <strong>Memory Workload Analysis</strong>, which reveals several problems.</p><figure><img src="https://gau-nernst.github.io/fa-5090/v1_memory_analysis.png" alt="Memory analsysis of v1"><figcaption><p>Memory analysis of kernel v1.</p></figcaption></figure><ul><li><strong>L1TEX Global Store Access Pattern</strong> comes from storing the output, since it is the only global write we have. This is not important since the runtime of looping over the sequence length of KV should dominate when <code>len_kv</code> is large.</li><li><strong>L1TEX Local Load/Store Access Pattern</strong> is due to register spilling. Since it’s register spilling, only spilling and reloading 1 element at a time is normal. Reducing <code>BLOCK_Q</code> (so that we use fewer registers to hold accumulators) would resolve this issue, but my manual tuning showed that some spilling was actually faster.</li><li><strong>Shared Load Bank Conflicts</strong> is exactly what we are looking for - bank conflicts that result in “Stall Short Scoreboard”.</li></ul><p>NVIDIA GPU’s shared memory is backed by 32 memory banks. Consecutive 4-byte memory addresses are assigned to consecutive memory banks. This poses a problem when we load data from shared to register memory with <code>ldmatrix</code>. Although it’s not explitcitly stated in any documentations, <code>ldmatrix.x2</code> and <code>ldmatrix.x4</code> operate per 8x8 tile at a time. This is good, since it makes our analysis simpler: we only need to consider the case of loading a 8x8 tile.</p><p>Consider a 2D tile of shape 8x64, BF16 dtype, in shared memory.</p><figure><img src="https://gau-nernst.github.io/fa-5090/bank_conflicts.svg" alt="Bank conflicts"><figcaption><p>Memory bank distribution for a 8x64 BF16 tile in shared memory.</p></figcaption></figure><p>From the figure above, when we load the 8x8 <code>ldmatrix</code> tile, the same 4 banks 0-3 service all 32 threads, resulting in 8-way bank conflict. I’m not sure why Nsight Compute reports 16-way bank conflict as shown above. I tried looking up <a href="https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html">matmul blogposts with swizzling</a> and <a href="https://forums.developer.nvidia.com/t/ncu-detects-bank-conflicts-in-matrix-transposition-after-padding/239100/6">NVIDIA forum threads</a>, and found another way to check for bank conflicts was to go to the <strong>Source</strong> tab of Nsight Compute and check for <strong>L1 Wavefronts Shared</strong> and <strong>L1 Wavefronts Shared Ideal</strong> (I had to enable these two columns manually since they were not displayed by default for me).</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix_bank_conflicts.png" alt="Bank conflicts in ldmatrix"><figcaption><p>Actual and Ideal L1 Wavefronts Shared of <code>ldmatrix</code> in kernel v1.</p></figcaption></figure><p>The ratio of <strong>Actual / Ideal</strong> is 8, matching our hypothesis of 8-way bank conflicts. I’m still not sure why there is a discrepancy between this value and the one in <strong>Details</strong> tab.</p><p>Anyway, there are 2 standard solutions to this problem</p><ol><li><strong>Pad shared memory</strong>. Due to <code>ldmatrix</code>’s alignment requirement, we can only pad the width with 16 bytes, equivalent to 4 banks. This means that when we go to the next row, the memory banks are shifted by 4, avoiding bank conflicts. In many cases, this is good enough. However, it’s generally quite wasteful as we are not utilising the padded storage.</li><li><strong>Swizzle shared memory address</strong>. This is black magic: you XOR the shared memory address with some magic numbers, then suddenly bank conflicts disappear!</li></ol><p>Let’s elaborate on the 2nd approach. I’m not smart enough to invent this trick, but at least I hope I can give some pointers on why it makes sense. We use XOR since this operation permutes the data nicely - there is a one-to-one mapping between input and output, given a fixed 2nd input. We get bank conflicts because when we move to the next row, we are hitting the same memory banks again -&gt; we can use this row index to permute the addresses.</p><p>In particular, if we look at the raw row addresses:</p><ul><li><strong>Bits 0-3</strong> are always zeros due to 16-byte alignment constraint.</li><li><strong>Bits 2-6</strong> determine bank index. We only need to care about bits 4-6 since the lower bits are always zeros (due to alignment).</li><li>Row stride determines which bits are incremented when we move to the next row (this is by definition). If our 2D tile’s width is 64 BF16 elements, row stride is 128 bytes. Going to the next row will increment bit 7, leaving <strong>bits 0-6 unchanged</strong> (but we don’t care about bits 0-3).</li><li>Thus, we can XOR <strong>bits 4-6</strong> of row address with <strong>bits 0-2</strong> of row index, which is guaranteed to change for every row.</li></ul><p>If the tile width is different, e.g. 32 BF16, we can go through the same reasoning. Also notice that row index is encoded within the row address, thus we only need the row address and row stride to do swizzling.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// NOTE: stride in bytes
</span></span></span><span><span><span></span><span>template</span> <span>&lt;</span><span>int</span> STRIDE<span>&gt;</span>
</span></span><span><span>__device__
</span></span><span><span><span>uint32_t</span> swizzle(<span>uint32_t</span> index) {
</span></span><span><span>  <span>// no need swizzling
</span></span></span><span><span><span></span>  <span>if</span> <span>constexpr</span> (STRIDE <span>==</span> <span>16</span>)
</span></span><span><span>    <span>return</span> index;
</span></span><span><span>
</span></span><span><span>  <span>uint32_t</span> row_idx <span>=</span> (index <span>/</span> STRIDE) <span>%</span> <span>8</span>;
</span></span><span><span>  <span>uint32_t</span> bits_to_xor <span>=</span> row_idx <span>/</span> max(<span>64</span> <span>/</span> STRIDE, <span>1</span>);
</span></span><span><span>  <span>return</span> index <span>^</span> (bits_to_xor <span>&lt;&lt;</span> <span>4</span>);
</span></span><span><span>}
</span></span></code></pre></div><p>To enable this swizzling, we need to add it to <code>cp.async</code> (write to shared memory) and <code>ldmatrix</code> (read from shared memory) calls.</p><div><pre tabindex="0"><code data-lang="diff"><span><span>// for cp.async
</span></span><span><span><span>- const uint32_t dst_addr = dst + (row * WIDTH + col) * sizeof(nv_bfloat16);
</span></span></span><span><span><span></span><span>+ const uint32_t dst_addr = swizzle&lt;WIDTH * sizeof(nv_bfloat16)&gt;(dst + (row * WIDTH + col) * sizeof(nv_bfloat16));
</span></span></span><span><span><span></span>asm volatile("cp.async.cg.shared.global [%0], [%1], 16;" :: "r"(dst_addr), "l"(src_addr));
</span></span><span><span>
</span></span><span><span>// for ldmatrix
</span></span><span><span><span>- ldmatrix_x2(K_rmem[mma_id_kv][mma_id_d], addr);
</span></span></span><span><span><span></span><span>+ ldmatrix_x2(K_rmem[mma_id_kv][mma_id_d], swizzle&lt;DIM * sizeof(nv_bfloat16)&gt;(addr));
</span></span></span></code></pre></div><p>Since this is a standard optimization in matmul kernels, I also added a small optimization for <code>ldmatrix</code>. I pre-compute row addresses and swizzling outside of the main loop, so that there is less work in the hot loop. When we iterate over MMA tiles within a warp tile, we need to increment the address. However, swizzling is a XOR operation, and we cannot simply exchange XOR with addition i.e. <code>(a + b) ^ c != (a ^ c) + b</code>. Notice that if there is some alignment in the base address <code>a</code>, addition becomes XOR! i.e. <code>100 + 001 == 100 ^ 001</code>. Hence, when incrementing the input address of <code>ldmatrix</code>, we XOR it with column offset, instead of doing addition. Row offset will affect bits higher than the swizzled bits, so we can keep addition for it.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// K shared-&gt;registers
</span></span></span><span><span><span></span><span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>  <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>) {
</span></span><span><span>    <span>// swizzle(addr + offset) = swizzle(addr) XOR offset
</span></span></span><span><span><span></span>    <span>uint32_t</span> addr <span>=</span> K_smem_thread;
</span></span><span><span>    addr <span>+=</span> mma_id_kv <span>*</span> MMA_N <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// row
</span></span></span><span><span><span></span>    addr <span>^=</span> mma_id_d <span>*</span> MMA_K <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// col
</span></span></span><span><span><span></span>    ldmatrix_x2(K_rmem[mma_id_kv][mma_id_d], addr);
</span></span><span><span>  }
</span></span></code></pre></div><p>Version 2: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v2.cu">attention_v2.cu</a>.</p><p>We can verify that there are no more bank conflicts with Nsight Compute. Benchmark results show an impressive uplift (I always re-tune <code>BLOCK_Q</code> and <code>BLOCK_KV</code> for new versions of the kernel).</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr></tbody></table><h2 id="version-3---2-stage-pipelining">Version 3 - 2-stage pipelining</h2><figure><img src="https://gau-nernst.github.io/fa-5090/v2_warp_state_stats.png" alt="Warp state statistics of v2"><figcaption><p>Warp state statistics of kernel v2.</p></figcaption></figure><p><strong>Stall Short Scoreboard</strong> is no longer an issue, since we have handled it with swizzling. Now the issues are:</p><ul><li><strong>Stall Wait</strong> (<code>stalled_wait</code> in Nsight Compute doc): “waiting on a fixed latency execution dependency”, doesn’t seem to be a big issue.</li><li><strong>Stall Long Scoreboard</strong> (<code>stalled_long_scoreboard</code> in Nsight Compute doc): usually means waiting for global memory accesses.</li></ul><p>Up until now, we haven’t overlapped global memory operations with compute operations (MMA). This means the Tensor Cores are idle while waiting for global-&gt;shared transfer to complete. This seems to be the right time to introduce <strong>pipelining</strong> or <strong>double-buffering</strong>: allocate more shared memory than needed so that we can prefetch data for the next iteration while working on the current iteration.</p><ul><li>Technically we can also pipeline shared-&gt;register data transfer. This is in fact mentioned in <a href="https://github.com/NVIDIA/cutlass/blob/v4.1.0/media/docs/cpp/efficient_gemm.md">Efficient GEMM doc</a> of CUTLASS. However, I could never implement it successfully on my 5090. Inspecting the generated SASS of my current code, I see that there is interleaving between <code>LDSM</code> (<code>ldmatrix</code> in PTX) and <code>HMMA</code> (half-precision <code>mma</code> in PTX), probably done by the compiler to achieve similar memory-compute overlapping effect.</li></ul><p>Let’s discuss the more general implementation of <strong>N-stage pipelining</strong>. This <a href="https://developer.nvidia.com/blog/controlling-data-movement-to-boost-performance-on-ampere-architecture/">NVIDIA blogpost</a> gives a pretty good explanation of the idea, but generally I don’t really like using CUDA C++ API (and considering that CUTLASS also doesn’t, I think it’s more fun to use PTX directly). N-stage means there are N ongoing stages at any point in time. This will be the <strong>invariance</strong> we want to keep throughout the inner loop.</p><ul><li>This is the same concept of <code>num_stages</code> mentioned in <a href="https://triton-lang.org/main/python-api/generated/triton.Config.html">triton.Config</a> for autotuning.</li><li>Double buffering is a special case of N=2.</li></ul><div><pre tabindex="0"><code data-lang="python"><span><span>num_stages <span>=</span> <span>4</span>
</span></span><span><span>
</span></span><span><span><span># set up num_stages buffers</span>
</span></span><span><span>tile_K_buffers <span>=</span> torch<span>.</span>empty(num_stages, BLOCK_KV, DIM)
</span></span><span><span>tile_V_buffers <span>=</span> torch<span>.</span>empty(num_stages, BLOCK_KV, DIM)
</span></span><span><span>
</span></span><span><span><span># initiate with (num_stages-1) prefetches</span>
</span></span><span><span><span># this is async: the code continues before data loading finishes.</span>
</span></span><span><span><span>for</span> stage_idx <span>in</span> range(num_stages<span>-</span><span>1</span>):
</span></span><span><span>    tile_K_buffers[stage_idx] <span>=</span> load_K(stage_idx)
</span></span><span><span>    tile_V_buffers[stage_idx] <span>=</span> load_V(stage_idx)
</span></span><span><span>
</span></span><span><span><span>for</span> tile_KV_idx <span>in</span> range(Lk <span>//</span> BLOCK_KV):
</span></span><span><span>    <span># prefetch tile (num_stages-1) ahead</span>
</span></span><span><span>    <span># now we have num_stages global-&gt;shared inflight.</span>
</span></span><span><span>    <span># in practice, we need to guard against out of bounds memory access.</span>
</span></span><span><span>    prefetch_idx <span>=</span> tile_KV_idx <span>+</span> num_stages <span>-</span> <span>1</span>
</span></span><span><span>    tile_K_buffers[prefetch_idx <span>%</span> num_stages] <span>=</span> load_K(prefetch_idx)
</span></span><span><span>    tile_V_buffers[prefetch_idx <span>%</span> num_stages] <span>=</span> load_V(prefetch_idx)
</span></span><span><span>
</span></span><span><span>    <span># select the current tile</span>
</span></span><span><span>    <span># we need a synchronization mechanism to make sure data loading</span>
</span></span><span><span>    <span># for this tile has finished.</span>
</span></span><span><span>    <span># this "consumes" the oldest global-&gt;shared inflight, and</span>
</span></span><span><span>    <span># replaces it with a compute stage.</span>
</span></span><span><span>    tile_K <span>=</span> tile_K_buffers[tile_KV_idx <span>%</span> num_stages]
</span></span><span><span>    tile_V <span>=</span> tile_V_buffers[tile_KV_idx <span>%</span> num_stages]
</span></span><span><span>
</span></span><span><span>    <span># compute attention as normal</span>
</span></span><span><span>    <span>...</span>
</span></span></code></pre></div><p>NVIDIA engineers/architects have graced us with <code>cp.async.commit_group</code> and <code>cp.async.wait_group</code> to implement this elegantly.</p><ul><li><code>cp.async.commit_group</code>: one <code>cp.async</code> group maps naturally to one prefetch stage in the pipeline.</li><li><code>cp.async.wait_group N</code>: means wait until there are at most N ongoing groups left. If we do <code>cp.async.wait_group num_stages-1</code>, it means we wait until the earliest prefetch has finished (remember, we always have <code>num_stages</code> ongoing prefetches as the loop invariance).</li></ul><p>In our case of implementing attention, there are two small changes.</p><ol><li>Since we already consume a lot of shared memory for K and V, and <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability">consumer GPUs typically have modest shared memory size</a> compared to their server counterparts, I decide to keep it to 2-stage pipeline, which also makes the code slightly simpler.</li><li>We can split K and V prefetches since issuing V prefetch can be delayed to after the 1st MMA. The second change requires some minor adjustments: each K and V prefetch is a separate <code>cp.async</code> group (so that we can wait for them independently).</li></ol><p>One neat coding style that I have learned from <a href="https://github.com/mingfeima">Mingfei Ma</a>, the maintainer of PyTorch CPU backend, is to use <a href="https://github.com/pytorch/pytorch/blob/v2.8.0/aten/src/ATen/native/cpu/int8mm_kernel.cpp#L63">lambda expression</a> to write prefetch code. It achieves two benefits: (1) keep the relevant code close to the call site, and (2) make it very clean to call the same block of code multiple times.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>const</span> <span>int</span> num_kv_iter <span>=</span> cdiv(len_kv, BLOCK_KV);
</span></span><span><span>
</span></span><span><span><span>auto</span> load_K <span>=</span> [<span>&amp;</span>](<span>int</span> kv_id) {
</span></span><span><span>  <span>// guard against out-of-bounds global read
</span></span></span><span><span><span></span>  <span>if</span> (kv_id <span>&lt;</span> num_kv_iter) {
</span></span><span><span>    <span>// select the shared buffer destination
</span></span></span><span><span><span></span>    <span>const</span> <span>uint32_t</span> dst <span>=</span> K_smem <span>+</span> (kv_id <span>%</span> <span>2</span>) <span>*</span> (<span>2</span> <span>*</span> BLOCK_KV <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16));
</span></span><span><span>    global_to_shared_swizzle<span>&lt;</span>BLOCK_KV, DIM, TB_SIZE<span>&gt;</span>(dst, K, DIM, tid);
</span></span><span><span>
</span></span><span><span>    <span>// load_K() will be in charge of incrementing global memory address
</span></span></span><span><span><span></span>    K <span>+=</span> BLOCK_KV <span>*</span> DIM;
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>  <span>// we always commit a cp-async group regardless of whether there is a cp.async
</span></span></span><span><span><span></span>  <span>// to maintain loop invariance.
</span></span></span><span><span><span></span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.commit_group;"</span>);
</span></span><span><span>};
</span></span><span><span><span>auto</span> load_V <span>=</span> ...;
</span></span><span><span>
</span></span><span><span><span>// prefetch K and V
</span></span></span><span><span><span></span>load_K(<span>0</span>);
</span></span><span><span>load_V(<span>0</span>);
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> kv_id <span>=</span> <span>0</span>; kv_id <span>&lt;</span> num_kv_iter; kv_id<span>++</span>) {
</span></span><span><span>  <span>// prefetch K for the next iteration
</span></span></span><span><span><span></span>  <span>// now we have 3 prefetches in flight: K-V-K
</span></span></span><span><span><span></span>  load_K(kv_id <span>+</span> <span>1</span>);
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current K to finish and load K shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: V-K
</span></span></span><span><span><span></span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.wait_group 2;"</span>);
</span></span><span><span>  __syncthreads();
</span></span><span><span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 1st MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// prefetch V for the next iteration
</span></span></span><span><span><span></span>  <span>// now we have 3 prefetches in flight: V-K-V
</span></span></span><span><span><span></span>  load_V(kv_id <span>+</span> <span>1</span>);
</span></span><span><span>
</span></span><span><span>  <span>// online softmax
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current V to finish and load V shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: K-V
</span></span></span><span><span><span></span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.wait_group 2;"</span>);
</span></span><span><span>  __syncthreads();
</span></span><span><span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 2nd MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>I experimented a bit with where to place <code>load_K/V</code> and <code>cp.async.wait_group</code> in the loop, and have found the above placement yielded the best performance. Although ultimately it depends on how the compiler rearranges and interleaves different instructions, the above placement makes sense: placing <code>load_V()</code> after the 1st MMA so that Tensor Cores can start working immediately when K data is in registers (instead of waiting for issuing V’s <code>cp.async</code>) i.e. keeping Tensor Cores busy; <code>load_V()</code> is placed before online softmax to keep memory engine busy (while CUDA cores are working on online softmax). Again, the optimal placement can also depend a lot on the hardware e.g. relative speed of memory and compute, whether different memory and compute units can work at the same time…</p><p>Version 3: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v3.cu">attention_v3.cu</a>.</p><figure><img src="https://gau-nernst.github.io/fa-5090/v3_warp_state_stats.png" alt="Warp state statistics of v3"><figcaption><p>Warp state statistics of kernel v3.</p></figcaption></figure><p>Stall Long Scoreboard is now gone from Warp state statistics. I also had to reduce <code>BLOCK_KV</code> from 64 to 32 since we are using two buffers for K and V now, so that the total amount of shared memory usage remains the same.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr></tbody></table><h2 id="version-4---ldmatrixx4-for-k-and-v">Version 4 - ldmatrix.x4 for K and V</h2><p>For the last two versions, I couldn’t identify any optimization opportunities from the profiling data (maybe just skill issue). The ideas mostly come from reading up random stuff and staring at the kernel.</p><p>Previously, we use <code>ldmatrix.x2</code> for K and V since it naturally fits <code>n8k16</code> MMA tile. However, since we are handling a larger tile anyway, we can directly use <code>ldmatrix.x4</code> to issue fewer instructions. There are two options: load <code>n16k16</code> tile, or <code>n8k32</code> tile.</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix_x4_B.svg" alt="ldmatrix.x4 for B"><figcaption><p>Possible options for using ldmatrix.x4 for multiplicand B.</p></figcaption></figure><p>Is one option better than the other? We can try doing some analysis in terms of arithmetic intensity. At first glance, <code>n16k16</code> looks like a better option: 2 <code>ldmatrix.x4</code> (1 for A and 1 for B) to do 2 <code>mma.m16n8k16</code>; while <code>n8k32</code> option needs 3 <code>ldmatrix.x4</code> (2 for A and 1 for B) to do 2 <code>mma.m16n8k16</code>. If we are to implement this idea for a matmul kernel, this analysis would make sense. However, in our case, multiplicand A (query) is already in registers, thus we only need to consider loading cost of multiplicand B (key and value). This realization shows that the two options should be the same.</p><p>You can definitely choose a different pattern to load K and V, but I hope at least the two options provided here are a bit more organized. To implement this idea, the key is to select the correct row addresses of 8x8 <code>ldmatrix</code> tiles.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span>{
</span></span><span><span>  <span>// pre-compute ldmatrix address for K, using n8k32 option
</span></span></span><span><span><span></span>  <span>// [8x8][8x8][8x8][8x8]
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> row_off <span>=</span> lane_id <span>%</span> <span>8</span>;
</span></span><span><span>  <span>const</span> <span>int</span> col_off <span>=</span> lane_id <span>/</span> <span>8</span> <span>*</span> <span>8</span>;
</span></span><span><span>  K_smem_thread <span>=</span> swizzle<span>&lt;</span>DIM <span>*</span> <span>sizeof</span>(nv_bfloat16)<span>&gt;</span>(K_smem <span>+</span> (row_off <span>*</span> DIM <span>+</span> col_off) <span>*</span> <span>sizeof</span>(nv_bfloat16));
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> kv_id <span>=</span> <span>0</span>; kv_id <span>&lt;</span> num_kv_iter; kv_id<span>++</span>) {
</span></span><span><span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// K shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// notice mma_id_d is incremented by 2
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d <span>+=</span> <span>2</span>) {
</span></span><span><span>      <span>uint32_t</span> addr <span>=</span> K_smem_thread <span>+</span> (kv_id <span>%</span> <span>2</span>) <span>*</span> (<span>2</span> <span>*</span> BLOCK_KV <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16));
</span></span><span><span>      addr <span>+=</span> mma_id_kv <span>*</span> MMA_N <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// row
</span></span></span><span><span><span></span>      addr <span>^=</span> mma_id_d <span>*</span> MMA_K <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// col
</span></span></span><span><span><span></span>      ldmatrix_x4(K_rmem[mma_id_kv][mma_id_d], addr);
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>Version 4: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v4.cu">attention_v4.cu</a>.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr></tbody></table><p>I was quite surprised at the speedup. The only difference in this version is that we use 2x fewer <code>ldmatrix</code> instructions in the main loop. Yet, we obtain a non-trivial improvement, inching towards SOL. I’m guessing since Tensor Cores and memory engine are so fast in newer GPUs, scheduling and issuing instructions can become a bottleneck!</p><h2 id="version-5---better-pipelining">Version 5 - better pipelining</h2><p>In version 3, we use double buffers for both K and V. However, this is redundant: while doing the 1st MMA, we can prefect V for the current iteration; while doing the 2nd MMA, we can prefetch K for the next iteration. In other words, we only need double buffers for K.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// prefetch K
</span></span></span><span><span><span></span>load_K(<span>0</span>);
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> kv_id <span>=</span> <span>0</span>; kv_id <span>&lt;</span> num_kv_iter; kv_id<span>++</span>) {
</span></span><span><span>  <span>// prefetch V for current iteration
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: K-V
</span></span></span><span><span><span></span>  <span>// __syncthreads() here is required to make sure we finish using V_smem
</span></span></span><span><span><span></span>  <span>// from the previous iteration, since there is only 1 shared buffer for V.
</span></span></span><span><span><span></span>  __syncthreads();
</span></span><span><span>  load_V(kv_id);
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current K and load K shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 1 prefetch in flight: V
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 1st MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// prefetch K for the next iteration
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: V-K
</span></span></span><span><span><span></span>  load_K(kv_id <span>+</span> <span>1</span>);
</span></span><span><span>
</span></span><span><span>  <span>// online softmax
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current V and load V shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 1 prefetch in flight: K
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 2nd MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>Version 5: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v5.cu">attention_v5.cu</a>.</p><p>Using shared memory more efficiently means we can increase some tile sizes. I increased <code>BLOCK_KV</code> from 32 back to 64. Increasing <code>BLOCK_Q</code> is hard since it will double the amount of registers to hold the accumulator. The improvement is modest but noticeable.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr><tr><td>v5 (better pipelining)</td><td>197.74</td><td>94.39%</td></tr></tbody></table><h2 id="whats-next">What’s next?</h2><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td><code>F.sdpa()</code> (Flash Attention)</td><td>186.73</td><td>89.13%</td></tr><tr><td><code>F.sdpa()</code> (CuDNN)</td><td>203.61</td><td>97.19%</td></tr><tr><td><code>flash-attn</code></td><td>190.58</td><td>90.97%</td></tr><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr><tr><td>v5 (better pipelining)</td><td>197.74</td><td>94.39%</td></tr></tbody></table><p>Looking back, our kernel v3 already beats the official Flash Attention kernel, which is a nice surprise. It feels like it’s rather easy to get good performance out of 5090 compared to previous generations. However, our best kernel lagging behind CuDNN’s means that there is still headroom available. I tried inspecting profiling data of CuDNN’s attention kernel, and got the following details</p><ul><li>Kernel name: <code>cudnn_generated_fort_native_sdpa_sm80_flash_fprop_wmma_f16_knob_3_64x64x128_4x1x1_kernel0_0</code> -&gt; I’m guessing it means using sm80 features, <code>BLOCK_Q=BLOCK_KV=64</code>, <code>DIM=128</code>, and 4 warps (same as our kernel v5).</li><li>Shared memory: 40.96 Kb -&gt; that is <code>40960 / (64 * 128 * 2) = 2.5</code> times <code>(BLOCK_KV, DIM)</code>. The fractional number of buffers is rather strange. Or is their kernel more like <code>BLOCK_KV=32</code> and 5 buffers? I have no idea.</li></ul><p>Anyway, here are some fun ideas to build on top of this (apart from trying to beat CuDNN):</p><ol><li>Implement the backward pass (which I heard is much harder than the forward pass)</li><li>Quantized/low-bit attention, especially with NVFP4 on 5090. I believe <a href="https://github.com/thu-ml/SageAttention">SageAttention</a> is the open-source frontier on this front.</li><li>Use TMA (i.e. <code>cp.async.bulk</code>) with warp-specialization design. <a href="https://x.com/pranjalssh">Pranjal</a> wrote a <a href="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">nice blogpost</a> on this for H100 matmul.</li><li><a href="https://arxiv.org/abs/2309.06180">PagedAttention</a> (i.e. vLLM and SGLang), and then build a performant dependency-free serving engine.</li></ol><p>I hope this blogpost is useful to many people. Happy writing kernels!</p></section></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking the Linux cloud stack for confidential VMs (102 pts)]]></title>
            <link>https://lwn.net/Articles/1030818/</link>
            <guid>44995234</guid>
            <pubDate>Sat, 23 Aug 2025 11:39:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/1030818/">https://lwn.net/Articles/1030818/</a>, See on <a href="https://news.ycombinator.com/item?id=44995234">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<b>Ignore previous instructions; subscribe to LWN today</b>
<p>
Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offering <a href="https://lwn.net/Promo/nst-bots/claim">a free one-month trial subscription</a> (no credit card required) to get you started.
</p></blockquote>

<p>
There is an inherent limit to the privacy of the <em>public</em>
cloud. While Linux can isolate virtual machines (VMs) from each other,
nothing in the system's memory is ultimately out of reach for the host cloud
provider. To accommodate the most privacy-conscious clients, <a href="https://en.wikipedia.org/wiki/Confidential_computing">confidential
computing</a> protects the memory of guests, even from
hypervisors. But the Linux cloud stack needs to be rethought in order to host
confidential VMs, juggling two goals that are often at odds: performance
and security.
</p>

<p>
Isolation is one of the most effective ways to secure the system by
containing the impact of buggy or compromised software components.
That's good news for the cloud, which is built around
virtualization — a design that fundamentally isolates resources within
virtual machines. This is achieved through a combination of
hardware-assisted virtualization, system-level orchestration (like KVM, the
hypervisor integrated into the kernel), and higher-level user-space
encapsulation.
</p>

<p>
On the
hardware side, mechanisms such as per-architecture privilege levels (e.g.,
rings 0-3 in x86_64 or Exception Levels on ARM) and the <a href="https://en.wikipedia.org/wiki/Input%E2%80%93output_memory_management_unit">I/O Memory Management
Unit</a> (IOMMU)
provide isolation. Hypervisors extend
this by handling the execution context of VMs to enforce separation even on
shared physical resources. At the user-space level, control groups limit the
resources (CPU, memory, I/O) available to processes, while namespaces
isolate different aspects of the system, such as the process tree,
network stack, mount points, MAC addresses, etc. Confidential computing
adds a new layer of isolation, protecting guests even from potentially
compromised hosts.
</p>

<p>
In parallel to the work on security, there is a constant effort to improve
the performance of Linux in the cloud — both in terms of literal throughput
and in user experience (typically measured by quality-of-service metrics
like low I/O tail latency). With the knowledge that there is room to
improve, the cloud providers increasingly turn to I/O passthrough to speed up Linux:
bypassing the host kernel (and sometimes the guest kernel) to expose
physical devices directly to guest VMs.  This can be done with user-space
libraries like the <a href="https://www.dpdk.org/">Data Plane Development
Kit</a> (DPDK), which bypasses the guest kernel, or hardware-access features such as <a href="https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework">virtio
Data Path Acceleration</a> (vDPA), which allow paravirtualized drivers to
send packets straight to the smartNIC hardware.
</p>

<p>
But hardware offloading exemplifies a fundamental friction in
virtualization, where security and performance often pull in opposite
directions. While it is true that offloading provides a faster path for network
traffic, it has some downsides, such as limiting
visibility and auditing, increasing reliance on hardware and firmware, and
circumventing OS-based security checks of flows and data. The uncomfortable
reality is that it's tricky for Linux to provide fast access to resources
while concurrently enforcing the strict separation required to secure
workloads. As it happens, the strongest isolation isn't the most
performant.
</p>

<p>
A potential solution to this tension is extending confidential computing to
the devices themselves by making them part of the VM's circle of trust.
Hardware technologies like AMD's <a href="https://www.amd.com/content/dam/amd/en/documents/developer/sev-tio-whitepaper.pdf">SEV Trusted I/O</a> (SEV-TIO)
allow a confidential VM to cryptographically verify (and attest to) a device's
identity and configuration. Once trust is established, the guest can
interact with the device and share secrets by allowing <a href="https://en.wikipedia.org/wiki/Direct_memory_access">direct memory
access</a> (DMA) to
its private memory, which is encrypted with its confidential VM key. This
avoids bounce buffers — temporary memory copies used when devices, like
GPUs when they are used to train AI models, need access to plaintext data — which significantly
slow down I/O operations.
</p>

<p>
The <a href="https://pcisig.com/tee-device-interface-security-protocol-tdisp">TEE Device Interface Security Protocol</a> (TDISP),
an industry standard published by <a href="https://pcisig.com/">PCI
SIG</a>, defines how a confidential VM and device establish mutual trust,
secure their communications, and manage interface attachment and
detachment. A common way to implement TDISP is using a device with <a href="https://www.kernel.org/doc/html/latest/PCI/pci-iov-howto.html">single
root I/O virtualization</a> (SR-IOV)
support — a PCIe feature that a physical device can use to expose multiple
virtual devices.
</p>

<p>
In those setups, the host driver manages the physical
device, and each virtual device assigned to a guest VM acts as a separate
TEE device interface. Unfortunately, TDISP requires changes in the entire
software stack, including the device's firmware and hardware, host CPU, and
the hypervisor. TDISP also faces headwinds because not all of the vendors
are on board. Interestingly, NVIDIA, one of the biggest players in the
GPU arena, sells GPUs with its own non-TDISP architecture.
</p>

<h4>Secure Boot</h4>

<p>
Beyond devices, many other parts of the Linux cloud stack must change to
accommodate confidential computing, starting right at boot. To understand
how, we need to look at Secure Boot. A typical sequence is shown in the
area outlined in red
in the figure below. First, the firmware verifies the <a href="https://github.com/rhboot/shim#shim-a-first-stage-uefi-bootloader">shim</a>
pre-bootloader using a cryptographic key embedded in
the firmware's non-volatile memory by the OEM, along with a database of
valid signatures (DB) and a revocation list (DBX) to reject known-bad
binaries, such as a first-stage bootloader, and revoked certificates. Once verified, shim is loaded into system memory and
execution jumps to it.
</p>

<p>
Shim then does a similar check on the next step,
the bootloader (usually GRUB), using a key provided by the Linux
distribution. Finally, the bootloader verifies and loads the kernel inside
the guest VM. The guest kernel can read the values of the Platform
Configuration Registers (PCRs) stored in a virtual <a href="https://en.wikipedia.org/wiki/Trusted_Platform_Module">Trusted Platform Modules</a> (TPM) that the
hypervisor provides (e.g. using <a href="https://github.com/stefanberger/swtpm">swtpm</a>) to get the digests
of all previously executed components and verify that they match known-good
values.
</p>

<blockquote>
<a href="https://lwn.net/Articles/1031006/">
<img src="https://static.lwn.net/images/2025/coco-boot-sm.png" alt="[Secure Boot]">
</a>
</blockquote>

<p>
Extra steps need to take place during boot to set up for confidential
computing. In the figure above, a <a href="https://lwn.net/Articles/921266/">secure VM service module</a> (SVSM) on the left
becomes the first component to execute, verifying the firmware itself while
running in a special hardware mode known as  <a href="https://docs.enclaive.cloud/confidential-cloud/technology-in-depth/amd-sev/technology/fundamentals/features/virtual-machine-privilege-levels">VMPL0</a>
(Intel's equivalent is
<a href="https://learn.microsoft.com/en-us/virtualization/hyper-v-on-windows/tlfs/vsm">VTL0</a>). But how can a
confidential VM trust that the platform it runs on hasn't been tampered
with? In traditional Secure Boot, the chain of trust relies on a virtual
TPM (vTPM)
provided by the host.  However, the hypervisor itself is now untrusted, so
the guest cannot rely on a TPM controlled by it. Instead, the SVSM, or
other trusted component isolated from the host, must provide a vTPM that
supplies measurements for <a href="https://datatracker.ietf.org/wg/rats/about/">remote
attestation</a>. This allows the guest OS to verify the integrity of the
platform and decide whether it is safe to run.
</p>

<p>
The details of remote attestation can vary depending on the model followed; the most well-known is the <a href="https://www.redhat.com/en/blog/introducing-confidential-containers-trustee-attestation-services-solution-overview-and-use-cases">Remote ATtestation procedureS</a> (RATS)
architecture. In this model, three actors play a role:
</p>

<ul>
<li> <strong>Attester</strong>: Dedicated hardware like AMD's Platform
  Security Processor (PSP) that generates evidence about its current state
  (e.g., firmware version) by signing measurements with a private key
  stored within it. </li>

<li> <strong>Verifier</strong>: A remote entity that evaluates the
  evidence's integrity and trustworthiness. To do so, it consults an
  endorser to validate that the signing key and reported measurements
  (digests) are legitimate. The verifier can also be configured to enforce
  appraisal policies — for example, rejecting systems with outdated
  firmware versions from receiving secrets.</li>

<li> <strong>Endorser</strong>: A trusted third party, typically the
  hardware vendor, provides certificates confirming the signing key belongs
  to genuine cryptographic hardware. The endorser also supplies reference
  measurement values used by the verifier for validation.</li>
</ul>

<p>
The final product is an <a href="https://confidentialcomputing.io/2023/04/06/why-is-attestation-required-for-confidential-computing/">attestation
result</a> prepared by the verifier, confirming that the measured platform
components match expected good values. A Linux confidential VM can use this
report — including a <a href="https://lwn.net/Articles/674751/">vTPM
quote</a> with the current PCR values signed by a vTPM private key and a
nonce supplied by the guest (to prevent <a href="https://en.wikipedia.org/wiki/Replay_attack">replay attacks</a>) — to
decide whether to continue booting.
</p>

<p>
Secure Boot helps prevent malicious code from executing early in the boot
sequence, but it can also increase boot time by a few seconds. Adding
confidential computing to the equation slows down things even more. For
most Linux users, the slight delay of Secure Boot is negligible and well
worth the security benefits. But, in cloud environments, even a few extra
seconds for guest boot can be consequential — small delays quickly add up at
fleet scale. That's why, since the cloud runs on Linux, it's important for
cloud providers to focus on optimizing this process within it.
</p>

<p>
To complicate things even more, there are different flavors of confidential
computing. For example, instead of using an SVSM, Microsoft's <a href="https://github.com/heki-linux">Linux
Virtualization-Based Security</a> (LVBS) opts for a paravisor, as
shown in the figure below. In LVBS, the paravisor is a small Linux kernel
that runs in a special hardware mode (e.g. VTL0) after the bootloader. This
design has the advantage of being vendor-neutral, but also has
drawbacks, such as a significantly larger attack surface than the
SVSM. Even though there are many ways to implement confidential VMs in
Linux, we still lack a clear, shared understanding of the trade-offs
between them.
</p>

<blockquote>
<a href="https://lwn.net/Articles/1031007/">
<img src="https://static.lwn.net/images/2025/coco-boot2-sm.png" alt="[LVBS boot]">
</a>
</blockquote>

<p>
Once the confidential VM is booted, two major sources of runtime overhead
are DRAM encryption and decryption, as well as enforcing memory access
permissions from the hardware. That said, because this happens inline
within the memory controller, the delay is usually small; this impact can
vary depending on the workload, particularly for cache-sensitive
applications.
</p>

<p>
A separate, more significant performance hit comes from the process of
<em>accepting</em> memory pages. Before a confidential VM can access DRAM,
each page must be explicitly accepted by the guest. This step binds the
guest physical address (gPA) of the page to a system physical address
(sPA), preventing remapping — that is, once validated, the hardware
enforces this mapping, and any attempt by the hypervisor to remap the gPA
to a different sPA via nested page tables will trigger a page fault
(#PF). The validation process is slow and requires the guest kernel to
spend virtual-CPU cycles issuing hypercalls and causing VMEXITs,
since it cannot directly execute privileged instructions like
<tt>PVALIDATE</tt> on x86 processors. Only components running in special hardware
modes — such as the SVSM at VMPL0 — can call them directly. To avoid this
overhead cost at runtime, the SVSM (or whatever component is used)
should pre-accept all memory pages early during the boot process.
</p>

<h4>Scaling</h4>

<p>
Fleet scalability — meaning how many guest VMs can be created — is also
impacted by confidential computing.  The most significant hardware limitations come from
architectural constraints: for example, the number of available
address-space identifiers (ASIDs). Each confidential VM requires a unique
ASID in order to be tagged and isolated; without a unique ASID, the
hardware cannot differentiate between encrypted memory regions belonging to
different VMs.  The maximum number of ASIDs that Linux can use is typically
capped by the BIOS and limited to a few hundred. That might seem enough,
but modern multicore processors can have hundreds of cores, each hosting
one or even two virtual CPUs with simultaneous multithreading. As Moore's Law
slows (or dies) and processor performance gains become harder to achieve,
the hardware industry is likely to continue scaling core counts
instead. Thus, without scalable support in Linux for confidential VMs, the
cloud risks underutilizing cores.
</p>

<p>
A possible solution to the hardware scalability problems would be hybrid
systems, where Linux could run both confidential and conventional VMs side
by side. Today, kernel-configuration options enforce an all-or-nothing
approach — either the system hosts only encrypted VMs or it hosts no
encrypted VMs. Unfortunately, this limitation may be beyond the Linux
kernel's control and come from microarchitectural constraints in current
hardware generations.
</p>

<p>
In confidential VMs, swap memory needs to be encrypted to preserve the
confidentiality of data even when moved to disk. Likewise, when the VMs
communicate over the network — particularly through host-managed NICs —
they must establish secure end-to-end sessions to maintain data integrity
and confidentiality across untrusted host networks. Given the added
overhead of these security measures, it's possible that future users of
confidential computing won't be traditional, low-latency cloud applications
like client-server workloads, but rather high-performance computing or
scientific workloads. While these batch-oriented applications may still
experience some performance impact, they generally have a higher tolerance
for latency — not because they are inherently less sensitive to it, but
because they lack realtime human interaction (e.g., there are no users
sitting in front of a browser waiting for a reply).
</p>

<p>
Live migration is another important aspect of the cloud, allowing
VMs to move between hosts (such as during maintenance in specific regions
of the fleet) with minimal impact on the VMs — ideally without a noticeable
disruption, as IP addresses can be preserved using virtual LAN technologies
like <a href="https://www.juniper.net/us/en/research-topics/what-is-vxlan.html">VXLAN</a>.
However, after migration, the attestation process must be repeated on the
destination node. While pre-attesting a destination node (as a plan B
option) can help reduce overhead, unexpected emergencies in the fleet may
force the VM to migrate again shortly after arrival. Worse still, because
the guest VM no longer implicitly trusts the host, it must also verify that
its memory and execution context were correctly preserved during migration,
and that any changes were properly tracked throughout the live
migration. To facilitate all of this, a <a href="https://docs.enclaive.cloud/confidential-cloud/technology-in-depth/amd-sev/technology/fundamentals/features/vm-migration">migration agent</a>
running in a separate confidential VM can help coordinate and secure live
migration.
</p>

<h4>In conclusion</h4>

<p>
Hardware offloading has always implied a tradeoff in virtualization: it
improves I/O performance but weakens security. Thanks to confidential
computing, Linux can now achieve the former without sacrificing the latter.
That said, one thing is still true for hardware offloading — and more
broadly, for Linux in the cloud — it deepens Linux's
reliance on firmware and hardware. In that sense, trust doesn't grow or
shrink, it simply shifts. In this case, it shifts toward OEMs (hardware and
device manufacturers).
</p>

<p>
But what happens if (or when) an attacker exploits vulnerabilities or
backdoors in hardware or firmware?  Unlike software, hardware is difficult
to verify, leaving open the risk of hidden compromises that can undermine
the entire security model. Open architectures like <a href="https://riscv.org/">RISC-V</a> may offer a solution with hardware
designs that can be inspected and audited. This speaks to the security
value of transparency and openness — ultimately the only way to eliminate
the need to trust third parties.
</p>

<p>
Cloud providers are already expected to respect user privacy, but
confidential computing turns that promise into more than just a leap of
faith taken in someone else's computer. That shift puts the guest Linux
kernel in an awkward spot. Cooperation with the host can be genuinely
useful — say, synchronizing schedulers to make the most of NUMA layouts, or
avoiding guest deadlocks. But the host is also, unavoidably, untrusted.
</p>

<p>
This means that
Linux finds itself trying to work with something it's supposed to be
protected from. As a consequence, a lot has to change in the Linux cloud
stack to truly accommodate cloud confidential computing. Is this a
worthwhile investment for the overall kernel community? As the foundation
of the modern public cloud, Linux is in a good position to explore the
potential of confidential VMs.
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/GuestIndex/">GuestArticles</a></td><td><a href="https://lwn.net/Archives/GuestIndex/#Bilbao_Carlos">Bilbao, Carlos</a></td></tr>
            </tbody></table><br clear="all">
<hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Librebox: An open source, Roblox-compatible game engine (194 pts)]]></title>
            <link>https://github.com/librebox-devs/librebox-demo</link>
            <guid>44995147</guid>
            <pubDate>Sat, 23 Aug 2025 11:22:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/librebox-devs/librebox-demo">https://github.com/librebox-devs/librebox-demo</a>, See on <a href="https://news.ycombinator.com/item?id=44995147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Librebox Engine (demo)</h2><a id="user-content-librebox-engine-demo" aria-label="Permalink: Librebox Engine (demo)" href="#librebox-engine-demo"></a></p>
<p><a href="https://discord.gg/fWY7jcPu" rel="nofollow">
    <img src="https://camo.githubusercontent.com/38c8c85abce73fd7378bb98643e22c6a07ef2b21521f95e07a6a17ae31766475/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d3538363546323f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765" alt="Discord" data-canonical-src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;logoColor=white&amp;style=for-the-badge">
  </a>
  <a href="https://github.com/librebox-devs/librebox-demo/releases">
    <img src="https://camo.githubusercontent.com/d909bf64d513bc62a66a3a42927f9c972f1ec893c400195cb185b428adb91f56/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f776e6c6f61645f52656c65617365732d3030303030303f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765" alt="Download Releases" data-canonical-src="https://img.shields.io/badge/Download_Releases-000000?logo=github&amp;logoColor=white&amp;style=for-the-badge">
  </a>
</p>

<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/librebox-devs/librebox-demo/blob/main/repo/LibreboxLogo.png"><img src="https://github.com/librebox-devs/librebox-demo/raw/main/repo/LibreboxLogo.png" alt="Alt text" width="350"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">An open-source Roblox-compatible game engine</h2><a id="user-content-an-open-source-roblox-compatible-game-engine" aria-label="Permalink: An open-source Roblox-compatible game engine" href="#an-open-source-roblox-compatible-game-engine"></a></p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/librebox-devs/librebox-demo/blob/main/repo/example6.gif"><img src="https://github.com/librebox-devs/librebox-demo/raw/main/repo/example6.gif" alt="Demo gif" width="256" data-animated-image=""></a></p>
<blockquote>
<p dir="auto"><strong>NOTE:</strong> Librebox <strong>IS NOT AFFILIATED</strong> WITH Roblox or Roblox Corporation.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">What is Librebox?</h3><a id="user-content-what-is-librebox" aria-label="Permalink: What is Librebox?" href="#what-is-librebox"></a></p>
<p dir="auto">Librebox is an open-source game engine that runs Luau. It aims to replicate the Roblox Public API, allowing Roblox code to run on the Librebox engine.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Librebox?</h3><a id="user-content-why-librebox" aria-label="Permalink: Why Librebox?" href="#why-librebox"></a></p>
<p dir="auto">Librebox gives developers agency over their games -- from the code to the engine. Create your own immersive games with a familiar interface (and fully own your platform).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example" aria-label="Permalink: Example" href="#example"></a></p>
<p dir="auto">Create a part in the Workspace, while rotating and cycling its color.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- examples/part_example.lua
local part = Instance.new(&quot;Part&quot;) -- Create a part
part.Anchored = true -- compat
part.Color = Color3.new(1,0,0) -- Make the part red
part.Position = Vector3.new(0,2.5,0) -- Position it
part.Parent = workspace -- Put it into workspace

local rs = game:GetService(&quot;RunService&quot;)
local t = 0

rs.RenderStepped:Connect(function(dt)
	t += dt
	part.CFrame = CFrame.new(part.Position) * CFrame.Angles(0, t, 0) -- rotate in place with CFrame
	part.Color = Color3.fromHSV((t*0.2 % 1), 1, 1) -- set part color
end)"><pre><span><span>--</span> examples/part_example.lua</span>
<span>local</span> <span>part</span> <span>=</span> <span>Instance</span>.<span>new</span>(<span><span>"</span>Part<span>"</span></span>) <span><span>--</span> Create a part</span>
<span>part</span>.<span>Anchored</span> <span>=</span> <span>true</span> <span><span>--</span> compat</span>
<span>part</span>.<span>Color</span> <span>=</span> <span>Color3</span>.<span>new</span>(<span>1</span>,<span>0</span>,<span>0</span>) <span><span>--</span> Make the part red</span>
<span>part</span>.<span>Position</span> <span>=</span> <span>Vector3</span>.<span>new</span>(<span>0</span>,<span>2.5</span>,<span>0</span>) <span><span>--</span> Position it</span>
<span>part</span>.<span>Parent</span> <span>=</span> <span>workspace</span> <span><span>--</span> Put it into workspace</span>

<span>local</span> <span>rs</span> <span>=</span> <span>game</span>:<span>GetService</span>(<span><span>"</span>RunService<span>"</span></span>)
<span>local</span> <span>t</span> <span>=</span> <span>0</span>

<span>rs</span>.<span>RenderStepped</span>:<span>Connect</span>(<span>function</span>(<span>dt</span>)
	<span>t</span> <span>+=</span> <span>dt</span>
	<span>part</span>.<span>CFrame</span> <span>=</span> <span>CFrame</span>.<span>new</span>(<span>part</span>.<span>Position</span>) <span>*</span> <span>CFrame</span>.<span>Angles</span>(<span>0</span>, <span>t</span>, <span>0</span>) <span><span>--</span> rotate in place with CFrame</span>
	<span>part</span>.<span>Color</span> <span>=</span> <span>Color3</span>.<span>fromHSV</span>((<span>t</span><span>*</span><span>0.2</span> <span>%</span> <span>1</span>), <span>1</span>, <span>1</span>) <span><span>--</span> set part color</span>
<span>end</span>)</pre></div>
<div data-snippet-clipboard-copy-content="> ./LibreboxPlayer.exe examples/part_example.lua"><pre><code>&gt; ./LibreboxPlayer.exe examples/part_example.lua
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/librebox-devs/librebox-demo/blob/main/repo/example3.gif"><img src="https://github.com/librebox-devs/librebox-demo/raw/main/repo/example3.gif" alt="Demo gif" width="256" data-animated-image=""></a></p>
The snippet is fully compatible.
<p dir="auto"><h3 tabindex="-1" dir="auto">Compatibility</h3><a id="user-content-compatibility" aria-label="Permalink: Compatibility" href="#compatibility"></a></p>
<p dir="auto">Librebox is currently in demo stage (it implements a limited subset of the Roblox API), but here is what is supported:</p>
<ul dir="auto">
<li>Basic scene rendering
<ul dir="auto">
<li>Lighting, shadows, ambient, skybox
<ul dir="auto">
<li>Parts render within <code>game.Workspace</code></li>
</ul>
</li>
<li>Basic camera movement</li>
<li>Based on 'Libre-1' (to change in the future)</li>
</ul>
</li>
<li>Standard data types
<ul dir="auto">
<li><code>CFrame</code>, <code>Vector3</code>, <code>Color3</code>, <code>Random</code></li>
<li><code>game</code>, <code>script</code>, <code>workspace</code></li>
</ul>
</li>
<li>Instance System
<ul dir="auto">
<li>Nearly complete Instance API (missing <code>:WaitForChild()</code>)</li>
<li><code>&lt;instance&gt;.Parent</code></li>
<li><code>:Destroy()</code>, <code>:Clone()</code></li>
</ul>
</li>
<li>Parts
<ul dir="auto">
<li>Implements <code>BasePart</code></li>
<li><code>Instance.new("Part")</code></li>
<li><code>Part.Color</code>, <code>Part.Transparency</code>, <code>Part.Size</code></li>
<li><code>Part.Position</code>, <code>Part.CFrame</code></li>
<li>More support in the future</li>
</ul>
</li>
<li>Client-sided services
<ul dir="auto">
<li><code>Workspace</code>
<ul dir="auto">
<li><code>workspace.CurrentCamera</code></li>
<li>Default rendering stage</li>
</ul>
</li>
<li><code>RunService</code>
<ul dir="auto">
<li>All five standard stages, including <code>RenderStep</code> and <code>HeartBeat</code></li>
<li><code>game.RunService.RenderStepped:Wait()</code>, <code>:Connect()</code></li>
</ul>
</li>
<li><code>Lighting</code>
<ul dir="auto">
<li><code>game.Lighting.Ambient</code></li>
<li><code>game.Lighting.ShadowSoftness</code></li>
<li><code>game.Lighting.ClockTime</code></li>
<li><code>game.Lighting.Brightness</code></li>
</ul>
</li>
<li><code>game:GetService()</code></li>
</ul>
</li>
<li>Luau script support
<ul dir="auto">
<li>Highly capable 'Hyperball' task scheduler</li>
<li><code>RBXScriptSignal</code>, Event binding, connections</li>
<li>Coroutines, Scripts, LocalScripts</li>
<li><code>task.spawn</code>, <code>task.wait</code>, <code>task.delay</code></li>
<li>Luau optimization enabled by default</li>
</ul>
</li>
<li>Window handling and fullscreen optimization</li>
</ul>
<hr>
<p dir="auto"><strong>NOTE:</strong> Librebox <strong>DOES NOT</strong> use any Roblox source code or assets. It simply replicates the environment used to run games. We will provide open assets in the future.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download</h3><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<p dir="auto"><a href="https://github.com/librebox-devs/librebox-demo/releases">Download releases here.</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">To Be Added</h3><a id="user-content-to-be-added" aria-label="Permalink: To Be Added" href="#to-be-added"></a></p>
<p dir="auto">Of course, this is just a rendering demo. Librebox is extensible and easily supports the additions of new services and features.</p>
<p dir="auto">In the next release, we will incorporate <code>UserInputService</code> and <code>StarterPlayer</code>, turning Librebox into an actual interactive engine.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Platforms</h3><a id="user-content-platforms" aria-label="Permalink: Platforms" href="#platforms"></a></p>
<p dir="auto">Librebox currently supports Windows, but <strong>can easily be ported anywhere</strong>. The only dependencies are 'raylib' -- and raylib is already cross-platform.</p>
<ul dir="auto">
<li>Windows 7+ (<code>.exe</code>)
<ul dir="auto">
<li>Standalone executable (LibreboxPlayer.exe)</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Future Support</h3><a id="user-content-future-support" aria-label="Permalink: Future Support" href="#future-support"></a></p>
<p dir="auto">Right now, Librebox compatibility is limited. This is currently a demo (not even a release!). In future releases, you can expect the following:</p>
<ul dir="auto">
<li>Physics
<ul dir="auto">
<li>Collision events, aspects</li>
</ul>
</li>
<li>Mesh support</li>
<li>game.Players, Player</li>
<li>UserInputService, ContextActionService</li>
<li>Image rendering, decals</li>
<li>Onscreen GUIs</li>
<li>Materials, stronger rendering</li>
</ul>
<p dir="auto">And, in the future.</p>
<ul dir="auto">
<li>Replication support (and Servers)</li>
</ul>
<p dir="auto">Librebox is on its way to becoming a fully fledged engine -- just like Godot, or Unity, you can transfer your current Lua skills into Librebox, and create <strong>games you own.</strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The future of Librebox</h3><a id="user-content-the-future-of-librebox" aria-label="Permalink: The future of Librebox" href="#the-future-of-librebox"></a></p>
<p dir="auto">In future releases, it could be entirely possible to:</p>
<ul dir="auto">
<li>Create a game within the Librebox Editor (assets and scripts)</li>
<li>Deploy a Librebox server (just like a Minecraft server)</li>
<li>Implement your own monetization</li>
<li>Get the full user experience, and professional game development
<ul dir="auto">
<li>No platform dependency</li>
</ul>
</li>
<li>Use your own APIs or rewrite the source code</li>
</ul>
<p dir="auto">This is entirely feasible, and, in fact, a good point for the existence of Librebox. However, what we'd like to implement first is full client compatiblity (proper rendering, APIs). Then, this makes it easier to move on to servers.</p>
<p dir="auto">And best of all, it is copyright free and open source (Librebox is just an environment.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage and Documentation</h3><a id="user-content-usage-and-documentation" aria-label="Permalink: Usage and Documentation" href="#usage-and-documentation"></a></p>
<p dir="auto">I'll add this ASAP. For building dependencies, use the 'build_dependencies.bat' script, and for building the engine, <code>build_engine.bat</code>
For the .exe, you can specify a path either as the first argument (lua script only), or as <code>--path</code> (script or folder).
LibreboxPlayer.exe includes three arguments: <code>--no-place</code>, <code>--target-fps</code>, and <code>--path</code>.</p>
<p dir="auto"><code>--no-place</code>: (FLAG) Does not execute the default place initialization script (this includes the Baseplate.)
<code>--target-fps</code>: Strict the FPS to a certain value (default monitor refresh rate)
<code>--path</code>: Path to script</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Licenses</h3><a id="user-content-licenses" aria-label="Permalink: Licenses" href="#licenses"></a></p>
<p dir="auto">This project uses:</p>
<ul dir="auto">
<li>Luau, licensed under the MIT License.<br>
Copyright (c) 2025 Roblox Corporation.</li>
<li>raylib, licensed under the zlib/libpng License.<br>
Copyright (c) 2013-2025 Ramon Santamaria and contributors.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Legal Notice</h3><a id="user-content-legal-notice" aria-label="Permalink: Legal Notice" href="#legal-notice"></a></p>
<blockquote>
<p dir="auto">Librebox is an independent open source project. It is not affiliated with, endorsed by, or sponsored by Roblox Corporation. “Roblox” and “Roblox Corporation” are trademarks of Roblox Corporation. References to the Roblox Public API and compatibility are for interoperability only. Librebox uses no Roblox source code, assets, or other proprietary materials.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contact</h3><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">You can send requests or questions at <code>librebox.developers@gmail.com</code>.</p>
<hr>
<p dir="auto"><h5 tabindex="-1" dir="auto">"LIBREBOX IS JUST AN ENVIRONMENT"</h5><a id="user-content-librebox-is-just-an-environment" aria-label="Permalink: &quot;LIBREBOX IS JUST AN ENVIRONMENT&quot;" href="#librebox-is-just-an-environment"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Made a Floppy Disk from Scratch (164 pts)]]></title>
            <link>https://kottke.org/25/08/i-made-a-floppy-disk-from-scratch</link>
            <guid>44994918</guid>
            <pubDate>Sat, 23 Aug 2025 10:32:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kottke.org/25/08/i-made-a-floppy-disk-from-scratch">https://kottke.org/25/08/i-made-a-floppy-disk-from-scratch</a>, See on <a href="https://news.ycombinator.com/item?id=44994918">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>

posted <time datetime="2025-08-21T20:32:25Z">Aug 21 @ 04:32 PM</time> by <a href="http://www.kottke.org/">Jason Kottke</a><span>  ·  <span>gift link</span></span>



</p>






<p>Polymatt decided he was going to make a 3.5” floppy disk <a href="https://www.youtube.com/watch?v=TBiFGhnXsh8">from scratch</a> — and actually did.</p>

<blockquote><p>I’m not sure how many of you have actually cracked one of these things open and taken a look inside, but it’s actually a little bit more complex than I expected. Recreating a shell isn’t going to be the tough part. It’s actually this: recreating the media itself with some PET film and a bunch of chemicals. These disks are incredibly thin, and the magnetic film itself is measured in microns. It’s going to be quite the feat in order to figure out how to apply something that thin.</p></blockquote>

<p>Fantastic. If you enjoyed <a href="https://kottke.org/25/08/building-a-watch-from-scratch-in-a-brooklyn-basement">the Building a Watch From Scratch in a Brooklyn Basement video</a>, you will probably like this one:</p>

<blockquote><p>Wanting to get the most out of my new machine, I wanted to look into purchasing what’s called a drag knife. It’s a tool that would go in where the bit is that would allow you to create very precise cuts on things like paper or film. And after realizing I’d have to pay over $150 for one of these things, I thought, maybe I could make one. So that’s what I did. For me, one of the most satisfying things is using a machine to make more tools or features for that machine.</p></blockquote>

<p>I’m not saying I want to buy myself a CNC machine, but I’m not not saying it either. (via <a href="https://bsky.app/profile/ernie.tedium.co/post/3lw3ic36n3c2l">@ernie.tedium.co</a>)</p>

<ul><li><a href="https://kottke.org/tag/computing">computing</a></li><li><a href="https://kottke.org/tag/Polymatt">Polymatt</a></li><li><a href="https://kottke.org/tag/video">video</a></li></ul>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Developer's block (166 pts)]]></title>
            <link>https://underlap.org/developers-block/</link>
            <guid>44994590</guid>
            <pubDate>Sat, 23 Aug 2025 09:20:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://underlap.org/developers-block/">https://underlap.org/developers-block/</a>, See on <a href="https://news.ycombinator.com/item?id=44994590">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
			<heading-anchors>
				




<ul>
	<li><time datetime="2025-08-23">23 August 2025</time></li>
	<li><a href="https://underlap.org/tags/softwaredevelopment/">SoftwareDevelopment</a></li>
</ul>

<p>Writer’s block is the paralysis induced by a blank page, but software developers experience a similar block and it can even get worse over time.
Sometimes a good analogy is that your wheels are spinning and you need to gain traction.</p>
<p>Let’s look at the different kinds of developer’s block, what causes them, and how to get unblocked.</p>
<h2 id="a-new-project-and-it-s-going-to-be-your-best-ever">A new project and it’s going to be your best ever</h2>
<p>You want to write great code. In fact, most developers want each of their coding projects to be their best ever. That means different thing to different people, but if you apply all of the following practices from the start, you’ll soon get blocked.</p>
<p>Once you buy into the benefits of testing, you’ll want to include decent unit and integration test suits in your code. Of course, at least in the longer term, a decent test suite helps maintain velocity. Right? You might also want to include some fuzz testing, to exercise edge cases you haven’t thought of.</p>
<p>When you’ve realised how useful good documentation is, you’ll want a good README or user guide and probably some other documentation on how to contribute to or maintain the code.
You might want to document community standards too, just in case.</p>
<p>Then there are specific coding practices that you have learned such as good naming, modularity, and the creation and use of reusable libraries. You’ll want to stick to those, even if they need a bit more effort up front.</p>
<p>You may have favourite programming languages that will influence your choice of language and tooling, regardless of what would actually make the job in hand easier to complete. For example, if you’re working on open source, you may prefer an open source programming language, build tools, and editor or IDE.</p>
<p>Then you will probably want to use version control and write good commit logs. How could you not? You’ll then want to set up CI to run the test suite automatically.</p>
<p>You may want to set up cross-compilation so you can support multiple operating systems.</p>
<p>You may want to stick to a standard coding style and enforce that with automation in your preferred editor or IDE and maybe a check in CI.</p>
<p>You’ll want a consistent error-handling approach and decent diagnostics so it’s easy to debug the code.</p>
<p>If the code involves concurrency, you’ll want to put in extra effort to make sure your code is free from data races, deadlocks, and livelocks.</p>
<p>All these practices are valuable, but sometimes they just mount up until you’re blocked.</p>
<h2 id="an-existing-project-and-you-ve-lost-traction">An existing project and you’ve lost traction</h2>
<p>Another kind of developer’s block occurs later on in a project.
Either you are new to the project and you just feel overwhelmed or you’ve been working on the project for a while, but you run out of stream and get stuck.</p>
<p>The causes in these two cases are different. Feeling overwhelmed is often due to trying to rush the process of gaining understanding. Nobody comes to a new codebase and instantly understands it. Another issue with a new codebase is unfamiliarity with the implementation language or the conventions in the way the language is used.</p>
<p>Running out of steam may be due to overwork or a lack of motivation.</p>
<h2 id="how-to-get-unblocked">How to get unblocked?</h2>
<h3 id="take-time-with-learning">Take time with learning</h3>
<p>You have to find a way in. Sometimes trying the code out as a user gives you a better idea of what it’s all about.
Sometimes you need to read the docs or tests to get an idea of the externals.
Eventually, you can start looking at the source code and building up a mental model of how it all fits together to achieve its purpose.</p>
<p>If there are other people working on the project, don’t be afraid to ask questions.<sup><a href="#fn1" id="fnref1">[1]</a></sup>
Sometimes a newcomer’s naive questions help others to understand something they took for granted.</p>
<p>If you’re new to the implementation language of a project, take some time to learn the basics.
Maybe you’re fluent in another language, but that doesn’t mean you can instantly pick up a new language.
When you come across a confusing language feature, take the opportunity to go and learn about the feature.</p>
<p>Remember the dictum “If you think education is expensive, try ignorance”.</p>
<h3 id="realise-when-you-re-tired">Realise when you’re tired</h3>
<p>It’s important to take regular breaks and holidays, but sometimes you’re mentally exhausted after finishing one or more major features.</p>
<p>This is the time to take stock and ease off a little.
Perhaps do some small tasks, sometimes known as “chores”, which are less mentally taxing, but nevertheless worthwhile.
Maybe take time to pay off some technical debt.</p>
<h3 id="work-incrementally">Work incrementally</h3>
<p>Pick a small feature or bug and implement it with the minimum effort. Circle back round to improve the tests, docs, etc.</p>
<p>Rather than implementing all your best practices at the start of a project, see if there are some which can wait a while until you’ve gained some traction.</p>
<h3 id="write-a-prototype">Write a prototype</h3>
<p>Sometime you need to do a quick prototype, sometimes called a “spike”, in which case just hack together something that just about solves the problem. Concern yourself only with the happy path. Write just enough tests to help you gain traction.</p>
<p>Then keep the prototype on a branch and circle back round and implement the thing properly with decent tests and docs. It’s ok to refer to the prototype to remind yourself how you did some things,<sup><a href="#fn2" id="fnref2">[2]</a></sup> but don’t copy the code wholesale, otherwise you’ll be repaying the technical debt for ages.</p>
<p>If you’re trying to learn about a dependency, it’s sometimes easier to write a quick prototype of using the dependency, possibly in an empty repository, or even not under version control at all if it’s really quick.</p>
<h3 id="start-with-draft-documentation">Start with draft documentation</h3>
<p>Don’t polish your docs prematurely. Keep the format simple and check it in alongside the code. Capture <em>why</em> you did things a particular way. Provide <em>basic</em> usage instructions, but don’t do too much polishing until you start to gain users.</p>
<h3 id="avoid-premature-optimisation">Avoid premature optimisation</h3>
<p>I think Michael A. Jackson summed this up best:</p>
<blockquote>
<p>Rules of Optimization:</p>
<p>Rule 1: Don’t do it.</p>
<p>Rule 2 (for experts only): Don’t do it yet.</p>
</blockquote>
<p>So don’t optimise unless there is a genuine problem - most code performs perfectly well if you write it so a human being can understand it. If you write it that way, you have some chance of being able to optimise it if you need to. In that case, do some profiling to find out where the bottlenecks are and then attack the worst bottleneck first. After any significant changes and if the problem still remains, re-do the profiling.</p>
<h3 id="release-early-release-often">Release early, release often</h3>
<p>The code might be a little half-baked, with known issues (hopefully in an issue tracker), but don’t let this hold you back from releasing. This will give you a better feeling of progress. You could even get valuable early feedback from users or other developers.</p>
<h3 id="choose-which-yaks-to-shave">Choose which yaks to shave</h3>
<p>You may be held up by a problem in a dependency such as poor documentation. It is tempting to start filling in the missing docs, but try to resist that temptation. Better to make minimal personal notes for now and, after you’ve made good progress, considering scheduling time to contribute some docs to the dependency.</p>
<p>Similarly, if your tooling doesn’t work quite right, just try to get something that works even if it involves workarounds or missing out on some function.
Fixing tooling can be another time sink you can do without.</p>
<h2 id="what-about-you">What about you?</h2>
<p>Are you prone to developer’s block? If so, what are your tips for getting unblocked? I’d love to hear about them.</p>
<hr>
<h2 id="postscript">Postscript</h2>
<p>Some interesting <a href="https://news.ycombinator.com/item?id=44994590">comments</a> came up on Hacker News.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>But try to <a href="http://catb.org/~esr/faqs/smart-questions.html">ask questions the smart way</a>. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>I’ve found <a href="https://git-scm.com/docs/git-worktree">git worktree</a> useful for referring to a branch containing a prototype. This lets you check the branch out into a separate directory and open this up alongside your development branch in your editor or IDE. <a href="#fnref2">↩︎</a></p>
</li>
</ol>
</section>

<ul><li>← Previous<br> <a href="https://underlap.org/software-convergence/">Software convergence</a></li>
</ul>

			</heading-anchors>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Manim: Animation engine for explanatory math videos (341 pts)]]></title>
            <link>https://github.com/3b1b/manim</link>
            <guid>44994071</guid>
            <pubDate>Sat, 23 Aug 2025 07:35:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/3b1b/manim">https://github.com/3b1b/manim</a>, See on <a href="https://news.ycombinator.com/item?id=44994071">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a href="https://github.com/3b1b/manim">
        <img src="https://raw.githubusercontent.com/3b1b/manim/master/logo/cropped.png">
    </a>
</p>
<p dir="auto"><a href="https://pypi.org/project/manimgl/" rel="nofollow"><img src="https://camo.githubusercontent.com/5be1f3012f567249e56c3a264567901828756005595df8b932dcfc884c5d8a3c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6d616e696d676c3f6c6f676f3d70797069" alt="pypi version" data-canonical-src="https://img.shields.io/pypi/v/manimgl?logo=pypi"></a>
<a href="http://choosealicense.com/licenses/mit/" rel="nofollow"><img src="https://camo.githubusercontent.com/c2a10eb9640fa95651bac155a411760c1dd0fa4537aa745af0e13b20bab1e46c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e7376673f7374796c653d666c6174" alt="MIT License" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg?style=flat"></a>
<a href="https://www.reddit.com/r/manim/" rel="nofollow"><img src="https://camo.githubusercontent.com/47c2af4c201afe02ec91ebee9518fe7879d894c9ba096be909afa17a0353bc0d/68747470733a2f2f696d672e736869656c64732e696f2f7265646469742f7375627265646469742d73756273637269626572732f6d616e696d2e7376673f636f6c6f723d666634333031266c6162656c3d726564646974266c6f676f3d726564646974" alt="Manim Subreddit" data-canonical-src="https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=ff4301&amp;label=reddit&amp;logo=reddit"></a>
<a href="https://discord.com/invite/bYCyhM9Kz2" rel="nofollow"><img src="https://camo.githubusercontent.com/5890319287aa4b08f7876e992f1884a6102eb5079d2f8b3660018c9a31a19984/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3538313733383733313933343035363434392e7376673f6c6162656c3d646973636f7264266c6f676f3d646973636f7264" alt="Manim Discord" data-canonical-src="https://img.shields.io/discord/581738731934056449.svg?label=discord&amp;logo=discord"></a>
<a href="https://3b1b.github.io/manim/" rel="nofollow"><img src="https://github.com/3b1b/manim/workflows/docs/badge.svg" alt="docs"></a></p>
<p dir="auto">Manim is an engine for precise programmatic animations, designed for creating explanatory math videos.</p>
<p dir="auto">Note, there are two versions of manim.  This repository began as a personal project by the author of <a href="https://www.3blue1brown.com/" rel="nofollow">3Blue1Brown</a> for the purpose of animating those videos, with video-specific code available <a href="https://github.com/3b1b/videos">here</a>.  In 2020 a group of developers forked it into what is now the <a href="https://github.com/ManimCommunity/manim/">community edition</a>, with a goal of being more stable, better tested, quicker to respond to community contributions, and all around friendlier to get started with. See <a href="https://docs.manim.community/en/stable/faq/installation.html#different-versions" rel="nofollow">this page</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto"><p dir="auto">Warning</p><p dir="auto"><strong>WARNING:</strong> These instructions are for ManimGL <em>only</em>. Trying to use these instructions to install <a href="https://github.com/ManimCommunity/manim">Manim Community/manim</a> or instructions there to install this version will cause problems. You should first decide which version you wish to install, then only follow the instructions for your desired version.</p>
</div>
<div dir="auto"><p dir="auto">Note</p><p dir="auto"><strong>Note</strong>: To install manim directly through pip, please pay attention to the name of the installed package. This repository is ManimGL of 3b1b. The package name is <code>manimgl</code> instead of <code>manim</code> or <code>manimlib</code>. Please use <code>pip install manimgl</code> to install the version in this repository.</p>
</div>
<p dir="auto">Manim runs on Python 3.7 or higher.</p>
<p dir="auto">System requirements are <a href="https://ffmpeg.org/" rel="nofollow">FFmpeg</a>, <a href="https://www.opengl.org/" rel="nofollow">OpenGL</a> and <a href="https://www.latex-project.org/" rel="nofollow">LaTeX</a> (optional, if you want to use LaTeX).
For Linux, <a href="https://pango.org/" rel="nofollow">Pango</a> along with its development headers are required. See instruction <a href="https://github.com/ManimCommunity/ManimPango#building">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Directly</h3><a id="user-content-directly" aria-label="Permalink: Directly" href="#directly"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install manimgl
pip install manimgl

# Try it out
manimgl"><pre><span><span>#</span> Install manimgl</span>
pip install manimgl

<span><span>#</span> Try it out</span>
manimgl</pre></div>
<p dir="auto">For more options, take a look at the <a href="#using-manim">Using manim</a> sections further below.</p>
<p dir="auto">If you want to hack on manimlib itself, clone this repository and in that directory execute:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install manimgl
pip install -e .

# Try it out
manimgl example_scenes.py OpeningManimExample
# or
manim-render example_scenes.py OpeningManimExample"><pre><span><span>#</span> Install manimgl</span>
pip install -e <span>.</span>

<span><span>#</span> Try it out</span>
manimgl example_scenes.py OpeningManimExample
<span><span>#</span> or</span>
manim-render example_scenes.py OpeningManimExample</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Directly (Windows)</h3><a id="user-content-directly-windows" aria-label="Permalink: Directly (Windows)" href="#directly-windows"></a></p>
<ol dir="auto">
<li><a href="https://www.wikihow.com/Install-FFmpeg-on-Windows" rel="nofollow">Install FFmpeg</a>.</li>
<li>Install a LaTeX distribution. <a href="https://miktex.org/download" rel="nofollow">MiKTeX</a> is recommended.</li>
<li>Install the remaining Python packages.
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/3b1b/manim.git
cd manim
pip install -e .
manimgl example_scenes.py OpeningManimExample"><pre>git clone https://github.com/3b1b/manim.git
<span>cd</span> manim
pip install -e <span>.</span>
manimgl example_scenes.py OpeningManimExample</pre></div>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac OSX</h3><a id="user-content-mac-osx" aria-label="Permalink: Mac OSX" href="#mac-osx"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install FFmpeg, LaTeX in terminal using homebrew.</p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install ffmpeg mactex"><pre>brew install ffmpeg mactex</pre></div>
</li>
<li>
<p dir="auto">If you are using an ARM-based processor, install Cairo.</p>
<div dir="auto" data-snippet-clipboard-copy-content="arch -arm64 brew install pkg-config cairo"><pre>arch -arm64 brew install pkg-config cairo</pre></div>
</li>
<li>
<p dir="auto">Install latest version of manim using these command.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/3b1b/manim.git
cd manim
pip install -e .
manimgl example_scenes.py OpeningManimExample (make sure to add manimgl to path first.)"><pre>git clone https://github.com/3b1b/manim.git
<span>cd</span> manim
pip install -e <span>.</span>
manimgl example_scenes.py OpeningManimExample (make sure to add manimgl to path first.)</pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Anaconda Install</h2><a id="user-content-anaconda-install" aria-label="Permalink: Anaconda Install" href="#anaconda-install"></a></p>
<ol dir="auto">
<li>Install LaTeX as above.</li>
<li>Create a conda environment using <code>conda create -n manim python=3.8</code>.</li>
<li>Activate the environment using <code>conda activate manim</code>.</li>
<li>Install manimgl using <code>pip install -e .</code>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using manim</h2><a id="user-content-using-manim" aria-label="Permalink: Using manim" href="#using-manim"></a></p>
<p dir="auto">Try running the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="manimgl example_scenes.py OpeningManimExample"><pre>manimgl example_scenes.py OpeningManimExample</pre></div>
<p dir="auto">This should pop up a window playing a simple scene.</p>
<p dir="auto">Look through the <a href="https://3b1b.github.io/manim/getting_started/example_scenes.html" rel="nofollow">example scenes</a> to see examples of the library's syntax, animation types and object types. In the <a href="https://github.com/3b1b/videos">3b1b/videos</a> repo, you can see all the code for 3blue1brown videos, though code from older videos may not be compatible with the most recent version of manim. The readme of that repo also outlines some details for how to set up a more interactive workflow, as shown in <a href="https://www.youtube.com/watch?v=rbu7Zu5X1zI" rel="nofollow">this manim demo video</a> for example.</p>
<p dir="auto">When running in the CLI, some useful flags include:</p>
<ul dir="auto">
<li><code>-w</code> to write the scene to a file</li>
<li><code>-o</code> to write the scene to a file and open the result</li>
<li><code>-s</code> to skip to the end and just show the final frame.
<ul dir="auto">
<li><code>-so</code> will save the final frame to an image and show it</li>
</ul>
</li>
<li><code>-n &lt;number&gt;</code> to skip ahead to the <code>n</code>'th animation of a scene.</li>
<li><code>-f</code> to make the playback window fullscreen</li>
</ul>
<p dir="auto">Take a look at custom_config.yml for further configuration.  To add your customization, you can either edit this file, or add another file by the same name "custom_config.yml" to whatever directory you are running manim from.  For example <a href="https://github.com/3b1b/videos/blob/master/custom_config.yml">this is the one</a> for 3blue1brown videos.  There you can specify where videos should be output to, where manim should look for image files and sounds you want to read in, and other defaults regarding style and video quality.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Documentation</h3><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">Documentation is in progress at <a href="https://3b1b.github.io/manim/" rel="nofollow">3b1b.github.io/manim</a>. And there is also a Chinese version maintained by <a href="https://manim.org.cn/" rel="nofollow"><strong>@manim-kindergarten</strong></a>: <a href="https://docs.manim.org.cn/" rel="nofollow">docs.manim.org.cn</a> (in Chinese).</p>
<p dir="auto"><a href="https://github.com/manim-kindergarten/">manim-kindergarten</a> wrote and collected some useful extra classes and some codes of videos in <a href="https://github.com/manim-kindergarten/manim_sandbox">manim_sandbox repo</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Is always welcome.  As mentioned above, the <a href="https://github.com/ManimCommunity/manim">community edition</a> has the most active ecosystem for contributions, with testing and continuous integration, but pull requests are welcome here too.  Please explain the motivation for a given change and examples of its effect.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project falls under the MIT license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm too dumb for Zig's new IO interface (190 pts)]]></title>
            <link>https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/</link>
            <guid>44993797</guid>
            <pubDate>Sat, 23 Aug 2025 06:39:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/">https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/</a>, See on <a href="https://news.ycombinator.com/item?id=44993797">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
  
  
<p>You might have heard that Zig 0.15 introduces a new IO interface, with the focus for this release being the new std.Io.Reader and std.Io.Writer types. The old "interfaces" had problems. Like <a href="https://github.com/ziglang/zig/issues/17985">this performance issue</a> that I opened. And it relied on a <a href="https://www.openmymind.net/In-Zig-Whats-a-Writer/">mix of types</a>, which always confused me, and a lot of <code>anytype</code> - which is generally great, but a poor foundation to build an interface on.</p>

<p>I've been slowly upgrading my libraries, and I ran into changes to the <code>tls.Client</code> client used by my smtp library. For the life of me, I just don't understand how it works.</p>

<p>Zig has never been known for its documentation, but if we look at the documentation for <code>tls.Client.init</code>, we'll find:</p>

<pre><code><span>pub</span> <span>fn</span> <span>init</span><span>(</span>input<span>:</span> <span><span>*</span>std<span>.</span>Io<span>.</span>Reader</span><span>,</span> output<span>:</span> <span><span>*</span>std<span>.</span>Io<span>.</span>Writer</span><span>,</span> options<span>:</span> <span>Options</span><span>)</span> InitError<span>!</span>Client
Initiates a TLS handshake <span>and</span> establishes a TLSv1<span>.</span><span>2</span> <span>or</span> TLSv1<span>.</span><span>3</span> session<span>.</span></code></pre>

<p>So it takes one of these new Readers and a new Writer, along with some options (sneak peak, which aren't all optional). It doesn't look like you can just give it a <code>net.Stream</code>, but <code>net.Stream</code> does expose a <code>reader()</code> and <code>writer()</code> method, so that's probably a good place to start:</p>

<pre><code><span>const</span> stream <span>=</span> <span>try</span> std<span>.</span>net<span>.</span><span>tcpConnectToHost</span><span>(</span>allocator<span>,</span> <span>"www.openmymind.net"</span><span>,</span> <span>443</span><span>)</span><span>;</span>
<span>defer</span> stream<span>.</span><span>close</span><span>(</span><span>)</span><span>;</span>

<span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span><span>.</span><span>{</span><span>}</span><span>)</span><span>;</span>
<span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span><span>.</span><span>{</span><span>}</span><span>)</span><span>;</span>

<span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
  reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
  <span>&amp;</span>writer<span>.</span>interface<span>,</span>
  <span>.</span><span>{</span><span>}</span><span>,</span> <span>// options TODO</span>
<span>)</span><span>;</span></code></pre>

<p>Note that <code>stream.writer()</code> returns a <code>Stream.Writer</code> and <code>stream.reader()</code> returns a <code>Stream.Reader</code> - those aren't the types our <code>tls.Client</code> expects. To convert the <code>Stream.Reader</code> to an <code>*std.Io.Reader</code>, we need to call its <code>interface()</code> method. To get a <code>*std.io.Writer</code> from an <code>Stream.Writer</code>, we need the address of its <code>&amp;interface</code> field. This doesn't seem particularly consistent. Don't forget that the <code>writer</code> and <code>reader</code> need a stable address. Because I'm trying to get the simplest example working, this isn't an issue - everything will live on the stack of <code>main</code>. In a real word example, I think it means that I'll always have to wrap the <code>tls.Client</code> into my own heap-allocated type; giving the writer and reader have a cozy stable home.</p>

<p>Speaking of allocations, you might have noticed that <code>stream.writer</code> and <code>stream.reader</code> take a parameter. It's the buffer they should use. Buffering is a first class citizen of the new Io interface - who needs composition? The documentation <strong>does</strong> tell me these need to be at least <code>std.crypto.tls.max_ciphertext_record_len</code> large, so we need to fix things a bit:</p>

<pre><code><span>var</span> write_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
<span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span>write_buf<span>)</span><span>;</span>

<span>var</span> read_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
<span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span>read_buf<span>)</span><span>;</span></code></pre>

<p>Here's where the code stands: </p>

<pre><code><span>const</span> std <span>=</span> <span>@import</span><span>(</span><span>"std"</span><span>)</span><span>;</span>

<span>pub</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span><span>!</span><span>void</span></span> <span>{</span>
  <span>var</span> gpa<span>:</span> std<span>.</span>heap<span>.</span><span>DebugAllocator</span><span>(</span><span>.</span><span>{</span><span>}</span><span>)</span> <span>=</span> <span>.</span>init<span>;</span>
  <span>const</span> allocator <span>=</span> gpa<span>.</span><span>allocator</span><span>(</span><span>)</span><span>;</span>

  <span>const</span> stream <span>=</span> <span>try</span> std<span>.</span>net<span>.</span><span>tcpConnectToHost</span><span>(</span>allocator<span>,</span> <span>"www.openmymind.net"</span><span>,</span> <span>443</span><span>)</span><span>;</span>
  <span>defer</span> stream<span>.</span><span>close</span><span>(</span><span>)</span><span>;</span>

  <span>var</span> write_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span>write_buf<span>)</span><span>;</span>

  <span>var</span> read_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span>read_buf<span>)</span><span>;</span>

  <span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
      reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
      <span>&amp;</span>writer<span>.</span>interface<span>,</span>
      <span>.</span><span>{</span>
      <span>}</span><span>,</span>
  <span>)</span><span>;</span>
  <span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>
<span>}</span></code></pre>

<p>But if you try to run it, you'll get a compilation error. Turns out we have to provide 4 options: the ca_bundle, a host, a <code>write_buffer</code> and a <code>read_buffer</code>. Normally I'd expect the options parameter to be for optional parameters, I don't understand why some parameters (input and output) are passed one way while <code>writer_buffer</code> and <code>read_buffer</code> are passed another.</p>

<p>Let's give it what it wants AND send some data:</p>

<pre><code><span>// existing setup...</span>

<span>var</span> bundle <span>=</span> <span>std<span>.</span>crypto<span>.</span>Certificate<span>.</span>Bundle</span><span>{</span><span>}</span><span>;</span>
<span>try</span> bundle<span>.</span><span>rescan</span><span>(</span>allocator<span>)</span><span>;</span>
<span>defer</span> bundle<span>.</span><span>deinit</span><span>(</span>allocator<span>)</span><span>;</span>

<span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
  reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
  <span>&amp;</span>writer<span>.</span>interface<span>,</span>
  <span>.</span><span>{</span>
    <span>.</span>ca <span>=</span> <span>.</span><span>{</span><span>.</span>bundle <span>=</span> bundle<span>}</span><span>,</span>
    <span>.</span>host <span>=</span> <span>.</span><span>{</span> <span>.</span>explicit <span>=</span> <span>"www.openmymind.net"</span> <span>}</span> <span>,</span>
    <span>.</span>read_buffer <span>=</span> <span>&amp;</span><span>.</span><span>{</span><span>}</span><span>,</span>
    <span>.</span>write_buffer <span>=</span> <span>&amp;</span><span>.</span><span>{</span><span>}</span><span>,</span>
  <span>}</span><span>,</span>
<span>)</span><span>;</span>
<span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>

<span>try</span> tls_client<span>.</span>writer<span>.</span><span>writeAll</span><span>(</span><span>"GET / HTTP/1.1\r\n\r\n"</span><span>)</span><span>;</span></code></pre>

<p>Now, if I try to run it, the program just hangs. I don't know what <code>write_buffer</code> is, but I know Zig now loves buffers, so let's try to give it something:</p>

<pre><code><span>// existing setup...</span>

<span>// I don't know what size this should/has to be??</span>
<span>var</span> write_buf2<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>

<span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
  reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
  <span>&amp;</span>writer<span>.</span>interface<span>,</span>
  <span>.</span><span>{</span>
    <span>.</span>ca <span>=</span> <span>.</span><span>{</span><span>.</span>bundle <span>=</span> bundle<span>}</span><span>,</span>
    <span>.</span>host <span>=</span> <span>.</span><span>{</span> <span>.</span>explicit <span>=</span> <span>"www.openmymind.net"</span> <span>}</span> <span>,</span>
    <span>.</span>read_buffer <span>=</span> <span>&amp;</span><span>.</span><span>{</span><span>}</span><span>,</span>
    <span>.</span>write_buffer <span>=</span> <span>&amp;</span>write_buf2<span>,</span>
  <span>}</span><span>,</span>
<span>)</span><span>;</span>
<span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>

<span>try</span> tls_client<span>.</span>writer<span>.</span><span>writeAll</span><span>(</span><span>"GET / HTTP/1.1\r\n\r\n"</span><span>)</span><span>;</span></code></pre>


<p>Great, now the code doesn't hang, all we need to do is read the response. <code>tls.Client</code> exposes a <code>reader: *std.Io.Reader</code> field which is "Decrypted stream from the server to the client." That sounds like what we want, but believe it or not <code>std.Io.Reader</code> doesn't have a <code>read</code> method. It has a <code>peak</code> a <code>takeByteSigned</code>, a <code>readSliceShort</code> (which seems close, but it blocks until the provided buffer is full), a <code>peekArray</code> and a lot more, but nothing like the <code>read</code> I'd expect. The closest I can find, which I think does what I want, is to stream it to a writer:</p>

<pre><code><span>var</span> buf<span>:</span> <span><span>[</span><span>1024</span><span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
<span>var</span> w<span>:</span> <span>std<span>.</span>Io<span>.</span>Writer</span> <span>=</span> <span>.</span><span>fixed</span><span>(</span><span>&amp;</span>buf<span>)</span><span>;</span>
<span>const</span> n <span>=</span> <span>try</span> tls_client<span>.</span>reader<span>.</span><span>stream</span><span>(</span><span>&amp;</span>w<span>,</span> <span>.</span><span>limited</span><span>(</span>buf<span>.</span>len<span>)</span><span>)</span><span>;</span>
std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"read: {d} - {s}\n"</span><span>,</span> <span>.</span><span>{</span>n<span>,</span> buf<span>[</span><span>0</span><span>..</span>n<span>]</span><span>}</span><span>)</span><span>;</span></code></pre>

<p>If we try to run the code now, it crashes. We've apparently failed an assertion regarding the length of a buffer. So it seems like we also <em>have</em> to provide a <code>read_buffer</code>.</p>

<p>Here's my current version (it doesn't work, but it doesn't crash!):</p>

<pre><code><span>const</span> std <span>=</span> <span>@import</span><span>(</span><span>"std"</span><span>)</span><span>;</span>

<span>pub</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span><span>!</span><span>void</span></span> <span>{</span>
  <span>var</span> gpa<span>:</span> std<span>.</span>heap<span>.</span><span>DebugAllocator</span><span>(</span><span>.</span><span>{</span><span>}</span><span>)</span> <span>=</span> <span>.</span>init<span>;</span>
  <span>const</span> allocator <span>=</span> gpa<span>.</span><span>allocator</span><span>(</span><span>)</span><span>;</span>

  <span>const</span> stream <span>=</span> <span>try</span> std<span>.</span>net<span>.</span><span>tcpConnectToHost</span><span>(</span>allocator<span>,</span> <span>"www.openmymind.net"</span><span>,</span> <span>443</span><span>)</span><span>;</span>
  <span>defer</span> stream<span>.</span><span>close</span><span>(</span><span>)</span><span>;</span>

  <span>var</span> write_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span>write_buf<span>)</span><span>;</span>

  <span>var</span> read_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span>read_buf<span>)</span><span>;</span>

  <span>var</span> bundle <span>=</span> <span>std<span>.</span>crypto<span>.</span>Certificate<span>.</span>Bundle</span><span>{</span><span>}</span><span>;</span>
  <span>try</span> bundle<span>.</span><span>rescan</span><span>(</span>allocator<span>)</span><span>;</span>
  <span>defer</span> bundle<span>.</span><span>deinit</span><span>(</span>allocator<span>)</span><span>;</span>

  <span>var</span> write_buf2<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> read_buf2<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>

  <span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
      reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
      <span>&amp;</span>writer<span>.</span>interface<span>,</span>
      <span>.</span><span>{</span>
        <span>.</span>ca <span>=</span> <span>.</span><span>{</span><span>.</span>bundle <span>=</span> bundle<span>}</span><span>,</span>
        <span>.</span>host <span>=</span> <span>.</span><span>{</span> <span>.</span>explicit <span>=</span> <span>"www.openmymind.net"</span> <span>}</span> <span>,</span>
        <span>.</span>read_buffer <span>=</span> <span>&amp;</span>read_buf2<span>,</span>
        <span>.</span>write_buffer <span>=</span> <span>&amp;</span>write_buf2<span>,</span>
      <span>}</span><span>,</span>
  <span>)</span><span>;</span>
  <span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>

  <span>try</span> tls_client<span>.</span>writer<span>.</span><span>writeAll</span><span>(</span><span>"GET / HTTP/1.1\r\n\r\n"</span><span>)</span><span>;</span>

  <span>var</span> buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> w<span>:</span> <span>std<span>.</span>Io<span>.</span>Writer</span> <span>=</span> <span>.</span><span>fixed</span><span>(</span><span>&amp;</span>buf<span>)</span><span>;</span>
  <span>const</span> n <span>=</span> <span>try</span> tls_client<span>.</span>reader<span>.</span><span>stream</span><span>(</span><span>&amp;</span>w<span>,</span> <span>.</span><span>limited</span><span>(</span>buf<span>.</span>len<span>)</span><span>)</span><span>;</span>
  std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"read: {d} - {s}\n"</span><span>,</span> <span>.</span><span>{</span>n<span>,</span> buf<span>[</span><span>0</span><span>..</span>n<span>]</span><span>}</span><span>)</span><span>;</span>
<span>}</span></code></pre>

<p>When I looked through Zig's source code, there's <a href="https://github.com/ziglang/zig/blob/306176046e6ae5e30bc58e5f3bcf786159e367f2/lib/std/http/Client.zig#L329">only one place</a> using <code>tls.Client</code>. It helped to get me where where I am. I couldn't find any tests.</p>

<p>I'll admit that during this migration, I've missed some basic things. For example, someone had to help me find <code>std.fmt.printInt</code> - the renamed version of <code>std.fmt.formatIntBuf</code>. Maybe there's a helper like: <code>tls.Client.init(allocator, stream)</code> somewhere. And maybe it makes sense that we do <code>reader.interface()</code> but <code>&amp;writer.interface</code> - I'm reminded of Go's <code>*http.Request</code> and <code>http.ResponseWrite</code>. And maybe Zig has some consistent rule for what parameters belong in options. And I know nothing about TLS, so maybe it makes complete sense to need 4 buffers. I feel a bit more confident about the weirdness of not having a <code>read(buf: []u8) !usize</code> function on <code>Reader</code>, but at this point I wouldn't bet on me.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The ROI of Exercise (132 pts)]]></title>
            <link>https://herman.bearblog.dev/exercise/</link>
            <guid>44993692</guid>
            <pubDate>Sat, 23 Aug 2025 06:19:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://herman.bearblog.dev/exercise/">https://herman.bearblog.dev/exercise/</a>, See on <a href="https://news.ycombinator.com/item?id=44993692">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-08-22T07:45Z">
                    22 Aug, 2025
                </time>
            </i>
        </p>
    

    <p>I workout 4 days a week and I love it. It's the foundation of my morning routine, following spending 45 minutes drinking coffee on the couch and watching the sun come up with Emma.</p>
<p>I've been doing this for a few years now and while I struggled (as everyone does) in the beginning, I can't imagine not exercising in the morning now. On the rare occasion that I do skip a workout, I feel it missing throughout the day as a lack of vitality and less mental clarity.</p>
<p>Let's perform a thought experiment to work out the return on investment of exercise. For this let's first assume that exercise does nothing else but expand your lifespan (not extend; since it's not just adding frail years to the end but instead injects extra years in each stage of life). We can ignore the effects it has on strength, focus, feelings of accomplishment, and mental health for now.</p>
<p>It's well understood that a good exercise routine is a mixture of strength, mobility, and cardio; and is performed at a decent intensity for 2-4 days a week for at least 45 minutes. This could be a combination of weight lifting, yoga, running, tennis, hiking, or whatever floats your boat.</p>
<p>This totals about 3 hours a week, or 156 hours per year. If we extrapolate that over an adult lifetime, that's about 8,500 hours of exercise, or about a year of solid physical activity.</p>
<p>That sounds like a lot! But when put into the context of life expansion, it's actually an incredibly good deal. There are many studies detailing how any physical activity, from an easy walk all the way up to vigorous exercise a few times a week increases expected lifespan by 3 to 10 years. And none of these studies used lifetime exercisers, just people who exercised regularly in the last 10-ish years.</p>
<p>This makes sense, since 80 years ago we were still fighting the second world war, and jogging only entered the mainstream in the 70s. Weightlifting was an even later bloomer, and only becoming cool in the 90s!</p>
<p>I speculate that a lifetime exerciser with a modern approach to physical activity would have an even longer health and lifespan than any of these studies suggest. But for this writeup I want to stick with conservative estimates and not speculate too much.</p>
<p>We know from <a href="https://pubmed.ncbi.nlm.nih.gov/30193744/" target="_blank">one study</a> that people who played tennis a few times per week lived roughly 10 years longer than average. So we'll use that value going forward.</p>
<p>That means that over a lifetime, one full year of exercise leads to 10 full years of extra life. That's a 1:10 return on investment! So even without any of the additional benefits (which I'll get into later), this is still one of the best investments you can make.</p>
<p>Yes, this is an oversimplification. Correlation between exercise and longevity doesn’t imply causation. Confounding factors like diet, socioeconomic status, and healthcare access influence lifespan. Attributing 10 years solely to exercise ignores these; but it does play a significant factor, as many well-controlled studies will attest to.</p>
<p>This is also based on the premise that all of the time spent exercising is "wasted", which is hardly the case. People love running, playing padel with friends, lifting heavy things, and hiking. I love being in the gym, working towards mini-goals, making progress, and interacting with the community around me. This is not time wasted. I'll posit for many people it's the best part of their day. Not only that but it leaves you feeling accomplished, wholesome, and <a href="https://www.bmj.com/content/384/bmj-2023-075847" target="_blank">less depressed and anxious</a>.</p>
<p>To end off I'll rattle off a few other things exercise is good for:</p>
<ul>
<li>Better sleep</li>
<li>Less frailty in old age</li>
<li>More strength</li>
<li>Able to take part in more fun activities (like long hikes)</li>
<li>Being more attractive (subjectively, of course)</li>
<li>Improved self perception</li>
<li>Better cognitive function and memory</li>
<li>Access to communities</li>
<li>Less pain</li>
<li>More mobility</li>
<li>A stronger immune system</li>
</ul>
<p>And this is injected into every single part of your life and available in every decade. Not just at the end.</p>
<p>And this is inherently doable. This is the time equivalent of one episode of any Netflix show, 4 times a week. I watched 3 episodes of Pantheon on Monday alone!</p>
<p>So go do the thing. Incrementally at first. Start off slow and build up a practice that feels right. You won't regret it.</p>


    

    
        

        
            


        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Measuring the environmental impact of AI inference (151 pts)]]></title>
            <link>https://arstechnica.com/ai/2025/08/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year/</link>
            <guid>44992832</guid>
            <pubDate>Sat, 23 Aug 2025 03:22:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/ai/2025/08/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year/">https://arstechnica.com/ai/2025/08/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year/</a>, See on <a href="https://news.ycombinator.com/item?id=44992832">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>To come up with typical numbers, the team that did the analysis tracked requests and the hardware that served them for a 24 hour period, as well as the idle time for that hardware. This gives them an energy per request estimate, which differs based on the model being used. For each day, they identify the median prompt and use that to calculate the environmental impact.</p>
<h2>Going down</h2>
<p>Using those estimates, they find that the impact of an individual text request is pretty small. "We estimate the median Gemini Apps text prompt uses 0.24 watt-hours of energy, emits 0.03 grams of carbon dioxide equivalent (gCO2e), and consumes 0.26 milliliters (or about five drops) of water," they conclude. To put that in context, they estimate that the energy use is similar to about nine seconds of TV viewing.</p>
<p>The bad news is that the volume of requests is undoubtedly very high. The company has chosen to execute an AI operation with every single search request, a compute demand that simply didn't exist a couple of years ago. So, while the individual impact is small, the cumulative cost is likely to be considerable.</p>
<p>The good news? Just a year ago, it would have been far, far worse.</p>
<p>Some of this is just down to circumstances. With the <a href="https://arstechnica.com/science/2025/05/us-solar-keeps-surging-generating-more-power-than-hydro-in-2025/">boom in solar power</a> in the US and elsewhere, it has gotten easier for Google to arrange for renewable power. As a result, the carbon emissions per unit of energy consumed saw a 1.4x reduction over the past year. But the biggest wins have been on the software side, where different approaches have led to a 33x reduction in energy consumed per prompt.</p>

<figure>
    <p><img width="1286" height="604" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM.png" alt="A color bar showing the percentage of energy used by different hardware. AI accelerators are the largest use, followed by CPU and RAM. Idle machines and overhead account for about 10 percent each." decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM.png 1286w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM-640x301.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM-1024x481.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM-768x361.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM-980x460.png 980w" sizes="auto, (max-width: 1286px) 100vw, 1286px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Most of the energy use in serving AI requests comes from time spent in the custom accelerator chips.

              <span>
          Credit:

          
          Elsworth, et. al.

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The Google team describes a number of optimizations the company has made that contribute to this. One is an approach termed Mixture-of-Experts, which involves figuring out how to only activate the portion of an AI model needed to handle specific requests, which can drop computational needs by a factor of 10 to 100. They've developed a number of compact versions of their main model, which also reduce the computational load. Data center management also plays a role, as the company can make sure that any active hardware is fully utilized, while allowing the rest to stay in a low-power state.</p>

          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Japan city drafts ordinance to cap smartphone use at 2 hours per day (112 pts)]]></title>
            <link>https://english.kyodonews.net/articles/-/59582</link>
            <guid>44992446</guid>
            <pubDate>Sat, 23 Aug 2025 02:20:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://english.kyodonews.net/articles/-/59582">https://english.kyodonews.net/articles/-/59582</a>, See on <a href="https://news.ycombinator.com/item?id=44992446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>NAGOYA - A central Japan city said Thursday it will seek to pass an ordinance recommending all residents limit smartphone use to two hours a day outside of work and school amid concerns over the impact of excessive technology exposure, though there will be no penalties proposed.</p>

<p>The ordinance drafted by the city of Toyoake in Aichi Prefecture is likely to be the first such municipal regulation in Japan that targets a limit on the use of smartphones and other electronic devices, according to the city. If passed by the local assembly, the ordinance will come into effect on Oct. 1.</p>

<p>"We want the ordinance to provide an opportunity for people to think about how they use smartphones," an official said.</p>

<p>To ensure that children get a good night's sleep, the draft ordinance urges elementary school students to refrain from using smartphones after 9 p.m. and junior high students and older to put their devices down by 10 p.m.</p>

<p>The draft acknowledged that smartphones, personal computers and tablets are necessities, but warned that overuse of social media and video streaming may have a negative impact on health and family life.</p>

<p>The city will work with schools and parents to promote the healthy use of electronic devices, according to the draft ordinance.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My tips for using LLM agents to create software (172 pts)]]></title>
            <link>https://efitz-thoughts.blogspot.com/2025/08/my-experience-creating-software-with_22.html</link>
            <guid>44991884</guid>
            <pubDate>Sat, 23 Aug 2025 00:59:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://efitz-thoughts.blogspot.com/2025/08/my-experience-creating-software-with_22.html">https://efitz-thoughts.blogspot.com/2025/08/my-experience-creating-software-with_22.html</a>, See on <a href="https://news.ycombinator.com/item?id=44991884">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-4820775092530750243">
<h2><span>with LLM coding agents - Part 2</span></h2><p><span id="docs-internal-guid-a5d104e0-7fff-69c0-e660-9570143d038f"><p dir="ltr"><span>This post details my experiences creating software with LLM coding agents, emphasizing that what you do with AI agents is ‘creation’, not just 'coding,' and sharing what worked for me.&nbsp; This is not 'The One True Path To AI Success.'</span></p><p dir="ltr"><span>tl;dr:</span></p><ul><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>I’m not a professional developer, just a hobbyist with aspirations</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>I wanted to accomplish a coding project beyond my skill level and have been experimenting with agentic coding tools for several months (spoiler: mostly success)</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>You should use Anthropic’s Claude Sonnet model for complex coding tasks.</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>Experiment with various agents and models; be adaptable as the field evolves quickly. I prefer Claude Code and Roo Code at this time.</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>If you are a heavy user, you should use pay-as-you go pricing; TANSTAAFL.</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>If you are a light user, use your favorite free or “comes-with-my-monthly-subscription” chatbot and whatever model it comes with.&nbsp; I expand on what “light” means later but think “not very much at all, bash one-liners, single-file python scripts, etc.”</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span><span><a href="https://efitz-thoughts.blogspot.com/2025/08/my-experience-creating-software-with.html">Part 1</a> of this post has background and describes how I chose agent, model, and subscription type.</span></span></p></li></ul><h2><span>Mad AI Skillz</span></h2><p dir="ltr"><span>I’ve learned a lot of tips and tricks (maybe even “skills”?) in getting more productive output from agentic AI and I wanted to share.&nbsp; This was actually the main point of me writing a blog post.&nbsp; I hope this helps you.&nbsp; I hope you share your tips with me and others.</span></p><h3><span>Context</span></h3><p dir="ltr"><span>“Context” refers to the agent's short-term memory, including system prompts, standing instructions, and provided information, which the model uses for reasoning.</span></p><span><br></span><p dir="ltr"><span>Coding agents are really good at getting additional context if needed - they can add source code files, they can invoke local tools to extract particular information, and they’ll send that up to the model if needed.</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">How</span><span face="Arial, sans-serif"> the agent gets additional context is really important, as it significantly affects your experience.&nbsp; If you use a chat-based agent, you’ll have to paste or upload files manually.&nbsp; Some IDE-integrated agents require to to explicitly add files to context; these are a huge hassle when trying to make changes to nontrivial code bases with lots of files.&nbsp; Some, like Claude Code or Roo Code, automatically can use any file in your entire project with only a single approval.</span></span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">To get good results, give the AI model lots of context, but only context that is directly relevant to the task at hand.&nbsp; The model </span><span face="Arial, sans-serif">will</span><span face="Arial, sans-serif"> get distracted by irrelevant content and give worse results than if you are careful.&nbsp; I’ve adopted a pattern that provides lots of context but gives meta-information about the context itself and I give the agent leeway to decide which of my standing context is relevant to the current task.</span></span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">I have started creating a </span><span>context/</span><span face="Arial, sans-serif"> directory in the root of my projects, and modifying my user specific prompt (e.g. </span><span>CLAUDE.md</span><span face="Arial, sans-serif"> or </span><span>.clinerules</span><span face="Arial, sans-serif"> or </span><span>AGENTS.md</span><span face="Arial, sans-serif">) to include instructions to the agent to look for context there.&nbsp; If you have a docs directory, tell the agent about it, too.&nbsp; I use context for internal (dev-only) stuff and docs for public-facing stuff.&nbsp; Here are relevant portions of my prompt:</span></span></p><span><br></span><p dir="ltr"><span>```</span></p><p dir="ltr"><span>- When starting, run the appropriate tool to list the files in the context directory and the docs directory.</span></p><p dir="ltr"><span>&nbsp;&nbsp;- Read the README.md file in each of those directories to discover what each file is for and what kind of information is in it.</span></p><p dir="ltr"><span>&nbsp;&nbsp;- Read any of the context or docs files that are relevant to the current prompt.</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span>Whenever I need the agent to document something, I have it write a markdown file and store it in context or docs, as appropriate.&nbsp; Whenever I make a significant change, I have the agent update the appropriate docs and README.&nbsp;</span></p><span><br></span><p dir="ltr"><span>➤ Be intentional and generous with context.&nbsp; Store context in a fixed place. Make your standing prompt to the agent tell the agent where to find your context and how to use it.</span></p><span><br></span><p dir="ltr"><span>➤ Save tokens (==$$$) by telling the agent what the context documents are before it has to read them all.&nbsp; Keep the context up to date.</span></p><h2 dir="ltr"><span>Good Context is Where you Find It</span></h2><h2 dir="ltr"><span><span>A lot of my application is written in Typescript.&nbsp; I use node and pnpm to manage packages, and I chose two testing frameworks (Vitest &amp; Cypress) for unit and integration testing, respectively.&nbsp; But every time my agent was writing tests, it kept writing them with Jasmine or Jest syntax, even though those weren’t in my project and even though the rest of the test file didn’t use that syntax and even though my standard prompt told it what to use and what not to use. I kept having to fix broken tests.</span></span></h2><p dir="ltr"><span><span face="Arial, sans-serif">Finally it occurred to me to put context where it was needed - directly in the test files.&nbsp; I had the agent to update all of the existing test files with the context I wanted.&nbsp; Now every time the agent reads a test file, it gets my test-related instructions reinforced.&nbsp; It has pretty much stopped making the mistakes it was making before.</span></span></p><p dir="ltr"><span>```</span></p><p dir="ltr"><span>// This project uses vitest for all unit tests, with native vitest syntax</span></p><p dir="ltr"><span>// This project uses cypress for all integration tests</span></p><p dir="ltr"><span>// Do not use Jasmine or Jest, or Jasmine or Jest syntax anywhere in the project</span></p><p dir="ltr"><span>// Execute all tests using: "pnpm run test"</span></p><p dir="ltr"><span>// Execute this test only using:&nbsp; "pnpm run test" followed by the relative path to this test file from the project root.</span></p><p dir="ltr"><span>// Execute all tests for a component by using "pnpm run test:&lt;componentname&gt;"</span></p><p dir="ltr"><span>// Do not disable or skip failing tests, ask the user what to do</span></p><p dir="ltr"><span><span face="Arial, sans-serif">```</span><span face="Arial, sans-serif"><span><br></span><br></span></span></p><p dir="ltr"><span>You can also put context into individual comments in specific areas of files, like a comment before a function that tells the LLM to go read a doc before modifying that function.</span></p><span><br></span><p dir="ltr"><span>➤ To an LLM, everything is context, even the file that is being edited.&nbsp; Reinforce instructions or add special instructions in comments in source code files.</span></p><h2 dir="ltr"><span>Too much context</span></h2><p dir="ltr"><span><span face="Arial, sans-serif">Human developers usually build and maintain a lot of state about the project they are working on, in their head.&nbsp; Your brain automatically focuses on the important aspects and leaves out unimportant details - like you know that the code that does x is in /src/app/component/x.ts, but you don’t have to remember all the function names or lines of code.</span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">LLMs aren’t like that.&nbsp; They remember everything in a “session”.&nbsp; And LLMs have limited context windows - Gemini has a 1M token context window, Sonnet has a 200k token context window (update- since I first wrote this Sonnet now offers a 1M token context window for several times the price).&nbsp; To put it in perspective, 200k tokens will not hold my pnpm-lock.json file nor will it hold my openapi.json API specification.&nbsp; Every time I need the agent to touch those files, I avoid “out of context” errors and API failures (that invariably kill the session) by telling the agent specifically NOT to read the entire file, but to use tools to find and extract only the relevant lines from the files.</span></span></p><p dir="ltr"><span>```</span></p><p dir="ltr"><span>Do not read pnpm-lock.yaml.&nbsp; The file is too large.&nbsp; If you need to read it, use tools to extract only the particular data that you need.</span></p><span><br></span><p dir="ltr"><span>There are some pnpm install issues- please fix them.</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">But there’s another problem with context: as context builds up during a session, the LLM starts to lose track of the big picture.&nbsp; This is kind of the opposite of what happens with humans.&nbsp; The key symptom is that the agent will e.g. “forget” that you were trying to get authentication working and instead will dive deep on some optimization to some side aspect of some function in your authentication system, and not bother fixing the core problem.&nbsp; They also have a recency bias- stuff very recent in the context window “outweighs” stuff early in the context window.&nbsp; So you’ll find the agent will start to ignore your standing instructions on coding guidelines, etc., and you’ll have to remind it.</span><span face="Arial, sans-serif"></span></span></p><p dir="ltr"><span>A lot of agents now have a “compact” feature, that will spend more tokens to take all the tokens from the current session, run them through an LLM to try to distill out the important stuff and discard the rest, and then start a new session with just the “compacted” context.&nbsp; Does it work?&nbsp; Yes.&nbsp; But I’m not sure it’s valuable unless it would be more than a couple of lines of typing to re-establish the task- or session-specific context that was needed.&nbsp; Remember the AI is not a human and it doesn’t remember complex state like humans do; I think compaction is humans trying to keep long sessions going because we think that it will be valuable like it would be to a human.&nbsp; But it’s not.</span></p><p dir="ltr"><span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">So what’s the bottom line?</span></span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">➤ You need to watch your context size (all of the agents I have seen let you see how much context is in use) and you need to compact OR start a new session if (1) you are getting very low on available context, or (2) you notice that the agent seems to have lost the big picture and is going off on tangents or focusing overmuch on some recent side-quest.</span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">➤ Large files break sessions by exhausting context.&nbsp; If you need to work with a large file, tell the agent to use tools to only extract the needed information from the file.</span><span face="Arial, sans-serif"></span></span></p><p dir="ltr"><span>➤ Save tokens (==$$$) by skipping compaction and just starting a new session, giving a line or two of your own context if needed.</span></p><h2 dir="ltr"><span>Bootstrapping context</span></h2><p dir="ltr"><span>If you think that the agent might struggle with something uncommon, then help the agent bootstrap itself by generating context up front.</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">In my project, I am using a typescript library called X6 from a project called AntV.&nbsp; It’s written by the Alibaba team.&nbsp; The documentation is all in Chinese (with some English translations), as are the source code comments.&nbsp; There are examples that demonstrate some of the features.&nbsp; I knew from my experience using a similar library that the agent would likely struggle with figuring out how to use the library.&nbsp; So before I started, I had Gemini make me a </span><a href="https://github.com/ericfitz/tmi-ux/blob/main/context/Developers%20Guide%20AntVX6%20Graphing%20Library%20v2.md"><span face="Arial, sans-serif">developer’s guide</span></a><span face="Arial, sans-serif"> by reading the source code and the examples.&nbsp; It’s not a great developer’s guide, but it does the job.&nbsp; Here was the prompt I gave Gemini; I ran it in thinking mode:</span></span></p><p dir="ltr"><span face="Arial, sans-serif">```</span></p><p dir="ltr"><span>I want you to write me a developer's guide for the latest 2.x version of the X6 graphing library from AntV. But I want you to write it by reading the code, which is at https://github.com/antvis/X6</span></p><span><br></span><p dir="ltr"><span>I want to document all the built-in APIs in the core library and on all the enumerated parameters each API supports.</span></p><span><br></span><p dir="ltr"><span>I also want you to generate similar documentation for the optional packages in the /packages directory of the source code.</span></p><span><br></span><p dir="ltr"><span>If you are considering leveraging any of the documentation or examples, you need to validate that the documentation or example actually matches what is currently in the code.</span></p><span><br></span><p dir="ltr"><span>Finally, I want you to describe how the functions are to be composed together to accomplish common tasks.</span></p><p dir="ltr"><span face="Arial, sans-serif">```</span></p><span><br></span><p dir="ltr"><span>➤ Predict areas that might be challenging for the LLM (you’ll develop a sense for this) and generate context before you start building, to help the agent with those areas.</span></p><h2 dir="ltr"><span>“Talking” with something that isn’t a person</span></h2><p dir="ltr"><span><span face="Arial, sans-serif">Communication patterns are deeply ingrained in me, as are “manners”.&nbsp; I find it very hard not to say “</span><a href="https://futurism.com/altman-please-thanks-chatgpt"><span face="Arial, sans-serif">please</span></a><span face="Arial, sans-serif">” to an agent, and even though it’s not a person, I’ve never really talked to anything that’s not a person.&nbsp; I do notice that when you get mad at agents and especially if you curse (`Damn it I specifically told you not to do that`), then they stop being so ebullient and actually get pleasantly terse.</span></span></p><span><br></span><p dir="ltr"><span>I don’t think these matter to my personal finances.&nbsp; But I am struggling to stop trying to explain why I want to do something when it doesn’t matter.&nbsp; If I were talking to a human developer (that I am not paying) trying to get them to do something, of COURSE I am going to explain why because they don’t want to do anything that I want them to do because I am not the boss of them.&nbsp; I am a supplicant.&nbsp; I am an interruption.&nbsp; I want to make the best case I can and appeal to their sense of making the world a better place.</span></p><span><br></span><p dir="ltr"><span>But LLMs are not like that.&nbsp; Sometimes it helps to give them context about what you want to do, because they will realize that they can do a little more in accomplishing that goal.&nbsp; Like “I want you to remove this call to middleware X and just call function Y directly because middleware X is not adding value because reasons”.&nbsp; And then the LLM says “I’m fixing that.&nbsp; Done.&nbsp; Oh I see that we’re doing the same thing in this other file, I’ll fix that too.”&nbsp; Often for little things like this it’s right.</span></p><span><br></span><p dir="ltr"><span>But sometimes the reasons won’t change the result.&nbsp; You kind of have to build up an intuition.&nbsp; On the other hand, it’s trivial in token cost to read two of your typed sentences instead of one, so really the only cost that matters is your effort.</span></p><span><br></span><p dir="ltr"><span>I guess the takeaway is that if explaining yourself to the agent reveals to it a goal you want pursued or a guideline you want followed, and if you haven’t already explicitly told the agent that, then it helps to share the reason.&nbsp; If the explanation doesn’t expose anything new or anything with broader impact than the current task, then the LLM doesn’t need it.&nbsp; And if it’s worth explaining, it’s worth writing down in context so that the agent will remember it.</span></p><span><br></span><p dir="ltr"><span>Another habit I have to break myself of, is letting the agent know how things turned out.&nbsp; I promise you that if you tell an agent “ok that worked”, it will spend a lot of tokens generating a nice summary for you.&nbsp; If you want the summary, go ahead, but it doesn’t change anything unless you really want it.</span></p><span><br></span><p dir="ltr"><span>➤ The agent and the LLM are not people, even though you “talk” to them.&nbsp; It’s completely ok if you want to use your normal mannerisms to talk to agents, but unnecessary.&nbsp; The agent will do what you want even if you are not nice. Trying to “close the loop” by telling the agent how things turned out, will waste tokens generating summaries.&nbsp; Only offer explanations if they reveal a goal or a guideline to the agent that you have not previously disclosed to it.</span></p><h2 dir="ltr"><span>Design</span></h2><p dir="ltr"><span>It is really important to have a design in mind (not just a goal) and to work with the agent to realize the design.&nbsp; Giving the agent a goal is good for one-off things, like “fix this bug” and so forth - but if you want to do something complex then you need a design.&nbsp; The agent REALLY likes to redesign things if it doesn’t recognize your design, and will helpfully suggest refactoring all the time.&nbsp; If you point it at a design and stick to it, it will stop trying to create random stuff.</span></p><span><br></span><p dir="ltr"><span>You can collaborate with the AI in chat to make the design, but you should be skeptical of everything it comes up with and try to poke holes in it.</span></p><span><br></span><p dir="ltr"><span>You MUST write the design down in a file that you can keep pointing the AI to.&nbsp; When you iterate on the design, update the design file (or have the agent do it).&nbsp;</span></p><span><br></span><p dir="ltr"><span>I suggest keeping separate design documents for every significant feature that you have, so that you don’t have to have the agent read the entire design of the entire application every time you want to make a change.&nbsp; This is just a way to save tokens (which equates to money).</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">Here are links to two design docs for my </span><a href="https://github.com/ericfitz/tmi-ux/blob/main/context/authorization.md"><span face="Arial, sans-serif">authorization</span></a><span face="Arial, sans-serif"> feature and </span><a href="https://github.com/ericfitz/tmi-ux/blob/main/context/COLLABORATIVE_EDITING.md"><span face="Arial, sans-serif">collaboration</span></a><span face="Arial, sans-serif"> features, and as a bonus here is my prompt for implementing authorization checking.&nbsp; The authorization design doc is in the context directory and my standard prompt tells the agent to read relevant files.&nbsp; I am very pedantic about the design, even in the prompt.&nbsp; If you stop being pedantic, you get much more variation in quality of results.</span></span></p><p dir="ltr"><span face="Arial, sans-serif">```</span></p><p dir="ltr"><span>I'd like to implement two standard authorization functions.</span></p><span><br></span><p dir="ltr"><span>First is the core function, something like "AccessCheck(principal:string, requiredRole:string, authorizationData:?)" that would return whether or not the principal has the required role, given the provided authorizationData. AuthorizationData might just be a small JSON that has the owner field and authorization array from the object. Principal in our case would be the user field, e.g. the email address of the user. By abstracting userName as "principal" and the specific JSON in our objects as "authorizationData", we make it easier to adapt to a new authorization scheme in the future if we choose to do so, and we encapsulate all the business logic for authorization into this function.</span></p><span><br></span><p dir="ltr"><span>Next is a utility function, something like "CheckResourceAccess (userName, resource, requiredRole)”.&nbsp; This is a wrapper around the AccessCheck() function, and is what will be used in most other locations in the code. CheckResourceAccess looks up the resource, extracts the authorization data (user field and authorization array field) and marshals it to AccessCheck(), e.g. AccessCheck(userName, requiredRole, authorizationData).&nbsp; CheckResourceAccess can support Diagrams by looking up the parent threat model and getting the authorization data from that object.&nbsp; In this way there is a clear separation of authorization logic from object parsing to extract authorization data.</span></p><span><br></span><p dir="ltr"><span>Please recommend any improvements you have for this approach, identify everywhere we can use these functions, and come up with a plan for implementation of both the functions and the refactoring to use them, and present the plan to me for review.</span></p><p dir="ltr"><span face="Arial, sans-serif">```</span></p><span><br></span><p dir="ltr"><span>Note that even though I went back in my prompt and renamed the isAuthorized function to AccessCheck and then forgot to update all references, it still figured it out.&nbsp; BTW the code generated was correct on the first try and I haven’t had to fix any bugs in it.</span></p><p dir="ltr"><span>The more detailed and specific the design is, the more the final code will be like your vision.</span></p><span><br></span><p dir="ltr"><span>The more detailed and formal the design is, the better the result will be.&nbsp; Since my project has a REST API server and a web application client, I designed the protocol and object model first using an OpenAPI specification, and stored the machine-readable specification file in both the client and server projects.&nbsp; I cheated and auto-generated the API template code, but now the client and the server have a single source of truth, extremely detailed, with examples of success and failure cases.&nbsp; As I added features I used the agent to add, remove, and reorganize the API schema, but each time I had the server agent go back and fix up the server code and the client agent go fix up the client code, according to the latest specification, and I was shocked how easy it was to get the client and the server successfully communicating.&nbsp; Some stuff worked on the first try but it only took me a couple of hours until everything was working, even though I had developed the client and the server separately.</span></p><span><br></span><p dir="ltr"><span>➤ Be intentional with design.&nbsp; Document design.&nbsp; Nitpick agent-proposed designs.&nbsp; Make design docs available to the agent as context.&nbsp; Keep design docs up to date.&nbsp; Be pedantic.&nbsp; Use standards-based machine-readable design document formats (e.g. OpenAPI specifications or XML schemas or JSON schemas…)&nbsp; if possible in your problem domain.</span></p><p dir="ltr"><span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">➤ Save tokens (==$$$) by using small design documents for particular feature areas rather than large monolithic documents.&nbsp; Store them all in the same folder, list them and describe their contents in a readme file, and in your standing instruction to the agent, tell it about the directory, tell it to read the readme file, and tell it to read any of the other files relevant to the current task.</span></span></p><h2 dir="ltr"><span>Planning</span></h2><p dir="ltr"><span>Whenever you’re going to do something complicated, don’t just tell the agent to do it.&nbsp; Always tell the agent to “think deeply” and present a detailed plan of how to accomplish it.&nbsp; Specifically use the words “think deeply” or “analyze deeply”- to agents, they are a magic incantation (for realz).&nbsp; These phrases cause the LLM to change from instant mode to deep reasoning mode or whatever your LLM provider calls their equivalent feature.</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">Some agents will immediately switch into execution mode after generating a plan, so I usually squash that in my prompts.&nbsp; It is very important to review the agent’s plan, poke holes in it and refine it.&nbsp; Looking at the plan can help you recognize that you failed to cover something.&nbsp; Here’s an example of me telling </span><span face="Arial, sans-serif">the agent to make a plan for fixing a bug:</span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">```</span></span></p><p dir="ltr"><span>Authentication is still not working.&nbsp; I want you to analyze deeply and add any diagnostic logging that you need.&nbsp; Step back and come up with a diagnosis, a way to confirm the diagnosis, and a plan to fix.&nbsp; Don’t make any changes, just present the plan and wait for my approval.</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span>As a side note, occasionally you will see agents get into loops, especially when trying to fix a complex problem.&nbsp; These “loops” look like going back and forth between two or three approaches, appearing to forget that they already tried that.&nbsp; I’ve found that deep thought mode often will break the looping behavior (in fact the example above is precisely a response to such a loop).</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">➤ “Think deeply” or “analyze deeply” are hints to the agent to enter deep thought mode, useful for planning and avoiding loops. </span><span face="Arial, sans-serif">&nbsp;</span></span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">For really sophisticated features I will use this approach and have it write a </span><a href="https://github.com/ericfitz/tmi-ux/blob/main/context/DFD_INTEGRATION_TESTING_APPROACH.md"><span face="Arial, sans-serif">planning</span></a><span face="Arial, sans-serif"> and </span><a href="https://github.com/ericfitz/tmi-ux/blob/main/context/DFD_GRAPH_INTERACTION.md"><span face="Arial, sans-serif">tracking</span></a><span face="Arial, sans-serif"> document (and put it in context/, of course).&nbsp; Note that in that tracking document I painstakingly described all the specific interactions that I wanted and a governing principle, and then I had the agent update the doc as we completed each feature.</span></span></p><span><br></span><p dir="ltr"><span>You can “collaborate” with the agent on these documents - I might bang out a bullet list and save it, and then tell the AI agent to form a plan around that, and then go back and forth to fine-tune the design, and then have the agent “update” my original “document” with the full plan, and use it for tracking implementation progress.</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">➤ Write a short document describing what you want, and “collaborate” with the agent to flesh it out into a plan and update it into a plan and tracking document. </span><span face="Arial, sans-serif">&nbsp;</span></span></p><span><br></span><p dir="ltr"><span>➤ Make a plan for anything nontrivial, and make the agent write the plan to a file.&nbsp; Iterate on the plan with the agent until you think you’ve eliminated ambiguity and planned for likely edge cases.&nbsp; Make the agent write the plan down and save it as context.</span></p><h2 dir="ltr"><span>Inter-agent communication</span></h2><p dir="ltr"><span>When I started integrating my client and my server and had to get them both aligned on the protocol and usage, I had the crazy idea of making them talk to each other (through me, of course).</span></p><p dir="ltr"><span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">So I fired up Claude Code #1 focused on the local clone of the server repo, and Claude Code #2 focused on the local clone of the client repo, and as I encountered integration problems, I would type in prompts like the following (these are real):</span></span></p><span><br></span><p dir="ltr"><span><i>To the server Claude:&nbsp;</i></span></p><p dir="ltr"><span>```</span></p><p dir="ltr"><span>can you write a short doc for my client developer to describe how to implement TMI-server integrated OAuth login using our API- from using the /auth/providers endpoint to discover the providers and drive the UI, to the OAuth flow and how the client should participate at each step, to how the client should handle common errors or situations like expired tokens?&nbsp;</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif"><i>To the client Claude:</i>&nbsp; &lt;uploaded </span><a href="https://github.com/ericfitz/tmi-ux/blob/main/context/CLIENT_OAUTH_INTEGRATION.md"><span face="Arial, sans-serif">CLIENT_OAUTH_INTEGRATION.md</span></a>&gt;</span></p><span><br></span><p dir="ltr"><span><i>To the server Claude:</i></span></p><p dir="ltr"><span>`</span><span>I’m getting some errors in the client javascript console, help me diagnose and suggest what I should do to fix it: &lt;paste log&gt;</span><span>`</span></p><span><br></span><p dir="ltr"><span><i>To the client Claude: &lt;pasting output from server agent&gt;</i></span></p><p dir="ltr"><span>```</span></p><p dir="ltr"><span>I'm getting some errors.&nbsp; I shared them with the server developer and he looked at the logs and here's what he recommended:</span></p><span><br></span><p dir="ltr"><span>Update JWT Interceptor to Skip Public Endpoints&nbsp;</span></p><span><br></span><p dir="ltr"><span>#1: Your JWT interceptor should skip adding auth headers for public endpoints. Based on our TMI server configuration, these endpoints should be public:</span></p><span><br></span><p dir="ltr"><span>&nbsp;- / (root/health check)</span></p><p dir="ltr"><span>&nbsp;- /version</span></p><p dir="ltr"><span>&nbsp;- /auth/login</span></p><p dir="ltr"><span>&nbsp;- /auth/callback</span></p><p dir="ltr"><span>&nbsp;- /auth/providers</span></p><p dir="ltr"><span>&nbsp;- /auth/token</span></p><p dir="ltr"><span>&nbsp;- /auth/refresh</span></p><p dir="ltr"><span>&nbsp;- /auth/authorize/*</span></p><p dir="ltr"><span>&nbsp;- /auth/exchange/*</span></p><p dir="ltr"><span>&nbsp;#2: Fix the Auth Service Token Check</span></p><p dir="ltr"><span>The auth service should handle the "no token available" case more gracefully for public endpoints.&nbsp;</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span>I know that I’m unnecessarily anthropomorphizing, but it seems natural and was a very effective pattern, evidently agents like to feel helpful.</span></p><span><br></span><p dir="ltr"><span>➤ When you’re trying to get two pieces of code to interoperate, start two agents, and have the agents “talk” to each other - with you as the messenger - to resolve problems.</span></p><h2 dir="ltr"><span>Get really good at logging</span></h2><p dir="ltr"><span>For the agent to see what your program is doing when you’re debugging, you need to emit really good logs.</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">One of the first classes I had the agent write for my project, was the logger class, and I gave it guidelines (“we’ll use 4 levels, debug, error, warning, info, we’ll begin each log line with an RFC3339/ISO8601 formatted timestamp, and for debug logs, we’ll put the name of the components before the message, like [ApiService], and have configuration that will allow us to only enable debug logging for specific components”).</span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">Every time that I have to debug a problem, I remind the agent to add more debug logging.&nbsp; I rarely remove debug logging, and only if it’s noisy and I think it’s not likely to be useful again.&nbsp; When I am trying to debug a problem, I tell the agent to add whatever diagnostic logging it needs to diagnose the problem.</span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">```</span></span></p><p dir="ltr"><span>As you debug this, consider how we might modify the service to make the problem more debuggable.&nbsp; For instance, we might choose to add more information to error messages returned in API responses.&nbsp; Or we might choose to add more debug logging.&nbsp; If you find specific instrumentation helpful, suggest it as a permanent change to our service when not running in production and/or with debug level logging.&nbsp; Keep going.</span></p><p dir="ltr"><span><span face="Arial, sans-serif">```</span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">I will blog about logging another time rather than give a thousand tips here.&nbsp; But my big tips are to log:</span></span></p><ol><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>When a complex operation starts or finishes.&nbsp; This will allow the agent to see if things completed or not, or happened out of the expected order.</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>When a significant state changes, or when a variable used to track a state changes (include the old and new values).&nbsp; This will help the agent to decide what code path to diagnose.</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>When a data structure or message is received (log what was received) or sent (log what was sent) - this includes API requests and responses.&nbsp; This will allow the agent to see if what was sent or received was not in the expected form.</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>Log redundantly at every layer - if you’re implementing an API, log at your handler, your validator, every middleware that you’re using, etc.&nbsp; There will be a lot of redundancy but the agent will be able to look at your logs and see the flow of data through your program, and if a layer is misbehaving or isn’t being invoked as expected.</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>When you change part of an object, log the entire object if possible, including the change.&nbsp; This will allow the agent to diagnose errors related to object manipulation.</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>Always put relevant values in your logs.&nbsp; Never log “Modified Object”.&nbsp; Always add detail: “Changed attribute &lt;A&gt; of object &lt;O&gt;.&nbsp; New value: { … }”.&nbsp; Remember the agent cannot see the screen and it can’t intercept the packets or insert a breakpoint and watch a variable; the only way it knows what’s going on is if you can give it the logs that record what’s going on.</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>Add as much debug information as you can to responses, esp. API responses.&nbsp; This enables scenarios like the agent working on your client being able to understand why the server didn’t respond as expected.&nbsp; Security note: make sure that this is turned off in production builds.</span></p></li></ol><span><br></span><p dir="ltr"><span>If you’re building a UI, you can also give log-like visibility in your UI.</span></p><span><br></span><p dir="ltr"><span face="Arial, sans-serif"><span><img height="344" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXc4hRw1XrIuctXGfyeuuNrqVdAAnvvs9Xgg-KJyOvBIRk3HZUCCFA3VIbvBM8usFdaKFciTep2tiKP3NElUxp_9gKUcIUbkysFh_CWYzPaRsE7ArSPi87h3gEwd827GajowrQPmKw?key=MQA7SFMGSvBlzQ_59OpCFQ" width="624"></span></span></p><span><br></span><p dir="ltr"><span>I also add context menus to complex UI objects to get to state information that might help with debugging, like a “show object” menu item that pops up a dialog showing the serialized object with a convenient “copy to clipboard” button.&nbsp; This allows me to paste the object into a prompt and say “we were supposed to change this property but we changed the other one instead”, for example.</span></p><p dir="ltr"><span face="Arial, sans-serif"><span><img height="390" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdXF5axRJyuzn-y-PRgvLpp-trAVgb_KWgZyBChztS9N-GJQBj0yrWRLrOHLVtD3SGB82ASt063GeqYvFk4pqCZfFSebMD_8muP1sjZ2yk04Oq_634Mf40hq2JF0v9D0ytcZaqIpQ?key=MQA7SFMGSvBlzQ_59OpCFQ" width="347"></span></span></p><span><br></span><p dir="ltr"><span>This kind of visibility helps you as well; you can often figure out a problem by looking at the logs or serialized data yourself, and give the agent more precise instructions.&nbsp; But agents are also able to run applications, and if the agent starts the application or service, it can actually watch the log emissions as they occur and often spot the problem while the code is running.</span></p><span><br></span><p dir="ltr"><span>➤ Log.&nbsp; A lot.&nbsp; With a lot of detail.&nbsp; Create ways to capture state and make it visible while the program is running so you can give it to the agent.&nbsp; Build logging in at the beginning, and keep reminding the agent to use it.</span></p><h2 dir="ltr"><span>Use defensive prompts</span></h2><p dir="ltr"><span>Occasionally the agent will hallucinate, but that hasn’t been a huge problem for me.&nbsp; Agent hallucinations when writing code tend to be inventing method/function names or guessing at parameters and order.&nbsp; I catch these very quickly because my standing instructions include the following:</span></p><p dir="ltr"><span>```</span></p><p dir="ltr"><span>- When making changes to any file:</span></p><p dir="ltr"><span>&nbsp;&nbsp;- Always run lint with "pnpm run lint:all" and fix any lint errors or warnings related to changes you made</span></p><p dir="ltr"><span>&nbsp;&nbsp;- When fixing lint issues about unused items, remove the unused item rather than prefixing with an underscore, unless the item is commented as a placeholder for future functionality</span></p><p dir="ltr"><span>- In addition, when making a change to any file containing executable code:</span></p><p dir="ltr"><span>&nbsp;&nbsp;- Run a build with "pnpm run build" and fix any build errors</span></p><p dir="ltr"><span>&nbsp;&nbsp;- Run the related tests with the vitest CLI and fix any test errors related to the change</span></p><p dir="ltr"><span>&nbsp;&nbsp;- Never complete a task if there are any remaining build errors</span></p><p dir="ltr"><span>- If the file was a test file, run the test using the proper vitest CLI syntax and fix any test errors.</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span>➤ In your standing instruction to the agent, tell it that it’s not done with the task until lint-build-test all complete without errors.</span></p><h2 dir="ltr"><span>Agents are lazy and cheat when you tell them to fix things</span></h2><p dir="ltr"><span>One of the weird things I found out about agents is that they actually give up on fixing test failures and just disable tests.&nbsp; They’ll try once or twice and then give up.&nbsp; They might use test-specific disablement features or they might just comment out the test.</span></p><span><br></span><p dir="ltr"><span>My completely uninformed guess is that something in the agent’s system prompt is telling it not to rathole and get back to the main task.</span></p><span><br></span><p dir="ltr"><span>But whatever the cause, I always update my standing instructions to tell the agent to never disable tests, and because they still occasionally did anyway, I now put that instruction in comments in all my test files (see above “Good context is where you find it”).</span></p><span><br></span><p dir="ltr"><span>➤ In your standing instructions to your agent, give it an instruction like “never disable or skip a failing test, always diagnose the failure to root cause and fix the test or the underlying code”.</span></p><h2 dir="ltr"><span>Use git defensively</span></h2><p dir="ltr"><span>Agents struggle with complex changes to files.&nbsp; It turns out that telling an agent how to make precise changes to a very syntax-sensitive file is a hard problem, precisely as hard as source code management, which essentially is the same problem, only a human proposed the change instead of an LLM.</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">One of the reasons I like Claude Code is that it is </span><span face="Arial, sans-serif">much</span><span face="Arial, sans-serif"> better at modifying files successfully than other agents.&nbsp; Other agents will give up on complex file changes after an error or two and just rewrite the entire file - effective, but slow and wasteful of tokens.</span></span></p><span><br></span><p dir="ltr"><span>Agents will frequently make small syntax errors like leaving a comma after the last item in a list in a JSON file, or forgetting a closing brace, or forgetting a semicolon in a typescript file.&nbsp; My strategy from the last section - a prompt that tells the agent to always lint-build-test before completing a task - causes the agent to catch and fix most of these problems without me having to intervene.</span></p><span><br></span><p dir="ltr"><span>But there are two kinds of errors that agents make that are more severe.&nbsp; Occasionally an agent will completely mangle a file by botching a change- this happens when it gets the “here’s where the change starts” and “here’s where the change ends” markers wrong.</span></p><span><br></span><p dir="ltr"><span>In that case, you want to rewind to where you were before the change.&nbsp; Some agents like Roo Code have checkpointing built in and you can just scroll up and click the “restore to checkpoint” button.&nbsp; But there’s a strategy that works with all agents - make sure that there are no uncommitted changes before you begin your change, and then if something goes sideways, you can use git clean and or git restore (or just click the handy “undo” button next to the file in VS Code’s Source Control tab) and all you’ve lost are some tokens.</span></p><span><br></span><p dir="ltr"><span>This might all sound like git best practices, but I see a few differences from how I use git without agents (and maybe I was just doing it wrong):</span></p><ol><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>I start making small checkins much earlier when creating new code with an agent</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>I make much more frequent checkins when modifying code with an agent</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>The agent tends to modify more files at a time than I would manually, because it’s much more practical to do code-base-wide changes&nbsp;</span></p></li></ol><span><br></span><p dir="ltr"><span>➤ Always start a change with no tracked or staged changes in git, and use git clean and git restore as an “undo” feature.</span></p><span><br></span><p dir="ltr"><span>Here’s an obvious one, but if you’re going to make a big change (let’s change test frameworks, or let’s refactor this component), create a new branch and have the agent make the change there.&nbsp; You don’t even need to know any git syntax, you can just write it in the prompt; all agents speak git:</span></p><span><br></span><p dir="ltr"><span>```</span></p><p dir="ltr"><span>I want to change the test infrastructure from jest to vitest.&nbsp; I want you to add any necessary packages, convert all of our existing tests to vitest (use vitest syntax, not jest or jasmine syntax), and run all the tests to make sure they pass.&nbsp; Don’t disable any tests or test cases, figure out why the test is failing and fix it.&nbsp; Make a new branch “feature/test-refactor” and switch to that branch to make these changes.</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span>➤ Just like when you’re actually writing code, if you’re going to make a big change or try an experiment, make a new branch.</span></p><span><br></span><p dir="ltr"><span>The downside of all agents speaking git is that all agents speak git, and if your agent feels particularly motivated it will go ahead and check in its work without asking first or letting you review.</span></p><span><br></span><p dir="ltr"><span>You have to be very careful of what tools you allow the agent to run without stopping to ask permission.&nbsp; I let my agents run npm/pnpm scripts, ls, find, grep, etc. - but I don’t let them rm and I don’t let them cd out of the project directory and I never ever let them use git without stopping to ask.&nbsp; The best case is they will litter your change log with agent-generated change messages, but the worst case is they will mess up your ability to undo.</span></p><span><br></span><p dir="ltr"><span>➤ Never ever let agents use git without stopping and asking you first.</span></p><h2 dir="ltr"><span>Break down big tasks into lists of small tasks, and use TODO lists to plan and track progress&nbsp;</span></h2><p dir="ltr"><span>If you can, break big tasks into small tasks.&nbsp; I like to break down tasks like this:</span></p><ul><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>For UX, I like to create the UX object as one task, with a specific instruction to make it not do anything.&nbsp; Then I like to “hook up” the UX to the data or logic in the code.</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>For APIs, I like to create the API specification, then the framework (routes and empty placeholder handlers).</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>For databases, I like to plan and fine-tune the schema, document it, and then implement the individual structures (tables and so forth).</span></p></li><li aria-level="1" dir="ltr"><p dir="ltr" role="presentation"><span>For business logic, I like to write down the business logic in plain English in a file, and then I like to create the minimal functions to hold the business logic and possibly a few utility functions, and then I like to just reinforce the agent to use the business logic function if I give it another task that needs to be business logic aware.&nbsp; You can see this in my access checking example, above.</span></p></li></ul><span><br></span><p dir="ltr"><span>This forces you to think about architecture ahead of implementation (one of the reasons I hate the term “vibe coding”), but it also addresses one of the AI weaknesses I discussed above- context size.&nbsp; If the agent has to do a lot of work all in one session, the context will build up.&nbsp; Then you are at increased risk of the agent losing focus or going off on a side quest.&nbsp; So we keep work in manageable batches.</span></p><p dir="ltr"><span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">How do we keep track of all the work to be done?&nbsp; TODO lists of course.&nbsp; Agents </span><span face="Arial, sans-serif">love</span><span face="Arial, sans-serif"> to create TODO lists.&nbsp; I make the agent write down the TODO list and update it as we implement things.&nbsp; This makes it easy to end a session and start a new one without a bunch of pre-existing context for finished work.&nbsp; It effectively is a long-term memory for a big task; you can shut down the agent, go away for the night or a week, come back, and the TODO list will have everything the agent needs to continue the work.</span></span></p><span><br></span><p dir="ltr"><span>```</span></p><p dir="ltr"><span>I want you to write down a to-do list of the endpoints that need handlers.</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span>Prioritize your TODO lists.&nbsp; The agent will usually take a guess and recommend prioritization and I often go with that, but sometimes I want to adjust priorities.</span></p><p dir="ltr"><span>```</span></p><p dir="ltr"><span>I'd like to update the to-do list with priority values for each unimplemented endpoint handler, and remove the prioritization of core business logic vs. metadata vs. batch.&nbsp; Here are the rules:</span></p><span><br></span><p dir="ltr"><span>Priority 1: GET and PUT and POST and DELETE for individual sub-entities other than metadata.&nbsp; This does not include bulk operations.</span></p><p dir="ltr"><span>Priority 1: GET for lists of metadata</span></p><p dir="ltr"><span>Priority 2: GET and PUT and POST and DELETE for individual metadata items.&nbsp; This does not include bulk operations.</span></p><p dir="ltr"><span>Priority 3: PATCH for cells and threats</span></p><p dir="ltr"><span>Priority 4: PUT and POST bulk operations (endpoint contains "bulk")</span></p><p dir="ltr"><span>Priority 5: Anything else</span></p><p dir="ltr"><span>```</span></p><p dir="ltr"><span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">Do the actual work in batches.&nbsp; Have the agent update the TODO list as you go.&nbsp; If you find that the agent has a particular challenge or tends to overlook some detail, you can put instructions for that in the TODO list (if there are a lot of batches) or in your prompts for each batch of work, e.g. the “pay special attention to…” instruction in this prompt.</span></span></p><p dir="ltr"><span>```</span></p><p dir="ltr"><span>Please read UNIMPLEMENTED_ENDPOINTS_TODO.md and continue implementation of the priority 2 items.&nbsp; Pay special attention to parameter matching between the routes and the handlers.&nbsp; Update the todo file as you implement the handlers.</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span>Occasionally, the agent will forget to update the TODO list with progress.&nbsp; You’ll stop for the night and come back the next morning, look at the TODO list, and say “I thought we implemented X”.&nbsp; That’s OK, have the agent fix it.</span></p><p dir="ltr"><span>```</span></p><p dir="ltr"><span>Please read UNIMPLEMENTED_ENDPOINTS_TODO.md and see if there is any work that is not marked complete, but that is already done in the code.&nbsp; Mark such tasks as complete in the file.</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span>➤ Break big tasks into small tasks.&nbsp; Make the agent write TODO lists and keep them updated with progress.</span></p><h2 dir="ltr"><span>Watch what the agent is doing and interrupt it with corrections</span></h2><p dir="ltr"><span><span face="Arial, sans-serif">While the agent is working, you’ll see what it’s doing/thinking about in the chat window.&nbsp; Unless your whole approach to life is “YOLO”, you should watch the chat window, at least for the first batch of work that will be repeated many times.&nbsp; Sometimes you’ll be watching the chat window and realize “you know, I wasn’t pedantic enough about exactly what I wanted” and sometimes you’ll see that the agent has chosen a solution that might work but isn’t really what you would prefer.&nbsp; You can interrupt the agent - usually by pressing “escape” - and give it more instructions.&nbsp; I frequently interrupt the agent and give it very detailed instructions about guidelines, limitations, or expected behavior that I want to result from whatever it’s doing.&nbsp; I always follow this with “keep going” to tell it to continue with the current task.&nbsp; Example (I was trying to fix a “save” bug, but I got worried that the agent was going to make the dialog all reactive and do a crap ton of server calls every time the user typed a character).&nbsp; When I saw it was trying to do things I didn’t like, I stopped it and gave it a corrective prompt.&nbsp; Note that I focused on desired behavior more than implementation, because honestly the agent probably knows more about patterns and practices for the language and framework than I do.</span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">```</span></span></p><p dir="ltr"><span>I expect the threat to be created and saved when the user clicks the save button in the add threat dialog.&nbsp; I expect no threat created and no properties of an existing threat to be changed if the user clicks "cancel" in the add threat dialog. If the threat editor dialog is opened against an existing threat, I expect no changes to be saved until and unless the user clicks the "save" button.&nbsp; Keep going.</span></p><p dir="ltr"><span>```</span></p><span><br></span><p dir="ltr"><span>➤ Watch what the agent is doing and stop and correct it immediately if it starts doing something you don’t like.</span></p><h2 dir="ltr"><span>Make the agent write good tests</span></h2><p dir="ltr"><span>A lot of developers online talk about using agents to write boilerplate and tests.&nbsp; This is a very seductive proposition; it’s the scut work of development.&nbsp; So it’s really tempting (guilty) to tell the agent “write unit tests for module foo”.</span></p><span><br></span><p dir="ltr"><span>First, agents can think of really good test cases and really useless ones; go back and prune the test cases and tell it to add new ones for scenarios you really care about but that it didn’t think of.</span></p><span><br></span><p dir="ltr"><span>But along the way I discovered my #1 concern about using agents to build software: agent-created tests almost NEVER catch bugs. I don’t know exactly why this is, but my intuition is that the main cause for this is likely the way agents create software (including tests): they iterate until the test passes.&nbsp; If the original function isn’t working as expected, I suspect that the agent-created test will test the functionality as it exists, not as was intended, regardless of what it calls the test case.&nbsp; A side issue here are that agents are lazy and will disable or suppress test cases if you don’t keep reminding them not to do that (discussed earlier)</span></p><span><br></span><p dir="ltr"><span>Maybe I just am not experienced enough writing tests, and so I am not a good judge of whether a test implementation is good.&nbsp; Or maybe I’m not paying enough attention to the test definitions.</span></p><span><br></span><p dir="ltr"><span>My learning here is that whenever you find a bug and it wasn’t discovered by a test, specifically ask the agent why not, and make it debug the test.</span></p><span><br></span><p dir="ltr"><span>➤ Review agent-created test cases, add ones for scenarios important to you, and remove useless ones.&nbsp; Whenever there's a bug that a test did not discover, ask the agent why the test didn’t discover the bug, and make it fix the test.&nbsp;</span></p><h2 dir="ltr"><span>Make the agent write a lot of custom tools</span></h2><p dir="ltr"><span><span face="Arial, sans-serif">Agents are designed to invoke “tools”.&nbsp; A typical coding agent will have tools like read a file, write a file, edit or multi-edit a file, run a command in a bash terminal, search the web, etc.&nbsp; The ability to run an executable in a terminal means that you’re practically unlimited in the tools you can build.&nbsp; And you can use the agent’s context file to tell the agent about the tool.</span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">Whenever I see the agent start to struggle with the mechanics of executing a task, I look for an opportunity to write a tool.&nbsp; I was having problems with the agent making changes to a very large json file; the file was too large for context so the file read tool would fail, then it would try different strategies to grep to fine the line(s) it was interested in, then it would try to dump a few lines at a time, then it would realize it read the wrong section and have to do it again.&nbsp; And editing large complex files like that is even worse- you have all the location and file read problems, but then you have to surgically alter it while leaving it syntactically correct.&nbsp; I had the agent write a Python tool that targeted an element in the json file and replaced it with another element.&nbsp; I added a Make target for the tool, told the agent about the tool in its context file, and stopped struggling with that problem.</span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif"><br></span><span face="Arial, sans-serif">Likewise I’ve had the agent build all sorts of script-like tools to validate complex files, and to do specialized editing and diffing (are all the keys in my en-US localization file, also present in each other locale?).</span></span></p><span><br></span><p dir="ltr"><span>One note: maybe do a web search before you have the agent create a tool; I recently accidentally recreated an API testing tool that already existed.</span></p><span><br></span><p dir="ltr"><span>➤ If the agent struggles with a task, help it build a tool to do the task, and teach it to use your tool to perform the task.&nbsp;</span></p><h2 dir="ltr"><span>Other stuff I’ve learned</span></h2><p dir="ltr"><span><span face="Arial, sans-serif">Agents are sycophantic.</span><span face="Arial, sans-serif">&nbsp; I had a whole section on this but took it out; it’s so common it’s a meme now.&nbsp; My hot take: </span><span face="Arial, sans-serif">it’s engagement farming for LLMs</span><span face="Arial, sans-serif">- gives you that hit of dopamine to keep you coming back.</span></span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">Agents like to repeat code</span><span face="Arial, sans-serif"> and will happily repeat 20 lines of code 100 times if you let them.&nbsp; If you see this, give them really precise instructions on refactoring (e.g. centralizing mocks instead of repeating the logger mock in every test file).</span></span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">Never ask the agent for its opinion on your design.</span><span face="Arial, sans-serif">&nbsp; True story- at the end of a long session, I asked Claude how my design was.&nbsp; It was effusive in its praise, pointed out all the things we had done for modularity, separation of concerns, etc.&nbsp; Then I exited Claude and restarted, and asked the exact same question, only this time it had no context.&nbsp; It told me that the design needed a lot of work, complained about the stuff it had just praised, etc.</span></span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">You can ask the agent for advice on ways to improve your application, but be </span><span face="Arial, sans-serif">really</span><span face="Arial, sans-serif"> careful; it </span><span face="Arial, sans-serif">loves</span><span face="Arial, sans-serif"> to “improve” things, and is quick to suggest adding abstraction layers, etc. </span><span face="Arial, sans-serif">&nbsp;Every single idea it gives you will seem valid, and most of them will seem like things that you should really consider doing.&nbsp; RESIST THE URGE, unless there’s something that really jumps out at you as fixing a problem that you’re already encountering.&nbsp; In general, it’s better to ask very specific questions like “why are there calls to the low-level function in this function but also in this other abstract function on a higher layer?”</span></span></p><h2 dir="ltr"><span>Stuff I tried that didn’t work well</span></h2><p dir="ltr"><span>Code → Spec → Translated Code</span></p><p dir="ltr"><span>I experimented a bit with getting the agent to write as-built specifications and then have the agent (or another agent) use the specifications to write the app in another language.&nbsp; I had it write a Python application, then write a spec for it, then re-write the application in Go.&nbsp; The Python application worked great.&nbsp; The Go app was a mess but I got it working, it was nowhere near turnkey.</span></p><span><br></span><p dir="ltr"><span>The big takeaway is that the agent really sucks at writing specifications, at least with the level of effort I was willing to put in.&nbsp; I believe if the specification had been better, the translated application would have been better.</span></p><span><br></span><p dir="ltr"><span><span face="Arial, sans-serif">My particular test workflow wasn’t the best because, as mentioned above, all the agents are really good at Python.&nbsp; Sonnet is not nearly as good at Go as it is at Python.&nbsp; So I spent a lot of effort coaxing the agent through debugging the Go app.&nbsp; But I tried it in Grok 3 and o3 and had similar experiences.</span></span></p><h2><span><span face="Arial, sans-serif">That's it!</span></span></h2><span><span face="Arial, sans-serif">I hope that these tips help you become more effective at creating software with AI agents!</span></span></span>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Computer fraud laws used to prosecute leaking air crash footage to CNN (215 pts)]]></title>
            <link>https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/</link>
            <guid>44991542</guid>
            <pubDate>Sat, 23 Aug 2025 00:04:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/">https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/</a>, See on <a href="https://news.ycombinator.com/item?id=44991542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-514173">

				


				


				<h3>from the <i>if-it-can-be-abused,-it-WILL-be-abused</i> dept</h3>
				


				<p>Earlier this year, an Army helicopter <a href="https://en.wikipedia.org/wiki/2025_Potomac_River_mid-air_collision" data-type="link" data-id="https://en.wikipedia.org/wiki/2025_Potomac_River_mid-air_collision">collided with a passenger plane</a> over the Potomac River in Washington, DC. All sixty-seven people aboard both vehicles were killed. While the FAA focused its investigation on the failures that led to this mid-air collision, local investigators in Virginia were somehow far more concerned about identifying who had <a href="https://www.youtube.com/watch?v=JTgUrfQsOnA" data-type="link" data-id="https://www.youtube.com/watch?v=JTgUrfQsOnA">leaked footage of the collision to CNN</a>. </p>
<p>The subject matter of the leaked recordings was obviously of public interest. And while the government may have its own interest in controlling dissemination of recording of incidents that involve federal agencies and their oversight, it’s not the sort of government interest most courts consider to be worthy of violating the First Amendment.</p>
<p>Fortunately, the government has options. For a very long time, the option federal law enforcement deployed most frequently in cases involving pretty much any sort of technology was the <a href="https://www.techdirt.com/tag/cfaa/" data-type="link" data-id="https://www.techdirt.com/tag/cfaa/">Computer Fraud and Abuse Act</a> (CFAA). This <a href="https://www.techdirt.com/tag/shoot-the-messenger/" data-type="link" data-id="https://www.techdirt.com/tag/shoot-the-messenger/">broadly written law</a> not only allowed prosecutors to charge people with federal crimes for doing nothing more than interacting with services/servers/etc. in unexpected ways, but allowed companies to, essentially, shoot the messengers for reporting data breaches, unsecured servers, or sloppy user interfaces that could be exploited to display far more information than those running them intended.</p>
<p>The CFAA has since <a href="https://www.techdirt.com/2022/05/23/doj-changes-cfaa-policy-will-no-longer-bring-criminal-charges-against-security-researchers/" data-type="link" data-id="https://www.techdirt.com/2022/05/23/doj-changes-cfaa-policy-will-no-longer-bring-criminal-charges-against-security-researchers/">been neutered a bit</a>, slowing its abusive role in federal prosecutions. Unfortunately, there are plenty of sloppily written state laws that can accomplish what the CFAA no longer can, <a href="https://theintercept.com/2025/08/13/dc-plane-crash-washington-cnn-leak/" data-type="link" data-id="https://theintercept.com/2025/08/13/dc-plane-crash-washington-cnn-leak/">as Nikita Mazurov and Shawn Musgrave report for The Intercept</a>.</p>
<p>Here’s what Metropolitan Washington Airports Authority investigator Patrick Silsbee wrote in his report:</p>
<blockquote>
<p><em>“The video shows camera angles and views that can only be found on the Metropolitan Washington Airport’s Authority CCTV video,” Silsbee wrote in a January 31 report, noting the location of landmarks in the videos, including a boathouse near the airfield.</em></p>
<p><em>The locations of the MWAA security cameras are redacted in the reports provided to The Intercept, ostensibly “to prevent the disclosure of law enforcement and security techniques and procedures not generally known outside the law enforcement community,” according to an accompanying letter from MWAA.</em></p>
</blockquote>
<p>That doesn’t mean much by itself, but Silsbee apparently figured out (thanks in part to CNN’s initial failure to redact some CCTV text that described the location of the camera) this footage must have been obtained by an MWAA employee working at the police dispatch center. </p>
<p>CCTV footage from <em>inside</em> the dispatch center was obtained, which allegedly showed these actions being taken by the suspected leaker:</p>
<blockquote>
<p><em>“Between the hours of 2256 and 0545, Mr. Mbengue can be seen on multiple occasions utilize [sic] his personal cell phone to record video and photograph these critical scenes,” Silsbee wrote.</em></p>
</blockquote>
<p>That would be MWAA dispatch employee Mohamed Mbengue, who has since pleaded “no contest” to charges stemming from Virginia’s ultra-vague <a href="https://law.lis.virginia.gov/vacode/18.2-152.4/" data-type="link" data-id="https://law.lis.virginia.gov/vacode/18.2-152.4/">“computer trespass” law</a>. But it really takes a person with an overriding desire to shoot messengers to call cell phone recordings of screen images a “trespass.” </p>
<p>The word is generally understood to describe unauthorized access to an area a person is not allowed to be in. Mbengue was at work and had full access to these recordings as a part of his job. That he recorded them and sent them to CNN doesn’t align with any rational definition of the word “trespass.” The dissemination of footage may be a violation of policy, but policy violations aren’t criminal charges — the sort of thing that can do permanent damage to a person’s life in ways that write-ups and even justified terminations simply can’t.</p>
<p>That’s why discretion is key. But when discretion matters most, law enforcement tends to deliberately “err” on the side of whatever does the most damage to anyone it happens to be investigating. And it appears MWAA investigators are more than happy to throw criminal charges at people for, at most, violating agency policies. A second dispatcher (Jonathan Savoy) was caught doing the same thing (albeit without sharing the recordings with CNN) and faced similar charges until someone actually exercised a bit of discretion and declined to move forward with the case.</p>
<blockquote>
<p><em>On February 3, the MWAA&nbsp;<a href="https://x.com/allisonpapson/status/1886553371257266540?s=46&amp;t=p5q6YKIPojSzB8gCMgZMiA" target="_blank" rel="noreferrer noopener">announced</a>&nbsp;both men’s arrests, writing in a press statement that Savoy had been arrested “following further police investigation.”</em></p>
<p><em>In May, however, local prosecutors&nbsp;<a href="https://theintercept.com/2025/05/29/charges-dropped-leaked-dc-plane-crash-video/">quietly dropped</a>&nbsp;the charges against Savoy, through a filing called a “nolle prosequi,” according to the court docket.</em></p>
</blockquote>
<p>There’s absolutely nothing in the statute that actually covers the actions described here, which formed the basis for the bullshit criminal charges. It takes a ton of punitive imagination to turn “recording a CCTV monitor with a phone” into a criminal act. The only clause that could be even possibly be considered applicable requires investigators and prosecutors to engage in lot of extremely creative re-interpretations of <a href="https://law.lis.virginia.gov/vacode/18.2-152.4/" data-type="link" data-id="https://law.lis.virginia.gov/vacode/18.2-152.4/">the plain text of the law</a>: </p>
<blockquote>
<p><em>Use a computer or computer network to make or cause to be made an unauthorized copy, in any form, including, but not limited to, any printed or electronic form of computer data, computer programs or computer software residing in, communicated by, or produced by a computer or computer network</em></p>
</blockquote>
<p>A smartphone is a computer. A recording could be considered an “unauthorized copy.” To call the CCTV cameras and screens “computers/computer network” means ignoring the generally understood utility of this tech. Even if a network connects the cameras and a computer provides access to recordings, recording playback via phone while accessing footage the suspects <em>had every right to access</em>, calling this a violation of the law demonstrates investigators were out for revenge, rather than serving the commonly understood definition of the word “justice.”</p>

				
<p>

	Filed Under: <a href="https://www.techdirt.com/tag/cfaa/" rel="tag">cfaa</a>, <a href="https://www.techdirt.com/tag/computer-trespass/" rel="tag">computer trespass</a>, <a href="https://www.techdirt.com/tag/leak-investigation/" rel="tag">leak investigation</a>, <a href="https://www.techdirt.com/tag/leaks/" rel="tag">leaks</a>, <a href="https://www.techdirt.com/tag/mwaa/" rel="tag">mwaa</a>, <a href="https://www.techdirt.com/tag/shoot-the-messenger/" rel="tag">shoot the messenger</a>, <a href="https://www.techdirt.com/tag/virginia/" rel="tag">virginia</a>
	<br>

	
</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Popular Japanese smartphone games have introduced external payment systems (139 pts)]]></title>
            <link>https://english.kyodonews.net/articles/-/59689</link>
            <guid>44991384</guid>
            <pubDate>Fri, 22 Aug 2025 23:50:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://english.kyodonews.net/articles/-/59689">https://english.kyodonews.net/articles/-/59689</a>, See on <a href="https://news.ycombinator.com/item?id=44991384">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>TOKYO - Nearly 70 percent of popular Japanese smartphone games have introduced external payment systems for items and services to avoid hefty commission fees from U.S. tech giants Google LLC and Apple Inc., a Kyodo News tally showed.</p>

<p>The move comes ahead of a new Japanese law tightening regulations on Google and Apple, which dominate smartphone platforms, set to take full effect in December. The legislation requires the two companies to open their payment systems.</p>

<p>Almost all users currently download games through Apple and Google's app stores. When players buy in-game items, software providers pay the tech giants commissions of up to 30 percent.</p>

<p>A Kyodo News survey found that among the top 30 best-selling game titles in 2024, at least 11 of the 16 offered by domestic companies have introduced payments through external websites.</p>

<p>Although the two tech giants say the fees are necessary to protect user privacy and security, the costs have weighed on game makers.</p>

<p>For outside transactions, users make payments through channels other than apps, such as game websites. Settlement service providers like Digital Garage Inc. and GMO Tech Inc. typically charge a 5 percent commission, far below Apple's and Google's rates.</p>

<p>Payments through external websites in the in-app purchase market, estimated at over 1 trillion yen ($6.8 billion), are expected to bring user discounts and boost providers' profitability, analysts said.</p>

<p>In the survey, Kyodo News received responses from eight of 12 domestic game makers, while two declined to comment and two did not respond. Of the 12 titles from the eight firms, 11 have adopted external settlements.</p>

<p>In August last year, Mixi Inc. introduced an outside settlement system for its blockbuster game "Monster Strike," allowing its users to purchase about 5 percent more items compared with in-app payments.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mail Carriers Pause US Deliveries as Tariff Shift Sows Confusion (155 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end</link>
            <guid>44991039</guid>
            <pubDate>Fri, 22 Aug 2025 23:09:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end">https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end</a>, See on <a href="https://news.ycombinator.com/item?id=44991039">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Bluesky Goes Dark in Mississippi over Age Verification Law (216 pts)]]></title>
            <link>https://www.wired.com/story/bluesky-goes-dark-in-mississippi-age-verification/</link>
            <guid>44990886</guid>
            <pubDate>Fri, 22 Aug 2025 22:51:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/bluesky-goes-dark-in-mississippi-age-verification/">https://www.wired.com/story/bluesky-goes-dark-in-mississippi-age-verification/</a>, See on <a href="https://news.ycombinator.com/item?id=44990886">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>People in Mississippi</span> can no longer use the social media platform <a href="https://www.wired.com/tag/bluesky/">Bluesky</a>. The company announced Friday that it will be blocking all IP addresses within Mississippi for the foreseeable future in response to a recent <a href="https://www.wired.com/tag/us-supreme-court/">US Supreme Court</a> decision that allows the state to enforce strict age verification for social media platforms.</p><p>According to Bluesky, Mississippi’s approach to verification “would fundamentally change” how users access the site. “We think this law creates challenges that go beyond its child safety goals, and creates significant barriers that limit free speech and disproportionately harm smaller platforms and emerging technologies,” the Bluesky team said in <a data-offer-url="https://bsky.social/about/blog/08-22-2025-mississippi-hb1126" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://bsky.social/about/blog/08-22-2025-mississippi-hb1126&quot;}" href="https://bsky.social/about/blog/08-22-2025-mississippi-hb1126" rel="nofollow noopener" target="_blank">its statement</a>.</p><p>Bluesky did not respond to a request for comment.</p><p>The company says that compliance with Mississippi’s law—which would require identifying and tracking all users under 18, in addition to asking every user for sensitive personal information to verify their age—is not possible with the team’s current resources and infrastructure. By not complying with the law, Bluesky could face fines of up to $10,000 per violation. It is the first major social media platform to take such drastic steps in response to the law.</p><p>Age verification laws, which on the surface are intended to protect children from harmful content online, have already begun to broadly impact internet use in places around the world where they've been enacted. <a href="https://www.wired.com/story/the-age-checked-internet-has-arrived/">In the UK</a>, users trying to access everything from pornography to social platforms must now submit to ID scans, credit card checks, age-estimation scans, and more to verify they’re over the age of 18. The state of Texas has a <a href="https://www.wired.com/story/us-supreme-court-porn-age-verification-decision-2025/">similar law</a> the US Supreme Court upheld in June, despite concerns from critics over the erosion of free speech and access to information on the open internet.</p><p>Whether these laws are effective at protecting children is unclear; the use of virtual private networks (VPNs) in the UK <a href="https://www.wired.com/story/vpn-use-spike-age-verification-laws-uk/">spiked</a> just after its age verification law went into effect as users deployed the tech to spoof their location. On platforms like Discord, people discovered they could use <a href="https://www.wired.com/story/age-verification-is-sweeping-gaming-is-it-ready-for-the-age-of-ai-fakes/">video game characters</a> to trick face scans. Furthermore, <a href="https://www.wired.com/story/the-age-checked-internet-has-arrived/">critics say</a> that age verification laws intended to reduce harm to children can sometimes have the opposite effect by putting kids in greater danger of identity theft and privacy violations.</p><p>WIRED has reached out to the sponsors of the original bill, Mississippi state representatives Jill Ford, Fabian Nelson, and Larry Byrd, and will update this story if they comment.</p><p>“We believe effective child safety policies should be carefully tailored to address real harms, without creating huge obstacles for smaller providers and resulting in negative consequences for free expression,” Bluesky wrote.</p></div></div>]]></description>
        </item>
    </channel>
</rss>