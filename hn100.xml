<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 17 Sep 2023 14:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Bounties Damage Open Source Projects (102 pts)]]></title>
            <link>https://ziglang.org/news/bounties-damage-open-source-projects/</link>
            <guid>37541994</guid>
            <pubDate>Sun, 17 Sep 2023 06:06:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ziglang.org/news/bounties-damage-open-source-projects/">https://ziglang.org/news/bounties-damage-open-source-projects/</a>, See on <a href="https://news.ycombinator.com/item?id=37541994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main"><p>Authors: Andrew Kelley and Loris Cro</p><p>Please don’t use bounties to incentivize Zig development.</p><p>This blog post is inspired by this GitHub issue from three days ago: <a href="https://github.com/ziglang/zig/issues/17115" target="_blank" rel="noopener">Support WASIX</a> (<a href="https://twitter.com/wasmerio/status/1701168305103331701?s=20" target="_blank" rel="noopener">see also</a>)</p><p>Here are some reasons why we believe bounties are a poor form of sponsorship when it comes to software development:</p><ul><li>Bounties foster competition at the expense of cooperation.</li><li>Bounties are an utterly simplistic way of dealing with the business management side of creating software:<ul><li>Instead of scouting for a suitable candidate, you’re letting battle royale dynamics pick a winner for you, at the expense of everybody who’s going to lose the competition.</li><li>Instead of creating a clear contract where you take on some of the risk, you implicitly put the entirety of the risk on the contestants (eg partial solutions don’t get any payout).</li><li>Instead of allocating time and resources to proper due diligence, you instead penalize any form of thoughtfulness in favor of reckless action (eg a solution just needs to pass a test suite).</li><li>Instead of planning for the full lifecycle of software, which also includes maintenance, you end up with a quickly bitrotting artifact that is of no practical use to anybody.</li><li>Instead of spreading unease to all the people involved, it would be preferable you instead learned how to do business properly.</li></ul></li><li>On projects less radical than Zig, you might also put pressure on the development team to accept the winning submission, which, given the above, will probably not be the most well-thought-out and maintainable solution.</li></ul><p>While we do understand that continued exposure to venture capital might have caused some to build an appreciation for endless rat racing, we <em>kindly</em> request that you keep unwarranted hustling away from the Zig community.</p><p>We’re not only building free and open source software but, most importantly, we’re building <a href="https://kristoff.it/blog/the-open-source-game/" target="_blank" rel="noopener">software you can love</a> and this can only be done by employing well-designed business management processes.</p><h2 id="a-word-about-bug-bounties">A Word About Bug Bounties</h2><p>We do believe that bug bounties, like the one
<a href="https://github.com/tigerbeetle/viewstamped-replication-made-famous" target="_blank" rel="noopener">offered by TigerBeetle</a>,
can be a virtuous way of sponsoring development since they focus on the
<em>discovery</em> aspect of software defects. Yes, it’s possible for different people
to independently discover the same bug, but that’s an unlucky coincidence, not
a structural aspect of the resulting system.</p><hr><p>If you’re interested in investing in the Zig ecosystem, we’re happy to chat with you and give you all the necessary insight both when it comes to the project’s roadmap, and also in terms of publicizing the opportunity within the community to help you find a suitable candidate, but we do expect that you treat your collaborators with the respect they deserve.</p><p>For more info, contact Loris Cro (VP of Community) at <a href="mailto:loris@ziglang.org">loris@ziglang.org</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Americas richest 10% is responsible for 40% of its planet heating pollution (156 pts)]]></title>
            <link>https://journals.plos.org/climate/article?id=10.1371/journal.pclm.0000190</link>
            <guid>37541243</guid>
            <pubDate>Sun, 17 Sep 2023 03:31:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journals.plos.org/climate/article?id=10.1371/journal.pclm.0000190">https://journals.plos.org/climate/article?id=10.1371/journal.pclm.0000190</a>, See on <a href="https://news.ycombinator.com/item?id=37541243">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">

<header>





<ul id="almSignposts">
  <li id="loadingMetrics">
    <p>Loading metrics</p>
  </li>
</ul>







    <div>
  <p id="licenseShort">Open Access</p>
  <p id="peerReviewed">Peer-reviewed</p>

<p id="artType">Research Article</p>


</div>
    <div>



<div>
  

<ul data-js-tooltip="tooltip_container" id="author-list">



<li data-js-tooltip="tooltip_trigger">
       
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="1">
Craig Nicolson,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="2">
Michael Ash,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="3">
Ezra M. Markowitz,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="4">
Daniel Moran</a>    
</li>

</ul>


</div>


<div id="floatTitleTop" data-js-floater="title_author" role="presentation">
    <div>
      <h2><!--?xml version="1.0" encoding="UTF-8"?-->Income-based U.S. household carbon footprints (1990–2019) offer new insights on emissions inequality and climate finance</h2>

<ul id="floatAuthorList" data-js-floater="floated_authors">

  <li data-float-index="1">Jared Starr,&nbsp;

  </li>
  <li data-float-index="2">Craig Nicolson,&nbsp;

  </li>
  <li data-float-index="3">Michael Ash,&nbsp;

  </li>
  <li data-float-index="4">Ezra M. Markowitz,&nbsp;

  </li>
  <li data-float-index="5">Daniel Moran

  </li>

</ul>



    </div>
    <div id="titleTopCloser">
      <p><img src="https://journals.plos.org/resource/img/logo-plos.png" alt="PLOS"></p><p>x</p>
    </div>
  </div>

      <ul>
        <li id="artPubDate">Published: August 17, 2023</li>
        <li id="artDoi">
<a href="https://doi.org/10.1371/journal.pclm.0000190">https://doi.org/10.1371/journal.pclm.0000190</a>
        </li>
        <li></li>
      </ul>

    </div>
  
</header>
  <div>






<div id="figure-carousel-section">
  <h2>Figures</h2>

  
</div>





        <div id="artText">
          



<div xmlns:plos="http://plos.org"><h2>Abstract</h2><div><p>Current policies to reduce greenhouse gas (GHG) emissions and increase adaptation and mitigation funding are insufficient to limit global temperature rise to 1.5°C. It is clear that further action is needed to avoid the worst impacts of climate change and achieve a just climate future. Here, we offer a new perspective on emissions responsibility and climate finance by conducting an environmentally extended input output analysis that links 30 years (1990–2019) of United States (U.S.) household-level income data to the emissions generated in creating that income. To do this we draw on over 2.8 billion inter-sectoral transfers from the Eora MRIO database to calculate both supplier- and producer-based GHG emissions intensities and connect these with detailed income and demographic data for over 5 million U.S. individuals in the IPUMS Current Population Survey. We find significant and growing emissions inequality that cuts across economic and racial lines. In 2019, fully 40% of total U.S. emissions were associated with income flows to the highest earning 10% of households. Among the highest earning 1% of households (whose income is linked to 15–17% of national emissions) investment holdings account for 38–43% of their emissions. Even when allowing for a considerable range of investment strategies, passive income accruing to this group is a major factor shaping the U.S. emissions distribution. Results suggest an alternative income or shareholder-based carbon tax, focused on investments, may have equity advantages over traditional consumer-facing cap-and-trade or carbon tax options and be a useful policy tool to encourage decarbonization while raising revenue for climate finance.</p>
</div></div>


<div xmlns:plos="http://plos.org"><p><strong>Citation: </strong>Starr J, Nicolson C, Ash M, Markowitz EM, Moran D (2023) Income-based U.S. household carbon footprints (1990–2019) offer new insights on emissions inequality and climate finance. PLOS Clim 2(8):
           e0000190.
        
        https://doi.org/10.1371/journal.pclm.0000190</p><p><strong>Editor: </strong>Marco Grasso, University of Milan-Bicocca Faculty of Sociology: Universita degli Studi di Milano-Bicocca Dipartimento di Sociologia e Ricerca Sociale, ITALY</p><p><strong>Received: </strong>February 17, 2023; <strong>Accepted: </strong>July 13, 2023; <strong>Published: </strong> August 17, 2023</p><p><strong>Copyright: </strong> © 2023 Starr et al. This is an open access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Data Availability: </strong>All data reported in the study are included in the published article and <a href="#sec018">Supporting Information</a> flies.</p><p><strong>Funding: </strong>The authors received no specific funding for this work.</p><p><strong>Competing interests: </strong> The authors have declared that no competing interests exist.</p></div>





<div xmlns:plos="http://plos.org" id="section1"><h2>Introduction</h2><p>Anthropogenic climate change is an existential threat to all of humanity [<a href="#pclm.0000190.ref001">1</a>, <a href="#pclm.0000190.ref002">2</a>]. Yet, extreme economic inequality, across and within societies, results in a powerful disconnect between those facing the worst climate impacts and those reaping the economic and consumption benefits that drive greenhouse gas (GHG) emissions [<a href="#pclm.0000190.ref003">3</a>–<a href="#pclm.0000190.ref008">8</a>]. This disparity in harm and benefits has been a central tension at international climate negotiations, particularly when trying to allocate responsibility and financial compensation between developed and developing countries.</p>
<p>In recognition of such disparities, wealthy nations at the 2009 United Nations Climate Change Conference (UNCCC–COP 15) agreed to mobilize $100 billion a year, by 2020, to fund mitigation and adaptation efforts in poorer developing nations. The creation of a “loss and damage” fund at the recent UNCCC COP 27 marks an additional commitment to address disparities between those disproportionately driving emissions and those disproportionately experiencing the harms they cause. These efforts represent progress, yet there is also some reason for skepticism. Existing climate commitments will not keep global temperature rise within 1.5°C [<a href="#pclm.0000190.ref009">9</a>], finance pledges fall about 5-10x short of the need [<a href="#pclm.0000190.ref010">10</a>], and nations have consistently failed to meet these insufficient emissions and finance pledges [<a href="#pclm.0000190.ref009">9</a>, <a href="#pclm.0000190.ref011">11</a>]. This has made the current moment pivotal to address an increasingly urgent climate crisis and suggests addition perspectives may be useful in motivating such efforts.</p>

<div id="section1"><h3>Emissions responsibility frameworks and prior work</h3>
<p>While existing climate agreements are based on national-level territorial emissions, alternative consumption-based and income-based frameworks have been proposed to account for trade related emissions transfers and to better align responsibility with the flow of benefits. At the national level, consumption-based emissions have been well studied over the last several decades [<a href="#pclm.0000190.ref012">12</a>–<a href="#pclm.0000190.ref021">21</a>], while income-based emissions [<a href="#pclm.0000190.ref022">22</a>–<a href="#pclm.0000190.ref029">29</a>] have received less attention. Because consumption and income ultimately flow to households, these alternative frameworks also allow emissions responsibility to be quantified sub-nationally at the household-level.</p>
<p>The United States (U.S.) provides an interesting case for consumption- and income-based analysis due to its significant emissions, high levels of consumption, and extreme economic inequality. Since the Industrial Revolution the U.S. has cumulatively emitted more GHGs and captured more wealth (GDP) than any other country. At the same time, the U.S. has significant economic inequality, with the top 10% of income earners capturing 46% of pre-tax national income, in 2021, and the top 1% alone capturing 19% [<a href="#pclm.0000190.ref030">30</a>].</p>
<p>From a consumption-based standpoint, prior U.S. analyses have been conducted by Weber and Matthews [<a href="#pclm.0000190.ref018">18</a>], Jones and Kammen [<a href="#pclm.0000190.ref031">31</a>], Song et al. [<a href="#pclm.0000190.ref017">17</a>], Sager [<a href="#pclm.0000190.ref032">32</a>], Feng et al. [<a href="#pclm.0000190.ref016">16</a>] and others. In a recent paper we fill in a gap in these studies by explicitly addressing the undersampling and underestimation of top 1% and top 0.1% households’ emissions in prior work [<a href="#pclm.0000190.ref033">33</a>]. We find extreme and growing emissions disparities between very high-income households and the rest of U.S. society.</p>
<p>While emissions related to household <em>consumption</em> (consumer responsibility) have been well explored for the U.S. and many countries—informing climate equity debates–very little work [<a href="#pclm.0000190.ref034">34</a>, <a href="#pclm.0000190.ref035">35</a>] has been done linking households to the emissions used in generating their <em>incomes</em> (income responsibility). This misses a critical connection between climate altering GHG emissions and those households reaping a tangible benefit from these emissions—obscuring alternative policy solutions.</p>
<p>To date, the only research we are familiar with on income-based household GHG footprints are an initial U.S.-based analysis we conducted [<a href="#pclm.0000190.ref034">34</a>] and recent work by Pottier and Le Truet [<a href="#pclm.0000190.ref035">35</a>] that report wage-based footprints for households in France. Here we present results for an analysis that links GHG emissions to the full range of U.S. household incomes (wages, investments, retirement, etc.) over a 30 year period (1990–2019). In doing so, we offer a new perspective on emissions responsibility, fill in a key knowledge gap for a major GHG emitting nation, and highlight some alternative tax policies that could help close the climate finance gap [<a href="#pclm.0000190.ref010">10</a>]–including post-COP 27 loss and damage funds.</p>
</div>

<div id="section2"><h3>Research approach</h3>
<p>To link U.S. households with the GHG emissions that enable their income we calculate global GHG emissions intensities (metric tons (t) CO<sub>2</sub>e per dollar) of income using a multi-region input-output (MRIO) model (see <a href="#sec016">Materials and Methods</a>) [<a href="#pclm.0000190.ref036">36</a>, <a href="#pclm.0000190.ref037">37</a>]. We calculate emissions intensity using two distinct accounting approaches: direct <em>producer</em> emissions and <em>supplier</em> emissions. In the producer framework, each industry’s direct operational emissions (Scope 1) are allocated to households in proportion to the share of total income they receive from that industry. The supplier framework allocates emissions to households in the same proportional way, but each industry’s emissions are calculated as the sum of emissions occurring in all activities which directly and indirectly provide sales revenue to that industry in its role as a supplier. For example, in the producer framework households receiving wage or investment income from a power plant are responsible for the direct emissions it generates, while in the supplier framework households receiving wage or investment income from selling financial services or fossil fuel to that power plant are responsible for the plant’s emissions, proportional to their importance as a supplier. The producer approach allocates direct emissions from 429 U.S. industries, while the supplier approach includes the full downstream supply chain emissions of 9,812 industries across 190 countries (about 2.8 billion inter-sectoral transfers or around 96 million per year).</p>
<p>Industry-specific emissions intensities are linked with an individual’s wage income from that industry, using the nationally representative Integrated Public Use Microdata Series (IPUMS) harmonized Current Population Survey (CPS), which includes around 5.4 million individuals (~181,000 individuals annually) [<a href="#pclm.0000190.ref038">38</a>]. Emissions for unearned income, such as investment and retirement (social security, IRA, 401(k), etc.) income, are also included and based on weighted national average multipliers that model a range of diversified investment portfolios. In total, emissions associated with 12 pre-tax income categories are included and aggregated by household (~65,000 annually). The post-tax analysis includes 35 income categories that capture social and government transfers and reduces the household’s income responsibility by the amount of taxes paid (see Table E in <a href="#pclm.0000190.s001">S1 Text</a> for a comprehensive list of income variables). To compare emissions responsibility across the income distribution, households are then binned into income groups including deciles 1–9, the <em>next 9%</em> (90–99.0th percentile), <em>top 1%</em> (99.0th - 100th percentile), <em>next 0</em>.<em>9%</em> (99.0th—99.9th percentile), and <em>top 0</em>.<em>1%</em> (99.9th - 100th percentile) (see <a href="#sec016">Materials and Methods</a> for how we estimate top 1% households, which are under sampled in CPS and <a href="#pclm.0000190.s001">S1 Text</a> for additional methodological details).</p>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section2"><h2>Results</h2><p>Below we present results for both supplier and producer frameworks. For brevity, Figures use the supplier framework and all Figures except <a href="#pclm-0000190-g001">Fig 1</a> present pre-tax income footprints. Producer-based Figures generally show similar results. They are included as Supporting information files and referenced in the corresponding Figure legends below. We mainly focus on pre-tax footprints since they provide a clear picture of the raw income-based emissions distribution. Post-tax results are mostly presented to show the impact of tax policy and social transfers on this distribution (see <a href="#pclm-0000190-t001">Table 1</a> and <em>Tax effects on emissions footprints</em> in <a href="#pclm.0000190.s001">S1 Text</a>).</p>

<div id="section1"><h3>Income sources and carbon intensity</h3>
<p>Across the income distribution, household income sources and GHG intensities are heterogeneous. Social transfers make up a significant share of lower income groups’ post-tax footprint, wages dominate middle income groups’ emissions, and wage and investment income are the key drivers of very high-income groups’ emissions (<a href="#pclm-0000190-g001">Fig 1</a>). For average top 0.1% households, we find investment income drives &gt;50% of emissions.</p>
<p>The CO<sub>2</sub>e intensity of incomes also varies across the income distribution (<a href="#pclm-0000190-g002">Fig 2</a>). In the supplier framework, the CO<sub>2</sub>e intensity of wages tends to increase with income, though there is significant dispersion within groups. In the producer-based analysis, middle income households have the most CO<sub>2</sub>e intensive wages while low- and high-income households, employed in various service sectors, have less emission intensive incomes (see <a href="#pclm.0000190.s003">S2 Fig</a>). These differences result in some decoupling of national income and national emissions (NE) shares (<a href="#pclm-0000190-t001">Table 1</a>), for some income groups. For example, top 1% households have NE shares that are higher than their income share in the supplier framework and lower in the producer framework. While underlying income inequality is by far the most important factor shaping extreme inequality in emissions footprints, differences in income sources and GHG intensity (see <a href="#pclm.0000190.s004">S3</a> and <a href="#pclm.0000190.s005">S4</a> Figs) cause emissions heterogeneity at a given income level and a divergence between national income shares and emissions shares for some income groups.</p>
<div data-doi="10.1371/journal.pclm.0000190.g002"><div><p><a title="Click for larger image" href="https://journals.plos.org/climate/article/figure/image?size=medium&amp;id=10.1371/journal.pclm.0000190.g002" data-doi="10.1371/journal.pclm.0000190" data-uri="10.1371/journal.pclm.0000190.g002"><img src="https://journals.plos.org/climate/article/figure/image?size=inline&amp;id=10.1371/journal.pclm.0000190.g002" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 2. </span> Annual mean CO<sub>2</sub>e intensity of supplier-based wages and other income sources (1990–2019).</p><p>Wages and employer healthcare contributions are industry specific CO<sub>2</sub>e multipliers and are presented by income group. The other income categories use weighted national average multipliers (shaded blue area). <em>(Producer-based results are presented in <a href="#pclm.0000190.s003">S2 Fig</a>)</em>.</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.g002">
              https://doi.org/10.1371/journal.pclm.0000190.g002</a></p></div></div>

<div id="section2"><h3>Most recent year (2019)</h3>
<p>In 2019, we estimate U.S. household income-based emissions range from ~0 to over 8,000 t and follow a strong linear relationship (R<sup>2</sup> &gt; 0.94) with an elasticity of 1.0 (<a href="#pclm-0000190-g003">Fig 3</a>). We find a highly unequal emissions distribution with Gini coefficients of 0.57 (producer) and 0.58 (supplier) (for Lorenz curves, see <a href="#pclm.0000190.s006">S5</a> and <a href="#pclm.0000190.s007">S6</a> Figs). It is worth noting that at a given income level differences in the GHG intensities of income sources result in emissions variability. For example, we estimate a pre-tax income around $1 million has emissions as low as ~200 t or as high as ~1,300 t depending on the type of profession or investments that are generating that income.</p>
<div data-doi="10.1371/journal.pclm.0000190.g003"><div><p><a title="Click for larger image" href="https://journals.plos.org/climate/article/figure/image?size=medium&amp;id=10.1371/journal.pclm.0000190.g003" data-doi="10.1371/journal.pclm.0000190" data-uri="10.1371/journal.pclm.0000190.g003"><img src="https://journals.plos.org/climate/article/figure/image?size=inline&amp;id=10.1371/journal.pclm.0000190.g003" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 3. </span> Relationship between pre-tax income and household GHG footprint (log-log) using the supplier income method (2019) (n = 69,483 –includes 2,000 synthetic data points for next 0.9% and top 0.1% households).</p><p>(Producer-based results are presented in <a href="#pclm.0000190.s008">S7 Fig</a>).</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.g003">
              https://doi.org/10.1371/journal.pclm.0000190.g003</a></p></div><p>Binning households into income groups, we estimate the highest earning 30% of households are responsible for about 70% of income-based NE while the lowest earning 70% are responsible for only about 30% NE (<a href="#pclm-0000190-g004">Fig 4</a>). Depending on the framework, the highest earning top 10% of households drive 40–43% of NE. At the top of the income distribution, we estimate top 0.1% households account for 7–8% NE and have average absolute emissions &gt; 2,000 t (<strong>producer:</strong> <span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e001" loading="lazy"></span> = 2,110; <span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e002" loading="lazy"></span> = 1,870; 95% CI = 2,035 / 2,180 and <strong>supplier:</strong> <span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e003" loading="lazy"></span> = 2,670; <span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e004" loading="lazy"></span> = 2,395; 95% CI = 2,585 / 2,765).</p>
<div data-doi="10.1371/journal.pclm.0000190.g004"><div><p><a title="Click for larger image" href="https://journals.plos.org/climate/article/figure/image?size=medium&amp;id=10.1371/journal.pclm.0000190.g004" data-doi="10.1371/journal.pclm.0000190" data-uri="10.1371/journal.pclm.0000190.g004"><img src="https://journals.plos.org/climate/article/figure/image?size=inline&amp;id=10.1371/journal.pclm.0000190.g004" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 4. </span> Mean household t CO<sub>2</sub>e emissions (2019) per income group under the pre-tax supplier framework.</p><p>The width of each income group, on the x-axis, corresponds with each group’s share of national emissions. Color indicates income category. Black error bars are bootstrapped 95% confidence intervals for <em>total</em> t CO<sub>2</sub>e from all three sources. Similarly, gray error bars are bootstrapped 95% confidence intervals on the total t CO<sub>2</sub>e given an assumed ±20% error in carbon intensity per dollar. <em>(Producer-based results are presented in <a href="#pclm.0000190.s009">S8 Fig</a>)</em>.</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.g004">
              https://doi.org/10.1371/journal.pclm.0000190.g004</a></p></div></div>

<div id="section3"><h3>Super emitters</h3>
<p>We term households with emissions &gt;3,000 t CO<sub>2</sub>e per year as “super emitters”. For pre-tax income, we estimate about 43,200 U.S. households or 34% of the top 0.1% households are super emitters with the supplier framework (<span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e005" loading="lazy"></span> = 4,317 t, <span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e006" loading="lazy"></span> = 4,053). About 26,500 households, or 21% of top 0.1% households surpass this threshold with the producer framework (<span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e007" loading="lazy"></span> = 3,906 t, <span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e008" loading="lazy"></span> = 3,583). Post-tax, the percent of <em>top 0</em>.<em>1%</em> households classified as super emitters drops to about 9% (supplier) and 3% (producer).</p>
<p>Almost all super emitting households come from the top 0.1% income group. They had average incomes of over $10.6 million (supplier) and $11.5 (producer) (<a href="#pclm-0000190-t002">Table 2</a>). Because GHG intensity varies widely across sectors, a household may surpass the 3,000 t threshold with either much lower or much higher income than the average, depending on the GHG intensity of their income source. While super emitting households can also be employed in any sector of the economy (<a href="#pclm-0000190-t002">Table 2</a>), they are markedly overrepresented in finance, real estate, and insurance; manufacturing; mining and quarrying; and services (other). Meanwhile, households earning income from accommodations and restaurants; education; retail and wholesale trade; and some other fields are underrepresented among super emitting households. Generally, both producer and supplier frameworks show the same directionality in terms of divergence from U.S. average employment by sector, but there is variability in the scale of this divergence.</p>
</div>

<div id="section4"><h3>Relationship to racial inequality</h3>
<p>Black households had mean pre-tax footprints of 19 t CO<sub>2</sub>e (supplier and producer) (<span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e009" loading="lazy"></span> = 11 t in both), White Hispanic households had 26 t (supplier) and 25 t (producer) (<span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e010" loading="lazy"></span> = 16 t in both), and White non-Hispanic households had 40 t (supplier) and 36 t (producer) (<span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e011" loading="lazy"></span> = 22 t in both). The fact that White non-Hispanic household emissions were 1.4x - 2.1x higher than other groups partly reflect differences in the CO<sub>2</sub>e intensity of employment across groups. For example, in the supplier footprint White non-Hispanic households had emissions intensity of wages 1.12x higher than Black households. More critical, however, is the extreme racial inequity of the underlying income distribution. In 2019, the <em>top 1%</em> of the income distribution was 76% White non-Hispanic, 8% Hispanic, and only 3% Black. Meanwhile, Black households make up a disproportionate share of bottom decile households. Post-tax the racial emissions gap closes somewhat, but White non-Hispanic households still have emissions 1.3–1.7x higher than other groups. An additional observation is that significant emissions inequality exists within each racial group. Comparing the median to the means, shows that the emissions distribution is right skewed, with most of the population having emissions far below the mean and a relatively small percent of the population having emissions much higher than the group mean.</p>
</div>

<div id="section5"><h3>Emissions by age group</h3>
<p>In terms of age, average emissions tend to increase with age until peaking within the 45–54 years old head of household age group (<a href="#pclm-0000190-t003">Table 3</a>). After this point they tend to decline. This mirrors household incomes, which tend to increase as cohorts gain experience and seniority within the labor force, then decline as they enter early retirement and retirement age.</p>
</div>

<div id="section6"><h3>Time series: (1990–2019)</h3>
<p>Looking across 30 years of data a few noteworthy trends emerge. First, <em>emissions intensities</em> consistently differ across income groups and have fallen (45–49%) over time (<a href="#pclm-0000190-g005">Fig 5</a>). Second, these falling emission intensities have resulted in <em>decreasing national average</em> household emissions, despite rising incomes (<a href="#pclm-0000190-g006">Fig 6</a>). Yet, this declining national average belies a divergence that has occurred between the bottom 99% of the income distribution and the top 1%. While the bottom 99% have seen rising incomes, their absolute emissions have fallen (<a href="#pclm-0000190-g007">Fig 7</a>) due to declining emissions intensities. For the top 1% however, income growth has outpaced falling emission intensities and resulted in flat or rising absolute emissions (<a href="#pclm-0000190-g007">Fig 7</a>). Finally, these trends have resulted in a large and increasing <em>share of national emissions</em> generating economic benefits for high income households (<a href="#pclm-0000190-g008">Fig 8</a>).</p>
<p>Put together, these trends reveal an interesting emissions story: despite falling emissions intensities, declining national average emissions and rising incomes for all groups, unequal income growth has created significant and increasing emissions inequality between extremely high-income households and the rest of U.S. society. This has moved the income-based national emissions Gini coefficient from 0.51 (producer and supplier) in 1990 to 0.57 (producer) and 0.58 (supplier) in 2019.</p>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section3"><h2>Discussion</h2>
<div id="section1"><h3>Limitations and sources of uncertainty</h3>
<p>Our study is limited in scope, makes certain assumptions about unearned income that are important for top 1% households, and relies on survey and emissions databases that can introduce errors. First, this study focuses on linking emissions with <em>income</em>. Household <em>wealth</em> is only considered insofar as it generates realized income as capital gains or dividends. Because wealth is even more unequally distributed than income, a wealth-based emissions analysis would very likely show greater emissions inequality than our results.</p>
<p>In estimating the emissions intensity of <em>unearned income</em>, it is not feasible to estimate itemized sources of investment income per household. Instead, we assume that households have a diversified passive investment portfolio generating unearned <em>investment income</em> equal to the weighted mean GHG intensity of the U.S. economy. When creating the synthetic dataset for top 1% households, where investments are a key source of income, we allow the GHG intensity of individual household’s investment income to vary up to ±25% from the mean. This creates a distribution of households whose average equals the mean but whose individual portfolios can be overweighted to either more or less GHG intensive industries than the national average. While some households may be outside these bounds, we assume extremely overweight portfolios in either GHG intensive or non-intensive industries are somewhat rare, tend to balance out and in aggregate do not meaningfully affect the overall group mean. This study assumes that on average, investments are passively managed, though further study of how investors can actively influence the carbon intensity of their investments is an interesting topic for future work.</p>
<p>The emissions per industry for <em>wage income</em> are taken from the Eora MRIO (see <a href="#sec016">Materials and Methods</a>). In Eora, import and export data reported across countries may not exactly align and balancing used in Eora to resolve these discrepancies may lead to minor estimation error, though the affect is minimal on large economies like the U.S. In all MRIO models emissions data are taken from IPCC-style inventories which itemize emissions by activity rather than by economic sector. Reallocating from activity-based inventories to sector-based inventories introduces error that could affect the accuracy of estimated emissions per wage income sector. Additionally, converting from symmetrical and non-symmetrical Supply-Use (SUT), Industry-Industry (II), and Commodity-Commodity (CC) tables, in the original Eora, to a symmetrical II intermediate transaction matrix involves the Fixed Product Sales Structure Assumption [<a href="#pclm.0000190.ref039">39</a>] and again moves away from the original national data reports. While some error is inherent, for a large economy with robust GHG reporting the effect on the final income group GHG estimates is limited.</p>
<p>All surveys, including the IPUMS CPS we use for household income are sensitive to sampling and non-sampling error. Most important to our study, top 1% households are under sampled in CPS (See <em>Undersampling and underestimating top 1% incomes in CPS</em> and Table A in <a href="#pclm.0000190.s001">S1 Text</a>). To address this, we use income data from the World Inequality Database (WID) [<a href="#pclm.0000190.ref030">30</a>] and Congressional Budget Office (CBO) estimates on capital income shares to estimate incomes for these missing households (see <a href="#sec016">Materials and Methods</a>). Linking household incomes with Eora GHG intensities also requires the use of a concordance matrix. This reduces the number of U.S. industries from 429 to 246 and can impact an individual household’s emission estimate if their employer has a much higher or lower emissions intensity than the sector average. Yet, it seems reasonable that over- or under-estimates for individual household emissions intensity tend to balance out at the income group level.</p>
<p>While some degree of measurement error is unavoidably present in any estimate of carbon intensity, ultimately income-based footprints are the direct result of the total income dollars received and the carbon intensity of those dollars. As the U.S. is a large economy with fairly accurate income and emissions data collection, we consider error in overall estimates for these variables is likely small. Considering the limited heterogeneity of emissions intensity across income groups and between capital and wage income, any error in group-level CO<sub>2</sub>e intensity is also fairly limited. Nevertheless, to quantify the impact from a quite high level of error, we ran our model with carbon intensity ±20% from the baseline analysis. We then bootstrapped the results for each income group and extract lower 95% bounds from the -20% analysis and upper 95% bounds from the +20% error estimates (gray error bars in <a href="#pclm-0000190-g004">Fig 4</a>). In practice this yields lower and upper bounds ±21–24% from the baseline group means. While the ±20% choice is arbitrary and we believe far higher than the actual error, given that the underlying error is unknown, it was chosen as a reasonable starting point. Results show that even when a high degree of error is tested, the absolute emissions and NE share from next 0.9% and top 1% households remains quite high and distinct from the lowest earning 99% of households. (See the <a href="#pclm.0000190.s001">S1 Text</a> for additional discussion on uncertainty).</p>
</div>

<div id="section2"><h3>Income and emissions inequality</h3>
<p>Across all accounting methods, those at the very top of the income distribution are responsible for striking absolute t CO<sub>2</sub>e and disproportionate shares of national emissions. Disparities between these top income groups and the rest of society have also been growing over time. Between 1990 and 2019, Deciles 1–9 all saw declining absolute emissions and NE shares. For the bottom 5 deciles absolute emissions fell an average of 51% (producer) and 38% (supplier), while their NE shares showed average declines of 20% (producer) and 17% (supplier). For top 1% households, trends moved in the opposite direction. The next 0.9% and top 0.1% groups saw their NE shares respectively rise 46% and 82% (producer) and 43% and 83% (supplier). By 2019, the top 1% alone (<span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e012" loading="lazy"></span> = 475 t (producer), <span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e013" loading="lazy"></span> = 595 t (supplier)) were responsible for more emissions (15–17% NE) than the poorest 50% of U.S. households put together (14% NE). Average top 0.1% households (<span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e014" loading="lazy"></span> = 2,110 t (producer), <span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e015" loading="lazy"></span> = 2,670 t (supplier)) have emissions 1,650–1,700x higher than an average bottom decile household (<span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e016" loading="lazy"></span> = 1.3 t (producer), <span><img src="https://journals.plos.org/climate/article/file?type=thumbnail&amp;id=10.1371/journal.pclm.0000190.e017" loading="lazy"></span> = 1.6 t CO<sub>2</sub>e (supplier)). This divergence between top 1% households and the rest of society has been driven by rising income inequality (<a href="#pclm-0000190-g006">Fig 6</a>) and has occurred despite the falling GHG <em>intensity</em> of incomes (<a href="#pclm-0000190-g005">Fig 5</a> and <a href="#pclm.0000190.s010">S9 Fig</a>).</p>
<p>Income-based emissions responsibility closely correlates with income inequality. We find that in 2019 the poorest 50% of the U.S. population captured just 15% of pre-tax national income and was responsible for 14% of pre-tax NE in both frameworks (<a href="#pclm-0000190-t001">Table 1</a>). The top 10%, <em>top 1%</em>, and <em>top 0</em>.<em>1%</em> captured about 41%, 16%, and 7% of national income. We find they have fairly comparable NE shares of 43%, 17%, and 8% for pre-tax supplier and 40%, 15%, and 7% for pre-tax producer. While limited, the differences between income shares and emission shares are due to variations in GHG-intensity across income sources, income groups, and accounting methods. An interesting recent study by Pottier and Le Treut analyzing wage-based emissions in French households finds the emissions distribution is more unequal than the wage distribution [<a href="#pclm.0000190.ref035">35</a>]. Here they find large variability in the carbon intensity of wages is driving this difference. They also find an interesting gender gap in emissions, with men tending to earn wages from more carbon intensive activities than women.</p>
<p>While income-based emissions accounting is distinct from consumption-based accounting, it is worth noting that an income-based approach estimates greater inequality. For example, in a related consumption-based study that we conducted, we estimate the top 10%, top 1%, and top 0.1% of U.S. households are responsible for a much lower 24%, 6%, and 2.3% of national emissions [<a href="#pclm.0000190.ref033">33</a>]. The discrepancy is due to large savings rates among high-income households (which reduces consumption) and significant heterogeneity in GHG intensity across income groups, with low-income households purchasing more GHG intensive goods. In France, Pottier and Le Truet find this same trend of income-based emissions being more unequal than consumption-based emissions [<a href="#pclm.0000190.ref035">35</a>, <a href="#pclm.0000190.ref040">40</a>].</p>
<p>We find that because income-based emissions are the result of both income and the GHG intensity of that income (which varies across industries and producer and supplier accounting principles (<a href="#pclm-0000190-t002">Table 2</a>)), households at the same income level can have very different emissions levels. For example, in the producer framework a household earning $980,000 from the petroleum refining industry would be a super emitter (&gt;3,000 t), while the same amount of emissions would require $11 million in income from the hospital industry. In the supplier framework, becoming a super emitting household would take at least $18 million in hospital income, but only $2.7 million in income from the coal industry. Households may also have very different GHG intensities from different income sources. This is particularly true for high income households that have a significant share of income flowing from investments. For example, one might earn wages from a low GHG intensive sector like education but have above average GHG intensity of investment income from a portfolio that is overweight on fossil fuel companies.</p>
<p>In terms of how age relates to emissions, prior consumption-based work by Zheng et al. has shown that seniors (age 60+) in the U.S. had the highest per capita footprint (20 t) of any age group [<a href="#pclm.0000190.ref041">41</a>]. With our income-based analysis we find the highest household emissions are among the 45–54 years old peak earning group. If our analysis was wealth-based however, rather than income-based, it would likely agree with the findings from Zheng et al. that seniors have the largest emissions footprints. This is because, while incomes tend to decline for cohorts above age 54, household net worth continues rising and peaks within the 65–74 years old cohort [<a href="#pclm.0000190.ref042">42</a>]. Even the 75 and above group has a higher net worth than the 45–54 peak income group.</p>
<p>While different in methodology, an interesting study by Lucas Chancel looking at global income groups’ consumption and investment found that for the global top 1%, emissions related to investments account for a much greater share of their emissions responsibly (70%) than their consumption (30%) [<a href="#pclm.0000190.ref043">43</a>]. Finally, it is worth noting that household income is shaped by a variety of demographic factors. While we report on emissions disparity by race, ethnicity, and age, we are principally focused on quantifying the emissions distribution in relation to the income distribution as it exists and do not investigate in great detail how demographic factors influence the income distribution.</p>
</div>

<div id="section3"><h3>Policy implications</h3>
<p>Carbon pricing, either through cap-and-trade or a carbon tax, are seen by economists as an essential and cost-effective way to help decarbonize the US economy [<a href="#pclm.0000190.ref044">44</a>, <a href="#pclm.0000190.ref045">45</a>]. Prior work has suggested this tax would need to be &gt;$200 per t CO<sub>2</sub>e, to achieve even a 5% reduction in oil consumption, and estimates that 70–80% of this cost would be initially passed onto consumers [<a href="#pclm.0000190.ref045">45</a>]. Another paper we have published looking at consumption-based U.S. emissions suggests that a tax this high would present a significant burden to low-income families, even though they have comparatively small GHG footprints [<a href="#pclm.0000190.ref033">33</a>]. Meanwhile the tax may not be sufficient to shift behavior of high-income households, who have significant consumption-based emissions, but adequate savings rates to absorb the tax. While a carbon tax-funded dividend has been proposed to reduce financial strain on low income households [<a href="#pclm.0000190.ref046">46</a>], consumer-facing carbon taxes have not found sufficient political support, despite two decades of development.</p>
<p>The fact that income-based footprints are more inequitable than consumption-based footprints [<a href="#pclm.0000190.ref016">16</a>, <a href="#pclm.0000190.ref017">17</a>, <a href="#pclm.0000190.ref035">35</a>, <a href="#pclm.0000190.ref047">47</a>–<a href="#pclm.0000190.ref049">49</a>] suggests an alternative income-based approach to carbon pricing schemes, applied to wage earners or investors, could have equity and political advantages over consumer-facing carbon taxes. Such a tax could be calculated based on direct emissions (producer), on the supply of fossil fuels into the economy (supplier), or some split between the two. Tax revenue could be used for climate mitigation or adaptation projects either within the U.S. or to meet and increase international climate finance pledges including loss and damage funding agreed to at COP 27.</p>
<p>While either wage or investment income could be the focus of such a tax, a wage-based tax has some drawbacks. Just like low-income consumers, low-income wage earners would have the least ability to absorb a tax. To address this, a carbon income tax could be applied progressively, to shield low-income workers. Yet the effectiveness of such a tax to shift the economy to lower GHG emissions may be insubstantial as workers generally have limited agency in shifting their industry’s emissions behavior. While a tax may generate revenue that could be invested in decarbonization or climate finance, it may be politically unpopular to create a wage-based carbon tax that would impact a wide swath of the public.</p>
<p>Because unearned investment income and asset ownership are heavily concentrated at the top of the income distribution, limiting a carbon tax to either of these items could further focus it on those reaping the most economic benefit from GHG emissions, increase public support, and reduce GHG-intensive economic activity in a more direct way. A consideration here is that while there is some overlap of households in the top 1% or 0.1% of <em>income earners</em> and the top 1% or 0.1% of <em>wealth holders</em> there is far more annual churn among the top income group [<a href="#pclm.0000190.ref050">50</a>]. Here, households may see huge profits one year from the sale of a business or stocks, but far less income in subsequent years. In this way, an asset-based shareholder carbon tax may be more desirable than an unearned income tax because it would set a more stable annual tax rate and keep the focus on those with the most economic power. It would also be more equitable in that it accounts for the historical emissions embodied in unrealized capital gains, rather than focusing solely on present day emissions that generate unearned income. Furthermore, it concentrates behavior change incentives on the executives and large shareholders who have the most agency and power to reduce their industries’ emissions activities.</p>
<p>At the extreme end of the income distribution, an interesting 2022 study by Oxfam International on 125 global billionaires with assets in excess of $2 trillion, estimates emissions related to their investments were over 3 million t per person annually [<a href="#pclm.0000190.ref051">51</a>]. If these individuals were incentivized to reduce the GHG intensity of their industries or shift their investments to other industries, in response to a tax, it could meaningfully impact emissions. Indeed, they estimate the overall carbon intensity of billionaires’ emissions, in their study, could be reduced fourfold if investments were shifted to funds with stronger environmental and social standards [<a href="#pclm.0000190.ref051">51</a>]. Work by Lucas Chancel suggests that a progressive carbon tax tied to the carbon intensity of investments could be helpful to accelerate decarbonization, while having limited impact on most households [<a href="#pclm.0000190.ref043">43</a>]. Further work by Chancel, Bothe, and Voituriez [<a href="#pclm.0000190.ref052">52</a>] estimate a progressive global wealth tax starting at 1.5% for individuals with net worth’s &gt;$100 million (~65,000 individuals or &lt;0.001% of the global population) and going as high as 3% for individuals with assets above $100 billion could raise $300 billion annually for decarbonization, loss and damage, or other climate funding. Even if just the U.S. and European countries adopted such as tax, Chancel and colleagues estimate $175 billion could be raised annually. As they note, because wealth tends to grow 7–9% annually for extremely wealthy individuals, their overall fortunes would still increase, even in the face of these progressive climate-focused taxes [<a href="#pclm.0000190.ref052">52</a>]. Kapeller, Leitch, and Wildauer also find that a progressive European wealth tax has the potential to raise enough revenue to close the European Union’s (E.U.) several hundred billion dollar per year green investment gap [<a href="#pclm.0000190.ref053">53</a>]. If revenue from capital taxes is reinvested in public infrastructure, such as decarbonization efforts, it can also benefit wealthy countries by increasing social welfare, while reducing extreme economic inequality [<a href="#pclm.0000190.ref054">54</a>].</p>
<p>While it would increase the complexity of tax administration, basing such a tax on Scope 1 (or full supply chain Scope 1–3) emissions, rather than a tax based solely on net worth, would keep the tax close to the source of the emissions and encourage divestment from high emitting (highly-taxed) industries. In the U.S., new climate disclosure rules proposed by the Securities and Exchange Commission in 2022, which require Scope 1 and 2 reporting (and Scope 3 for companies with Scope 3 emissions targets) would provide company-specific emissions data that could be used for calculating an appropriate tax rate for investments in that company. In Europe, similar data will become available as the E.U.’s Corporate Sustainability Reporting directive, that came into effect in 2023, requires Scope 3 emissions reporting for E.U. based companies. At the asset manager level, interesting recent work by Zengkai Zhang and colleagues has highlighted the carbon emissions in firms’ portfolios in China [<a href="#pclm.0000190.ref055">55</a>] and emissions associated with investments for multinational enterprises [<a href="#pclm.0000190.ref056">56</a>].</p>
<p>Finally, while it is impractical to assume large numbers of high-income households could or would easily switch to lower GHG intensive professions, it seems reasonable that in their role as investors high-income households and their asset managers can nimbly shift to lower GHG intensive investments if the market rewards such moves. Linking the shareholder tax rate to the GHG intensity of the industry would also spur fiduciary fund managers to divest from GHG intensive industries in search of higher returns elsewhere. From an industry perspective, such a tax may also encourage firms to decarbonize their operations in order to attract investors with the promise of higher returns, via relatively lower taxes on ownership of the company’s shares. It could also encourage executives, who have seen ballooning compensation over the last several decades [<a href="#pclm.0000190.ref057">57</a>] to decarbonize their operations and supply chains to reduce taxes on the income and shares they receive. If high income households did shift their investments in response to such a tax, we would see further decoupling of the national income shares and national emission shares (<a href="#pclm-0000190-t001">Table 1</a>) among high income households.</p>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section4"><h2>Conclusion</h2><p>To avoid the worst impacts of climate change it is imperative that global temperature rise is limited to 1.5°C [<a href="#pclm.0000190.ref007">7</a>, <a href="#pclm.0000190.ref058">58</a>, <a href="#pclm.0000190.ref059">59</a>]. Yet, the window to achieve this goal is rapidly shrinking. By 2030, even if the existing Paris Agreement National Determined Contributions are achieved, global emissions are still projected to overshoot the 1.5°C pathway by at least 60% [<a href="#pclm.0000190.ref009">9</a>]. At the same time, climate finance falls well short of what is needed to mitigate and adapt to a warming world. Recent work by Andrew Fanning and Jason Hickel has shown that if nations were each allocated an equality-based fair share of emissions, by 2050 wealthy countries in the global North would owe $192 trillion (or about $6.2 trillion per year) to poorer countries in the global South to compensate for their atmospheric over-appropriations [<a href="#pclm.0000190.ref060">60</a>]. To date, climate finance flows to developing nations have struggled to reach the $100 billion a year goal set over a decade ago. It is clear that the economy needs to decarbonize faster than its current trajectory and that more money is needed to both fund this transition and equitably adapt societies to a warming world.</p>
<p>By linking GHG emissions with the incomes it enables our work has quantified the scale of emissions inequality in U.S. society and the extreme and growing concentration of emissions among very wealthy households. It also offers some suggestions on how accelerated decarbonization and revenue generation might occur, such as an income or shareholder-based carbon tax that reflects the GHG intensity of one’s income sources or financial assets. This is distinct from consumer facing carbon taxes that rely on individuals decarbonizing the economy by shifting their consumption to less GHG intensive goods and services and thereby encouraging companies to respond to their new preferences. A consumer-facing approach assumes individual consumers have the knowledge, financial resources, and agency to shift spending and the power to alter corporate decision making on the GHG intensity of their supply chain and operations. An alternative income or shareholder facing carbon tax puts pressure on executives and large shareholders (i.e. those with the most economic and corporate power) to act in their own self-interest and decarbonize their supply chain and operations in order to reduce taxes on their compensation and investments. Recent work has calculated that a climate inspired wealth tax could indeed be an effective tool to raise revenue for adaptation and mitigation efforts [<a href="#pclm.0000190.ref052">52</a>, <a href="#pclm.0000190.ref053">53</a>].</p>
<p>By thinking of carbon as an outcome of income generation rather than just an outcome of consumption, such alternative policy solutions become possible. While consumer-facing carbon taxes have struggled to move from proposal to law in the U.S. an investment-based carbon tax may be more equitable, politically palatable, and equally justifiable. Of course, any such proposals would likely face significant pushback from the economically advantaged households who dominate policymaking [<a href="#pclm.0000190.ref061">61</a>]. Finally, given the urgent nature of the climate crisis and the shrinking window in which limiting warming to 1.5°C is possible, policymakers may be wise to consider adopting multiple approaches (both consumer and income or investor facing carbon taxes) that can simultaneously put pressure on different actors to equitably decarbonize the economy and fund a just transition to a still warming world. We suggest further work quantifying the potential effects of such proposals is warranted.</p>
</div>

<div xmlns:plos="http://plos.org" id="section5"><h2>Materials and methods</h2><p>For both the producer- and supplier-income approach we link income to GHG emissions using survey data on individual-level income (see Table E in <a href="#pclm.0000190.s001">S1 Text</a> for income categories) and an Environmentally-Extended Multi-Region Input-Output Model (EE-MRIO). For earned income, the GHG intensity per dollar of wages, for each industry, is calculated and multiplied by an individual’s income from that industry. The GHG intensities of unearned investment income, retirement, and employer contributions to healthcare are also included (12 categories in total) in the pre-tax analysis. In post-tax footprints, emissions responsibilities are increased by 23 categories of social transfers and reduced by the amount of taxes paid. Individuals are aggregated into households and households are ranked into percentiles and deciles, for income group comparisons.</p>
<p>To calculate the CO<sub>2</sub>e intensity of income, we use the Eora MRIO database [<a href="#pclm.0000190.ref036">36</a>, <a href="#pclm.0000190.ref037">37</a>] covering 14,839 sectors, 190 countries, and 1,140 final demand and value-added categories. For each of the 30 years, EORA is converted from a 14,839 x 14,839 heterogeneous classification system to a square 9,812 x 9,812 industry by industry input-output table, using the Fixed Product Sales Structure assumption [<a href="#pclm.0000190.ref039">39</a>]. Current year dollars are adjusted to constant 2020 US$. GHG emissions data come from the PRIMAPHIST database (Version 2.3—available in Eora) [<a href="#pclm.0000190.ref062">62</a>, <a href="#pclm.0000190.ref063">63</a>]. This includes the six Kyoto GHGs and excludes land use, land use change, and forestry (LULUCF).</p>
<p>In a producer-income approach the CO<sub>2</sub>e intensity of each industry’s direct emissions are calculated by proportionally allocating emissions to each value added category and calculating the GHG intensity per dollar of various income types. In the supplier income emissions framework, we calculate the enabled emissions, in t CO<sub>2</sub>e per dollar, using the Ghosh inverse (see <a href="#pclm.0000190.s001">S1 Text</a>). This captures all <em>direct</em> and <em>indirect</em> CO<sub>2</sub>e emissions, along the whole downstream global supply chain (~96 million inter-sectoral transfers each year) that were enabled to produce a dollar of value added.</p>
<p>For each year, these supply chain and direct emissions factors are matched with individual-level IPUMS CPS income data. IPUMS CPS is a harmonized dataset drawn from the Census Bureau’s Current Population Survey [<a href="#pclm.0000190.ref038">38</a>]. It includes approximately 65,000 U.S. households and about 181,000 individuals per year. From CPS, we extract 58 variables related to income, healthcare, social benefits, industry from which wages are earned, and individual or household characteristics. Each year yields approximately 10,500,000 data points, totaling about 315,000,000 data points across the 30-year period. Individual-level emissions and income matching is done by applying a concordance matrix to convert emissions factors from the 429 U.S. industries in Eora to the 246 U.S. industries reported by CPS, using International Standard Industrial Classification (ISIC) system coding (For more details see <em>Converting Eora industries into CPS industries</em> in <a href="#pclm.0000190.s001">S1 Text</a> and <a href="#pclm.0000190.s024">S12 Data</a>). Individual-level wage data in CPS includes both the amount (in dollars) and the industry from which income is earned (IND90LY). Individual-level wage data are then multiplied by the corresponding CO<sub>2</sub>e intensity for that industry. The income value of employer healthcare contributions is added, based on the employing industry’s CO<sub>2</sub>e multiplier. This is included for all years except 1990–1992 and 2019, where data is not reported in CPS.</p>
<p>The GHG responsibility of social security, retirement, capital gains, interest and dividends, and social benefits are also accounted for using weighted average income-based emissions intensities of the U.S. economy. These national average intensities are calculated to reflect each industry’s share of emissions from the relevant value added category/ies (see <em>Converting Eora industries into CPS industries</em> and <em>Calculating CO</em><sub><em>2</em></sub><em>e intensities of income sources from value added</em> in <a href="#pclm.0000190.s001">S1 Text</a>). For example, because Social Security comes out of employee compensation, an industry specific weight is calculated based on each industry’s share of total employee compensation. Each industry’s weight is then multiplied by its CO<sub>2</sub>e intensity. All the weighted CO<sub>2</sub>e intensities are then summed into the weighted national average CO<sub>2</sub>e intensity per dollar Social Security. Interests and dividends multipliers are calculated the same way but based on each industry’s share of Net Operating Surplus. Social benefits are also calculated in this way but based on each industry’s share out of all value added categories. In practice, the difference between these three different national average CO<sub>2</sub>e intensities is generally quite small (a few percentage points).</p>
<p>For each individual in CPS, income dollars from wages, investments, retirement, and employer healthcare contributions are multiplied by the corresponding CO<sub>2</sub>e intensity of that income source. Individuals are then merged into their respective households and t CO<sub>2</sub>e are summed. This yields the pre-tax emissions footprint of each household. Households are then binned into income groups.</p>
<p>To calculate the post-tax and transfer footprint, the value of social transfers such as monetary gifts and publicly provided benefits such as veterans benefits, unemployment, home heating, rental, and educational assistance are multiplied by the social benefit CO<sub>2</sub>e intensity. Tons CO<sub>2</sub>e are summed and added to the pre-tax footprint. Finally, post-tax footprints are reduced by subtracting emissions equivalent to the percentage of taxes paid. For example, a household with a 20% tax rate would have its t CO<sub>2</sub>e pre-tax plus benefits footprint reduced by 20%.</p>

<div id="section1"><h3>Top 1% households</h3>
<p>While CPS is the most authoritative source on U.S. household income, top 1% households are under sampled, average incomes are underestimated (see <a href="#pclm.0000190.s001">S1 Text</a>), and top-coding affects some income categories. To address this, we create an over-sampled synthetic dataset for the next 0.9% and top 0.1% households and estimate their income, income sources, and CO<sub>2</sub>e intensity. This is done by creating a distribution of 1,000 households, for each group, whose mean pre-tax income and upper and lower thresholds come from WID and whose distributions is right-skewed to reflect within-group income inequality (see <a href="#pclm.0000190.s001">S1 Text</a> for detailed methodology).</p>
<p>We then extract the IPUMS CPS households that meet the WID top 1% threshold and bootstrap these into two 1,000 row datasets that are matched with the next 0.9% and top 0.1% WID synthetic income distributions. In this merged synthetic distribution, total household income from WID is then distributed to wage, investment, retirement, and employer healthcare income categories, based on values from the bootstrapped CPS households. Income related to retirement and employer healthcare, which makes up an exceedingly small share of total income (and emissions) for <em>top 1%</em> households, are directly extracted from the CPS households and subtracted from the household’s total income.</p>
<p>The remaining income is divided among wages and investment income. Due to limited reporting on capital gains and investment income in CPS we do not directly apply the raw income share allocation from the CPS bootstrapped households. Instead, we use CBO estimates on the share of capital income versus wage income for next 0.9% and top 0.1% households [<a href="#pclm.0000190.ref064">64</a>]. To do this we use CBO values for each group’s mean capital income share and create a normal distribution (<em>s</em> = 15%) of capital share values around this mean. In so doing we simulate variations in household income streams for each group. For each household, this wage and capital share (which sum to 100) are multiplied by the remaining household total income dollars. This yields dollars per wage and investment income category (see <em>Investment CO</em><sub><em>2</em></sub><em>e intensity</em> and <em>Investments and capital gains</em> in <a href="#pclm.0000190.s001">S1 Text</a> for more details on the investment income approach).</p>
<p>Each income category total dollars are then multiplied by the corresponding CO<sub>2</sub>e intensities for that category. These intensities come from the bootstrapped dataset but are allowed to randomly vary up ±25% from the original value. This is done to reflect the natural variation in GHG intensity that exists between top 1% households due to differences in employment and investment choices. Summing all categories yields pre-tax income based GHG footprints for the next 0.9% and top 0.1% groups. Post-tax footprints are calculated by adding the emissions value of social benefits (which are exceeding small as a portion of total income for these groups) and reducing these footprints in proportion to the household’s tax rate.</p>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section6"><h2>Supporting information</h2><div><h3><a href="https://journals.plos.org/climate/article/file?type=supplementary&amp;id=10.1371/journal.pclm.0000190.s002">S1 Fig. </a>Producer-based post-tax emissions share per income category, by income group (2018).</h3><p><em>Note</em>, <em>we present 2018 results here because “Healthcare (employment)” is not available for 2019</em>. <em>The 2019 results are otherwise similar</em>. (Supplier-based results are presented in <a href="#pclm-0000190-g001">Fig 1</a>).</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.s002">https://doi.org/10.1371/journal.pclm.0000190.s002</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/climate/article/file?type=supplementary&amp;id=10.1371/journal.pclm.0000190.s003">S2 Fig. </a>Annual mean CO<sub>2</sub>e intensity of producer-based wages and other income sources (1990–2019).</h3><p>Wages and employer healthcare contributions are industry specific CO<sub>2</sub>e multipliers and are presented by income group. The other income categories use weighted national average multipliers (shaded blue area). <em>Note</em>: <em>Healthcare (employment) has the same intensity as wage income and Healthcare (public) has the same intensity as social benefits</em>. (Supplier-based results are presented in <a href="#pclm-0000190-g002">Fig 2</a>).</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.s003">https://doi.org/10.1371/journal.pclm.0000190.s003</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/climate/article/file?type=supplementary&amp;id=10.1371/journal.pclm.0000190.s004">S3 Fig. </a>Supplier-based CO<sub>2</sub>e intensity of wages per individual (n = 141,159), by sector (2019).</h3><p>The blue diamond denotes the mean. Note, means here are calculated based on individual-level wage data and the CO<sub>2</sub>e intensity of their primary employment industry. This leads to some minor differences from the industry-level (n = 246) CO<sub>2</sub>e intensities used to calculate sectoral means in <a href="#pclm-0000190-t002">Table 2</a>. See <em>Carbon intensity by sector</em> in <a href="#pclm.0000190.s001">S1 Text</a> for further discussion.</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.s004">https://doi.org/10.1371/journal.pclm.0000190.s004</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/climate/article/file?type=supplementary&amp;id=10.1371/journal.pclm.0000190.s005">S4 Fig. </a>Producer-based CO<sub>2</sub>e intensity by sector (2019) (n = 141,031).</h3><p>The blue diamond denotes the mean. Note, to make differences between sectors clearer, 128 outlier observations related to Manufacturing are not displayed. The max value of those observations is 3.03 t CO<sub>2</sub>e per $1000. Note, means here are calculated based on individual-level wage data and the CO<sub>2</sub>e intensity of their primary employment industry. This leads to some minor differences from the industry-level (n = 246) CO<sub>2</sub>e intensities used to calculate sectoral means in <a href="#pclm-0000190-t002">Table 2</a>. See <em>Carbon intensity by sector</em> in <a href="#pclm.0000190.s001">S1 Text</a> for further discussion.</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.s005">https://doi.org/10.1371/journal.pclm.0000190.s005</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/climate/article/file?type=supplementary&amp;id=10.1371/journal.pclm.0000190.s006">S5 Fig. </a>Supplier-based emissions Lorenz curve (2019).</h3><p>Note, the population on the x axis is ranked by emissions, rather than income. While emissions strongly correlate with income there is some emissions variability at any given income level. Therefore, there is some minor discrepancy between the national emissions shares here and those seen in <a href="#pclm-0000190-g004">Fig 4</a>.</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.s006">https://doi.org/10.1371/journal.pclm.0000190.s006</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/climate/article/file?type=supplementary&amp;id=10.1371/journal.pclm.0000190.s007">S6 Fig. </a>Producer-based emissions Lorenz curve (2019).</h3><p>Note, the population on the x axis is ranked by emissions, rather than income. While emissions strongly correlate with income there is some emissions variability at any given income level. Therefore, there is some minor discrepancy between the national emissions shares here and those seen in <a href="#pclm.0000190.s009">S8 Fig</a>.</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.s007">https://doi.org/10.1371/journal.pclm.0000190.s007</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/climate/article/file?type=supplementary&amp;id=10.1371/journal.pclm.0000190.s008">S7 Fig. </a>Relationship between pre-tax income and household GHG footprint (log-log) using the producer income method (2019) (n = 69,483 –includes 2,000 synthetic data points for next 0.9% and top 0.1% households).</h3><p>(Supplier-based results are presented in <a href="#pclm-0000190-g003">Fig 3</a>).</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.s008">https://doi.org/10.1371/journal.pclm.0000190.s008</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/climate/article/file?type=supplementary&amp;id=10.1371/journal.pclm.0000190.s009">S8 Fig. </a>Mean household t CO<sub>2</sub>e emissions (2019) per income group under the pre-tax producer framework.</h3><p>The width of each income group, on the x-axis, corresponds with each group’s share of national emissions. Color indicates income category. Black error bars are bootstrapped 95% confidence intervals. Gray error bars are bootstrapped 95% confidence intervals when assuming ±20% error in carbon intensity per dollar. (Supplier-based results are presented in <a href="#pclm-0000190-g004">Fig 4</a>).</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.s009">https://doi.org/10.1371/journal.pclm.0000190.s009</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/climate/article/file?type=supplementary&amp;id=10.1371/journal.pclm.0000190.s010">S9 Fig. </a>Producer-based GHG emissions intensity per income group, pre-tax (1990–2019).</h3><p><em>Note</em>, <em>we believe the convergence of CO</em><sub><em>2</em></sub><em>e intensities in 2015 is the result of some error in the dataset</em>. <em>As this single year is not critical to the overall results</em>, <em>we do not attempt to impute alternative values</em>. (Supplier-based results are presented in <a href="#pclm-0000190-g005">Fig 5</a>).</p>
<p><a href="https://doi.org/10.1371/journal.pclm.0000190.s010">https://doi.org/10.1371/journal.pclm.0000190.s010</a></p><p>(TIF)</p>
</div></div>



<div xmlns:plos="http://plos.org"><h2>References</h2><ol><li id="ref1"><span>1.
            </span><a name="pclm.0000190.ref001" id="pclm.0000190.ref001"></a>Intergovernmental Panel on Climate Change. Climate Change 2022: Impacts, Adaptation and Vulnerability. 2020 Feb. Available: <a href="https://report.ipcc.ch/ar6wg2/pdf/IPCC_AR6_WGII_FinalDraft_FullReport.pdf">https://report.ipcc.ch/ar6wg2/pdf/IPCC_AR6_WGII_FinalDraft_FullReport.pdf</a> <ul><li><a href="#" data-author="" data-cit="Intergovernmental%20Panel%20on%20Climate%20Change.%20Climate%20Change%202022%3A%20Impacts%2C%20Adaptation%20and%20Vulnerability.%202020%20Feb.%20Available%3A%20https%3A%2F%2Freport.ipcc.ch%2Far6wg2%2Fpdf%2FIPCC_AR6_WGII_FinalDraft_FullReport.pdf" data-title="Climate%20Change%202022" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Climate+Change+2022++2020" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref2"><span>2.
            </span><a name="pclm.0000190.ref002" id="pclm.0000190.ref002"></a>United Nations Environmental Programme. Emissions Gap Report 2020. Nairobi; 2020. Available: <a href="https://www.unep.org/emissions-gap-report-2020">https://www.unep.org/emissions-gap-report-2020</a> <ul><li><a href="#" data-author="" data-cit="United%20Nations%20Environmental%20Programme.%20Emissions%20Gap%20Report%202020.%20Nairobi%3B%202020.%20Available%3A%20https%3A%2F%2Fwww.unep.org%2Femissions-gap-report-2020" data-title="Emissions%20Gap%20Report%202020.%20Nairobi" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Emissions+Gap+Report+2020.+Nairobi++2020" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref3"><span>3.
            </span><a name="pclm.0000190.ref003" id="pclm.0000190.ref003"></a>Diffenbaugh NS, Burke M. Global warming has increased global economic inequality. Proc Natl Acad Sci U S A. 2019;116: 9808–9813.  pmid:31010922 <ul data-doi="10.1073/pnas.1816020116"><li><a href="https://doi.org/10.1073/pnas.1816020116" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/31010922" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Global+warming+has+increased+global+economic+inequality+Diffenbaugh+2019" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref4"><span>4.
            </span><a name="pclm.0000190.ref004" id="pclm.0000190.ref004"></a>Hsiang S, Kopp R, Jina A, Rising J, Delgado M, Mohan S, et al. Estimating economic damage from climate change in the United States. Science (1979). 2017;356: 1362–1369.  pmid:28663496 <ul data-doi="10.1126/science.aal4369"><li><a href="https://doi.org/10.1126/science.aal4369" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/28663496" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Estimating+economic+damage+from+climate+change+in+the+United+States+Hsiang+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref5"><span>5.
            </span><a name="pclm.0000190.ref005" id="pclm.0000190.ref005"></a>Althor G, Watson JEM, Fuller RA. Global mismatch between greenhouse gas emissions and the burden of climate change. Scientific Reports 2016 6:1. 2016;6: 1–6.  pmid:26848052 <ul data-doi="10.1038/srep20281"><li><a href="https://doi.org/10.1038/srep20281" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/26848052" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Global+mismatch+between+greenhouse+gas+emissions+and+the+burden+of+climate+change+Althor+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref6"><span>6.
            </span><a name="pclm.0000190.ref006" id="pclm.0000190.ref006"></a>Islam N, Winkel J. Climate Change and Social Inequality. New York; 2017. Report No.: 152. Available: <a href="https://www.oecd-ilibrary.org/content/paper/2c62335d-en">https://www.oecd-ilibrary.org/content/paper/2c62335d-en</a> <ul><li><a href="#" data-author="Islam" data-cit="IslamN%2C%20WinkelJ.%20Climate%20Change%20and%20Social%20Inequality.%20New%20York%3B%202017.%20Report%20No.%3A%20152.%20Available%3A%20https%3A%2F%2Fwww.oecd-ilibrary.org%2Fcontent%2Fpaper%2F2c62335d-en" data-title="Climate%20Change%20and%20Social%20Inequality.%20New%20York" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Climate+Change+and+Social+Inequality.+New+York+Islam+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref7"><span>7.
            </span><a name="pclm.0000190.ref007" id="pclm.0000190.ref007"></a>King AD, Harrington LJ. The Inequality of Climate Change From 1.5 to 2°C of Global Warming. Geophys Res Lett. 2018;45: 5030–5033.  <ul data-doi="10.1029/2018GL078430"><li><a href="https://doi.org/10.1029/2018GL078430" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+Inequality+of+Climate+Change+From+1.5+to+2%C2%B0C+of+Global+Warming+King+2018" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref8"><span>8.
            </span><a name="pclm.0000190.ref008" id="pclm.0000190.ref008"></a>Leichenko R, Silva JA. Climate change and poverty: vulnerability, impacts, and alleviation strategies. Wiley Interdiscip Rev Clim Change. 2014;5: 539–556.  <ul data-doi="10.1002/wcc.287"><li><a href="https://doi.org/10.1002/wcc.287" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Climate+change+and+poverty%3A+vulnerability%2C+impacts%2C+and+alleviation+strategies+Leichenko+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref9"><span>9.
            </span><a name="pclm.0000190.ref009" id="pclm.0000190.ref009"></a>United Nations Environmental Programme. Emissions Gap Report 2022: The Closing Window—Climate crisis calls for rapid transformation of societies. Nairboi; 2022. Available: <a href="https://www.unep.org/resources/emissions-gap-report-2022">https://www.unep.org/resources/emissions-gap-report-2022</a> <ul><li><a href="#" data-author="" data-cit="United%20Nations%20Environmental%20Programme.%20Emissions%20Gap%20Report%202022%3A%20The%20Closing%20Window%E2%80%94Climate%20crisis%20calls%20for%20rapid%20transformation%20of%20societies.%20Nairboi%3B%202022.%20Available%3A%20https%3A%2F%2Fwww.unep.org%2Fresources%2Femissions-gap-report-2022" data-title="Emissions%20Gap%20Report%202022%3A%20The%20Closing%20Window%E2%80%94Climate%20crisis%20calls%20for%20rapid%20transformation%20of%20societies.%20Nairboi%3B" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Emissions+Gap+Report+2022%3A+The+Closing+Window%E2%80%94Climate+crisis+calls+for+rapid+transformation+of+societies.+Nairboi%3B++2022" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref10"><span>10.
            </span><a name="pclm.0000190.ref010" id="pclm.0000190.ref010"></a>United Nations Environmental Programme. Adaptation Gap Report 2022: Too Little, Too Slow, Climate adaptation failure puts world at risk. Nairobi, Kenya; 2022 Nov. Available: <a href="https://www.unep.org/resources/adaptation-gap-report-2022">https://www.unep.org/resources/adaptation-gap-report-2022</a> <ul><li><a href="#" data-author="" data-cit="United%20Nations%20Environmental%20Programme.%20Adaptation%20Gap%20Report%202022%3A%20Too%20Little%2C%20Too%20Slow%2C%20Climate%20adaptation%20failure%20puts%20world%20at%20risk.%20Nairobi%2C%20Kenya%3B%202022%20Nov.%20Available%3A%20https%3A%2F%2Fwww.unep.org%2Fresources%2Fadaptation-gap-report-2022" data-title="Adaptation%20Gap%20Report%202022%3A%20Too%20Little%2C%20Too%20Slow%2C%20Climate%20adaptation%20failure%20puts%20world%20at%20risk.%20Nairobi%2C%20Kenya" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Adaptation+Gap+Report+2022%3A+Too+Little%2C+Too+Slow%2C+Climate+adaptation+failure+puts+world+at+risk.+Nairobi%2C+Kenya++2022" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref11"><span>11.
            </span><a name="pclm.0000190.ref011" id="pclm.0000190.ref011"></a>OECD. Aggregate Trends of Climate Finance Provided and Mobilised by Developed Countries in 2013–2020. Climate Finance and the USD 100 Billion Goal. Paris; 2022. <a href="https://doi.org/10.1787/d28f963c-en">https://doi.org/10.1787/d28f963c-en</a> <ul></ul></li><li id="ref12"><span>12.
            </span><a name="pclm.0000190.ref012" id="pclm.0000190.ref012"></a>Davis SJ, Caldeira K. Consumption-based accounting of CO2 emissions. Proc Natl Acad Sci U S A. 2010;107: 5687–5692.  pmid:20212122 <ul data-doi="10.1073/pnas.0906974107"><li><a href="https://doi.org/10.1073/pnas.0906974107" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/20212122" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Consumption-based+accounting+of+CO2+emissions+Davis+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref13"><span>13.
            </span><a name="pclm.0000190.ref013" id="pclm.0000190.ref013"></a>Ivanova D, Stadler K, Steen-Olsen K, Wood R, Vita G, Tukker A, et al. Environmental Impact Assessment of Household Consumption. J Ind Ecol. 2016;20: 526–536.  <ul data-doi="10.1111/jiec.12371"><li><a href="https://doi.org/10.1111/jiec.12371" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Environmental+Impact+Assessment+of+Household+Consumption+Ivanova+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref14"><span>14.
            </span><a name="pclm.0000190.ref014" id="pclm.0000190.ref014"></a>Hertwich EG, Peters GP. Carbon footprint of nations: A global, trade-linked analysis. Environ Sci Technol. 2009;43: 6414–6420.  pmid:19746745 <ul data-doi="10.1021/es803496a"><li><a href="https://doi.org/10.1021/es803496a" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/19746745" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Carbon+footprint+of+nations%3A+A+global%2C+trade-linked+analysis+Hertwich+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref15"><span>15.
            </span><a name="pclm.0000190.ref015" id="pclm.0000190.ref015"></a>Hubacek K, Baiocchi G, Feng K, Muñoz Castillo R, Sun L, Xue J. Global carbon inequality. Energy Ecol Environ. 2017;2: 361–369.  <ul data-doi="10.1007/s40974-017-0072-9"><li><a href="https://doi.org/10.1007/s40974-017-0072-9" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Global+carbon+inequality+Hubacek+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref16"><span>16.
            </span><a name="pclm.0000190.ref016" id="pclm.0000190.ref016"></a>Feng K, Hubacek K, Song K. Household carbon inequality in the U.S. J Clean Prod. 2021;278: 123994.  <ul data-doi="10.1016/j.jclepro.2020.123994"><li><a href="https://doi.org/10.1016/j.jclepro.2020.123994" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Household+carbon+inequality+in+the+U.S+Feng+2021" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref17"><span>17.
            </span><a name="pclm.0000190.ref017" id="pclm.0000190.ref017"></a>Song K, Qu S, Taiebat M, Liang S, Xu M. Scale, distribution and variations of global greenhouse gas emissions driven by U.S. households. Environ Int. 2019;133: 105137.  pmid:31518931 <ul data-doi="10.1016/j.envint.2019.105137"><li><a href="https://doi.org/10.1016/j.envint.2019.105137" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/31518931" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Scale%2C+distribution+and+variations+of+global+greenhouse+gas+emissions+driven+by+U.S.+households+Song+2019" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref18"><span>18.
            </span><a name="pclm.0000190.ref018" id="pclm.0000190.ref018"></a>Weber CL, Matthews HS. Quantifying the global and distributional aspects of American household carbon footprint. Ecological Economics. 2008;66: 379–391.  <ul data-doi="10.1016/j.ecolecon.2007.09.021"><li><a href="https://doi.org/10.1016/j.ecolecon.2007.09.021" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Quantifying+the+global+and+distributional+aspects+of+American+household+carbon+footprint+Weber+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref19"><span>19.
            </span><a name="pclm.0000190.ref019" id="pclm.0000190.ref019"></a>Lenzen M. Energy and greenhouse gas cost of living for Australia during 1993/94. Energy. 1998;23: 497–516.  <ul data-doi="10.1016/S0360-5442(98)00020-6"><li><a href="https://doi.org/10.1016/S0360-5442(98)00020-6" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Energy+and+greenhouse+gas+cost+of+living+for+Australia+during+1993%2F94+Lenzen+1998" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref20"><span>20.
            </span><a name="pclm.0000190.ref020" id="pclm.0000190.ref020"></a>Bicknell KB, Ball RJ, Cullen R, Bigsby HR. New methodology for the ecological footprint with an application to the New Zealand economy. Ecological Economics. 1998;27: 149–160.  <ul data-doi="10.1016/S0921-8009(97)00136-5"><li><a href="https://doi.org/10.1016/S0921-8009(97)00136-5" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=New+methodology+for+the+ecological+footprint+with+an+application+to+the+New+Zealand+economy+Bicknell+1998" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref21"><span>21.
            </span><a name="pclm.0000190.ref021" id="pclm.0000190.ref021"></a>Moran D, Wood R, Hertwich E, Mattson K, Rodriguez JFD, Schanes K, et al. Quantifying the potential for consumer-oriented policy to reduce European and foreign carbon emissions. Climate Policy. 2020;20: S28–S38.  <ul data-doi="10.1080/14693062.2018.1551186"><li><a href="https://doi.org/10.1080/14693062.2018.1551186" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Quantifying+the+potential+for+consumer-oriented+policy+to+reduce+European+and+foreign+carbon+emissions+Moran+2020" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref22"><span>22.
            </span><a name="pclm.0000190.ref022" id="pclm.0000190.ref022"></a>Liang S, Qu S, Zhu Z, Guan D, Xu M. Income-Based Greenhouse Gas Emissions of Nations. Environ Sci Technol. 2017;51: 346–355.  pmid:27936320 <ul data-doi="10.1021/acs.est.6b02510"><li><a href="https://doi.org/10.1021/acs.est.6b02510" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/27936320" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Income-Based+Greenhouse+Gas+Emissions+of+Nations+Liang+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref23"><span>23.
            </span><a name="pclm.0000190.ref023" id="pclm.0000190.ref023"></a>Zhang Y. Provincial responsibility for carbon emissions in China under different principles. Energy Policy. 2015;86: 142–153.  <ul data-doi="10.1016/j.enpol.2015.07.002"><li><a href="https://doi.org/10.1016/j.enpol.2015.07.002" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Provincial+responsibility+for+carbon+emissions+in+China+under+different+principles+Zhang+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref24"><span>24.
            </span><a name="pclm.0000190.ref024" id="pclm.0000190.ref024"></a>Zhang Y. The responsibility for carbon emissions and carbon efficiency at the sectoral level: Evidence from China. Energy Econ. 2013;40: 967–975.  <ul data-doi="10.1016/j.eneco.2013.05.025"><li><a href="https://doi.org/10.1016/j.eneco.2013.05.025" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+responsibility+for+carbon+emissions+and+carbon+efficiency+at+the+sectoral+level+Zhang+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref25"><span>25.
            </span><a name="pclm.0000190.ref025" id="pclm.0000190.ref025"></a>Marques A, Rodrigues J, Lenzen M, Domingos T. Income-based environmental responsibility. Ecological Economics. 2012;84: 57–65.  <ul data-doi="10.1016/j.ecolecon.2012.09.010"><li><a href="https://doi.org/10.1016/j.ecolecon.2012.09.010" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Income-based+environmental+responsibility+Marques+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref26"><span>26.
            </span><a name="pclm.0000190.ref026" id="pclm.0000190.ref026"></a>Gallego B, Lenzen M. A consistent input-output formulation of shared producer and consumer responsibility. Economic Systems Research. 2005;17: 365–391.  <ul data-doi="10.1080/09535310500283492"><li><a href="https://doi.org/10.1080/09535310500283492" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=A+consistent+input-output+formulation+of+shared+producer+and+consumer+responsibility+Gallego+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref27"><span>27.
            </span><a name="pclm.0000190.ref027" id="pclm.0000190.ref027"></a>Marques A, Rodrigues J, Domingos T. International trade and the geographical separation between income and enabled carbon emissions. Ecological Economics. 2013;89: 162–169.  <ul data-doi="10.1016/j.ecolecon.2013.02.020"><li><a href="https://doi.org/10.1016/j.ecolecon.2013.02.020" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=International+trade+and+the+geographical+separation+between+income+and+enabled+carbon+emissions+Marques+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref28"><span>28.
            </span><a name="pclm.0000190.ref028" id="pclm.0000190.ref028"></a>Liang S, Wang H, Qu S, Feng T, Guan D, Fang H, et al. Socioeconomic Drivers of Greenhouse Gas Emissions in the United States. Environ Sci Technol. 2016;50: 7535–7545.  pmid:27276120 <ul data-doi="10.1021/acs.est.6b00872"><li><a href="https://doi.org/10.1021/acs.est.6b00872" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/27276120" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Socioeconomic+Drivers+of+Greenhouse+Gas+Emissions+in+the+United+States+Liang+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref29"><span>29.
            </span><a name="pclm.0000190.ref029" id="pclm.0000190.ref029"></a>Zhang Z, Zhang ZX, Zhu K. Allocating carbon responsibility: The role of spatial production fragmentation. Energy Econ. 2020;87: 104491.  <ul data-doi="10.1016/J.ENECO.2019.104491"><li><a href="https://doi.org/10.1016/J.ENECO.2019.104491" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Allocating+carbon+responsibility%3A+The+role+of+spatial+production+fragmentation+Zhang+2020" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref30"><span>30.
            </span><a name="pclm.0000190.ref030" id="pclm.0000190.ref030"></a>World Inequality Database. USA Income Inequality. 2021 [cited 21 Mar 2021]. Available: <a href="https://wid.world/country/usa/">https://wid.world/country/usa/</a> <ul><li><a href="#" data-author="" data-cit="World%20Inequality%20Database.%20USA%20Income%20Inequality.%202021%20%5Bcited%2021%20Mar%202021%5D.%20Available%3A%20https%3A%2F%2Fwid.world%2Fcountry%2Fusa%2F" data-title="USA%20Income%20Inequality" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=USA+Income+Inequality++2021" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref31"><span>31.
            </span><a name="pclm.0000190.ref031" id="pclm.0000190.ref031"></a>Jones CM, Kammen DM. Quantifying carbon footprint reduction opportunities for U.S. households and communities. Environ Sci Technol. 2011;45: 4088–4095.  pmid:21449584 <ul data-doi="10.1021/es102221h"><li><a href="https://doi.org/10.1021/es102221h" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21449584" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Quantifying+carbon+footprint+reduction+opportunities+for+U.S.+households+and+communities+Jones+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref32"><span>32.
            </span><a name="pclm.0000190.ref032" id="pclm.0000190.ref032"></a>Sager L. Income inequality and carbon consumption: Evidence from Environmental Engel curves. Energy Econ. 2019;84: 104507.  <ul data-doi="10.1016/J.ENECO.2019.104507"><li><a href="https://doi.org/10.1016/J.ENECO.2019.104507" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Income+inequality+and+carbon+consumption%3A+Evidence+from+Environmental+Engel+curves+Sager+2019" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref33"><span>33.
            </span><a name="pclm.0000190.ref033" id="pclm.0000190.ref033"></a>Starr J, Nicolson C, Ash M, Markowitz EM, Moran D. Assessing U.S. consumers’ carbon footprints reveals outsized impact of the top 1%. Ecological Economics. 2023;205: 107698.  <ul data-doi="10.1016/J.ECOLECON.2022.107698"><li><a href="https://doi.org/10.1016/J.ECOLECON.2022.107698" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Assessing+U.S.+consumers%E2%80%99+carbon+footprints+reveals+outsized+impact+of+the+top+1%25+Starr+2023" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref34"><span>34.
            </span><a name="pclm.0000190.ref034" id="pclm.0000190.ref034"></a>Starr J. United States Household Carbon Footprints: Quantifying the relationship between household-level income inequality and greenhouse gas emissions (1996–2015). Doctoral Dissertations. Amherst, Massachusetts; 2021 Sep. <a href="https://doi.org/10.7275/24493178">https://doi.org/10.7275/24493178</a> <ul></ul></li><li id="ref35"><span>35.
            </span><a name="pclm.0000190.ref035" id="pclm.0000190.ref035"></a>Pottier A, Le Treut G. Quantifying GHG emissions enabled by capital and labor: Economic and gender inequalities in France. J Ind Ecol. 2023 [cited 12 Feb 2023].  <ul data-doi="10.1111/JIEC.13383"><li><a href="https://doi.org/10.1111/JIEC.13383" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Quantifying+GHG+emissions+enabled+by+capital+and+labor%3A+Economic+and+gender+inequalities+in+France+Pottier+2023" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref36"><span>36.
            </span><a name="pclm.0000190.ref036" id="pclm.0000190.ref036"></a>Lenzen M, Moran D, Kanemoto K, Geschke A. Building Eora: A Global multi-region input-output database at high country and sector resolution. Economic Systems Research. 2013;25: 20–49.  <ul data-doi="10.1080/09535314.2013.769938"><li><a href="https://doi.org/10.1080/09535314.2013.769938" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Building+Eora%3A+A+Global+multi-region+input-output+database+at+high+country+and+sector+resolution+Lenzen+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref37"><span>37.
            </span><a name="pclm.0000190.ref037" id="pclm.0000190.ref037"></a>Lenzen M, Kanemoto K, Moran D, Geschke A. Mapping the structure of the world economy. Environ Sci Technol. 2012;46: 8374–8381.  pmid:22794089 <ul data-doi="10.1021/es300171x"><li><a href="https://doi.org/10.1021/es300171x" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22794089" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Mapping+the+structure+of+the+world+economy+Lenzen+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref38"><span>38.
            </span><a name="pclm.0000190.ref038" id="pclm.0000190.ref038"></a>King S, King M, Rodgers R, Ruggles S, Warren JR. Integrated Public Use Microdata Series, Current Population Survey: Version 8.0 [dataset]. Minneapolis, MN; 2020. <a href="https://doi.org/10.18128/D030.V8.0">https://doi.org/10.18128/D030.V8.0</a> <ul></ul></li><li id="ref39"><span>39.
            </span><a name="pclm.0000190.ref039" id="pclm.0000190.ref039"></a>Eurostat. Eurostat Manual of Supply, Use and Input-Output Tables 2008 edition. 2008. Available: <a href="http://bookshop/">http://bookshop</a>. <ul><li><a href="#" data-author="" data-cit="Eurostat.%20Eurostat%20Manual%20of%20Supply%2C%20Use%20and%20Input-Output%20Tables%202008%20edition.%202008.%20Available%3A%20http%3A%2F%2Fbookshop." data-title="Eurostat.%20Eurostat%20Manual%20of%20Supply%2C%20Use%20and%20Input-Output%20Tables%202008%20edition" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Eurostat.+Eurostat+Manual+of+Supply%2C+Use+and+Input-Output+Tables+2008+edition++2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref40"><span>40.
            </span><a name="pclm.0000190.ref040" id="pclm.0000190.ref040"></a>Pottier A. Expenditure elasticity and income elasticity of GHG emissions: A survey of literature on household carbon footprint. Ecological Economics. 2022;192: 107251.  <ul data-doi="10.1016/J.ECOLECON.2021.107251"><li><a href="https://doi.org/10.1016/J.ECOLECON.2021.107251" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Expenditure+elasticity+and+income+elasticity+of+GHG+emissions%3A+A+survey+of+literature+on+household+carbon+footprint+Pottier+2022" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref41"><span>41.
            </span><a name="pclm.0000190.ref041" id="pclm.0000190.ref041"></a>Zheng H, Long Y, Wood R, Moran D, Zhang Z, Meng J, et al. Ageing society in developed countries challenges carbon mitigation. Nature Climate Change 2022 12:3. 2022;12: 241–248.  <ul data-doi="10.1038/s41558-022-01302-y"><li><a href="https://doi.org/10.1038/s41558-022-01302-y" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Ageing+society+in+developed+countries+challenges+carbon+mitigation+Zheng+2022" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref42"><span>42.
            </span><a name="pclm.0000190.ref042" id="pclm.0000190.ref042"></a>Board of Governors of the Federal Reserve System. Survey of Consumer Finances, 1989–2019. 4 Nov 2021 [cited 29 Jun 2023]. Available: <a href="https://www.federalreserve.gov/econres/scf/dataviz/scf/chart/#series:Net_Worth;demographic:agecl;population:1">https://www.federalreserve.gov/econres/scf/dataviz/scf/chart/#series:Net_Worth;demographic:agecl;population:1</a>,2,3,4,5,6;units:mean <ul><li><a href="#" data-author="" data-cit="Board%20of%20Governors%20of%20the%20Federal%20Reserve%20System.%20Survey%20of%20Consumer%20Finances%2C%201989%E2%80%932019.%204%20Nov%202021%20%5Bcited%2029%20Jun%202023%5D.%20Available%3A%20https%3A%2F%2Fwww.federalreserve.gov%2Feconres%2Fscf%2Fdataviz%2Fscf%2Fchart%2F%23series%3ANet_Worth%3Bdemographic%3Aagecl%3Bpopulation%3A1%2C2%2C3%2C4%2C5%2C6%3Bunits%3Amean" data-title="Survey%20of%20Consumer%20Finances" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Survey+of+Consumer+Finances++2021" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref43"><span>43.
            </span><a name="pclm.0000190.ref043" id="pclm.0000190.ref043"></a>Chancel L. Global carbon inequality over 1990–2019. Nature Sustainability 2022. 2022; 1–8.  <ul data-doi="10.1038/s41893-022-00955-z"><li><a href="https://doi.org/10.1038/s41893-022-00955-z" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Global+carbon+inequality+over+1990%E2%80%932019+Chancel+2022" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref44"><span>44.
            </span><a name="pclm.0000190.ref044" id="pclm.0000190.ref044"></a>Stavins RN. The Future of US Carbon-Pricing Policy. Environ Energy Policy Econ. 2020;1: 8–64.  <ul data-doi="10.1086/706792"><li><a href="https://doi.org/10.1086/706792" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+Future+of+US+Carbon-Pricing+Policy+Stavins+2020" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref45"><span>45.
            </span><a name="pclm.0000190.ref045" id="pclm.0000190.ref045"></a>Heal G, Schlenker W. Coase, Hotelling and Pigou: The Incidence of a Carbon Tax and CO <sub>2</sub> Emissions. Cambridge, MA; 2019 Jul. Report No.: 26086.  <ul data-doi="10.3386/w26086"><li><a href="https://doi.org/10.3386/w26086" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Coase%2C+Hotelling+and+Pigou%3A+The+Incidence+of+a+Carbon+Tax+and+CO+2+Emissions.+Cambridge%2C+MA+Heal+2019" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref46"><span>46.
            </span><a name="pclm.0000190.ref046" id="pclm.0000190.ref046"></a>Boyce JK. Carbon Pricing: Effectiveness and Equity. Ecological Economics. 2018;150: 52–61.  <ul data-doi="10.1016/J.ECOLECON.2018.03.030"><li><a href="https://doi.org/10.1016/J.ECOLECON.2018.03.030" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Carbon+Pricing+Boyce+2018" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref47"><span>47.
            </span><a name="pclm.0000190.ref047" id="pclm.0000190.ref047"></a>Umell K. Who Pollutes? A Household-Level Database of America’s Greenhouse Gas Footprint. Washington, D.C.; 2014 Oct. Report No.: 381. Available: <a href="https://www.cgdev.org/sites/default/files/who_pollutes_greenhouse_gas_footprint_0.pdf">https://www.cgdev.org/sites/default/files/who_pollutes_greenhouse_gas_footprint_0.pdf</a> <ul><li><a href="#" data-author="Umell" data-cit="UmellK.%20Who%20Pollutes%3F%20A%20Household-Level%20Database%20of%20America%E2%80%99s%20Greenhouse%20Gas%20Footprint.%20Washington%2C%20D.C.%3B%202014%20Oct.%20Report%20No.%3A%20381.%20Available%3A%20https%3A%2F%2Fwww.cgdev.org%2Fsites%2Fdefault%2Ffiles%2Fwho_pollutes_greenhouse_gas_footprint_0.pdf" data-title="Who%20Pollutes%3F%20A%20Household-Level%20Database%20of%20America%E2%80%99s%20Greenhouse%20Gas%20Footprint" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Who+Pollutes%3F+A+Household-Level+Database+of+America%E2%80%99s+Greenhouse+Gas+Footprint+Umell+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref48"><span>48.
            </span><a name="pclm.0000190.ref048" id="pclm.0000190.ref048"></a>Kartha S, Kemp-Benedict E, Ghosh E, Nazareth A, Gore T. The Carbon Inequality Era: An assessment of the global distribution of consumption emissions among individuals from 1990 to 2015 and beyond. 2020. <ul><li><a href="#" data-author="Kartha" data-cit="KarthaS%2C%20Kemp-BenedictE%2C%20GhoshE%2C%20NazarethA%2C%20GoreT.%20The%20Carbon%20Inequality%20Era%3A%20An%20assessment%20of%20the%20global%20distribution%20of%20consumption%20emissions%20among%20individuals%20from%201990%20to%202015%20and%20beyond.%202020." data-title="The%20Carbon%20Inequality%20Era%3A%20An%20assessment%20of%20the%20global%20distribution%20of%20consumption%20emissions%20among%20individuals%20from%201990%20to%202015%20and%20beyond" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+Carbon+Inequality+Era%3A+An+assessment+of+the+global+distribution+of+consumption+emissions+among+individuals+from+1990+to+2015+and+beyond+Kartha+2020" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref49"><span>49.
            </span><a name="pclm.0000190.ref049" id="pclm.0000190.ref049"></a>Chancel L, Piketty T. Carbon and inequality: from Kyoto to Paris Trends in the global inequality of carbon emissions (1998–2013) &amp; prospects for an equitable adaptation fund. Paris; 2015 Nov. Available: <a href="http://piketty.pse.ens.fr/files/ChancelPiketty2015.pdf">http://piketty.pse.ens.fr/files/ChancelPiketty2015.pdf</a> <ul><li><a href="#" data-author="Chancel" data-cit="ChancelL%2C%20PikettyT.%20Carbon%20and%20inequality%3A%20from%20Kyoto%20to%20Paris%20Trends%20in%20the%20global%20inequality%20of%20carbon%20emissions%20%281998%E2%80%932013%29%20%26%20prospects%20for%20an%20equitable%20adaptation%20fund.%20Paris%3B%202015%20Nov.%20Available%3A%20http%3A%2F%2Fpiketty.pse.ens.fr%2Ffiles%2FChancelPiketty2015.pdf" data-title="Carbon%20and%20inequality%3A%20from%20Kyoto%20to%20Paris%20Trends%20in%20the%20global%20inequality%20of%20carbon%20emissions%20%281998%E2%80%932013%29%20%26amp%3B%20prospects%20for%20an%20equitable%20adaptation%20fund" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Carbon+and+inequality%3A+from+Kyoto+to+Paris+Trends+in+the+global+inequality+of+carbon+emissions+%281998%E2%80%932013%29+%26amp%3B+prospects+for+an+equitable+adaptation+fund+Chancel+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref50"><span>50.
            </span><a name="pclm.0000190.ref050" id="pclm.0000190.ref050"></a>Hirschl TA, Rank MR. The Life Course Dynamics of Affluence. PLoS One. 2015;10.  pmid:25629530 <ul data-doi="10.1371/journal.pone.0116370"><li><a href="https://doi.org/10.1371/journal.pone.0116370" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25629530" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+Life+Course+Dynamics+of+Affluence+Hirschl+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref51"><span>51.
            </span><a name="pclm.0000190.ref051" id="pclm.0000190.ref051"></a>Dabi N, Khalfan A, Lawson M, Maitland A, Poidatz A, Stroot H. Carbon billionaires: The investment emissions of the world’s richest people. 2022 Nov.  <ul data-doi="10.21201/2022.9684"><li><a href="https://doi.org/10.21201/2022.9684" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Carbon+billionaires%3A+The+investment+emissions+of+the+world%E2%80%99s+richest+people+Dabi+2022" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref52"><span>52.
            </span><a name="pclm.0000190.ref052" id="pclm.0000190.ref052"></a>Chancel L, Bothe P, Voituriez T. Climate inequality report 2023: Fair taxes for a sustainable future in the global south. 2023. Available: <a href="https://wid.world/wp-content/uploads/2023/01/CBV2023-ClimateInequalityReport-1.pdf">https://wid.world/wp-content/uploads/2023/01/CBV2023-ClimateInequalityReport-1.pdf</a> <ul><li><a href="#" data-author="Chancel" data-cit="ChancelL%2C%20BotheP%2C%20VoituriezT.%20Climate%20inequality%20report%202023%3A%20Fair%20taxes%20for%20a%20sustainable%20future%20in%20the%20global%20south.%202023.%20Available%3A%20https%3A%2F%2Fwid.world%2Fwp-content%2Fuploads%2F2023%2F01%2FCBV2023-ClimateInequalityReport-1.pdf" data-title="Climate%20inequality%20report%202023%3A%20Fair%20taxes%20for%20a%20sustainable%20future%20in%20the%20global%20south" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Climate+inequality+report+2023%3A+Fair+taxes+for+a+sustainable+future+in+the+global+south+Chancel+2023" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref53"><span>53.
            </span><a name="pclm.0000190.ref053" id="pclm.0000190.ref053"></a>Kapeller J, Leitch S, Wildauer R. Can a European wealth tax close the green investment gap? Ecological Economics. 2023;209: 107849.  <ul data-doi="10.1016/J.ECOLECON.2023.107849"><li><a href="https://doi.org/10.1016/J.ECOLECON.2023.107849" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Can+a+European+wealth+tax+close+the+green+investment+gap+Kapeller+2023" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref54"><span>54.
            </span><a name="pclm.0000190.ref054" id="pclm.0000190.ref054"></a>Mattauch L, Klenert D, Stiglitz JE, Edenhofer O. Overcoming wealth inequality by capital taxes that finance public investment. Structural Change and Economic Dynamics. 2022;63: 383–395.  <ul data-doi="10.1016/J.STRUECO.2022.05.009"><li><a href="https://doi.org/10.1016/J.STRUECO.2022.05.009" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Overcoming+wealth+inequality+by+capital+taxes+that+finance+public+investment+Mattauch+2022" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref55"><span>55.
            </span><a name="pclm.0000190.ref055" id="pclm.0000190.ref055"></a>Zhang Z, Li J, Guan D. Value chain carbon footprints of Chinese listed companies. Nature Communications 2023 14:1. 2023;14: 1–9.  pmid:37193691 <ul data-doi="10.1038/s41467-023-38479-5"><li><a href="https://doi.org/10.1038/s41467-023-38479-5" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/37193691" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Value+chain+carbon+footprints+of+Chinese+listed+companies+Zhang+2023" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref56"><span>56.
            </span><a name="pclm.0000190.ref056" id="pclm.0000190.ref056"></a>Zhang Z, Guan D, Wang R, Meng J, Zheng H, Zhu K, et al. Embodied carbon emissions in the supply chains of multinational enterprises. Nature Climate Change 2020 10:12. 2020;10: 1096–1101.  <ul data-doi="10.1038/s41558-020-0895-9"><li><a href="https://doi.org/10.1038/s41558-020-0895-9" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Embodied+carbon+emissions+in+the+supply+chains+of+multinational+enterprises+Zhang+2020" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref57"><span>57.
            </span><a name="pclm.0000190.ref057" id="pclm.0000190.ref057"></a>Bivens J, Kandra J. CEO pay has skyrocketed 1,460% since 1978: CEOs were paid 399 times as much as a typical worker in 2021. Washington DC; 2022 Oct. Available: <a href="https://www.epi.org/publication/ceo-pay-in-2021/">https://www.epi.org/publication/ceo-pay-in-2021/</a> <ul><li><a href="#" data-author="Bivens" data-cit="BivensJ%2C%20KandraJ.%20CEO%20pay%20has%20skyrocketed%201%2C460%25%20since%201978%3A%20CEOs%20were%20paid%20399%20times%20as%20much%20as%20a%20typical%20worker%20in%202021.%20Washington%20DC%3B%202022%20Oct.%20Available%3A%20https%3A%2F%2Fwww.epi.org%2Fpublication%2Fceo-pay-in-2021%2F" data-title="CEO%20pay%20has%20skyrocketed%201%2C460%25%20since%201978%3A%20CEOs%20were%20paid%20399%20times%20as%20much%20as%20a%20typical%20worker%20in%202021" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=CEO+pay+has+skyrocketed+1%2C460%25+since+1978%3A+CEOs+were+paid+399+times+as+much+as+a+typical+worker+in+2021+Bivens+2022" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref58"><span>58.
            </span><a name="pclm.0000190.ref058" id="pclm.0000190.ref058"></a>Intergovernmental Panel on Climate Change. Global Warming of 1.5°C—. 2019. Available: <a href="https://www.ipcc.ch/sr15/">https://www.ipcc.ch/sr15/</a> <ul><li><a href="#" data-author="" data-cit="Intergovernmental%20Panel%20on%20Climate%20Change.%20Global%20Warming%20of%201.5%C2%B0C%E2%80%94.%202019.%20Available%3A%20https%3A%2F%2Fwww.ipcc.ch%2Fsr15%2F" data-title="Global%20Warming%20of%201.5%C2%B0C%E2%80%94.%202019" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Global+Warming+of+1.5%C2%B0C%E2%80%94.+2019++" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref59"><span>59.
            </span><a name="pclm.0000190.ref059" id="pclm.0000190.ref059"></a>Hoegh-Guldberg O, Jacob D, Taylor M, Guillén Bolaños T, Bindi M, Brown S, et al. The human imperative of stabilizing global climate change at 1.5°C. Science (1979). 2019;365.  pmid:31604209 <ul data-doi="10.1126/science.aaw6974"><li><a href="https://doi.org/10.1126/science.aaw6974" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/31604209" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+human+imperative+of+stabilizing+global+climate+change+at+1.5%C2%B0C+Hoegh-Guldberg+2019" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref60"><span>60.
            </span><a name="pclm.0000190.ref060" id="pclm.0000190.ref060"></a>Fanning AL, Hickel J. Compensation for atmospheric appropriation. Nature Sustainability 2023. 2023; 1–10.  <ul data-doi="10.1038/s41893-023-01130-8"><li><a href="https://doi.org/10.1038/s41893-023-01130-8" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Compensation+for+atmospheric+appropriation+Fanning+2023" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref61"><span>61.
            </span><a name="pclm.0000190.ref061" id="pclm.0000190.ref061"></a>Gilens M, Page BI. Testing theories of American politics: Elites, interest groups, and average citizens. Perspectives on Politics. 2014;12: 564–581.  <ul data-doi="10.1017/S1537592714001595"><li><a href="https://doi.org/10.1017/S1537592714001595" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Testing+theories+of+American+politics%3A+Elites%2C+interest+groups%2C+and+average+citizens+Gilens+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref62"><span>62.
            </span><a name="pclm.0000190.ref062" id="pclm.0000190.ref062"></a>Gütschow J, Jeffery ML, Gieseke R, Gebel R, Stevens D, Krapp M, et al. The PRIMAP-hist national historical emissions time series. Earth Syst Sci Data. 2016;8: 571–603.  <ul data-doi="10.5194/essd-8-571-2016"><li><a href="https://doi.org/10.5194/essd-8-571-2016" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+PRIMAP-hist+national+historical+emissions+time+series+G%C3%BCtschow+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref63"><span>63.
            </span><a name="pclm.0000190.ref063" id="pclm.0000190.ref063"></a>Gütschow J, Günther A, Pflüger M. The PRIMAP-hist national historical emissions time series (1750–2019) v2.3. 2021 [cited 15 Jun 2023].  <ul data-doi="10.5281/ZENODO.5175154"><li><a href="https://doi.org/10.5281/ZENODO.5175154" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+PRIMAP-hist+national+historical+emissions+time+series+G%C3%BCtschow+2021" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref64"><span>64.
            </span><a name="pclm.0000190.ref064" id="pclm.0000190.ref064"></a>Office C of the USCB. The Distribution of Household Income and Federal Taxes. 2014. Available: <a href="https://www.cbo.gov/about/products/major-recurring-reports#15">https://www.cbo.gov/about/products/major-recurring-reports#15</a> <ul><li><a href="#" data-author="" data-cit="Office%20C%20of%20the%20USCB.%20The%20Distribution%20of%20Household%20Income%20and%20Federal%20Taxes.%202014.%20Available%3A%20https%3A%2F%2Fwww.cbo.gov%2Fabout%2Fproducts%2Fmajor-recurring-reports%2315" data-title="The%20Distribution%20of%20Household%20Income%20and%20Federal%20Taxes" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+Distribution+of+Household+Income+and+Federal+Taxes++2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li></ol></div>



          

        </div>
      </div>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A look at Apple’s new Transformer-powered predictive text model (336 pts)]]></title>
            <link>https://jackcook.com/2023/09/08/predictive-text.html</link>
            <guid>37541093</guid>
            <pubDate>Sun, 17 Sep 2023 03:00:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jackcook.com/2023/09/08/predictive-text.html">https://jackcook.com/2023/09/08/predictive-text.html</a>, See on <a href="https://news.ycombinator.com/item?id=37541093">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="post">
  
  <p>
    New York, NY — September 08, 2023
  </p>
  <p>At WWDC earlier this year, Apple announced that upcoming versions of iOS and macOS would ship with a new feature powered by “<a href="https://www.apple.com/newsroom/2023/06/ios-17-makes-iphone-more-personal-and-intuitive/">a Transformer language model</a>” that will give users “predictive text recommendations inline as they type.”</p>

<p>Upon hearing this announcement, I was pretty curious about how this feature works.
Apple hasn’t deployed many language models of their own, despite most of their competitors going all-in on large language models over the last couple years.
I see this as a result of Apple generally priding themselves on polish and perfection, while language models are fairly unpolished and imperfect.</p>

<p>As a result, this may be one of the first Transformer-based models that Apple will ship in one of its operating systems, or at least one of the first that they’ve acknowledged publicly.
This left me with some questions about the feature, notably:</p>

<ul>
  <li>What underlying model is powering this feature?</li>
  <li>What is its architecture?</li>
  <li>What data was used to train the model?</li>
</ul>

<p>After spending some time with these questions, I was able to find some answers, but many of the details still remain unclear.
If you’re able to get any further than I could, please get in touch!</p>

<h2 id="how-does-the-feature-work">How does the feature work?</h2>

<p>After installing the macOS beta, I immediately opened the Notes app and started typing.
Despite trying many different sentence structures, the feature generally appeared less often than I expected it to.
It mostly completes individual words.</p>

<figure>
<img src="https://jackcook.com/img/blog/predictive-text/individual-word-2.png">
<figcaption>Predictive text completing one word at a time.</figcaption>
</figure>

<p>The feature will occasionally suggest more than one word at a time, but this is generally limited to instances where the upcoming words are extremely obvious, similar to the autocomplete in Gmail.</p>

<figure>
<img src="https://jackcook.com/img/blog/predictive-text/multiple-words.jpeg">
<figcaption>Predictive text completing two words at a time.</figcaption>
</figure>

<h2 id="can-we-dig-deeper">Can we dig deeper?</h2>

<p>Finding the model itself was a little tough, but I eventually found the model being used by <code>AppleSpell</code>, an internal macOS application that checks for spelling and grammar mistakes as you type.
With the help of <a href="https://github.com/hot3eed/xpcspy"><code>xpcspy</code></a>, I wrote a Python script that snoops on <code>AppleSpell</code> activity and streams the most probable suggestions from the predictive text model as you type in any application.</p>

<figure>
<video controls="" src="https://jackcook.com/img/blog/predictive-text/script-demo.mov" type="video/mov"></video>
<figcaption>My “predictive spy” script in action.</figcaption>
</figure>

<p>Unfortunately, I wrote this script earlier in the summer, on the first macOS Sonoma beta.
In one of the subsequent betas (I’m not sure which), Apple removed the unused completions from the XPC messages sent by AppleSpell.
I wasn’t able to glean too much about the model’s behavior from these completions, but it was still a cool find.</p>

<h2 id="where-is-the-model">Where is the model?</h2>

<p>After some more digging, I’m pretty sure I found the predictive text model in <code>/System/Library/LinguisticData/RequiredAssets_en.bundle/AssetData/en.lm/unilm.bundle</code>.
The bundle contains multiple Espresso model files that are used while typing (Espresso <a href="https://machinethink.net/blog/peek-inside-coreml/">appears to be</a> the internal name for the part of CoreML that runs inference on models).
I wasn’t ultimately able to reverse-engineer the model, but I’m fairly confident this is where the predictive text model is kept.
Here’s why:</p>

<ol>
  <li>Many of the files in <code>unilm.bundle</code> don’t exist on macOS Ventura (13.5), but they do exist on the macOS Sonoma beta (14.0). And the files that do exist in both versions have all been updated in Sonoma.</li>
  <li><code>sp.dat</code>, one of the files in <code>unilm.bundle</code>, exists on Ventura, but it’s been updated in the Sonoma beta. In the updated version of the file, I found what looks pretty clearly like a set of tokens for a tokenizer.</li>
  <li>The number of tokens in <code>sp.dat</code> matches the shape of the output layer in both <code>unilm_joint_cpu.espresso.shape</code> and <code>unilm_joint_ane.espresso.shape</code> (ANE = Apple Neural Engine), two files in <code>unilm.bundle</code> that describe the shapes of layers in an Espresso/CoreML model. This is what we would expect to see for a model that is trained to predict the next token.</li>
</ol>

<h2 id="the-predictive-text-models-tokenizer">The predictive text model’s tokenizer</h2>

<p>I found a set of 15,000 tokens in <code>unilm.bundle/sp.dat</code> that pretty clearly look like they form the vocabulary set for a large language model.
I wrote a script that you can use to see this vocabulary file for yourself, which you can check out <a href="https://github.com/jackcook/predictive-spy">on GitHub</a>.</p>

<p>The vocabulary starts with <code>&lt;pad&gt;</code>, <code>&lt;s&gt;</code>, <code>&lt;/s&gt;</code>, and <code>&lt;unk&gt;</code> tokens, which are all fairly common special tokens (<code>roberta-base</code> and <code>t5-base</code> are two popular language models):</p>

<pre><code>&gt;&gt;&gt; from transformers import AutoTokenizer</code>
<code>&gt;&gt;&gt;</code>
<code>&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("roberta-base")</code>
<code>&gt;&gt;&gt; tokenizer.convert_ids_to_tokens([0, 1, 2, 3])</code>
<code>['&lt;s&gt;', '&lt;pad&gt;', '&lt;/s&gt;', '&lt;unk&gt;']</code>
<code>&gt;&gt;&gt;</code>
<code>&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("t5-base")</code>
<code>&gt;&gt;&gt; tokenizer.convert_ids_to_tokens([0, 1, 2])</code>
<code>['&lt;pad&gt;', '&lt;/s&gt;', '&lt;unk&gt;']</code>
</pre>

<p>Next come the following sequences:</p>

<ul>
  <li>20 special tokens, named <code>UniLMCTRL0</code> through <code>UniLMCTRL19</code></li>
  <li>79 contractions (I’d, couldn’t, you’ve…)</li>
  <li>1 special <code>_U_CAP_</code> token</li>
  <li>20 special tokens, named <code>_U_PRE0_</code> through <code>_U_PRE19_</code></li>
  <li>60 special tokens, named <code>_U_NT00_</code> through <code>_U_NT59_</code></li>
  <li>100 emojis</li>
</ul>

<p>And then comes a more normal-looking list of 14,716 tokens, most of which are followed by the special character ▁ (U+9601), which is commonly used in byte-pair encoding (BPE) tokenizers, such as the GPT-2 tokenizer, to denote a space.</p>

<p>I have to say that this vocabulary file strikes me as pretty unique, but it’s definitely not out of the question for a language model deployed in this setting.
I’ve personally never seen emojis featured so prominently in a language model’s tokenizer, but <a href="https://arxiv.org/abs/2007.15779">existing</a> <a href="https://arxiv.org/abs/2303.17564">research</a> has shown that domain-specific models and tokenizers can drastically improve downstream model performance.
So it makes sense that a model trained for use in things like text messages, in which emojis and contractions will be used a lot, would prioritize them.</p>

<h2 id="model-architecture">Model architecture</h2>

<p>Based on the contents of the <code>unilm_joint_cpu</code> model from earlier, we can make some assumptions about the predictive text network.
Despite sharing the name of Microsoft’s <a href="https://www.microsoft.com/en-us/research/publication/unified-language-model-pre-training-for-natural-language-understanding-and-generation/">UniLM</a> from 2019, it looks more to me like a model based on <a href="https://openai.com/research/better-language-models">GPT-2</a>.</p>

<p>GPT-2 has four main parts: token embeddings, positional encodings, a series of 12-48 decoder blocks, and an output layer.
The network described by <code>unilm_joint_cpu</code> appears to be the same, except with only 6 decoder blocks.
Most of the layers within each decoder block have names like <code>gpt2_transformer_layer_3d</code>, which would also seem to suggest it’s based on a GPT-2 architecture.</p>

<p>From my calculations based on sizes of each layer, Apple’s predictive text model appears to have about 34 million parameters, and it has a hidden size of 512 units.
This makes it much smaller than even the smallest version of GPT-2.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Decoder Blocks</th>
      <th>Parameters</th>
      <th>Hidden Size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Apple’s predictive text model</td>
      <td>6</td>
      <td>34M</td>
      <td>512</td>
    </tr>
    <tr>
      <td>gpt2</td>
      <td>12</td>
      <td>117M</td>
      <td>768</td>
    </tr>
    <tr>
      <td>gpt2-medium</td>
      <td>24</td>
      <td>345M</td>
      <td>1024</td>
    </tr>
    <tr>
      <td>gpt2-large</td>
      <td>36</td>
      <td>762M</td>
      <td>1280</td>
    </tr>
    <tr>
      <td>gpt2-xl</td>
      <td>48</td>
      <td>1542M</td>
      <td>1600</td>
    </tr>
  </tbody>
</table>

<p>For the limited scope of the predictive text feature, this makes sense to me.
Apple wants a model that can run very quickly and very frequently, without draining much of your device’s battery.
When I was testing the predictive text feature, suggestions appeared almost instantly as I typed, making for a great user experience.
While the model’s limited size means it wouldn’t be very good at writing full sentences or paragraphs, when it exhibits very high confidence in the next word or two, they’re likely to be good enough to suggest to the user.</p>

<p>However, with my script that snoops on activity from <code>AppleSpell</code>, we can get the model to write full sentences anyway.
If I type “Today” as the first word of my sentence and take the model’s top suggestion each time, here’s what I get (<a href="https://jackcook.com/img/blog/predictive-text/longer-demo.mov">video</a>):</p>

<blockquote>
  <p>Today is the day of the day and the day of the week is going to be a good thing I have to do is get a new one for the next couple weeks and I think I have a lot of…</p>
</blockquote>

<p>Not very inspiring.
We can compare this with the output from the smallest GPT-2 model:</p>

<blockquote>
  <p>Today, the White House is continuing its efforts against Iran to help the new President, but it will also try to build new alliances with Iran to make more…</p>
</blockquote>

<p>Or the largest GPT-2 model:</p>

<blockquote>
  <p>Today, the U.S. Department of Justice has filed a lawsuit against the city of Chicago, the Chicago Police Department, and the city’s Independent Police Review Authority, alleging that the police department and the Independent Police Review Authority engaged in a pattern or practice…</p>
</blockquote>

<p>Pretty cool seeing the effects of all those extra parameters!
It’ll be interesting to see how this feature grows and evolves in the future, and whether Apple decides to keep its scope fairly narrow or someday expand its abilities.</p>

<p>If you’re interested in trying any of this out for yourself, all of my code is on <a href="https://github.com/jackcook/predictive-spy">GitHub</a>.</p>
 
  
  
  
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Breakfast cereal is in long-term decline (118 pts)]]></title>
            <link>https://www.wsj.com/finance/investing/cereal-decline-breakfast-trend-6dd591e8</link>
            <guid>37540770</guid>
            <pubDate>Sun, 17 Sep 2023 02:05:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/finance/investing/cereal-decline-breakfast-trend-6dd591e8">https://www.wsj.com/finance/investing/cereal-decline-breakfast-trend-6dd591e8</a>, See on <a href="https://news.ycombinator.com/item?id=37540770">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Copyright ©<!-- -->2023<!-- --> Dow Jones &amp; Company, Inc. All Rights Reserved</p><p>
This copy is for your personal, non-commercial use only. Distribution and use of this material are governed by
our Subscriber Agreement and by copyright law. For non-personal use or to order multiple copies, please contact
Dow Jones Reprints at 1-800-843-0008 or visit www.djreprints.com.
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pirates of the Caribbean (Metric Edition) (2017) (106 pts)]]></title>
            <link>https://www.nist.gov/blogs/taking-measure/pirates-caribbean-metric-edition</link>
            <guid>37540529</guid>
            <pubDate>Sun, 17 Sep 2023 01:19:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nist.gov/blogs/taking-measure/pirates-caribbean-metric-edition">https://www.nist.gov/blogs/taking-measure/pirates-caribbean-metric-edition</a>, See on <a href="https://news.ycombinator.com/item?id=37540529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
         
  <figure data-lightbox="https://www.nist.gov/sites/default/files/styles/2800_x_2800_limit/public/images/2017/09/19/pirate_flag_with_grave_960x600.jpg?itok=OBuwu_FF" data-media-id="386311">
        
    <img src="https://www.nist.gov/sites/default/files/styles/960_x_960_limit/public/images/2017/09/19/pirate_flag_with_grave_960x600.jpg?itok=TzKWTOPq" width="959" height="600" alt="Pirate flag with grave" loading="lazy" typeof="foaf:Image">




            <figcaption>
      <p>Grave, ahoy!</p>
                    <p><span>Credit:</span>
          
  J. Stoughton/NIST

        </p>
          </figcaption>
  </figure>

  
  
  
      <p>To save his own life, Joseph Dombey had an idea. As two pirate ships surrounded the ship he was on in the Caribbean Sea in 1794, Dombey scrambled below deck, disrobing as he went. He appropriated the outfit of one of the ship’s many Spanish sailors and prayed that he had picked up enough of their language during his trips to South America to blend in. Dombey shouldn’t have been in this position. In fact, he shouldn’t have been in the Caribbean at all. None other than Thomas Jefferson himself was expecting to meet with Dombey in Philadelphia at that very moment.</p>



 
  <figure data-lightbox="https://www.nist.gov/sites/default/files/styles/2800_x_2800_limit/public/images/2017/09/19/dombey_and_jefferson_composite.jpg?itok=-DIko8dP" data-media-id="386326"><img alt="Left: a&nbsp;bust of Joseph Dombey; Right: Thomas Jefferson, patiently waiting ..." height="582" loading="lazy" src="https://www.nist.gov/sites/default/files/images/2017/09/19/dombey_and_jefferson_composite.jpg" typeof="foaf:Image" width="935"><figcaption><p>
  Left: a&nbsp;bust of Joseph Dombey; Right: Thomas Jefferson, patiently waiting ...
</p>
                </figcaption></figure><p>Dombey’s fate that day arguably delayed the adoption of the metric system in the United States by almost a century and left us as one of the few countries in the world still using non-metric units for our everyday measurements.</p>

<h3><strong>A doomed voyage</strong></h3>

<p>The marauders now swarming Dombey’s ship were a particular breed of pirate: British <a href="https://en.wikipedia.org/wiki/Privateer">privateers</a>—the state-sponsored terrorists of the 18th century. These waterborne gangs had the tacit approval of the government in London to harass and plunder other countries’ maritime commerce and keep part of the spoils as their profit.</p>

<p>After seizing control of the ship, the pirates came across a sailor speaking Spanish with a curiously French accent—Joseph Dombey. A French physician and botanist acting under orders from the French government, Dombey had left the port city of Le Havre, France, weeks earlier for Philadelphia and the meeting with Jefferson, the United States’ first secretary of state and future president. But storms had pushed Dombey’s ship off course and deep into pirate territory.&nbsp;</p>

<p>France had supported the United States against the British in the War of Independence, and now they intended to build closer economic ties with the new American nation. Dombey was to negotiate with Jefferson for grain exports to France and to deliver two new French measurement standards: a standard of length (the meter) and a standard of mass called, rather ominously, a grave, to be considered by the U.S. for adoption. (The grave would be renamed the kilogram a year later in 1795.)</p>

<p>In many ways, Dombey was an excellent choice for this mission. Having already been on several trips to South America to collect botanical specimens, he was an experienced trans-Atlantic traveler. His knowledge of plants would also be of help in his agricultural trade negotiations with Jefferson. And Dombey’s scientific training as a physician and botanist gave him an understanding of the importance of accurate weights and measures, so it was highly likely that he would be able to convince Congress to adopt the new French standards, which would later come to be known as the metric system.</p>

<p>Despite his qualifications, Dombey lacked one important attribute: luck. His previous trips had all ended in failure. He had spent two years in Peru collecting plants that could be usefully cultivated in France, only to have the shipment captured by the British. A second collecting trip, this&nbsp;time in Chile and in collaboration with Spain, fell apart over a business dispute, with Spain keeping all the valuable specimens. But Dombey’s voyage to Philadelphia would turn out to be his most disastrous.</p>



 
  <figure data-lightbox="https://www.nist.gov/sites/default/files/styles/2800_x_2800_limit/public/images/2017/09/19/modern_day_montserrat.jpg?itok=DQqbs85L" data-media-id="386346"><img alt="Modern-day Montserrat, an idyllic beach" height="768" loading="lazy" src="https://www.nist.gov/sites/default/files/images/2017/09/19/modern_day_montserrat.jpg" typeof="foaf:Image" width="1024"><figcaption><p>
  Montserrat is a lot nicer when you're not being held for ransom by pirates.&nbsp;
</p>
                    <p><span>Credit:</span>
          
  David Stanley/CC BY 2.0

        </p>
          </figcaption></figure><p>Upon learning his true identity, the pirates imprisoned Dombey on the Caribbean island of Montserrat. Unfortunately, Dombey died before they were able to ransom him to the French, and the units of measure in his charge never made it into Jefferson’s hands.</p>

<h2><strong>A missed opportunity</strong></h2>

<p>Some historians view this event as a tragic missed opportunity whose consequences we are still living with today. When the U.S. became an independent nation, it inherited an inconsistent collection of traditional British weights and measures. Congress was aware of the flaws with its British measures, and a congressional committee was formed to recommend solutions. Thomas Jefferson, an admirer of French scientific ideas, lobbied for a measurement system similar to that of France. But Congress didn’t adopt it, and the British-influenced system took hold in the U.S. instead. However, If pirates hadn’t intercepted Dombey on his way to Philadelphia, the situation might be very different today. As historian Andro Linklater writes in his book&nbsp;<em>Measuring America</em>,</p>

<p>“The sight [in Congress] of those two copper objects [Dombey’s meter and grave], so easily copied and sent out to every state in the Union, together with the weighty scientific arguments supporting them, might well have clarified the minds of senators and representatives alike. And today the U.S. might not be the last country in the world to resist the metric system.”</p>

<p>It would take almost 100 years after Dombey’s failed mission before the United States, with the&nbsp;<a href="https://en.wikipedia.org/wiki/Mendenhall_Order">Mendenhall Order of 1893</a>, officially adopted metric standards as our fundamental standards for weights and measures. While many U.S. industries have since converted to metric, the nation's long history with British-influenced standards&nbsp;has slowed the widespread adoption of metric units in common practice.</p>



<h2><strong>The well-traveled </strong><strong>g</strong><strong>rave</strong></h2>





 
  <figure data-lightbox="https://www.nist.gov/sites/default/files/styles/2800_x_2800_limit/public/images/2017/09/19/edmund_randolph_head-and-shoulders_portrait_0_0.jpg?itok=ruwLL1Fa" data-media-id="386351"><img alt="Secretary of State Edmund Randolph" height="220" loading="lazy" src="https://www.nist.gov/sites/default/files/styles/220_x_220_limit/public/images/2017/09/19/edmund_randolph_head-and-shoulders_portrait_0_0.jpg?itok=K-KjcVND" typeof="foaf:Image" width="175"><figcaption><p>
  Secretary of State Edmund Randolph (1794-1795). Thanks&nbsp;a lot, Edmund.
</p>
                </figcaption></figure><p>Whatever became of the meter and grave Dombey had with him in 1794? Only six sets of these standards were made. And only Dombey’s set is known to have traveled to the Americas. The cargo on Dombey’s ship was eventually auctioned off. The meter and grave were purchased, and through a series of French intermediaries, the standards were turned over to the next U.S. secretary of state, Edmund Randolph. Randolph failed to understand the significance of the standards and took no action regarding them.</p>



<p>Somehow a grave ended up in the possession of <a href="https://en.wikipedia.org/wiki/Andrew_Ellicott">Andrew Ellicott</a>, a contemporary of Dombey and a well-regarded land surveyor.&nbsp;Ellicott surveyed the boundaries for what became Washington, D.C., and completed the street plan for D.C. that had originally been envisioned by <a href="https://en.wikipedia.org/wiki/Pierre_Charles_L%27Enfant">Pierre L'Enfant</a>. The grave remained in the Ellicott family until 1952 when his descendant Andrew Ellicott Douglass donated it to the NIST museum.</p>



<p>Is the grave now on display in the museum the same one captured by pirates in 1794? That remains a mystery.</p>





 
  <figure data-lightbox="https://www.nist.gov/sites/default/files/styles/2800_x_2800_limit/public/images/2017/09/19/grave_photo.jpg?itok=7ki82ZxQ" data-media-id="386381"><img alt="Grave, a brass mass standard" height="619" loading="lazy" src="https://www.nist.gov/sites/default/files/styles/960_x_960_limit/public/images/2017/09/19/grave_photo.jpg?itok=VTDorsZG" typeof="foaf:Image" width="960"><figcaption><p>
  The grave of 1793, an early version of the kilogram. It’s possible this object, now owned by the NIST museum, was once pirate treasure.
</p>
                    <p><span>Credit:</span>
          
  NIST Museum

        </p>
          </figcaption></figure>
  
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD’s Phoenix SoC (171 pts)]]></title>
            <link>https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/</link>
            <guid>37539686</guid>
            <pubDate>Sat, 16 Sep 2023 23:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/">https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/</a>, See on <a href="https://news.ycombinator.com/item?id=37539686">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>AMD’s mobile and small form factor journey has been arduous. In the early 2010s, the company’s Bulldozer-derived CPU cores stood no chance as Intel made massive gains in power efficiency. Zen narrowed the gap, but AMD still had a lot of work to do. Idle power consumption was still worse than Intel’s. The GPU side was stronger thanks to AMD’s acquisition of ATI, but AMD’s integrated GPUs were often left using horribly out of date graphics architectures. Terascale 3 remained in use on APUs well after GCN discrete GPUs hit the market. AMD launched the Ryzen 7 5800H in 2021 with Vega (improved GCN) graphics, when desktop GPUs had already transitioned to using RDNA 2.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21531" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/amd_phoenix_title_slide/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?fit=1276%2C716&amp;ssl=1" data-orig-size="1276,716" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_phoenix_title_slide" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?fit=1276%2C716&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?fit=688%2C386&amp;ssl=1" decoding="async" width="688" height="386" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?resize=688%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?w=1276&amp;ssl=1 1276w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?resize=1200%2C673&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?w=1276&amp;ssl=1 1276w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?resize=1200%2C673&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phoenix_title_slide.jpg?resize=688%2C386&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Recent AMD products have been turning the situation around. Van Gogh (Steam Deck APU) and Rembrandt finally brought RDNA 2 into integrated GPUs. Phoenix takes another step in the right direction, combining current-generation Zen 4 cores and RDNA 3 graphics into a potent package. We’d like to thank AMD for putting together an excellent Hot Chips presentation on the Phoenix SoC.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="20985" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/amd_phx_intro/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?fit=1276%2C714&amp;ssl=1" data-orig-size="1276,714" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_phx_intro" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?fit=1276%2C714&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?fit=688%2C385&amp;ssl=1" decoding="async" width="688" height="385" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?w=1276&amp;ssl=1 1276w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?resize=1200%2C671&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?w=1276&amp;ssl=1 1276w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?resize=1200%2C671&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_intro.png?resize=688%2C385&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Alongside using up-to-date architectures for the all-important CPU and GPU, Phoenix integrates a variety of accelerators to improve power efficiency in specific applications. Intel had been integrating accelerators like their GNA AI accelerator for a while, and AMD is looking to catch up. An XDNA accelerator helps with machine learning inference, and an audio controller offloads signal processing from the CPU. Importantly for a mobile SoC, Phoenix also features a capable video engine.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21006" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/amd_phx_if/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?fit=2557%2C1431&amp;ssl=1" data-orig-size="2557,1431" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_phx_if" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?fit=2557%2C1431&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?fit=688%2C385&amp;ssl=1" decoding="async" width="688" height="385" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?w=2557&amp;ssl=1 2557w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=1536%2C860&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=2048%2C1146&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=1200%2C672&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=1600%2C895&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=1320%2C739&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?w=2557&amp;ssl=1 2557w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=1536%2C860&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=2048%2C1146&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=1200%2C672&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=1600%2C895&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=1320%2C739&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?w=1376&amp;ssl=1 1376w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?resize=688%2C385&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>We’re going to complement our coverage of AMD’s Phoenix presentation with test data, since Cheese owns a laptop with the Phoenix-based Ryzen 7 7840HS. That laptop uses DDR5-5600 46-45-45-90, and runs the cores at up to 4.5 GHz. HP has thus chosen to artificially limit clock speeds, because the SKU should be boosting to 5.1 GHz.</p>
<p>Jiray has also contributed some data from the Ryzen Z1 Extreme as implemented in the ASUS ROG Ally handheld. That configuration gives us a look at Phoenix with LPDDR5. Curiously, ASUS lets the handheld reach 5.1 GHz boost clocks.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="20988" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/amd_phx_die/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?fit=1278%2C716&amp;ssl=1" data-orig-size="1278,716" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_phx_die" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?fit=1278%2C716&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?fit=688%2C385&amp;ssl=1" decoding="async" loading="lazy" width="688" height="385" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?resize=1200%2C672&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?resize=1200%2C672&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_die.jpg?resize=688%2C385&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Phoenix is manufactured on TSMC’s N4 process and occupies 178 mm2. The chip has 25.4 billion transistors, and combines Zen 4 cores with RDNA 3 graphics. Alongside the all-important CPU and GPU, AMD has added several supporting IP blocks to accelerate ML inference and signal processing. Phoenix’s die ends up being smaller than AMD’s prior Rembrandt, and fits into the same 25x35mm BGA package. </p>
<h2>CPU Side</h2>
<p>On the CPU side, Phoenix has a cluster of eight Zen 4 cores. We’re very familiar with the core architecture already, and recommend checking out <a href="https://chipsandcheese.com/2022/11/05/amds-zen-4-part-1-frontend-and-execution-engine/">our </a><a href="https://chipsandcheese.com/2022/11/08/amds-zen-4-part-2-memory-subsystem-and-conclusion/">articles</a> on that. Phoenix however has a different cache setup, with only 16 MB of L3 instead of the usual 32. AMD likely shrank the L3 slices to 2 MB per core to reduce die area usage.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="20991" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/amd_phx_zen4_core/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?fit=2558%2C1434&amp;ssl=1" data-orig-size="2558,1434" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_phx_zen4_core" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?fit=2558%2C1434&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?fit=688%2C386&amp;ssl=1" decoding="async" loading="lazy" width="688" height="386" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=688%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?w=2558&amp;ssl=1 2558w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=1536%2C861&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=2048%2C1148&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=1200%2C673&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=1600%2C897&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=1320%2C740&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?w=2558&amp;ssl=1 2558w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=1536%2C861&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=2048%2C1148&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=1200%2C673&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=1600%2C897&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=1320%2C740&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?w=1376&amp;ssl=1 1376w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_zen4_core.jpg?resize=688%2C386&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>From our testing, latency is identical to desktop Zen 4’s. Actual latency will be slightly worse because desktop Zen 4 clocks higher. For example, the Ryzen 9 7950X3D’s non-VCache chiplet sees 8.85 ns of latency at a 16 MB test size. The Ryzen 7 7840HS gets 10.92 ns of latency at the 10 MB test size. Part of that difference is because HP insists on limiting clock speed to 4.5 GHz, even though the 7840HS should be able to boost to 5.1 GHz.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_latency_large_pages.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="20993" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_latency_large_pages/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_latency_large_pages.png?fit=1021%2C503&amp;ssl=1" data-orig-size="1021,503" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_latency_large_pages" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_latency_large_pages.png?fit=1021%2C503&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_latency_large_pages.png?fit=688%2C339&amp;ssl=1" decoding="async" loading="lazy" width="688" height="339" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_latency_large_pages.png?resize=688%2C339&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_latency_large_pages.png?w=1021&amp;ssl=1 1021w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_latency_large_pages.png?resize=768%2C378&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_latency_large_pages.png?w=1021&amp;ssl=1 1021w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_latency_large_pages.png?resize=768%2C378&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_latency_large_pages.png?resize=688%2C339&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Memory latency is also higher at 103.26 ns for the 1 GB test size, compared to 87.68 ns on the 7950X3D. Memory timings play a role here, because my 7950X3D is using DDR5-5600 36-36-36-88. Laptops often have worse memory timings, and the DDR5-5600 CL46 memory used by HP is no exception.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_dram_latency.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21037" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_dram_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_dram_latency.png?fit=694%2C382&amp;ssl=1" data-orig-size="694,382" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_dram_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_dram_latency.png?fit=694%2C382&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_dram_latency.png?fit=688%2C379&amp;ssl=1" decoding="async" loading="lazy" width="688" height="379" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_dram_latency.png?resize=688%2C379&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_dram_latency.png?resize=688%2C379&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Phoenix’s memory controller supports both DDR5 and LPDDR5. The latter is useful in handhelds where power efficiency is of paramount importance. Memory latency with LPDDR5 is higher at 119.81 ns. However, AMD has dramatically improved LPDDR5 latency compared to Van Gogh, which suffered massive latency when CPU cores accessed DRAM.</p>
<h3>Bandwidth to Infinity Fabric</h3>
<p>The CPU cluster interfaces with the rest of the system through a 32 byte per cycle Infinity Fabric link. Unlike desktop designs where the write path is half as wide, the CPU-to-fabric write path can also handle 32 bytes per cycle. This is very unlikely to affect real world performance, as I have seen no workload where a single core needs more than 30 GB/s of write bandwidth. A workload with more than one thread could demand more bandwidth, but can also be split across CCX-es.</p>
<blockquote>
<p>Try writing zeroes and see what happens</p>
<cite>Paraphrased, from Mahesh Subramony on the sidelines at Hot Chips 2023</cite></blockquote>
<p>Furthermore, AMD has implemented write optimizations to reduce Infinity Fabric traffic. Write bandwidth from a single 7950X3D CCD can exceed 68 GB/s when zeroing cachelines with CLZERO. Zeroing out memory is surprisingly common, because programs will initialize memory to ensure newly allocated memory starts at a known state. Operating systems will often do so as well. If applications zero memory with recognized methods, they can see higher effective write bandwidth than a generic test may suggest.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_write_bw.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="20999" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_write_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_write_bw.png?fit=962%2C477&amp;ssl=1" data-orig-size="962,477" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_write_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_write_bw.png?fit=962%2C477&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_write_bw.png?fit=688%2C341&amp;ssl=1" decoding="async" loading="lazy" width="688" height="341" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_write_bw.png?resize=688%2C341&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_write_bw.png?w=962&amp;ssl=1 962w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_write_bw.png?resize=768%2C381&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_write_bw.png?w=962&amp;ssl=1 962w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_write_bw.png?resize=768%2C381&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_write_bw.png?resize=688%2C341&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Filling CCX-es one by one with a very contrived set of test runs</figcaption></figure></div>
<p>Stepping back, maximum write bandwidth is very close to that of desktop Zen 4. Read behavior is slightly different because the single cluster’s 32 byte per cycle interface can’t saturate the memory controller.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_read_bw.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21002" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_read_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_read_bw.png?fit=962%2C477&amp;ssl=1" data-orig-size="962,477" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_read_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_read_bw.png?fit=962%2C477&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_read_bw.png?fit=688%2C341&amp;ssl=1" decoding="async" loading="lazy" width="688" height="341" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_read_bw.png?resize=688%2C341&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_read_bw.png?w=962&amp;ssl=1 962w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_read_bw.png?resize=768%2C381&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_read_bw.png?w=962&amp;ssl=1 962w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_read_bw.png?resize=768%2C381&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_read_bw.png?resize=688%2C341&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>However, 60 GB/s is still plenty of bandwidth for eight cores. Caches should be absorbing the bulk of memory accesses in most client programs. Some applications may take all the bandwidth they can get, like scientific computing or machine learning, but mobile SoCs aren’t your best bet for that.</p>
<h3>Infinity Fabric Optimizations</h3>
<p>Infinity Fabric is AMD’s coherent interconnect. It’s critical to power efficiency because interconnect power can be a significant portion of chip power consumption, especially in workloads that aren’t pushing compute at full tilt. AMD has profiled a variety of workloads, and set up Phoenix’s Infinity Fabric to enter different operational modes based on whether a workload is compute bound, IO bound, or has very specific characteristics (like videoconferencing).</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21020" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/amd_phx_if_1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?fit=2545%2C1431&amp;ssl=1" data-orig-size="2545,1431" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_phx_if_1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?fit=2545%2C1431&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?fit=688%2C387&amp;ssl=1" decoding="async" loading="lazy" width="688" height="387" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?w=2545&amp;ssl=1 2545w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=1536%2C864&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=2048%2C1152&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=1200%2C675&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=1600%2C900&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=1320%2C742&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?w=2545&amp;ssl=1 2545w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=1536%2C864&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=2048%2C1152&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=1200%2C675&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=1600%2C900&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=1320%2C742&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?w=1376&amp;ssl=1 1376w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if_1.jpg?resize=688%2C387&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>These optimizations avoid Van Gogh’s problems, where it was limited to <a href="https://chipsandcheese.com/2023/03/05/van-gogh-amds-steam-deck-apu/">around 25 GB/s of CPU-side bandwidth</a>, thanks to an Infinity Fabric implementation heavily optimized for gaming within a tiny power budget. On Cheese’s HP laptop, Infinity Fabric clock varies depending on workload:</p>
<figure><table><tbody><tr><td></td><td>Ryzen 7840HS (Phoenix)</td><td>Ryzen 4800H (Renoir)</td></tr><tr><td>CPU Load</td><td>2 GHz</td><td>1.6 GHz</td></tr><tr><td>GPU Load</td><td>1.4 GHz</td><td>1.6 GHz</td></tr><tr><td>Mixed CPU/GPU Load</td><td>1.6 GHz</td><td>1.6 GHz</td></tr></tbody></table><figcaption>Infinity Fabric clock when running CPU/GPU bandwidth tests</figcaption></figure>
<p>My suspicion is AMD’s using low fabric clocks to improve energy efficiency when the GPU’s pulling a lot of bandwidth. The GPU has four 32B/cycle ports to fabric, letting it get enough memory bandwidth even at low fabric clock. CPU workloads are given higher fabric clocks because client programs tend to be more sensitive to latency than bandwidth, and higher fabric clock improves latency. Phoenix’s variable Infinity Fabric clocks contrast with Renoir’s, where fabric clock goes to 1.6 GHz regardless of what component is generating memory traffic.</p>
<p>To further save power, AMD aggressively pursues power and clock gating opportunities. A new Z8 sleep state allows power and clock gating for brief periods of idleness, like between keystrokes, with imperceptible wake time. Phoenix can achieve high residency in Z8 state during video playback, suggesting the media engine’s buffers and caches are large enough to let it make memory accesses in short bursts.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21042" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_power_optimizations/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?fit=1276%2C714&amp;ssl=1" data-orig-size="1276,714" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_power_optimizations" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?fit=1276%2C714&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?fit=688%2C385&amp;ssl=1" decoding="async" loading="lazy" width="688" height="385" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?w=1276&amp;ssl=1 1276w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?resize=1200%2C671&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?w=1276&amp;ssl=1 1276w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?resize=1200%2C671&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_power_optimizations.jpg?resize=688%2C385&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Additional power savings come from optimizing various physical interfaces. Memory controllers can dynamically change clock and voltage states depending on how much traffic they have to serve. A USB 2.0 interface that AMD has been using for years turned out to have a lot of power optimization opportunities, so AMD tuned that as well.</p>
<h2>GPU Side</h2>
<p>Pheonix’s GPU is based on AMD’s current-generation RDNA 3 architecture, and gets a proper “Radeon 780M” moniker rather than just being called Vega or gfx90c. It features six WGPs for a total of 768 SIMD lanes, capable of 1536 FP32 operations per cycle. These are divided into two shader arrays, which each have 256 KB of mid-level cache. A 2 MB L2 cache helps insulate the iGPU from DRAM, and is pretty important to the iGPU’s performance. Compared to <a href="https://chipsandcheese.com/2023/03/05/van-gogh-amds-steam-deck-apu/">Van Gogh</a>, Phoenix has twice as much L2 cache, slightly more memory bandwidth, and 50% more SIMD lanes. </p>
<figure><img data-lazy-fallback="1" data-attachment-id="21009" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/amd_phx_igpu/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?fit=2557%2C1437&amp;ssl=1" data-orig-size="2557,1437" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_phx_igpu" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?fit=2557%2C1437&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?fit=688%2C387&amp;ssl=1" decoding="async" loading="lazy" width="688" height="387" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?w=2557&amp;ssl=1 2557w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=1536%2C863&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=2048%2C1151&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=1200%2C674&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=1600%2C899&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=1320%2C742&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?w=2557&amp;ssl=1 2557w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=1536%2C863&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=2048%2C1151&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=1200%2C674&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=1600%2C899&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=1320%2C742&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?w=1376&amp;ssl=1 1376w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_igpu.jpg?resize=688%2C387&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>To get a sense of how disproportionately big Phoenix’s L2 cache is, AMD’s RX 7600 discrete GPU also has 2 MB of L2 cache, as does Nvidia’s old RTX 3050.</p>
<p>Latency at L0, L1, and L2 is quite similar when compared to the RX 7600, though the discrete card is slightly faster thanks to higher clock speeds.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gpu_latency.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21011" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_gpu_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gpu_latency.png?fit=1130%2C516&amp;ssl=1" data-orig-size="1130,516" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_gpu_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gpu_latency.png?fit=1130%2C516&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gpu_latency.png?fit=688%2C314&amp;ssl=1" decoding="async" loading="lazy" width="688" height="314" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gpu_latency.png?resize=688%2C314&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gpu_latency.png?w=1130&amp;ssl=1 1130w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gpu_latency.png?resize=768%2C351&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gpu_latency.png?w=1130&amp;ssl=1 1130w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gpu_latency.png?resize=768%2C351&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gpu_latency.png?resize=688%2C314&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Past L2, Phoenix’s iGPU suffers much higher latency because it lacks an Infinity Cache. (LP)DDR5 gives the iGPU a better bandwidth per SIMD operation ratio than that of desktop GPUs, so dedicating area towards another level of cache can’t be justified.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_igpu_vulkan_bw-2.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21036" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_igpu_vulkan_bw-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_igpu_vulkan_bw-2.png?fit=791%2C432&amp;ssl=1" data-orig-size="791,432" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_igpu_vulkan_bw-2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_igpu_vulkan_bw-2.png?fit=791%2C432&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_igpu_vulkan_bw-2.png?fit=688%2C376&amp;ssl=1" decoding="async" loading="lazy" width="688" height="376" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_igpu_vulkan_bw-2.png?resize=688%2C376&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_igpu_vulkan_bw-2.png?w=791&amp;ssl=1 791w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_igpu_vulkan_bw-2.png?resize=768%2C419&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_igpu_vulkan_bw-2.png?w=791&amp;ssl=1 791w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_igpu_vulkan_bw-2.png?resize=768%2C419&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_igpu_vulkan_bw-2.png?resize=688%2C376&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>DDR5’s bandwidth really can’t be understated. With DDR5-5600 or LPDDR5-5600, Phoenix can achieve higher GPU-side DRAM bandwidth than Nvidia’s GDDR5 equipped GTX 1050 3 GB from several years ago. 5.6 gigabits per pin is also faster than some early GDDR5 implementations.</p>
<p>If GPU-side memory accesses hit cache, Phoenix’s iGPU enjoys several times as much L2 bandwidth as Van Gogh.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21614" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phoenix_vk_bw-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?fit=1336%2C631&amp;ssl=1" data-orig-size="1336,631" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phoenix_vk_bw-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?fit=1336%2C631&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?fit=688%2C325&amp;ssl=1" decoding="async" loading="lazy" width="688" height="325" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?resize=688%2C325&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?w=1336&amp;ssl=1 1336w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?resize=768%2C363&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?resize=1200%2C567&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?resize=1320%2C623&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?w=1336&amp;ssl=1 1336w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?resize=768%2C363&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?resize=1200%2C567&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?resize=1320%2C623&amp;ssl=1 1320w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_vk_bw-1.png?resize=688%2C325&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Using Nemes’s Vulkan based bandwidth test because Van Gogh doesn’t have OpenCL capable drivers</figcaption></figure></div>
<p>For perspective, Nvidia’s flagship GTX 980 Ti from 2015 gets 686.9 GB/s of L2 bandwidth in the same test. Phoenix won’t have to worry about L2 bandwidth issues at all, even with its large shader array.</p>
<p>Unlike top end desktop RDNA 3 parts, the iGPU runs the frontend and shader array at the same clock. With smaller GPUs, AMD did not see much benefit in clocking the work distribution hardware faster, because the shader array is often the bottleneck. We already saw signs of this on <a href="https://chipsandcheese.com/2023/06/04/amds-rx-7600-small-rdna-3-appears/">AMD’s RX 7600</a>, and Pheonix’s iGPU has an even smaller shader array.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21016" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_gaming_perf/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?fit=2557%2C1435&amp;ssl=1" data-orig-size="2557,1435" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_gaming_perf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?fit=2557%2C1435&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?fit=688%2C386&amp;ssl=1" decoding="async" loading="lazy" width="688" height="386" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=688%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?w=2557&amp;ssl=1 2557w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=1536%2C862&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=2048%2C1149&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=1200%2C673&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=1600%2C898&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=1320%2C741&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?w=2557&amp;ssl=1 2557w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=1536%2C862&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=2048%2C1149&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=1200%2C673&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=1600%2C898&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=1320%2C741&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?w=1376&amp;ssl=1 1376w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_gaming_perf.jpg?resize=688%2C386&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Valve’s Steam Deck already demonstrated how a lot of PC games can run on a handheld. Phoenix’s iGPU is wider, faster, and enjoys more bandwidth than the Steam Deck’s. AMD didn’t provide framerate figures, instead choosing to show Phoenix’s performance advantage over Intel’s iGPU. But given the Steam Deck’s performance, I expect Phoenix to deliver playable framerates at 1080P or 720P with some settings turned down.</p>
<h3>Video Engine</h3>
<p>Even if it’s not used for gaming, RDNA 3 comes with a AV1-capable video engine, making Phoenix more future proof as next-generation video codecs start to gain traction. While the engine is not new, AMD did reveal that it uses a race-to-idle scheme to save power. It also has enough throughput to handle several simultaneous video streams, which is important for videoconferencing.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21050" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_video/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?fit=1278%2C716&amp;ssl=1" data-orig-size="1278,716" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_video" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?fit=1278%2C716&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?fit=688%2C385&amp;ssl=1" decoding="async" loading="lazy" width="688" height="385" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?resize=1200%2C672&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?resize=1200%2C672&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_video.jpg?resize=688%2C385&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<h2>XDNA AI Engine</h2>
<p>Machine learning has taken off in the past few years, and AMD has put an AI engine (XDNA) on Phoenix to accelerate inferencing. XDNA is built from AIE-ML tiles using a Xilinx developed architecture. Phoenix’s XDNA implementation has 16 AIE-ML tiles, and can be spatially partitioned to let multiple applications share the AI engine.</p>
<p>AMD didn’t go into specifics of the AIE architecture, but thankfully documentation is public.</p>

<p>XDNA aims for enough throughput to handle small ML loads while achieving very high power efficiency. It runs at very low clock and uses very wide vector execution. Documentation suggests a 1 GHz clock, but Phoenix’s XDNA might be running at 1.25 GHz as AMD says BF16 is supported with 5 TFLOPS of throughput. To drive the very wide vector units, XDNA has two vector register files. One 6 KB set of registers provides multiplier inputs. A separate 8 KB register file holds accumulator values and is accessed at a later stage in the multiply-add pipeline. Both register files can be addressed in 1024-bit, 512-bit, or 256-bit modes, with consecutive 256-bit registers forming a 512-bit or 1024-bit one.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_bf16_pipeline.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21553" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/aie-ml_bf16_pipeline/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_bf16_pipeline.png?fit=526%2C457&amp;ssl=1" data-orig-size="526,457" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="aie-ml_bf16_pipeline" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_bf16_pipeline.png?fit=526%2C457&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_bf16_pipeline.png?fit=526%2C457&amp;ssl=1" decoding="async" loading="lazy" width="526" height="457" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_bf16_pipeline.png?resize=526%2C457&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_bf16_pipeline.png?resize=526%2C457&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>6-stage pipeline for BF16 FMA. One cycle longer than the integer FMA pipeline because an extra stage is used for FP normalization. Imagine wanting to be normal.</figcaption></figure></div>
<p>Multiply-add operations seem to have 5-6 cycle latency depending on whether you need FP normalization. It’s a bit long for something running around 1 GHz, but XDNA gives you permutes as part of the deal. Simpler 512-bit adds and shuffles enjoy faster 3 cycle latency.</p>
<p>Each XDNA tile feeds its massive vector units with 64 KB of directly addressed data memory. The data memory is not a cache, saving power because no tag checks are needed. Further power savings come from using 20-bit addressing, which ought to be enough for anyone. Addresses are generated by three AGUs, which operate on 20-bit pointer and modifier registers. Using separate registers for addresses is nothing new (the 6502 had two index registers), and avoids needing six extra ports on the main scalar register file.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_vec_int_pipeline.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21559" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/aie-ml_vec_int_pipeline/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_vec_int_pipeline.png?fit=531%2C525&amp;ssl=1" data-orig-size="531,525" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="aie-ml_vec_int_pipeline" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_vec_int_pipeline.png?fit=531%2C525&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_vec_int_pipeline.png?fit=531%2C525&amp;ssl=1" decoding="async" loading="lazy" width="531" height="525" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_vec_int_pipeline.png?resize=531%2C525&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/aie-ml_vec_int_pipeline.png?resize=531%2C525&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Integer vector pipeline, showing decompression to handle sparsity</figcaption></figure></div>
<p>After addresses are generated, the data memory can handle two 256-bit loads and one 256-bit write per cycle. XDNA enjoys the same bandwidth when accessing data memory on neighboring tiles, which gives a single tile high bandwidth access to 256 KB of memory. XDNA can save significant memory bandwidth (and computational power) with support for 50% sparsity, though ML models have to be developed with sparsity in mind to see benefits. The memory subsystem can decompress sparse matrices, and mask registers let the vector units avoid pointless calculations. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_xdna.drawio.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21537" data-permalink="https://chipsandcheese.com/phoenix_xdna-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_xdna.drawio.png?fit=990%2C539&amp;ssl=1" data-orig-size="990,539" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phoenix_xdna.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_xdna.drawio.png?fit=990%2C539&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_xdna.drawio.png?fit=688%2C375&amp;ssl=1" decoding="async" loading="lazy" width="688" height="375" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_xdna.drawio.png?resize=688%2C375&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_xdna.drawio.png?w=990&amp;ssl=1 990w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_xdna.drawio.png?resize=768%2C418&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_xdna.drawio.png?w=990&amp;ssl=1 990w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_xdna.drawio.png?resize=768%2C418&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phoenix_xdna.drawio.png?resize=688%2C375&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Rough sketch of an AIE-ML tile</figcaption></figure></div>
<p>At the frontend, each XDNA tile has 16 KB of program memory for instruction storage. Again, this isn’t a cache, so power is saved by avoiding tag checks and using fewer address bits. VLIW instruction formats allow simple decode logic while letting the frontend bring in up to six instructions per cycle, as long as they’re in the right mix. A VLIW bundle with 6 instruction occupies 16 bytes, but bundles can be shorter too if not all of the slots are needed.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_aie_mesh.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21507" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/amd_aie_mesh/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_aie_mesh.png?fit=969%2C833&amp;ssl=1" data-orig-size="969,833" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_aie_mesh" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_aie_mesh.png?fit=969%2C833&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_aie_mesh.png?fit=688%2C591&amp;ssl=1" decoding="async" loading="lazy" width="688" height="591" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_aie_mesh.png?resize=688%2C591&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_aie_mesh.png?w=969&amp;ssl=1 969w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_aie_mesh.png?resize=768%2C660&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_aie_mesh.png?w=969&amp;ssl=1 969w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_aie_mesh.png?resize=768%2C660&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_aie_mesh.png?resize=688%2C591&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>AIE-ML tile interconnect layout, from AMD’s documentation</figcaption></figure></div>
<p>At the engine level, XDNA uses a specialized interconnect and memory subsystem. I already mentioned how each tile can directly access data memory on its neighbors with very high bandwidth. But that’s not it. Tiles can forward accumulator outputs to their neighbors too, with dedicated 512-bit interfaces. A general AXI4 interface allows more generic transfers between tiles, and crossing tile boundaries happens with two cycle latency. The whole engine has a shared block of L2 SRAM, which acts as staging memory. AMD didn’t disclose the L2 SRAM size, but I suspect it’s a row of four memory tiles. Each memory tile has 512 KB of capacity, so Phoenix’s XDNA engine could have 2 MB of L2 SRAM. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21019" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/amd_xdna/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?fit=2561%2C1433&amp;ssl=1" data-orig-size="2561,1433" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_xdna" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?fit=2560%2C1432&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?fit=688%2C385&amp;ssl=1" decoding="async" loading="lazy" width="688" height="385" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?w=2561&amp;ssl=1 2561w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=1536%2C859&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=2048%2C1146&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=1200%2C671&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=1600%2C895&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=1320%2C739&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?w=2561&amp;ssl=1 2561w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=1536%2C859&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=2048%2C1146&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=1200%2C671&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=1600%2C895&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=1320%2C739&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?w=1376&amp;ssl=1 1376w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_xdna.jpg?resize=688%2C385&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>System memory is accessed via DMA, which streams in data using a 32-byte per cycle port to Infinity Fabric.</p>
<p>If applications take advantage of it, XDNA should let Phoenix handle AI workloads with better power efficiency than the GPU. Technically, the RDNA 3 iGPU can achieve higher BF16 throughput with <a href="https://gpuopen.com/learn/wmma_on_rdna3/">WMMA instructions</a>. However, doing so would likely require a lot more power than the more tightly targeted XDNA architecture. Saving every bit of power is vital in a mobile device running off battery, so XDNA should let programs use AI with minimal power cost. AMD says most applications they’re targeting are friendly to INT8 operations, and XDNA should provide massive throughput there.</p>
<p>Zooming up, AMD’s Xilinx acquisition is starting to bear fruit. Previously, AMD’s smaller company size and lower budget meant they had less capability to branch out into side projects. AMD’s mobile chips had no answers for Intel’s GNA or IPU. XDNA is a step in the right direction, and gives AMD a potent accelerator for machine learning.</p>
<h2>Audio Co-Processor</h2>
<p>As you might have noticed, offloading processing to dedicated accelerators is a becoming a favored way to improve power efficiency. Phoenix goes further than offloading AI by offloading audio processing too. In fact, you could run AI noise reduction on the audio coprocessor if you can fit it within its two DSPs. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21044" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_audo_coprocessor/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?fit=1278%2C717&amp;ssl=1" data-orig-size="1278,717" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_audo_coprocessor" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?fit=1278%2C717&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?fit=688%2C386&amp;ssl=1" decoding="async" loading="lazy" width="688" height="386" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?resize=688%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?resize=1200%2C673&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?resize=1200%2C673&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_audo_coprocessor.jpg?resize=688%2C386&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>The audio processor can run at a very low clock while it’s being used as a wake source. With voice assistants playing a role across several operating systems, this could potentially save a lot of power.</p>
<p>Another cool use of low power, always-on DSPs is using ultrasound sonar to detect humans. AMD’s ultrasound runs at above 20 KHz but below 35 KHz, letting it get through the microphone and speaker’s band pass filters. Then, it can use Doppler shift to distinguish human movement from static objects in the same way that a look-down radar filters out ground clutter.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="21046" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_ultrasound/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?fit=1278%2C716&amp;ssl=1" data-orig-size="1278,716" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_ultrasound" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?fit=1278%2C716&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?fit=688%2C385&amp;ssl=1" decoding="async" loading="lazy" width="688" height="385" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?w=1278&amp;ssl=1 1278w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?resize=1200%2C672&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?w=1278&amp;ssl=1 1278w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?resize=1200%2C672&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_ultrasound.png?resize=688%2C385&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>AMD has opened up this ultrasound engine to third parties. As another note, dog hearing can cover the 20-35 KHz range, so the ultrasound engine may be able to detect static dogs by making them non-static, after which they will cause a Doppler shift.</p>
<h2>Final Words</h2>
<p>Phoenix is the latest addition to AMD’s long line of APUs (chips with integrated graphics). Ever since Picasso launched with Zen cores and Vega graphics, AMD’s APUs saw massive improvements from generation to generations. That’s largely because AMD started from so far behind. But Zen 2 and Zen 3 APUs were already very solid products, so Phoenix’s improvements make it a very dangerous competitor.</p>
<p>AMD has put a lot of focus into reducing power consumption across every area of the chip. Zen 4 cores do an excellent job on the CPU side, while RDNA 3 provides strong graphics performance. Hardware offload helps power efficiency on specialized AI and audio processing workloads. To support all this, Infinity Fabric gets lower power states and very flexible clock behavior. Phoenix ends up being able to perform well across a wide range of form factors and power targets. </p>
<figure><img data-lazy-fallback="1" data-attachment-id="21055" data-permalink="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/phx_summary/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?fit=1280%2C717&amp;ssl=1" data-orig-size="1280,717" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phx_summary" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?fit=1280%2C717&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?fit=688%2C385&amp;ssl=1" decoding="async" loading="lazy" width="688" height="385" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?w=1280&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?resize=1200%2C672&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?w=1280&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?resize=1200%2C672&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/phx_summary.jpg?resize=688%2C385&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Today, Phoenix has already surfaced in a variety of products. These include handhelds like ASUS’s ROG Ally, along with a variety of laptops and mini PCs. AMD now has a mobile CPU ready to take on Intel’s best, and Intel should be worried.</p>
<p>We’d like to thank AMD for putting together an excellent presentation at Hot Chips 2023, and hope to see their mobile efforts reach even further in the future.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img data-lazy-fallback="1" alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80" loading="lazy" decoding="async" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"> </p>

</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Satya Nadella: “I want to use this tactically vs. GOOG/AAPL” (2022) (203 pts)]]></title>
            <link>https://twitter.com/TechEmails/status/1703172745893404913</link>
            <guid>37539410</guid>
            <pubDate>Sat, 16 Sep 2023 22:27:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/TechEmails/status/1703172745893404913">https://twitter.com/TechEmails/status/1703172745893404913</a>, See on <a href="https://news.ycombinator.com/item?id=37539410">Hacker News</a></p>
Couldn't get https://twitter.com/TechEmails/status/1703172745893404913: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[America's advanced manufacturing problem and how to fix it (150 pts)]]></title>
            <link>https://americanaffairsjournal.org/2023/08/americas-advanced-manufacturing-problem-and-how-to-fix-it/</link>
            <guid>37538914</guid>
            <pubDate>Sat, 16 Sep 2023 21:32:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://americanaffairsjournal.org/2023/08/americas-advanced-manufacturing-problem-and-how-to-fix-it/">https://americanaffairsjournal.org/2023/08/americas-advanced-manufacturing-problem-and-how-to-fix-it/</a>, See on <a href="https://news.ycombinator.com/item?id=37538914">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><span>I</span>ndustrial policy is no longer taboo in the United States.<a href="#notes"><sup>1</sup></a> In the last two years, the federal government has undertaken multiple industrial‑innovation policy initiatives. The <span>chips</span> and Science Act of 2022 is designed to revitalize domestic production of semiconductors as well as to add an applied science directorate to the National Science Foundation (NSF) focused on advanced technologies. The Executive Order on Bio­tech and Biomanufacturing hopes to ensure that the next generation of medicines are manufactured in the United States. And the Infrastructure Act and the Inflation Reduction Act include massive investments in clean energy technologies.<a href="#notes"><sup>2</sup></a> These industrial and innovation strategies are underpinned by a common vision, one centered on nurturing advanced technologies. The belief is that these policies will result in the United States once again being able to manufacture critical technologies at home.</p>
<p>But can the United States still make things domestically? What if America lacks capabilities for advanced manufacturing which the most recent round of industrial policies have not fully addressed? These go beyond the lack of resilient supply chains, or the unsurprising fact that a country which has aggressively deindustrialized during the last several decades currently lacks a work­force with skills in advanced manufacturing. What if there are more profound weaknesses inhibiting advanced manufacturing in the United States, as is indicated by the trade deficit in advanced technology products? This deficit is accelerating, growing from $128 billion in 2019, to $195 billion in 2021, to $244 billion in 2022.<a href="#notes"><sup>3</sup></a> In 2021, the United States accounted for 78 percent of the total trade deficit among industrial nations, while China accounted for 45 percent of the total trade surplus.<a href="#notes"><sup>4</sup></a> Productivity in manufacturing in the United States, across most measures, has actually been declining.</p>
<p>The United States was once the global leader in manufacturing, ushering in the mass production era from the end of the nineteenth through the mid-twentieth century. It is not a global leader in the advanced manufacturing of the twenty-first century. (Advanced manu­facturing can be defined as the application of innovative technologies to improve manufacturing processes and products, adding significant value through productivity advances and innovation. These would include digital technologies, robotics, 3-D printing, advanced materials, bio-fabrication, artificial intelligence, and nanofabrication.<a href="#notes"><sup>5</sup></a>)</p>
<p>The United States does not currently have the correct institutional infrastructure and accompanying operational mechanisms to support ad­vanced manufacturing. Industry, government, and academia are largely unlinked when it comes to advanced production technology and processes, and there is a similar lack of interagency coordination within the government. Pathways necessary for diffusing new technologies and getting them to market are missing, including a lack of scale-up financing mechanisms. The vocational education system has withered as has the corporate lab system. The Department of Defense’s (DoD) mission has traditionally been one of military security rather than economic security and assuring a strong American industrial base. Yet economic security and military security are now inseparable, and by failing to pursue innovation in production, the DoD is putting U.S. economic and therefore national security at risk. Financial markets do not reward advanced manufacturing. They favor outsourcing and the disaggregation of integrated firms. Corporations are not rewarded for pursuing production as opposed to, say, stock buybacks. What is sometimes called the U.S. developmental state has many <span>strengths—in</span> basic research as well as applications in the areas of defense technology, software, and biopharma <span>development—but</span> advanced manufacturing is not one of them.</p>
<p>Robert J. Gordon, in his widely read book <em>The Rise and Fall of American Growth</em>, argues that recent generations of technologies are inherently less conducive to job creation, compared to earlier breakthroughs, leading to lower growth.<a href="#notes"><sup>6</sup></a> But the real culprit may be the way in which our innovation system was designed, leaving a manufacturing focus out of the innovation equation. The result of all this has been the decline of U.S. manufacturing and the corresponding weakening of the American working class, growing economic inequality, and protracted political confrontation.</p>
<p>How did we arrive at this state of affairs, and how can we fix it? The design of the existing system has been informed by many factors: which sectors in the United States are seen as prestigious and which are not; false beliefs about the strengths of U.S. manufacturing; beliefs that a postindustrial future is both inevitable and desirable; policy decisions that promoted diffusing technologies globally and not domestic indus­trial strength; the DoD’s assumption that it would always have a robust industrial base to rely upon; hubris and insularity dating from the period when the United States was the unchallenged leader in mass manufacturing; financial market pressures; and the ideological and methodological limitations of mainstream economics.</p>
<p>A pervasive source of weakness is the disconnect between the U.S. production system and the U.S. innovation system. Unlike Germany, Japan, Korea, Taiwan, and now China, the United States failed to put manufacturing at the center of its innovation system. In fact, it didn’t even consider manufacturing as part of innovation. When the United States was assembling its innovation system after World War II, it assumed its lead in manufacturing would be eternal. It built its innovation system in the area it needed to strengthen: R&amp;D. Thus America’s innovation system is in many ways a historical artifact, maladapted to the global economic competition of today.</p>
<p>The challenges America faces in adopting advanced manufacturing practices are, however, surmountable. Overcoming these challenges would allow current and future industrial-innovation policies to reach their full potential and revive domestic manufacturing. Yet these problems are not amenable to an updating of a single process or a management-consultant fix. Solving them will require a major system redesign, relinking innovation to production, building on efforts already underway as well as new initiatives.</p>
<h3><span><strong>America as the Leader in Mass Production</strong></span></h3>
<p>In the 1930s, Detroit was the technological and commercial leader of the world’s auto industry. Detroit used mass production techniques con­sisting of assembly lines and standardized products to produce an ever-increasing number of cars at reduced cost. Detroit had nearly double the wages of other industrial centers like Chicago. At the time, nearly five out of six of the world’s cars and trucks were manufactured in the USA.</p>
<p>The most colossal and advanced factory in the world’s most advanced center for car manufacturing was Ford’s River Rouge plant. Here, all production used the assembly line method with conveyor belts bringing together different components of the car in synchronized harmony. Mass production technologies and the division of labor created a workforce consisting of relatively unskilled employees on the line combined with a smaller group of highly skilled workers and the engineering teams that designed it. Visitors from around the world came to view this modern technological marvel, including delegations from Nazi Germany and Soviet Russia. Stefan Link, in his book <em>Forging Global Fordism</em>, writes, “Ford’s new factory became the destination of engineering delegations. Italian, German, Russian, and Japanese special­ists traveled to Detroit and spent weeks, months, even years at River Rouge to learn the American secret of mass production.”<a href="#notes"><sup>7</sup></a></p>
<p>Mass production was furthermore a dual-use technology. Industrialist Henry Kaiser, who had no background in shipbuilding (he built the Hoover dam and the Oakland–San Francisco bridge) applied mass pro­duction techniques to shipbuilding in World War II. Kaiser shipyards could produce Liberty Ships in a matter of weeks and sometimes even days as opposed to the two-thirds of a year it took to build a comparable ship in the past. The mass production method dramatically raised manufacturing productivity. The United States produced so much war materiel so quickly that it had to scrap more planes after World War II than Nazi Germany was able to produce during the last four years of the war.<a href="#notes"><sup>8</sup></a></p>
<p>Mass production itself has military origins. The system of interchangeable machine-made parts was implemented in the early 1800s at the federal government’s Harper’s Ferry and Springfield, Massachusetts, arsenals, for the production of firearms. These production techniques then spread to the civilian sector. This precursor to mass production, which used specialized machines for the production of interchangeable parts, was called the American system of manufacturing or sometimes, more specifically, “armory practice.”<a href="#notes"><sup>9</sup></a> Ford added the moving assembly lines which marked the starting point of true mass production.</p>
<h3><span><strong>Postwar Innovation Policy and Its Assumptions about U.S. Manufacturing</strong></span></h3>
<p>The U.S. victory in World War II was supported by rapid advances in science and technology and the scaling of mass production, with central planning combining the two, and the military promoting improved production techniques for private firms. Before the war was even over, President Roosevelt considered the implications of this organized approach to scientific development for peacetime. On November 17, 1944, he wrote a letter to Vannevar Bush, his science adviser and the director of the Office of Scientific Research and Development, stating that Bush’s office “represents a unique experiment coordinating scientific research and in applying scientific knowledge to the solution of the technical problems paramount in war. There is, however, no reason why the lessons to be found in this experiment cannot be profitably employed in times of peace.”<a href="#notes"><sup>10</sup></a> Roosevelt asked for Bush’s recommendations, adding, “New frontiers of the mind are before us.”</p>
<p>Bush’s report, written in response, <em>Science—the Endless Frontier</em>, is widely credited as the basis for postwar U.S. science and innovation policy.<a href="#notes"><sup>11</sup></a> It conceived of innovation as a linear pipeline. The federal government would fund basic research and the development of scientific talent. The knowledge would be available to industry which, it was assumed, would develop it further, moving it along the pipeline until it was ready for mass production, eventually reaching the marketplace. The report called for a new central science agency to oversee the research funding.</p>
<p>There were other influences on postwar innovation policy, however, which would prove to be as important as Bush’s report. John Steelman, assistant to President Truman (today’s equivalent of a chief of staff) proposed instead a more decentralized model of research no longer overseen by a central agency, which is how federal research would come to be organized, overturning Bush. Nevertheless, Bush’s emphasis on basic research remained the focus of the U.S. innovation system, at the NSF, other government agencies, and U.S. universities, as did his linear innovation model.<a href="#notes"><sup>12</sup></a></p>
<p>The Department of Defense was a holdout from the Bush model in that it continued to support prototyping, testing, demonstrating, and implementing technologies in addition to basic research. Particularly after Sputnik, the DoD pursued the more integrated wartime approach, and was also the customer for the new technologies it developed. In contrast, the civilian R&amp;D agencies and universities supported research only through early-stage development. The civilian sector therefore fol­lowed Bush’s basic research architecture as well as Steelman’s decentralized one. The result, effectively, was two innovation systems in the United States, an integrated defense one, and a disjointed civilian one.<a href="#notes"><sup>13</sup></a></p>
<p>Missing from <em>both </em>new R&amp;D-led innovation systems was any focus on manufacturing or the understanding that manufacturing-led innova­tions could occur along the pipeline rather than just as an afterthought.<a href="#notes"><sup>14</sup></a> The <em>Endless Frontier</em> report doesn’t contemplate that new technologies could transform manufacturing processes <span>themselves—the</span> word manufacturing only occurs once. The Pentagon likewise didn’t pay that much attention to broad manufacturing capabilities in the United States, nor did it need to, given the country’s recent performance during the war.</p>
<p>Instead, the U.S. postwar approach to innovation rested on the assumption that the United States <span>was—and</span> would <span>remain—the</span> pre­eminent manufacturing power, deploying the most advanced production processes from a position of unchallenged supremacy. After all, the industrial bases of the Axis powers had just been destroyed. The foreign policy establishment, assuming U.S. industrial strength would be endur­ing, made trade deals aimed at building up allies at the expense of domestic manufacturing, a position that has only recently changed.<a href="#notes"><sup>15</sup></a></p>
<p>The private sector similarly took America’s supremacy in manufacturing for granted. The corporate lab system of AT&amp;T, GE, GM, Ford, Xerox, and IBM, much like the government lab system, focused on technology research, not incremental advances or breakthroughs in manufacturing production. Henry Kressel, who led the central research lab at RCA, explains why: “The returns are deemed faster and higher in focusing on product development and marketing. And there was a lack of trained <span>people—manufacturing</span> carries a stigma for college trained elites. Regarding the old labs, they rarely focused on production <span>technology—with</span> the exception of Corning.”<a href="#notes"><sup>16</sup></a> The corporate labs declined with the pressures of global competition, with most cutting back their role on potential breakthroughs in favor of safer, shorter-term incremental advances. There has been no replacement for the old lab system when they phased down. And manufacturing technology was never an empha­sis.</p>
<p>Overall, the U.S. federal innovation system of government labs and government-funded academic research was disconnected from manufacturing. On the nondefense side, it emphasized basic rather than applied research. The focus was on scientific breakthroughs, and not on manu­facturing technology. The government did not coordinate the innovative activities it funded across market actors. There were no features spe­cifically designed to move a new technology along the commercial pipeline.</p>
<p>The United States further lacked collaborative government, academic, and industry institutions centered on improving manufacturing tech­niques and sharing related new technologies. The system was therefore very unlike that of agricultural colleges, cited in <em>Endless Frontier</em> as a model, through which government-supported research is transmitted into practice. (The United States remains highly competitive in agriculture to this day.<a href="#notes"><sup>17</sup></a>)</p>
<p>The government planners and businessmen of the postwar era may have presumed that America’s leadership in the prior manufacturing innovation <span>wave—mass</span> <span>production—would</span> be repeated in subsequent waves, but this was not to be. The disconnect in the United States between innovation and production would lead to profound weaknesses. America often missed out on newer production approaches starting with quality manufacturing. As the mass production era was succeeded by new waves of manufacturing advances, America would no longer be king of manufacturing.</p>
<h3><span><strong>The Japan Shock </strong></span></h3>
<p>Ford was building cars in Yokohama, Japan, as early as the 1920s. By the 1930s, Ford, together with GM, completely dominated the Japanese auto market. The Japanese government, however, sought to change this, and pursued interventionist policies to foster a locally owned industry. In 1936, Japan enacted the Automobile Manufacturing Enterprise Law favoring domestic carmakers. The American firms were forced to leave Japan. Toyota and Nissan took over their manufacturing equipment.<a href="#notes"><sup>18</sup></a></p>
<p>In the 1970s and 1980s, Japan’s automakers, with support of the Ministry of International Trade and Industry (MITI), pioneered the “quality production paradigm.” Japanese producers pursued precision production, just-in-time production, quality built into all production stages (not just at the end), continuous improvements by the entire workforce, and collaborative labor practices, all of which led to dramatically increased efficiencies and quality. MITI funded industry rather than basic research to support this new approach. It was Japan’s turn to lead technological innovation in car manufacturing, with Detroit a follower.</p>
<p>Currency manipulation by the Japanese government further aided its exports, and the oil shock of 1973 was helpful to Japanese automakers given that they specialized in smaller, more efficient cars while the United States produced gas-guzzlers. Japan’s exports to the United States soared. U.S. automakers, in contrast, took too long to understand and then adjust their manufacturing technologies and techniques to the quality revolution. Rather than making the necessary changes or investments, the standard reaction of Detroit was to offshore lower-end models to lower-wage countries. Japan, bolstered by the quality revolu­tion and the modernization of its industrial production process, and taking advantage of open U.S. markets while keeping its own closed, leapt ahead in economic competitiveness with the United States in consumer electronics as well as autos.</p>
<p>The U.S. government eventually countered in the 1980s with industrial policies of its own: the Bayh-Dole Act (which encouraged universities to commercialize federally funded research); the Manufacturing Extension Partnership (modeled on the U.S. agriculture extension program to bring advances to small manufacturers); the SBIR program (R&amp;D grant funding to small and start-up companies); <span>sematech</span> (the consortium to restore U.S. semiconductor technology leadership though improved production efficiency and quality); and the R&amp;D tax credit (which benefited firms that increased their R&amp;D investments). Finally, a joint currency intervention between the United States and its allies, the Plaza Accord, resulted in the depreciation of the dollar.</p>
<p>These industrial policies were modestly funded and piecemeal in comparison to Japan’s efforts. S<span>ematech</span>, for instance, only targeted one sector, semiconductors. Generally, these programs targeted early stages of innovation, rather than actual production technologies. The quality revolution in manufacturing of the 1970s and ’80s, pioneered by Japan, largely passed America <span>by—it</span> was only adopted slowly. The country faced a widespread economic malaise and the emergence of the “Rust Belt” in the Midwest.<a href="#notes"><sup>19</sup></a> The manufacturing competition with Japan was a harbinger of the far graver China shock to come.</p>
<h3><span><strong>The Future Will Be Postindustrial </strong></span></h3>
<p>There are reasons why the United States was slow to adopt the quality revolution in manufacturing and the production innovation waves which followed. The handful of industrial policies launched by the U.S. government in response to Japan’s industrial successes were overwhelmed by larger trends, including beliefs, incentives, and policies pushing in the opposite direction, against reviving domestic manufacturing. Simply put, manufacturing was no longer a prestigious sector in the United States.</p>
<p>America’s success in the IT revolution in the 1990s completely diverted U.S. attention from following through on the piecemeal industrial policy reforms of the 1980s. By this point, the United States was moving too slowly, if at all, on new production techniques, and adopted policies that often hastened deindustrialization altogether.</p>
<p>The widespread <span>belief—and</span> <span>prophecy—that</span> the most advanced economy was a postindustrial one made concerns about America’s manufacturing malaise seem irrelevant. Daniel Bell’s influential 1973 book <em>The Coming of Post-Industrial Society: A Venture in Social Forecasting</em> is illustrative of American intellectuals’ thinking about manufacturing during this era.<a href="#notes"><sup>20</sup></a> The book took the transition from a goods to services economy as a given. Indeed, it is more of a meditation about the features of this transformed society.</p>
<p>As early as the 1960s, Bell argued, “the leadership of the new society will rest, not with businessmen or corporations as we know them, for a good deal of production will have been routinized . . . but rather with intellectual institutions. Social prestige and social status will be rooted in the intellectual and scientific communities.”<a href="#notes"><sup>21</sup></a> It is quite clear which sectors and activities Bell valued. The idea that production could move beyond the routine is not considered. Instead, manufacturing seems to almost vanish entirely in his conception of the future; it is a relic.</p>
<p>Bell also claimed, “what has now become decisive for society is the new centrality of theoretical knowledge, the primacy of theory over empiricism.”<a href="#notes"><sup>22</sup></a> This statement contains a great deal of self-insight, about Bell, but also American intellectuals and social scientists. Bell’s theory-driven approach lacked any empirical understanding of what was going on in American manufacturing in the 1970s. The routinized production in the United States he refers to was being globally displaced by the newer type of manufacturing of Japan.</p>
<p>Bell’s book and others like it, such as those from futurist Herman Kahn, were prescient about the future of the United <span>States—but</span> not about the future of East Asian countries. These nations did not embrace the postindustrial paradigm theorized by American intellectuals and academics. More damagingly, the prophecies about a postindustrial American future might have been self-fulfilling.</p>
<h3><span><strong>Workforce Education</strong></span></h3>
<p>The state of American vocational education is emblematic of the low prestige associated with production. The U.S. vocational education system of the 1950s, ’60s, and ’70s was not quality education. It shunted off too many students into what became a dead-end track. It was representative of the low status in the United States of factory jobs or jobs which did not require a college degree.</p>
<p>The college-for-all movement, which arose in response, was a well-intentioned reform which created its own problems. It largely dismantled vocational education. It did not, however, replace it with a better system. Instead, academic courses crowded out vocational courses, but most students who were previously on the vocational track did not end up going to college.</p>
<p>Some states and regions maintained and improved their vocational schools. Today, the United States has a small number of effective vocational-ed high schools, often with long waiting lists for admissions. These typically put students into programs that combine work and learning. But these remaining programs are the exceptions.</p>
<p>Instead, the education system for America’s technical workforce is largely broken.<a href="#notes"><sup>23</sup></a> Yet without a workforce trained in the skills required for new technologies, those technologies cannot be implemented. The overall problems include: a history (until recently) of disinvestment by government and employers in workforce education; Labor Department training programs that do not reach either higher technical skills or incumbent workers that need to upskill; Education Department programs that are focused on college not workforce needs and not linked to the Labor Department programs; underfunded community colleges that lack the resources to provide advanced training in new fields and have too-low completion rates; colleges and universities that are disconnected from workforce education; no system for lifelong learning; underfunded advanced technical education programs (at the NSF and at advanced manufacturing institutes); and a broken labor market information system. These problems are compounded by the reality that the existing actors are in a “legacy” <span>sector—the</span> long-established education sector is hard to change.</p>
<p>There is a burgeoning literature on innovation ecosystems as essential to technology-based growth. However, the role of workforce education as a factor in innovation ecosystems has largely been missed by these analysts.<a href="#notes"><sup>24</sup></a> It is time to put it into the equation. Overall, the United States has never created a widespread or effective work-learn model supporting skilled manufacturing jobs, akin to the vocational and apprenticeship programs of other industrial countries like Germany or Japan. But the issues go beyond just the collapse of vocational education in the United States. In addition, the American higher educational sys­tem is largely disconnected from manufacturing technology and <span>pro­cesses—exactly</span> as Vannevar Bush designed.</p>
<h3><span><strong>Economists’ Perspectives</strong></span></h3>
<p>In the 1970s, when American heavy industry was decimated by the combination of the oil shock and the rise of East Asian exporters, neoclassical economists could offer little advice about how to stem the decline. The Keynesian demand regime which had successfully managed the U.S. economic cycle over the previous thirty years now only pro­duced stagflation and was no longer tenable. The neoliberal approach which replaced it, with its emphasis on privatization, deregulation, monetarism, and free trade, also was unable to revive <span>manufacturing—and</span> was not particularly interested in doing so. These inadequacies stem from both the methodological limitations and ideological biases of mainstream economics.</p>
<p>Macroeconomic theory has been unable to adequately model innova­tion, nor does it deeply grapple with innovation systems. Nevertheless, there have been some attempts to include innovation in growth models: Robert Solow’s models identify growth as coming predominantly from technological progress in addition to labor and capital, but this is treated as an exogenous input, a black box with the model offering no further insight. Other models try to endogenize innovation, incorporating learning by doing.<a href="#notes"><sup>25</sup></a> But the details of America’s innovation system and its weaknesses are essentially alien to macroeconomics and not part of this field.</p>
<p>In addition to these methodological flaws, ideological preferences creep into modeling. A standard assumption is sector agnosticism. Manufacturing is treated just like any other sector, despite the evidence of the importance of manufacturing innovation to long-term growth. Although manufacturing is only 11 percent of U.S. GDP and 8 percent of direct employment, it drives 20 percent of capital investment, 30 percent of productivity growth, 60 percent of exports, and 70 percent of business R&amp;D.<a href="#notes"><sup>26</sup></a> Yet economists have often held manufacturing in disdain, with deindustrialization a sign of progress akin to sociologists’ prophecies. Alan Blinder, former member of Clinton’s Council of Eco­nomic Advisers and vice-chair of the Fed, wrote in 2005, “The shift to services is still viewed with alarm in America and many other rich countries, where people bemoan rather than welcome the resulting losses of manufacturing jobs.”<a href="#notes"><sup>27</sup></a></p>
<p>There are parallel beliefs, misconceptions, and dogmas in trade theory, though its recommendations were largely ignored until the neoliberal era. The United States was historically a high tariff nation, only reducing tariffs once it reached economic supremacy after World War II. It turned a blind eye to the mercantilist trade practices of South Korea, Germany, and Japan (until the imbalances with Japan became too outsized and the Plaza Accord was enacted) because these were allied nations during the Cold War. The United States offered these countries access to its markets for geopolitical reasons. The result too often was one-sided free trade.</p>
<p>Then, during the neoliberal era, ideology suffused into policy. The United States extended the same courtesy of domestic market access to China, not an ally, that it had previously offered Germany and Japan. It did so for many reasons: the desire to change China; hubris after winning the Cold War; pressure from U.S. corporations; but also because standard trade models showed this to be net-welfare improving for the United States. N. Gregory Mankiw, chairman of George W. Bush’s Council of Economic Advisers, in his 2004 <em>Economic Report to the President</em>, stated “when a good or service is produced more cheaply abroad, it makes more sense to import it than make or provide it domestically.”<a href="#notes"><sup>28</sup></a> Mankiw told reporters, “offshoring is the latest mani­festation of the gains from trade that economists have talked about for centuries.”<a href="#notes"><sup>29</sup></a></p>
<p>Since then, there have been changes in trade policy attempting to reverse this neoliberal approach. Theory, however, has generally lagged policy with some exceptions. Economists’ apparent disdain for manufacturing is still a constant. Mankiw’s indifference to America’s manufacturing sector remains representative of economists’ thinking. Although manufacturing jobs result in significantly higher net employment than services jobs when indirect jobs are included, the report treats manufacturing as just another sector. It is oblivious to the underlying weaknesses in U.S. manufacturing and its many causes, which are not addressed. The 2004 <em>Economic Report</em>’s advice: “The best policy response to recent developments in manufacturing is to focus on stimulating the overall economy.”</p>
<h3><span><strong>Mis-Incentives and Misconceptions </strong></span></h3>
<p>Though economics is conspicuous for its lack of insights about what exactly is wrong with American manufacturing, this doesn’t mean economic incentives are not key. Financialization and globalization combined to reduce incentives for manufacturers to adopt advanced production techniques in the United States.</p>
<p>American financial markets favor a capital-light production model, or one of no production at all. Jack Welch was well aware of this in his transformation of GE from an engineering <span>company—one</span> cofounded by Thomas <span>Edison—to</span> a financial engineering company. In 1985, during his early years as CEO, he canceled the company’s “factory of the future” initiative consisting of automation systems, robotics, and ad­vanced machine tools, which would have allowed GE to catch up with Siemens and Japanese competitors in its offerings.<a href="#notes"><sup>30</sup></a> Instead, that same year he acquired RCA, parent company of the television broadcaster NBC. Eventually, 60 percent of company profits came from GE capital. Welch’s strategy worked, at least for a while: in 1993, GE became the world’s most valuable company, before the stock price collapsed during the great financial crisis and GE was broken up.</p>
<p>The assessment of author David Gelles, in his critical biography of Welch, <em>The Man Who Broke Capitalism</em>, is that “instead of trying to fix American manufacturing, he effectively abandoned it, and would soon start shuttering factories around the country and shipping jobs overseas.”<a href="#notes"><sup>31</sup></a> Welch once stated, “Ideally, you’d have every plant you own on a barge to move with currencies and changes in the economy.”</p>
<p>This dream might have been unthinkable in a previous age, when American elites had more of a national commitment. But it would have been unfeasible without the disaggregation of production and the rise of the internet conveying information. Political economist Suzanne Berger has explained that IT-based precise specifications and designs enabled manufacturing, which previously had to be vertically integrated, to be distributed across many sites. Manufacturing could take place in coun­tries which subsidized it through direct grants, cheap electricity,<a href="#notes"><sup>32</sup></a> and manipulated currencies. None of these factors favored investing in technological improvements in U.S. production. Ultimately, the IT revolution made the whole manufacturing issue moot in the United States. The future was the Information Age, and platform companies which manufactured nothing. Manufacturing didn’t matter. It was a legacy sector at best. Daniel Bell’s prophecy about a postindustrial future seemed true.</p>
<p>Paradoxically, buried in this disdain for manufacturing were long-held assumptions about U.S. strengths, updated with beliefs that IT was transforming American production. The unprecedented decline in U.S. manufacturing employment, falling by 28 percent between 2000 and 2016, was often interpreted as resulting from automation rather than trade with <span>China—what</span> would later be called the China <span>shock—and</span> other Asian nations. This faulty analysis, which still remains widespread, rests on the mistaken belief that U.S. manufacturing output remained solid. In fact, statistical anomalies involving how growth in computer processing capabilities is credited to growth in manufacturing output gave a misleadingly positive picture of the overall state of U.S. manufacturing.<a href="#notes"><sup>33</sup></a> There is no need to turn to explanations about automation to explain the seeming disconnect between manufacturing employment and output, because there is no actual discrepancy: both declined from the China shock.</p>
<p>In fact, the United States has been slow in adopting factory automation compared to leading competitor nations. All evidence points toward weakness. According to the International Federation of Robotics, the world’s top five most automated countries in manufacturing in 2021 were South Korea, Singapore, Japan, Germany, and China. The United States ranks a distant ninth, just slightly ahead of Slovenia in robot density per worker, a key indicator of automation.<a href="#notes"><sup>34</sup></a> The United States is not close to being as dominant in advanced manufacturing as it was in mass manufacturing during the first half of the twentieth century.</p>
<p>Nonetheless, even today, there is a burgeoning genre of white papers from management consultants which insists that U.S. manufacturing is undergoing a technological renaissance. Often published as “native advertising” (sponsored content) in the financial press, they discuss the Third Industrial <span>Revolution—using</span> IT to automate <span>production—and</span> now the Fourth (4IR), which involves biological processes, blockchain, virtual reality, and more. The most prominent of these white papers are published in partnership with the World Economic Forum,<a href="#notes"><sup>35</sup></a> and the recommendations are distinctly neoliberal, meaning they are CEO-led rather than government led. There is no analysis of the design flaws of the overall U.S. innovation system, including its delinking from produc­tion, nor of the successes of the Chinese system, including the heavy use of subsidies. Instead, resilience, sustainability, and inclusivity are the key words of this corporate literature. They can appear in any order. Words that do not make an appearance include industrial policy, subsidies, mercantilism, and the CCP. Reading about this imminent Fourth Industrial Revolution, one would never imagine that during the Covid pandemic the United States struggled to produce the most basic PPE gear and doctors resorted to wearing plastic bags.</p>
<h3><span><strong>Case Study: Clean Energy</strong></span></h3>
<p>Often for the United States, the choice hasn’t been between mass production and advanced production, but whether to pursue any domestic production at all. The federal government continues to fund basic research as Vannevar Bush planned, but the production chain beyond that is broken. Most <span>manufacturing—and</span> products developed based on innovations in manufacturing <span>processes—takes</span> place outside the United States.</p>
<p>The clean energy sector is just one example of many. Despite the initial invention of key technologies, the United States has ceded manufacturing leadership of one energy sector after another, from solar to offshore wind to batteries to nuclear power. Jonas Nahm in his book <em>Collaborative Advantage</em><a href="#notes"><sup>36</sup></a> analyzes these differences, comparing the United States, Germany, and China’s experiences in developing clean-energy industries.<a href="#notes"><sup>37</sup></a></p>
<p>“In the United States firms have typically taken the form of start-ups with skills in the invention of new technologies, but with far fewer capabilities in the commercialization and production of these inventions,” Nahm writes. In contrast, he found Germany’s highly skilled small manufacturers had prowess in the customization of small batch production of <em>equipment </em>and components. China pursued what Nahm terms “innovative manufacturing”: Chinese manufacturers “focused on the R&amp;D required for scaling and commercializing novel technologies. Far fewer firms prioritized the production of manufacturing equipment or the invention of new technologies.”</p>
<p>As a result of China’s strength in innovative manufacturing processes and scale-up, supported by its financial system, demand policies, and workforce <span>training—as</span> well as massive production subsidies, local content requirements, and forced technology <span>transfer—only</span> China cur­rently manufactures new energy technologies like solar and increasingly batteries and soon EVs at worldwide scale. Although Nahm sees the new global system as a positive because it presents new opportunities for firms and countries to specialize and collaborate, the United States misses out on the large potential base of employment from, and control of, the technologies it invented.</p>
<h3><span><strong>The Innovation System <span>Revisited—by</span> China</strong></span></h3>
<p>Today, it is China which is the pacesetter for auto manufacturing, not Germany or Japan, and certainly not Detroit. China dominates the production of full electric vehicles and, in 2023, surpassed Japan as the world’s largest auto exporter. As soon as 2025, Europe could become a net importer of cars, given the rapid growth of electric vehicles made in China. Some of these are Chinese brands but others are European (and American) brands that have shifted their EV manufacturing to China. China leads in battery production and offers well-developed supply chains, a vast domestic market, and technological expertise.</p>
<p>The <em>Wall Street Journal</em> observed, “Thanks to competition and [China’s] focus on execution, the EV industry went from a niche industrial policy project to a sprawling ecosystem of predominantly private companies. Much of this happened below the Western radar.”<a href="#notes"><sup>38</sup></a> The <em>Journal</em> also referenced a statement from a Chinese entrepreneur who said, “Ideas are not important in <span>China—execution</span> is.” This attention to execution in production is very different from the basic research ambitions and goals of America’s innovation system. But China’s seemingly overnight success in electric vehicles derives from something else, too: the industrial and frequently outright mercantilist policies<a href="#notes"><sup>39</sup></a> it has successfully used in industry after industry. These include market access and consumer financing contingent upon local production, forced technology transfer, exclusion of foreign battery producers, and massive subsidies through government guidance funds.<a href="#notes"><sup>40</sup></a></p>
<p>China’s emphasis on manufacturing is the exact opposite of the postindustrial discourse which has been dominant in the United States from World War II until very recently. “Industry is the main engine of economic growth. Industry is the main battlefield of technological innovation,” writes Party Secretary Jin Zhuanglong, head of the Minis­try of Industry and Information Technology. His article, “Accelerate the Promotion of New Industrialization,” published in an official CCP theory journal, further stated, “keep the proportion of manufacturing industry to GDP basically stable, and prevent the economy from ‘moving away from the real to the virtual.’ Promoting new industrialization is urgent to build a large country’s competitive advantage. It is the foundation for winning the initiative in international economic competi­tion.”</p>
<p>The United States, however, is no longer sitting still. It has put in place significant industrial policies under the Biden administration, some of which passed with bipartisan support in 2021 and 2022. China is not sitting still, either.<a href="#notes"><sup>41</sup></a> Its industrial policies include the equivalent of up to $400 billion in annual scale-up financing for growing <span>industries—the</span> United States has no comparable program.<a href="#notes"><sup>42</sup></a> It is rapidly reforming its own innovation system, including addressing its needs in basic research. This is the last missing link in its “innovation chain,” one required for Chinese technological self-sufficiency. Funding for basic research increased 24 percent in 2021 compared to a year earlier.</p>
<p>“China’s policy makers are seeking to systematically address and integrate every step of the innovation process,” notes a 2023 report by the German think tank <span>merics</span>.<a href="#notes"><sup>43 </sup></a>“These include launching large projects, promoting industry-university-research synergy, and integrating the innovation, industrial, capital, and talent chains.”</p>
<p>Basic as well as applied scientific research is directed towards national strategic goals such as technological self-sufficiency according to the <span>merics</span> report. “China’s government has brought the work of State Key Laboratories closer to commercialization by embedding labs in compa­nies including Huawei and ZTE.” With government backing, China is reproducing in its way the vanished U.S. corporate lab system. Even basic research conducted at universities shares these strategic and commercial imperatives. Under the National Key R&amp;D Projects pro­gram, which provides research funding, “there is no strict division between funding for basic research and support for developing practical applications,” according to the <span>merics</span> report. China is now second in the world in R&amp;D spending and on a path to pass the United States.<a href="#notes"><sup>44</sup></a> Unlike in the United States, in China there are combined thrusts for invention, innovation, and production, integrated with trade policy.</p>
<h3><span><strong>Implementing Advanced Manufacturing Policies </strong></span></h3>
<p>As of 2021, China’s manufacturing sector accounted for a 28 percent share of the value add of its GDP. In the United States, manufacturing makes up only 11 percent of the value add of its GDP. Roughly 40 to 45 percent of China’s manufacturing output is directed to exports.<a href="#notes"><sup>45</sup></a></p>
<p>Furthermore, the United States ran a trade deficit in advanced technology goods, largely with Asian nations (predominantly China), as noted above, that grew to $244 billion in 2022. This is not a deficit in commodity goods; it is a deficit in the goods the United States needs for technology and innovation leadership. And this is an accelerating prob­lem for the United States, not a declining or stabilizing one.</p>
<p>Although it’s long been in denial, the United States is now recognizing that it has a deep manufacturing problem. If it is going to tackle this problem, it must shift to advanced manufacturing methods to get the kind of productivity and efficiency gains needed to compete in these goods. The issue now is how to do this. A systematic approach is needed; trying to tackle the problem by relying on just R&amp;D capabilities will not be enough. Trade policy, for instance, will be part of any successful solution and is necessary to protect manufacturing from predatory pricing by mercantilist competitors. We focus here, however, on pragmatic steps the United States could take to strengthen its innovation chain. The general theme, usually overlooked in standard policy analysis, is relinking the innovation system to production.</p>
<p>Policy discussions in recent years increasingly focus on “direction­ality,” that is, the idea that policies should set the&nbsp;<em>direction for</em>&nbsp;technol­ogy development<em>,</em> as opposed to simply letting markets work their will. But what role should government play in setting that direction for manufacturing? And how can “directional” policy decisions be pursued given the complex mix of actors (from industry in numerous sectors to universities to government) and the highly federalized political system of the United States?</p>
<p>Given this complexity, it is important for manufacturing policy design to bring together the critical actors in the system. These include industry, including small manufacturing firms and large; government, including federal, state, and local; and educators, including universities and community colleges. The tool kits of each set of actors will all be needed to form effective policies. Below are ten steps that the federal government, working with the other actors, could undertake.</p>
<p><strong>(1) Improve the Manufacturing Institutes.</strong> Although policymakers began to see that American manufacturing was facing challenges when Japan’s quality model began making inroads into U.S. auto and consumer electronics sectors in the 1970s and 80s, only limited policy steps were taken. Industry, while trying to copy Japan on quality, continued to believe that the overall solutions to its problems lay in tax and trade policies. It was not until the Obama administration that an innovation-based set of policies started to evolve, with the creation of sixteen manufacturing innovation institutes during the period of 2012 to 2017.<a href="#notes"><sup>46</sup></a> The motivating concept was that U.S. manufacturers could only compete with lower-cost Asian producers, particularly from China, if they became much more efficient and productive, to offset their cost and wage disadvantages. The institutes were collaborations, with federal core funding cost-shared with industry, universities, and state governments. Each institute was organized around a particular key manufacturing technology <span>strand—robotics,</span> digital production, bio-fabrication, ad­vanced composites, <span>etc.—and</span> they were loosely modeled on Germany’s Fraunhofer Institutes, which play a comparable role. Congress has recently approved three more institutes. While the institute model involves the critical actors required for manufacturing innovation listed above, program details need attention.<a href="#notes"><sup>47</sup></a></p>
<p><em>End the term limits.</em> The institutes have never had the budget or long-term commitment that a manufacturing technology transformation requires. At the outset of the program, the institutes were placed on term limits<strong>, </strong>with federal support set to end after five years. This was a political decision but was never realistic: the structural problems in manufacturing that the institutes were designed to tackle cannot be solved in five years; they represent long-term challenges. The term limits forced the institutes to organize around the needs of large <span>companies—the</span> only source that could sustain them over time when the government pulled <span>out—which</span> meant that they had to neglect the smaller firms that were furthest behind in adopting new technologies and limit their workforce education investments. The term limits should be ended, with institutes subject to rigorous review at the end of their current terms; if they are successful they should be extended with comparable funding. The Commerce and Defense Departments have now adopted this approach, although the Energy Department has not, and many of its institutes have become shells with very limited programs. While Commerce has reviewed and fully funded its first institute, DoD has reviewed its institutes but renewed them at significantly lower funding levels. These problems require correction.</p>
<p><em>Network the institutes.</em> In addition, efforts need to be networked across the institutes. Companies don’t want to adopt advanced manufacturing for one technology at a time just because they were developed at separate, stove-piped institutes. Instead, institutes need to cooperate to offer packages of their technology advances to firms. For example, firms don’t want to adopt just robots; they want robotics integrated with digital production technologies and other technologies such as 3-D printing or advanced composites. We need an institute network function to pull together advances from across institutes and package them for easier adaption.</p>
<p><em>Collaborate closely with state and local governments</em>. Although the institutes must have a national mission, manufacturing is highly regional, and the states and local governments play the leading roles in economic development and workforce education.&nbsp;Collaborations between them and the institutes are ongoing but deepening them will be key to institute success. New manufacturing technologies need to be available for national adoption but tested and piloted regionally, which means institutes must operate at both national and regional levels.</p>
<p><em>Undertake testing, demonstration, and certification</em>. The Defense Department has long supported the testing and demonstration stages, key steps for technology acceptance and scaling. Most manufacturing institutes have created technology centers and should receive additional funding to undertake rigorous testing and demonstration of new tech­nologies developed by participating companies in the areas the institutes support, such as robotics or 3-D Printing. Part of these demonstrations should be to identify the cost and efficiency savings to manufacturers these technologies can provide. After effective demonstration, the insti­tutes should certify and validate the feasability of the new technologies, which can help with prompt market acceptance.</p>
<p><strong>(2) Back R&amp;D for manufacturing technologies</strong>. We also need to enlist our federal R&amp;D agencies in the cause. Manufacturing is not a subject of their research, but it needs to be, so technology advances move into the manufacturing institutes for experimentation and adop­tion. The federal basic research agencies work at what DoD and NASA have characterized as “Technology Readiness Levels” (TRL) 1–3, meaning they support early-stage research up to proof-of-concept experimentation. Since industry typically devotes most of its resources to the scale-up and initial production <span>stages—TRL</span> levels <span>7–9—there</span> is a major gap (the infamous “valley of death”) in between. The manufacturing institutes are designed for these in-between levels, from development and prototyping to technology demonstration (TRL levels 4–6).</p>
<p>But if the institutes focus at these post-research stages, over time they will need new input from earlier stage research. Our R&amp;D agencies have historically shunned research on manufacturing <span>technologies—that’s</span> been viewed as industry’s purview, although industry emphasizes later-stage development not research. We need to see manufacturing as a system, with connections throughout, from research to production.</p>
<p>Although the research agencies never developed significant portfolios around manufacturing, now is the time for them to do so if we are to have a connected innovation system to spur advanced manufacturing. This means, too, that they will need to include applied work that adds manufacturing solutions to their earlier-stage research. We need a cross-agency agenda for R&amp;D on manufacturing technologies and processes at our R&amp;D agencies, linked to the development initiatives at the institutes. Otherwise, over time, tech development at the institutes will become stranded and thin out.</p>
<p><strong>(3) Provide scale-up financing. </strong>While venture capital has been a major force for financing innovation in the United States, it is now focused overwhelmingly on software, biotech, and various services sectors. It is largely out of “hard <span>tech”—innovations</span> that must be manufactured. This means there are few mechanisms to scale up manufacturing production in the United States. In recent decades, firms have been shifting prototyping and production of hard-tech goods to China.</p>
<p>A significant manufacturing gap in the United States is seen in its lack of entrepreneurial start-up firms in this sector, which are needed to advance innovation. The share of&nbsp;young manufacturing firms (less than&nbsp;five years old) among U.S. manufacturers has steadily declined over the past three decades, falling far below the levels in sectors supported by VC, such as biotech, software, or IT.<a href="#notes"><sup>48</sup></a> Yet newer firms tend to adopt and develop new manufacturing technologies and processes at a higher rate than established firms. Manufacturing is missing among new entrants, and scale-up financing could help encourage their entry into manufacturing and corresponding production innovation.</p>
<p>If the United States is to shift to advanced manufacturing at both old firms and new, it will need scale-up financing to support it.<a href="#notes"><sup>49</sup></a> In contrast to the United States, China has long provided massive manufacturing scale-up assistance to its firms, which accounts for much of its dominance of world manufacturing output. What form could scale-up fi­nancing take in the United States? Operation Warp Speed in 2020 used guaranteed contracts to Covid-19 vaccine makers to reduce their risks and assure production. Senator Chris Coons and colleagues have pro­posed an industrial finance corporation for innovative manufacturing.<a href="#notes"><sup>50</sup></a> The DoD in 2022 created a new “Office of Strategic Capital” for tech­nology scale-up, although it appears focused more on national security rather than broader economic security goals.<a href="#notes"><sup>51</sup></a> The Biden administration has proposed repurposing an established federal bank, the Ex-Im Bank, to provide manufacturing scale-up support alongside its long-standing export financing role.<a href="#notes"><sup>52</sup></a> The Depart­ment of Energy’s Loan Programs Office, since 2005, has provided financing for scale-up of new energy technologies. While the program suffered a political backlash for a failed loan to a thin-film solar company, Solyndra, in 2011, it has had numerous successes. The most notable was a $465 million loan to Tesla in 2009–13 for expansion of its Fremont, California, production site, which enabled the company to avoid bankruptcy and accelerated the scale-up of electric vehicles in the United States. In addition, states in their economic development role have often provided financing to firms to support regional job creation. In contrast to the United States,<a href="#notes"><sup>53</sup></a> China has long provided massive manufacturing scale-up assistance to its firms, which accounts for much of its dominance of world manufacturing output.</p>
<p>There are thus a range of possible mechanisms available, from forming a new industrial bank to expanding the role of existing institutions. Applying a number of these mechanisms with expanded lending authority for advanced manufacturing scale-up could be a solution to this glaring manufacturing system gap.<a href="#notes"><sup>54</sup></a></p>
<p><strong>(4) Use government procurement power to promote new manufacturing technologies. </strong>The DoD is by far the largest procurement agency, and it needs to strengthen its industrial base and build more resilient supply chains for increasingly pressing national security needs. The productivity and efficiency gains possible from advanced manufacturing therefore should be a priority. DoD historically played a role in fostering new manufacturing technologies, such as the interchangeable machine-made parts developed at Army arsenals in the 1840s. A more recent example is computer numerically controlled (CNC) equipment, pervasive now in all manufacturing sites. It was first developed through DoD-supported research at MIT. When DoD saw the new level of precision manufacturing it enabled, it required its contractors to implement CNC machining for its missile programs, which spread this advance throughout U.S. industry.</p>
<p>DoD, working with manufacturing innovation institutes, could identify productivity savings available from new technologies such as digital production advances, robotics, or 3-D printing, and require its defense contractors, through specifications and contracts, to implement them. It also has authority under the Defense Production Act to acquire these new technologies and lease them back to its contractors. While DoD procurement does not dominate U.S. manufacturing, it has a significant market share. DoD could adopt the approach of mandating improvements, along with funding support for new equipment, that it used with CNC machining, enabling larger-scale adoption of the new manufacturing technologies.</p>
<p><strong>(5) Direct production support</strong>. In the case of some critical technologies, we may need more direct production support to build factories. We need a more sustained <span>chips</span> Act, and for other critical sectors, not just semiconductors.</p>
<p>In the 2022 <span>chips</span> Act, the federal government provided $39 billion in grants as well as loan guarantees and investment tax credits to semiconductor makers to fund new advanced production facilities. Similarly, the federal government, through the Commerce or Defense Departments, could provide support for firms to acquire and install new advanced manufacturing technologies. This could be applied particularly toward production of critical or national-security-related technologies.<a href="#notes"><sup>55</sup></a></p>
<p><strong>(6) Provide both “top-down” and “bottom-up” support</strong>. Many support tools are going to be needed for a manufacturing revival and will involve a series of organizations and agencies, from the advanced manufacturing institutes, to the R&amp;D agencies noted above, and the Defense and Commerce Departments. Each has existing policy imple­mentation mechanisms and could acquire new ones based on the recommendations here. These tools can be offered to manufacturing firms in two basic forms. Top-down tools have long been employed by <span>darpa</span> and were also employed by Operation Warp Speed. Here, the government identifies a technology challenge to be addressed, and picks a small portfolio of leading companies to pursue it. The DOE’s funding of eight companies for fusion development is yet another example. Bottom-up tools involve incentives widely available to interested com­panies to meet a technology <span>challenge—it’s</span> up to the firms to pursue them. For example, the Inflation Reduction Act provides tax credits for purchases of EVs manufactured in America if the vehicles meet sourcing and processing requirements for critical materials used in their batteries. Tesla notably took advantage of many other bottom-up tools in rising to EV technology leadership.<a href="#notes"><sup>56</sup></a> The manufacturing chal­lenge is complex so both top-down and bottom-up approaches will be needed.</p>
<p><strong>(7) Build a manufacturing focus into existing industrial policy programs. </strong>The United States has recently passed a major package of industrial policy programs (including the energy demonstrations in the Infrastructure Act, the <span>chips</span> and Science Act, and the Inflation Reduction Act) with over $500 billion in new funding. Yet programs specifically focused on manufacturing technologies are largely missing from these initiatives. While the new legislation aims to rebuild U.S. supply chains, if the bulk of this manufacturing does not occur in the United States, supply-chain resilience will remain elusive. In effect, we need to put the “industrial” into these industrial policy programs. Each requires a policy focus on manufacturing. And future industrial policy approaches require more of a specific manufacturing focus. Rather than just incentivizing the growth of a particular manufacturing sector through subsidies or tax credits, this would also mean incentivizing adoption of advanced manufacturing processes such as digital technologies, robotics, artificial intelligence, 3-D printing, or bio-fabrication, both in general and in those sectors.</p>
<p><strong>(8) Map and fill gaps in supply chains</strong>. The Covid pandemic and the resulting economic fallout exposed the weaknesses of U.S. supply chains not only in protective equipment, pharmaceuticals, and pharmaceutical materials but in a host of areas, from semiconductors to critical materials and supplier manufacturing capabilities. A fundamental lesson from Operation Warp Speed in 2020 was the need to better map supply chains in order to assure production capability, and the Defense Production Act enabled the Defense Department, working closely with industry, to intervene and fill gaps to assure sufficient vaccine production to meet the emergency. Similarly, in 2020, the executive branch, through a series of participating agencies and the White House, began a major effort to identify and fill supply-chain gaps in advanced batteries, pharmaceuticals and pharmaceutical ingredients, semiconductors, and critical miner­als.<a href="#notes"><sup>57</sup></a> Particularly for the production of critical and nation­al-securi­ty‑related technologies, relevant federal agencies could work with industry to support the mapping of supply chains for implementing advanced manufacturing technologies and processes.</p>
<p><strong>(9) Fix workforce education. </strong>Germany has long gained productivity improvements from a famously well-trained manufacturing workforce based on an apprenticeship system. In contrast, U.S. companies have generally tried to get productivity gains from capital, plant, and equip­ment investments and ignored the workforce side. German firms under­stand, however, that productivity gains from new equipment can soon spread worldwide, while a gain from a high-quality workforce will be enduring and provides a long-term competitive edge.</p>
<p>The United States has a broken workforce-education system, with a deep disconnect between the education system and workplaces. If the United States wants to adopt advanced manufacturing, its workforce must be ready for it. This requires rebuilding much of workforce education at all levels.<a href="#notes"><sup>58</sup></a> Community colleges must introduce advanced manufacturing curricula, create short programs more adapted to upskilling workers already in the workforce, establish certificates around particular skills that stack toward degrees, and turn around low completion rates. At the federal level, disconnected Labor and Education Department programs need to be integrated and efforts to expand registered apprenticeship programs accelerated. At the industry level, firms must collaborate with each other and with community colleges to build new training and apprenticeship programs, including youth apprenticeship starting in high school. Solutions will have to be pursued throughout the U.S. federal system, since education is largely state- and local-government led. These governments need to improve community college funding and support, integrate advanced manufacturing pro­grams across community colleges, promote apprenticeships, and work closely with area firms on reforms and curricula.</p>
<p><strong>(10) Put someone in charge. </strong>The above steps require a series of agencies to act in concert, not an easy task in our stove-piped government, and ways must be found to pull these varied manufacturing pieces together. An interagency committee will be inadequate for the task. The Obama administration in 2008, faced with the bankruptcy of most of the U.S. auto industry, named a manufacturing czar to supervise federal support and put the industry back together, who then began to attack manufacturing challenges more broadly.<a href="#notes"><sup>59</sup></a> A comprehensive program to spur and implement new manufacturing technologies and processes, with varied mechanisms of support, as well as new trade approaches, calls for such a position in a permanent White House office. Operation Warp Speed provides guidance on how to organize it. OWS was a task force with a specific mission that included key representatives from relevant agencies led by a highly experienced industry leader and senior Army logistics expert, as well as an independent and expert supporting staff. That model could work here, assuring White House authority, links to agency powers, and independent leadership. Since the mission will require industry collaboration, an advisory group of industry, engi­neering, university, and labor leaders could be formed.<a href="#notes"><sup>60</sup></a></p>
<p>This is not an exclusive list for what needs to be done through government policy for the United States to shift to advanced manufacturing, but it marks a beginning. Most are relatively manageable initial steps, many within reach of the existing policymaking process and not requiring new laws. Nearly all require collaboration between the actors in the U.S. manufacturing <span>system—industry,</span> government, and <span>education—and</span> there will be no substitute for committed companies. The remedies here are largely federal but some are shared across federal, state, and local governments. But more “directionality” from new government policies will be required for this shift to advanced manufacturing.</p>
<h3><span><strong>A Rare Opportunity</strong></span></h3>
<p>Only about twenty countries have managed to move from developing to developed-nation status since World War II. Fourteen of them applied industrial policies based on governmental interventions.<a href="#notes"><sup>61</sup></a> This approach has been most famously expressed in the East Asian development model. Directed and targeted interventionist policies were key to Japan’s recovery after World War II and were also applied by Korea, Taiwan, Singapore, and in recent decades by China. All these countries relied on strong manufacturing through industrial policy approaches to drive economic growth. Since these examples suggest that the restoration of manufacturing prowess requires interventionist government policies, there are lessons here for the United States about the kinds of policies it needs to consider.</p>
<p>U.S. global power paralleled its rise in manufacturing power. It was the first nation to introduce interchangeable machine-made parts before the mid-nineteenth century. It grew this breakthrough in technology and process into its system of mass production, which led the world in manufacturing by the early twentieth century. By the end of World War II, it had achieved production <span>supremacy—no</span> other nation was close to U.S. manufacturing capacity and output. Through this manufacturing advance it forged a deep connection between national security and economic security with manufacturing as the enabler of both. But this link, never explicit, has been neglected since the end of the Cold War. America’s problems with, and indeed indifference to, manufacturing have historical and societal roots, which&nbsp;predate the rise of China and the offshoring of manufacturing. In some ways, offshoring is the culmi­nation of the trend, not the cause.&nbsp;But by missing a focus on manufacturing, the United States has missed an innovation capability step that many other leading nations have grasped.</p>
<p>The recent industrial policies of the Biden administration mark a sea change in America’s approach. They are attempts to address both the technological competition with China and the need for new energy technologies.<a href="#notes"><sup>62</sup></a> These initiatives are important and <span>necessary—but</span> they are not sufficient. No nation can maintain a world-power position while walking away from manufacturing. Major gaps remain in advanced manufacturing and scale-up financing, which are not a significant focus of the new programs compared to the level of effort needed. The United States is now pursuing a series of industrial policies, although so far they are light on the “industrial.” For the United States to successfully adopt advanced manufacturing at scale, its industrial innovation policies need to address production directly. China is facing its own challenges, including adverse demographic trends, rising wages, regulatory uncer­tainties, and wasted resources, but a negative shift in China’s growth model does not mean the United States will succeed in manufacturing.</p>
<p>The way forward is for the United States to redesign its innovation system, so that innovation is more closely linked to production. It can’t just focus on basic R&amp;D as has been the case since World War II. The United States needs to retain these basic research capabilities, but at the same time sharpen its focus on execution. Overall, the United States needs to more fully integrate government policies, trade, finance, education, innovation, and production.</p>
<p>There is a rare opportunity to do this. Economic security, including the technological competition with China, as well as the quest to renew American economic leadership, provide a strategic imperative.<a href="#notes"><sup>63</sup></a> But there is something more. The United States created breakthroughs in technologies which can transform advanced manufacturing: 3-D print­ing, advanced composites, new materials, biomanufacturing, photonics, power electronics, and other emerging fields including AI manufacturing.<a href="#notes"><sup>64</sup></a> We are at a once-in-a-century moment where we could fundamentally change the way we undertake production.</p>
<p>Whereas EU nations define advanced manufacturing primarily in terms of current digital technologies, the federal government’s modest manufacturing programs are researching and pursuing a broader host of <span>technologies—a</span> more revolutionary approach. The United States may have fallen behind in implementing today’s advanced production tech­niques, but with the right political commitment, new scale-up financing and other tools, and tighter linking of innovation and production, America may be able to leapfrog to the frontier of an industrial trans­formation.</p>
<h6><i>This article originally appeared in </i>American Affairs<i> Volume VII, Number 3 (Fall 2023): 3</i>–<i>30.</i></h6>
<hr>
<h6>Notes</h6>
<div><p><a name="notes"></a><sup>1 </sup>Reda Cherif, Marc Engher, and Fuad Hasanov, “<a href="https://www.imf.org/en/Publications/WP/Issues/2020/11/08/Crouching-Beliefs-Hidden-Biases-The-Rise-and-Fall-of-Growth-Narratives-49730">Crouching Beliefs, Hidden Biases: The Rise and Fall of Growth Narratives</a>,” Working Paper No. 20/228, International Monetary Fund, November 2020; Reda Cherif and Fuad Hasanov, “<a href="https://www.imf.org/en/Publications/WP/Issues/2019/03/26/The-Return-of-the-Policy-That-Shall-Not-Be-Named-Principles-of-Industrial-Policy-46710.">The Return of the Policy That Shall Not Be Named: Principles of Industrial Policy</a>,” Working Paper No. 2019/074, International Monetary Fund, March 2019, 23–24; Reka Juhasz et al., “<a href="https://steg.cepr.org/publications/who-what-when-and-how-industrial-policy-text-based-approach">The Who, What, When and How of Industrial Policy: A Text-Based Approach</a>,” Structural Transformation and Economic Growth (STEG) Working Paper 050, January 12, 2023, 22.</p>
<p><sup>2 </sup>For details on each, see William B. Bonvillian, “Industrial Innovation Policy in the United States,” <em>Annals of Science and Technology Policy</em> 6, no. 4 (2022): 315–411.</p>
<p><sup>3 </sup>“<a href="https://www.census.gov/foreign-trade/balance/c0007.html">Trade in Goods with Advanced Technology Products</a>,” U.S. Census Bureau, 2023.</p>
<p><sup>4 </sup>Ian Clay, “<a href="https://itif.org/publications/2023/02/28/china-exporter-united-states-importer/">China <span>Exporter—United</span> States Importer</a>,” Information Technology and Innovation Foundation, February 28, 2023.</p>
<p><sup>5 </sup><a href="https://www.manufacturing.gov/glossary/advanced-manufacturing.">Manufacturing.gov</a>, the federal government’s advanced manufacturing portal, defines advanced manufacturing as “Use of innovative technologies to create existing products and the creation of new products. Advanced manufacturing can include production activities that depend on information, automation, computation, software, sensing, and networking.”</p>
<p><sup>6 </sup>Robert J. Gordon, <em>The Rise and Fall of American Growth</em> (Princeton: Princeton University Press, 2016).</p>
<p><sup>7 </sup>Stefan J. Link, <em>Forging Global Fordism: Nazi Germany, Soviet Russia, and the Contest over the Industrial Order </em>(Princeton: Princeton University Press, 2020), 3.</p>
<p><sup>8 </sup>Dwight Jon Zimmerman, “<a href="https://www.defensemedianetwork.com/stories/henry-j-kaiser-and-the-liberty-ships/">Henry J. Kaiser and the Liberty Ships</a>,” Defense Media Network, June 24, 2021.</p>
<p><sup>9 </sup>David Hounshell, <em>From the American System to Mass Production, 1800–1932: The Development of Manufacturing Technology in the United States</em> (Baltimore: Johns Hopkins University Press, 1984), 3.</p>
<p><sup>10</sup>&nbsp;Vannevar Bush, <a href="https://www.nsf.gov/od/lpa/nsf50/vbush1945.htm#transmittal"><em>Science—the Endless Frontier: A Report to the President</em></a> (Washington, D.C.: United States Government Printing Office, 1945), reprints Roosevelt letter of November 17, 1944.</p>
<p><sup>11 </sup>Bush, <em>Science—the Endless Frontier</em>.</p>
<p><sup>12 </sup>William B. Bonvillian, “Encompassing the Innovation Panoply,” <em>Issues in Science and Technology</em> 38, no. 2 (Winter 2022): 37–43.</p>
<p><sup>13 </sup>Bonvillian, “Encompassing the Innovation Panoply.”</p>
<p><sup>14 </sup>William B. Bonvillian and Peter L. Singer, <em>Advanced Manufacturing: The New American Innovation Policies</em> (Cambridge: MIT Press, 2018), ch. 2.</p>
<p><sup>15 </sup>Jake Sullivan, “<a href="https://www.whitehouse.gov/briefing-room/speeches-remarks/2023/04/27/remarks-by-national-security-advisor-jake-sullivan-on-renewing-american-economic-leadership-at-the-brookings-institution/">Remarks on Renewing American Economic Leadership</a>,” White House, April 27, 2023; Robert Lighthizer, <em>No Trade is Free</em> (New York: Broadside Books, 2023).</p>
<p><sup>16 </sup>Henry Kressel, email interview, 2023. See also Kressel, “Edison’s Legacy: Industrial Laboratories and Innovation,” <em>American Affairs</em> 1, no. 4 (Winter 2017): 115–29.</p>
<p><sup>17 </sup>Bonvillian and Singer, <em>Advanced Manufacturing</em>, n. 12.</p>
<p><sup>18 </sup>Jeffrey A. Hart, “A Comparative Analysis of the Sources of America’s Relative Economic Decline,” <em>Understanding American Economic Decline,</em> eds. Michael A. Bernstein and David E. Adler (Cambridge: Cambridge University Press, 1994), 207.</p>
<p><sup>19 </sup>Bonvillian and Singer, <em>Advanced Manufacturing</em>, ch. 3.</p>
<p><sup>20 </sup>Daniel Bell, <em>The&nbsp;Coming&nbsp;of Post-Industrial Society: A Venture in Social Forecasting</em> (New York: Basic Books, 1973).</p>
<p><sup>21 </sup>Daniel Bell, “Notes on the Post-Industrial Society,” <em>Public Interest</em> (Winter 1967): 27.</p>
<p><sup>22 </sup>Bell, “Notes on the Post-Industrial Society,” 28.</p>
<p><sup>23 </sup>See William B. Bonvillian and Sanjay E. Sarma, <em>Workforce Education: A New Roadmap </em>(Cambridge: MIT Press, 2021).</p>
<p><sup>24 </sup>Paul Lewis, “<a href="https://www.tandfonline.com/doi/full/10.1080/13636820.2023.2215749">Innovation, Technician Skills, and Vocational Education and Training: Connecting Innovation Systems and Vocational Education and Training</a>,” <em>Journal of Vocational Education and Training</em>, May 27, 2023.</p>
<p><sup>25 </sup>For more, see Bonvillian and Singer, “Advanced Manufacturing,” ch. 4.</p>
<p><sup>26 </sup>Rana Foroohar, “<a href="https://www.ft.com/content/22dd4058-c283-45c3-8877-8c2507ec7d6b">Why Manufacturing Matters to Economic Superpowers</a>,”<em> Financial Times</em>, April 14, 2021.</p>
<p><sup>27 </sup>Alan S. Blinder, “Fear of Offshoring,” CEPS Working Paper No. 119, December 2005.</p>
<p><sup>28 </sup>U.S. Council of Economic Advisers, <a href="https://www.govinfo.gov/content/pkg/ERP-2004/pdf/ERP-2004.pdf"><em>Annual Report of the Council of Economic Advisers</em></a><em>: Economic Report of the President</em> (Washington, D.C.: United States Government Printing Office, 2004).</p>
<p><sup>29 </sup>Jonathan Weisman, “<a href="https://www.washingtonpost.com/archive/politics/2004/02/11/bush-adviser-assailed-for-stance-on-offshoring-jobs/b1fa394d-f353-4749-92b8-119285392352/">Bush Adviser Assailed for Stance on ‘Offshoring’ Jobs</a>,” <em>Washington Post</em>, February 11, 2004.</p>
<p><sup>30</sup> Peter Petre, “<a href="https://money.cnn.com/magazines/fortune/fortune_archive/1985/11/11/66592/index.htm">How GE Bobbled the Factory of the Future</a>,” <em>Fortune</em>, November 11, 1985.</p>
<p><sup>31 </sup>David Gelles, <em>The Man Who Broke Capitalism: How Jack Welch Gutted the Heartland and Crushed the Soul of Corporate America</em><em>―</em><em>and How to Undo His Legacy</em> (New York: Simon and Schuster, 2022).</p>
<p><sup>32&nbsp; </sup>Suzanne Berger, <em>How We Compete</em> (New York: Crown Business, 2005).</p>
<p><sup>33 </sup>David Adler, “<a href="https://www.city-journal.org/article/the-real-challenge-for-u-s-industry">The Real Challenge for U.S. Industry</a>,” <em>City Journal</em>, March 29, 2017.</p>
<p><sup>34 </sup>“<a href="https://ifr.org/ifr-press-releases/news/china-overtakes-usa-in-robot-density">China Overtakes USA in Robot Density</a>,” International Federation of Robotics, December 5, 2022.</p>
<p><sup>35 </sup>See Klaus Schwab, “<a href="https://www.weforum.org/agenda/2016/01/the-fourth-industrial-revolution-what-it-means-and-how-to-respond/">The Fourth Industrial Revolution: What It Means, How to Respond</a>,” World Economic Forum, January 14, 2016; “<a href="https://www.weforum.org/focus/fourth-industrial-revolution">Fourth Industrial Revolution</a>,” World Economic Forum, 2023.</p>
<p><sup>36 </sup>Jonas Nahm, <em>Collaborative Advantage: Forging Green Industries in the New Global Economy</em> (Oxford: Oxford University Press, 2021).</p>
<p><sup>37 </sup>Although Nahm’s book predates the major energy investments in the 2022 Inflation Reduction Act, that act focused on consumer and industry tax and other incentives for spreading green technologies, not on manufacturing capability. Although the <span>chips</span> and Science Act calls for regional innovation hubs to try to spread innovation mastery to new areas, potentially including production capability, it is not yet funded. So Nahm’s analysis remains relevant for understanding comparative approaches to innovation and production.</p>
<p><sup>38 </sup>Greg Ip, “<a href="https://www.wsj.com/articles/chinas-ev-juggernaut-is-a-warning-for-the-west-1389f718">China’s EV Juggernaut Is a Warning for the West</a>,” <em>Wall Street Journal</em>, June 7, 2023; Zeyi Yang, “<a href="https://www.technologyreview.com/2023/02/21/1068880/how-did-china-dominate-electric-cars-policy/">How Did China Come to Dominate Electric Cars?</a>,” <em>MIT Technology Review</em>, February 21, 2023.</p>
<p><sup>39 </sup>Gregor Sebastian and Francois Chimits, “‘<a href="https://merics.org/en/comment/made-china-electric-vehicles-could-turn-sino-eu-trade-its-head">Made in China’ Electric Vehicles Could turn Sino-EU Trade on its Head</a>,” <span>merics</span>, May 30, 2022.</p>
<p><sup>40</sup> David Adler, “Guiding Finance, China’s Strategy for Funding Advanced Manufacturing,” <em>American Affairs</em> 6, no. 2 (Summer 2022): 17–40.</p>
<p><sup>41 </sup>Barry Naughton, Siwen Xiao, and Yaosheng Xu, “The Trajectory of China’s Industrial Policies,” Working Paper, UC Institute on Global Conflict and Cooperation, June 2, 2023.</p>
<p><sup>42 </sup>Gerard DiPippo et al., <em>Red Ink: Estimating Chinese Industrial Policy Spending in Comparative Perspective</em> (Washington, D.C.: Centre for Strategic and International Studies, 2022).</p>
<p><sup>43 </sup>Jeroen Groenewegen-Lau and Michael Laha, “<a href="https://merics.org/en/report/controlling-innovation-chain">Controlling the Innovation Chain: China’s Strategy to Become a Science and Technology Superpower</a>,” <span>merics</span>, Feb 2, 2023.</p>
<p><sup>44 </sup>Bailey Crane, “<a href="https://www.csis.org/analysis/chinas-drive-leadership-global-research-and-development">China’s Drive for Leadership in Global Research and Development</a>,” Center for Strategic and International Studies, June 30, 2023.</p>
<p><sup>45 </sup>Andrew Batson, “<a href="https://andrewbatson.com/2023/06/13/breaking-down-chinas-manufacturing/">Breaking Down China’s Manufacturing</a>,” <em>The Tangled Woof</em> (blog), June 13, 2023.</p>
<p><sup>46 </sup>See, generally, Bonvillian and Singer, <em>Advanced Manufacturing</em>.</p>
<p><sup>47 </sup>For additional discussion of needed improvements, see William B. Bonvillian, “<a href="https://fas.org/publication/ensuring-manufacturing-usa-reaches-its-potential/">Ensuring Manufacturing USA Reaches Its Potential</a>,” Federation of American Scientists, August 10, 2021.</p>
<p><sup>48 </sup>Ben Armstrong, Suzanne Berger, and Bill Bonvillian, <em>Advanced Technology, Advanced Training</em> (Cambridge: MIT Initiative for Knowledge and Innovation in Manufacturing, 2021): 23–24.</p>
<p><sup>49 </sup>Although investments in manufacturing plant construction have accelerated this year (see Javier David, “<a href="https://www.axios.com/newsletters/axios-macro-9acf8827-6d4c-40a1-bbfc-a5a96ce0244d.html?chunk=0&amp;utm_term=emshare#story0">1 Big Thing: A Manufacturing Supercycle is Starting</a>,” <em>Axios</em>, June 16, 2023) this is largely spending in the semiconductor sector driven by the <span>chips</span> Act, and for several EV and battery plants mostly in the South by foreign investors; it is not broad-based.</p>
<p><sup>50 </sup>“<a href="https://www.coons.senate.gov/news/press-releases/sen-coons-colleagues-seek-to-create-new-domestic-manufacturing-investment-corporation">Sen. Coons, Colleagues Seek to Create New Domestic Manufacturing Investment Corporation</a>,” Office of Senator Chris Coons, news release, August 12, 2021.</p>
<p><sup>51 </sup>C. Todd Lopez, “<a href="https://www.defense.gov/News/News-Stories/Article/Article/3233894/new-defense-office-connects-next-gen-tech-developers-with-much-needed-capital/">New Defense Office Connects with Next-Gen Tech Developers with Much-Needed Capital</a>,” U.S. Department of Defense, December 1, 2022.</p>
<p><sup>52 </sup>“<a href="https://www.exim.gov/news/statement-export-import-bank-united-states-president-and-chair-reta-jo-lewis-one-year">Statement From the Export Import Bank of the U.S. President and Chair Reta Jo Lewis on the One Year Anniversary of President Biden’s Supply Chain Executive Order</a>,” Export-Import Bank of the United States, February 24, 2022.</p>
<p><sup>53 </sup>Tesla’s loan from the Department of Energy (DOE) was approved in 2009 and was available in January of 2010. Tesla repaid the loan in 2013, ahead of schedule; see “<a href="https://www.energy.gov/lpo/tesla">Tesla: Loan Programs Office</a>,” U.S. Department of Energy, accessed July 22, 2023; Tim Higgins, <em>Power Play: Tesla, Elon Musk and the Bet of the Century</em> (New York: Doubleday, 2021): 109–12, 159, 169, 194.</p>
<p><sup>54 </sup>Gerard DiPippo et al., <em>Red Ink</em>; David Adler, “Guiding Finance.”</p>
<p><sup>55 </sup>The <span>chips</span> and Science Act identified ten critical technology areas for government programs to focus on, with a process to periodically update this list (See, HR 4346, 117th Cong., 2nd Sess., Subtitle G, 2021; Tim Clancy, “C<span>hips</span><a href="https://ww2.aip.org/fyi/2022/chips-and-science-act-enshrines-policy-new-nsf-technology-directorate">&nbsp;and Science Act Enshrines Policy for New NSF Technology Directorate</a>,” American Institute of Physics, November 23, 2022.</p>
<p><sup>56 </sup>Bonvillian, “Industrial Innovation Policy in the United States.”</p>
<p><sup>57 </sup>“<a href="https://www.whitehouse.gov/wp-content/uploads/2021/06/100-day-supply-chain-review-report.pdf">Building Resilient Supply Chains, Revitalizing American Manufacturing, and Fostering Broad-Based Growth</a>,” White House, June 2021.</p>
<p><sup>58 </sup>See detailed recommendations in William B. Bonvillian and Sanjay E. Sarma, <em>Workforce Education: A New Roadmap </em>(Cambridge: MIT Press, 2021).</p>
<p><sup>59 </sup>Noam Scheiber, “<a href="https://newrepublic.com/article/71699/manufacturing-bloom">Manufacturing Bloom</a>,” <em>New Republic</em>, December 7, 2009.</p>
<p><sup>60 </sup>The Advanced Manufacturing Partnership, an Obama administration panel of industry, university, and labor representatives that developed advanced manufacturing policies in 2011–14, provides a model (Executive Office of the President, “<a href="https://obamawhitehouse.archives.gov/the-press-office/2011/06/24/president-obama-launches-advanced-manufacturing-partnership">President Obama Launches Advanced Manufacturing Partnership</a>,” news release, June 24, 2011).</p>
<p><sup>61</sup> Reda Cherif and Fuad Hasanov, “The Return of the Policy That Shall Not Be Named.” See also David Oks and Henry Williams, “The Long, Slow Death of Global Development,” <em>American Affairs</em> 6, no. 4 (Winter 2022): 122–50.</p>
<p><sup>62 </sup>Bonvillian, “Industrial Innovation Policy in the United States.”</p>
<p><sup>63 </sup>Tai Ming Cheung and Thomas G. Mahnken, <em>The Decisive Decade: United States–China Competition in Defense Innovation and Defense Industrial Policy in and beyond the 2020s</em> (Washington, D.C.: Center for Strategic and Budget Assessments, 2023); Sullivan, “Remarks on Renewing American Economic Leadership.”</p>
<p><sup>64 </sup>The Georgia AI Manufacturing (<span>ga-aim</span>) coalition, led by the Georgia Tech Research Corporation, will receive approximately $65 million from the Commerce Department’s Economic Development Administration to accelerate the adoption of artificial intelligence across the state’s legacy industrial sectors. See U.S. Economic Development Corporation, “<a href="https://www.eda.gov/news/press-release/2022/09/02/us-department-commerce-invests-approximately-65-million-accelerate">U.S. Department of Commerce Invests Approximately $65 Million to Accelerate Integration of Artificial Intelligence Technologies in Industry in Georgia through American Rescue Plan Regional Challenge</a>,” news release<strong>,</strong> September 2, 2022.</p>
</div>
<!-- AddThis Advanced Settings above via filter on the_content --><!-- AddThis Advanced Settings below via filter on the_content --><!-- AddThis Advanced Settings generic via filter on the_content --><!-- AddThis Share Buttons above via filter on the_content --><!-- AddThis Share Buttons below via filter on the_content --><!-- AddThis Share Buttons generic via filter on the_content -->		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Adobe will charge “credits” for generative AI (124 pts)]]></title>
            <link>https://helpx.adobe.com/firefly/using/generative-credits-faq.html</link>
            <guid>37538878</guid>
            <pubDate>Sat, 16 Sep 2023 21:28:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://helpx.adobe.com/firefly/using/generative-credits-faq.html">https://helpx.adobe.com/firefly/using/generative-credits-faq.html</a>, See on <a href="https://news.ycombinator.com/item?id=37538878">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="root_content_flex">





    <div id="root_content_flex_items_position" data-animations="[]" daa-im="true" daa-lh="HelpX in-article navigation">
    
    
<div>









    

    
     
     
    
        <p>Wondering what generative credits are, how many you have in your account, and how you can use them? We've got you covered.<br>
</p>
    


    



</div>
<div>
    <p>Until November 1, 2023, Creative Cloud, Adobe Firefly, Adobe Express, and Adobe Stock paid subscribers won't be subject to generative credit limits. Starting November 1, 2023, credit limits will apply.<br>
</p>
    


    


</div>
<div>

            <p>Generative AI is a type of artificial intelligence that enhances creativity by producing amazing results from simple text prompts. Generative AI features powered by Firefly are now available in our core creative tools and the standalone Firefly web app. We’re starting with images, text effects, and vectors, with Generative Fill and Generative Expand in Adobe Photoshop, Text to Image in Adobe Firefly, Generative Recolor in Adobe Illustrator, Text Effects in Adobe Express, and more. Next, we plan to bring generative AI powered by Firefly to 3D, animation, and video. Each groundbreaking generative AI feature unlocks new creative possibilities, empowering users to play, experiment, dream, and create the extraordinary.</p>
<p>Since generating content with AI models requires significant computational resources, we have updated our plans to include a monthly allocation of “generative credits.”</p>

        </div>
<div>









    

    
     
     
    
        <p>

            <h2><span>Frequently asked questions</span></h2>

        </p>
    


    



</div>
<div>









    

    
     
     
    
        <p>

            <h2>What are generative credits?</h2>

        </p>
    


    



</div>
<div>









    

    
     
     
    
        <p>Generative credits provide priority processing of generative AI content across features powered by Firefly in the applications that you are entitled to. Generative credit counts reset each month.</p>
    


    



</div>
<div>









    

    
     
     
    
        <p>

            <h2><b><a id="generative-credits-consumption"></a>How are generative credits consumed?</b></h2>

        </p>
    


    



</div>
<div>

            <p><span>The consumption of generative credits depends on the generated output's computational cost and the value of the generative AI feature used.</span></p>
<p><span>Examples of actions where you are debited generative credits:</span><br>
</p>
<ul>
<li>Select <b>Generate</b> in Text Effects</li>
<li>Select <b>Load more</b>&nbsp;or <b>Refresh</b> in Text to Image</li>
</ul>

        </div>
<div id="root_content_flex_items_position_position-par_flex_copy_copy_copy">





    <div id="root_content_flex_items_position_position-par_flex_copy_copy_copy_items_position" data-animations="[]">
            





<figure>
    
    
        
        
            
                <img id="root_content_flex_items_position_position-par_flex_copy_copy_copy_items_position_position-par_image" src="https://helpx.adobe.com/content/dam/help/en/firefly/using/generative-credits/generative-recolor-ai-faq-firefly.jpg.img.jpg" alt="Example of how generative credits are consumed using generative AI features in Adobe Illustrator.">
                
            
        
    
    <figcaption>Use Generative Recolor in Adobe Illustrator.</figcaption>
</figure>

        </div>
<div id="root_content_flex_items_position_position-par_flex_copy_copy_copy_items_position_577539748" data-animations="[]">
            





<figure>
    
    
        
        
            
                <img id="root_content_flex_items_position_position-par_flex_copy_copy_copy_items_position_577539748_position-par_image_copy" src="https://helpx.adobe.com/content/dam/help/en/firefly/using/generative-credits/text-effect-express-faq-firefly.jpg.img.jpg" alt="Example of how generative credits are consumed using generative AI features in Adobe Express.">
                
            
        
    
    <figcaption>Use Text Effects in Adobe Express.</figcaption>
</figure>

        </div>
<div id="root_content_flex_items_position_position-par_flex_copy_copy_copy_items_position_1524634138" data-animations="[]">
            





<figure>
    
    
        
        
            
                <img id="root_content_flex_items_position_position-par_flex_copy_copy_copy_items_position_1524634138_position-par_image_copy_copy" src="https://helpx.adobe.com/content/dam/help/en/firefly/using/generative-credits/generative-fill-ps-faq-firefly.jpg.img.jpg" alt="Example of how generative credits are consumed using generative AI features in Adobe Photoshop.">
                
            
        
    
    <figcaption>Use Generative Fill in Adobe Photoshop.</figcaption>
</figure>

        </div>

    

</div>
<div>

            <p>Examples of when you're not debited generative credits:</p>
<ul>
<li>Using generative AI features&nbsp;defined in the rate table&nbsp;as "0.”</li>
<li>Trying a prompt in the&nbsp;<a href="https://firefly.adobe.com/gallery" target="_blank" disablelinktracking="false">Firefly gallery</a>&nbsp;since opening the art isn't a new generation. But, you'll be debited generative credits if you select <b>Refresh</b>, which requires a new generation.</li>
</ul>
<p><a id="rate-table"></a>Generative credits usage rate table:</p>

        </div>
<div>

            <table>
<tbody><tr><td><b>Feature</b><br>
</td>
<td><b>Generative credit use</b></td>
</tr><tr><td>Generative Fill, Generative Expand, Text to Image, Generative Recolor<br>
</td>
<td>1 credit*</td>
</tr><tr><td>Text Effects</td>
<td><ul>
<li>Before November 1, 2023:&nbsp;0 credits<br>
</li>
<li>Starting November 1, 2023: 1 credit*</li>
</ul>
</td>
</tr></tbody></table>

        </div>
<div>

            <p>* <i>For standard images of up to 2000 x 2000 pixels. To receive the above-listed consumption rates, you must use the latest version of the software. Usage rates may vary. Plans are subject to change.</i></p>
<p>We plan to offer higher-resolution images, animation, video, and 3D generative AI features in the future. The number of generative credits consumed for those features may be greater.</p>

        </div>
<div>









    

    
     
     
    
        <p>

            <h2>How many generative credits do I get in my plan?<br>
</h2>

        </p>
    


    



</div>
<div>

            <p>Generative credits can be used across generative AI features powered by Adobe Firefly in the applications you are entitled to. Generative credit counts reset each month. If you have multiple subscriptions, the total number of generative credits available is the aggregate of what is included in each plan.&nbsp;<span>We have updated our plans to include access to generative AI creations in the following ways:</span></p>
<ul>
<li>Creative Cloud and Adobe Stock paid subscriptions include a plan-specific number of monthly generative AI creations of vector graphics or standard-resolution images* powered by Adobe Firefly. After the plan-specific number of generative credits is reached, you can keep taking generative AI actions, but your use of generative AI features may be slower.</li>
<li>Adobe Express and Adobe Firefly paid plans also include a plan-specific number of monthly generative AI creations of vector graphics or standard-resolution images* powered by Adobe Firefly. After the plan-specific number of generative credits is reached, you'll be able to take two generative AI actions to create vector graphics or standard-resolution images per day until your credits reset the next month.</li>
<li>Creative Cloud, Adobe Firefly, and Adobe Express free plans receive a plan-specific number of monthly generative AI creations of vector graphics or standard resolution images* powered by Adobe Firefly. After the plan-specific number of generative credits is reached, you can upgrade to a paid plan to continue creating assets with features powered by Firefly or wait until credits reset the next month.</li>
</ul>
<p><i>* Standard resolution imagery is up to 2000 x 2000 pixels. Usage rates may vary. Plans are subject to change.</i></p>

        </div>
<div>



    
        
        
    

<spectrum-accordion data-name="Generativecredits" data-multi-select="true">
        <spectrum-accordion-item data-label="Creative Cloud individual plans" data-titleid="item-Individualplans-Generativecredits">
        </spectrum-accordion-item>

        <spectrum-accordion-item data-label="Adobe Stock plans" data-titleid="item-AdobeStockplans-Generativecredits">
        </spectrum-accordion-item>

        <spectrum-accordion-item data-label="Education plans" data-titleid="item-Educationplans-Generativecredits">
        </spectrum-accordion-item>

        <spectrum-accordion-item data-label="Teams plans" data-titleid="item-Teamsplans-Generativecredits">
        </spectrum-accordion-item>

        <spectrum-accordion-item data-label="Enterprise plans" data-titleid="item-Enterpriseplans-Generativecredits">
        </spectrum-accordion-item>

        <spectrum-accordion-item data-label="Enterprise Term License Agreements" data-titleid="item-EnterpriseTermLicenseAgreements-Generativecredits">
        </spectrum-accordion-item>

        <spectrum-accordion-item data-label="Adobe Express and Adobe Firefly paid plans" data-titleid="item-FreeuserswithanAdobeID-Generativecredits">
        </spectrum-accordion-item>

        <spectrum-accordion-item data-label="Free Creative Cloud, Adobe Firefly and Adobe Express Users" data-titleid="item-FreeCreativeCloudAdobeFireflyandAdobeExpressUsers-Generativecredits">
        </spectrum-accordion-item>
</spectrum-accordion>
<div>
    <div>
    
    <div>

            <table>
<tbody><tr><td><b>Plan</b></td>
<td><b>Monthly generative credits</b></td>
</tr><tr><td>Creative Cloud All Apps</td>
<td>1,000</td>
</tr><tr><td><p>Creative Cloud Single App</p>
<ul>
<li>Illustrator, InDesign, Photoshop, Premiere Pro, After Effects, Audition, Animate, Adobe Dreamweaver, Adobe Stock, Photography 1TB<br>
</li>
</ul>
</td>
<td>500</td>
</tr><tr><td><p>Creative Cloud Single App</p>
<ul>
<li>Creative Cloud Photography 20GB:<ul>
<li>Subscribers before November 1, 2023</li>
<li>Subscribers after&nbsp;November 1, 2023</li>
</ul>
</li>
</ul>
</td>
<td>

<p>250</p>
<p>100</p>
</td>
</tr><tr><td><p>Creative Cloud Single App</p>
<ul>
<li>Lightroom</li>
</ul>
</td>
<td>100</td>
</tr><tr><td><p>Creative Cloud Single App</p>
<ul>
<li>InCopy, Substance 3D Collection, Substance 3D Texturing, Acrobat Pro<br>
</li>
</ul>
</td>
<td>25</td>
</tr></tbody></table>

        </div>
<div>









    

    
     
     
    
        <p>Creative Cloud plans that are not listed include 25 generative credits.&nbsp;For more information,&nbsp;contact&nbsp;<a href="https://helpx.adobe.com/support.html" target="_blank" disablelinktracking="false">Adobe Customer Care</a><span>&nbsp;(for individual users) or <a disablelinktracking="false" href="https://helpx.adobe.com/enterprise/kb/contact-administrator.html" target="_blank">your administrator</a> (for teams or enterprise users).</span></p>
    


    



</div>

    
</div>

    <div>

            <table>
<tbody><tr><td><b>Plan</b></td>
<td><b>Monthly generative credits</b></td>
</tr><tr><td>Adobe Stock paid subscriptions<br>
</td>
<td>500</td>
</tr></tbody></table>

        </div>

    <div>

            <table>
<tbody><tr><td><b>Plan</b></td>
<td><b>Monthly generative credits</b></td>
</tr><tr><td>Higher Education Creative Cloud for enterprise Pro, Student Teacher Edition All Apps</td>
<td>1000</td>
</tr><tr><td>Higher Education All Apps</td>
<td>500</td>
</tr><tr><td>K-12 Express Premium, K-12 All Apps, Higher Education Express Premium, Single App, Student Teacher Edition Single App, Adobe Express</td>
<td><p>250<br>
</p>
</td>
</tr></tbody></table>

        </div>

    <div>

            <table>
<tbody><tr><td><b>Plan</b></td>
<td><b>Monthly generative credits</b></td>
</tr><tr><td>Creative Cloud for teams All Apps, Creative Cloud Pro for teams All Apps</td>
<td>1000</td>
</tr><tr><td>Creative Cloud for teams Single App, Creative Cloud Pro for teams Single App</td>
<td>500</td>
</tr></tbody></table>

        </div>

    <div>

            <table>
<tbody><tr><td><b>Plan</b></td>
<td><b>Monthly generative credits</b></td>
</tr><tr><td>Creative Cloud for enterprise All Apps, Creative Cloud Pro for enterprise All Apps</td>
<td>1000<br>
</td>
</tr><tr><td><p>Creative Cloud for enterprise Single App, Creative Cloud Pro for enterprise Single App<br>
</p>
</td>
<td>500</td>
</tr></tbody></table>

        </div>

    <div>

            <table>
<tbody><tr><td><b>Plan</b></td>
<td><b>Monthly generative credits</b></td>
</tr><tr><td>Creative Cloud Pro Plus All Apps<br>
</td>
<td><p>3000<br>
</p>
</td>
</tr><tr><td>Adobe Express &amp; Firefly site license and seat-based licenses, Creative Cloud for enterprise Pro Firefly All Apps</td>
<td>1200<br>
</td>
</tr><tr><td>Creative Cloud for enterprise All Apps, Creative Cloud Pro for enterprise All Apps</td>
<td>1000</td>
</tr><tr><td>Creative Cloud for enterprise Pro Firefly Single App, Creative Cloud Pro Plus Single App</td>
<td>700</td>
</tr><tr><td>Creative Cloud for enterprise Single App, Creative Cloud Pro for enterprise Single App</td>
<td>500</td>
</tr><tr><td>Adobe Express trial</td>
<td>250<br>
</td>
</tr><tr><td>Document Cloud only plans<br>
</td>
<td>25<br>
</td>
</tr></tbody></table>

        </div>

    <div>

            <table>
<tbody><tr><td><b>Plan</b></td>
<td><b>Monthly generative credits</b></td>
</tr><tr><td>Adobe Express Premium plan</td>
<td>250<br>
</td>
</tr><tr><td>Adobe Firefly Premium plan<br>
</td>
<td>100</td>
</tr></tbody></table>

        </div>

    <div>

            <table>
<tbody><tr><td><b>Plan</b></td>
<td><b>Monthly generative credits</b></td>
</tr><tr><td>Free users with an Adobe ID: Adobe Express, Adobe Firefly, Creative Cloud<br>
</td>
<td>25<br>
</td>
</tr></tbody></table>

        </div>
</div>



</div>
<div>









    

    
     
     
    
        <p>

            <h2><b>How do I check how many generative credits I have remaining?</b><br>
</h2>

        </p>
    


    



</div>
<div>

            <ul>
<li>Free users with an Adobe ID can access their generative credit count in-app or on their&nbsp;<a href="https://account.adobe.com/" target="_blank" disablelinktracking="false">Adobe account</a><a>.</a></li>
<li>Creative Cloud, Adobe Firefly, Adobe Express, and Adobe Stock paid subscribers won't be subject to generative credit limits and won't see generative credits or usage counts in their accounts until November 1, 2023.</li>
</ul>

        </div>
<div>
            





<figure>
    
    
        
        
            
                <img id="root_content_flex_items_position_position-par_image" src="https://helpx.adobe.com/content/dam/help/en/firefly/using/generative-credits/faq-generative-credits-firefly.jpg.img.jpg" alt="Screen displaying how you can access your generative credit count in-app.">
                
            
        
    
    <figcaption>Select your profile icon in an Adobe application to check your generative credit count.</figcaption>
</figure>

        </div>
<div>









    

    
     
     
    
        <p>

            <h2><b>Do generative credits roll over month to month?</b></h2>

        </p>
    


    



</div>
<div>









    

    
     
     
    
        <p>No, generative credits don't roll over to the next month because the cloud-based computational resources are fixed and assume a certain allocation per user in a given month. Your generative credit balance will reset to your allocated amount on a monthly basis.<br>
</p>
    


    



</div>
<div>









    

    
     
     
    
        <p>

            <h2><b>What if I have multiple subscriptions?</b></h2>

        </p>
    


    



</div>
<div>









    

    
     
     
    
        <p>If you have multiple subscriptions, the total number of generative credits available is the aggregate of what is included in each plan.&nbsp;<span>For example, if you subscribe to both Illustrator and Photoshop single app, you can use your generative credits for generative AI features in either application and even in Adobe Express or the Firefly web app. Your monthly generative credit allocation is the sum of each subscription’s allocation.</span></p>
    


    



</div>
<div id="root_content_flex_items_position_position-par_flex_copy_copy_copy_">





    <div id="root_content_flex_items_position_position-par_flex_copy_copy_copy__items_position_577539748" data-animations="[]">
            





<figure>
    
    
        
        
            
                <img id="root_content_flex_items_position_position-par_flex_copy_copy_copy__items_position_577539748_position-par_image_copy" src="https://helpx.adobe.com/content/dam/help/en/firefly/using/generative-credits/text-effect-express-faq-firefly-2.jpg.img.jpg" alt="Example of how generative credits are consumed using generative AI features in Adobe Express.">
                
            
        
    
    <figcaption>Use Text Effects in Adobe Express.</figcaption>
</figure>

        </div>
<div id="root_content_flex_items_position_position-par_flex_copy_copy_copy__items_position" data-animations="[]">
            





<figure>
    
    
        
        
            
                <img id="root_content_flex_items_position_position-par_flex_copy_copy_copy__items_position_position-par_image_copy_copy" src="https://helpx.adobe.com/content/dam/help/en/firefly/using/generative-credits/generative-fill-ps-faq-firefly-2.jpg.img.jpg" alt="Example of how generative credits are consumed using generative AI features in Adobe Photoshop.">
                
            
        
    
    <figcaption>Use Generative Fill in Adobe Photoshop.</figcaption>
</figure>

        </div>

    

</div>
<div>









    

    
     
     
    
        <p>

            <h2><b>What happens if I use all my generative credits?&nbsp;&nbsp;</b><br>
</h2>

        </p>
    


    



</div>
<div>

            <p>Generative credit counts reset each month. Starting November 1, 2023, if users reach their plan-specific generative credit limit during the month and until generative credits reset:</p>
<ul>
<li>Creative Cloud and Adobe Stock paid users can keep taking generative AI actions, but the use of generative AI features may be slower.</li>
<li>Adobe Express and Adobe Firefly paid users can take two generative AI actions to create vector graphics or standard resolution* images per day.</li>
<li>Free Creative Cloud, Adobe Firefly, and Adobe Express users can subscribe to a new paid plan to continue creating Firefly-powered assets.</li>
</ul>
<p>*<i> Standard resolution imagery is up to 2000 x 2000 pixels. Usage rates may vary. Plans are subject to change.</i></p>

        </div>
<div>









    

    
     
     
    
        <p>

            <h2>What do I do if I need more generative credits?<br>
</h2>

        </p>
    


    



</div>
<div>

            <p>Until November 1, 2023, Creative Cloud, Adobe Firefly, Adobe Express, and Adobe Stock paid subscribers won't be subject to generative credit limits and can continue creating beyond their plan-specific monthly credit limit.</p>
<p>After November 1, 2023, generative credit limits will be enforced, with paid users either experiencing slower use of the features or receiving a daily generation cap. Also, Adobe plans for users to be able to purchase additional priority processing generative credits through a new subscription plan, starting at US$4.99/month for 100 credits.<br>
</p>

        </div>
<div>









    

    
     
     
    
        <p>

            <h2><b>Why does Adobe use generative credits?</b><br>
</h2>

        </p>
    


    



</div>
<div>

            <p>We want you to play, experiment, dream, and create the extraordinary using the new Adobe Firefly generative AI technology in our apps. Each groundbreaking feature unlocks new creative possibilities, from Text to Image in Adobe Firefly to Generative Fill in Adobe Photoshop, Text Effects in Adobe Express, and so much more.</p>
<p>Since generating content with AI models requires significant computational resources, we have updated our plans to include a monthly allocation of generative credits that provide priority processing of generative AI content across features powered by Firefly in the applications to which users are entitled. The number of monthly generative credits each user receives depends on&nbsp;<span>their subscription. The consumption of generative credits depends on the generated output's computational cost and the value of the generative AI feature used.<br>
</span></p>

        </div>
<div>









    

    
     
     
    
        <p>

            <h2><b>Are generative credits pooled in Creative Cloud for teams or enterprise plans?</b><br>
</h2>

        </p>
    


    



</div>
<div>









    

    
     
     
    
        <p>Generative credits aren't pooled and can't be shared across multiple users.<br>
</p>
    


    



</div>
<div>









    

    
     
     
    
        <p>

            <h2><b>Are Adobe Stock credits and generative credits the same thing?</b><br>
</h2>

        </p>
    


    



</div>
<div>

            <p>No, Adobe Stock credits can't be used to generate content using Firefly-powered features. Only generative credits are used to generate content using Firefly-powered features.&nbsp;</p>
<p>Adobe Stock credits are used to license content from the Adobe Stock website as defined in the <a href="https://www.adobe.com/go/stockterms" target="_blank" disablelinktracking="false">Adobe Stock additional terms</a>&nbsp;or your customer agreement, as applicable.</p>

        </div>
<div>









    

    
     
     
    
        <p>

            <h2>What about future generative AI capabilities and functionalities?<br>
</h2>

        </p>
    


    



</div>
<div>









    

    
     
     
    
        <p>We might introduce new media types&nbsp;— for example, 3D and video — or higher resolution image and vector generation in the future that may incur additional generative credits per generation or additional costs. Check our <a disablelinktracking="false" href="#rate-table">rate table</a> for details.<br>
</p>
    


    



</div>
<div>









    

    
     
     
    
        <p>

            <h2><span>Join our community to connect, learn, and engage</span><br>
</h2>

        </p>
    


    



</div>
<div id="root_content_flex_items_position_position-par_imageandtext_copy_co">





    <div>
            





<p><img id="root_content_flex_items_position_position-par_imageandtext_copy_co_items_imageandtextimage" src="https://helpx.adobe.com/content/dam/help/en/firefly/using/text-to-image/community.png.img.png" alt="Ask the Community">
                
            
        
    
    
</p>

        </div>
<div>









    

    
     
     
    
        <p><span>For inspiration, expert tips, and solutions to common issues, visit&nbsp;<a href="https://discord.gg/dJnsV5s8PZ" target="_blank" disablelinktracking="false">Discord</a>&nbsp;or the&nbsp;<a href="https://www.adobe.com/go/firefly_forum" target="_blank" disablelinktracking="false">Adobe Firefly Community forum</a>. Connect with our team and fellow users to exchange ideas, share your creations, stay updated with the latest features and announcements, and provide feedback.<br>
 </span></p>
    


    



</div>

    

</div>

    
</div>
<div daa-im="true" daa-lh="Helpx Personalization" daa-level="1" id="root_content_flex_items_position_754629352" data-animations="[]">
    <!-- Logged out experience -->
    <div daa-level="2" daa-lh="Sign-in Prompt">





<p><img id="id-f7f5a32ffb64ed739e41c3ba635267c5" src="https://helpx.adobe.com/content/dam/adobe_logo_white.svg" alt="Adobe logo">
                
            
        
    
    
</p>
</div>
    <!-- Logged in experience -->
    <div daa-level="2" daa-lh="Plan Card">
                <p><img src="" alt="Avatar" width="40px" height="40px">
                </p>
                
            </div>
    

</div>


    

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux on a MacBook Pro (M1 Pro): How Good Is Asahi Now? (145 pts)]]></title>
            <link>https://www.youtube.com/watch?v=ZFx6R26aRHw</link>
            <guid>37538482</guid>
            <pubDate>Sat, 16 Sep 2023 20:40:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=ZFx6R26aRHw">https://www.youtube.com/watch?v=ZFx6R26aRHw</a>, See on <a href="https://news.ycombinator.com/item?id=37538482">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Lodash just declared issue bankruptcy and closed every issue and open PR (211 pts)]]></title>
            <link>https://twitter.com/danielcroe/status/1703127430523703432</link>
            <guid>37538426</guid>
            <pubDate>Sat, 16 Sep 2023 20:32:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/danielcroe/status/1703127430523703432">https://twitter.com/danielcroe/status/1703127430523703432</a>, See on <a href="https://news.ycombinator.com/item?id=37538426">Hacker News</a></p>
Couldn't get https://twitter.com/danielcroe/status/1703127430523703432: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Layoffs.fyi (159 pts)]]></title>
            <link>https://layoffs.fyi/</link>
            <guid>37538400</guid>
            <pubDate>Sat, 16 Sep 2023 20:28:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://layoffs.fyi/">https://layoffs.fyi/</a>, See on <a href="https://news.ycombinator.com/item?id=37538400">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="tab-content_471">
												 <div role="tabpanel" id="tabs_desc_471_1">
								<!--<p><b>+ Seen a layoff list or know about a layoff?</b> <a href="/share-layoff-intel/">Let us know</a> and help the affected people get exposure to companies that are hiring.</p>-->

<p><i>Companies are in reverse chronological order. View site on a 
desktop to sort, filter, search.</i></p>

<p>Visit <a href="https://layoffs.fyi/list/comprehensive/" target="_blank"> <b>Comprehensive.io</b></a> for FREE salary benchmarking data from 3,000 tech companies. Find out what companies are paying today for your role.</p>

<!--<div class="custom-notice"><b>Hit by a layoff?</b> Get a FREE Expo+ Pass to TechCrunch Disrupt on Sept. 19-21 in San Francisco. Pivot that layoff into a liftoff! <a href="https://layoffs.fyi/list/tc-disrupt-2023/" target="_blank">Register Now</a>.</div>-->

<!--<div class="custom-notice"><b>Media Partner:</b> Visit  <a href="https://layoffs.fyi/list/techmeme/" target="_blank">Techmeme</a> for general tech industry news</div>-->

<!--<span style="text-align:right"><label for="excludePublic" id="excludePublicLabel">Exclude public companies: <input type="checkbox" name="excludePublic" id="excludePublic"></span>-->


<!--<p style="text-align: right; font-size:13px"><a href="/feedback/" target="_blank">Request CSV export</a></p>-->

<!--
<p style="margin-bottom: 2.5em"></p>
<h4>Unverified layoffs (not counted in stats)</h4>
<p>Unlike the confirmed layoffs above, the ones below were contributed by readers but could not be verified:</p> 

<iframe class="airtable-embed" src="https://airtable.com/embed/shr0JnXpbrEuw8xKx?backgroundColor=green&viewControls=on" frameborder="0" onmousewheel="" width="80%" height="533" style="background: transparent; border: 1px solid #ccc;"></iframe>
-->						 </div>
												 <p role="tabpanel" id="tabs_desc_471_2">
								<!--<div class="custom-notice">Recently Laid off? Book a FREE Expo Pass to TechCrunch Disrupt on Oct. 18-20 in San Francisco & start planning your next career move. <a href="https://techcrunch.com/events/tc-disrupt-2022/free-expo-pass-for-recently-laid-off/" target="_blank">Register Now</a>.</div>-->





<!--<iframe width="861" height="398" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRluXcoyZOl3YRgzY_FSdeyu1t1dPNjs39KQjxhCq7aP1V9QmlvJ9XdfpBR49dBT894gVXipjEAcazG/pubchart?oid=1014763564&format=interactive"></iframe>-->



<!--<iframe width="961" height="628" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRluXcoyZOl3YRgzY_FSdeyu1t1dPNjs39KQjxhCq7aP1V9QmlvJ9XdfpBR49dBT894gVXipjEAcazG/pubchart?oid=506702107&format=interactive"></iframe>-->

<!--<iframe width="875" height="450" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRluXcoyZOl3YRgzY_FSdeyu1t1dPNjs39KQjxhCq7aP1V9QmlvJ9XdfpBR49dBT894gVXipjEAcazG/pubchart?oid=149462740&format=interactive"></iframe>-->

<h4>Biggest tech layoffs since COVID-19</h4>

						 </p>
												 <div role="tabpanel" id="tabs_desc_471_3">
								<!--<p><b>+ To add yourself</b>: <a href="/add-yourself/">Fill out this form</a> to get added to the Layoffs.fyi List.

<h4>Layoffs.fyi List</h4>

We've combined all the opt-in lists of laid-off employees together into a single list that's easy to filter and search.

<div class="wp-block-button" style="margin-top: 1em"><a class="wp-block-button__link has-background" href="https://list.layoffs.fyi/" style="background-color:#2ac176" title="Layoffs.fyi List">Explore the Layoffs.fyi List -></a></div>-->

<!--<div class="custom-notice">Recently Laid off? Book a FREE Expo Pass to TechCrunch Disrupt on Oct. 18-20 in San Francisco & start planning your next career move. <a href="https://techcrunch.com/events/tc-disrupt-2022/free-expo-pass-for-recently-laid-off/" target="_blank">Register Now</a>.</div>-->

<!--<div class="custom-notice"><b>🚀 Hiring engineers?</b> Our sponsor <a href="https://triplebyte.com/company?ref=layoffsfyi" target="_blank">Triplebyte</a> has a database of 55,000 technically-assessed engineers, with a 3x positive response rate compared to LinkedIn. Used by Instacart, Box, Gusto, and 450+ leading tech companies.</div>-->

<h4>Lists of Employees Laid Off</h4>
<p>Below are links to crowdsourced layoff lists created by other people. <b>Please only use for recruiting purposes!</b><br><i>View site on a desktop to sort, filter, search.</i></p>

						 </div>
												 <div role="tabpanel" id="tabs_desc_471_3">
								<!--<p><b>+ To add yourself</b>: <a href="/add-yourself/">Fill out this form</a> to get added to the Layoffs.fyi List.

<h4>Layoffs.fyi List</h4>

We've combined all the opt-in lists of laid-off employees together into a single list that's easy to filter and search.

<div class="wp-block-button" style="margin-top: 1em"><a class="wp-block-button__link has-background" href="https://list.layoffs.fyi/" style="background-color:#2ac176" title="Layoffs.fyi List">Explore the Layoffs.fyi List -></a></div>-->

<!--<div class="custom-notice">Recently Laid off? Book a FREE Expo Pass to TechCrunch Disrupt on Oct. 18-20 in San Francisco & start planning your next career move. <a href="https://techcrunch.com/events/tc-disrupt-2022/free-expo-pass-for-recently-laid-off/" target="_blank">Register Now</a>.</div>-->

<!--<div class="custom-notice"><b>🚀 Hiring engineers?</b> Our sponsor <a href="https://triplebyte.com/company?ref=layoffsfyi" target="_blank">Triplebyte</a> has a database of 55,000 technically-assessed engineers, with a 3x positive response rate compared to LinkedIn. Used by Instacart, Box, Gusto, and 450+ leading tech companies.</div>-->

<h4>Lists of Employees Laid Off</h4>
<p>Below are links to crowdsourced layoff lists created by other people. <b>Please only use for recruiting purposes!</b><br><i>View site on a desktop to sort, filter, search.</i></p>

							 </div>
							
					 </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Java 21 makes me like Java again (322 pts)]]></title>
            <link>https://wscp.dev/posts/tech/java-pattern-matching/</link>
            <guid>37538333</guid>
            <pubDate>Sat, 16 Sep 2023 20:20:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wscp.dev/posts/tech/java-pattern-matching/">https://wscp.dev/posts/tech/java-pattern-matching/</a>, See on <a href="https://news.ycombinator.com/item?id=37538333">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <div>
    <h3>Note</h3>
    <p>
        Whatever notation is used in this article does not represent how the math is usually presented. If you study this further or have already studied this subject, there may be mistakes, or terms I’ve used incorrectly. Please point out such mistakes in the comments, and I will update the article ASAP. Also, note that I’m borrowing the type theory notation from Wikipedia.
    </p>
</div>
<p>Java 21 will be released on September 19, 2023, supporting record patterns in switch blocks and expressions. Such syntax is monumental (At least, in Java land). It marks the point where Java could be considered to properly support functional programming patterns in ways similar to Kotlin, Rust, or C#. And it marks the first point where I can say, as a Kotlin developer, that I feel jealous.</p>
<h2 id="a-brief-history-of-recent-java-versions">A brief history of recent Java versions.
<span>
    <a href="#a-brief-history-of-recent-java-versions">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h2><p>Java has evolved rapidly in the past 10 years (As of 2023). Java 9 was the last “slow” release, as all subsequent releases happened 6 months apart. Below is a table showing Java updates over the past decade and the major syntactic changes/additions made in each version (Most changes are omitted to stay on topic). <br></p>
<table>
<thead>
<tr>
<th>Java Version</th>
<th>Release Date</th>
<th>Major Syntax Related Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>Java 8</td>
<td>Mar 18, 2014</td>
<td><ul><li><a href="https://openjdk.org/jeps/126">Lambda expressions</a></li></ul></td>
</tr>
<tr>
<td>Java 9</td>
<td>Sep 21, 2017</td>
<td><ul><li><a href="https://openjdk.org/projects/jigsaw/spec/">Modules(Project Jigsaw)</a></li></ul></td>
</tr>
<tr>
<td>Java 10</td>
<td>Mar 20, 2018</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/286"><code>var</code> for local variables</a></li></ul></td>
</tr>
<tr>
<td>Java 11</td>
<td>Sep 25, 2018</td>
<td>No syntactic changes</td>
</tr>
<tr>
<td>Java 12</td>
<td>Mar 19, 2019</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/325">Switch expressions (Preveiw 1)</a></li></ul></td>
</tr>
<tr>
<td>Java 13</td>
<td>Sep 17, 2019</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/354">Switch expressions (Preview 2)</a></li> <li><a href="https://openjdk.java.net/jeps/355">Text blocks (Preview 1)</a></li></ul></td>
</tr>
<tr>
<td>Java 14</td>
<td>Mar 17, 2020</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/361">Switch expressions (Stable release)</a></li> <li><a href="https://openjdk.java.net/jeps/305"><code>instanceof</code> pattern matching with <code>if</code> (Preview 1)</a></li> <li><a href="https://openjdk.java.net/jeps/359">Records (Preview 1)</a></li> <li><a href="https://openjdk.java.net/jeps/368">Text blocks (Preview 2)</a></li></ul></td>
</tr>
<tr>
<td>Java 15</td>
<td>Sep 15, 2020</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/360">Sealed classes (Preview 1)</a></li> <li><a href="https://openjdk.java.net/jeps/375"><code>instanceof</code> pattern matching with <code>if</code> (Preview 2)</a></li> <li><a href="https://openjdk.java.net/jeps/378">Text blocks (Stable release)</a></li> <li><a href="https://openjdk.java.net/jeps/384">Records (Preview 2)</a></li></ul></td>
</tr>
<tr>
<td>Java 16</td>
<td>Mar 16, 2021</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/394"><code>instanceof</code> pattern matching with <code>if</code> (Stable release)</a></li> <li><a href="https://openjdk.java.net/jeps/395">Records (Stable release)</a></li> <li><a href="https://openjdk.java.net/jeps/397">Sealed classes (Preview 2)</a></li></ul></td>
</tr>
<tr>
<td>Java 17</td>
<td>Sep 14, 2021</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/406">Switch pattern matching (Preview 1)</a></li> <li><a href="https://openjdk.java.net/jeps/409">Sealed classes (Stable release)</a></li></ul></td>
</tr>
<tr>
<td>Java 18</td>
<td>Mar 22, 2022</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/420">Switch pattern matching (Preview 2)</a></li></ul></td>
</tr>
<tr>
<td>Java 19</td>
<td>Sep 20, 2022</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/405">Record patterns (Preview 1)</a></li> <li><a href="https://openjdk.java.net/jeps/427">Switch pattern matching (Preview 3)</a></li></ul></td>
</tr>
<tr>
<td>Java 20</td>
<td>Mar 21, 2023</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/432">Record patterns (Preview 2)</a></li> <li><a href="https://openjdk.java.net/jeps/433">Switch pattern matching (Preview 4)</a></li></ul></td>
</tr>
<tr>
<td>Java 21</td>
<td>Sep 19, 2023</td>
<td><ul><li><a href="https://openjdk.java.net/jeps/440">Record patterns (Stable release)</a></li> <li><a href="https://openjdk.java.net/jeps/441">Switch pattern matching (Stable release)</a></li></ul></td>
</tr>
</tbody>
</table>
<p>There are a few notable releases here.</p>
<p>Java 14 stabilised switch expressions, 16 records and <code>instanceof</code> pattern matching, 17 sealed classes, and now, 21 will stabilise record patterns and switch pattern matching.</p>
<p>This set of changes allows Java to express one of the foundations of functional programming that the language never could before - Algebraic data types, along with idiomatic ways of using them.</p>
<p>Algebraic data types are a concept born from <em>Type theory</em>, which is a branch of Set theory that focuses specifically on questions like “Is an <em>Apple</em> a <em>Fruit</em>?” and other such whimsical conundrums math teachers like to pose to hapless students the world over.</p>
<h2 id="a-very-minimal-introduction-to-some-terms-from-type-theory">A <em>very</em> minimal introduction to some terms from type theory
<span>
    <a href="#a-very-minimal-introduction-to-some-terms-from-type-theory">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h2><p>Type theory has quite a lot of meat to it, and most of it isn’t relevant to this article.</p>
<p>So, instead of explaining type theory, I will talk about a few specific kinds of types that it would be useful to know about.</p>
<h3 id="the-bottom-or-empty-type-">The Bottom, or Empty Type (<code>⊥</code>)
<span>
    <a href="#the-bottom-or-empty-type-">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h3><p>This type describes the set of all values which <em>can’t be computed</em><sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. This set is usually empty for any normal programming language (Ø).</p>
<p>No object can be cast to bottom since it is an empty set.</p>
<p>An example of such a type is Kotlin’s <code>Nothing</code>. It is considered an error for an instance of <code>Nothing</code> to exist. The way Kotlin prevents <code>Nothing</code> from being instantiated is <a href="https://github.com/JetBrains/kotlin/blob/7a7d392b3470b38d42f80c896b7270678d0f95c3/core/builtins/native/kotlin/Nothing.kt#L23">by setting its constructor to be private</a>.</p>
<p>The Java version of this type is <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Void.html"><code>Void</code></a>, the wrapper class for the <code>void</code> primitive type. It is, again, impossible to construct a <code>Void</code> instance because its constructor is private. However, <code>Void</code> cannot be considered a true bottom type because a <code>Void</code> variable can still hold <code>null</code>, which means it is technically a unit type<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> (more on that below). In that sense, the primitive <code>void</code> does better. There’s absolutely no way to use it as a variable type, so you can’t even have an empty <code>void</code> variable.</p>
<h3 id="the-top-type-">The Top Type (<code>⊤</code>)
<span>
    <a href="#the-top-type-">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h3><p>This type represents every value of every type - the universal set of values, <code>U</code>. In Kotlin, this type is named <code>Any</code>. And it may be tempting to think <code>Object</code> (the Java equivalent of <code>Any</code>) is a top type, except for the fact that primitives exist. Primitives are wholly separate from Java’s object model, and interact with objects in rather non-standard ways. Because of this, Java technically does not have a top type the way other languages do.</p>
<p>Meanwhile, <code>C</code> does as it do and overloads <code>void</code> by using <code>void *</code> to represent a pointer that can refer to a value of any type instead. How droll!</p>
<p>Every object can be cast to top since <code>U</code> contains every value that exists.</p>
<p>There isn’t much to say about top other than that a variable of this type can hold anything. Including a value of the bottom type. Good luck finding that value, though.</p>
<h3 id="the-unit-type-">The Unit Type (<code>()</code>)
<span>
    <a href="#the-unit-type-">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h3><p>This type has only one value. There is only one instance of that one value, and it is impossible to create more of it.</p>
<p>Java’s <code>void</code> primitive technically works like this. When a method returns <code>void</code>, you can treat it as if it implicitly returns the sole instance of the <code>void</code> type under the hood (This is <em>not</em> how the JVM handles <code>void</code>). Java deviates from the theoretical norm in that <code>void</code> can never be passed into a method as a parameter.</p>
<div>
    <h3>Note</h3>
    <p>
        The truth is, Java just mashes the bottom and the unit types together to give us <code>void</code>.
    </p>
</div>
<p>You can technically simulate the unit type in Java by declaring a new class that is final and has no fields other than a static instance value. You would then be able to treat that instance as the sole unit type value.</p>
<p>In fact, this is precisely how Kotlin defines its own <code>Unit</code>. If you navigate to the definition of <code>Unit</code>, you’ll see how simply it is defined; <a href="https://github.com/JetBrains/kotlin/blob/7a7d392b3470b38d42f80c896b7270678d0f95c3/core/builtins/src/kotlin/Unit.kt#L22">it’s just an <code>object</code></a>!</p>
<p>Unlike Java, Kotlin allows you to use <code>Unit</code> anywhere, including as a parameter to a method. The following snippet is thus legal:</p>
<div><pre tabindex="0"><code data-lang="kotlin"><span><span><span>fun</span> <span>identity</span>(param1: Unit): Unit = param1
</span></span><span><span>
</span></span><span><span><span>val</span> result = identity(param1 = Unit) <span>// just returns the Unit instance again.
</span></span></span></code></pre></div><h3 id="the-boolean-type">The Boolean Type
<span>
    <a href="#the-boolean-type">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h3><p>We’re now in familiar territory.</p>
<p>The boolean type has two valid values, <code>true</code> and <code>false</code> (Or whatever other names you want to use). Indeed, you don’t even need to use your language’s native boolean type to represent this type; you can do just as well by using a nullable instance of a unit type. If the variable is non-null, it’s <code>true</code>, and if it is <code>null</code>, <code>false</code>. This is of course, a useless waste of time fit only for those interested in obfuscating their source code.</p>
<p>So far, we’ve looked at some basic examples of “rules” used to define types in type theory. Let’s move onto the heart of the matter and discuss sum and product types, and how Java 21 allows us to represent them via records and sealed classes.</p>
<h3 id="the-product-type">The Product type
<span>
    <a href="#the-product-type">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h3><p>Product types are composed of two or more constituent types. In general, a product type is a list of two or more types grouped together. A product type’s <em>arity</em>, or <em>degree</em>, is the number of constituent types within it.</p>
<p>If you’d like a nice, concrete example of a product type, look no further than the humble C <code>struct</code>:</p>
<div><pre tabindex="0"><code data-lang="c"><span><span><span>struct</span> some_type {
</span></span><span><span>    <span>int</span> val1; <span>// Type 1
</span></span></span><span><span><span></span>    <span>char</span> <span>*</span>val2; <span>// Type 2
</span></span></span><span><span><span></span>    <span>double</span> val3; <span>// Type 3
</span></span></span><span><span><span></span>    <span>int</span> val4; <span>// Type 4
</span></span></span><span><span><span></span>};
</span></span></code></pre></div><p>In the struct above, <code>some_type</code> is a product type composed of four different types: <code>int</code>, <code>char *</code>, <code>double</code>, and <code>int</code> again. Notice that we’re repeating <code>int</code> here. How do we figure out which <code>int</code> is which when we perform operations on <code>some_type</code>? This may seem obvious, but it’s a problem in math, because you have to construct all the building blocks and concepts you use from scratch!</p>
<p>In this case, we already have the tools to make this work. We associate each type with the name given to it in the struct (Duh). Mathematically speaking, a product type is not merely a list of types but a list of <em>ordered pairs</em>, where each ordered pair consists of a type and a name associated with that type.</p>
<p>For example, we can represent the first value of <code>some_type</code> as the ordered pair <code>(int, "val1")</code>. That way, it’s impossible to mix up the two <code>int</code> components; they’ve got different names!</p>
<h4 id="but-what-about-tuples-like-in-python-or-rust">But what about tuples like in Python or Rust?
<span>
    <a href="#but-what-about-tuples-like-in-python-or-rust">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h4><p>You can just think of those as product types where the “name” is the index of the component type in the tuple.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>some_tuple <span>=</span> (<span>1</span>, <span>'2'</span>, <span>True</span>, <span>5</span>)
</span></span><span><span>
</span></span><span><span>int_1 <span>=</span> some_tuple[<span>0</span>] <span># (int, 0)</span>
</span></span><span><span>str_2 <span>=</span> some_tuple[<span>1</span>] <span># (str, 1)</span>
</span></span><span><span>
</span></span><span><span><span>...</span>
</span></span></code></pre></div><h4 id="why-do-we-call-them-product-types-anyway">Why do we call them product types anyway?
<span>
    <a href="#why-do-we-call-them-product-types-anyway">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h4><p>In set theory, the word product usually refers to the <em>Cartesian</em> product of two sets.</p>
<div>
    <h3>Note</h3>
    <p>
        The Cartesian product of two sets is a set of ordered pairs made from every possible combination of every element from both sets.
    </p>
</div>
<p>You can use set theory notation to express the product of two types <code>A</code> and <code>B</code> as <code>C = A × B</code>. This product operation is not commutative; <code>A × B</code> is <em>not</em> the same as <code>B × A</code>. If you think about it for a bit, you’ll see why: you’d be switching around the order of the declared components! The example I just talked about only uses two component types: <code>A</code> and <code>B</code>. How would we represent <code>some_type</code>, for example? The answer is to chain multiple product operations together, like so:</p>
<pre tabindex="0"><code>some_type = int × char* × double × int
</code></pre><p>The set of values within a product type could be expressed like this (Please leave your complaints about my (ab)use of math symbols in the comments):</p>
<pre tabindex="0"><code>C = A × B = {(a, b) | a ∈ A, b ∈ B}
</code></pre><p>You could represent the set of all values in <code>some_type</code> like this:</p>
<pre tabindex="0"><code>some_type = { (val1, val2, val3, val4) | val1 ∈ int, val2 ∈ char*, val3 ∈ double, val4 ∈ int }
</code></pre><h4 id="alright-weve-established-what-product-types-are-whats-this-got-to-do-with-java">Alright, we’ve established what product types are. What’s this got to do with Java?
<span>
    <a href="#alright-weve-established-what-product-types-are-whats-this-got-to-do-with-java">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h4><p>When Java 16 was released, the record class feature was stabilised. Record classes are a great example of what a product type is. All fields are final; you can’t inherit from these classes either. All of a record’s state is set at the time of its construction<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> and once a record is created, that’s how it’ll look for the rest of its lifetime, all 200 milliseconds of it.</p>
<p>This contrasts with normal Java classes, which are all over the place. You can have public and private state, there’s hidden state via inheritance that you don’t think about until it pops up like some haunted animatronic on caffeine to jumpscare you with weird bugs, and you can have mutable fields, static fields, and all sorts of other distracting things that yadda yadda yadda… (<em>you get the idea</em>).</p>
<p>The problem with normal Java types is that it is impossible to generalise what a type’s components are. And that matters when all you want is to efficiently process data; you have to navigate a maze of potentially nonstandard getters to even get at your data in the first place, let alone munge it.</p>
<p>Now, Java never did support destructuring like JavaScript or Rust do. But even if Java had supported it, the spec would still probably restrict that feature to records. Let’s ask ourselves a few questions to better understand why.</p>
<div>
    <h3>Note</h3>
    <p>
        Destructuring is a feature certain languages (very famously, JavaScript) have which allows you to take a complex value, and, to borrow a PHPism, <em><strong>EXPLODE</strong></em> it into its components as a list of completely independent variables. Read more about this feature <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment">here</a>.
    </p>
</div>
<h4 id="how-would-you-even-destructure-a-normal-java-class-anyway">How would you even destructure a normal Java class anyway?
<span>
    <a href="#how-would-you-even-destructure-a-normal-java-class-anyway">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h4><p>A Java class’s internal state includes all its fields, both public and private. However, allowing the extraction of private fields by destructuring doesn’t seem like a good idea; we all know how mad old Uncle Bob gets about breaking encapsulation. So fine, we have to exclude private state.</p>
<h4 id="what-about-public-state-then">What about public state, then?
<span>
    <a href="#what-about-public-state-then">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h4><p>Let’s first think about this: How do Java objects expose public state? Sure, you can define a field as public, and if you want to prevent undue modification, make the field final. But there is another extremely common approach to this as well. Most Java objects set every field to be private and make all fields accessible only through accessor methods for reading and writing.</p>
<p>Unfortunately, there are no <em>language enforced</em> conventions for defining accessors; you could give the getter for <code>foo</code> the name <code>getBar</code>, and it’ll still work fine, except for the fact that it would confuse anybody trying to access <code>bar</code> and not `foo'.</p>
<p>Sure, you can use frameworks like Lombok to take away the complexity and uncertainty by slapping a few annotations on your POJO classes, but that doesn’t change the underlying fact that normal classes in Java are <em>very difficult to statically reason about</em> due to how many “variables” contribute to defining the state of a class.</p>
<p>I suspect this is one of the problems that prevented the Java Language Specification authors from adding pattern matching to all classes right off the bat.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p>
<p>To fix this, they created records, an entirely different class hierarchy. There is already a precedent; Java 5 introduced enums, which inherit from <code>java.lang.Enum</code>. Similarly, all records inherit from <code>java.lang.Record</code>.</p>
<h4 id="so-how-do-records-do-what-normal-classes-dont">So how do records do what normal classes don’t?
<span>
    <a href="#so-how-do-records-do-what-normal-classes-dont">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h4><p>Records solve this problem by restricting how they can be defined and rigidly defining the set of properties they can have.</p>
<p>Specifically:</p>
<ul>
<li>Records are implicitly final classes and can’t be inherited from.
<ul>
<li>No more illegitimate child classes birthed of a tryst with an entirely different concubine library.</li>
</ul>
</li>
<li>Records cannot extend any class but <code>java.lang.Record</code>.
<ul>
<li>This avoids the pitfall of inherited state polluting the record’s code.</li>
</ul>
</li>
<li>A record’s components cannot have any visibility modifiers.</li>
<li>A record’s components are always final and immutable.
<ul>
<li>This does not extend the immutability to the contents of each record component, however.</li>
<li>Only the references of a record’s components are treated as immutable.</li>
</ul>
</li>
<li>When you declare a record and do not define getter methods, getters will be defined using very specific syntax.
<ul>
<li>This syntax is very regular; Java just uses the name of the field as the getter’s name.</li>
<li>For field <code>a</code>, the getter would be <code>a()</code>.</li>
<li>Java will use your definition if you manually define a getter that matches the naming conventions. Otherwise, Java will automatically create a getter method that correctly follows the conventions. The nonstandard getter won’t make much of a difference.</li>
</ul>
</li>
<li>The fields that back a record’s components are always implicitly private and are accessed only through getters.</li>
</ul>
<p>(There is a little more to it, but this seems like a good stopping point.)</p>
<p>These properties of records guarantee that any new language feature Java brings out that uses records such as pattern matching will always work, because the language spec itself guarantees the behaviour and structure of records.</p>
<h4 id="sweet-tell-me-what-i-can-do-with-them">Sweet. Tell me what I can do with them.
<span>
    <a href="#sweet-tell-me-what-i-can-do-with-them">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h4><p>Pattern matching.</p>
<h4 id="continue">Continue…
<span>
    <a href="#continue">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h4><p>It’s quite a drag to have to write extremely nested code when you have a lot of conditions based on the types of your data. This problem will come into perspective when I introduce sum types further into the article. Pattern matching is a way to statically (Meaning at compile time, as you write the code) verify that certain patterns are present in the data you are processing.</p>
<p>Take a look at the example below. Note that the data within <code>A</code> is a <code>Record</code> instance, and could contain <em>any</em> record type. We first try printing <code>r</code>’s contents with normal Java if statements, before doing it with switch pattern matching.</p>
<div><pre tabindex="0"><code data-lang="java"><span><span>record <span>A</span><span>(</span>Record inner<span>)</span> <span>{}</span>
</span></span><span><span>record <span>B</span><span>(</span><span>char</span> b<span>)</span> <span>{}</span>
</span></span><span><span>record <span>SomeOtherRecord</span><span>()</span> <span>{}</span>
</span></span><span><span>
</span></span><span><span>Record <span>eitherAorB</span><span>()</span> <span>{</span>
</span></span><span><span>  <span>boolean</span> cond1 <span>=</span> <span>((</span><span>int</span><span>)(</span>Math<span>.</span><span>random</span><span>()</span> <span>*</span> 100<span>)</span> <span>%</span> 2 <span>==</span> 0<span>);</span>
</span></span><span><span>  <span>boolean</span> cond2 <span>=</span> <span>((</span><span>int</span><span>)(</span>Math<span>.</span><span>random</span><span>()</span> <span>*</span> 100<span>)</span> <span>%</span> 2 <span>==</span> 0<span>);</span>
</span></span><span><span>    <span>return</span> cond1 <span>?</span> <span>new</span> A<span>(</span>cond2 <span>?</span> <span>new</span> A<span>(</span><span>null</span><span>)</span> <span>:</span> <span>new</span> B<span>(</span><span>'e'</span><span>))</span> <span>:</span> <span>new</span> B<span>(</span><span>'f'</span><span>);</span> <span>// returns either A or B.
</span></span></span><span><span><span></span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>void</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>  var r <span>=</span> eitherAorB<span>();</span>
</span></span><span><span>
</span></span><span><span>  String oldJavaResult <span>=</span> <span>""</span><span>;</span>
</span></span><span><span>
</span></span><span><span>  <span>if</span> <span>(</span>r <span>instanceof</span> A<span>)</span> <span>{</span>
</span></span><span><span>    var inner <span>=</span> <span>((</span>A<span>)</span>r<span>).</span><span>inner</span><span>();</span> <span>// We have to cast it...
</span></span></span><span><span><span></span>    <span>if</span> <span>(</span>inner <span>instanceof</span> B<span>)</span> <span>{</span>
</span></span><span><span>      oldJavaResult <span>=</span> String<span>.</span><span>valueOf</span><span>(((</span>B<span>)</span>inner<span>).</span><span>b</span><span>());</span>
</span></span><span><span>    <span>}</span> <span>else</span> <span>if</span> <span>(</span>inner <span>instanceof</span> SomeOtherRecord<span>)</span> <span>{</span>
</span></span><span><span>      oldJavaResult <span>=</span> <span>null</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>  <span>}</span> <span>else</span> <span>if</span> <span>(</span>r <span>instanceof</span> B<span>)</span> <span>{</span>
</span></span><span><span>    oldJavaResult <span>=</span> String<span>.</span><span>valueOf</span><span>((</span>B<span>)</span>r<span>.</span><span>b</span><span>());</span>
</span></span><span><span>  <span>}</span> <span>else</span> <span>{</span>
</span></span><span><span>    oldJavaResult <span>=</span> <span>"r does not match any pattern"</span><span>;</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span>
</span></span><span><span>  System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"With the old method: \""</span> <span>+</span> oldJavaResult <span>+</span> <span>"\""</span><span>);</span>
</span></span><span><span>
</span></span><span><span>  <span>// The type is Record.
</span></span></span><span><span><span></span>  var result <span>=</span> <span>switch</span> <span>(</span>r<span>)</span> <span>{</span>
</span></span><span><span>    <span>case</span> A<span>(</span>B<span>(</span><span>char</span> a<span>))</span> <span>-&gt;</span> String<span>.</span><span>valueOf</span><span>(</span>a<span>);</span> <span>// Destructuring!
</span></span></span><span><span><span></span>    <span>case</span> A<span>(</span>SomeOtherRecord<span>(</span><span>/* ... */</span><span>))</span> <span>-&gt;</span> <span>{</span>
</span></span><span><span>      <span>// handle it.
</span></span></span><span><span><span></span>      yield <span>null</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>    <span>case</span> B<span>(</span><span>char</span> b<span>)</span> <span>-&gt;</span> String<span>.</span><span>valueOf</span><span>(</span>b<span>);</span>
</span></span><span><span>    <span>default</span> <span>-&gt;</span> <span>"r does not match any pattern"</span><span>;</span>
</span></span><span><span>  <span>};</span>
</span></span><span><span>  
</span></span><span><span>  System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span>result<span>.</span><span>toString</span><span>());</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>The switch block is clearly better structured than the if-else ladder above. Switch patterns are very powerful when you want to quickly and easily extract deeply nested data without waffling around with <code>instanceof</code> checks and cumbersome type casts. If you ever get a chance to work with Java 21 (May you be blessed with management that isn’t allergic to new Java versions), I’m sure you’ll appreciate this feature.</p>
<p>If you’d like to try this yourself, here’s how. Install Java 21 (There’s a handy <a href="https://aur.archlinux.org/packages/jdk21-jetbrains-bin"><code>jdk21-jetbrains-bin</code></a> package available in the AUR if anybody wants to immediately try it. I use Arch, btw). Copy this code into a <code>main.java</code> and run it with:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>java --enable-preview --source <span>21</span> main.java
</span></span></code></pre></div><p>This also showcases another nice new preview feature, <a href="https://openjdk.org/jeps/445">unnamed main methods</a>.</p>
<p>In the previous example, we were able to use pattern matching to switch over different record types. Now, let’s set that aside for a second and talk about how we manage choices in Java.</p>
<h2 id="choices-choices">Choices, choices…
<span>
    <a href="#choices-choices">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h2><p>What do you think of when you need a restricted set of alternatives to choose from? Java enums fit that bill; they are composed of a set of static variants and cannot change the data they contain inside.</p>
<div><pre tabindex="0"><code data-lang="java"><span><span><span>public</span> <span>enum</span> Color <span>{</span>
</span></span><span><span>  RED<span>(</span>255<span>,</span> 0<span>,</span> 0<span>),</span>
</span></span><span><span>  GREEN<span>(</span>0<span>,</span> 255<span>,</span> 0<span>),</span>
</span></span><span><span>  BLUE<span>(</span>0<span>,</span> 0<span>,</span> 255<span>);</span>
</span></span><span><span>
</span></span><span><span>  <span>public</span> <span>final</span> <span>int</span> red<span>;</span>
</span></span><span><span>  <span>public</span> <span>final</span> <span>int</span> green<span>;</span>
</span></span><span><span>  <span>public</span> <span>final</span> <span>int</span> blue<span>;</span>
</span></span><span><span>
</span></span><span><span>  Color<span>(</span><span>int</span> red<span>,</span> <span>int</span> green<span>,</span> <span>int</span> blue<span>)</span> <span>{</span>
</span></span><span><span>    <span>this</span><span>.</span><span>red</span> <span>=</span> red<span>;</span>
</span></span><span><span>    <span>this</span><span>.</span><span>green</span> <span>=</span> green<span>;</span>
</span></span><span><span>    <span>this</span><span>.</span><span>blue</span> <span>=</span> blue<span>;</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>The enum above defines three colours, red, green, and blue, with set values for the various fields, and it isn’t possible to change the colour values within without messing up every single place where this enum is used (The values are final in the code above, but imagine if they weren’t).</p>
<p>Now, imagine a different problem. You want different colour representations such as RGB, HSL, and CMYK. Maybe just make an enum for it?</p>
<div><pre tabindex="0"><code data-lang="java"><span><span><span>public</span> <span>enum</span> ColorRepresentation <span>{</span>
</span></span><span><span>  RGB<span>,</span>
</span></span><span><span>  HSL<span>,</span>
</span></span><span><span>  YUV<span>,</span>
</span></span><span><span>  CMYK
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>This gives us a nice, restricted set of values we can choose from. But it’s cumbersome to use; if you want to have multiple colour values for different representations, you’d need to store the actual colour data separately and keep a <code>ColorRepresentation</code> enum value within to help figure out what is actually going on…</p>
<div><pre tabindex="0"><code data-lang="java"><span><span><span>class</span> <span>Color</span> <span>{</span>
</span></span><span><span>  <span>public</span> <span>final</span> ColourRepresentation repr<span>;</span>
</span></span><span><span>  <span>public</span> <span>final</span> Number val1<span>;</span>
</span></span><span><span>  <span>public</span> <span>final</span> Number val2<span>;</span>
</span></span><span><span>  <span>public</span> <span>final</span> Number val3<span>;</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Obviously, this is NOT how anybody who knows Java would design the <code>Color</code> class. A much better way of implementing multiple colour representations without sacrificing readability is to use polymorphism!</p>








<div id="colour-abstract-class">
    
    <p><span>
            
        </span> 
        <span>Listing:&nbsp;<code>Color</code>&nbsp;implemented with an abstract class</span>
    </p>

    <hr>

    
</div>
<p>Now, given a <code>Color</code> instance, you’d just need to check if it is an <code>instanceof</code> your desired colour representation, and you’d be able to access data from that representation. But this implementation has a flaw. How do we restrict what a colour is in our class hierarchy? Any user of your library could create a new <code>RYB</code> class that inherits from <code>Color</code>, or from <code>RGB</code>, for example. This becomes a problem when your library does not expect any new variants of <code>Color</code> to exist or if it does not expect specific <code>Color</code> variants to change their behaviour. Unless the API is designed to be extensible, creating new representations could cause crashes in the best case (So you have a chance of knowing what went wrong) or in the worst case, subtle bugs that affect code far away from the problem’s source.</p>
<p>To fix this, we could do a few things:</p>
<ol>
<li>Make all variants <code>final</code>.
<ul>
<li>While this helps, it doesn’t preclude the creation of new variants directly from <code>Color</code>, since it’s impossible to make <code>Color</code> itself final.</li>
</ul>
</li>
<li>Ensure that all internal logic always has a default case for unrecognised variants.
<ul>
<li>This will reduce the extensibility of the library, but if that is not a goal, it will help.</li>
<li>But this will also be a lot more error-prone; if even one part of the logic forgets to account for the bad case, there’ll be problems.</li>
</ul>
</li>
<li>Or, we could use sealed classes or interfaces and kill two birds with one stone.</li>
</ol>
<h3 id="sum-types">Sum types
<span>
    <a href="#sum-types">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h3><p>Java 17’s sealed classes enable design patterns based on the concept of <em>Sum types</em>. Where a product type’s range of values is the product of the value ranges of its constituent types, a sum type’s range of values is the <em>sum</em>. Well, that was pretty obvious from the name… But what does it mean for the range of values to be a sum?</p>
<p>Sum types encode that a type can be <em>any one</em> of its constituents at a single time. They are also known as tagged union types because, in type theory, they are usually represented as a type whose range of values is the union set of its components, where each component type is “tagged” with a label.</p>
<p>You could express a sum type like this if you were to use my pseudo-type-theory notation:</p>
<pre tabindex="0"><code>T = A + B + C
</code></pre><p>The set of values that are in T could be expressed with this logical predicate:</p>
<pre tabindex="0"><code>T = { x | x ∈ A ⋃ B ⋃ C }
</code></pre><p>You may be reminded of <code>C</code>’s <code>unions</code> when you hear the term union types.</p>
<div><pre tabindex="0"><code data-lang="c"><span><span><span>union</span> MyUnion {
</span></span><span><span>    <span>int</span> intValue;
</span></span><span><span>    <span>double</span> doubleValue;
</span></span><span><span>    <span>char</span> charValue;
</span></span><span><span>};
</span></span></code></pre></div><p><code>MyUnion</code> is composed of three component types, and <code>C</code> allows you to treat a value of <code>MyUnion</code> as a container for any of these three types:</p>
<div><pre tabindex="0"><code data-lang="c"><span><span><span>union</span> MyUnion myUnion;
</span></span><span><span>myUnion.intValue <span>=</span> <span>42</span>;
</span></span><span><span><span>printf</span>(<span>"Integer value: %d</span><span>\n</span><span>"</span>, myUnion.intValue);
</span></span><span><span>myUnion.doubleValue <span>=</span> <span>3.14159</span>;
</span></span><span><span><span>printf</span>(<span>"Double value: %lf</span><span>\n</span><span>"</span>, myUnion.doubleValue);
</span></span></code></pre></div><p>Note that the value of the union is overwritten in the second assignment of <code>doubleValue</code>. If you were to print <code>myUnion.intValue</code> after the second assignment, you’d see gibberish; that’s actually just the bytes of <code>doubleValue</code> cut in half and interpreted as an integer.</p>
<p>This exposes the most significant flaw <code>C</code> unions have; there’s no built-in way to know which variant is contained within a union value without external information. Thus, we need an external <em>discriminant</em> to determine what’s inside. Java’s polymorphism can do that for us with <code>instanceof</code>. But Java’s class hierarchy is too open; there’s no way to restrict the number of variants of a Java “union”.</p>
<p>What we need are <em>tagged unions</em>, which is exactly what sealed types allow us to represent. The <code>sealed</code> modifier exists to make it clear that you can’t extend a sealed class beyond the classes allowed to inherit from it. This allows the developer to control how users interact with their library’s API.</p>








<div id="colour-sealed-class">
    
    <p><span>
            
        </span> 
        <span>Listing:&nbsp;<code>Color</code>&nbsp;implemented with a sealed class</span>
    </p>

    <hr>

    
</div>
<p>Note the syntax of the sealed class <code>Color</code>. There is a <code>sealed</code> modifier and a <code>permits</code> clause with the names of all the subclasses of <code>Color</code>. <code>permits</code> is used to specify which classes get to inherit from a particular class and is used to prevent any unwanted inheritance. Note also that each implementation of <code>Color</code> is marked as <code>final</code>, so you only get the four colour representations you see here; you can’t make your own.</p>
<p>This class hierarchy is locked down. No new classes can inherit from <code>Color</code>, whether they are located in the same package or not.</p>
<p>If your class hierarchy starts with a <code>sealed</code> class, you must mark all inheritors (direct or indirect) as <code>sealed</code>, <code>non-sealed</code>, or <code>final</code>. If an inheriting class doesn’t have these modifiers, it is a compile error.</p>
<p>Here’s what each of these modifiers mean:</p>
<ul>
<li><code>sealed</code> - The class cannot be inherited unless the inheritor’s name is mentioned after <code>permits</code>.</li>
<li><code>non-sealed</code> - The class can be inherited normally. Helpful in controlling the scope of custom behaviours.</li>
<li><code>final</code> - The class is a “leaf” in the inheritance tree; you can’t extend it anymore.</li>
</ul>
<p>These modifiers can be used to control exactly how the classes of an API can behave and how they are used. This avoids situations where the only thing stopping everything from exploding are rules that can only be enforced by developers agreeing to not do the wrong thing.</p>
<p>Imagine we’re writing code to handle colours based on their format. We already have a nice, restricted way of ensuring there’ll be no funny business. But how would we structure the code that handles the sealed class instances?</p>
<p>Earlier Java versions wouldn’t give us much of a choice; the best we can manage is an if-else ladder:</p>
<div><pre tabindex="0"><code data-lang="java"><span><span>Color color <span>=</span> <span>new</span> RGB<span>(</span>255<span>,</span> 0<span>,</span> 0<span>);</span>
</span></span><span><span>
</span></span><span><span><span>if</span> <span>(</span>color <span>instanceof</span> RGB<span>)</span> <span>{</span>
</span></span><span><span>  RGB rgb <span>=</span> <span>(</span>RGB<span>)</span> color<span>;</span>
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span><span>}</span> <span>else</span> <span>if</span> <span>(</span>color <span>instanceof</span> CMYK<span>)</span> <span>{</span>
</span></span><span><span>  CMYK cmyk <span>=</span> <span>(</span>CMYK<span>)</span> color<span>;</span>
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span><span>}</span> <span>else</span> <span>if</span> <span>(</span>color <span>instanceof</span> YUV<span>)</span> <span>{</span>
</span></span><span><span>  YUV yuv <span>=</span> <span>(</span>YUV<span>)</span> color<span>;</span>
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span><span>}</span> <span>else</span> <span>if</span> <span>(</span>color <span>instanceof</span> HSL<span>)</span> <span>{</span>
</span></span><span><span>  HSL hsl <span>=</span> <span>(</span>HSL<span>)</span> color<span>;</span>
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span><span>}</span> <span>else</span> <span>{</span>
</span></span><span><span>  System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Unknown color type"</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>We could do a bit better if we’re on Java 16+ by using <code>if</code> pattern matching:</p>
<div><pre tabindex="0"><code data-lang="java"><span><span><span>if</span> <span>(</span>color <span>instanceof</span> RGB rgb<span>)</span> <span>{</span>
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span><span>}</span> <span>else</span> <span>if</span> <span>(</span>color <span>instanceof</span> CMYK cmyk<span>)</span> <span>{</span>
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span><span>}</span> <span>else</span> <span>if</span> <span>(</span>color <span>instanceof</span> YUV yuv<span>)</span> <span>{</span>
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span><span>}</span> <span>else</span> <span>if</span> <span>(</span>color <span>instanceof</span> HSL hsl<span>)</span> <span>{</span>
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span><span>}</span> <span>else</span> <span>{</span>
</span></span><span><span>  System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Unknown color type"</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>But wouldn’t it be nice to switch on a colour variable and extract the contents at the same time as well the way Rust can?</p>
<p>Rust would let you do this, for example:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span><span>let</span> color <span>=</span> Color::<span>RGB</span>(<span>255</span>, <span>0</span>, <span>0</span>);
</span></span><span><span>
</span></span><span><span><span>match</span> color {
</span></span><span><span>  Color::<span>RGB</span>(red, green, blue) <span>=&gt;</span> {
</span></span><span><span>    <span>// ...
</span></span></span><span><span><span></span>  }
</span></span><span><span>  Color::<span>CMYK</span>(cyan, magenta, yellow, black) <span>=&gt;</span> {
</span></span><span><span>    <span>// ...
</span></span></span><span><span><span></span>  }
</span></span><span><span>  Color::<span>YUV</span>(y, u, v) <span>=&gt;</span> {
</span></span><span><span>    <span>// ...
</span></span></span><span><span><span></span>  }
</span></span><span><span>  Color::<span>HSL</span>(hue, saturation, lightness) <span>=&gt;</span> {
</span></span><span><span>    <span>// ...
</span></span></span><span><span><span></span>  }
</span></span><span><span>}
</span></span></code></pre></div><p>Note that these colour values are being destructured in the <code>match</code> block above. How would we get that done with sealed classes? Switch pattern matching with destructuring only works on records, and records can’t inherit from any class but <code>Record</code>…</p>
<p>The solution is to just use sealed interfaces. They work in exactly the same way as sealed classes, except even records and enums can implement them.</p>
<div><pre tabindex="0"><code data-lang="java"><span><span><span>public</span> sealed <span>interface</span> <span>Color</span> permits RGB<span>,</span> CMYK<span>,</span> YUV<span>,</span> HSL <span>{</span>
</span></span><span><span>    <span>// Common properties or methods for all color representations
</span></span></span><span><span><span></span>    String <span>getDescription</span><span>();</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span>record <span>RGB</span><span>(</span><span>int</span> red<span>,</span> <span>int</span> green<span>,</span> <span>int</span> blue<span>)</span> <span>implements</span> Color <span>{</span>
</span></span><span><span>    <span>public</span> String <span>getDescription</span><span>()</span> <span>{</span>
</span></span><span><span>        <span>return</span> <span>"RGB Color: ("</span> <span>+</span> red <span>+</span> <span>", "</span> <span>+</span> green <span>+</span> <span>", "</span> <span>+</span> blue <span>+</span> <span>")"</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span>record <span>CMYK</span><span>(</span><span>double</span> cyan<span>,</span> <span>double</span> magenta<span>,</span> <span>double</span> yellow<span>,</span> <span>double</span> black<span>)</span> <span>implements</span> Color <span>{</span>
</span></span><span><span>    <span>public</span> String <span>getDescription</span><span>()</span> <span>{</span>
</span></span><span><span>        <span>return</span> <span>"CMYK Color: ("</span> <span>+</span> cyan <span>+</span> <span>"%, "</span> <span>+</span> magenta <span>+</span> <span>"%, "</span> <span>+</span> yellow <span>+</span> <span>"%, "</span> <span>+</span> black <span>+</span> <span>"%)"</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span>record <span>YUV</span><span>(</span><span>int</span> y<span>,</span> <span>int</span> u<span>,</span> <span>int</span> v<span>)</span> <span>implements</span> Color <span>{</span>
</span></span><span><span>    <span>public</span> String <span>getDescription</span><span>()</span> <span>{</span>
</span></span><span><span>        <span>return</span> <span>"YUV Color: (Y="</span> <span>+</span> y <span>+</span> <span>", U="</span> <span>+</span> u <span>+</span> <span>", V="</span> <span>+</span> v <span>+</span> <span>")"</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span>record <span>HSL</span><span>(</span><span>double</span> hue<span>,</span> <span>double</span> saturation<span>,</span> <span>double</span> lightness<span>)</span> <span>implements</span> Color <span>{</span>
</span></span><span><span>    <span>public</span> String <span>getDescription</span><span>()</span> <span>{</span>
</span></span><span><span>        <span>return</span> <span>"HSL Color: (H="</span> <span>+</span> hue <span>+</span> <span>", S="</span> <span>+</span> saturation <span>+</span> <span>"%, L="</span> <span>+</span> lightness <span>+</span> <span>"%)"</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>And here’s how you can neatly pattern-match and extract the data out of a <code>Color</code> instance, which is nice if all you want to do is get the data out of the class without calling any methods on it:</p>
<div><pre tabindex="0"><code data-lang="java"><span><span>Color color <span>=</span> <span>new</span> RGB<span>(</span>255<span>,</span> 0<span>,</span> 0<span>);</span>
</span></span><span><span>
</span></span><span><span><span>switch</span> <span>(</span>color<span>)</span> <span>{</span>
</span></span><span><span>  <span>case</span> RGB<span>(</span><span>int</span> red<span>,</span> <span>int</span> green<span>,</span> <span>int</span> blue<span>)</span> <span>-&gt;</span> <span>{</span>
</span></span><span><span>    <span>// red, green and blue are in scope here.
</span></span></span><span><span><span></span>  <span>}</span>
</span></span><span><span>  <span>case</span> CMYK<span>(</span><span>double</span> cyan<span>,</span> <span>double</span> magenta<span>,</span> <span>double</span> yellow<span>,</span> <span>double</span> black<span>)</span> <span>-&gt;</span> <span>{</span>
</span></span><span><span>    <span>// ...
</span></span></span><span><span><span></span>  <span>}</span>
</span></span><span><span>  <span>case</span> YUV<span>(</span><span>int</span> y<span>,</span> <span>int</span> u<span>,</span> <span>int</span> v<span>)</span> <span>-&gt;</span> <span>{</span>
</span></span><span><span>    <span>// ...
</span></span></span><span><span><span></span>  <span>}</span>
</span></span><span><span>  <span>case</span> HSL hsl <span>-&gt;</span> <span>{</span>
</span></span><span><span>    <span>// you can also leave the value intact and directly use the pattern matched value.
</span></span></span><span><span><span></span>    System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span>hsl<span>.</span><span>getDescription</span><span>());</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span>  <span>case</span> <span>null</span> <span>-&gt;</span> <span>{</span>
</span></span><span><span>    System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"How did color become null?!"</span><span>);</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><div>
    <h3>Note</h3>
    <p>
        Java 21 allows you to catch the <code>null</code> case within <code>switch</code> blocks and expressions now, so you don’t need to precheck for <code>null</code> before you get to a switch.
    </p>
</div>
<p>You may notice we aren’t using a default case here. Java would have normally raised an error stating that all cases haven’t been covered. However, because <code>Color</code> is a sealed class, Java can tell that every case has been handled.</p>
<h3 id="guards-guards">Guards! Guards!
<span>
    <a href="#guards-guards">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h3><p>Did I mention we’ve got guard clauses? Guard clauses! You can attach additional conditions to switch arms that have to be true for the arm to be executed! Guard clauses allow the succinct expression of more complex conditions within switch statements and expressions. No more extremely nested <code>if</code> conditions in your switches.</p>
<p>Consider a situation where we need to special-case RGB colours where <code>red &gt; 200</code> is true. Before, we had to put that condition inside an <code>if</code> condition within the corresponding case’s body:</p>
<div><pre tabindex="0"><code data-lang="java"><span><span><span>switch</span> <span>(</span>color<span>)</span> <span>{</span>
</span></span><span><span>  <span>case</span> RGB<span>(</span><span>int</span> red<span>,</span> <span>int</span> green<span>,</span> <span>int</span> blue<span>)</span> <span>-&gt;</span> <span>{</span>
</span></span><span><span>    <span>if</span> <span>(</span>red <span>&gt;</span> 200<span>)</span> <span>{</span>
</span></span><span><span>      System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Very red."</span><span>);</span>
</span></span><span><span>    <span>}</span> <span>else</span> <span>{</span>
</span></span><span><span>      System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Not that red..."</span><span>);</span>  
</span></span><span><span>    <span>}</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span><span>}</span>
</span></span></code></pre></div><p>That isn’t the ugliest thing ever, but it still means you end up nesting your code a bit more in the long run. It’s generally easier to parse code that stretches out vertically than horizontally since there are fewer scopes to keep track of.</p>
<p>Java 21 allows us to integrate that condition within the case label with the <code>when</code> keyword.</p>
<div><pre tabindex="0"><code data-lang="java"><span><span><span>switch</span> <span>(</span>color<span>)</span> <span>{</span>
</span></span><span><span>  <span>// A guarded destructuring case.
</span></span></span><span><span><span></span>  <span>case</span> RGB<span>(</span><span>int</span> red<span>,</span> <span>int</span> green<span>,</span> <span>int</span> blue<span>)</span> when red <span>&gt;</span> 200  <span>-&gt;</span> <span>{</span>
</span></span><span><span>    System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Very red."</span><span>);</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span>  <span>// You can use guards with direct pattern matching as well.
</span></span></span><span><span><span></span>  <span>case</span> RGB rgb when rgb<span>.</span><span>green</span> <span>&gt;</span> 100 <span>-&gt;</span> <span>{</span>
</span></span><span><span>    System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Sort of green..."</span><span>);</span>
</span></span><span><span>    <span>// ...  
</span></span></span><span><span><span></span>  <span>}</span>
</span></span><span><span>  <span>// this case is needed to preserve exhaustivity.
</span></span></span><span><span><span></span>  <span>case</span> RGB rgb <span>-&gt;</span> <span>{</span>
</span></span><span><span>    System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Not that red..."</span><span>);</span>
</span></span><span><span>    <span>// ...  
</span></span></span><span><span><span></span>  <span>}</span>
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span><span>}</span>
</span></span></code></pre></div><div>
    <h3>Note</h3>
    <p>
        Java will still eagerly match whichever case evaluates to true first, so make sure to put more specific cases (guarded or not) first, followed by less specific ones.
    </p>
</div>
<h2 id="ugh-exceptions-ft-an-example-from-jep-441">Ugh, exceptions (ft. an example from JEP 441)
<span>
    <a href="#ugh-exceptions-ft-an-example-from-jep-441">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h2><p>We have a new class of exceptions to deal with now. Specifically, <a href="https://download.java.net/java/early_access/jdk21/docs/api/java.base/java/lang/MatchException.html"><code>java.lang.MatchException</code></a><sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>.</p>
<p>What happens when a pattern match goes wrong? Consider the case of a bad record getter implementation:</p>
<div><pre tabindex="0"><code data-lang="java"><span><span>record <span>R</span><span>(</span><span>int</span> i<span>)</span> <span>{</span>
</span></span><span><span>    <span>public</span> <span>int</span> <span>i</span><span>()</span> <span>{</span>    <span>// bad (but legal) accessor method for i
</span></span></span><span><span><span></span>        <span>return</span> i <span>/</span> 0<span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>static</span> <span>void</span> <span>exampleAnR</span><span>(</span>R r<span>)</span> <span>{</span>
</span></span><span><span>    <span>switch</span><span>(</span>r<span>)</span> <span>{</span>
</span></span><span><span>        <span>case</span> R<span>(</span>var i<span>):</span> System<span>.</span><span>out</span><span>.</span><span>println</span><span>(</span>i<span>);</span> <span>// i's accessor always throws!
</span></span></span><span><span><span></span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>The switch block above will throw a <code>MatchException</code> because when i’s getter is called, an <code>ArithmeticException</code> is thrown.</p>
<p><a href="https://openjdk.org/jeps/441">JEP 441</a> states:</p>
<blockquote>
<p>(A record accessor method which always throws an exception is highly irregular, and an exhaustive pattern switch which throws a MatchException is highly unusual.)</p>
</blockquote>
<p>Exhaustive switch blocks will throw if none of the specified variants can match the selector. Regarding this, the JEP states:</p>
<blockquote>
<p>(An exhaustive switch over an enum fails to match only if the enum class is changed after the switch has been compiled, which is highly unusual.)</p>
</blockquote>
<p>A <code>MatchException</code> will also be thrown if a guard clause raises an exception when executed.</p>
<div><pre tabindex="0"><code data-lang="java"><span><span><span>static</span> <span>void</span> <span>example</span><span>(</span>Object obj<span>)</span> <span>{</span>
</span></span><span><span>    <span>switch</span> <span>(</span>obj<span>)</span> <span>{</span>
</span></span><span><span>        <span>case</span> R r <span>when</span> <span>(</span>r<span>.</span><span>i</span> <span>/</span> 0 <span>==</span> 1<span>):</span> System<span>.</span><span>out</span><span>.</span> <span>println</span><span>(</span><span>"It's an R!"</span><span>);</span>
</span></span><span><span>        <span>default</span><span>:</span> <span>break</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>The example here is very easy to spot; we can easily statically determine that there’s a divide-by-zero error here. However, the waters would be muddier if the dividend were to be a dynamic, possibly <code>0</code> value.</p>
<p>As my late grandfather (A doctor) always said:</p>
<blockquote>
<p>Divide by zero, not even once.</p>
</blockquote>
<h2 id="conclusion">Conclusion
<span>
    <a href="#conclusion">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"></path></svg>
    </a>
</span>
</h2><p>In this article, we’ve looked at a bunch of things that Java 21 allows us to do (I haven’t covered certain things like how generics interact with switch patterns, however). In the next one, I’ll show you some interesting quirks and a few practical examples of how we can leverage these functional building blocks to improve how we write Java code.</p>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Every Breath You Take – Heart Rate Variability Training (296 pts)]]></title>
            <link>https://github.com/kbre93/every-breath-you-take</link>
            <guid>37538028</guid>
            <pubDate>Sat, 16 Sep 2023 19:48:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kbre93/every-breath-you-take">https://github.com/kbre93/every-breath-you-take</a>, See on <a href="https://news.ycombinator.com/item?id=37538028">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Every Breath You Take – Heart Rate Variability Training with the Polar H10 Monitor</h2>
<p dir="auto">Through controlled breathing it is possible to regulate your body's stress reponse. This application allows you to measure and train this effect with a Polar H10 Heart Rate monitor.</p>
<p dir="auto">Heart rate variability, the small changes in heart rate from beat-to-beat, is a reliable measure of stress response. Heart rate variability reflects the balance between the two sides of the autonomic nervous system: the fight-or-flight response (from the sympathetic nervous system) and the rest-and-digest response (from the parasympathetic nervous system).</p>
<p dir="auto">In any moment it is possible to restore balance to the autonomic nervous system by breathing slower and deeper. With every breath you take, you can set the pace of your breathing rate, measure your breathing control with the chest accelerometer, and see how heart rate variability responds.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kbre93/every-breath-you-take/blob/master/img/screen_record.gif"><img src="https://github.com/kbre93/every-breath-you-take/raw/master/img/screen_record.gif" alt="" data-animated-image=""></a></p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul dir="auto">
<li>Connect and stream from a Polar H10, acceleration and heart rate data</li>
<li>Live breathing control feedback and adjustable pace setting</li>
<li>Track breathing and heart rate oscillations in real-time</li>
<li>Explore how heart rate vairability repsonses to different breathing rates</li>
</ul>
<h2 tabindex="-1" dir="auto">Installation and usage</h2>
<div data-snippet-clipboard-copy-content="python -m venv venv
source venv/bin/activate  # On Windows, use `my_project_env\Scripts\activate`
pip install -r requirements.txt
python EBYT.py "><pre><code>python -m venv venv
source venv/bin/activate  # On Windows, use `my_project_env\Scripts\activate`
pip install -r requirements.txt
python EBYT.py 
</code></pre></div>
<p dir="auto">The program will automatically connect to your Polar device. For best breathing detection, ensure the Polar H10 is fitted around the widest part of the ribcage, stay seated and still while recording.</p>
<p dir="auto">Set the breathing pace with the slider (in breaths per minute), and follow the cadence as the gold circle expands and contracts. The blue circle shows your breathing control.</p>
<p dir="auto">Track each breath cycle in the top graph, and how heart rate oscillates in repsonse.</p>
<p dir="auto">Adjust breathing pace and control to target the green zone of heart rate variability in the bottom graph (&gt; 150 ms).</p>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">Feedback, bug reports, and pull requests are welcome. Feel free to submit an issue or create a pull request on GitHub.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running one’s own root Certificate Authority in 2023 (181 pts)]]></title>
            <link>https://wejn.org/2023/09/running-ones-own-root-certificate-authority-in-2023/</link>
            <guid>37537689</guid>
            <pubDate>Sat, 16 Sep 2023 19:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wejn.org/2023/09/running-ones-own-root-certificate-authority-in-2023/">https://wejn.org/2023/09/running-ones-own-root-certificate-authority-in-2023/</a>, See on <a href="https://news.ycombinator.com/item?id=37537689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><span>Written</span>
    

    
      <span>on&nbsp;</span><time datetime="2023-09-16 14:23:00 +0200">2023-09-16</time>
    
    
  </p>

  

  <br>

  

  <h2 id="problem-statement">Problem statement</h2>
<p>Anyone wanting their own X509 cert these days has free-beer alternatives
like ZeroSSL or Let’s Encrypt.</p>
<p>But, what if it’s just for internal services, some of them even cut off
from the ‘Net? And more importantly, what if you don’t wish to be bothered
with juggling the renewals every 3 months, or want that sweet wildcard
x509 cert with minimum hassle?</p>
<p>Well then… how about the age-old solution of rolling out your own root
CA? Ideally one that will be accepted without issue by major browsers,
including the ones on iOS.</p>
<p>Let’s dig in.</p>
<h2 id="background--history-lesson">Background / history lesson</h2>
<p>Way back when I was just a little bitty boy… we used to do this whole
shindig with <code>sign.sh -- Sign a SSL Certificate Request (CSR)</code> script
and a modicum of <code>openssl genrsa</code>, <code>openssl req -new -x509</code>,
<code>openssl req -new</code> elbow grease.</p>
<p>If the copyright line<sup><a href="#fn1" id="fnref1">1</a></sup> is to be believed, that thing predates the
height of the Y2K mania.</p>
<p>And there isn’t that much wrong with it even now, save for a questionable
choice of hash functions.</p>
<p>Or so I thought, as I was running that thing with <code>default_days = 3650</code>
for the past many many years. And while that continues to work fine in
Firefox on Linux to this day<sup><a href="#fn2" id="fnref2">2</a></sup>, you’re bound to hit a brick wall when
trying to use server certificates generated that way with Apple devices<sup><a href="#fn3" id="fnref3">3</a></sup>.</p>
<p>Turns out<sup><a href="#fn4" id="fnref4">4</a></sup>, Apple <a href="https://support.apple.com/en-us/102028">really doesn’t want you to use server certs that
are valid for longer than 398 days</a><sup><a href="#fn5" id="fnref5">5</a></sup>,
along with a <a href="https://support.apple.com/en-us/HT210176">plethora of other restrictions</a>.
tldr: 2048bit+, SHA2 digest, CN ignored, altnames is king, keyusage=serverAuth.</p>
<p>And with that, here’s what works as of the time of writing… in Firefox
and on recent MacOS/iOS.</p>
<h2 id="solution">Solution</h2>
<p>These “make your own x509 root cert” guides are a dime a dozen. And if you’re
lazy, go use <a href="https://github.com/FiloSottile/mkcert">mkcert</a> instead. It might
even work out of the box<sup><a href="#fn6" id="fnref6">6</a></sup>.</p>
<p>If you’re like me, and simply want to take the road less traveled by (because
understanding all this stuff makes all the difference), here’s the requirements:</p>
<ol>
<li>a x509 cert for the certificate authority</li>
<li>a x509 cert for every internal service, or maybe just a singleton
<code>*.int.wejn.org</code><sup><a href="#fn7" id="fnref7">7</a></sup></li>
<li>a way to serve the CA certificate somewhere</li>
</ol>
<p>The last one is out of scope, but basically you need a static website that
spits out the PEM-encoded certificate with <code>application/x-x509-ca-cert</code>
mime type. More on that some other time.</p>
<p>Viewing from the top, I want something like:</p>
<div><pre><code><span># _gen_all.sh</span>
<span>## generate CA key+cert</span>
./cacert.sh
<span>## generate host key+cert for a single host</span>
./hostcert.sh snowflake.int.wejn.org
<span>## generate wildcard host key+cert with two alt names</span>
./hostcert.sh int.wejn.org <span>'*.int.wejn.org'</span>
</code></pre></div>
<p>that in the end produces <code>ca.crt</code>, <code>ca.key</code>, <code>snowflake.int.wejn.org.{crt,key}</code>,
<code>int.wejn.org.{crt,key}</code> in working order<sup><a href="#fn8" id="fnref8">8</a></sup>.</p>
<h3 id="generating-the-ca-certificate">Generating the CA certificate</h3>
<p>The certificate is a two-parter:</p>
<ol>
<li>an RSA key</li>
<li>x509 certificate with proper extensions</li>
</ol>
<p>Here’s what worked for me (<code>cacert.sh</code>):</p>
<div><pre><code><span>#!/bin/bash</span>

<span>if</span> <span>[</span> <span>-f</span> <span>"ca.cnf"</span> <span>]</span><span>;</span> <span>then
        </span><span>echo</span> <span>"CA already exists."</span>
        <span>exit </span>1
<span>fi

</span><span>umask </span>066

<span># Generate a CA password, because openssl (reasonably) wants to protect</span>
<span># the key material... and dump it to `ca.pass`.</span>
<span>export </span><span>CAPASS</span><span>=</span><span>$(</span>xkcdpass <span>-n</span> 64<span>)</span>

<span>if</span> <span>[</span> <span>-z</span> <span>"</span><span>$CAPASS</span><span>"</span> <span>]</span><span>;</span> <span>then
        </span><span>echo</span> <span>"Error: password empty; no xkcdpass?"</span>
        <span>exit </span>1
<span>fi

</span><span>echo</span> <span>"</span><span>$CAPASS</span><span>"</span> <span>&gt;</span> <span>"ca.pass"</span>

<span># Generate the 4096 bit RSA key for the CA</span>
openssl genrsa <span>-aes256</span> <span>-passout</span> <span>env</span>:CAPASS  <span>-out</span> <span>"ca.key"</span> 4096

<span># Strip the encryption off it; IOW, now they're are two things worth</span>
<span># protecting -- the `ca.pass` and `ca.key.unsecure`.</span>
openssl rsa <span>-in</span> <span>"ca.key"</span> <span>-passin</span> <span>env</span>:CAPASS <span>-out</span> <span>"ca.key.unsecure"</span>

<span># At this point, you can decide whether to memorize `ca.pass` and</span>
<span># delete it along with `ca.key.unsecure`, or protect `ca.key.unsecure`</span>
<span># with your life, and maybe forget all about `ca.key` and `ca.pass`.</span>
<span>#</span>
<span># (I'm sure you would have no trouble rewriting this to do away with</span>
<span># the `ca.pass` and `xkcdpass` dependency altogether)</span>

<span># Configure the CSR with necessary fields</span>
<span>cat</span> <span>&gt;</span> <span>"ca.cnf"</span> <span>&lt;&lt;</span><span>'</span><span>EOF</span><span>'
[ req ]
x509_extensions = v3_req
distinguished_name = req_distinguished_name
prompt = no

[ v3_req ]
# This is the money shot -- we are the cert authority (CA:TRUE),
# and there are no other CAs below us in the chain (pathlen:0),
# and the constraint is non-negotiable (critical)
basicConstraints = critical, CA:TRUE, pathlen:0

## This is optional but maybe needed for some platforms
#keyUsage = nonRepudiation, digitalSignature, keyEncipherment
#extendedKeyUsage = serverAuth, clientAuth, emailProtection

[ req_distinguished_name ]
C = CH
L = Zurich
O = int.wejn.org CA
CN = ca.int.wejn.org
emailAddress = ca@int.wejn.org
</span><span>EOF

</span><span># Do the deed -- generate the `ca.crt`, with 10 year (3650 days) validity</span>
openssl req <span>-new</span> <span>-x509</span> <span>-days</span> 3650 <span>-sha512</span> <span>-passin</span> <span>env</span>:CAPASS <span>-config</span> ca.cnf <span>\</span>
        <span>-key</span> ca.key <span>-out</span> ca.crt <span>-text</span>
</code></pre></div>
<h3 id="generating-the-host-certificates">Generating the host certificates</h3>
<p>I’m going to assume we stick with the <code>ca.pass</code> and <code>ca.key</code> above, just to
make the <code>sign.sh</code> work as originally written<sup><a href="#fn9" id="fnref9">9</a></sup>.</p>
<p>To generate a host certificate signed by the CA, we need:</p>
<ol>
<li>an RSA key</li>
<li>a certificate signing request (CSR)</li>
<li>sign the CSR with the CA key</li>
</ol>
<p>Here’s my take on <code>hostcert.sh</code>:</p>
<div><pre><code><span>#!/bin/bash</span>

<span># Read the CA password, used by `sign.sh` later</span>
<span>export </span><span>CAPASS</span><span>=</span><span>$(</span><span>cat </span>ca.pass<span>)</span>

<span>if</span> <span>[</span> <span>-f</span> <span>"</span><span>$1</span><span>.cnf"</span> <span>]</span><span>;</span> <span>then
        </span><span>echo</span> <span>"Host: </span><span>$1</span><span> already exists."</span>
        <span>exit </span>1
<span>fi

if</span> <span>[</span> <span>-z</span> <span>"</span><span>$1</span><span>"</span> <span>]</span><span>;</span> <span>then
        </span><span>echo</span> <span>"Error: No hostname given"</span>
        <span>exit </span>1
<span>fi

</span><span>umask </span>066

<span># Generate the certificate's password, and dump it.</span>
<span>export </span><span>PASS</span><span>=</span><span>$(</span>xkcdpass <span>-n</span> 64<span>)</span>

<span>if</span> <span>[</span> <span>-z</span> <span>"</span><span>$PASS</span><span>"</span> <span>]</span><span>;</span> <span>then
        </span><span>echo</span> <span>"Error: password empty; no xkcdpass?"</span>
        <span>exit </span>1
<span>fi

</span><span>echo</span> <span>"</span><span>$PASS</span><span>"</span> <span>&gt;</span> <span>"</span><span>$1</span><span>.pass"</span>

<span># Figure out what the hostname / altnames are, and confirm.</span>
<span>echo</span> <span>"</span><span>$1</span><span>"</span> | fgrep <span>-q</span> <span>"."</span>
<span>if</span> <span>[</span> <span>$?</span> <span>-eq</span> 0 <span>]</span><span>;</span> <span>then
        </span><span>CN</span><span>=</span><span>"</span><span>$1</span><span>"</span>
        <span>ALTNAMES</span><span>=</span><span>"</span><span>$@</span><span>"</span>
<span>else
        </span><span>CN</span><span>=</span><span>"</span><span>$1</span><span>.int.wejn.org"</span>
        <span>ALTNAMES</span><span>=</span><span>"</span><span>$1</span><span>.int.wejn.org"</span>
<span>fi
</span><span>echo</span> <span>"CN: </span><span>$CN</span><span>"</span>
<span>echo</span> <span>"ANs: </span><span>$ALTNAMES</span><span>"</span>
<span>echo</span> <span>"Enter to confirm."</span>
<span>read </span>A

<span># Generate the RSA key, unlock it into the "unsecure" file</span>
openssl genrsa <span>-aes256</span> <span>-passout</span> <span>env</span>:PASS  <span>-out</span> <span>"</span><span>$1</span><span>.key"</span> <span>${</span><span>SSL_KEY_SIZE</span><span>-4096</span><span>}</span>
openssl rsa <span>-in</span> <span>"</span><span>$1</span><span>.key"</span> <span>-passin</span> <span>env</span>:PASS <span>-out</span> <span>"</span><span>$1</span><span>.key.unsecure"</span>

<span># Construct the CSR data</span>
<span>cat</span> <span>&gt;</span> <span>"</span><span>$1</span><span>.cnf"</span> <span>&lt;&lt;</span><span>EOF</span><span>
[ req ]
req_extensions = v3_req
distinguished_name = req_distinguished_name
prompt = no

[ v3_req ]
# We are NOT a CA, this is for server auth, and these are the altnames
basicConstraints = critical,CA:FALSE
# We are, however, a certificate for server authentication (important!)
extendedKeyUsage=serverAuth
subjectAltName = @alt_names

[alt_names]
</span><span>EOF

</span><span>I</span><span>=</span>1
<span>for </span>AN <span>in</span> <span>$ALTNAMES</span><span>;</span> <span>do
        </span><span>echo</span> <span>"DNS.</span><span>$I</span><span> = </span><span>$AN</span><span>"</span> <span>&gt;&gt;</span> <span>"</span><span>$1</span><span>.cnf"</span>
        <span>I</span><span>=</span><span>$[$I</span> + 1]
<span>done

</span><span>cat</span> <span>&gt;&gt;</span> <span>"</span><span>$1</span><span>.cnf"</span> <span>&lt;&lt;</span><span>EOF</span><span>

[ req_distinguished_name ]
C = CH
L = Zurich
O = int.wejn.org host cert
CN = </span><span>$CN</span><span>
</span><span>EOF

</span><span># Create the CSR</span>
openssl req <span>-new</span> <span>-key</span> <span>"</span><span>$1</span><span>.key"</span> <span>-sha512</span> <span>-passin</span> <span>env</span>:PASS <span>-config</span> <span>"</span><span>$1</span><span>.cnf"</span> <span>\</span>
        <span>-out</span> <span>"</span><span>$1</span><span>.csr"</span>

<span># Sign the CSR by the CA, resulting in `$1.crt`; needs env;CAPASS</span>
./sign.sh <span>"</span><span>$1</span><span>.csr"</span>

<span># Optional: put both cert and key into a single `$1.pem` file</span>
<span>#ruby -pe 'next unless /BEGIN/../END/' "$1.crt" "$1.key.unsecure" &gt; "$1.pem"</span>
</code></pre></div>
<p>And, of course, the current <code>sign.sh</code>, with a few comments:</p>
<div><pre><code><span>#!/bin/sh</span>
<span>##</span>
<span>##  sign.sh -- Sign a SSL Certificate Request (CSR)</span>
<span>##  Copyright (c) 1998-2001 Ralf S. Engelschall, All Rights Reserved. </span>
<span>##</span>

<span>#   argument line handling</span>
<span>CSR</span><span>=</span><span>$1</span>
<span>if</span> <span>[</span> <span>$# </span><span>-ne</span> 1 <span>]</span><span>;</span> <span>then
    </span><span>echo</span> <span>"Usage: sign.sign &lt;whatever&gt;.csr"</span><span>;</span> <span>exit </span>1
<span>fi
if</span> <span>[</span> <span>!</span> <span>-f</span> <span>$CSR</span> <span>]</span><span>;</span> <span>then
    </span><span>echo</span> <span>"CSR not found: </span><span>$CSR</span><span>"</span><span>;</span> <span>exit </span>1
<span>fi
case</span> <span>$CSR</span> <span>in</span>
   <span>*</span>.csr <span>)</span> <span>CERT</span><span>=</span><span>"</span><span>`</span><span>echo</span> <span>$CSR</span> | <span>sed</span> <span>-e</span> <span>'s/\.csr/.crt/'</span><span>`</span><span>"</span> <span>;;</span>
       <span>*</span> <span>)</span> <span>CERT</span><span>=</span><span>"</span><span>$CSR</span><span>.crt"</span> <span>;;</span>
<span>esac</span>

<span>#   make sure environment exists</span>
<span>if</span> <span>[</span> <span>!</span> <span>-d</span> ca.db.certs <span>]</span><span>;</span> <span>then
    </span><span>mkdir </span>ca.db.certs
<span>fi
if</span> <span>[</span> <span>!</span> <span>-f</span> ca.db.serial <span>]</span><span>;</span> <span>then
    </span><span>echo</span> <span>'01'</span> <span>&gt;</span>ca.db.serial
<span>fi
if</span> <span>[</span> <span>!</span> <span>-f</span> ca.db.index <span>]</span><span>;</span> <span>then
    </span><span>cp</span> /dev/null ca.db.index
<span>fi</span>

<span>#   create an own SSLeay config</span>
<span>cat</span> <span>&gt;</span>ca.config <span>&lt;&lt;</span><span>EOT</span><span>
[ ca ]
default_ca              = CA_own
[ CA_own ]
dir                     = .
certs                   = </span><span>\$</span><span>dir
new_certs_dir           = </span><span>\$</span><span>dir/ca.db.certs
database                = </span><span>\$</span><span>dir/ca.db.index
serial                  = </span><span>\$</span><span>dir/ca.db.serial
RANDFILE                = </span><span>\$</span><span>dir/ca.db.rand
certificate             = </span><span>\$</span><span>dir/ca.crt
private_key             = </span><span>\$</span><span>dir/ca.key
# all hail our fruity overlords:
default_days            = 365
default_crl_days        = 30
# need sane message digest, too:
default_md              = sha512
preserve                = no
policy                  = policy_anything
copy_extensions         = copy
x509_extensions         = v3
[ v3 ]
basicConstraints = critical, CA:FALSE

[ policy_anything ]
countryName             = optional
stateOrProvinceName     = optional
localityName            = optional
organizationName        = optional
organizationalUnitName  = optional
commonName              = supplied
emailAddress            = optional
</span><span>EOT

</span><span>#  sign the certificate</span>
<span>if</span> <span>[</span> <span>"x</span><span>$CAPASS</span><span>"</span> <span>=</span> <span>"x"</span> <span>]</span><span>;</span> <span>then
	</span><span>echo</span> <span>"No </span><span>\$</span><span>CAPASS present, will have to specify pass"</span>
	<span>PASSIN</span><span>=</span><span>""</span>
<span>else
	</span><span>echo</span> <span>"Reading pass from </span><span>\$</span><span>CAPASS"</span>
	<span>PASSIN</span><span>=</span><span>"-passin env:CAPASS"</span>
<span>fi

</span><span>echo</span> <span>"CA signing: </span><span>$CSR</span><span> -&gt; </span><span>$CERT</span><span>:"</span>
openssl ca <span>-batch</span> <span>-config</span> ca.config <span>$PASSIN</span> <span>-out</span> <span>$CERT</span> <span>-infiles</span> <span>$CSR</span>
<span>echo</span> <span>"CA verifying: </span><span>$CERT</span><span> &lt;-&gt; CA cert"</span>
<span>if</span> <span>[</span> <span>-f</span> ca-chain.pem <span>]</span><span>;</span> <span>then
	</span>openssl verify <span>-CAfile</span> ca-chain.pem <span>$CERT</span>
<span>else
	</span>openssl verify <span>-CAfile</span> ca.crt <span>$CERT</span>
<span>fi</span>

<span>#  cleanup after SSLeay </span>
<span>rm</span> <span>-f</span> ca.config
<span>rm</span> <span>-f</span> ca.db.serial.old
<span>rm</span> <span>-f</span> ca.db.index.old

<span>#  die gracefully</span>
<span>exit </span>0
</code></pre></div>
<h3 id="running-the-toy-example">Running the toy example:</h3>
<p>Moment of truth, young lad:</p>
<div><pre><code><span>$ </span><span>ls
</span>cacert.sh  hostcert.sh  sign.sh

<span>$ </span><span>chmod </span>a+x <span>*</span>.sh

<span>$ </span>./cacert.sh
Generating RSA private key, 4096 bit long modulus <span>(</span>2 primes<span>)</span>
..++++
..............................++++
e is 65537 <span>(</span>0x010001<span>)</span>
writing RSA key

<span>$ </span>./hostcert.sh snowflake.int.wejn.org
CN: snowflake.int.wejn.org
ANs: snowflake.int.wejn.org
Enter to confirm.

Generating RSA private key, 4096 bit long modulus <span>(</span>2 primes<span>)</span>
..............................++++
............++++
e is 65537 <span>(</span>0x010001<span>)</span>
writing RSA key
Reading pass from <span>$CAPASS</span>
CA signing: snowflake.int.wejn.org.csr -&gt; snowflake.int.wejn.org.crt:
Using configuration from ca.config
Check that the request matches the signature
Signature ok
The Subject<span>'s Distinguished Name is as follows
countryName           :PRINTABLE:'</span>CH<span>'
localityName          :ASN.1 12:'</span>Zurich<span>'
organizationName      :ASN.1 12:'</span>int.wejn.org host cert<span>'
commonName            :ASN.1 12:'</span>snowflake.int.wejn.org<span>'
Certificate is to be certified until Sep 15 13:47:28 2024 GMT (365 days)

Write out database with 1 new entries
Data Base Updated
CA verifying: snowflake.int.wejn.org.crt &lt;-&gt; CA cert
snowflake.int.wejn.org.crt: OK

$ ./hostcert.sh int.wejn.org '</span><span>*</span>.int.wejn.org<span>'
CN: int.wejn.org
ANs: int.wejn.org *.int.wejn.org
Enter to confirm.

Generating RSA private key, 4096 bit long modulus (2 primes)
.............................++++
.............................................++++
e is 65537 (0x010001)
writing RSA key
Reading pass from $CAPASS
CA signing: int.wejn.org.csr -&gt; int.wejn.org.crt:
Using configuration from ca.config
Check that the request matches the signature
Signature ok
The Subject'</span>s Distinguished Name is as follows
countryName           :PRINTABLE:<span>'CH'</span>
localityName          :ASN.1 12:<span>'Zurich'</span>
organizationName      :ASN.1 12:<span>'int.wejn.org host cert'</span>
commonName            :ASN.1 12:<span>'int.wejn.org'</span>
Certificate is to be certified <span>until </span>Sep 15 13:48:05 2024 GMT <span>(</span>365 days<span>)</span>

Write out database with 1 new entries
Data Base Updated
CA verifying: int.wejn.org.crt &lt;-&gt; CA cert
int.wejn.org.crt: OK
</code></pre></div>
<p>Looks like it worked:</p>
<div><pre><code><span>$ </span><span>ls</span> <span>-w</span> 80
cacert.sh             int.wejn.org.crt
ca.cnf                int.wejn.org.csr
ca.crt                int.wejn.org.key
ca.db.certs           int.wejn.org.key.unsecure
ca.db.index           int.wejn.org.pass
ca.db.index.attr      sign.sh
ca.db.index.attr.old  snowflake.int.wejn.org.cnf
ca.db.serial          snowflake.int.wejn.org.crt
ca.key                snowflake.int.wejn.org.csr
ca.key.unsecure       snowflake.int.wejn.org.key
ca.pass               snowflake.int.wejn.org.key.unsecure
hostcert.sh           snowflake.int.wejn.org.pass
int.wejn.org.cnf

<span>$ </span>openssl verify <span>-CAfile</span> ca.crt int.wejn.org.crt snowflake.int.wejn.org.crt 
int.wejn.org.crt: OK
snowflake.int.wejn.org.crt: OK

<span>$ </span>egrep <span>'(Public|bit|Alternative|DNS|v3.e|Sign|Vali|Not)'</span> snow<span>*</span>.crt 
        Signature Algorithm: sha512WithRSAEncryption
        Validity
            Not Before: Sep 16 13:47:28 2023 GMT
            Not After : Sep 15 13:47:28 2024 GMT
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                RSA Public-Key: <span>(</span>4096 bit<span>)</span>
        X509v3 extensions:
            X509v3 Subject Alternative Name: 
                DNS:snowflake.int.wejn.org
    Signature Algorithm: sha512WithRSAEncryption

<span>$ </span><span>grep</span> <span>-A1</span> <span>'Alternative'</span> int.wejn.org.crt 
            X509v3 Subject Alternative Name: 
                DNS:int.wejn.org, DNS:<span>*</span>.int.wejn.org
</code></pre></div>
<p>Note: 4096 bit RSA with SHA512 (SHA2 family), with <code>TLS Web Server Authentication</code> key usage, correct <code>Subject Alternative Name</code>, and one
year validity. ✅</p>
<p>Whether it actually works on the device I can’t show here. But it does on
mine. Swearsies.</p>
<p>Obviously the <code>ca.crt</code> needs to be imported on every device as a root CA,
and a guide explaining how to is different for each OS / browser. But having
the static website outlined above goes a long way to simplify the import
process<sup><a href="#fn10" id="fnref10">10</a></sup>, especially on iOS. :)</p>
<h2 id="closing-words">Closing words</h2>
<p>This was my brief foray into the wonderful world of running your own root
certificate authority in 2023… one that gets accepted by both Apple devices
and Linux browsers.</p>
<p>An obvious downside of this is having to guard a bunch of secrets<sup><a href="#fn11" id="fnref11">11</a></sup> and
the need to rotate the host certificates yearly – because Apple says so.</p>
<section>
<ol>
<li id="fn1">
<p><a href="http://www.faqs.org/docs/securing/chap24sec195.html">Copyright (c) 1998-1999 Ralf S. Engelschall, All Rights Reserved.</a> <a href="#fnref1">↩</a></p>
</li>
<li id="fn2">
<p>And I mean… on the surface, why wouldn’t it? <a href="#fnref2">↩</a></p>
</li>
<li id="fn3">
<p>Spoiler alert: <a href="https://support.apple.com/en-us/HT210176">HT210176</a>,
<a href="https://support.apple.com/en-us/102028">Apple support: 102028</a>. <a href="#fnref3">↩</a></p>
</li>
<li id="fn4">
<p>Which you can read as: I spent several hours banging my head against
the wall staring at “Zis koneksn isn’t sikjure” screen… until I’ve cracked
it. <a href="#fnref4">↩</a></p>
</li>
<li id="fn5">
<p>OTOH, root CA certs valid for 10 years are no problem. Praised be our
fruity overlords. <a href="#fnref5">↩</a></p>
</li>
<li id="fn6">
<p>Although in light of <a href="https://support.apple.com/en-us/102028">Apple support: 102028</a>
I’d be worried about that <a href="https://github.com/FiloSottile/mkcert/blob/2a46726cebac0ff4e1f133d90b4e4c42f1edf44a/cert.go#L59-L62">2 years 3 months</a>
expiration used in the source. <a href="#fnref6">↩</a></p>
</li>
<li id="fn7">
<p>I’m a guide, not a cop. <a href="#fnref7">↩</a></p>
</li>
<li id="fn8">
<p>If you choose to do this, and then import the <code>ca.crt</code> into a root store
on your device(s)… I hope you at least understand the risks and/or know how
to mitigate them. Hint: low bar is an air-gapped host to generate this, and
a safe way to long-term store the <code>ca.key</code>… otherwise enjoy your hard-earned
dose of fresh <a href="https://en.wikipedia.org/wiki/Man-in-the-middle_attack">mitm</a>. <a href="#fnref8">↩</a></p>
</li>
<li id="fn9">
<p>Modulo slight modernization of the hash etc. <a href="#fnref9">↩</a></p>
</li>
<li id="fn10">
<p>The irony of the possibility of running that web publicly with an SSL
certificate issued by letsencrypt.org is not lost on me. :-D <a href="#fnref10">↩</a></p>
</li>
<li id="fn11">
<p>Sort of like your life – or at least your digital life – depended on it. <a href="#fnref11">↩</a></p>
</li>
</ol>
</section>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't be afraid to be wrong (131 pts)]]></title>
            <link>https://nlopes.dev/writing/dont-be-afraid-to-be-wrong</link>
            <guid>37537443</guid>
            <pubDate>Sat, 16 Sep 2023 18:44:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nlopes.dev/writing/dont-be-afraid-to-be-wrong">https://nlopes.dev/writing/dont-be-afraid-to-be-wrong</a>, See on <a href="https://news.ycombinator.com/item?id=37537443">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <section>
  <h2>Being afraid</h2>
  Did you know there is a word for fear of being wrong? It’s known as <a href="https://www.oxfordreference.com/display/10.1093/acref/9780199534067.001.0001/acref-9780199534067-appendix-0001#:~:text=without%20%2B%20taxis%20order%5D-,atelophobia,-Incompleteness%20or%20imperfection">atelophobia</a>.
  <p>
  
  I don’t think most people have such an intense feeling of being wrong I’d say they have a phobia. Nevertheless I think it is worth talking about the fear as I find it very common in the context of teams.
  </p><p>
  
  Being afraid is normal but it shouldn’t get in the way of getting things done, or getting you outside of your “comfort zone”, so that you can grow effectively.
  </p><p>
  
  The feeling occurs because we perceive a threat or some form of danger. It differs in intensity and frequency depending on past experiences usually - either from your environment (e.g: a psychologically unsafe environment) or from specific interactions (e.g: someone once called you dumb when you put forth an idea).
  </p><p>
  
  I believe that in almost all workplace situations, it is not worth being afraid of being wrong, even if there is an elevated risk (rather fear maybe) of social rejection.
  </p><div>
    <p><span>Q:
        <p><span>Wait, hold on, what if my manager is awful, or my team mates are horrible?</span>
        </p>
      </span>
    </p>
    <p><span>A:
        <p><span>In my posts, especially when providing
          advice, I will assume you work in a healthy workplace environment. If you work
          in a psychologically unsafe environment, none of this even matters - if
          possible, and safe, search for a different job.</span>
        </p>
      </span>
    </p>
  </div>
  Let’s take a look at how this fear will get in your way.
</section>
<section>
  <h2>It will hold you back</h2>
  It will make you avoid certain situations - this can remove you from contexts in which you can learn, teach others, etc.
  <p>
  
  For example, instead of going to a brainstorming session where there are more senior folks to you in which you might accelerate your growth just by listening, you choose to work on implementing something you’re comfortable with. Or perhaps you’d attend but not speak at all.
  </p><p>
  
  Whenever you’re in a position where you could voice an (well thought) opinion, you opt instead to not speak. This will be safe for you but you lose the opportunity to teach someone something new. And not speaking means you have less presence (usually), which in turn means people might think you have nothing to offer. That will then mean people stop inviting you as you never contribute.
</p></section>

<section>
  <h2>It will make you more defensive</h2>
  I’ve seen this more times than I can count.
  <p>
  
  Someone brilliant is so afraid of being wrong that they work intensely, take twice as long to get to that draft as they should have, and try to capture all of the nuances and tradeoffs, that when they show it to people they think there can’t be objections.
  </p><p>
  
  Then the objections appear. And then you defend your position because you know <span>everything</span>. And then another objection appears. And you are less sure because it’s a good point but you didn’t think of it, and so you defend it by saying you don’t think it’s <span>that</span> important. And then another objection comes and you’re now thinking that everyone thinks you didn’t think carefully enough about things. And so you…. You see where I’m going.
  </p><p>
  
  At the end of a meeting/presentation/document feedback like that you are frustrated, you think people think less of you, and people around you are wondering why it took you so long to show the draft. Everyone loses.
  </p><p>
  
  Oh, and next time you try this, you’re going to be even more careful - and therefore more defensive when the objections show up.
</p></section>
<section>
  <h2>It will stress you out more</h2>
  If you are constantly self regulating due to fear, you’ll feel and be more stressed.
  <p>
  
  Stress means you find everything more difficult. You will be less patient. You will close yourself to feedback a lot more.
  </p><p>
  
  Because you’re constantly in this fight or flight mode, you question yourself and your abilities.
  </p><p>
  
  And I’m here to tell you: none of it is worth the stress.
  </p><p>
  
  It won’t do you any good for me to simply say “don’t stress about it”. So let’s see what you can do to overcome your fear of being wrong.
</p></section>
<section>
  <h2>What you can do to continue engaging</h2>
  The main thing to remember is that being wrong isn't the end of the world. Most likely everyone has already forgotten you got something wrong. The feeling you're having right after being wrong will fade away. With some practice, you will be able to recover from being wrong faster and faster.
  <p>
  
  Let’s go over a few situations and how to recover from them when you’re wrong. There are many more but these should get you started on how to think about it.
  </p><section>
    <h3>Group discussions</h3>
    In group discussions, the best way to recover is to say it out loud and continue engaged with the conversation.
    <p>
    Saying something along the lines of "I got it wrong. I guess I'm one of the&nbsp;<a href="https://xkcd.com/1053/">lucky 10000 today"</a>. A bit of humour will go a long way to disarm folks and make you feel better. My recommendation in this specific situation is not to use self deprecating humour though.
    </p><p>
    It is important you keep a growth mindset and continue engaged and asking curious questions.
    </p><p>
    This will help you get up to speed faster, it will signal to folks you aren't afraid of being wrong, and it will help you compartmentalise the feeling of being wrong.
  </p></section>
  <section>
    <h3>Decision making contexts</h3>
    In decision making contexts (especially through async methods, like documents), the best way to recover is to acknowledge you were wrong, then move to working on the next draft with the new information.
    <p>
    It's also imperative you keep communicating with the stakeholders on where you're at, your new understanding and how you're re-engaging with the problem.
    </p><p>
    This will do two things: prevent paralysis and also show that you’re above the fact you made a mistake.
  </p></section>
  <section>
    <h3>1:1 meetings</h3>
    For managers: this one is going to hurt. You will immediately be disappointed, get some impostor syndrome, and think you’re not suitable to be a manager.
    <p>
    First things first: very much like if you were in a group discussion, blurt out “I got it wrong.”
    </p><p>
    Then you can open up from a place of vulnerability - after all you’re supposed to be in a safe place.
    </p><p>
    For direct reports: I’ve got good news for you. The moment you state “I got it wrong. How do I go about fixing this?” your manager will appreciate you more for it.
    </p><p>
    It’s a delight for a manager to get that kind of response - get over the mistake, keep head high, move towards improving and growing.
  </p></section>
</section>
<section>
  <h2>Preparing mentally for when you're wrong</h2>
  It’s very helpful to prepare yourself for the very fact you will end up being wrong. Doing so will ensure you can react in the best ways.
  <ul>
    <li>Without over-thinking and over-preparing, map out what is the worst thing that can happen in any way you could be wrong. And map out the best outcome if you’re right. This is useful in asynchronous contexts - you can easily figure out how to react, navigate the objections if you’re wrong.</li>
    <li>Write down what you know and what you don’t know about the topic. Explicitly state when it’s an idea that can be discarded, when it’s a hunch, when it’s something that you know nothing about but it has occurred to you - this sets the scene for you and everyone to safely discard/accept what you’re saying.</li>
    <li>Train yourself to not regurgitate the “this is how I’ve always done it, therefore we should do it”. This won’t make you right, or remove the fear of being wrong - it will just make everyone question if there was a contribution in there somewhere.</li>
    <li>Think back and write down times when you were wrong and real consequences of those. Usually you will realise there has never been a terrible consequence except some embarrassment that lingered for a day or two. (As an experiment I have asked people a few times if they remember that one time when I said something that was wrong - almost always they have no idea what I am talking about - they simply immediately forgot about it).</li>
  </ul>
</section>
<section>
  <h2>Conclusion</h2>
  Everything is fine. Deep breaths, keep your head high, your sights on the future, and growing resilience.
  <p>
  
  If the feeling persists, go for a walk alone (I tend to do this and I can only vouch that it works for me). A walk will make you reflect without the pressure of people around you, phones, computers, etc.
  </p><p>
  
  And that reflection bit is what will make you evolve. Everyone has already forgotten you were wrong.
  </p><p>
  
  One final word of advice: being wrong is fine but you don’t want to always be wrong - that means my advice here is no substitute for working hard and thinking critically.
  </p><p>
  
  Now go forth and engage. Be human. Sometimes you’ll be wrong. And that’s fine.
</p></section>
<p>Thank you Hanna for the many suggestions and edits.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is this fraud? And if so, to what extent am I responsible? (186 pts)]]></title>
            <link>https://workplace.stackexchange.com/questions/193234/is-this-fraud-and-if-so-to-what-extent-am-i-responsible</link>
            <guid>37536361</guid>
            <pubDate>Sat, 16 Sep 2023 16:42:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://workplace.stackexchange.com/questions/193234/is-this-fraud-and-if-so-to-what-extent-am-i-responsible">https://workplace.stackexchange.com/questions/193234/is-this-fraud-and-if-so-to-what-extent-am-i-responsible</a>, See on <a href="https://news.ycombinator.com/item?id=37536361">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<blockquote>
<p>Have I done anything fraudulent so far?</p>
</blockquote>
<p>Yes.  You created a document which contains false, misleading, or inaccurate information, and distributed it to others.  It doesn't matter if you were explicitly told to do this, if you provided separate information explaining the deception, or if the person you gave it to understood the deception.  At the point that you created and distributed it you expressed a lie to another person.  The creation AND distribution are the key points - if it never left your procession then there's no problem.  If you didn't create it then there's no problem.  Adding your signature to something someone else created is the same.</p>
<blockquote>
<p>If I take this zoom meeting, and toe the company line , am I committing fraud which I could be personally liable for? (Again, not looking for legal advice, just opinion)</p>
</blockquote>
<p>When you use the words "fraud" and "liable" then you are explicitly asking for legal advice.  Consult an attorney. In some areas lying to some government employees could be considered criminal.</p>
<p>If we ignore legal culpability, civil or criminal liability, and just ask, "Is taking the call and maintaining the deception fraudulent?" then the answer is clearly yes.  You are not an actor, paid to play a part in a work of fiction.  You are an expert in your field paid to represent objective facts and professional opinions about a product you financially benefit from.</p>
<p>Carrying on the deception is defrauding those you convey the lie to.</p>
<p>Further, your director is setting you up to take full blame and responsibility.  If the customer buys the product, and never finds out about the fraud, then the director wins a sale and they continue to take risks for financial gain - lying will sometimes net you a sale where it otherwise wouldn't.</p>
<p>If their risk doesn't pay off and the customer discovers the deception then they merely need to state "My expert told me it was pen-tested, provided this document in answer to the customer's questions, and then took a call with the customer telling them the same thing.  All the communication I sent was misunderstood - I was never asking them to lie, I understood from their conversations with me that it was pen-tested and I was merely helping them understand how to fill out the form given what they told me in person."</p>
<p>Keep a paper trail.  Make sure to include others in the company in your communications - always cc one other person in all emails with this director.</p>
<blockquote>
<p>If this is fraud, how can I explain that I don't want to be a part of this to the directors? And is such an issue worth resigning over?</p>
</blockquote>
<p>Send to all directors:</p>
<p>"I incorrectly filled out that document and will contact the customer to retract it.  If you'd prefer to resolve this with the customer please cc the email to me as well so I can clearly document the resolution of the error. I'm happy to meet with the customer at any point to explain the error and the current status of the software including the lack of pen-testing. I'm also happy to discuss possible timelines for pen-testing. I regret the error, and will endeavor in the future to avoid similar errors."</p>
<p>The company will always seek to protect itself, so making threats of any kind will cause you more problems.  Resigning is a reasonable course of action if the culture of the company allows for such deception in general.  If they're lying to gain customers then the chances that they'll lie to you are pretty high.  If this individual appears to be an outlier, and if, during this process of revision and reconciliation, you find others are unhappy with the deception, then it might be worth staying with the company.</p>
<p>If resigning, get a new job first, and hand in your minimum (or two weeks) notice only after you've secured another position.  Again, if they're lying to clients, then you don't want to give them opportunities to hurt you or your job prospects.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Single-bladed floating wind turbine promises half the cost, more power (120 pts)]]></title>
            <link>https://newatlas.com/energy/touchwind-floating-wind-turbine/</link>
            <guid>37536190</guid>
            <pubDate>Sat, 16 Sep 2023 16:24:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/energy/touchwind-floating-wind-turbine/">https://newatlas.com/energy/touchwind-floating-wind-turbine/</a>, See on <a href="https://news.ycombinator.com/item?id=37536190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>We haven't seen a floating offshore wind turbine like this before. Touchwind claims its innovative single-blade turbines will solve several problems to drive down cost and downtime, using a single, huge blade with no fancy active pitch controls.</p><p>Most of the world's best wind resources are way offshore, in ocean far too deep to exploit with typical fixed-tower turbines. The deep sea could thus make a huge clean power contribution, while creating far less trouble for residents and wildlife than onshore wind farms.</p><p>But the technology to harness offshore wind from floating devices anchored to the sea bed is far from settled, so there's a gold rush of sorts in progress as some <a href="https://newatlas.com/tag/floating-offshore-wind/" data-cms-ai="0">radically different designs</a> duke it out on the spec sheet, in wave tanks and in prototype testing. They're all hoping to find the sweet spot between cost, power generation, cost, longevity, reliability, cost, ease of manufacture, ease of installation and maintenance, cost, cost and cost. </p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="Buoyant barrel floats are anchored to the sea floor" width="960" height="540" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/eaa7d76/2147483647/strip/true/crop/960x540+0+0/resize/440x248!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fe3%2Fbc%2F2e67c2f74018a09f16ff1e56a156%2Funderwater-thumb.jpg 440w,https://assets.newatlas.com/dims4/default/fbacd0a/2147483647/strip/true/crop/960x540+0+0/resize/800x450!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fe3%2Fbc%2F2e67c2f74018a09f16ff1e56a156%2Funderwater-thumb.jpg 800w,https://assets.newatlas.com/dims4/default/1209ddc/2147483647/strip/true/crop/960x540+0+0/resize/1200x675!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fe3%2Fbc%2F2e67c2f74018a09f16ff1e56a156%2Funderwater-thumb.jpg 1200w,https://assets.newatlas.com/dims4/default/aa82907/2147483647/strip/true/crop/960x540+0+0/resize/1920x1080!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fe3%2Fbc%2F2e67c2f74018a09f16ff1e56a156%2Funderwater-thumb.jpg 1920w" data-src="https://assets.newatlas.com/dims4/default/e3e31ec/2147483647/strip/true/crop/960x540+0+0/resize/960x540!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fe3%2Fbc%2F2e67c2f74018a09f16ff1e56a156%2Funderwater-thumb.jpg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/eaa7d76/2147483647/strip/true/crop/960x540+0+0/resize/440x248!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fe3%2Fbc%2F2e67c2f74018a09f16ff1e56a156%2Funderwater-thumb.jpg 440w,https://assets.newatlas.com/dims4/default/fbacd0a/2147483647/strip/true/crop/960x540+0+0/resize/800x450!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fe3%2Fbc%2F2e67c2f74018a09f16ff1e56a156%2Funderwater-thumb.jpg 800w,https://assets.newatlas.com/dims4/default/1209ddc/2147483647/strip/true/crop/960x540+0+0/resize/1200x675!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fe3%2Fbc%2F2e67c2f74018a09f16ff1e56a156%2Funderwater-thumb.jpg 1200w,https://assets.newatlas.com/dims4/default/aa82907/2147483647/strip/true/crop/960x540+0+0/resize/1920x1080!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fe3%2Fbc%2F2e67c2f74018a09f16ff1e56a156%2Funderwater-thumb.jpg 1920w" src="https://assets.newatlas.com/dims4/default/e3e31ec/2147483647/strip/true/crop/960x540+0+0/resize/960x540!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fe3%2Fbc%2F2e67c2f74018a09f16ff1e56a156%2Funderwater-thumb.jpg">
</p>



    
    

    
        <div><figcaption itemprop="caption">Buoyant barrel floats are anchored to the sea floor</figcaption><p>Touchwind</p></div>
    
</figure>

                
            </div><p>Dutch company Touchwind has an interesting spin on the idea... <a href="https://www.youtube.com/watch?v=Z8uY79zQeak" target="_blank" data-cms-ai="0">waka waka</a>. It's designed around a massive single-piece rotor, sitting on the end of a pole that's draped over a big barrel, with a large floating buoy hanging beneath it.</p><p>This one huge double blade, says Touchwind, should cost around 30% as much to make as the triple-bladed arrangements on traditional turbines. It doesn't require any expensive active blade-pitch control systems, and where most standard turbines need to shut down in wind speeds above 25 or so m/sec (90 km/h / 56 mph), this one is rated for speeds as high as 70 m/sec (252 km/h / 157 mph). Less downtime equals more productive hours and more energy.</p><p>The blade is fixed to the mast at a slight upward angle. At low wind speeds, the mast tilts right over, and effectively the blade stays out of the water with the assistance of that dangling buoy. But as wind speeds pick up and the blade starts spinning fast, it develops lift, much like a helicopter's main rotor, and begins pulling the mast upright. </p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="At higher wind speeds, the big blade pulls the tower upright, exposing a smaller section to the wind" width="1440" height="810" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/11badf6/2147483647/strip/true/crop/1922x1081+0+0/resize/440x248!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2F9e%2F1272b9b7438fad2e78cab0442152%2Fstorm-01.jpg 440w,https://assets.newatlas.com/dims4/default/578a82e/2147483647/strip/true/crop/1922x1081+0+0/resize/800x450!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2F9e%2F1272b9b7438fad2e78cab0442152%2Fstorm-01.jpg 800w,https://assets.newatlas.com/dims4/default/2e8a463/2147483647/strip/true/crop/1922x1081+0+0/resize/1200x675!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2F9e%2F1272b9b7438fad2e78cab0442152%2Fstorm-01.jpg 1200w,https://assets.newatlas.com/dims4/default/754bf94/2147483647/strip/true/crop/1922x1081+0+0/resize/1920x1080!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2F9e%2F1272b9b7438fad2e78cab0442152%2Fstorm-01.jpg 1920w" data-src="https://assets.newatlas.com/dims4/default/9f4fd37/2147483647/strip/true/crop/1922x1081+0+0/resize/1440x810!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2F9e%2F1272b9b7438fad2e78cab0442152%2Fstorm-01.jpg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/11badf6/2147483647/strip/true/crop/1922x1081+0+0/resize/440x248!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2F9e%2F1272b9b7438fad2e78cab0442152%2Fstorm-01.jpg 440w,https://assets.newatlas.com/dims4/default/578a82e/2147483647/strip/true/crop/1922x1081+0+0/resize/800x450!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2F9e%2F1272b9b7438fad2e78cab0442152%2Fstorm-01.jpg 800w,https://assets.newatlas.com/dims4/default/2e8a463/2147483647/strip/true/crop/1922x1081+0+0/resize/1200x675!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2F9e%2F1272b9b7438fad2e78cab0442152%2Fstorm-01.jpg 1200w,https://assets.newatlas.com/dims4/default/754bf94/2147483647/strip/true/crop/1922x1081+0+0/resize/1920x1080!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2F9e%2F1272b9b7438fad2e78cab0442152%2Fstorm-01.jpg 1920w" src="https://assets.newatlas.com/dims4/default/9f4fd37/2147483647/strip/true/crop/1922x1081+0+0/resize/1440x810!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2F9e%2F1272b9b7438fad2e78cab0442152%2Fstorm-01.jpg">
</p>



    
    

    
        <div><figcaption itemprop="caption">At higher wind speeds, the big blade pulls the tower upright, exposing a smaller section to the wind</figcaption><p>Touchwind</p></div>
    
</figure>

                
            </div><p>Thus, in high wind speeds, it sits nearly flat to the horizon, greatly limiting the wind's ability to spin it faster. And as this happens, the buoy is lifted out of the water, becoming a ballast weight acting against the lift of the main blade, helping to reduce stress on the sea floor anchors and prevent the whole thing from taking off and starting a new life where nobody knows its name.</p><p>As with many other floating designs, it's agnostic to the direction of the incoming wind, and will passively float around to orient itself in the optimal direction at all times. </p><p>Touchwind says the design lends itself to easy manufacture at more or less any harbor facility capable of handling the 200-m (656-ft) blade required for a 12-MW turbine, and it's similarly easy to tow out to site and attach to a ground anchor and power export cable for installation. </p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="Touchwind says the devices should be relatively straightforward to manufacture and deploy from harbors" width="1440" height="810" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/8ff0ea1/2147483647/strip/true/crop/1920x1080+0+0/resize/440x248!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2Fed%2F9960cb224afa879c76f3c8c18896%2Fharbour.jpg 440w,https://assets.newatlas.com/dims4/default/1e36b0c/2147483647/strip/true/crop/1920x1080+0+0/resize/800x450!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2Fed%2F9960cb224afa879c76f3c8c18896%2Fharbour.jpg 800w,https://assets.newatlas.com/dims4/default/080db5b/2147483647/strip/true/crop/1920x1080+0+0/resize/1200x675!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2Fed%2F9960cb224afa879c76f3c8c18896%2Fharbour.jpg 1200w,https://assets.newatlas.com/dims4/default/b914b85/2147483647/strip/true/crop/1920x1080+0+0/resize/1920x1080!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2Fed%2F9960cb224afa879c76f3c8c18896%2Fharbour.jpg 1920w" data-src="https://assets.newatlas.com/dims4/default/23befe7/2147483647/strip/true/crop/1920x1080+0+0/resize/1440x810!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2Fed%2F9960cb224afa879c76f3c8c18896%2Fharbour.jpg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/8ff0ea1/2147483647/strip/true/crop/1920x1080+0+0/resize/440x248!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2Fed%2F9960cb224afa879c76f3c8c18896%2Fharbour.jpg 440w,https://assets.newatlas.com/dims4/default/1e36b0c/2147483647/strip/true/crop/1920x1080+0+0/resize/800x450!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2Fed%2F9960cb224afa879c76f3c8c18896%2Fharbour.jpg 800w,https://assets.newatlas.com/dims4/default/080db5b/2147483647/strip/true/crop/1920x1080+0+0/resize/1200x675!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2Fed%2F9960cb224afa879c76f3c8c18896%2Fharbour.jpg 1200w,https://assets.newatlas.com/dims4/default/b914b85/2147483647/strip/true/crop/1920x1080+0+0/resize/1920x1080!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2Fed%2F9960cb224afa879c76f3c8c18896%2Fharbour.jpg 1920w" src="https://assets.newatlas.com/dims4/default/23befe7/2147483647/strip/true/crop/1920x1080+0+0/resize/1440x810!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F66%2Fed%2F9960cb224afa879c76f3c8c18896%2Fharbour.jpg">
</p>



    
    

    
        <div><figcaption itemprop="caption">Touchwind says the devices should be relatively straightforward to manufacture and deploy from harbors</figcaption><p>Touchwind</p></div>
    
</figure>

                
            </div><p>The company has completed both land-based and floating platform prototypes at small scale, and is beginning to expand testing thanks to fresh investment from Japanese shipping company Mitsui O.S.K. Lines.</p><p>"We have been working together for a year now on the further development of our floating wind turbine," said Touchwind Founder and CEO Rikus van de Klippe, in a press release. "Field testing with a 6-m diameter rotor is in full preparation at the Oostvoorne lake in the Netherlands. With MOL as a shareholder and their investments we can speed up our testing program, prove our technology and reduce time to market."</p><p>We're not sure when the company expects to be operating at scale, and unfortunately, there are no projections at this stage on what the levelized cost of energy (LCoE) from these beasties might look like. So it's hard to get a read on how competitive it might be in a commercial deployment, assuming development and funding proceed without too much drama. </p><p>Check out a video below.</p><p>Source: <a href="https://touchwind.org/" target="_blank" data-cms-ai="0">Touchwind</a> via <i><a href="https://www.rechargenews.com/wind/this-tilting-one-blader-floating-wind-turbine-just-got-a-japanese-shipping-giant-on-board/2-1-1515923" target="_blank" data-cms-ai="0">Recharge News</a></i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The database servers powering Let's Encrypt (2021) (195 pts)]]></title>
            <link>https://letsencrypt.org/2021/01/21/next-gen-database-servers.html</link>
            <guid>37536103</guid>
            <pubDate>Sat, 16 Sep 2023 16:16:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letsencrypt.org/2021/01/21/next-gen-database-servers.html">https://letsencrypt.org/2021/01/21/next-gen-database-servers.html</a>, See on <a href="https://news.ycombinator.com/item?id=37536103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<article>
		<p>Let’s Encrypt helps to protect a huge portion of the Web by providing TLS certificates to more than <a href="https://letsencrypt.org/stats/">235 million websites</a>. A database is at the heart of how Let’s Encrypt manages certificate issuance. If this database isn’t performing well enough, it can cause API errors and timeouts for our subscribers. Database performance is the single most critical factor in our ability to scale while meeting service level objectives. In late 2020, we upgraded our database servers and we’ve been very happy with the results.</p>
<h2 id="what-exactly-are-we-doing-with-these-servers">What exactly are we doing with these servers?</h2>
<p>Our CA software, <a href="https://github.com/letsencrypt/boulder">Boulder</a>, uses MySQL-style schemas and queries to manage subscriber accounts and the entire certificate issuance process. It’s designed to work with a single MySQL, MariaDB, or Percona database. We currently use MariaDB, with the InnoDB database engine.</p>
<p>We run the CA against a single database in order to minimize complexity. Minimizing complexity is good for security, reliability, and reducing maintenance burden. We have a number of replicas of the database active at any given time, and we direct some read operations to replica database servers to reduce load on the primary.</p>
<p>One consequence of this design is that our database machines need to be pretty powerful. Eventually we may need to shard or break the single database into multiple databases, but hardware advancements have allowed us to avoid that so far.</p>
<h2 id="hardware-specifications">Hardware Specifications</h2>
<p>The previous generation of database hardware was powerful but it was regularly being pushed to its limits. For the next generation, we wanted to more than double almost every performance metric in the same 2U form factor. In order to pull that off, we needed AMD EPYC chips and Dell’s <a href="https://www.dell.com/en-us/work/shop/cty/pdp/spd/poweredge-r7525/">PowerEdge R7525</a> was ideal. Here are the specifications:</p>
<div>
<table>
	<tbody><tr>
		<td></td>
		<td><b>Previous Generation</b></td>
		<td><b>Next Generation</b></td>
	</tr>
	<tr>
		<td><b>CPU</b></td>
		<td>2x Intel Xeon E5-2650<br>Total 24 cores / 48 threads</td>
		<td>2x <a href="https://www.amd.com/en/products/cpu/amd-epyc-7452">AMD EPYC 7542</a><br>Total 64 cores / 128 threads
</td>
	</tr>
	<tr>
		<td><b>Memory</b></td>
		<td>1TB 2400MT/s</td>
		<td>2TB 3200MT/s</td>
	</tr>
	<tr>
		<td><b>Storage</b></td>
		<td>24x 3.8TB Samsung PM883<br>SATA SSD<br>560/540 MB/s read/write</td>
		<td>24x 6.4TB Intel P4610<br>NVMe SSD<br>3200/3200 MB/s read/write</td>
	</tr>
</tbody></table>
</div>
<figure>
<img src="https://letsencrypt.org/images/2021.01.21-next-gen-db-chassis.jpg" width="600" alt="Dell PowerEdge R7525 Chassis">
<figcaption>Dell PowerEdge R7525 internals. The two silver rectangles in the middle are the CPUs. The RAM sticks, each 64GB, are above and below the CPUs. The 24x NVMe drives are in the front of the server, on the far left.</figcaption>
</figure>
<p>By going with AMD EPYC, we were able to get 64 physical CPU cores while keeping clock speeds high: 2.9GHz base with 3.4GHz boost. More importantly, EPYC provides 128 PCIe v4.0 lanes, which allows us to put 24 NVMe drives in a single machine. NVMe is incredibly fast (~5.7x faster than the SATA SSDs in our previous-gen database servers) because it uses PCIe instead of SATA. However, PCIe lanes are typically very limited: modern consumer chips typically have only 16 lanes, and Intel’s Xeon chips have 48. By providing 128 PCI lanes per chip (v4.0, no less), AMD EPYC has made it possible to pack large numbers of NVMe drives into a single machine. We’ll talk more about NVMe later.</p>
<h2 id="performance-impact">Performance Impact</h2>
<p>We’ll start by looking at our median time to process a request because it best reflects subscribers’ experience. Before the upgrade, we turned around the median API request in ~90 ms. The upgrade decimated that metric to ~9 ms!</p>
<p><img src="https://letsencrypt.org/images/2021.01.21-next-gen-db-api-latency.png" alt="API Latency"></p>
<p>We can clearly see how our old CPUs were reaching their limit. In the week before we upgraded our primary database server, its CPU usage (from /proc/stat) averaged over 90%:</p>
<p><img src="https://letsencrypt.org/images/2021.01.21-next-gen-db-cpu-before.png" alt="CPU Usage Before Upgrade"></p>
<p>The new AMD EPYC CPUs sit at about 25%. You can see in this graph where we promoted the new database server from replica (read-only) to primary (read/write) on September 15.</p>
<p><img src="https://letsencrypt.org/images/2021.01.21-next-gen-db-cpu-after.png" alt="CPU Usage After Upgrade"></p>
<p>The upgrade greatly reduced our overall database latency. The average query response time (from INFORMATION_SCHEMA) used to be ~0.45ms.</p>
<p><img src="https://letsencrypt.org/images/2021.01.21-next-gen-db-db-latency-before.png" alt="Database Latency Before Upgrade"></p>
<p>Queries now average <em>three times faster</em>, about 0.15ms.</p>
<p><img src="https://letsencrypt.org/images/2021.01.21-next-gen-db-db-latency-after.png" alt="Database Latency After Upgrade"></p>
<h2 id="openzfs-and-nvme">OpenZFS and NVMe</h2>
<p>NVMe drives are becoming increasingly popular because of their incredible performance. Up until recently, though, it was nearly impossible to get many of them in a single machine because NVMe uses PCIe lanes. Those were very limited: Intel’s Xeon processors come with just 48 PCIe v3 lanes, and a number of those are used up by the chipset and add-on cards such as network adapters and GPUs. You can’t fit many NVMe drives in the remaining lanes.</p>
<p>AMD’s latest generation of EPYC processors come with 128 PCIe lanes - more than double what Intel offers - and they’re PCIe v4! This is enough to pack a 2U server full of NVMe drives (24 in our case).</p>
<p>Once you have a server full of NVMe drives, you have to decide how to manage them. Our previous generation of database servers used hardware RAID in a RAID-10 configuration, but there is no effective hardware RAID for NVMe, so we needed another solution. One option was software RAID (Linux mdraid), but we got several recommendations for OpenZFS and decided to give it a shot. We’ve been very happy with it!</p>
<p>There wasn’t a lot of information out there about how best to set up and optimize OpenZFS for a pool of NVMe drives and a database workload, so we want to share what we learned. You can find detailed information about our setup in <a href="https://github.com/letsencrypt/openzfs-nvme-databases">this GitHub repository</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This database upgrade was necessary as more people rely on Let’s Encrypt for the security and privacy that TLS/SSL provides. The equipment is quite expensive and it was a sizable undertaking for our SRE team to plan and execute the transition, but we gained a lot through the process.</p>
<h2 id="support-let-s-encrypt">Support Let’s Encrypt</h2>
<p>We depend on contributions from our supporters in order to provide our services. If your company or organization would like to <a href="https://www.abetterinternet.org/sponsor/">sponsor</a> Let’s Encrypt please email us at <a href="mailto:sponsor@letsencrypt.org">sponsor@letsencrypt.org</a>. We ask that you make an <a href="https://letsencrypt.org/donate/">individual contribution</a> if it is within your means.</p>

	</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Generative Image Dynamics (283 pts)]]></title>
            <link>https://generative-dynamics.github.io/</link>
            <guid>37536016</guid>
            <pubDate>Sat, 16 Sep 2023 16:08:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://generative-dynamics.github.io/">https://generative-dynamics.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=37536016">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>Abstract</h2>
        <p>
We present an approach to modeling an image-space prior on scene dynamics. Our prior is learned from a collection of motion trajectories extracted from real video sequences containing natural, oscillating motion such as trees, flowers, candles, and clothes blowing in the wind. Given a single image, our trained model uses a frequency-coordinated diffusion sampling process to predict a per-pixel long-term motion representation in the Fourier domain, which we call a neural stochastic motion texture. This representation can be converted into dense motion trajectories that span an entire video. Along with an image-based rendering module, these trajectories can be used for a number of downstream applications, such as turning still images into seamlessly looping dynamic videos, or allowing users to realistically interact with objects in real pictures. 
          </p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unity's Self-Combustion Engine (141 pts)]]></title>
            <link>https://www.gamesindustry.biz/unitys-self-combustion-engine-this-week-in-business</link>
            <guid>37535910</guid>
            <pubDate>Sat, 16 Sep 2023 15:57:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamesindustry.biz/unitys-self-combustion-engine-this-week-in-business">https://www.gamesindustry.biz/unitys-self-combustion-engine-this-week-in-business</a>, See on <a href="https://news.ycombinator.com/item?id=37535910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



            <p><em>This Week in Business is <a href="https://www.gamesindustry.biz/topics/this-week-in-business">our weekly recap column</a>, a collection of stats and quotes from recent stories presented with a dash of opinion (sometimes more than a dash) and intended to shed light on various trends. Check back every Friday for a new entry.</em></p>
<p>
Unity had a pretty bad week.
</p>
<p>
I'm actually writing this lead on Tuesday afternoon, but feeling pretty confident that whatever happens from here on out, this much will be true.
</p>
<p>
(<em>Friday morning update: Yup.</em>)
</p>
<p>
Tuesday morning, Unity decided to introduce its new Runtime Fee, which Unity developers of a certain size will have to pay every time their game is installed on a new device after January 1, 2024.
</p>
<p>
<strong>STAT</strong> | $0.20 – The fee per install for small developers (Unity Personal subscribers) whose games brought in $200,000 in the past year and have 200,000 lifetime installs to date.
</p>
<p>
<strong>STAT</strong> | $0.01 – The fee per install for large developers (Unity Enterprise clients) whose games brought in $1 million in the past year and have lifetime sales of one million or more.
</p>
<p>
It doesn't sound like a lot, and if a developer just had to pay the fee each time they sold a $60 game, it wouldn't be. But there are problems because these charges are based on game installs, not sales, and Unity has not done a great job communicating what's going on here.
</p>
<p>
<strong>QUOTE</strong> | "An install is defined as the installation and initialization of a project on an end user's device." – Unity's initial FAQ on the new fee.
</p>
<p>
We asked Unity if that would include demos, since those are distinct projects that get installed on end user devices and certainly seem to count.
</p>
<p>
<strong>QUOTE</strong> | "No, demos, trials, game bundles and giveaways - like the Humble Bundle - do not count as installs." – A Unity spokesperson, in an email response to use on Tuesday afternoon.
</p>
<p>
Oh, phew! That could have been bad, but thankfully Unity was quick to dispel—
</p>
<p>
<strong>QUOTE</strong> | "If it's early access, Beta, or a demo of the full game then yes [it will result in a Runtime Fee]. If you can get from the demo to a full game then yes. If it's not, like a single level that can't upgrade, then no." – A Unity representative in the company's official forums in <a href="https://forum.unity.com/threads/unity-plan-pricing-and-packaging-updates.1482750">a thread edited Tuesday night</a>.
</p>
<p>
What.
</p>
<p>
How about games included in streaming services? They aren't really installed on an end user's device, so they shouldn't count, right?
</p>
<p>
<strong>QUOTE</strong> | "Subscription services, like Game Pass, do count as an install." – Unity's spokesperson responds to our inquiry about how streaming games would be counted for the Runtime Fee.
</p>
<p>
Bummer, but at least we got an answer I suppose, and now we know developers will be on the hook for—
</p>
<p>
<strong>QUOTE</strong> | "As for Game Pass and other subscription services, [Unity Create GM Marc] Whitten said that developers… would not be on the hook, as the fees are charged to distributors, which in the Game Pass example would be Microsoft." – <a href="https://www.axios.com/2023/09/13/unity-runtime-fee-policy-marc-whitten">Axios</a> adding key context for Unity's policy Tuesday evening.
</p>
<p>
That's great, but how will Unity even track this? If I buy a copy of Dear Esther Landmark Edition (made with Unity) on GOG.com, I get a DRM-free copy that does not require any online connection to play. How does Unity tell if I install the game on a hundred computers or none? Or the same computer 100 times?
</p>
<figure>
<a data-lightbox="true" href="https://assetsio.reedpopcdn.com/dearesther_m45iXe3.jpg?width=1920&amp;height=1920&amp;fit=bounds&amp;quality=80&amp;format=jpg&amp;auto=webp" target="_blank">
<img alt="Screenshot of Dear Esther showing a point of view from inside a cave. Through the mouth of the cave you can see a night sky lit up by a bright full moon." height="405" loading="lazy" src="https://assetsio.reedpopcdn.com/dearesther_m45iXe3.jpg?width=720&amp;quality=70&amp;format=jpg&amp;auto=webp" srcset="https://assetsio.reedpopcdn.com/dearesther_m45iXe3.jpg?width=720&amp;quality=70&amp;format=jpg&amp;auto=webp 1x, https://assetsio.reedpopcdn.com/dearesther_m45iXe3.jpg?width=720&amp;quality=70&amp;format=jpg&amp;dpr=2&amp;auto=webp 2x" width="720"> </a>
<figcaption>Just imagining Riccitiello shaking his fist at the heavens, screaming "Esssssstherrrrr!"</figcaption>
</figure>
<p>
<strong>QUOTE</strong> | "We leverage our own proprietary data model, so you can appreciate that we won't go into a lot of detail, but we believe it gives an accurate determination of the number of times the Runtime is distributed for a given project." – Unity's representative tells us the company definitely has a very good, very accurate process for determining how much money you need to pay it, but you're not allowed to know what it is.
</p>
<p>
Also, even if you did know how many times the Runtime was <em>distributed</em>, that's not the same as how many times it was <em>installed</em>, which is the thing you're actually charging people for. 
</p>
<p>
The company also said it has anti-fraud technology to detect pirated copies of the game being installed so developers won't be charged for those, but we question a) whether such technology exists (If I legally install that DRM-free copy of Dear Esther on my laptop and then on my PC, how exactly does that work differently from me installing it on my laptop and then pirating it by giving a copy to a friend to install on their PC?) and b) how eager Unity will be to invest in technology that will reduce the amount of money it is owed.
</p>
<p>
Remember when Facebook <a href="https://www.gamesindustry.biz/facebook-ignored-solution-to-children-overspending-in-games#:~:text=However%2C%20instead%20of%20implementing%20this%20solution%2C%20Facebook%20decided%20to%20focus%20on%20%22maximising%20revenue%22%20rather%20thank%20blocking%20%27friendly%20fraud%27%20%2D%2D%20in%20fact%2C%20an%20internal%20memo%20instructed%20employees%20on%20what%20%27friendly%20fraud%27%20was%2C%20and%20%22why%20you%20shouldn%27t%20try%20to%20block%20it.%22">sent a memo to employees about "friendly fraud"</a> (kids using their parents' credit cards to buy stuff) and why they shouldn't try to stop it? Good times.
</p>
<p>
Obviously, Unity's new fees did not go over well. A collective of studios <a href="https://www.gamesindustry.biz/developers-switch-off-unity-ads-in-runtime-fee-protest">pulled Unity and IronSource ads from all their titles as a form of protest</a>, and called upon others to do the same. The developers of <a href="https://twitter.com/InnerslothDevs/status/1701731398498013575">Among Us</a>, <a href="https://twitter.com/AggroCrabGames/status/1701691036832309260">Going Under</a>, <a href="https://twitter.com/MassiveMonster/status/1701808567140102633">Cult of the Lamb</a>, <a href="https://twitter.com/LandfallGames/status/1701893371290075315">Totally Accurate Battle Simulator</a>, <a href="https://twitter.com/MegaCrit/status/1702077576209207611">Slay the Spire</a>, <a href="https://twitter.com/galvanicgames/status/1702416458095972427">Wizard With A Gun</a>, and <a href="https://twitter.com/rosecitygames/status/1702072158447362142">Floppy Knights</a> have all publicly said they are inclined to switch engines if the changes go through.
</p>
<p>
A lot of those statements talked about trust, because trust is crucial for developers using a commercial game engine. Games can take years to build, and if a developer is going to commit to building a project in Unity, Unity needs to commit to supporting them over that span. And Unity knows this.
</p>
<p>
<strong>QUOTE</strong> | "It's a huge commitment to us. Because as soon as we bet on that platform, we have to stay convicted to it for a very long time. Because creators will download it and start using it, and once we've said we're going to support the platform, we need to make sure they know we're there." - Unity Create VP Marc Whitten <a href="https://www.gamesindustry.biz/unity-on-managing-perceptions-and-cloudless-generative-ai#:~:text=%22But%20it%27s%20a,some%20of%20those.%22">told us earlier this year</a> why Unity has to think in the long-term every time it adds support for a new platform or a technology like VR.
</p>
<p>
I don't think the problem with Unity's new Runtime Fee is that Unity botched the messaging, although it definitely did that. It's not that this new fee is a prime example of enshittification, although it definitely is that, too. I'm not even sure the problem is the extra money it takes out of developers' pockets.
</p>
<p>
I think the real problem here is trust.
</p>
<p>
Trust is essential in any relationship, personal or professional, and trust is something Unity has squandered a bit of lately. Like when developers expressed widespread concerns about a future where employers replaced them with AI and Unity decided there was <a href="https://www.gamesindustry.biz/unity-unveils-ai-tools-unity-muse-and-unity-sentis">no time like the present</a>.
</p>
<p>
Or when one of the vetted and verified third-party AI solutions it offered was not so much "using cutting-edge generative 3D AI technology to enable the creation of assets and virtual worlds" as it was <a href="https://www.gamesindustry.biz/unity-drops-ai-partner-after-accusations-it-stole-assets">"using stolen assets."</a>
</p>
<figure>
<a data-lightbox="true" href="https://assetsio.reedpopcdn.com/verifiedsolutions.jpg?width=1920&amp;height=1920&amp;fit=bounds&amp;quality=80&amp;format=jpg&amp;auto=webp" target="_blank">
<img alt="Screen capture of Unity's AI hub with four programs listed, including Atlas 3D Asset Creator. The text above the programs reads: 

Build confidently with Verified Solutions
Find professional solutions that have undergone enhanced vetting from creators who are committed to providing high-quality solutions, service, and long term support." height="296" loading="lazy" src="https://assetsio.reedpopcdn.com/verifiedsolutions.jpg?width=720&amp;quality=70&amp;format=jpg&amp;auto=webp" srcset="https://assetsio.reedpopcdn.com/verifiedsolutions.jpg?width=720&amp;quality=70&amp;format=jpg&amp;auto=webp 1x, https://assetsio.reedpopcdn.com/verifiedsolutions.jpg?width=720&amp;quality=70&amp;format=jpg&amp;dpr=2&amp;auto=webp 2x" width="720"> </a>
<figcaption>Build confidently with Unity's verified solutions that have undergone enhanced vetting that did not catch asset descriptions openly advertising that they have been stolen</figcaption>
</figure>
<p>
On top of that, game developers work in a notoriously unstable field with precious little job security, so it probably didn't instill much trust in Unity when the company <a href="https://www.gamesindustry.biz/unity-lays-off-nearly-300">laid off nearly 300 people in January</a>, even though Riccitiello said he was "thrilled with the work they've done" in one group's case. That trust might have taken another hit when Riccitiello <a href="https://www.gamesindustry.biz/unity-lays-off-hundreds-more-closing-half-of-offices">cut 600 more jobs a few months later</a> saying the shrinking headcount was "all about setting ourselves up for higher growth." 
</p>
<p>
As for this week, Unity had an arrangement with these developers that they understood, and one day Unity decided to change that arrangement for its own benefit, without consultation or recourse, in a way that will impose additional costs on developers for as long as their games continue to be installed, which is entirely outside their control at this point.
</p>
<p>
So even if you're a successful developer who can take the financial hit of the new fee, if you see that Unity is willing to do this, would you trust it not to do it again? Would you trust Unity to not unilaterally change the deal in other ways that disadvantage you? If you stick around and show Unity that you're willing to put up with this violation, how can you trust it won't take it that much further next time?
</p>
<p>
And if you're not a Unity developer yet, how could you possibly trust that it is worth the time and effort it takes to learn the engine having seen what the company is willing to do? If you're building a business, you reasonably want stability from the partnerships you rely on. Unity's Runtime Fee this week – both the fee itself and the haphazard, ill-advised way it has been rolled out – show that Unity doesn't put much value in stability.
</p>
<h2>The reason Unity did this</h2>
<p>
So why would Unity kick such a hornet's nest here? I hope you won't think me too cynical when I say it's about profit. Or more precisely, the lack thereof.
</p>
<p>
<strong>STAT</strong> | $2.6 billion – Unity's lifetime accumulated deficit over 19 years of operations, a figure we got by combining the $2.2 billion lifetime debt of its last annual report with the $447 million in net losses it reported for the first two quarters of its current fiscal year.
</p>
<p>
That's a lot! It has also doubled in pretty short order.
</p>
<p>
<strong>STAT</strong> | $1.3 billion – Unity's lifetime accumulated deficit as of December 31, 2021.
</p>
<p>
Unity has never had a profitable quarter in its history. It has posted modest operating profits in the past three quarters for the first time ever, but I imagine there are lots of businesses whose books look better if you ignore income taxes, depreciation of property and equipment, amortization of intangible assets, millions in restructuring costs associated with <a href="https://www.gamesindustry.biz/is-unity-still-focused-on-making-games#:~:text=%22We%20didn%27t%20cut%201%25%20of%20jobs%20because%20we%20were%20trying%20to%20make%20a%20P%26L.%20We%20did%20it%20because%20we%20were%20focusing%20our%20energy.%20Like%20with%20lots%20of%20things%20%E2%80%93%20your%20parents%20when%20they%20ask%20you%20to%20eat%20your%20vegetables%20%E2%80%93%20they%27re%20not%20doing%20it%20to%20punish%20you.%20They%27re%20doing%20it%20to%20facilitate%20your%20better%20and%20more%20nutritious%20self.">laying people off because you think it's a healthy habit to get into</a>, and of course stock-based compensation.
</p>
<p>
<strong>STAT</strong> | $494 million - The cumulative stock-based compensation Unity reported paying out over its past three quarters.
</p>
<p>
<strong>STAT</strong> | $152 million – The cumulative "profit" Unity reported over those same three quarters.
</p>
<p>
Speaking of stock-based compensation, there's been some coverage of recent stock sales made by Unity CEO John Riccitiello with the implication that it was insider trading because he knew this Runtime Fee would go over like a lead balloon and sink the stock price.
</p>
<p>
I would stop short of saying Riccitiello would never – he was after all president at EA during a span where <a href="https://www.cfo.com/news/electronic-arts-facing-sec-inquiry/676165/">the SEC reportedly investigated the company for stock options backdating</a> (as was <a href="https://www.gamesindustry.biz/former-take-two-execs-to-face-fraud-allegations">the fashion</a> in <a href="https://www.gamesindustry.biz/activision-faces-shareholder-lawsuit">the games industry</a> at <a href="https://www.gamesindustry.biz/sec-chooses-not-to-recommend-action-against-thq">the time</a>) – I will point out that the vast majority of Riccitiello's pay at Unity comes in the form of stock and stock options, and the reported amount he sold is almost negligible compared to what he would have left.
</p>
<p>
Riccitiello's total compensation last year was $11.8 million, but the actual cash part of that was just $380,000, which is still a fabulous amount of money in absolute terms but around Unity's San Francisco headquarters is about enough to get you a two-bedroom apartment with mold problems. (Also, Whole Foods cashiers get super snippy when you try to pay with stock options.)
</p>
<figure>
<a data-lightbox="true" href="https://assetsio.reedpopcdn.com/twitterhq.jpg?width=1920&amp;height=1920&amp;fit=bounds&amp;quality=80&amp;format=jpg&amp;auto=webp" target="_blank">
<img alt="A picture of the exterior of Twitter's San Francisco headquarters" height="405" loading="lazy" src="https://assetsio.reedpopcdn.com/twitterhq.jpg?width=720&amp;quality=70&amp;format=jpg&amp;auto=webp" srcset="https://assetsio.reedpopcdn.com/twitterhq.jpg?width=720&amp;quality=70&amp;format=jpg&amp;auto=webp 1x, https://assetsio.reedpopcdn.com/twitterhq.jpg?width=720&amp;quality=70&amp;format=jpg&amp;dpr=2&amp;auto=webp 2x" width="720"> </a>
<figcaption>The worst part of living in San Francisco is that the city is overrun with parasitic freeloaders who can't even pay rent.</figcaption>
</figure>
<p>
When <a href="https://www.gamesindustry.biz/is-unity-still-focused-on-making-games">we spoke with Riccitiello last year</a> about Unity's decades-long bubble bath in red ink, he explained it was simply the company's strategy, where it took losses in order to fuel growth.
</p>
<p>
<strong>QUOTE</strong> | "Should you be profitable? Well we could be, but we wouldn't be serving the industry in the ways we do now. Our market shares wouldn't have increased." – Riccitiello, noting that Unity's market share went from the low teens to <a href="https://unity.com/solutions/game#:~:text=Over%2070%25%20of%20top%201000%20mobile%20games%20are%20made%20with%20Unity%C2%B2">over 70%</a> as a result of the strategy.
</p>
<p>
<strong>QUOTE</strong> | "Avoid references to markets or market shares or dominance." - A Google internal document surfaced by the Justice Department this week in its antitrust trial against the search giant, as reported by <a href="https://www.bloomberg.com/news/newsletters/2023-09-14/googlers-told-to-avoid-words-like-share-and-bundle-us-says-lmj27bhl">Bloomberg</a>. The company has spent decades coaching employees to avoid certain words and phrases since they can be red flags for monopolistic behavior.
</p>
<p>
The emphasis on growing market share is understandable, but companies don't exist to grow market share. They exist to earn money. Market share is just a means to that end. And you're really only going to be willing to lose billions to build your market share if you know you can take it all back and then some down the road.
</p>
<p>
First, you make yourself essential to the market, even if it costs you billions to get there. Then once you hit a threshold – let's say, I don't know, 70% of the market – you lean into the enshittification process. You charge more for your services, you give your customers worse terms, you turn the heat up slowly and continuously, confident in the knowledge that people are so locked in to your business and have so few viable alternatives that they may grumble but they will ultimately put up with it.
</p>
<p>
What are they going to do? Not use Unity? Are your developers familiar with alternative engines? The competition for top talent is already fierce; how much tougher will it be if you're hiring for a proprietary engine? If you jump to something like Unreal, how do you know Epic won't do the exact same thing once you're invested in their software? I see a lot of developers talking about Godot, but I also see developers talking about Linux whenever Microsoft pulls some underhanded shenanigans with Windows and that doesn't seem to have cracked the OS market wide open.
</p>
<blockquote>Unity lit money on fire for decades to buy a market advantage that overrules the basic economic incentives that supposedly ensure free markets work best for customers</blockquote>
<p>
And what about the developers of existing Unity hits like Call of Duty Mobile, Pokémon Go, and Marvel Snap? If they aren't happy with these changes are they supposed to rebuild their games from scratch in another engine? Take them down entirely just to avoid racking up fees? 
</p>
<p>
Unity lit money on fire for decades to buy a market advantage that overrules the basic economic incentives that supposedly ensure free markets work best for customers. It was successful in doing that because it's very hard for a sustainable business to compete against one that is fine losing billions of dollars.
</p>
<p>
And it's such a common strategy in so many industries today that there's just no sense of horror or outrage from the onlookers. Industry watchers and Serious Business People have seen this play out so many times they just acknowledge it's happening and treat it as if it's a perfectly cool and normal thing and not illegal predatory pricing.
</p>
<p>
<strong>QUOTE</strong> | "Pricing below your own costs is not a violation of the law unless it is part of a strategy to eliminate competitors, and when that strategy has a dangerous probability of creating a monopoly for the discounting firm so that it can raise prices far into the future and recoup its losses." – The FTC explains that <a href="https://www.ftc.gov/advice-guidance/competition-guidance/guide-antitrust-laws/single-firm-conduct/predatory-or-below-cost-pricing#:~:text=Pricing%20below%20your%20own%20costs%20is%20also%20not%20a%20violation%20of%20the%20law%20unless%20it%20is%20part%20of%20a%20strategy%20to%20eliminate%20competitors%2C%20and%20when%20that%20strategy%20has%20a%20dangerous%20probability%20of%20creating%20a%20monopoly%20for%20the%20discounting%20firm%20so%20that%20it%20can%20raise%20prices%20far%20into%20the%20future%20and%20recoup%20its%20losses.">this strategy is basically illegal</a>, even though it hasn't had much luck getting courts to agree. 
</p>
<p>
<strong>QUOTE</strong> | "The market's reaction to the blowback that Unity is receiving from the developer community may be overdone, particularly as developers have few lower-cost options to turn to… Given the utility provided by Unity's solutions and its dominant market share within mobile gaming, we do not anticipate a rush for the exits from its core customers." – In a note to investors Wednesday, Wedbush analyst Michael Pachter says Unity should be fine because the market lacks viable competitors.
</p>
<p>
<strong>QUOTE</strong> | "When it comes to 'fair', it usually depends on who you ask. From Unity's (and likely its shareholders') point of view, it has technically been subsidising developers' work (i.e., operating at a substantial GAAP-basis loss) every year since inception." – In <a href="https://www.gamesindustry.biz/unitys-pricing-is-a-symptom-not-the-cause-of-tougher-times-ahead-for-the-games-industry-opinion">a guest column</a> on Thursday, Midia Research's Karol Severin suggests it could be seen as fair for Unity to add this Runtime Fee because it has lost so much money over the years.
</p>
<p>
The difference here is that Unity <em>deliberately chose to lose money</em>. Unity set its terms and prices for developers every step of the way knowing that they would not be profitable. Developers did not choose this strategy for Unity, so Unity cannot insist that they now make the company whole.
</p>
<p>
Unity also never told developers this was an exchange, a sweetheart deal now in exchange for Unity putting the screws to them later. In fact, that was pretty much the opposite of the sales pitch.
</p>
<p>
<strong>QUOTE</strong> | "There's no royalties, no fucking around. It's simple." – In 2015, John Riccitiello <a href="https://www.gamesindustry.biz/theres-no-royalties-no-f-ing-around-riccitiello">talks to us about the new Unity business model</a>, contrasting it to the royalty-based model Epic had announced the day before for its Unreal Engine.
</p>
<p>
Riccitiello was on a bit of a roll that day, so let's let him expound on that thought.
</p>
<p>
<strong>QUOTE</strong> | "I do think you could argue that royalties are quite a bit like free-to-play. They sort of hook you and then try to exploit that relationship. That's not what we're trying to do."
</p>
<p>
No? Because that is 100% what you have done.
</p>
<p>
<strong>QUOTE</strong> |"If you were to walk around Unity, you'll find this point about transparency, clarity... democracy is like every other paragraph of every other conversation. It's a deeply embedded value. We thought for a while about things like royalties, [but] we just didn't think it was right. We thought about the nickel-and-dime model of free-to-play, not to implement it, just to see whether it had any implications for us, but we didn't think so."
</p>
<p>
Obviously Unity's a very different place these days. There's not much clarity, as we've seen from the continual restatements about how the Runtime Fee works and what it covers. There's not much transparency, as Unity isn't telling people how it figures out how much they'll owe. And there certainly isn't much democracy, because Unity's subscribers certainly wouldn't vote for what's happened this week, and it must have happened over the concerns and objections of a whole lot of non-executive Unity employees, too.  
</p>
<p>
The internal pushback against this has actually escalated beyond any reasonable bounds, with police saying <a href="https://www.gamesindustry.biz/unity-closes-offices-in-wake-of-death-threat">the death threat that closed two of Unity's offices yesterday and today was made by an employee</a>.
</p>
<p>
Before we move on, there's one more quote from that "no royalties, no fucking around" Riccitiello interview that we should get to.
</p>
<p>
<strong>QUOTE</strong> | "With Unity, it's capped. It's $75 a month or $1,500 for a perpetual license; we're not nickel-and-diming people and we're not charging them a royalty. When we say it's free, it's free. When we say $75 a month, it's $75 a month. Yeah, you can buy other stuff from us. We're not a one-trick pony, but we're not charging a royalty, which I think is akin to looking for whales."
</p>
<p>
I don't know what's more hypocritical here, telling people you're not nickel-and-diming them and explicitly nickel-and-diming them with per install fees that range from two dimes to a fraction of a nickel, or throwing shade on whale hunting when you have spent years making tools to help whale hunters hunt whales better and are in the midst of pivoting toward whale hunting yourself.
</p>
<p>
<strong>QUOTE</strong> | "The price increase is very targeted. In fact, more than 90% of our customers will not be affected by this change." – In <a href="https://twitter.com/unity/status/1702077049425596900">one of its many "clarifications" this week</a>, Unity says the Runtime Fee was designed in such a way that it will bring in a lot of money from a relative handful of its biggest customers.
</p>
<p>
This of course isn't the first sign that Riccitiello had a change of heart about whale hunting, given his well-publicized remark last year that <a href="https://www.gamesindustry.biz/riccitiello-developers-who-push-back-against-monetisation-are-pure-brilliant-and-fucking-idiots">developers who focus on creativity rather than monetization are "some of the biggest fucking idiots."</a>
</p>
<h2>The reason Unity did <em>this</em></h2>
<p>
One angle I find interesting here is that Unity had alternative methods of enshittification it could have resorted to that would not have resulted in the same blowback.
</p>
<p>
For example, it could have simply jacked up the cost of its various subscriptions a bit and gestured in the direction of inflation. So if Unity could have padded its coffers without the grief, why upend the status quo like this?
</p>
<p>
Part of it seems to be a way to upsell developers to get them even more deeply invested in Unity's ecosystem. They can reduce their Runtime Fees by upgrading their subscription program, or using Unity Gaming Services live-ops backend solution or Unity LevelPlay (the IronSource ad monetization platform), which would be an appealing way to increase Unity's market share in other services that it can enshittify down the road.
</p>
<blockquote>However impactful this might seem today, Unity believes it's going to be more impactful to the way the industry develops down the road</blockquote>
<p>
Beyond that, I suspect another part of the reasoning here is because however impactful this might seem today, Unity believes it's going to be more impactful to the way the industry develops down the road.
</p>
<p>
We already know Unity management are big believers in AI, and if companies really can use the tech to cut headcount, they'll not only be paying fewer salaries but fewer Unity seat license fees. So if Unity expects its existing (unprofitable) model to be disrupted, it's naturally going to want to adopt a model better suited to the industry landscape of the future.
</p>
<p>
Fortunately, Unity released <a href="https://create.unity.com/gaming-report">a 2023 Gaming Report</a> earlier this year detailing trends it sees shaping that landscape, a number of which would make the Runtime Fee more profitable for the company going forward. 
</p>
<p>
<strong>QUOTE</strong> | "Studios are starting more mobile-only games compared to 2021" – One of Unity's top five trends to watch.
</p>
<p>
<strong>STAT</strong> | 44% - The increase in mobile-only games made by large studios (300+ people) in 2022, according to Unity's 2023 Gaming Report. The report found a slight decline in such projects from mid-size studios (10-49 people), while every other size of studio saw an increase in mobile-only game development.
</p>
<p>
The Runtime Fee policy is designed with free-to-play games most obviously in mind. It makes no distinction between monetization models, but it's pretty clear where the upside is considering a free-to-play hit can result in one billion downloads while an absurdly successful premium game like Grand Theft Auto 5 has been running strong for a decade and <a href="https://www.gamesindustry.biz/take-two-growth-slows-with-anniversary-of-zynga-acquisition#:~:text=During%20the%20quarter%2C%20Grand%20Theft%20Auto%205%20added%20another%205%20million%20to%20its%20units%20shipped%20total%2C%20which%20now%20stands%20at%20185%20million%20copies">still shipped "just" 185 million copies</a>.
</p>
<p>
That doesn't mean smaller developers won't be hurt by the fee, especially since they will pay the most per download by far and are the least likely to be running a free-to-play live service game. It's just to say that John Riccitiello's eyes are much more likely to do the Tex Avery horny wolf gag when he's reading about <a href="https://www.gamesindustry.biz/lessons-from-the-endless-runner-minion-rush">Minion Rush</a> than the latest indie darling.
</p>
<p>
So if you're seeing a marked shift toward mobile games, especially from the deep-pocketed studios with the greatest ability to produce one of those rare mobile megahits, pivoting your monetization model to take greater advantage of the free-to-play games that dominate the mobile industry makes sense.
</p>
<p>
<strong>QUOTE</strong> | "Large studios are increasing the number of multiplatform games" – Another of Unity's top five trends to watch.
</p>
<p>
Unity found that in 2022, large studios (300 or more people) released 16% more multiplatform games than in 2021, and 110% more compared to 2019. On top of that, mobile was an overwhelmingly common option for multiplatform games. 75% of large studios' multiplatform offerings had a mobile option, as did 100% of upper midmarket (150-299 people) studio's multiplatform titles. 
</p>
<p>
Given how commonly multiplatform players will use the game on multiple platforms (creating multiple installs) and how much more likely these larger studios are to launch the kind of megahits that would pay eternal dividends through Runtime Fees, this is another trend the new change would benefit from.
</p>
<p>
<strong>QUOTE</strong> | "In 2022, there were more players but fewer payers"– One more point from Unity's 2023 Gaming Report.
</p>
<p>
Last year, daily active users went up 8% year-over-year for the median game, while the rate of paying active users went down 2% for games in the top 50th percentile. That may not sound like a ton, but if you pull back to include games in the 90th percentile, the decline in the rate of paying active users was 30%. The average dollar value per transaction was also down 8% for the top 50th percentile, and 17% for the top 90th percentile.
</p>
<p>
If Unity thinks mobile revenues are going down while the userbase keeps growing, imposing a fee tied to installs instead of revenues makes sense.
</p>
<p>
I don't think these trends are necessarily going to have massive effects for Unity's business, but the Runtime Fee plan strikes me as an optimization for them, a way to improve Unity's outlook whether the industry evolves this way or that.
</p>
<p>
Honestly, I think this new Runtime Fee makes perfect sense from a mile-high point of view, if you think about Unity as a business where you just turn whichever dials and pull whatever levers will make the numbers go up the most.
</p>
<p>
The only problem is it makes no sense at all if you instead think about Unity as a game development tool that game developers should want to use. 
</p>
<h2>The rest of the week in review</h2>
<p>
<strong>STAT</strong> | $1.3 billion – How much Embracer paid in 2021 to acquire Gearbox Entertainment, the Borderlands developer it is <a href="https://www.gamesindustry.biz/report-embracer-may-sell-gearbox-entertainment">now reportedly looking to sell</a>.
</p>
<p>
<strong>STAT</strong> | $1.5 billion – <a href="https://www.gamesindustry.biz/embracer-group-net-sales-rise-by-47-in-q1">Embracer's net debt as of June 30</a>. The company's financial situation has prompted a restructuring that has seen it close a number of studios, including Saints Row developer Volition and Campfire Cabal.
</p>
<p>
<strong>STAT</strong> | Around 40 people – How many Ascendant Studios developers have been laid off, or <a href="https://www.gamesindustry.biz/ascendant-studios-lays-off-45-of-its-workforce">about 45% of the company</a>. The AAA studio led by former Call of Duty developer Bret Robbins released its first game, Immortals of Aveum, in August with EA as the publisher.
</p>
<p>
<strong>STAT</strong> | 54 people – The number of Ubisoft London employees whose jobs are at risk as Ubisoft <a href="https://www.gamesindustry.biz/hungry-shark-developer-ubisoft-london-faces-closure">announced plans to close the Hungry Shark developer</a>.
</p>
<p>
<strong>STAT</strong> | 76% - The <a href="https://www.gamesindustry.biz/xbox-series-s-and-x-uk-sales-jumped-76-on-starfield-launch">week-on-week sales boost Xbox Series X|S hardware received in the UK</a> for the week ending September 2, when Starfield was first made playable.
</p>
<p>
<strong>QUOTE</strong> | "Sometimes people are receptive, sometimes they clam up, sometimes they hire us back, and sometimes we never hear from them again. It goes to that systemic nature, and are people going with the flow, or are the DEI efforts just lip service? Or are they wanting to interrupt the status quo?" – <a href="https://www.gamesindustry.biz/take-this-the-bumpy-road-for-mental-health-advocacy">Take This executive director Eve Crevoshay</a> says that just because companies make use of the organization's mental health-focused consulting services, that doesn't mean they'll be receptive to the group's feedback.
</p>
<p>
<strong>QUOTE</strong> | "So finally we have smartphones that are powerful enough to play a fully fledged console game, with the correct controls, and it will run natively without lag. This feels like the beginning of a major step-change for mobile gaming, and the games industry in general – and yet I have questions and doubts that linger." – Our editor-in-chief James Batchelor <a href="https://www.gamesindustry.biz/aaa-games-on-iphone-15-pro-game-changers-or-gimmick-opinion">takes a closer look at Apple's push to bring current AAA blockbusters to the iPhone 15 Pro</a>. 
</p>
<p>
<strong>STAT</strong> | 9 – The number of years Stig Asmussen spent at Respawn Entertainment, where he led development on Star Wars Jedi: Fallen Order and Survivor. <a href="https://www.gamesindustry.biz/stig-asmussen-leaving-respawn-and-ea">Asmussen left this week</a> to "pursue other adventures."
</p>
<p>
<strong>STAT</strong> | 9 – The number of years Jesse Houston led Phoenix Labs as CEO. We reported this week that <a href="https://www.gamesindustry.biz/phoenix-labs-ceo-and-coo-step-down">studio co-founder Houston and COO Jeanne-Marie Owens are taking more removed roles with the company</a> (director and advisor, respectively). Phoenix Labs' other co-founders Sean Bender and Robin Mayne left the Fae Farm developer earlier this year.
</p>
<p>
<strong>QUOTE</strong> | "Piracy is a mixed bag because obviously piracy is what's kept these games alive for the last 20, 30, 40 years. So the community's done a huge service to the industry keeping them alive, better in some cases than some of the companies themselves." – Antstream CEO Steve Cottam <a href="https://www.gamesindustry.biz/antstream-looks-to-build-a-future-by-preserving-the-past">discusses the emulation-based retro game streaming service</a> and how he sees it preserving gaming's past.
</p>
<p>
<strong>STAT</strong> | Up to $300 million – How much Playtika is paying <a href="https://www.gamesindustry.biz/playtika-acquires-mobile-studio-innplay-labs">to acquire Tiles of Fortune developer Innplay Labs</a>.
</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I wouldn't be productive enough for today's academic system (2013) (199 pts)]]></title>
            <link>https://www.theguardian.com/science/2013/dec/06/peter-higgs-boson-academic-system</link>
            <guid>37535814</guid>
            <pubDate>Sat, 16 Sep 2023 15:46:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/science/2013/dec/06/peter-higgs-boson-academic-system">https://www.theguardian.com/science/2013/dec/06/peter-higgs-boson-academic-system</a>, See on <a href="https://news.ycombinator.com/item?id=37535814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Peter Higgs, the British physicist who gave his name to the <a href="https://www.theguardian.com/science/higgs-boson" data-link-name="in body link" data-component="auto-linked-tag">Higgs boson</a>, believes no university would employ him in today's academic system because he would not be considered "productive" enough.</p><p>The emeritus professor at Edinburgh University, who says he has never sent an email, browsed the internet or even made a mobile phone call, published fewer than 10 papers after his groundbreaking work, which identified the mechanism by which subatomic material acquires mass, was published in 1964.</p><p>He doubts a similar breakthrough could be achieved in today's academic culture, because of the expectations on academics to collaborate and keep churning out papers. He said: "It's difficult to imagine how I would ever have enough peace and quiet in the present sort of climate to do what I did in 1964."</p><p>Speaking to the Guardian en route to Stockholm to receive the 2013 Nobel prize for science, Higgs, 84, said he would almost certainly have been sacked had he not been nominated for the Nobel in 1980.</p><p>Edinburgh University's authorities then took the view, he later learned, that he "might get a Nobel prize – and if he doesn't we can always get rid of him".</p><p>Higgs said he became "an embarrassment to the department when they did research assessment exercises". A message would go around the department saying: "Please give a list of your recent publications." Higgs said: "I would send back a statement: 'None.' "</p><p>By the time he retired in 1996, he was uncomfortable with the new academic culture. "After I retired it was quite a long time before I went back to my department. I thought I was well out of it. It wasn't my way of doing things any more. Today I wouldn't get an academic job. It's as simple as that. I don't think I would be regarded as productive enough."</p><p>Higgs revealed that his career had also been jeopardised by his disagreements in the 1960s and 70s with the then principal, Michael Swann, who went on to chair the BBC. Higgs objected to Swann's handling of student protests and to the university's shareholdings in South African companies during the apartheid regime. "[Swann] didn't understand the issues, and denounced the student leaders."</p><p>He regrets that the particle he identified in 1964 became known as the "God particle".</p><p>He said: "Some people get confused between the science and the theology. They claim that what happened at <a href="https://www.theguardian.com/science/cern" data-link-name="in body link" data-component="auto-linked-tag">Cern</a> proves the existence of God."</p><p>An atheist since the age of 10, he fears the nickname "reinforces confused thinking in the heads of people who are already thinking in a confused way. If they believe that story about creation in seven days, are they being intelligent?"</p><p>He also revealed that he turned down a knighthood in 1999. "I'm rather cynical about the way the honours system is used, frankly. A whole lot of the honours system is used for political purposes by the government in power."</p><p>He has not yet decided which way he will vote in the referendum on <a href="https://www.theguardian.com/politics/scottish-independence" data-link-name="in body link" data-component="auto-linked-tag">Scottish independence</a>. "My attitude would depend a little bit on how much progress the lunatic right of the Conservative party makes in trying to get us out of Europe. If the UK were threatening to withdraw from Europe, I would certainly want Scotland to be out of that."</p><p>He has never been tempted to buy a television, but was persuaded to watch The Big Bang Theory last year, and said he wasn't impressed.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[French supermarket using ‘shrinkflation’ stickers to pressure PepsiCo and others (119 pts)]]></title>
            <link>https://www.cnn.com/2023/09/15/business-food/carrefour-shrinkflation-stickers/index.html</link>
            <guid>37535575</guid>
            <pubDate>Sat, 16 Sep 2023 15:23:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2023/09/15/business-food/carrefour-shrinkflation-stickers/index.html">https://www.cnn.com/2023/09/15/business-food/carrefour-shrinkflation-stickers/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=37535575">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/lede-e8838c7a931167c3b573d0577e18d5e5@published" data-name="carrefour 011321" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6665" data-original-height="1333" data-original-width="2000" data-url="https://media.cnn.com/api/v1/images/stellar/prod/230915090247-carrefour-011321.jpg?c=original" data-editable="lede" data-freewheel-lede="true">
       <picture><source height="720" width="1280" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/230915090247-carrefour-011321.jpg?c=16x9&amp;q=h_720,w_1280,c_fill/f_webp" type="image/webp"><source height="540" width="960" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/230915090247-carrefour-011321.jpg?c=16x9&amp;q=h_540,w_960,c_fill/f_webp" type="image/webp"><source height="270" width="480" media="(-webkit-min-device-pixel-ratio: 2)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/230915090247-carrefour-011321.jpg?c=16x9&amp;q=h_270,w_480,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/230915090247-carrefour-011321.jpg?c=16x9&amp;q=h_720,w_1280,c_fill" alt="Supermarkets like Carrefour are also guilty of 'shrinkflation' in their private label products." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="1333" width="2000"></picture>
    </div><div data-editable="content" itemprop="articleBody" data-reorderable="content">
                    <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_0F00418C-4BD2-FD85-1520-97CE2142D07A@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      French supermarket chain Carrefour has slapped price warnings on products ranging<strong> </strong>from Lindt chocolates to Lipton Ice Tea to pressure suppliers such as <a href="http://cnn.com/2023/02/16/business-food/nestle-food-consumer-prices/index.html" target="_blank">Nestlé</a>, PepsiCo and Unilever <a href="https://cnn.com/2023/06/06/business/grocery-sales-prices/index.html" target="_blank">to cut their prices</a>. 
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_62B98D90-0360-94AC-53B1-97D16C69915D@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Carrefour is putting stickers on products that have shrunk in size but cost more even though<strong> </strong>raw materials prices have eased. It is trying to rally consumer support as retailers prepare to face the world’s biggest brands in negotiations due to start soon and end by October 15.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_2207F057-F7B5-59A8-D146-97D16C6A6AFE@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      “Obviously, the aim in stigmatizing these products is to be able to tell manufacturers to rethink their pricing policy,” Stefen Bompais, director of client communications at Carrefour, said in an interview.
  </p>

  


  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_E125CF96-DDD9-D012-0855-97D16C6AFDEC@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Carrefour CEO Alexandre Bompard, who also heads French retail industry lobby group FDC, has repeatedly said consumer goods companies are not cooperating in efforts to cut the price of thousands of staples despite a fall in the cost of raw materials.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_62C92392-ACF2-2BEE-BFCF-97D16C6B5AD0@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      In this he is backed by French finance minister Bruno Le Maire, who in June summoned 75 big retailers and consumer groups to his ministry urging them to cut prices. After a new round of meetings last month, Le Maire said Unilever (<a href="https://edition.cnn.com/markets/stocks/UL" target="_blank">UL</a>), Nestlé and PepsiCo (<a href="https://edition.cnn.com/markets/stocks/PEP" target="_blank">PEP</a>) were among companies not toeing the line on prices.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_DA0269C4-684F-560D-645B-97D16C6B37C3@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Since Monday, Carrefour has marked 26 products in its stores in France with labels reading, “This product has seen its volume or weight fall and the effective price by the supplier rise.”
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_F8BCAEB1-263D-49C0-A608-97D16C6C534A@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      For example, Carrefour said a bottle of sugar-free peach-flavored Lipton Ice Tea, produced by PepsiCo, shrank to 1.25 liters (0.33 gallons) from 1.5 liters (0.3 gallons), resulting in a 40% effective increase in the price per liter.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_F68959C6-AEC6-5A0D-C461-97D16C6DA722@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Guigoz infant formula produced by Nestlé went from 900 grams (31.75 oz) to 830 grams (29 oz), while Unilever’s Viennetta ice-cream cake shrank to 320 grams (11 oz) from 350 grams (12 oz).
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_636B181D-3969-9F2E-6002-97D16C6E34EB@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Lindt’s “chocolat au lait extra fin” was one of three of the Swiss chocolatier’s products named in the list.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_E8ADD263-060C-3580-2250-97D16C6ED0BE@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      “Lindt &amp; Sprüngli increased its prices groupwide<strong> </strong>on average by 9.3% in line with local cost structures,” a company spokesperson told Reuters. “We have made a concerted effort to compensate for increased costs by increasing efficiency as much as possible. Therefore, we have only passed on the costs we could not absorb ourselves in the form of price increases to our customers.”
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_C92BB82F-E68E-7DFB-6D8E-97D16C6F956B@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      PepsiCo did not respond to a request for comment. Nestlé and Unilever declined to comment.
  </p>

  


  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_70C73A46-BE70-7C59-9122-97D16C705450@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Consumer groups say “shrinkflation” is a widespread practice, which supermarkets like Carrefour are also guilty of in their private label products.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_C6FD93C4-CD9E-4D05-AFBB-97F0E6F3589B@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      France, like other European countries, has been trying for months to ease consumer pain in the face of a surge in the cost of living, strong-arming big business to freeze or cut food and transport prices — with mixed results.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_40C8B38D-8D15-C6A7-513D-97D16C709608@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      The shrinkflation warnings are in all French Carrefour stores, and will last until the targeted suppliers agree to price cuts, Bompais said. The retailer could extend warnings to other goods, but does not plan to extend the initiative to other countries.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/paragraph_29F0B441-1254-CEE9-3525-97D16C71DA6D@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Le Maire said last month that consumer goods companies and retailers had agreed to bring forward annual price negotiations — which would normally have taken place next year — to September. The talks will result in price cuts from January, he said.
  </p>

                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Life-changing cystic fibrosis treatment wins US$3M Breakthrough Prize (272 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-02890-1</link>
            <guid>37535537</guid>
            <pubDate>Sat, 16 Sep 2023 15:18:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-02890-1">https://www.nature.com/articles/d41586-023-02890-1</a>, See on <a href="https://news.ycombinator.com/item?id=37535537">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02890-1/d41586-023-02890-1_26045804.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02890-1/d41586-023-02890-1_26045804.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Left to Right: Negulescu, Hadida and Vangoor headshots." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02890-1/d41586-023-02890-1_26045804.jpg">
  <figcaption>
   <p><span>Paul Negulescu, Sabine Hadida and Fredrick Van Goor (left to right) developed a combination treatment that targets a malformed protein in people with cystic fibrosis.</span><span>Credit: Vertex Pharmaceuticals</span></p>
  </figcaption>
 </picture>
</figure><p>The triple-drug combination Trikafta has given a new lease of life to 90% of people with cystic fibrosis, an inherited disorder that affects the lungs and other organs. Now, the trio of researchers who spearheaded its development has won one of this year’s US$3-million Breakthrough prizes — the most lucrative awards in science.</p><p>Sabine Hadida, Paul Negulescu and Fredrick Van Goor at Vertex Pharmaceuticals in San Diego, California, developed the treatment by combining different drugs that help a faulty protein to function.</p><p>“The development of Trikafta has been one of the most phenomenal and outstanding achievements of biomedical research in the last 30 years,” says geneticist and physician Francis Collins at the US National Human Genome Research Institute (NHGRI) in Bethesda, Maryland, who co-discovered the gene for cystic fibrosis in 1989<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>.</p><p>The award was one of five Breakthrough prizes — awarded for achievements in life sciences, physics and mathematics — announced on 14 September.</p><h2>Life-changing treatment</h2><p>Cystic fibrosis affects around 100,000 people worldwide, and for many years was considered a life-limiting condition. However, a study this year projected that treatment with drugs such as Trikafta (comprising elexacaftor, tezacaftor and ivacaftor) — approved by the US Food and Drug Administration in 2019 — can increase life expectancy from around 30 to more than 80 years<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. “I hear almost every day from people who were really in trouble, with cystic fibrosis severely beginning to affect their chance of survival,” says Collins. “Now, after going on Trikafta, they are back at work and thinking about what they might want to do for retirement.”</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-020-02106-w" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02890-1/d41586-023-02890-1_18223328.jpg"><p>Cystic fibrosis drugs target the malformed proteins at the root of the disease</p></a>
 </article><p>The disease is caused by mutations in the gene that makes the cystic fibrosis transmembrane conductance regulator protein (CFTR), which ordinarily spans the membrane of cells that line several organs and is involved in the production of mucus, sweat and other fluids. In people with cystic fibrosis, these proteins are misfolded and don’t function correctly. This causes a build-up of unusually thick secretions, including mucus in the lungs, leading to serious health issues.</p><p>Once the <i>CFTR</i> gene had been identified, most efforts focused on ways to modify the gene to treat the disease, with little success. The team led by Hadida, Negulescu and Van Goor instead searched for a <a href="https://www.nature.com/articles/d41586-020-02106-w" data-track="click" data-label="https://www.nature.com/articles/d41586-020-02106-w" data-track-category="body text link">drug combination to coax the misfolded proteins into functioning correctly</a>. The three drugs in Trikafta work together: two help to deliver more CFTR to the cell surface and the third enables the protein to work better once it is there<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>.</p><p>“There was a lot of scepticism that this could be done,” Hadida recalls. “But the patient community was cheering for us.” The drug-discovery process required a marathon effort, testing the effects of more than one million compounds on isolated human lung cells to identify candidate drugs for clinical trials.</p><p>Hadida says she feels “very excited, honoured and surprised” by the award, adding that credit should be shared by the company, people with cystic fibrosis and their families.</p><h2>Genetic links</h2><p>A second life-sciences prize was awarded for the independent discovery of two genes associated with the risk of developing Parkinson’s disease: <i>GBA1</i><sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup>, identified by geneticist Ellen Sidransky at the NHGRI, and <i>LRRK2</i><sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup><sup>,</sup><sup><a href="#ref-CR6" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">6</a></sup> by neurogeneticist Andrew Singleton at the US National Institute on Aging in Bethesda and neuroscientist Thomas Gasser at the University of Tübingen, Germany.</p><p>Sidransky describes being “blown away” after learning of her award, especially because she didn’t set out to study Parkinson’s disease. Originally a paediatrician, she moved to researching the rare hereditary disorder Gaucher disease, in which fatty substances build up in various organs because of a mutation in the <i>GBA1</i> gene. When talking to people with the condition, she realized that many had relatives with Parkinson’s disease, allowing her to connect the dots. “I want to be a cheerleader for research into rare diseases,” Sidransky says. “This is a good example where a study can lead to insights for the wider population.”</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-00241-0" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02890-1/d41586-023-02890-1_20092060.jpg"><p>Last-resort cancer therapy holds back disease for more than a decade</p></a>
 </article><p>Njideka Okubadejo, a neurologist at the University of Lagos in Nigeria, welcomes the award announcement. Okubadejo, Singleton — who leads the Global Parkinson’s Genetics Program — and others have identified a new genetic risk factor for Parkinson’s disease in the <i>GBA1</i> gene in people with African ancestry that is rarely seen in those of European descent<sup><a href="#ref-CR7" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">7</a></sup>. “The next step is to find a biological mechanism by which the gene causes the disease,” Okubadejo says. “Then you can build upon that to reduce the likelihood of the disease occurring.”</p><p>A third life-sciences prize was awarded to immunologists Carl June at the University of Pennsylvania in Philadelphia, and Michel Sadelain at the Memorial Sloan Kettering Cancer Center in New York City for developing a CAR-T-cell immunotherapy treatment for leukaemia that stimulates patients’ own immune T cells to <a href="https://www.nature.com/articles/d41586-022-04465-y" data-track="click" data-label="https://www.nature.com/articles/d41586-022-04465-y" data-track-category="body text link">target and kill cancer cells</a><sup><a href="#ref-CR8" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">8</a></sup><sup>,</sup><sup><a href="#ref-CR9" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">9</a></sup>. This therapy can lead to <a href="https://www.nature.com/articles/d41586-022-00241-0" data-track="click" data-label="https://www.nature.com/articles/d41586-022-00241-0" data-track-category="body text link">long-lasting remission</a> of some cancers<sup><a href="#ref-CR10" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">10</a></sup>.</p><h2>Theories of everything</h2><p>Other Breakthrough winners include John Cardy at the University of Oxford, UK, and Alexander Zamolodchikov at Stony Brook University in New York, who share the physics prize for a body of work on ‘conformal field theories’ — a family of mathematical theories that applies to a huge range of physical phenomena, from boiling water to the surface of black holes. In most situations, the equations are extremely tough to solve exactly, but Zamolodchikov discovered a solution in 2D in the 1980s<sup><a href="#ref-CR11" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">11</a></sup>, and Cardy built on these results by proposing that the result could be generalized to 4D<sup><a href="#ref-CR12" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">12</a></sup> and by applying conformal field theories to better understand phase transitions in materials.</p><p>Cardy, who was “over the moon” to learn about the prize, recalls learning about Zamolodchikov’s work from two Russian scientists at a conference. “Hardly anybody in the West understood what they were talking about,” Cardy says. “But I came away convinced I had to start working on it.”</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-02999-9" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02890-1/d41586-023-02890-1_23522070.jpg"><p>AlphaFold developers win US$3-million Breakthrough Prize</p></a>
 </article><p>“Cardy and Zamolodchikov truly are giants of conformal field theories,” says David Tong, a physicist at the University of Cambridge, UK. “There are physicists around the world who are working on extraordinarily different problems, and yet they have a common language through the work of Cardy, Zamolodchikov and others.”</p><p>Simon Brendle at Columbia University in New York City has won the mathematics Breakthrough prize for his contributions to differential geometry — the study of curves, surfaces and spaces. Among other achievements, Brendle proved ‘Lawson’s conjecture’ concerning the minimal surface of a ‘3-sphere’, which is the surface of a hypothetical 4D sphere<sup><a href="#ref-CR13" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">13</a></sup>.</p><p>The Breakthrough prizes were founded in 2012 and are sponsored by Yuri Milner, a Russian-Israeli billionaire, and other Internet entrepreneurs, including Meta’s chief executive Mark Zuckerberg.</p>
                </div><div id="references" aria-labelledby="Bib1"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference"><li data-counter="1."><p id="ref-CR1">Riordan, J. R. <i>et al.</i> <i>Science</i> <b>245</b>, 1066–1073 (1989).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.2475911" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.2475911" aria-label="Article reference 1" data-doi="10.1126/science.2475911">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=2475911" aria-label="PubMed reference 1">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Science&amp;doi=10.1126%2Fscience.2475911&amp;volume=245&amp;pages=1066-1073&amp;publication_year=1989&amp;author=Riordan%2CJ.%20R.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">Lopez, A., Daly, D., Vega-Hernandez, G., MacGregor, G. &amp; Rubin, J. L. <i>J. Cyst. Fibros.</i> <b>22</b>, 607–614 (2023).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.jcf.2023.02.004" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.jcf.2023.02.004" aria-label="Article reference 2" data-doi="10.1016/j.jcf.2023.02.004">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36849331" aria-label="PubMed reference 2">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=J.%20Cyst.%20Fibros.&amp;doi=10.1016%2Fj.jcf.2023.02.004&amp;volume=22&amp;pages=607-614&amp;publication_year=2023&amp;author=Lopez%2CA.&amp;author=Daly%2CD.&amp;author=Vega-Hernandez%2CG.&amp;author=MacGregor%2CG.&amp;author=Rubin%2CJ.%20L.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="3."><p id="ref-CR3">Keating, D. <i>et al.</i> <i>N. Engl. J. Med.</i> <b>379</b>, 1612–1620 (2018).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1056/NEJMoa1807120" data-track-action="article reference" href="https://doi.org/10.1056%2FNEJMoa1807120" aria-label="Article reference 3" data-doi="10.1056/NEJMoa1807120">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30334692" aria-label="PubMed reference 3">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=N.%20Engl.%20J.%20Med.&amp;doi=10.1056%2FNEJMoa1807120&amp;volume=379&amp;pages=1612-1620&amp;publication_year=2018&amp;author=Keating%2CD.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="4."><p id="ref-CR4">Sidransky, E. <i>et al.</i> <i>N. Engl. J. Med.</i> <b>361</b>, 1651–1661 (2009).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1056/NEJMoa0901281" data-track-action="article reference" href="https://doi.org/10.1056%2FNEJMoa0901281" aria-label="Article reference 4" data-doi="10.1056/NEJMoa0901281">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19846850" aria-label="PubMed reference 4">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=N.%20Engl.%20J.%20Med.&amp;doi=10.1056%2FNEJMoa0901281&amp;volume=361&amp;pages=1651-1661&amp;publication_year=2009&amp;author=Sidransky%2CE.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="5."><p id="ref-CR5">Gilks, W. P. <i>et al.</i> <i>Lancet</i> <b>365</b>, 415–416 (2005).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S0140-6736(05)17830-1" data-track-action="article reference" href="https://doi.org/10.1016%2FS0140-6736%2805%2917830-1" aria-label="Article reference 5" data-doi="10.1016/S0140-6736(05)17830-1">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15680457" aria-label="PubMed reference 5">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Lancet&amp;doi=10.1016%2FS0140-6736%2805%2917830-1&amp;volume=365&amp;pages=415-416&amp;publication_year=2005&amp;author=Gilks%2CW.%20P.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="6."><p id="ref-CR6">Zimprich, A. <i>et al.</i> <i>Neuron</i> <b>44</b>, 601–607 (2004).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2004.11.005" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2004.11.005" aria-label="Article reference 6" data-doi="10.1016/j.neuron.2004.11.005">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15541309" aria-label="PubMed reference 6">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2004.11.005&amp;volume=44&amp;pages=601-607&amp;publication_year=2004&amp;author=Zimprich%2CA.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="7."><p id="ref-CR7">Rizig, M. <i>et al.</i> <i>Lancet Neurol</i>. https://doi.org/10.1016/S1474-4422(23)00283-1 (2023).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S1474-4422(23)00283-1" data-track-action="article reference" href="https://doi.org/10.1016%2FS1474-4422%2823%2900283-1" aria-label="Article reference 7" data-doi="10.1016/S1474-4422(23)00283-1">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37633302" aria-label="PubMed reference 7">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Lancet%20Neurol.&amp;doi=10.1016%2FS1474-4422%2823%2900283-1&amp;publication_year=2023&amp;author=Rizig%2CM.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="8."><p id="ref-CR8">Brentjens, R. J. <i>et al.</i> <i>Nature Med.</i> <b>9</b>, 279–286 (2003).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nm827" data-track-action="article reference" href="https://doi.org/10.1038%2Fnm827" aria-label="Article reference 8" data-doi="10.1038/nm827">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12579196" aria-label="PubMed reference 8">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Nature%20Med.&amp;doi=10.1038%2Fnm827&amp;volume=9&amp;pages=279-286&amp;publication_year=2003&amp;author=Brentjens%2CR.%20J.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="9."><p id="ref-CR9">Kalos, M. <i>et al.</i> <i>Sci. Transl. Med.</i> <b>3</b>, 95ra73 (2011).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/scitranslmed.3002842" data-track-action="article reference" href="https://doi.org/10.1126%2Fscitranslmed.3002842" aria-label="Article reference 9" data-doi="10.1126/scitranslmed.3002842">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21832238" aria-label="PubMed reference 9">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Sci.%20Transl.%20Med.&amp;doi=10.1126%2Fscitranslmed.3002842&amp;volume=3&amp;publication_year=2011&amp;author=Kalos%2CM.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="10."><p id="ref-CR10">Melenhorst, J. J. <i>et al.</i> <i>Nature</i> <b>602</b>, 503–509 (2022).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41586-021-04390-6" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-021-04390-6" aria-label="Article reference 10" data-doi="10.1038/s41586-021-04390-6">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=35110735" aria-label="PubMed reference 10">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Nature&amp;doi=10.1038%2Fs41586-021-04390-6&amp;volume=602&amp;pages=503-509&amp;publication_year=2022&amp;author=Melenhorst%2CJ.%20J.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="11."><p id="ref-CR11">Belavin, A. A., Polyakov, A. M. &amp; Zamolodchikov, A. B. <i>Nucl. Phys. B</i> <b>241</b>, 333–380 (1984).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/0550-3213(84)90052-X" data-track-action="article reference" href="https://doi.org/10.1016%2F0550-3213%2884%2990052-X" aria-label="Article reference 11" data-doi="10.1016/0550-3213(84)90052-X">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Nucl.%20Phys.%20B&amp;doi=10.1016%2F0550-3213%2884%2990052-X&amp;volume=241&amp;pages=333-380&amp;publication_year=1984&amp;author=Belavin%2CA.%20A.&amp;author=Polyakov%2CA.%20M.&amp;author=Zamolodchikov%2CA.%20B.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="12."><p id="ref-CR12">Cardy, J. L. <i>Phys. Lett. B</i> <b>215</b>, 749–752 (1988).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/0370-2693(88)90054-8" data-track-action="article reference" href="https://doi.org/10.1016%2F0370-2693%2888%2990054-8" aria-label="Article reference 12" data-doi="10.1016/0370-2693(88)90054-8">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Phys.%20Lett.%20B&amp;doi=10.1016%2F0370-2693%2888%2990054-8&amp;volume=215&amp;pages=749-752&amp;publication_year=1988&amp;author=Cardy%2CJ.%20L.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="13."><p id="ref-CR13">Brendle, S. <i>Acta. Math.</i> <b>211</b>, 177–190 (2013).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s11511-013-0101-2" data-track-action="article reference" href="https://doi.org/10.1007%2Fs11511-013-0101-2" aria-label="Article reference 13" data-doi="10.1007/s11511-013-0101-2">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Acta.%20Math.&amp;doi=10.1007%2Fs11511-013-0101-2&amp;volume=211&amp;pages=177-190&amp;publication_year=2013&amp;author=Brendle%2CS.">
                    Google Scholar</a>&nbsp;
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/d41586-023-02890-1?format=refman&amp;flavour=references">Download references</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Game developers turning off all IronSource and Unity Ads monetization (220 pts)]]></title>
            <link>https://docs.google.com/document/d/16PzpX6qIwJu57jCB2fhxqKtmxA6A6QmZWifsWZBqI2w/view</link>
            <guid>37535187</guid>
            <pubDate>Sat, 16 Sep 2023 14:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.google.com/document/d/16PzpX6qIwJu57jCB2fhxqKtmxA6A6QmZWifsWZBqI2w/view">https://docs.google.com/document/d/16PzpX6qIwJu57jCB2fhxqKtmxA6A6QmZWifsWZBqI2w/view</a>, See on <a href="https://news.ycombinator.com/item?id=37535187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Einige Tools sind möglicherweise nicht verfügbar, da die Datei gerade von vielen Nutzern bearbeitet wird.<a href="https://docs.google.com/document/d/16PzpX6qIwJu57jCB2fhxqKtmxA6A6QmZWifsWZBqI2w/edit">Bitte versuchen Sie es noch einmal.</a> <a href="https://support.google.com/docs/answer/2494822#share_with_many_people" target="_blank">Weitere Informationen</a><a href="#" id="docs-too-many-users-bar">Schließen</a></p></div></div>]]></description>
        </item>
    </channel>
</rss>