<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 14 Nov 2023 23:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Happy 1700M Epoch Second (130 pts)]]></title>
            <link>https://www.epochconverter.com/</link>
            <guid>38270582</guid>
            <pubDate>Tue, 14 Nov 2023 22:14:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.epochconverter.com/">https://www.epochconverter.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38270582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentwrapper">

<div id="clocktext"><p>The current <a href="https://www.epochconverter.com/clock">Unix epoch time</a> is&nbsp;
</p><p>1699995752</p>
</div>

<h2 id="efhead">Convert epoch to human-readable date and vice versa</h2>

<br>

&nbsp;
<p><span id="preferencelink"></span><br>
Press <kbd>c</kbd> to clear all forms.</p>
<h2 id="brhead">Epoch dates for the start and end of the year/month/day</h2>

<br>
<h2 id="tchead">Convert seconds to days, hours and minutes</h2>


<h2>What is epoch time?</h2>
<p>The <b>Unix epoch</b> (or <b>Unix time</b> or <b>POSIX time</b> or <b>Unix timestamp</b>) is the number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT), not counting leap seconds (in ISO 8601: 1970-01-01T00:00:00Z).
Literally speaking the epoch is Unix time 0 (midnight 1/1/1970), but 'epoch' is often used as a synonym for Unix time.
Some systems store epoch dates as a signed 32-bit integer, which might cause problems on January 19, 2038 (known as the Year 2038 problem or Y2038).
The converter on this page converts timestamps in seconds (10-digit), milliseconds (13-digit) and microseconds (16-digit) to readable dates.</p>
<div><table><thead>
<tr><th>Human-readable time&nbsp;</th><th>Seconds</th></tr>
</thead><tbody>
<tr><td>1 hour</td><td>3600 seconds</td></tr>
<tr><td>1 day</td><td>86400 seconds</td></tr>
<tr><td>1 week</td><td>604800 seconds</td></tr>
<tr><td>1 month (30.44 days)&nbsp;</td><td>2629743 seconds</td></tr>
<tr><td>1 year (365.24 days)&nbsp;</td><td>&nbsp;31556926 seconds</td></tr>
</tbody></table></div>
<h2 id="code">How to get the current epoch time in ...</h2>
<table>
<tbody><tr><td>PHP</td><td><code>time()</code> <a href="https://www.epochconverter.com/programming/php">More PHP</a></td></tr>
<tr><td>Python</td><td><code>import time; time.time()</code> <a target="_blank" href="https://docs.python.org/3/library/time.html" title="Source"><span>Source</span></a></td></tr>
<tr><td>Ruby</td><td><code>Time.now</code> (or <code>Time.new</code>). To display the epoch: <code>Time.now.to_i</code></td></tr>
<tr><td>Perl</td><td><code>time</code> <a href="https://www.epochconverter.com/programming/perl">More Perl</a></td></tr>
<tr><td>Java</td><td><code>long epoch = System.currentTimeMillis()/1000;</code> Returns epoch in seconds.</td></tr>
<tr><td>C#</td><td><code>DateTimeOffset.Now.ToUnixTimeSeconds()</code> (.NET Framework 4.6+/.NET Core), older versions: <code>var epoch = (DateTime.UtcNow - new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc)).TotalSeconds;</code></td></tr>
<tr><td>Objective-C</td><td><code>[[NSDate date] timeIntervalSince1970];</code> (returns double) or <code>NSString *currentTimestamp = [NSString stringWithFormat:@"%f", [[NSDate date] timeIntervalSince1970]];</code></td></tr>
<tr><td>C++11</td><td><code>double now = std::chrono::duration_cast&lt;std::chrono::seconds&gt;(std::chrono::system_clock::now().time_since_epoch()).count();</code></td></tr>
<tr><td>Lua</td><td><code>epoch = os.time([date])</code></td></tr>
<tr><td>VBScript/ASP</td><td><a href="https://www.epochconverter.com/programming/#asp">See the examples</a></td></tr>
<tr><td>AutoIT</td><td><code>_DateDiff('s', "1970/01/01 00:00:00", _NowCalc())</code></td></tr>
<tr><td title="Embarcadero Delphi">Delphi</td><td><code>Epoch := DateTimetoUnix(Now);</code> Tested in Delphi 2010.</td></tr>
<tr><td>R</td><td><code>as.numeric(Sys.time())</code></td></tr>
<tr><td>Erlang/OTP</td><td><code>erlang:system_time(seconds).</code> (version 18+), older versions: <code>calendar:datetime_to_gregorian_seconds(calendar:universal_time())-719528*24*3600.</code></td></tr>
<tr><td>MySQL</td><td><code>SELECT unix_timestamp(now())</code> <a href="https://www.epochconverter.com/programming/mysql">More MySQL examples</a></td></tr>
<tr><td>PostgreSQL</td><td><code>SELECT extract(epoch FROM now());</code></td></tr>
<tr><td>SQLite</td><td><code>SELECT strftime('%s', 'now');</code></td></tr>
<tr><td>Oracle PL/SQL</td><td><code>SELECT (CAST(SYS_EXTRACT_UTC(SYSTIMESTAMP) AS DATE) - TO_DATE('01/01/1970','DD/MM/YYYY')) * 24 * 60 * 60 FROM DUAL;</code></td></tr>
<tr><td>SQL Server</td><td><code>SELECT DATEDIFF(s, '1970-01-01 00:00:00', GETUTCDATE())</code></td></tr>
<tr><td>IBM Informix</td><td><code>SELECT dbinfo('utc_current') FROM sysmaster:sysdual;</code></td></tr>
<tr><td>JavaScript</td><td><code>Math.floor(new Date().getTime()/1000.0)</code> The getTime method returns the time in milliseconds.</td></tr>
<tr><td>Visual FoxPro</td><td><code>DATETIME() - {^1970/01/01 00:00:00}</code> Warning: time zones not handled correctly</td></tr>
<tr><td>Go</td><td><code>time.Now().Unix()</code> <a rel="nofollow" target="_blank" href="https://play.golang.org/p/6h0A0WPxtq">More Go </a></td></tr>
<tr><td>Adobe ColdFusion</td><td><code>&lt;cfset epochTime = left(getTickcount(), 10)&gt;</code></td></tr>
<tr><td>Tcl/Tk</td><td><code>clock seconds</code></td></tr>
<tr><td>Unix/Linux Shell</td><td><code>date +%s</code></td></tr>
<tr><td>Solaris</td><td><code>/usr/bin/nawk 'BEGIN {print srand()}'</code> Solaris doesn't support <i>date +%s</i>, but the default seed value for nawk's random-number generator is the number of seconds since the epoch.</td></tr>
<tr><td>PowerShell</td><td><code>[int][double]::Parse((Get-Date (get-date).touniversaltime() -UFormat %s))</code></td></tr>
<tr><td>Other OS's </td><td>Command line: <code>perl -e "print time"</code> (If Perl is installed on your system)</td></tr>
</tbody></table>

<h2>Convert from human-readable date to epoch</h2>
<table>
<tbody><tr><td>PHP</td><td><code>strtotime("15 November 2018")</code> (converts most English date texts) or:<br><code>date_create('11/15/2018')-&gt;format('U')</code> (using DateTime class) <a href="https://www.epochconverter.com/programming/php#date2epoch">More PHP</a></td></tr>
<tr><td>Python</td><td><code>import calendar, time; calendar.timegm(time.strptime('2000-01-01 12:34:00', '%Y-%m-%d %H:%M:%S'))</code></td></tr>
<tr><td>Ruby</td><td><code>Time.local(<i>year</i>, <i>month</i>, <i>day</i>, <i>hour</i>, <i>minute</i>, <i>second</i>, <i>usec</i> )</code> (or <code>Time.gm</code> for GMT/UTC input). To display add <code>.to_i</code></td></tr>
<tr><td>Perl</td><td>Use the <a href="https://www.epochconverter.com/programming/perl">Perl Epoch routines</a></td></tr>
<tr><td>Java</td><td><code>long epoch = new java.text.SimpleDateFormat("MM/dd/yyyy HH:mm:ss").parse("01/01/1970 01:00:00").getTime() / 1000;</code> Timestamp in seconds, remove '/1000' for milliseconds.</td></tr>
<tr><td>VBScript/ASP</td><td><code>DateDiff("s", "01/01/1970 00:00:00", <i>time field</i>)</code> <a href="https://www.epochconverter.com/programming/#asp">More ASP</a></td></tr>
<tr><td>AutoIT</td><td><code>_DateDiff('s', "1970/01/01 00:00:00", "YYYY/MM/DD HH:MM:SS")</code></td></tr>
<tr><td title="Embarcadero Delphi">Delphi</td><td><code>Epoch := DateTimeToUnix(StrToDateTime(myString));</code></td></tr>
<tr><td>C</td><td>Use the <a href="https://www.epochconverter.com/programming/c">C Epoch Converter routines</a></td></tr>
<tr><td>R</td><td><code>as.numeric(as.POSIXct("YYYY-MM-dd HH:mm:ss", tz = "GMT", origin="1970-01-01"))</code> The origin parameter is optional</td></tr>
<tr><td>Go</td><td><a rel="nofollow" target="_blank" href="https://play.golang.org/p/6h0A0WPxtq">Example code </a></td></tr>
<tr><td>Rust</td><td>
<code>SystemTime::now().duration_since(SystemTime::UNIX_EPOCH)</code> <a rel="nofollow" target="_blank" href="https://doc.rust-lang.org/std/time/struct.SystemTime.html"></a></td></tr>
<tr><td>Adobe ColdFusion</td><td><code>int(parseDateTime(datetime).getTime()/1000);</code></td></tr>
<tr><td>MySQL</td><td><code>SELECT unix_timestamp(<i>time</i>)</code> Time format: YYYY-MM-DD HH:MM:SS or YYMMDD or YYYYMMDD<br><a href="https://www.epochconverter.com/programming/mysql">More on using Epoch timestamps with MySQL</a></td></tr>
<tr><td>PostgreSQL</td><td><code>SELECT extract(epoch FROM date('2000-01-01 12:34'));</code><br>With timestamp: <code>SELECT EXTRACT(EPOCH FROM TIMESTAMP WITH TIME ZONE '2018-02-16 20:38:40-08');</code><br>With interval: <code>SELECT EXTRACT(EPOCH FROM INTERVAL '5 days 3 hours');</code></td></tr>
<tr><td>SQLite</td><td><code>SELECT strftime('%s',<i>timestring</i>);</code></td></tr>
<tr><td>SQL Server</td><td> <code>SELECT DATEDIFF(s, '1970-01-01 00:00:00', <i>time field</i>)</code></td></tr>
<tr><td>JavaScript</td><td>Use the <a href="https://www.epochconverter.com/programming/#javascript">JavaScript Date object</a></td></tr>
<tr><td>Unix/Linux Shell</td><td><code>date +%s -d"Jan 1, 1980 00:00:01"</code> Replace '-d' with '-ud' to input in GMT/UTC time.</td></tr>
</tbody></table>
<h2>Convert from epoch to human-readable date</h2>
<table>
<tbody><tr><td>PHP</td><td><code>date(<i>output format</i>, <i>epoch</i>);</code> Output format example: 'r' = RFC 2822 date, <a href="https://www.epochconverter.com/programming/php#epoch2date">more PHP examples</a></td></tr>
<tr><td>Python</td><td><code>import time; time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.localtime(<i>epoch</i>))</code>
Replace time.localtime with time.gmtime for GMT time. Or using datetime:
<code>import datetime; datetime.datetime.utcfromtimestamp(<i>epoch</i>).replace(tzinfo=datetime.timezone.utc)</code></td></tr>
<tr><td>Ruby</td><td><code>Time.at(<i>epoch</i>)</code></td></tr>
<tr><td>C#</td><td><code>private string epoch2string(int epoch) {<br>
return new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc).AddSeconds(epoch).ToShortDateString(); }</code></td></tr>
<tr><td>Perl</td><td>Use the <a href="https://www.epochconverter.com/programming/perl">Perl Epoch routines</a></td></tr>
<tr><td>Java</td><td><code>String date = new java.text.SimpleDateFormat("MM/dd/yyyy HH:mm:ss").format(new java.util.Date (<i>epoch</i>*1000));</code> Epoch in seconds, remove '*1000' for milliseconds.</td></tr>
<tr><td>Lua</td><td><code>datestring = os.date([format[,epoch]])</code></td></tr>
<tr><td>VBScript/ASP</td><td><code>DateAdd("s", <i>epoch</i>, "01/01/1970 00:00:00")</code> <a href="https://www.epochconverter.com/programming/#asp">More ASP</a></td></tr>
<tr><td>AutoIT</td><td><code>_DateAdd("s", $EpochSeconds , "1970/01/01 00:00:00")</code></td></tr>
<tr><td title="Embarcadero Delphi">Delphi</td><td><code>myString := DateTimeToStr(UnixToDateTime(Epoch));</code> Where Epoch is a signed integer.</td></tr>
<tr><td>C</td><td>Use the <a href="https://www.epochconverter.com/programming/c">C Epoch Converter routines</a></td></tr>
<tr><td>Objective-C</td><td><code>NSDate * myDate = [NSDate dateWithTimeIntervalSince1970:<i>epoch</i>]; NSLog(@"%@", date);</code></td></tr>
<tr><td>R</td><td><code>as.POSIXct(epoch, origin="1970-01-01", tz="GMT")</code></td></tr>
<tr><td>Go</td><td><a rel="nofollow" target="_blank" href="https://play.golang.org/p/6h0A0WPxtq">Example code </a></td></tr>
<tr><td>Adobe ColdFusion</td><td><code>DateAdd("s",epoch,"1/1/1970");</code></td></tr>
<tr><td>MySQL</td><td><code>FROM_UNIXTIME(<i>epoch</i>, <i>optional output format</i>)</code> Default output format is YYY-MM-DD HH:MM:SS. If you need support for negative timestamps: <code>DATE_FORMAT(DATE_ADD(FROM_UNIXTIME(0), interval -315619200 second),"%Y-%m-%d")</code> (replace -315619200 with epoch) <a href="https://www.epochconverter.com/programming/mysql">More MySQL</a></td></tr>
<tr><td>PostgreSQL</td><td>PostgreSQL version 8.1 and higher: <code>SELECT to_timestamp(<i>epoch</i>);</code> <a rel="nofollow" target="_blank" href="https://www.postgresql.org/docs/current/static/functions-formatting.html" title="Source"><span>Source</span></a> Older versions: <code>SELECT TIMESTAMP WITH TIME ZONE 'epoch' + <i>epoch</i> * INTERVAL '1 second';</code> </td></tr>
<tr><td>SQLite</td><td><code>SELECT datetime(<i>epoch_to_convert</i>, 'unixepoch');</code> or local timezone: <code>SELECT datetime(<i>epoch_to_convert</i>, 'unixepoch', 'localtime');</code></td></tr>
<tr><td>Oracle PL/SQL</td><td><code>SELECT to_date('01-JAN-1970','dd-mon-yyyy')+(1526357743/60/60/24) from dual</code><br>Replace 1526357743 with epoch.</td></tr>
<tr><td>SQL Server</td><td> <code>DATEADD(s, <i>epoch</i>, '1970-01-01 00:00:00')</code></td></tr>
<tr><td>IBM Informix</td><td><code>SELECT dbinfo('utc_to_datetime',<i>epoch</i>) FROM sysmaster:sysdual;</code></td></tr>
<tr><td>Microsoft Excel / LibreOffice Calc</td><td> <code>=(A1 / 86400) + 25569</code> Format the result cell for date/time, the result will be in GMT time (A1 is the cell with the epoch number). For other time zones: =((A1 +/- <a href="https://www.epochconverter.com/timezones">time zone adjustment</a>) / 86400) + 25569.</td></tr>
<tr><td>Crystal Reports</td><td> <code>DateAdd("s", {EpochTimeStampField}-14400, #1/1/1970 00:00:00#)</code> -14400 used for Eastern Standard Time. See <a href="https://www.epochconverter.com/timezones">Time Zones</a>.</td></tr>
<tr><td>JavaScript</td><td>Use the <a href="https://www.epochconverter.com/programming/#javascript">JavaScript Date object</a></td></tr>
<tr><td>Tcl/Tk</td><td><code>clock format 1325376000</code> <a rel="nofollow" target="_blank" href="https://www.tcl.tk/man/tcl8.6/TclCmd/clock.htm" title="Documentation"><span>Documentation</span></a></td></tr>
<tr><td>MATLAB</td><td><code>datestr(719529+TimeInSeconds/86400,'dd-mmm-yyyy HH:MM:SS')</code></td></tr>
<tr><td>IBM PureData System for Analytics</td><td><code>select 996673954::int4::abstime::timestamp;</code></td></tr>
<tr><td>Unix/Linux Shell</td><td><code>date -d @1520000000</code> Replace 1520000000 with your epoch, needs recent version of 'date'. Replace '-d' with '-ud' for GMT/UTC time.</td></tr>
<tr><td>Mac OS X</td><td><code>date -j -r 1520000000</code></td></tr>
<tr><td>PowerShell</td><td><code>Function get-epochDate ($epochDate)
{ [timezone]::CurrentTimeZone.ToLocalTime(([datetime]'1/1/1970').AddSeconds($epochDate)) }</code>, then use: <code>get-epochDate 1520000000</code>. Works for Windows PowerShell v1 and v2</td></tr>
<tr><td>Other OS's</td><td>Command line: <code>perl -e "print scalar(localtime(<i>epoch</i>))"</code> (If Perl is installed) Replace 'localtime' with 'gmtime' for GMT/UTC time.</td></tr>
</tbody></table>
<p><br>Thanks to everyone who sent me corrections and updates!</p>
<p>More date related programming examples: <a href="https://www.epochconverter.com/weeknumbers">What's the current week number?</a> - <a href="https://www.epochconverter.com/daynumbers">What's the current day number?</a></p>
<p><i>Please note:</i> All tools on this page are based on the date &amp; time settings of <i>your computer</i> and use JavaScript to convert times. Some browsers use the current DST (Daylight Saving Time) rules for all dates in history. JavaScript does not support <a target="_blank" href="https://en.wikipedia.org/wiki/Leap_second" rel="nofollow">leap seconds</a>.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Reptar', a new CPU vulnerability (121 pts)]]></title>
            <link>https://cloud.google.com/blog/products/identity-security/google-researchers-discover-reptar-a-new-cpu-vulnerability</link>
            <guid>38268043</guid>
            <pubDate>Tue, 14 Nov 2023 19:14:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/identity-security/google-researchers-discover-reptar-a-new-cpu-vulnerability">https://cloud.google.com/blog/products/identity-security/google-researchers-discover-reptar-a-new-cpu-vulnerability</a>, See on <a href="https://news.ycombinator.com/item?id=38268043">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>This year, Google has seen an increase in the number of vulnerabilities impacting central processing units (CPU) across hardware systems. Two of the most notable of these vulnerabilities were disclosed in August, when Google researchers discovered <a href="http://downfall.page/" target="_blank">Downfall</a> (<a href="https://nvd.nist.gov/vuln/detail/CVE-2022-40982" target="_blank">CVE-2022-40982</a>) and <a href="https://lock.cmpxchg8b.com/zenbleed.html" target="_blank">Zenbleed</a> (<a href="https://nvd.nist.gov/vuln/detail/CVE-2023-20593" target="_blank">CVE-2023-20593</a>), affecting Intel and AMD CPUs, respectively.</p><p>This trend proves only to be intensifying as time goes on. Left unmitigated, these types of vulnerabilities can impact billions of personal and cloud computers.</p><p>Today, we’re detailing the findings of <a href="https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00950.html" target="_blank">Reptar</a> (<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-23583" target="_blank">CVE-2023-23583</a>), a new CPU vulnerability that impacts several Intel desktop, mobile, and server CPUs. Google’s Information Security Engineering team reported the vulnerability to Intel, who disclosed the vulnerability today. Thanks to the thoughtful collaboration between Google, Intel, and industry partners, mitigations have been rolled out, and Googlers and our customers are protected. </p><h3><b>How Google found and responded to Reptar</b></h3><p>A Google security researcher identified a vulnerability related to how redundant prefixes are interpreted by the CPU which leads to bypassing the CPU’s security boundaries if exploited successfully. Prefixes allow you to change how instructions behave by enabling or disabling features. The full rules are complicated, but in general, if you use a prefix that doesn't make sense or conflicts with other prefixes, we call those redundant. Usually, redundant prefixes are ignored.</p><p>The impact of this vulnerability is demonstrated when exploited by an attacker in a multi-tenant virtualized environment, as the exploit on a guest machine causes the host machine to crash resulting in a Denial of Service to other guest machines running on the same host. Additionally, the vulnerability could potentially lead to information disclosure or privilege escalation.</p><p>You can read more technical details about the vulnerability at our <a href="https://lock.cmpxchg8b.com/reptar.html" target="_blank">researcher’s blog</a>.</p><p>Our security teams were able to identify this vulnerability and responsibly disclose it to Intel. Google worked with industry partners to identify and test a successful mitigation so all users are protected from this risk in a timely manner. In particular, Google’s response team ensured a successful rollout of the mitigation to our systems before it posed a risk to our customers, mainly Google Cloud and ChromeOS customers.</p><h3><b>Google’s commitment to collaboration and hardware security</b></h3><p>As Reptar, Zenbleed, and Downfall suggest, computing hardware and processors remain susceptible to these types of vulnerabilities. This trend will only continue as hardware becomes increasingly complex. This is why Google continues to invest heavily in CPU and <a href="https://bughunters.google.com/" target="_blank">vulnerability research</a>. Work like this, done in close collaboration with our industry partners, allows us to keep users safe and is critical to finding and mitigating vulnerabilities before they can be exploited.</p><p>We look forward to continuing this proactive cybersecurity work, and encourage others to join us on this journey to create a more secure and resilient technology ecosystem.</p></span></section><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/identity-security" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/identity-security" track-metadata-module="tag list" track-metadata-module_headline="posted in">Security &amp; Identity</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vehicles with higher, more vertical front ends pose greater risk to pedestrians (287 pts)]]></title>
            <link>https://www.iihs.org/news/detail/vehicles-with-higher-more-vertical-front-ends-pose-greater-risk-to-pedestrians</link>
            <guid>38267588</guid>
            <pubDate>Tue, 14 Nov 2023 18:42:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iihs.org/news/detail/vehicles-with-higher-more-vertical-front-ends-pose-greater-risk-to-pedestrians">https://www.iihs.org/news/detail/vehicles-with-higher-more-vertical-front-ends-pose-greater-risk-to-pedestrians</a>, See on <a href="https://news.ycombinator.com/item?id=38267588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<article id="content" v-track-docs="">
					<section>
						
						
						<span>
							<p>Vehicles with especially tall front ends are most dangerous to pedestrians, but a blunt profile makes medium-height vehicles deadly too.</p>
						</span>
						
						<p>November 14, 2023</p>
					</section>
					<!-- .article-head -->
					<figure>
						<img src="https://www.iihs.org/media/cbc04042-1dc0-4dae-ab98-4c47a9fe8868/-_C93w/News/2023/111423%20pedestrians%20and%20front-end%20shape/111423-news.jpg" alt="">
						<figcaption></figcaption>
					</figure>
					<p>Vehicles with especially tall front ends are most dangerous to pedestrians, but a blunt profile makes medium-height vehicles deadly too, new research from the Insurance Institute for Highway Safety shows.</p>
					<p>Whatever their nose shape, pickups, SUVs and vans with a hood height greater than 40 inches are about 45 percent more likely to cause fatalities in pedestrian crashes than cars and other vehicles with a hood height of 30 inches or less and a sloping profile, an IIHS study of nearly 18,000 pedestrian crashes found. However, among vehicles with hood heights between 30 and 40 inches, a blunt, or more vertical, front end increases the risk to pedestrians.</p>
					<p>“Some of today’s vehicles are pretty intimidating when you’re passing in front of them in a crosswalk,” IIHS President David Harkey said. “These results tell us our instincts are correct: More aggressive-looking vehicles can indeed do more harm.”</p>
					<p>Pedestrian crash deaths have risen 80 percent since hitting their low in 2009. Nearly 7,400 walkers — more than 20 people a day — lost their lives in 2021 after being struck by a vehicle. While speeding and poorly designed infrastructure have helped fuel the increase, many safety advocates have also drawn a connection to the growing portion of the U.S. vehicle fleet made up of pickups and SUVs.</p>
					<p>Over the past 30 years, the average U.S. passenger vehicle has gotten about 4 inches wider, 10 inches longer, 8 inches taller and 1,000 pounds heavier. Many vehicles are more than 40 inches tall at the leading edge of the hood. On some large pickups, the hoods are almost at eye level for many adults.</p>
					<p>To examine the connection between fatality risk and vehicle size and shape, IIHS researchers analyzed 17,897 crashes involving a single passenger vehicle and a single pedestrian. Using Vehicle Identification Numbers to identify the crash-involved vehicles, they calculated key front-end measurements corresponding to 2,958 unique car, minivan, large van, SUV and pickup models from photographs. They excluded vehicles with pedestrian automatic emergency braking systems and controlled for other factors that could affect the likelihood of a fatality, such as the speed limit and age and sex of the struck pedestrian.</p>
					<p>Vehicles with hoods more than 40 inches off the ground at the leading edge and a grille sloped at an angle of 65 degrees or less were 45 percent more likely to cause pedestrian fatalities than those with a similar slope and hood heights of 30 inches or less. Vehicles with hood heights of more than 40 inches and blunt front ends angled at greater than 65 degrees were 44 percent more likely to cause fatalities.</p>
					<p>“Manufacturers can make vehicles less dangerous to pedestrians by lowering the front end of the hood and angling the grille and hood to create a sloped profile,” said IIHS Senior Research Transportation Engineer Wen Hu, the lead author of the study. “There’s no functional benefit to these massive, blocky fronts.”</p>
					<p>While sloping front ends did not reduce the risk posed by vehicles with the tallest hoods, they did make a difference for vehicles with hood heights of 30-40 inches. Compared with low and sloped vehicles, medium-height vehicles with blunt fronts were 26 percent more likely to cause pedestrian fatalities. In contrast, the risk of a fatality was about the same for medium-height vehicles with sloped fronts as for low vehicles with either blunt or sloped fronts.</p>
					<p>The researchers looked at several other vehicle characteristics, including the angle of the windshield, length of the hood and angle of the hood. Among these, the slope of the hood had the biggest effect. There was a 25 percent increase in the risk of a fatality for vehicles with flat hoods — those with angles of 15 degrees or less — compared with vehicles with more sloping hoods. That was true regardless of height and front-end shape.</p>
					<p>To better understand how vehicles of different geometries injure pedestrians, IIHS examined detailed records from 121 crashes collected by the International Center for Automotive Medicine Pedestrian Consortium. In each crash, the front end of a car, pickup or SUV struck a teenager or adult. The data included detailed crash reconstructions, including information about the motion of the pedestrian’s body during the crash and the nature and severity of their injuries. The reports also included the year, make and model of the striking vehicle and the height of the pedestrian.</p>
					<p>The researchers used the same measurements as those used in the larger study to define vehicles with blunt and sloped front ends and tall and short ones. For this study, however, they divided the involved vehicles into only two height groups because of the smaller sample size. Taller vehicles were defined as those with a hood leading edge more than 35 inches off the ground. Shorter ones were those with a hood leading edge 35 inches or less from the ground.</p>
					<p>In general, vehicles taller than 35 inches were more dangerous to pedestrians than the shorter ones, mainly because they tended to cause more severe head injuries. Among vehicles taller than 35 inches, those with vertical front ends were more dangerous than those with sloped front ends. Torso and hip injuries from these vehicles were more frequent and severe.</p>
					<p>Unlike all other vehicle types, tall and blunt vehicles primarily inflicted torso injuries with their front ends rather than with the tops of their hoods. They were more likely to injure pedestrians by throwing them forward, while tall and sloped vehicles usually rolled them onto the hood of the vehicle first.</p>
					<p>Pedestrians who were shorter relative to the height of the striking vehicle also suffered more severe injuries.</p>
					<p>“It’s clear that the increasing size of the vehicles in the U.S. fleet is costing pedestrians their lives,” Harkey said. “We encourage automakers to consider these findings and take a hard look at the height and shape of their SUVs and pickups.”</p>
					<h3>Comparative risk of pedestrian&nbsp;fatality&nbsp;by hood leading edge height and shape</h3>
					<figure><img src="https://www.iihs.org/media/4e3e3373-3392-4c3d-9ce4-d5d171781b80/s-IsXg/News/2023/111423%20pedestrians%20and%20front-end%20shape/front-end-graphic-all.png" alt="Vehicle front-end shapes"></figure>
					<!-- .article-text -->
					
				</article>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Insanely Fast Whisper: Transcribe 300 minutes of audio in less than 98 seconds (153 pts)]]></title>
            <link>https://github.com/chenxwh/insanely-fast-whisper</link>
            <guid>38266833</guid>
            <pubDate>Tue, 14 Nov 2023 17:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/chenxwh/insanely-fast-whisper">https://github.com/chenxwh/insanely-fast-whisper</a>, See on <a href="https://news.ycombinator.com/item?id=38266833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-insanely-fast-whisper" dir="auto"><a href="#insanely-fast-whisper">Insanely Fast Whisper</a></h2>
<p dir="auto">Powered by 🤗 <em>Transformers</em>, <em>Optimum</em> &amp; <em>flash-attn</em></p>
<p dir="auto"><strong>TL;DR</strong> - Transcribe <strong>300</strong> minutes (5 hours) of audio in less than <strong>98</strong> seconds - with <a href="https://huggingface.co/openai/whisper-large-v3" rel="nofollow">OpenAI's Whisper Large v3</a>. Blazingly fast transcription is now a reality!⚡️</p>
<p dir="auto">Not convinced? Here are some benchmarks we ran on a free <a href="https://github.com/chenxwh/insanely-fast-whisper/blob/main/notebooks">Google Colab T4 GPU</a>! 👇</p>
<table>
<thead>
<tr>
<th>Optimisation type</th>
<th>Time to Transcribe (150 mins of Audio)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformers (<code>fp32</code>)</td>
<td>~31 (<em>31 min 1 sec</em>)</td>
</tr>
<tr>
<td>Transformers (<code>fp16</code> + <code>batching [24]</code> + <code>bettertransformer</code>)</td>
<td>~5 (<em>5 min 2 sec</em>)</td>
</tr>
<tr>
<td><strong>Transformers (<code>fp16</code> + <code>batching [24]</code> + <code>Flash Attention 2</code>)</strong></td>
<td><strong>~2 (<em>1 min 38 sec</em>)</strong></td>
</tr>
<tr>
<td>distil-whisper (<code>fp16</code> + <code>batching [24]</code> + <code>bettertransformer</code>)</td>
<td>~3 (<em>3 min 16 sec</em>)</td>
</tr>
<tr>
<td><strong>distil-whisper (<code>fp16</code> + <code>batching [24]</code> + <code>Flash Attention 2</code>)</strong></td>
<td><strong>~1 (<em>1 min 18 sec</em>)</strong></td>
</tr>
<tr>
<td>Faster Whisper (<code>fp16</code> + <code>beam_size [1]</code>)</td>
<td>~9.23 (<em>9 min 23 sec</em>)</td>
</tr>
<tr>
<td>Faster Whisper (<code>8-bit</code> + <code>beam_size [1]</code>)</td>
<td>~8 (<em>8 min 15 sec</em>)</td>
</tr>
</tbody>
</table>
<p dir="auto">Try the Relicate demo here: <a href="https://replicate.com/cjwbw/insanely-fast-whisper" rel="nofollow"><img src="https://camo.githubusercontent.com/9266324ca64204ed64b925470d4309e352de02642b18b378a22061644b478972/68747470733a2f2f7265706c69636174652e636f6d2f636a7762772f696e73616e656c792d666173742d776869737065722f6261646765" alt="Replicate" data-canonical-src="https://replicate.com/cjwbw/insanely-fast-whisper/badge"></a></p>
<h2 tabindex="-1" id="user-content--blazingly-fast-transcriptions-via-your-terminal-️" dir="auto"><a href="#-blazingly-fast-transcriptions-via-your-terminal-️">🆕 Blazingly fast transcriptions via your terminal! ⚡️</a></h2>
<p dir="auto">We've added a CLI to enable fast transcriptions. Here's how you can use it:</p>
<p dir="auto">Install <code>insanely-fast-whisper</code> with <code>pipx</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipx install insanely-fast-whisper"><pre>pipx install insanely-fast-whisper</pre></div>
<p dir="auto">Run inference from any path on your computer:</p>
<div dir="auto" data-snippet-clipboard-copy-content="insanely-fast-whisper --file-name <filename or URL>"><pre>insanely-fast-whisper --file-name <span>&lt;</span>filename or URL<span>&gt;</span></pre></div>
<p dir="auto">🔥 You can run <a href="https://huggingface.co/openai/whisper-large-v3" rel="nofollow">Whisper-large-v3</a> w/ <a href="https://github.com/Dao-AILab/flash-attention">Flash Attention 2</a> from this CLI too:</p>
<div dir="auto" data-snippet-clipboard-copy-content="insanely-fast-whisper --file-name <filename or URL> --flash True "><pre>insanely-fast-whisper --file-name <span>&lt;</span>filename or URL<span>&gt;</span> --flash True </pre></div>
<p dir="auto">🌟 You can run <a href="https://huggingface.co/distil-whisper" rel="nofollow">distil-whisper</a> directly from this CLI too:</p>
<div dir="auto" data-snippet-clipboard-copy-content="insanely-fast-whisper --model-name distil-whisper/large-v2 --file-name <filename or URL> "><pre>insanely-fast-whisper --model-name distil-whisper/large-v2 --file-name <span>&lt;</span>filename or URL<span>&gt;</span> </pre></div>
<p dir="auto">Don't want to install <code>insanely-fast-whisper</code>? Just use <code>pipx run</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipx run insanely-fast-whisper --file-name <filename or URL>"><pre>pipx run insanely-fast-whisper --file-name <span>&lt;</span>filename or URL<span>&gt;</span></pre></div>
<p dir="auto">Note: The CLI is opinionated and currently only works for Nvidia GPUs. Make sure to check out the defaults and the list of options you can play around with to maximise your transcription throughput. Run <code>insanely-fast-whisper --help</code> or <code>pipx run insanely-fast-whisper --help</code> to get all the CLI arguments and defaults.</p>
<h2 tabindex="-1" id="user-content-how-to-use-it-without-a-cli" dir="auto"><a href="#how-to-use-it-without-a-cli">How to use it without a CLI?</a></h2>
<p dir="auto">For older GPUs, all you need to run is:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from transformers import pipeline

pipe = pipeline(&quot;automatic-speech-recognition&quot;,
                &quot;openai/whisper-large-v2&quot;,
                torch_dtype=torch.float16,
                device=&quot;cuda:0&quot;)

pipe.model = pipe.model.to_bettertransformer()

outputs = pipe(&quot;<FILE_NAME>&quot;,
               chunk_length_s=30,
               batch_size=24,
               return_timestamps=True)

outputs[&quot;text&quot;]"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>pipeline</span>

<span>pipe</span> <span>=</span> <span>pipeline</span>(<span>"automatic-speech-recognition"</span>,
                <span>"openai/whisper-large-v2"</span>,
                <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
                <span>device</span><span>=</span><span>"cuda:0"</span>)

<span>pipe</span>.<span>model</span> <span>=</span> <span>pipe</span>.<span>model</span>.<span>to_bettertransformer</span>()

<span>outputs</span> <span>=</span> <span>pipe</span>(<span>"&lt;FILE_NAME&gt;"</span>,
               <span>chunk_length_s</span><span>=</span><span>30</span>,
               <span>batch_size</span><span>=</span><span>24</span>,
               <span>return_timestamps</span><span>=</span><span>True</span>)

<span>outputs</span>[<span>"text"</span>]</pre></div>
<p dir="auto">For newer (A10, A100, H100s), use <a href="https://github.com/chenxwh/insanely-fast-whisper/blob/main">Flash Attention</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from transformers import pipeline

pipe = pipeline(&quot;automatic-speech-recognition&quot;,
                &quot;openai/whisper-large-v2&quot;,
                torch_dtype=torch.float16,
                model_kwargs={&quot;use_flash_attention_2&quot;: True},
                device=&quot;cuda:0&quot;)

outputs = pipe(&quot;<FILE_NAME>&quot;,
               chunk_length_s=30,
               batch_size=24,
               return_timestamps=True)

outputs[&quot;text&quot;]                "><pre><span>import</span> <span>torch</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>pipeline</span>

<span>pipe</span> <span>=</span> <span>pipeline</span>(<span>"automatic-speech-recognition"</span>,
                <span>"openai/whisper-large-v2"</span>,
                <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
                <span>model_kwargs</span><span>=</span>{<span>"use_flash_attention_2"</span>: <span>True</span>},
                <span>device</span><span>=</span><span>"cuda:0"</span>)

<span>outputs</span> <span>=</span> <span>pipe</span>(<span>"&lt;FILE_NAME&gt;"</span>,
               <span>chunk_length_s</span><span>=</span><span>30</span>,
               <span>batch_size</span><span>=</span><span>24</span>,
               <span>return_timestamps</span><span>=</span><span>True</span>)

<span>outputs</span>[<span>"text"</span>]                </pre></div>
<h2 tabindex="-1" id="user-content-roadmap" dir="auto"><a href="#roadmap">Roadmap</a></h2>
<ul>
<li> Add a light CLI script</li>
<li> Deployment script with Inference API</li>
</ul>
<h2 tabindex="-1" id="user-content-community-showcase" dir="auto"><a href="#community-showcase">Community showcase</a></h2>
<p dir="auto">@ochen1 created a brilliant MVP for a CLI here: <a href="https://github.com/ochen1/insanely-fast-whisper-cli">https://github.com/ochen1/insanely-fast-whisper-cli</a> (Try it out now!)</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reptar (193 pts)]]></title>
            <link>https://lock.cmpxchg8b.com/reptar.html</link>
            <guid>38266773</guid>
            <pubDate>Tue, 14 Nov 2023 17:49:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lock.cmpxchg8b.com/reptar.html">https://lock.cmpxchg8b.com/reptar.html</a>, See on <a href="https://news.ycombinator.com/item?id=38266773">Hacker News</a></p>
<div id="readability-page-1" class="page">

<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#discovery">Discovery</a></li>
<li><a href="#solution">Solution</a></li>
<li><a href="#notes">Notes</a></li>
</ul>
</nav>
<p>We have a CPU mystery! We found a way to cause some processors to enter a glitch state where the normal rules don’t apply, but what does that mean…?</p>
<p>If you’re interested what can go wrong inside modern CPUs, read on!</p>
<section id="introduction">
<h2>Introduction</h2>
<p>If you’ve ever written any x86 assembly at all, you’ve probably used <code>rep movsb</code>. It’s the idiomatic way of moving memory around on x86. You set the <em>source</em>, <em>destination</em>, <em>direction</em> and the <em>count</em> - then just let the processor handle all the details!</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>lea</span> <span>rdi</span>, [rel dst]</span>
<span id="cb1-2"><span>lea</span> <span>rsi</span>, [rel src]</span>
<span id="cb1-3"><span>std</span></span>
<span id="cb1-4"><span>mov</span> <span>rcx</span>, <span>32</span></span>
<span id="cb1-5">rep <span>movsb</span></span></code></pre></div>
<p>The actual instruction here is <code>movsb</code>, the <code>rep</code> is simply a prefix that changes how the instruction works. In this case, it indicates that you want this operation <strong>rep</strong>eated multiple times.</p>
<p>There are lots of other prefixes too, but they don’t all apply to every instruction.</p>
<section id="prefix-decoding">
<h4>Prefix Decoding</h4>
<p>An interesting feature of x86 is that the instruction decoding is generally quite relaxed. If you use a prefix that doesn’t make sense or conflicts with other prefixes nothing much will happen, it will usually just be ignored.</p>
<p>This fact is sometimes useful; compilers can use redundant prefixes to pad a single instruction to a desirable alignment boundary.</p>
<p>Take a look at this snippet, this is exactly the same code as above, just a bunch of useless or redundant prefixes have been added:</p>
<div id="cb2"><pre><code><span id="cb2-1">            rep <span>lea</span> <span>rdi</span>, [rel dst]</span>
<span id="cb2-2">             <span>cs</span> <span>lea</span> <span>rsi</span>, [rel src]</span>
<span id="cb2-3">       <span>gs</span> <span>gs</span> <span>gs</span> <span>std</span></span>
<span id="cb2-4">          repnz <span>mov</span> <span>rcx</span>, <span>32</span></span>
<span id="cb2-5">rep rep rep rep <span>movsb</span></span></code></pre></div>
<p>Perhaps the most interesting prefixes are <code>rex</code>, <code>vex</code> and <code>evex</code>, all of which change how subsequent instructions are decoded.</p>
<p>Let’s take a look at how they work.</p>
</section>
<section id="the-rex-prefix">
<h4>The REX prefix</h4>
<p>The i386 only had 8 general purpose registers, so you could specify which register you want to use in just 3 bits (because 2^3 is 8).</p>
<p>The way that instructions were encoded took advantage of this fact, and reserved <em>just</em> enough bits to specify any of those registers.</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/rep-modrm.svg" alt=""><figcaption>modr/m example</figcaption>
</figure>
<p>Simple 2-byte instructions that use modr/m might be encoded like this, for example <code>mov eax, ebx</code>.</p>
<p>This is an 8-bit opcode, 2 bit addressing mode (labeled m), and 3 bits each for the source (s) and destination (d).</p>
</blockquote>
<p>Well, this is a problem, because x86-64 added 8 additional general purpose registers. We now have sixteen possible registers..that’s 2^4, so we’re going to need another bit! 😆</p>
<p>The solution to this is the <code>rex</code> prefix, which gives us some spare bits that the next instruction can borrow.</p>
<p>When we’re talking about rex, we usually write it like this:</p>

<p><code>rex</code> is a single-byte prefix, the first four bits are mandatory and the remaining four bits called <code>b</code>, <code>x</code>, <code>r</code> and <code>w</code> are all optional. If you see <code>rex.rb</code> that means only the <code>r</code> and <code>b</code> bits are set, all the others are unset.</p>
<p>These optional bits give us room to encode more general purpose registers in the following instruction.</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/rep-rexmodrm.svg" alt=""><figcaption>rex example</figcaption>
</figure>
<p>The <code>rex</code> prefix can lend the next instruction extra bits to use for operands, so now we can encode all 16 possible general purpose registers!</p>
</blockquote>
<p>Now we’re fine until someone <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html">adds another register</a>! 😂</p>
</section>
<section id="encoding-rules">
<h4>Encoding Rules</h4>
<p>So now we know that <code>rex</code> increases the available space for encoding operands, and that useless or redundant prefixes are usually ignored on x86. So… what should this instruction do?</p>

<p>The <code>movsb</code> instruction doesn’t have any operands - they’re all implicit - so any <code>rex</code> bits are meaningless, right?</p>
<p>If you guessed that the processor will just silently ignore the <code>rex</code> prefix, you would be correct!</p>
<p>Well… except on machines that support a new feature called <em>fast short repeat move</em>! We discovered that a bug with redundant <code>rex</code> prefixes could interact with this feature in an unexpected way and introduce a serious vulnerability, oops 🙂</p>
</section>
<section id="fast-short-repeat-move">
<h4>Fast Short Repeat Move</h4>
<p>FSRM is a new feature introduced in <a href="https://en.wikichip.org/wiki/intel/microarchitectures/ice_lake_(client)#New_instructions">Ice Lake</a> that fixes some of the shortcomings of ERMS. Hopefully that clears up any confusion. 😆</p>
<p>Just kidding, let’s quickly look at ERMS.</p>
<p>The hard part of moving strings around efficiently is getting all the buffers aligned so you can use the widest possible stores available. You <em>could</em> do this in software, but if we do it in microcode then the processor can just transparently make your existing code faster for you.</p>
<p>This requires some expensive setup, but once that’s done you get vastly improved throughput. This feature is known as <em>enhanced repeat move/store</em>, ERMS.</p>
<p>If you have a processor with ERMS support, simple <code>rep movsb</code> operations can sometimes perform comparably with more complicated hand-tuned vector move operations.</p>
<p>However, there is a problem with ERMS. That initial setup is so expensive that it just isn’t worth it for very short strings. This is what FSRM is designed to solve, it handles the case of only moving 128 bytes or less and makes that faster too!</p>
<p>I’m not aware of any documentation that explains exactly how FSRM works, but you can check if you have a processor that supports it by looking at the flags line in <code>/proc/cpuinfo</code>:</p>
<pre><code>flags       : fpu vme de pse tsc msr pae mce cx8 [...] fsrm</code></pre>
<p>Some of the processors that have this feature include:</p>
<ul>
<li>Ice Lake</li>
<li>Rocket Lake</li>
<li>Tiger Lake</li>
<li>Raptor Lake</li>
<li>Alder Lake</li>
<li>Sapphire Rapids</li>
</ul>
<blockquote>
<p>Note: This list may not be comprehensive, please see Intel advisory INTEL-SA-00950 for a complete list.</p>
</blockquote>
</section>
</section>
<section id="discovery">
<h2>Discovery</h2>
<p>I’ve written previously about a processor validation technique called <em>Oracle Serialization</em> that we’ve been using. The idea is to generate two forms of the same randomly generated program and verify their final state is identical.</p>
<blockquote>
<p>You can read more about Oracle Serialization in my <a href="https://lock.cmpxchg8b.com/zenbleed.html">previous writeup</a>.</p>
</blockquote>
<p>In August, our validation pipeline produced an interesting assertion. It had found a case where adding redundant <code>rex.r</code> prefixes to an FSRM optimized <code>rep movs</code> operation seemed to cause unpredictable results.</p>
<p>We observed some very strange behavior while testing. For example, branches to unexpected locations, unconditional branches being ignored and the processor no longer accurately recording the instruction pointer in <code>xsave</code> or <code>call</code> instuctions.</p>
<p>Oddly, when trying to understand what was happening we would see a debugger reporting impossible states!</p>
<p>This already seemed like it could be indicative of a serious problem, but within a few days of experimenting we found that when multiple cores were triggering the same bug, the processor would begin to report machine check exceptions and halt.</p>
<p>We verified this worked even inside an unprivileged guest VM, so this already has serious security implications for cloud providers. Naturally, we reported this to Intel as soon as we confirmed this was a security issue.</p>
<section id="reproduce">
<h4>Reproduce</h4>
<p>We’re publishing all of our research today to our <a href="https://github.com/google/security-research/">security research repository</a>. If you want to reproduce the vulnerability you can use our <code>icebreak</code> tool, I’ve also made a local mirror available <a href="https://lock.cmpxchg8b.com/files/icebreak.tar.gz">here</a>.</p>
<pre><code>$ ./icebreak -h
usage: ./icebreak [OPTIONS]
    -c N,M      Run repro threads on core N and M.
    -d N        Sleep N usecs between repro attempts.
    -H N        Spawn a hammer thread on core N.
icebreak: you must at least specify a core pair with -c! (see -h for help)</code></pre>
<p>The testcase enters what should be an infinite loop, and unaffected systems should see no output at all. On affected systems, a <code>.</code> is printed on each successful reproduction.</p>
<pre><code>$ ./icebreak -c 0,4
starting repro on cores 0 and 4
.........................................................................
.........................................................................
.........................................................................
.........................................................................
.........................................................................</code></pre>
<p>In general, if the cores are <abbr title="Symmetric Multithreading">SMT</abbr> siblings then you may observe random branches and if they’re <abbr title="Symmetric Multiprocessing">SMP</abbr> siblings from the same package then you may observe machine checks.</p>
<p>If you do <em>not</em> specify two different cores, then you might need to use a hammer thread to trigger a reproduction.</p>
</section>
<section id="analysis">
<h4>Analysis</h4>
<p>We know something strange is happening, but how microcode works in modern systems is a closely guarded secret. We can only theorize about the root cause based on observations.</p>
<section id="μops">
<h5>μops</h5>
<p>The CPU is split in two major components, the <em>frontend</em> and the <em>backend</em>. The frontend is responsible for fetching instructions, decoding them and generating μops to send to the backend for execution.</p>
<p>The backend executes instructions <em>out of order</em>, and uses a unit called the ROB, <em>reorder buffer</em>, to store and organize results.</p>
<p>We believe this bug causes the frontend to miscalculate the size of the <code>movsb</code> instruction, causing subsequent entries in the ROB to be associated with incorrect addresses. When this happens, the CPU enters a confused state that causes the instruction pointer to be miscalculated.</p>
<p>The machine can eventually recover from this state, perhaps with incorrect intermediate results, but becoming internally consistent again. However, if we cause multiple SMT or SMP cores to enter the state simultaneously, we can cause enough microarchitectural state corruption to force a machine check.</p>
</section>
</section>
<section id="questions">
<h4>Questions</h4>
<p>I’m sure some readers will have questions about what is possible in this unexpected “glitch” state. Well, so do we!</p>
<p>We know that we can corrupt the system state badly enough to cause machine check errors, and we’ve also observed threads interfere with execution of processes scheduled on SMT siblings.</p>
<p>However, we simply don’t know if we can control the corruption precisely enough to achieve privilege escalation. I suspect that it <em>is</em> possible, but we don’t have any way to debug μop execution!</p>
<p>If you’re interested in studying this, then we would love to get your input!</p>
</section>
<section id="credit">
<h4>Credit</h4>
<p>This bug was independently discovered by multiple research teams within Google, including the <a href="https://github.com/google/silifuzz">silifuzz</a> team and Google <a href="https://bughunters.google.com/blog">Information Security Engineering</a>. The bug was analyzed by Tavis Ormandy, Josh Eads, Eduardo Vela Nava, Alexandra Sandulescu and Daniel Moghimi.</p>
</section>
</section>
<section id="solution">
<h2>Solution</h2>
<p>Intel have <a href="https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00950.html">published</a> updated microcode for all affected processors. Your operating system or BIOS vendor may already have an update available!</p>
<section id="workaround">
<h4>Workaround</h4>
<p>If you can’t update for some reason, you <em>could</em> disable fast strings via the <code>IA32_MISC_ENABLE</code> model specific register.</p>
<p>This will cause a significant performance penalty, and should not be used unless absolutely necessary.</p>
</section>
</section>
<section id="notes">
<h2>Notes</h2>
<p>If you’re interested in more CPU bugs, we publish everything we find!</p>
<p>Not all the bugs we discover have security consequences, but they’re usually worth reading! For example, did you know that sometimes <a href="https://github.com/google/security-research/tree/master/pocs/cpus/errata/amd/genoa-lps-hps">movlps just doesn’t work</a>? or that registers can sometimes <a href="https://github.com/google/security-research/tree/master/pocs/cpus/errata/amd/1386">roll back</a> to previous values?</p>
</section>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rivian software update bricks infotainment system, fix not obvious (119 pts)]]></title>
            <link>https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/</link>
            <guid>38266340</guid>
            <pubDate>Tue, 14 Nov 2023 17:22:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/">https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/</a>, See on <a href="https://news.ycombinator.com/item?id=38266340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?quality=82&amp;strip=all&amp;w=1500" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1500" height="1000" alt="Rivian-R1T-Apocalypse" fetchpriority="high">
	
	</figure>

<p>On Monday, Rivian released an incremental software update 2023.42, which bricked the infotainment system in R1Ses and R1Ts. The company is frantically working on a fix, but it might not be an OTA…</p>



<p>Update: The company has acknowledged the issue with affected customers but has yet to issue a fix or plan to fix…</p>



<p><a href="https://www.linkedin.com/in/wassymbensaid/">Rivian’s vice president of software engineering</a>, Wassim Bensaid, <a href="https://www.reddit.com/r/Rivian/comments/17usikn/202342_ota_update_issue/">took to Reddit</a> (!?) to update users on the situation.</p>



<blockquote>
<p>Hi All,</p>



<p>We made an error with the 2023.42 OTA update – a fat finger where the wrong build with the wrong security certificates was sent out. We cancelled the campaign and we will restart it with the proper software that went through the different campaigns of beta testing.</p>



<p>Service will be contacting impacted customers and will go through the resolution options. That may require physical repair in some cases.</p>



<p>This is on us – we messed up. Thanks for your support and your patience as we go through this.</p>



<p>* Update 1 (11/13, 10:45 PM PT): The issue impacts the infotainment system. In most cases, the rest of the vehicle systems are still operational. A vehicle reset or sleep cycle will not solve the issue. We are validating the best options to address the issue for the impacted vehicles. Our customer support team is prioritizing support for our customers related to this issue. Thank you.</p>
</blockquote>



<p>That’s the last update we had over 10 hours after Rivian customer vehicles were fed the bad software update. Rivian’s PR team hasn’t replied to requests for comment. </p>



<p>The vehicles are drivable, but software and displays go black. It appears that the 2023.42 software update hangs at 90% on the vehicle screen or 50% on the app screen, and then the vehicle screens black out. All systems appear to still work except for the displays.</p>



<p>One Reddit user noted:</p>



<blockquote>
<p>Remotely setting climate controls appear to be working for me. You can’t adjust it while seated in the vehicle (feature to prevent competing for changes from inside and outside the vehicle?) but for those in cold/hot weather, you can at least pre-set and pre-heat/cool your vehicle even without the infotainment – at least if yours ended up in the same state that mind did.</p>
</blockquote>



<figure></figure>



<p>At the moment, it appears that Amazon vans are not impacted. We reached out to Rivian for comment and got the following response from a spokesperson for the automaker:</p>



<blockquote>
<p>We’ve identified an issue in our recent software update 2023.42.0 that impacts Rivian’s R1T and R1S infotainment system. In most cases, the rest of the vehicle systems are operational. We’ve paused the release of the update and our customer support team is prioritizing support for our customers related to this issue.</p>
</blockquote>



<p>This story is ongoing, and we will update as such.</p>



<p>Update: Rivian has been Texting and emailing affected customers</p>



<figure></figure>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>




	<p>This is a big deal, and not for whatever this fix ends up being, but about trusting the Rivian software team to deliver stuff that won’t break your car. </p>



<p>The fact that this could even happen is very troubling. A bad certificate should not be pushed via a fat thumb that causes the fleet to be bricked. The architecture shouldn’t have been designed this way. </p>



<p>I personally tried updating my vehicle last night before the errors were reported, and if I had been a few minutes earlier, I would also now have a bricked infotainment with the closest software center 200+ miles away and my vehicle covered in a few inches of fresh Vermont snow. </p>



<p>Hopefully, Rivian’s software team (who have probably had an awful night) can come up with an OTA or, more likely, a USB-stick type of update that affected owners can easily and quickly apply. </p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p><p><a href="https://bit.ly/3N3YHyD"><img src="https://electrek.co/wp-content/uploads/sites/3/2023/11/VMAX-Electrek-Banner-Nov-13.jpg?quality=82&amp;strip=all" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Miyazaki's 'The Boy and the Heron' makes clear the world is a fragile place (104 pts)]]></title>
            <link>https://www.latimes.com/entertainment-arts/awards/story/2023-11-13/japanese-animation-master-hayao-miyazaki-boy-and-the-heron</link>
            <guid>38266216</guid>
            <pubDate>Tue, 14 Nov 2023 17:14:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latimes.com/entertainment-arts/awards/story/2023-11-13/japanese-animation-master-hayao-miyazaki-boy-and-the-heron">https://www.latimes.com/entertainment-arts/awards/story/2023-11-13/japanese-animation-master-hayao-miyazaki-boy-and-the-heron</a>, See on <a href="https://news.ycombinator.com/item?id=38266216">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-element="story-body" data-subscriber-content=""> <p>When Hayao Miyazaki’s acclaimed new movie “The Boy and the Heron” made its North American premiere at the Toronto International Film Festival in September, one of the Japanese master’s biggest fans stepped on stage to introduce the long-awaited movie. “We are privileged enough to be living in a time where Mozart is composing symphonies,” said Oscar-winning filmmaker Guillermo del Toro. “Miyazaki <i>san</i> is a master of that stature, and we are so lucky to be here. He has changed the medium that he started in, revolutionized it, proved over and over again that it is a tremendous work of art.”</p><p>“Miyazaki, in my estimation, is the greatest director of animation ever, and he has made his films as full of dialogues and questions as he is,” continued Del Toro. “These are not easy films, but these are films that portray him so intimately, that you feel you’re having a conversation with him. And they are paradoxical because he understands that beauty cannot exist without horror, and delicacy cannot exist without brutality.”</p><p>Fans of the 82-year-old writer-director-animator and co-founder of Studio Ghibli are keenly aware of the magical powers of Miyazaki’s cinematic universe. Perhaps more than any other filmmaker alive today, he has been able to transport audiences to dreamlike worlds that have a distinctive, hand-crafted look and strange logic of their own. Yet, they speak to audiences all over the world because their main characters experience the full spectrum of emotions and experiences. The lead characters of the director’s 12 movies, which include the Oscar-winning “Spirited Away,” “My Neighbor Totoro” and “Princess Mononoke” stumble upon ethereal worlds and supernatural characters, but they are bound by human feelings of joy, love, fear and grief.</p><p>It’s a testament to the enduring appeal of the filmmaker that his new movie broke box-office records in Japan with little advance marketing or publicity ahead of its July release date. (As of early this month, the film has made over $63.5 million worldwide). The only clues potential audiences had about the film was a poster featuring the film’s very odd titular bird and the fact that its original title, “How Do You Live?,” was taken from a Genzaburō Yoshino book about a 15-year-old boy who has to learn about survival in a world consumed by death, war and sadness.</p><p>Echoing some of the themes and plotlines from Miyazaki’s oeuvre, the movie centers on a young Mahito, who encounters magical creatures and talking animals in a surreal world — like a cross between Lewis Carroll’s “Alice in Wonderland” and Ingmar Bergman’s “Fanny and Alexander.” The boy, who has lost his mother in a hospital fire in Tokyo, has a tough time adjusting to life with his father’s new wife — his mother’s younger sister — and their new home in the countryside. </p><p>Then, there’s the strange gray heron who badgers him and claims that his mother is still alive, and the mysterious tower built by Mahito’s long-vanished granduncle. This is all before our young hero enters the alternate world of the Tower, where he encounters menacing man-eating parakeets, younger versions of some of the characters from his real life, and cute bubble-like souls called the Warawara, who have to float to the surface to be born in Mahito’s world.</p><div data-click="enhancement" data-align-center=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/a05e864/2147483647/strip/true/crop/2024x1162+0+0/resize/320x184!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 320w,https://ca-times.brightspotcdn.com/dims4/default/853e000/2147483647/strip/true/crop/2024x1162+0+0/resize/568x326!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 568w,https://ca-times.brightspotcdn.com/dims4/default/88040b3/2147483647/strip/true/crop/2024x1162+0+0/resize/768x441!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 768w,https://ca-times.brightspotcdn.com/dims4/default/3f079e6/2147483647/strip/true/crop/2024x1162+0+0/resize/1024x588!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1024w,https://ca-times.brightspotcdn.com/dims4/default/edc3840/2147483647/strip/true/crop/2024x1162+0+0/resize/1200x689!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1200w" sizes="100vw">     <img alt="A girl chases two small creatures through a tunnel of trees in the animated film &quot;My Neighbor Totoro.&quot;" srcset="https://ca-times.brightspotcdn.com/dims4/default/b44e202/2147483647/strip/true/crop/2024x1162+0+0/resize/320x184!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 320w,https://ca-times.brightspotcdn.com/dims4/default/f1195c4/2147483647/strip/true/crop/2024x1162+0+0/resize/568x326!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 568w,https://ca-times.brightspotcdn.com/dims4/default/8cb3eea/2147483647/strip/true/crop/2024x1162+0+0/resize/768x441!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 768w,https://ca-times.brightspotcdn.com/dims4/default/0b3e89e/2147483647/strip/true/crop/2024x1162+0+0/resize/1024x588!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1024w,https://ca-times.brightspotcdn.com/dims4/default/62c89d0/2147483647/strip/true/crop/2024x1162+0+0/resize/1200x689!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1200w" sizes="100vw" width="1200" height="689" src="https://ca-times.brightspotcdn.com/dims4/default/62c89d0/2147483647/strip/true/crop/2024x1162+0+0/resize/1200x689!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg" decoding="async" loading="lazy">  </picture>  <div>   <p>Mei follows two small spirits in “My Neighbor Totoro.”</p>   <p>(© 1988 Studio Ghibli / GKids)</p>   </div>  </figure></div><p>As Del Toro noted in a recent interview, “Miyazaki proves again and again that his films are not about leaving you chirpy: It’s about showing you the sweetness and sourness of life— the loss, the love and the beauty all at the same time. Hitchcock said repetition with consciousness is style: Miyazaki is saying, ‘I didn’t get it quite right in that movie. Let me get it right on this one.’ Rhythmically, he is very contemplative. This is an old master who has dispensed with tools and flourishes. They stop trying to impress. They offer a simple and beautiful gesture with a brush.”</p><p>Indie distributor GKIDS, which handles Studio Ghibli titles in the U.S., will release the movie on Dec. 8, following special preview engagements in New York and Los Angeles next week. The English-language voice cast includes Florence Pugh, Christian Bale, Dave Bautista, Gemma Chan, Mark Hamill, Karen Fukuhara, Willem Dafoe and Luca Padovan.</p><p>“I first saw ‘The Boy and the Heron’ at Studio Ghibli earlier this year, and after five minutes I began to cry,” recalls Eric Beckman, founder of GKIDS and L.A.’s Animation Is Film Festival. “There was another Miyazaki film in the world. What a gift. After ‘Totoro,’ ‘Spirited Away,’ ‘Princess Mononoke’ and ‘Howl’s Moving Castle,’ it’s easy to take for granted how unique Miyazaki’s gifts are. The beauty, the lusciousness, the humor and playfulness, the nuance are all here ... the movement of the heron in flight, of fish in the water, the tottering old aunties. Nothing can compare.”</p><p>Beckman agrees with many critics who have pointed out that “The Boy and the Heron” is Miyazaki’s most personal film by far. “In this movie, we have a great creator struggling to express something profound and important about life itself — both what to say and how to say it,” he says. “If ‘Totoro’ embodied a basic optimism — a celebration of the power and miraculous wonder of nature, a trust in nature — after 35 years, ‘Heron’ is not so optimistic about the state of the world anymore. This is not a kids’ cartoon or an episode in a superhero franchise. It is a work of cinematic art made by one of our greatest living filmmakers. Miyazaki is saying that the world is a fragile place, it may only last another day. What is the next step forward? That is up to the viewer to decide.”</p><p>In 2021, Miyazaki himself told the New York Times that he has a simple goal in mind whenever he sets out to make a new movie. “The mission of my films is to comfort you — to fill in the gap that might be in your heart or your everyday life.” When he was asked about the answer to the Japanese title of his movie “How Do You Live?” Miyazaki <i>san</i> responded, “I am making this movie because I do not have the answer.”</p><p>He might not have provided us with an easy answer, but he has once again managed to fill our hearts with the same sense of wonder we felt the first time we entered one of his remarkable realms.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YJIT is the most memory-efficient Ruby JIT (111 pts)]]></title>
            <link>https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/</link>
            <guid>38265773</guid>
            <pubDate>Tue, 14 Nov 2023 16:46:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/">https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/</a>, See on <a href="https://news.ycombinator.com/item?id=38265773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>This year, the YJIT team and I have gotten a paper accepted at
<a href="https://2023.splashcon.org/home/mplr-2023">MPLR 2023</a> (Managed Programming Languages and Runtimes),
which is now freely available through <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">ACM open access</a>.
The paper, titled <em>“Evaluating YJIT’s Performance in a Production Context: A Pragmatic Approach”</em>, goes
into details of the strategy taken to evaluate YJIT’s performance in a production context.
One of our key findings, when comparing YJIT against other existing Ruby JITs such as JRuby
and TruffleRuby, is that YJIT is the most memory-efficient Ruby JIT (by a long shot).</p>

<p>A video recording of our presentation at MPLR is also
<a href="https://www.youtube.com/watch?v=fMGuQXNqlaE&amp;t=9900s">available on YouTube</a>.</p>

<h2 id="background">Background</h2>

<p>Many published papers about JIT compilers only look at peak performance in terms of
running time on benchmarks after a certain amount of warm-up time.
This can be deceptive because the amount of time needed for a JIT compiler to warm up can be
arbitrarily long. Typically, the JIT compiler implementation is given as many benchmark
iterations as it needs to reach peak performance, and the peak performance as measured then
is reported. The amount of time needed to reach peak performance is often not discussed.
The same goes for memory usage.</p>

<p>I believe that
those metrics are typically ignored by academic compiler researchers
because they may reveal an inconvenient reality. If you give your JIT compiler
an arbitrary amount of time and memory to reach peak performance, it’s easier to throw all
possible known optimizations at a piece of code and reach high peak performance numbers.
However, if your JIT compiler uses an arbitrarily high amount of memory and needs a very long
time to warm up, even though it may have the fastest peak performance, it may be
unusable in most real-world production environments.</p>

<p>When deploying code into production, peak performance is not the only thing that matters.
On our production servers at Shopify, there is not a huge amount of memory available for
the JIT compiler to use. Almost all of the memory is used to run multiple server processes, and
also to cache various resources in RAM. This has forced us to spend a significant amount of
effort on optimizing YJIT’s memory usage to make the compiler more resource-efficient.</p>

<p>At Shopify, we deploy frequently, with consecutive deployments sometimes less than 20 minutes
apart. This adds an extra layer of challenge because, despite these frequent deployments,
we can’t tolerate significant increases in response time.
If a JIT compiler needs a significant amount of time to
warm up, or suddenly deoptimizes large amounts of code, this can translate into requests timing out, and customers
abandoning their shopping carts, which ultimately would result in lost revenue. As such, smooth,
predictable warm-up and stable performance are of critical importance.</p>

<h2 id="methodology">Methodology</h2>

<p>In our paper, we look at YJIT’s warm-up, memory usage and peak performance on benchmarks,
as well as on our deployment on Shopify’s StoreFront Renderer (SFR). For context, SFR
renders all Shopify storefronts, which is the first thing buyers see when they navigate to a store hosted by Shopify.
It is mostly written in Ruby, depends on over 220 Ruby gems, renders over 4.5
million Shopify stores in over 175 countries, and is served by multiple clusters distributed worldwide. It is capable of serving over 1.27
million requests per second, and has processed over 197 billion USD in transaction
volume in 2022.
YJIT is currently deployed to all SFR servers. For this paper, we’ve performed
experiments using Ruby head on a subset of servers in all clusters. We’ve also
included some control servers which ran the same Ruby commit without YJIT. Data
for the SFR experiments was gathered over a 48-hour period.</p>

<p>For our experiments on benchmarks, we used 11 headline benchmarks from the
yjit-bench suite. These are all benchmarks that are based on real-world Ruby gems,
with a bias towards web workloads. This includes benchmarks such as <code>railsbench</code>,
<code>hexapdf</code>, <code>activerecord</code> and <code>liquid-render</code>, which is a benchmark using Shopify’s liquid
template language gem.
We benchmarked YJIT, RJIT (Takashi Kokubun’s experimental Ruby JIT written
in Ruby), JRuby, as well as both the JVM and native versions of TruffleRuby.
More details on our experimental setup
are provided in <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">the paper</a>.</p>

<p>We also maintain a website, <a href="https://speed.yjit.org/">speed.yjit.org</a>, which tracks
YJIT’s performance and memory overhead as well as various other statistics on this
benchmark suite over time. Recently, as we were looking for more challenging and realistic
benchmarks, we’ve also turned the codebase of the
<a href="https://lobste.rs/">lobste.rs</a> website
<a href="https://railsatscale.com/2023-08-25-we-turned-lobsters-into-a-rails-benchmark-for-yjit/">into a benchmark</a> as well.</p>

<h2 id="key-findings">Key Findings</h2>

<h3 id="performance-on-benchmarks">Performance on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_speedup.png" alt="" width="90%"></p>

<p>The above graph shows the average execution time on each benchmark for each of the Ruby JIT
implementations we benchmarked. The time is normalized to the time taken by the CRuby interpreter,
where the time taken by the interpreter has value 1.0, with values below 1 being faster than
the interpreter.
We were very generous in terms of warm-up time. Each benchmark was run for 1000 iterations, and
the first half of all the iterations were discarded as warm-up time, giving each JIT a more
than fair chance to reach peak performance.</p>

<p>As can be seen, TruffleRuby has the best peak performance on most (but not all) benchmarks.
YJIT outperforms the CRuby interpreter on every benchmark by a wide margin. We can also see
that YJIT performs very competitively compared to JRuby (a JVM-based Ruby JIT), outperforming
it on most benchmarks.</p>

<h3 id="warm-up-time-on-benchmarks">Warm-Up Time on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/railsbench_warmup.png" alt="" width="90%"></p>

<p>This graph shows a plot of the performance over time for each Ruby JIT for the <code>railsbench</code> benchmark.
The x-axis is the total execution time in seconds, while the y-axis is the time per benchmark iteration. This
allows us to visualize how the performance of each VM evolves over time. As can be seen, YJIT
almost immediately outperforms the CRuby interpreter, with RJIT not too far behind. JRuby takes over a
minute to reach peak performance, but does not reliably outperform CRuby on this benchmark.</p>

<p>TruffleRuby eventually outperforms the other JITs, but it takes about two minutes to do so. It is
also initially quite a bit slower than the CRuby interpreter, taking over 110
seconds to catch up to the interpreter’s speed. This would be
problematic in a production context such as Shopify’s, because it can lead to much slower
response times for some customers, which could translate into lost business. Such wide variations
in performance can also make the scaling of server resources more difficult. We should also note
that while <code>railsbench</code> is a somewhat challenging benchmark, it is much smaller than our actual
production deployment. Warm-up data for other benchmarks is provided in the paper.</p>

<h3 id="memory-usage-on-benchmarks">Memory Usage on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_memory.png" alt="" width="90%"></p>

<p>The above graph is in my opinion the most interesting graph of the paper. It is a plot of the memory
usage (RSS) of each Ruby implementation for each benchmark.
Because of the widely varying scale between data points,
the use of a logarithmic was considered <img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/trollface.png" alt="" height="15">.
However, we have decided to use a
linear scale to maintain a more accurate sense of proportions. Do note, however, that there is
a cut in the graph between 5GiB and 17GiB.</p>

<p>As can be seen in the graph above, thanks in large part to the work put in by our team to
optimize its memory
overhead, YJIT has the lowest memory overhead of any Ruby JIT by far. The JVM-based
Ruby implementations often use one or even two orders of magnitude more memory than YJIT and the
CRuby interpreter. The memory overhead compared to CRuby can be as much as several gigabytes.
This is on benchmarks that often require less than 100MiB to run with the CRuby interpreter,
which makes such a high amount of memory overhead seem disproportionate.</p>

<p>One significant caveat here is that we are measuring the total memory usage of each system. That
is, the memory overhead of the JVM itself has an impact. The way that JRuby and TruffleRuby
internally represent Ruby objects in memory, which is different from the way CRuby represents
objects in memory, also has an impact.
However, the bottom line is the same. Using several gigabytes more memory than CRuby to run simple
benchmarks likely bodes poorly for many production deployments.
For smaller production deployments, for example, a project running on inexpensive Virtual Private Servers (VPS),
there may be only 1GiB or 2GiB of RAM available in total.
For a company like Shopify running a large fleet of servers,
the amount of server processes that can be run on a single machine, and how much memory can be used
for caching matters.</p>

<p>There is another caveat, which is that JRuby and TruffleRuby, unlike CRuby, do not use a GVL
(Global VM Lock, analogous to Python’s GIL). In theory, this means that they can more effectively
use multithreading, and amortize some of their memory overhead across multiple server threads.
Still, there is a case to be made that the memory overhead of JRuby and TruffleRuby is something
that is often overlooked and probably should be better optimized.
Aside from production deployments, the <code>ruby-lsp</code> benchmark is a benchmark of the Ruby language
server, which can be used by code editors such as VSCode.
We can see that on this benchmark, the JVM-based Ruby implementations can use multiple gigabytes
of memory, and despite that, perform worse than the CRuby interpreter. This is far from ideal
for a Ruby gem that is meant to be run on developer laptops.</p>

<p>I would also like to note that RJIT, Takashi Kokubun’s experimental pure Ruby JIT, looks
quite good in this comparison. However, in the previous graph, the inclusion of the
JVM-based Ruby JITs distorts the sense of scale. The next graph below shows the same memory
usage comparison, but with only CRuby, YJIT and RJIT included. Currently, there are
situations where RJIT uses several times more memory than CRuby and YJIT:</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_mem_yjit_rjit_only.png" alt="" width="90%"></p>

<p>YJIT, being that it is written in Rust (a systems language), has
access to more tools to optimize memory usage in places where individual bits count. Matching
YJIT’s memory usage in a pure Ruby JIT would be difficult and likely would necessitate
augmenting Ruby with special systems programming primitives. For example, to be able to
efficiently pack structs and bit fields in memory and to also pack structs and
arrays inside of other structs.
Encoding data structures in memory as compactly as possible is likely challenging to do
in a JVM-based JIT implementation as well.</p>

<h3 id="performance-in-production">Performance In Production</h3>

<p>The following graph looks at the latency of YJIT on our SFR deployment when compared to
servers that run the same Ruby commit with YJIT disabled. If you are wondering why no other
Ruby JITs are included in this graph, it is because, at this time, other Ruby JITs could not
be deployed in production for this application, either due to compatibility issues or due to memory constraints.
On average, YJIT is 14.1% faster than the CRuby interpreter during the period examined.
Importantly, because YJIT is able to compile new code very fast, it is also faster than the
interpreter even on the slowest p99 requests.</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/sfr_latency.png" alt="" width="90%"></p>

<p>If a 14.1% speedup seems underwhelming to you, do keep in mind that the latency numbers
provided measure the total time needed to generate a response. This includes not only
time spent in JIT-compiled code, but also time spent in C functions that we cannot optimize,
and time the server spends waiting for database requests and other I/O operations.</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/sfr_speedup.png" alt="" width="90%"></p>

<p>The graph above shows the speedup provided by YJIT over the interpreter. The purple vertical
lines represent deployments.
During the time period we examined, there were 35 deployments of new code to production,
and the shortest interval during consecutive deployments was just 19 minutes 21 seconds.
The key takeaway here is that even though we perform frequent
deployments during the daytime, because YJIT is able to warm up very fast, it remains
consistently faster than the interpreter.
Again, more information is provided in <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">the paper</a>,
including data about YJIT’s memory usage in our production deployments.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We’ve recently published a paper about YJIT at MPLR 2023, in which we evaluate YJIT’s performance
on both benchmarks as well as on Shopify’s flagship production deployment, which serves an
enormous amount of traffic worldwide.
In this paper, we make it a point to examine not just peak performance, but to also discuss and
evaluate warm-up time and total memory usage.</p>

<p>The YJIT team has spent a significant amount of effort optimizing YJIT so that it doesn’t just
show good peak performance numbers, but also does this while being memory-efficient.
This effort has paid off, with YJIT having the least memory overhead of any Ruby JIT, which
has been crucial in enabling YJIT to handle Shopify’s SFR deployment.</p>

<p>Since our MPLR paper was published, we’ve kept improving
YJIT’s performance.
As of this writing, I am looking at our internal dashboard, and YJIT is providing a 27.2% average
speedup over the interpreter on our SFR deployment.
With the Ruby 3.3 release approaching, there will be a lot to be excited about this holiday season,
as we are once
again gearing up for a very strong Ruby release. This year, YJIT 3.3 will deliver better performance,
while using less memory, and also warming up faster than YJIT 3.2.
Expect another post on the <a href="https://railsatscale.com/">Rails at Scale blog</a> with more benchmark results soon!</p>

<p>For more information on how to use YJIT, see the <a href="https://github.com/ruby/ruby/blob/master/doc/yjit/yjit.md">YJIT README</a>.
Should you wish to cite <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">our MPLR 2023 paper</a>, I’ve also
included the bibtex snippet below:</p>

<div><pre><code>@inproceedings{yjit_mplr_2023,
author = {Chevalier-Boisvert, Maxime and Kokubun, Takashi and Gibbs, Noah and Wu, Si Xing (Alan) and Patterson, Aaron and Issroff, Jemma},
title = {Evaluating YJIT’s Performance in a Production Context: A Pragmatic Approach},
year = {2023},
isbn = {9798400703805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617651.3622982},
doi = {10.1145/3617651.3622982},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
pages = {20–33},
numpages = {14},
keywords = {dynamically typed, optimization, just-in-time, virtual machine, ruby, compiler, bytecode},
location = {Cascais, Portugal},
series = {MPLR 2023}
}
</code></pre></div>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[.NET 8 (270 pts)]]></title>
            <link>https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/</link>
            <guid>38264937</guid>
            <pubDate>Tue, 14 Nov 2023 16:00:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/">https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/</a>, See on <a href="https://news.ycombinator.com/item?id=38264937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="featured">
                         <p>
            November 14th, 2023</p><!-- .entry-meta -->
        
<p>We are happy to announce the availability of <a href="https://aka.ms/get-dotnet-8">.NET 8</a>, the latest <a href="https://dotnet.microsoft.com/platform/support/policy/dotnet-core#release-types">LTS</a> version of one of the world’s leading development platforms, starting today. .NET 8 delivers thousands of performance, stability, and security improvements, as well as platform and tooling enhancements that help increase developer productivity and speed of innovation. The .NET team, our partners, and the .NET community will be talking about what’s new in .NET 8 as well as what people are building with .NET today to meet their needs of tomorrow at  <a href="https://www.dotnetconf.net/">.NET Conf 2023, a three day virtual event (November 14-16)</a>. Come, join us!</p>
<p><a href="https://dotnet.microsoft.com/download/dotnet/8.0"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Banner3.png" alt=".NET 8 is now available" width="600"></a></p>
<p>With this release, .NET reshapes the way we build intelligent, cloud-native applications and high-traffic services that scale on demand. Whether you’re deploying to Linux or Windows, using containers or a cloud app model of your choice, .NET 8 makes building these apps easier. It includes a set of proven libraries that are used today by the many high-scale services at Microsoft to help you with fundamental challenges around observability, resiliency, scalability, manageability, and more.</p>
<p><a href="https://aka.ms/aspireannouncement"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Cloud-Native-dotnet8.png" alt=".NET 8 - Cloud Native Features"></a></p>
<p>Integrate large language models (LLMs) like OpenAI’s GPT directly into your .NET app. Use a single powerful component model to handle all your web UI needs with Blazor. Deploy your mobile applications to the latest version of iOS and Android with .NET MAUI. Discover new language enhancements that make your code more concise and expressive with C# 12.  </p>
<p>Let’s look at what’s new in .NET 8. </p>
<h2 id="unparalleled-performance-experience-the-fastest-net-to-date">Unparalleled Performance – Experience the fastest .NET to date</h2>
<p>.NET 8 comes with thousands of performance <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/">improvements</a> <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-aspnet-core-8/">across</a> <a href="https://devblogs.microsoft.com/dotnet/dotnet-8-performance-improvements-in-dotnet-maui/">the</a> <a href="https://devblogs.microsoft.com/dotnet/this-arm64-performance-in-dotnet-8/">stack</a>. A new code generator called Dynamic Profile-Guided Optimization (PGO) that optimizes your code based on real-world usage is enabled by default and can improve the performance of your apps up to 20%. The AVX-512 instruction set, which is now supported, enables you to perform parallel operations on 512-bit vectors of data, meaning you can process much more data in less time. The primitive types (numerical and beyond) now implement a new formattable and parsable interface, which enable them to directly format and parse as UTF-8 without any transcoding overhead.</p>
<p>Every year we talk about the performance gains across .NET. This year we continue our quest to push the performance of .NET to new heights. From the latest TechEmpower benchmarks with .NET 8, we’re seeing improvements in the JSON API scenario of 18%, hitting nearly one million requests per second with ASP.NET Core Minimal APIs.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Performance2.png" alt="TechEmpower requests per second (RPS)"></a></p>
<p>The Fortunes scenario is closer to a real-world workload, including database access and server-side HTML rendering. In this test, we see an even larger improvement of 24%, now over 300K requests per second with ASP.NET Core.</p>
<h2 id="net-aspire-an-opinionated-stack-to-build-observable-production-ready-cloud-native-applications">.NET Aspire – An opinionated stack to build observable, production-ready cloud-native applications</h2>
<p>.NET Aspire is a stack for building resilient, observable, and configurable cloud-native applications with .NET. It includes a curated set of components enhanced for cloud-native by including telemetry, resilience, configuration, and health checks by default. Combined with a sophisticated but simple local developer experience, .NET Aspire makes it easy to discover, acquire, and configure essential dependencies for cloud-native applications on day 1 as well as day 100. The <a href="https://aka.ms/aspireannouncement">first preview</a> of .NET Aspire is available today.</p>
<p><a href="https://aka.ms/aspireannouncement"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Aspire-CTAs.png" alt=".NET Aspire"></a></p>
<h2 id="net-8-container-enhancements-more-secure-compact-and-productive">.NET 8 Container Enhancements – More secure, compact, and productive</h2>
<p>Package your applications with <a href="https://devblogs.microsoft.com/dotnet/securing-containers-with-rootless/">containers more easily and more securely than ever with .NET</a>. Every .NET image includes a non-root user, enabling more secure containers with one-line configuration. The .NET SDK tooling publishes container images without a Dockerfile and are non-root by default. Deploy your containerized apps faster due to smaller .NET base images – including new experimental variants of our images that deliver truly minimal application sizes for native AOT. Opt-in to even more security hardening with the new Chiseled Ubuntu image variants to reduce your attack surface even further. Using Dockerfiles or SDK tooling, build apps and container images for any architecture.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/securing-containers-with-rootless/"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Containers2.png" alt="Modern containers"></a></p>
<h2 id="native-aot-journey-towards-higher-density-sustainable-compute">Native AoT – Journey towards higher density sustainable compute</h2>
<p>Compile your .NET apps into <a href="https://learn.microsoft.com/dotnet/core/deploying/native-aot">native code</a> that uses less memory and starts instantly. No need to wait for the JIT (just-in-time) compiler to compile the code at run time. No need to deploy the JIT compiler and IL code. AOT apps deploy just the code that’s needed for your app. Your app is now empowered to run in restricted environments where a JIT compiler isn’t allowed.</p>
<p><a href="https://learn.microsoft.com/dotnet/core/deploying/native-aot"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/AOTOptimizations3.png" alt="Before and After AOT"></a></p>
<h2 id="artificial-intelligence-infuse-ai-into-your-net-applications">Artificial Intelligence – Infuse AI into your .NET applications</h2>
<p>Generative AI and large language models are transforming the field of AI, providing developers the ability to create unique AI-powered experiences in their applications. <a href="https://aka.ms/dotnet-genai">.NET 8 makes it simple for you to leverage AI</a> via first-class out-of-the box AI features in the .NET SDK and seamless integration with several tools. </p>
<p>.NET 8 brings several enhancements to the <code>System.Numerics</code> library to improve its compatibility with Generative AI workloads, such as integrating Tensor Primitives. With the rise of AI-enabled apps, new tools and SDKs emerged. We collaborated with numerous internal and external partners, such as <a href="https://azure.microsoft.com/products/ai-services/openai-service">Azure OpenAI</a>, <a href="https://azure.microsoft.com/free/ai-services/?ef_id=_k_b34c5d449bf4175800e738086ecc7267_k_&amp;OCID=AIDcmm5edswduu_SEM__k_b34c5d449bf4175800e738086ecc7267_k_&amp;msclkid=b34c5d449bf4175800e738086ecc7267">Azure Cognitive Search</a>, <a href="https://milvus.io/docs/v2.2.x/install-csharp.md">Milvus</a>, <a href="https://github.com/qdrant/qdrant-dotnet">Qdrant</a>, and <a href="https://github.com/microsoft/teams-ai">Microsoft Teams</a>, to ensure .NET developers have easy access to various AI models, services, and platforms through their respective SDKs. Additionally, the open-source <a href="https://learn.microsoft.com/semantic-kernel/overview/">Semantic Kernel</a> SDK simplifies the integration of these AI components into new and existing applications, to help you deliver innovative user experiences.</p>
<p>Various samples and reference templates, showcasing patterns and practices, are now available to make it easy for developers to get started:</p>
<ul>
<li><a href="https://github.com/dotnet-architecture/eShop">Customer Chatbot</a></li>
<li><a href="https://github.com/Azure-Samples/azure-search-openai-demo-csharp">Retrieval Augmented Generation</a></li>
<li><a href="https://devblogs.microsoft.com/dotnet/demystifying-retrieval-augmented-generation-with-dotnet/">Developing Apps using Azure AI services</a> </li>
</ul>
<p><a href="https://github.com/Azure-Samples/azure-search-openai-demo-csharp/assets/2546640/b79090b8-6a8b-45f4-b42b-e21e22b1661a"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/InfuseAIdotnetapps1.png" alt="Infuse AI dotnet apps"></a></p>
<h2 id="blazor-build-full-stack-web-applications-with-net">Blazor – Build full stack web applications with .NET</h2>
<p>Blazor in .NET 8 can use both the server and client together to handle all your web UI needs. It’s full stack web UI! With several new enhancements focused towards optimizing page load time, scalability, and elevating the user experience, developers can now use <a href="https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor">Blazor Server and Blazor WebAssembly</a> in the same app, automatically shifting users from the server to the client at run time. Your .NET code runs significantly faster on WebAssembly thanks to the new “Jiterpreter”-based runtime and new built-in components. As a part enhancing the overall <a href="https://devblogs.microsoft.com/dotnet/whats-new-with-identity-in-dotnet-8/">authentication, authorization, and identity management in .NET 8</a>, Blazor now supports generating a full Blazor-based Identity UI.</p>
<p><a href="https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Blazor2.png" alt="Full stack Blazor"></a></p>
<h2 id="net-maui-elevated-performance-reliability-and-developer-experience"><a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-maui-in-dotnet-8">.NET MAUI – Elevated performance, reliability, and developer experience</a></h2>
<p>.NET MAUI provides you with a single project system and single codebase to build WinUI, Mac Catalyst, iOS, and Android applications. Native AOT (experimental) now supports targeting iOS-like platforms. <a href="https://aka.ms/maui-devkit-blog">A new Visual Studio Code extension for .NET MAUI</a> gives you the tools you need to develop cross-platform .NET mobile and desktop apps. Xcode 15 and Android API 34 are now supported allowing you to target the latest version of iOS and Android. A plethora of quality improvements were made to the <a href="https://devblogs.microsoft.com/dotnet/dotnet-8-performance-improvements-in-dotnet-maui">areas of performance</a>, controls and UI elements, and platform-specific behavior, such as desktop interaction adding better click handling, keyboard listeners, and more.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-maui-in-dotnet-8"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/dotnetMAUIin8-1.png" alt=".NET MAUI Visual Studio Code Extension"></a></p>
<h2 id="c-12-features-simplified-syntax-for-better-developer-productivity">C# 12 Features – Simplified syntax for better developer productivity</h2>
<p>C# 12 makes your coding experience more productive and enjoyable. You can now create primary constructors in any class and struct with a simple and elegant syntax. No more boilerplate code to initialize your fields and properties. Be delighted when creating arrays, spans, and other collection types with a concise and expressive syntax. Use new default values for parameters in lambda expressions. No more overloading or null checks to handle optional arguments. You can even use the <code>using</code> alias directive to alias any type, not just named types!</p>
<p><strong>Collection expressions</strong></p>
<pre><code>// Create a list:
List&lt;int&gt; a = [1, 2, 3, 4, 5, 6, 7, 8];

// Create a span
Span&lt;char&gt; b  = ['a', 'b', 'c', 'd', 'e', 'f', 'h', 'i'];

// Use the spread operator to concatenate
int[] array1 = [1, 2, 3];
int[] array2 = [4, 5, 6];
int[] array3 = [7, 8, 9];
int[] fullArray = [..array1, ..array2, ..array3]; // contents is [1, 2, 3, 4, 5, 6, 7, 8, 9]</code></pre>
<p>See more about the latest version of C# in <a href="https://devblogs.microsoft.com/dotnet/announcing-csharp-12">Announcing C# 12</a>.</p>

<p>We have a set of great tools that help you be the most productive in your development workflow and take advantage of .NET 8 today. Released alongside .NET 8, the <a href="https://aka.ms/vs/v178GA">Visual Studio 2022 17.8 release</a> brings support for .NET 8, C# 12 language enhancements, and various new productivity features. <a href="https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit">VS Code and C# Dev Kit</a> is a great way to get started with .NET 8 if you’re learning and/or want to quickly kick the tires of the runtime and is available on Linux, macOS, or in GitHub Codespaces. The new <a href="https://github.com/codespaces">GitHub Codespaces template for .NET</a>, which comes with the .NET SDK and a set of configured extensions, is one of the fastest ways to get started with .NET 8. </p>
<h3 id="additional-features-in-net-8">Additional features in .NET 8:</h3>
<ul>
<li><strong>ASP.NET Core.</strong> <a href="https://devblogs.microsoft.com/dotnet/whats-new-with-identity-in-dotnet-8/">Streamlines identity for single-page applications (SPA) and Blazor providing cookie-based authentication, pre-built APIs, token support, and a new identity UI.</a> and <a href="https://learn.microsoft.com/aspnet/core/release-notes/aspnetcore-8.0#minimal-apis">enhances minimal APIs with form-binding, antiforgery support to protect against cross-site request forgery (XSRF/CSRF), and <code>asParameters</code> support for parameter-binding with Open API definitions</a></li>
<li><strong>ASP.NET Core tooling.</strong> <a href="https://devblogs.microsoft.com/dotnet/aspnet-core-route-tooling-dotnet-8/">Route syntax highlighting, auto-completion, and analyzers to help you create Web APIs.</a></li>
<li><strong>Entity Framework Core.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-ef8-rc2/">Provides new “complex types” as value objects, primitive collections, and SQL Server support for hierarchical data.</a></li>
<li><strong>NuGet.</strong> <a href="https://learn.microsoft.com/nuget/concepts/auditing-packages">Helps you audit your NuGet packages in projects and solutions for any known security vulnerabilities.</a></li>
<li><strong>.NET Runtime.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8-rc1/#androidstripilafteraot-mode-on-android">Brings a new AOT compilation mode for WebAssembly (WASM) and Android.</a></li>
<li><strong>.NET SDK.</strong> <a href="https://learn.microsoft.com/dotnet/core/whats-new/dotnet-8#net-sdk">Revitalizes terminal build output and production-ready defaults.</a></li>
<li><strong>WPF.</strong> <a href="https://devblogs.microsoft.com/dotnet/wpf-file-dialog-improvements-in-dotnet-8/">Supports OpenFolderDialog</a> and <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8-rc1/#wpf-hardware-acceleration-in-rdp">Enabled HW Acceleration in RDP</a></li>
<li><strong>ARM64.</strong> <a href="https://devblogs.microsoft.com/dotnet/this-arm64-performance-in-dotnet-8/">Significant feature enhancements and improved code quality for ARM64 platforms through collaboration with ARM engineers.</a></li>
<li><strong>Debugging.</strong> <a href="https://devblogs.microsoft.com/dotnet/debugging-enhancements-in-dotnet-8/">Displays debug summaries and provides simplified debug proxies for commonly used .NET types.</a></li>
<li><strong>System.Text.Json.</strong> <a href="https://devblogs.microsoft.com/dotnet/system-text-json-in-dotnet-8/">Helps populate read-only members, customizes unmapped member handling, and improves Native AOT support.</a></li>
<li><strong>.NET Community Toolkit.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-the-dotnet-community-toolkit-821/">Accelerates building .NET libraries and applications while ensuring they are trim and AOT compatible (including the MVVM source generators!)</a></li>
<li><strong>Azure</strong> <a href="https://aka.ms/appservice-dotnet8">Supports .NET 8 with Azure’s PaaS services like App Service for Windows and Linux, Static Web Apps, Azure Functions, and Azure Container Apps.</a></li>
<li><strong>What’s new in .NET 8.</strong> <a href="https://learn.microsoft.com/dotnet/core/whats-new/dotnet-8">Check out our documentation for everything else!</a></li>
</ul>
<h3 id="get-started-with-net-8">Get started with .NET 8</h3>
<p>For the best development experience with .NET 8, we recommend that you use the latest release of <a href="https://visualstudio.microsoft.com/downloads/">Visual Studio</a> and <a href="https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit">Visual Studio Code’s C# Dev Kit</a>. Once you’re set up, here are some of the things you should do:</p>
<ul>
<li><strong>Try the new features and APIs.</strong> <a href="https://dotnet.microsoft.com/download/dotnet/8.0">Download .NET 8</a> and <a href="https://github.com/dotnet/core/issues/new/choose">report issues in our issue tracker</a>.</li>
<li><strong>Test your current app for compatibility.</strong> Learn whether your app is <a href="https://learn.microsoft.com/dotnet/core/compatibility/8.0">affected by default behavior changes in .NET 8</a>.</li>
<li><strong>Test your app with opt-in changes.</strong> .NET 8 has <a href="https://learn.microsoft.com/dotnet/core/compatibility/8.0">opt-in behavior changes</a> that only affect your app when enabled. It’s important to understand and assess these changes early as they may become default in the next release.</li>
<li><strong>Update your app with the Upgrade Assistant.</strong> <a href="https://dotnet.microsoft.com/platform/upgrade-assistant">Upgrade your app with just a few clicks using the Upgrade Assistant</a>.</li>
<li><strong>Know you’re supported.</strong> .NET 8 is officially supported by Microsoft as a <a href="https://dotnet.microsoft.com/platform/support/policy/dotnet-core#release-types">long term support (LTS) release that will be supported for three years</a>.</li>
<li><strong>Bonus: eShop Sample for .NET 8.</strong> Follow all the best coding and architecture practices with our <a href="https://github.com/dotnet/eshop">new eShop sample, now updated for .NET 8</a>!</li>
</ul>
<h3 id="celebrate-net-8">Celebrate .NET 8</h3>
<ul>
<li><strong>.NET Conf 2023</strong>. <a href="https://www.dotnetconf.net/">Join us November 14-16, 2023 to celebrate the .NET 8 release!</a></li>
<li><strong>What’s next in .NET?</strong> <a href="https://dotnet.microsoft.com/next">Get involved and learn the latest news on .NET 8 and the next version of .NET.</a></li>
<li><strong>Get C# Certified</strong>. <a href="https://devblogs.microsoft.com/dotnet/announcing-foundational-csharp-certification/">Earn a badge of honor with a freeCodeCamp C# certification.</a></li>
<li><strong>Learn .NET 8</strong>. <a href="https://aka.ms/learn-dotnet-8">Free tutorials, videos, courses, and more for beginner through advanced .NET developers. All updated for .NET 8!</a></li>
<li><strong>See Developer Stories</strong>. <a href="https://devblogs.microsoft.com/dotnet/category/developer-stories/">Take a look at success stories of developers migrating to modern .NET.</a></li>
<li><strong>Read about why .NET?</strong>. <a href="https://devblogs.microsoft.com/dotnet/why-dotnet/">Read through our recent blog series about the convenience of .NET.</a></li>
</ul>

<p>We would just like to end by saying one big…</p>
<p><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/ThankYou.png" alt="https://dotnet.microsoft.com/thanks/8.0"></p>

        

		
        <div>

            <p><img width="60" height="60" data-src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg" alt="" decoding="async" data-srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg 96w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-300x300.jpg 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-150x150.jpg 150w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-24x24.jpg 24w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-48x48.jpg 48w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth.jpg 648w" sizes="(max-width: 60px) 100vw, 60px" srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg 96w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-300x300.jpg 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-150x150.jpg 150w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-24x24.jpg 24w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-48x48.jpg 48w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth.jpg 648w"></p>
            <h3>
                <a data-bi-id="author follow within post" data-bi-area="" data-bi-name="Gaurav Seth" aria-label="Gaurav Seth" href="https://devblogs.microsoft.com/dotnet/author/gauravs">
                    Gaurav Seth                </a>
                <span>Partner Director of Product, Developer Platforms</span>
            </h3>
            
       </div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GraphCast: AI model for faster and more accurate global weather forecasting (363 pts)]]></title>
            <link>https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/</link>
            <guid>38264641</guid>
            <pubDate>Tue, 14 Nov 2023 15:42:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/</a>, See on <a href="https://news.ycombinator.com/item?id=38264641">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
  

  
  
  
  
    

    
    
      
        <div>
          
            
            
              
              

<div>
    <div>
      <p>Research</p>
      

      
        <dl>
          
            <dt>Published</dt>
            <dd>
              <time datetime="2023-11-14">
                14 November 2023
              </time>
            </dd>
          
          
            <dt>Authors</dt>
            
          
        </dl>
      

      
    </div>

    
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w2144-h1206-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1856-h1044-n-nu-rw 2x"><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1056-h594-n-nu-rw 2x">
      <img alt="GraphCast global weather forecasting of surface wind speed" height="603" src="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1072-h603-n-nu" width="1072">
    </picture>
    
  
  </div>
            
          
            
            
              
              <div>
  <p data-block-key="pnebs"><b>Our state-of-the-art model delivers 10-day weather predictions at unprecedented accuracy in under one minute</b></p><p data-block-key="f3n9e">The weather affects us all, in ways big and small. It can dictate how we dress in the morning, provide us with green energy and, in the worst cases, create storms that can devastate communities. In a world of increasingly extreme weather, fast and accurate forecasts have never been more important.</p><p data-block-key="baim">In a paper <a href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener" target="_blank">published in Science</a>, we introduce GraphCast, a state-of-the-art AI model able to make medium-range weather forecasts with unprecedented accuracy. GraphCast predicts weather conditions up to 10 days in advance more accurately and much faster than the industry gold-standard weather simulation system – the High Resolution Forecast (HRES), produced by the European Centre for Medium-Range Weather Forecasts (ECMWF).</p><p data-block-key="d53hu">GraphCast can also offer earlier warnings of extreme weather events. It can predict the tracks of cyclones with great accuracy further into the future, identifies atmospheric rivers associated with flood risk, and predicts the onset of extreme temperatures. This ability has the potential to save lives through greater preparedness.</p><p data-block-key="2cftd">GraphCast takes a significant step forward in AI for weather prediction, offering more accurate and efficient forecasts, and opening paths to support decision-making critical to the needs of our industries and societies. And, by <a href="https://github.com/google-deepmind/graphcast" rel="noopener" target="_blank">open sourcing the model code for GraphCast,</a> we are enabling scientists and forecasters around the world to benefit billions of people in their everyday lives. GraphCast is already being used by weather agencies, including ECMWF, which is running a live experiment of <a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850" rel="noopener" target="_blank">our model’s forecasts on its website</a>.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="ke1fd">A selection of GraphCast’s predictions rolling across 10 days showing specific humidity at 700 hectopascals (about 3 km above surface), surface temperature, and surface wind speed.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">The challenge of global weather forecasting</h2><p data-block-key="c69cv">Weather prediction is one of the oldest and most challenging–scientific endeavours. Medium range predictions are important to support key decision-making across sectors, from renewable energy to event logistics, but are difficult to do accurately and efficiently.</p><p data-block-key="27gma">Forecasts typically rely on Numerical Weather Prediction (NWP), which begins with carefully defined physics equations, which are then translated into computer algorithms run on supercomputers. While this traditional approach has been a triumph of science and engineering, designing the equations and algorithms is time-consuming and requires deep expertise, as well as costly compute resources to make accurate predictions.</p><p data-block-key="3ces3">Deep learning offers a different approach: using data instead of physical equations to create a weather forecast system. GraphCast is trained on decades of historical weather data to learn a model of the cause and effect relationships that govern how Earth’s weather evolves, from the present into the future.</p><p data-block-key="1lfp4">Crucially, GraphCast and traditional approaches go hand-in-hand: we trained GraphCast on four decades of weather reanalysis data, from the ECMWF’s ERA5 dataset. This trove is based on historical weather observations such as satellite images, radar, and weather stations using a traditional NWP to ‘fill in the blanks’ where the observations are incomplete, to reconstruct a rich record of global historical weather.</p><h2 data-block-key="f4hf9">GraphCast: An AI model for weather prediction</h2><p data-block-key="51oo3">GraphCast is a weather forecasting system based on machine learning and Graph Neural Networks (GNNs), which are a particularly useful architecture for processing spatially structured data.</p><p data-block-key="6c3sr">GraphCast makes forecasts at the high resolution of 0.25 degrees longitude/latitude (28km x 28km at the equator). That’s more than a million grid points covering the entire Earth’s surface. At each grid point the model predicts five Earth-surface variables – including temperature, wind speed and direction, and mean sea-level pressure – and six atmospheric variables at each of 37 levels of altitude, including specific humidity, wind speed and direction, and temperature.</p><p data-block-key="djs0f">While GraphCast’s training was computationally intensive, the resulting forecasting model is highly efficient. Making 10-day forecasts with GraphCast takes less than a minute on a single Google TPU v4 machine. For comparison, a 10-day forecast using a conventional approach, such as HRES, can take hours of computation in a supercomputer with hundreds of machines.</p><p data-block-key="7dee1">In a comprehensive performance evaluation against the gold-standard deterministic system, HRES, GraphCast provided more accurate predictions on more than 90% of 1380 test variables and forecast lead times (see our <a href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener" target="_blank">Science paper</a> for details). When we limited the evaluation to the troposphere, the 6-20 kilometer high region of the atmosphere nearest to Earth’s surface where accurate forecasting is most important, our model outperformed HRES on 99.7% of the test variables for future weather.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="x7trc">For inputs, GraphCast requires just two sets of data: the state of the weather 6 hours ago, and the current state of the weather. The model then predicts the weather 6 hours in the future. This process can then be rolled forward in 6-hour increments to provide state-of-the-art forecasts up to 10 days in advance.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">Better warnings for extreme weather events</h2><p data-block-key="7ki0b">Our analyses revealed that GraphCast can also identify severe weather events earlier than traditional forecasting models, despite not having been trained to look for them. This is a prime example of how GraphCast could help with preparedness to save lives and reduce the impact of storms and extreme weather on communities.</p><p data-block-key="csav9">By applying a simple cyclone tracker directly onto GraphCast forecasts, we could predict cyclone movement more accurately than the HRES model. In September, a live version of our publicly available GraphCast model, deployed on the ECMWF website, accurately predicted about nine days in advance that Hurricane Lee would make landfall in Nova Scotia. By contrast, traditional forecasts had greater variability in where and when landfall would occur, and only locked in on Nova Scotia about six days in advance.</p><p data-block-key="ecqbk">GraphCast can also characterize atmospheric rivers – narrow regions of the atmosphere that transfer most of the water vapour outside of the tropics. The intensity of an atmospheric river can indicate whether it will bring beneficial rain or a flood-inducing deluge. GraphCast forecasts can help characterize atmospheric rivers, which could help planning emergency responses together with <a href="https://sites.research.google/floodforecasting/" rel="noopener" target="_blank">AI models to forecast floods.</a></p><p data-block-key="a8m1r">Finally, predicting extreme temperatures is of growing importance in our warming world. GraphCast can characterize when the heat is set to rise above the historical top temperatures for any given location on Earth. This is particularly useful in anticipating heat waves, disruptive and dangerous events that are becoming increasingly common.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="df1sy">Severe-event prediction - how GraphCast and HRES compare.</p><p data-block-key="2dp0j">Left: Cyclone tracking performances. As the lead time for predicting cyclone movements grows, GraphCast maintains greater accuracy than HRES.</p><p data-block-key="3kpk6">Right: Atmospheric river prediction. GraphCast’s prediction errors are markedly lower than HRES’s for the entirety of their 10-day predictions</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">The future of AI for weather</h2><p data-block-key="795pr">GraphCast is now the most accurate 10-day global weather forecasting system in the world, and can predict extreme weather events further into the future than was previously possible. As the weather patterns evolve in a changing climate, GraphCast will evolve and improve as higher quality data becomes available.</p><p data-block-key="33jsp">To make AI-powered weather forecasting more accessible, we’ve <a href="https://github.com/google-deepmind/graphcast" rel="noopener" target="_blank">open sourced our model’s code</a>. ECMWF is already <a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850" rel="noopener" target="_blank">experimenting with GraphCast’s 10-day forecasts</a> and we’re excited to see the possibilities it unlocks for researchers – from tailoring the model for particular weather phenomena to optimizing it for different parts of the world.</p><p data-block-key="8e2gf">GraphCast joins other state-of-the-art weather prediction systems from Google DeepMind and Google Research, including a regional <a href="https://deepmind.google/discover/blog/nowcasting-the-next-hour-of-rain/" rel="noopener" target="_blank">Nowcasting model</a> that produces forecasts up to 90 minutes ahead, and <a href="https://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html" rel="noopener" target="_blank">MetNet-3</a>, a regional weather forecasting model already in operation across the US and Europe that produces more accurate 24-hour forecasts than any other system.</p><p data-block-key="au8nq">Pioneering the use of AI in weather forecasting will benefit billions of people in their everyday lives. But our wider research is not just about anticipating weather – it’s about understanding the broader patterns of our climate. By developing new tools and accelerating research, we hope AI can empower the global community to tackle our greatest environmental challenges.</p>
</div>
            
          
            
            
              
              


            
          
            
            
              
              



  
    
  

            
          
        </div>
      
    

    
  

  

  

  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Replacing HLS/Dash – Live Mass Fanout with Media over QUIC (115 pts)]]></title>
            <link>https://quic.video/blog/replacing-hls-dash/</link>
            <guid>38264174</guid>
            <pubDate>Tue, 14 Nov 2023 15:10:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quic.video/blog/replacing-hls-dash/">https://quic.video/blog/replacing-hls-dash/</a>, See on <a href="https://news.ycombinator.com/item?id=38264174">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<p>Low-latency, high bitrate, mass fan-out is hard. Who knew?</p>
<p>See <a href="https://quic.video/blog/replacing-webrtc">Replacing WebRTC</a> for the previous post in this series.</p>
<h2 id="tldr">tl;dr</h2>
<p>If you’re using HLS/DASH and your main priority is…</p>
<ul>
<li><strong>cost</strong>: wait until there CDN offerings.</li>
<li><strong>latency</strong>: you should seriously consider MoQ.</li>
<li><strong>features</strong>: it will take a while to implement everything.</li>
<li><strong>vod</strong>: it works great, why replace it?</li>
</ul>
<h2 id="intro">Intro</h2>
<p>Thanks for the positive reception on <a href="https://news.ycombinator.com/item?id=38069974">Hacker News</a>!
Anyway, I’m back.</p>
<p>I spent the last 9 years working on literally all facets of HLS and Twitch’s extension: <a href="https://www.theoplayer.com/blog/low-latency-hls-lhls">LHLS</a>.
We hit a latency wall and my task was to find an alternative, originally WebRTC but that eventually pivoted into <strong>Media over QUIC</strong>.</p>
<p>Hopefully this time I won’t be <em>“dunning-Krugerering off a cliff”</em>. Thanks random Reddit user for that confidence boost.</p>
<h2 id="why-hlsdash">Why HLS/DASH?</h2>
<p>Simple answer: <a href="https://developer.apple.com/library/archive/documentation/NetworkingInternet/Conceptual/StreamingMediaGuide/UsingHTTPLiveStreaming/UsingHTTPLiveStreaming.html">Apple</a></p>
<blockquote>
<p>If your app delivers video over cellular networks, and the video exceeds either 10 minutes duration or 5 MB of data in a five minute period, you are required to use HTTP Live Streaming.</p>
</blockquote>
<p>It’s an anti-climactic answer, but Twitch migrated from <a href="https://en.wikipedia.org/wiki/Real-Time_Messaging_Protocol">RTMP</a> to <a href="https://en.wikipedia.org/wiki/HTTP_Live_Streaming">HLS</a> to avoid getting kicked off the App Store.
The next sentence gives a hint as to why:</p>
<blockquote>
<p>If your app uses HTTP Live Streaming over cellular networks, you are required to provide at least one stream at 64 Kbps or lower bandwidth.</p>
</blockquote>
<p>This was back in 2009 when the iPhone 3GS was released and AT&amp;T’s network was <a href="https://www.wired.com/2010/07/ff-att-fail/">struggling to meet the demand</a>.
The key feature of HLS was <a href="https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming">ABR</a>: multiple copies of the same content at different bitrates.
This allowed the Apple-controlled HLS player to reduce the bitrate rather than pummel a poor megacorp’s cellular network.</p>
<p><a href="https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP">DASH</a> came afterwards in an attempt to standardize HLS minus the controlled by Apple part.
There’s definitely some cool features in DASH but the <a href="https://www.cloudflare.com/learning/video/what-is-mpeg-dash/">core concepts are the same</a> and they even share the same <a href="https://www.wowza.com/blog/what-is-cmaf">media container</a> now.
So the two get bundled together as <strong>HLS/DASH</strong>.</p>
<p>But I’ll focus more on HLS since that’s my shit.</p>
<h2 id="the-good-stuff">The Good Stuff</h2>
<p>While we were forced to switch protocols at the tech equivalent of gunpoint, HLS actually has some amazing benefits.
The biggest one is that it uses <strong>HTTP</strong>.</p>
<p>HLS/DASH works by breaking media into “segments”, each containing a few seconds of media.
The player will individually request each segment via a HTTP request and seamlessly stitch them together.
New segments are constantly being generated and announced to the player via a “playlist”.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/carrot.png" alt="carrot">
</p><figcaption>Thanks for the filer image, DALL·E</figcaption></figure>
<p>Because HLS uses HTTP, a service like Twitch can piggyback on the existing infrastructure of the internet.
There’s a plethora of optimized CDNs, servers, and clients that all speak HTTP and can be used to transport media.
You do have to do some extra work to massage live video into HTTP semantics, but it’s worth it.</p>
<p>The key is utilizing <a href="https://napkinfinance.com/napkin/what-are-economies-of-scale/">economies of scale</a> to make it cheap to mass distribute live media.
Crafting individual IP packets might the <em>correct</em> way to send live media with minimal latency (ie. WebRTC), but it’s not the most cost effective.</p>
<h2 id="the-bad-stuff">The Bad Stuff</h2>
<p>I hope you weren’t expecting a fluff piece.</p>
<h3 id="latency">Latency</h3>
<p>We were somewhat sad to bid farewell to Flash (<em>gasp</em>).
Twitch’s latency went from something like 3 seconds with RTMP to 15 seconds with HLS.</p>
<p>There’s a boatload of latency sources, anywhere from the duration of segments to the frequency of playlist updates.
Over the years we were able to slowly able to chip away at the problem, eventually extending HLS to get latency back down to theoretical RTMP levels.
I <a href="https://quic.video/blog/distribution-at-twitch">documented our journey</a> if you’re interested in the gritty details.</p>
<p>But one big source of latency remains: <strong>T</strong> <strong>C</strong> <strong>P</strong></p>
<p>I went into more detail with my <a href="https://quic.video/blog/replacing-webrtc">previous blog post</a>, but the problem is head-of-line blocking.
Once you flush a frame to the TCP socket, it will be delivered reliably and in order.
However, when the network is congested, the encoded media bitrate will exceed the network bitrate and queues will grow.
Frames will take longer and longer to reach the player until the buffer is depleted and the viewer gets to see their least favorite spinny boye.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/buffering.gif" alt="buffering">
</p><figcaption>&gt; tfw HLS/DASH</figcaption></figure>
<p>A HLS/DASH player can detect queuing and switch to a lower bitrate via ABR.
However, it can only do this at infrequent (ex. 2s) segment boundaries, and it can’t renege any frames already flushed to the socket.
So if you’re watching 1080p video and your network takes a dump, well you still need to download seconds of unsustainable 1080p video before you can switch down to a reasonable 360p.</p>
<p>You can’t just put the toothpaste back in the tube if you squeeze out too much.
You gotta use all of the toothpaste, even if it takes much longer to brush your teeth.</p>
<figure><p><img src="https://quic.video/blog/replacing-webrtc/toothpaste.jpg" alt="TCP toothpaste"></p><figcaption><p><a href="https://knowyourmeme.com/memes/shitting-toothpaste-pooping-toothpaste">Source</a>. The analogy falls apart but I
get to use this image again.</p></figcaption></figure>
<h3 id="clients">Clients</h3>
<p>HLS utilizes “smart” clients and “dumb” servers.
The client decides what, when, why, and how to download each media playlist, segment, and frame.
Meanwhile the server just sits there and serves HTTP requests.</p>
<p>The problem really depends on your perspective. If you control:</p>
<ul>
<li><strong>client only</strong>: Life is great!</li>
<li><strong>client and server</strong>: Life is great! You can even extend the protocol!</li>
<li><strong>server only</strong>: Life is <em>pain</em>.</li>
</ul>
<p>For a service like Twitch, the solution might seem simple: build your own client and server!
And we did, including a baremetal live CDN designed exclusively for HLS.</p>
<p>But <a href="https://bitmovin.com/managed-media-source">until quite recently</a>, we have been forced to use the Apple HLS player on iOS for AirPlay or Safari support.
And of course TVs, consoles, casting devices, and others have their own HLS players.
And if you’re offering your baremetal live CDN <a href="https://aws.amazon.com/ivs/">to the public</a>, you can’t exactly force customers to use your proprietary player.</p>
<p>So you’re stuck with a <em>dumb</em> server and a bunch of <em>dumb</em> clients.
These <em>dumb</em> clients make <em>dumb</em> decisions with no cooperation with the server, based on imperfect information.</p>
<h3 id="ownership">Ownership</h3>
<p>I love the simplicity of HLS compared to DASH.
There’s something so satisfying about a text-based playlist that you can actually read, versus a XML monstrosity designed by committee.</p>
<pre is:raw="" tabindex="0"><code><span><span>#EXTM3U</span></span>
<span><span>#EXT-X-TARGETDURATION:10</span></span>
<span><span>#EXT-X-VERSION:3</span></span>
<span><span>#EXTINF:9.009,</span></span>
<span><span>http://media.example.com/first.ts</span></span>
<span><span>#EXTINF:9.009,</span></span>
<span><span>http://media.example.com/second.ts</span></span>
<span><span>#EXTINF:3.003,</span></span>
<span><span>http://media.example.com/third.ts</span></span>
<span><span>#EXT-X-ENDLIST</span></span></code></pre>
<figure><figcaption><a href="https://datatracker.ietf.org/doc/html/draft-pantos-hls-rfc8216bis/#section-9.1">Orgasmic</a>.</figcaption></figure>
<p>But unfortunately Apple controls HLS.</p>
<p>There’s a misalignment of incentives between Apple and the rest of the industry.
I’m not even sure how Apple uses HLS, or why they would care about latency, or why they insist on being the sole arbiter of a live streaming protocol.
<a href="https://www.crunchbase.com/person/roger-pantos">Pantos</a> has done a great and thankless job, but it feels like a stand-off.</p>
<p>For example, LL-HLS originally <a href="https://www.theoplayer.com/blog/impact-of-apple-ll-hls-update-2020">required HTTP/2 server push</a> and it took nearly the entire industry to convince Apple that this was a bad idea.
The upside is that we got <a href="https://lists.apple.com/mailman/listinfo/hls-announce">a mailing list</a> so they can announce changes to developers first… but don’t expect the ability to propose changes any time soon.</p>
<p>DASH is it’s own can of worms as it’s controlled by <a href="https://en.wikipedia.org/wiki/Moving_Picture_Experts_Group">MPEG</a>.
The specifications are <a href="https://www.iso.org/standard/79106.html">behind a paywall</a> or <a href="https://www.streamingmedia.com/Articles/ReadArticle.aspx?ArticleID=133508">require patent licensing</a>?
I can’t even tell if I’m going to <a href="https://www.mpegla.com/wp-content/uploads/DASHWeb.pdf">get sued</a> for parsing a DASH playlist without paying the troll toll.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/troll.webp" alt="troll toll"></p><figcaption><p><a href="https://itsalwayssunny.fandom.com/wiki/The_Nightman_Cometh">Source</a>. 🎵 You gotta pay the Troll Toll 🎵</p></figcaption></figure>
<h2 id="whats-next">What’s next?</h2>
<p>You’re given a blank canvas and a brush to paint the greenest of fields, what do you make?</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/green.jpg" alt="green field"></p><figcaption><p><a href="https://www.freeimageslive.co.uk/free_stock_image/green-field-painting-jpg">Source</a>. Wow. That’s quite the
green field.</p></figcaption></figure>
<h2 id="tcp">TCP</h2>
<p>After my <a href="https://quic.video/blog/replacing-webrtc">previous blog post</a>, I had a few people hit up my DMs and claim they can do real-time latency with TCP.
And I’m sure a few more people will too after this post, so you get your own section that muddles the narrative.</p>
<p>Yes, you can do real-time latency with TCP (or WebSockets) under ideal conditions.</p>
<p>However, it just won’t work well enough on poor networks.
Congestion and buffer-bloat will absolutely wreck your protocol on poor networks.
A lot of my time spent at Twitch was optimizing for the 90th percentile; the shoddy cellular networks in Brazil or India or Australia.</p>
<p>But if you are going to reinvent RTMP, there are <a href="https://www.youtube.com/watch?v=cpYhm74zp0U">some ways to reduce queuing</a> but they are quite limited.
This is <em>especially</em> true in a browser environment when limited to HTTP or <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API">WebSockets</a>.</p>
<p>See my next blog post about <strong>Replacing RTMP</strong>.</p>
<h2 id="http">HTTP</h2>
<p>Notably absent thus far has been any mention of <a href="https://www.theoplayer.com/blog/low-latency-hls-lhls">LL-HLS</a> and <a href="https://www.wowza.com/blog/what-is-low-latency-dash">LL-DASH</a>.
These two protocols are meant to lower HLS/DASH latency respectively by breaking media segments into smaller chunks.</p>
<p>The chunks might be smaller, but they’re still served sequentially over TCP.
The latency floor is lower but the latency ceiling is still just as high, and you’re still going to buffer during congestion.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/buffering.gif" alt="buffering">
</p><figcaption>&gt; tfw LL-HLS/LL-DASH</figcaption></figure>
<p>We’re also approaching the limit of what you can do with HTTP semantics.</p>
<ul>
<li><strong>LL-HLS</strong> has configurable latency at the cost of and exponential number of sequential requests in the critical path. For example, 20 HTTP requests a second <em>per track</em> still only gets you +100ms of latency, which is not even viable for real-time latency.</li>
<li><strong>LL-DASH</strong> can be configured down to +0ms added latency, delivering frame-by-frame with chunked-transfer. However it absolutely wrecks client-side ABR algorithms. Twitch <a href="https://blog.twitch.tv/en/2020/01/15/twitch-invites-you-to-take-on-our-acm-mmsys-2020-grand-challenge/">hosted a challenge</a> to improve this but I’m convinced it’s impossible without server feedback.</li>
</ul>
<p><a href="https://www.theoplayer.com/solutions/hesp-high-efficiency-streaming">HESP</a> also gets a special shout-out because it’s cool.
It works by canceling HTTP requests during congestion and frankensteining the video encoding which is quite <del>hacky</del> clever, but suffers a similar fate.</p>
<p>We’ve hit a wall with HTTP over TCP.</p>
<h2 id="http3">HTTP/3</h2>
<p>If you’re an astute hyper text transport protocol aficionado, you might have noticed that I said “HTTP over TCP” above.
But <a href="https://www.cloudflare.com/learning/performance/what-is-http3">HTTP/3</a> uses <a href="https://www.rfc-editor.org/rfc/rfc9000.html">QUIC</a> instead of TCP.
Problem solved! We can replace any mention of <del>TCP</del> with QUIC!</p>
<p>Well, not quite. To use another complicated topic as a metaphor:</p>
<ul>
<li>A TCP connection is a single-core CPU.</li>
<li>A QUIC connection is a multi-core CPU.</li>
</ul>
<p>If you take a single threaded program and run it on a multi-core machine, it will run just as slow, and perhaps even slower.
This is the case with HLS/DASH as each segment request is made <em>sequentially</em>.
HTTP/3 is not a magic bullet and only has marginal benefits when used with HLS/DASH.</p>
<p>The key to using QUIC is to embrace concurrency.</p>
<p>This means utilizing multiple, independent streams that share a connection.
You can prioritize a stream so it gets more bandwidth during congestion, much like you can use <code>nice</code> on Linux to prioritize a process when CPU starved.
If a stream is taking too long, you can cancel it much like you can <code>kill</code> a process.</p>
<p>For live media, you want to prioritize new media over old media in order to skip old content.
You also want to prioritize audio over video, so you can hear what someone is saying without necessarily seeing their lips move.
If you can only transmit half of a media stream in time, make sure it’s the most important half.</p>
<p>To Apple/Pantos’ credit, LL-HLS is exploring <a href="https://mailarchive.ietf.org/arch/msg/hls-interest/RcZ2SG8Sz_zZEcjWnDKzcM_-TJk/">prioritization using HTTP/3</a>.
It doesn’t go far enough (yet!) and HTTP semantics get in the way, but it’s absolutely the right direction.
I’m convinced that somebody will make a <a href="https://mailarchive.ietf.org/arch/msg/moq/S3eOPU5XnvQ4kn1zJyDThG5U4sA/">HTTP/3 only media protocol</a> at some point.</p>
<p>But of course I’m biased towards…</p>

<p>MoQ utilizes WebTransport/QUIC directly to avoid TCP and HTTP.
But what about that whole <strong>economies of scale</strong> stuff?</p>
<p>Well, there are some important differences between Media over QUIC as compared to your standard <em>not invented here</em> protocol:</p>
<h2 id="reason-0-quic">Reason 0: QUIC</h2>
<p>QUIC is the future of the internet.
TCP is a relic of the past.</p>
<figure><img src="https://quic.video/home/quic.svg" alt="QUIC Logo"><figcaption>You’re going to see a lot of this logo, although not crudely traced or green.</figcaption></figure>
<p>It’s a <strong>bold</strong> claim I know.
But I struggle to think of a single reason why you would use TCP over QUIC going forward.
There are still some corporate firewalls that block UDP (used by QUIC) and hardware offload doesn’t exist yet, but I mean that’s about it.</p>
<p>It will take a few years, but every library, server, load balancer, and NIC will be optimized for QUIC delivery.
Media over QUIC offloads as much as possible into this powerful layer.
We benefit also from any new features, including proposals such as <a href="https://datatracker.ietf.org/doc/draft-ietf-quic-multipath/">multi-path</a>, <a href="https://datatracker.ietf.org/doc/draft-michel-quic-fec/">FEC</a>, <a href="https://datatracker.ietf.org/doc/rfc9330/">congestion control</a>, etc.
I don’t want network features in my media layer <em>thank you very much</em> (looking at you WebRTC).</p>
<p>It might not be obvious is that HTTP/3 is actually a thin layer on top of QUIC.
Likewise MoQ is also meant to be a thin layer on top of QUIC, effectively just providing pub/sub semantics.
We get all of the benefits of QUIC without the baggage of HTTP, and yet still achieve web support via <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebTransport_API">WebTransport</a>.</p>
<p>Instead we can focus on the important stuff instead: <strong>live media</strong>.</p>
<h2 id="reason-1-relay-layer">Reason 1: Relay Layer</h2>
<p>To avoid <a href="https://quic.video/blog/replacing-webrtc">the mistakes of WebRTC</a>, we need to decouple the application from the transport.
If a relay (ie. CDN) knows anything about media encoding, we have failed.</p>
<p>The idea is to break MoQ into layers.</p>
<p><a href="https://datatracker.ietf.org/doc/draft-ietf-moq-transport/">MoqTransport</a> is the base layer and is a typical pub/sub protocol, although catered toward QUIC.
The application splits data into “objects”, annotated with a header providing simple instructions on how the relay needs to deliver it.
These are generic signals, including stuff like the priority, reliability, grouping, expiration, etc.</p>
<p>MoqTransport is designed to be used for arbitrary applications.
Some examples include:</p>
<ul>
<li>live chat</li>
<li>end-to-end encryption</li>
<li>game state</li>
<li>live playlists</li>
<li>or even a clock!</li>
</ul>
<p>This is huge draw for CDN vendors.
Instead of building a custom WebRTC CDN that targets one specific niche, you can cast a much wider net with MoqTransport.
Akamai, Google, and Cloudflare have been involved in the standardization process thus far and CDN support is inevitable.</p>

<p>There will be at least one media layer on top of MoqTransport.
We’re focused on the transport right now so there’s no official “adopted” draft yet.</p>
<p>However, my proposal is <a href="https://datatracker.ietf.org/doc/draft-law-moq-warpstreamingformat/">Warp</a>.
It uses CMAF so it’s backwards compatible with HLS/DASH while still capable of real-time latency.
I think this is critically important, as any migration has to be done piecewise, client-by-client and user-by-user.
The same media segments can be served for a mixed roll-out and for VoD.</p>
<p>This website uses Warp! <a href="https://quic.video/watch">Try it out!</a> Or watch one of my <a href="https://www.youtube.com/watch?v=PncdrMPVaNc">presentations</a>.</p>
<p>There will absolutely be other mappings and containers; MoQ is not married to CMAF.
The important part is that only the encoder/decoder understand this media layer and not any relays in the middle.
There’s a lot of cool ideas floating around, such as a <a href="https://datatracker.ietf.org/doc/draft-wilaw-moq-catalogformat/">live playlist format</a> and a <a href="https://datatracker.ietf.org/doc/draft-mzanaty-moq-loc/">low-overhead container</a>.</p>
<h2 id="reason-3-ietf">Reason 3: IETF</h2>
<p>Media over QUIC is an <a href="https://datatracker.ietf.org/wg/moq/about/">IETF working group</a>.</p>
<figure><img src="https://quic.video/home/ietf.svg" alt="IETF Logo"><figcaption>I crudely traced and recolored this logo too.</figcaption></figure>
<p>If you know nothing about the IETF, just know that it’s the standards body behind favorites such as HTTP, DNS, TLS, QUIC, and even WebRTC.
But I think <a href="https://www.ietf.org/about/introduction/">this part</a> is especially important:</p>
<blockquote>
<p>There is no membership in the IETF. Anyone can participate by signing up to a working group mailing list (more on that below), or registering for an IETF meeting. All IETF participants are considered volunteers and expected to participate as individuals, including those paid to participate.</p>
</blockquote>
<p>It’s not a protocol owned by a company.
It’s not a protocol owned by lawyers.</p>
<p><a href="https://www.ietf.org/mailman/listinfo/moq">Join the mailing list</a>.</p>
<h2 id="whats-missing">What’s missing?</h2>
<p>Okay cool so hopefully I sold you on MoQ.
What can’t you use it today to replace HLS/DASH?</p>
<ol>
<li><strong>It’s not done yet</strong>: The IETF is many things, but fast is not one of them.</li>
<li><strong>Cost</strong>: QUIC is a new protocol that has yet to be fully optimized to match TCP. It’s possible and apparently Google is <a href="https://conferences.sigcomm.org/sigcomm/2020/files/slides/epiq/0%20QUIC%20and%20HTTP_3%20CPU%20Performance.pdf">near parity</a>.</li>
<li><strong>Support</strong>: Your favorite language/library/cdn/cloud/browser might not even provide HTTP/3 support yet, let alone WebTransport or QUIC.</li>
<li><strong>Features</strong>: Somebody has to reimplement all of the annoying HLS/DASH features like DRM and server-side advertisements…</li>
<li><strong>VoD</strong>: MoQ is currently live only. HLS/DASH work great, why replace it?</li>
</ol>
<p>We’ll get there eventually.</p>
<p>Feel free to use our <a href="https://github.com/kixelated/moq-rs">Rust</a> or <a href="https://github.com/kixelated/moq-js">Typescript</a> implementation is you want to experiment.
Join the <a href="https://discord.gg/FCYF3p99mr">Discord</a> if you want to help!</p>
<p>Written by <a href="https://github.com/kixelated">@kixelated</a>.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ultra-white ceramic cools buildings with record-high 99.6% reflectivity (150 pts)]]></title>
            <link>https://newatlas.com/materials/ultra-white-ceramic-cools-buildings-record-high-reflectivity/</link>
            <guid>38263934</guid>
            <pubDate>Tue, 14 Nov 2023 14:51:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/materials/ultra-white-ceramic-cools-buildings-record-high-reflectivity/">https://newatlas.com/materials/ultra-white-ceramic-cools-buildings-record-high-reflectivity/</a>, See on <a href="https://news.ycombinator.com/item?id=38263934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Scientists in Hong Kong have demonstrated a new ultra-white ceramic material that can drastically cool buildings by reflecting sunlight and heat at record highs. The beetle-inspired material gets its ability from its nanostructure, stays tough to the elements and should be relatively easy to scale up for production.</p><p>When our homes get too hot, the first solution for many people is to crank the air conditioning. It may be effective, but it’s not very energy efficient, as heating and cooling buildings accounts for a huge percentage of energy costs. So, scientists are investigating ways to passively manage interior temperatures, and one of the simplest is to just <a href="https://newatlas.com/white-roof-heat-island/21758/" data-cms-ai="0">paint buildings and rooftops white</a>.</p><p>Basic physics dictates that lighter colors absorb less light than darker ones, and therefore remain cooler. In recent years new <a href="https://newatlas.com/materials/super-white-paint-teflon-98-suns-heat/" data-cms-ai="0">ultra-white paints</a> have been developed that reflect over 95% of the sunlight that hits them, doing a decent job of cooling buildings. But these coatings have their issues, including durability.</p><p>For the new study, scientists at City University of Hong Kong (CityU) developed a new cooling ceramic material that performs better than others. Rather than just being a very white paint, the material gets its high reflectivity from its nanostructure, which efficiently scatters almost the entire spectrum of sunlight – just like the <a href="https://newatlas.com/beetle-scales-white/53789/" data-cms-ai="0">Cyphochilus beetle</a> which inspired it. This results in a solar reflectivity of 99.6%, which is a record high, along with an infrared thermal emission of 96.5%.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="Samples of the cooling ceramic material, which reflect a record high of 99.6% of sunlight that hits them, cooling interiors of buildings" width="957" height="805" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/f269aec/2147483647/strip/true/crop/957x805+0+0/resize/440x370!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 440w,https://assets.newatlas.com/dims4/default/371cea4/2147483647/strip/true/crop/957x805+0+0/resize/800x673!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 800w,https://assets.newatlas.com/dims4/default/e89623e/2147483647/strip/true/crop/957x805+0+0/resize/1200x1009!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1200w,https://assets.newatlas.com/dims4/default/a4d3c90/2147483647/strip/true/crop/957x805+0+0/resize/1920x1615!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1920w" data-src="https://assets.newatlas.com/dims4/default/008ee1a/2147483647/strip/true/crop/957x805+0+0/resize/957x805!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/f269aec/2147483647/strip/true/crop/957x805+0+0/resize/440x370!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 440w,https://assets.newatlas.com/dims4/default/371cea4/2147483647/strip/true/crop/957x805+0+0/resize/800x673!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 800w,https://assets.newatlas.com/dims4/default/e89623e/2147483647/strip/true/crop/957x805+0+0/resize/1200x1009!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1200w,https://assets.newatlas.com/dims4/default/a4d3c90/2147483647/strip/true/crop/957x805+0+0/resize/1920x1615!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1920w" src="https://assets.newatlas.com/dims4/default/008ee1a/2147483647/strip/true/crop/957x805+0+0/resize/957x805!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg">
</p>



    
    

    
        <div><figcaption itemprop="caption">Samples of the cooling ceramic material, which reflect a record high of 99.6% of sunlight that hits them, cooling interiors of buildings</figcaption><p>City University of Hong Kong</p></div>
    
</figure>

                
            </div><p>The material is made of alumina, which not only reduces solar absorption, but the team says it makes the cooling ceramic more durable in the face of weather. It resists degradation from UV light exposure, which is a weakness of other passive cooling materials and coatings, and boosts the rate of water evaporation from the surface, which adds the bonus effect of evaporative cooling. It even boasts fire resistance by withstanding temperatures of over 1,000 °C (1,832 °F).</p><p>““The beauty of the cooling ceramic is that it fulfills the requirements for both high-performance PRC and applications in real-life settings,” said Professor Edwin Tso Chi-yan, co-corresponding author of the study. “Our experiment found that applying the cooling ceramic on a house roof can achieve more than 20% electricity [reduction] for space cooling, which confirms the great potential of cooling ceramic in reducing people’s reliance on traditional active cooling strategies and provides a sustainable solution for avoiding electricity grid overload, greenhouse gas emissions and urban heat islands.”</p><p>Finally, the researchers also say that the material can be easily mass produced, using common materials like alumina and a two-step process of phase inversion and sintering. And if white is too boring for some houses, the material can apparently be produced in other colors and patterns by adding extra layers.</p><p>The research was published in the journal <i><a href="https://www.science.org/doi/10.1126/science.adi4725" target="_blank" data-cms-ai="0">Science</a></i>.</p><p>Source: <a href="https://www.cityu.edu.hk/research/stories/2023/11/10/new-cooling-ceramic-can-enhance-energy-efficiency-construction-sector-and-help-combat-global-warming-cityu-research" target="_blank" data-cms-ai="0">City University of Hong Kong</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PyPI has completed its first security audit (101 pts)]]></title>
            <link>https://blog.pypi.org/posts/2023-11-14-1-pypi-completes-first-security-audit/</link>
            <guid>38263786</guid>
            <pubDate>Tue, 14 Nov 2023 14:38:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.pypi.org/posts/2023-11-14-1-pypi-completes-first-security-audit/">https://blog.pypi.org/posts/2023-11-14-1-pypi-completes-first-security-audit/</a>, See on <a href="https://news.ycombinator.com/item?id=38263786">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Tags -->


<!-- Actions -->

  
  


<!--
  Hack: check whether the content contains a h1 headline. If it doesn't, the
  page title (or respectively site name) is used as the main headline.
-->

  



  by: <b>Dustin Ingram · </b>


  <span>2023-11-14</span>


<!-- Page content -->



<p><em>This is part one in a three-part series. See <a href="https://blog.pypi.org/posts/2023-11-14-2-security-audit-remediation-warehouse/">part two here</a>, and <a href="https://blog.pypi.org/posts/2023-11-14-3-security-audit-remediation-cabotage/">part three here</a></em></p>
<p>We are proud to announce that PyPI has completed its first ever external security audit.
This work was funded in partnership with the <a href="https://www.opentech.fund/">Open Technology Fund</a> (OTF), <a href="https://www.opentech.fund/results/supported-projects/pypi-improvements/">a previous supporter</a> of security-related improvements to PyPI.</p>
<p>The Open Technology Fund selected <a href="https://www.trailofbits.com/">Trail of Bits</a>, an industry-leading cybersecurity firm with significant open-source and Python experience, to perform the audit.
Trail of Bits spent a total of 10 engineer-weeks of effort identifying issues, presenting those findings to the PyPI team, and assisting us as we remediated the findings.</p>
<h2 id="scope">Scope</h2>
<p>The audit was focused on "Warehouse", the open-source codebase that powers <a href="https://pypi.org/">https://pypi.org</a>, and on "cabotage", the custom open-source container orchestration framework we use to deploy Warehouse.
It included code review of both codebases, prioritizing areas that accept user input, provide APIs and other public surfaces.
The audit also covered the continuous integration / continuous deployment (CI/CD) configurations for both codebases.</p>
<h2 id="findings">Findings</h2>
<p>Overall, the auditors determined the Warehouse codebase "was adequately tested and conformed to widely accepted best practices for secure Python and web development," and that while the cabotage codebase lacks the same level of testing, they did not identify any high severity issues in either codebase.</p>
<h2 id="results-impact">Results &amp; Impact</h2>
<p>As a result of the audit, Trail of Bits detailed 29 different advisories discovered across both codebases.
When evaluating severity level of each advisory, 14 were categorized as "informational", 6 as "low", 8 as "medium" and zero as "high".
At the time of writing, the PyPI team has remediated all advisories that posed a significant risk in both codebases where possible, and has worked with third-party teams to unblock additional remediations where necessary.</p>
<h2 id="more-details">More details</h2>
<p>In the interest of transparency, today we are publishing the <a href="https://github.com/trailofbits/publications#technology-product-reviews">full results of the audit</a>, as prepared by Trail of Bits.
You can read more about the audit from their perspective in their <a href="https://blog.trailofbits.com/2023/11/14/our-audit-of-pypi/">accompanying blog post</a>.</p>
<p>Additionally, in two additional blog posts published today, Mike Fiedler (PyPI Security &amp; Safety Engineer) goes into detail about <a href="https://blog.pypi.org/posts/2023-11-14-2-security-audit-remediation-warehouse/">how we remediated these findings in Warehouse</a> and Ee Durbin (Python Software Foundation Director of Infrastructure) <a href="https://blog.pypi.org/posts/2023-11-14-3-security-audit-remediation-cabotage/">similarly details remediation's in cabotage</a>.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We would like to thank the Open Technology Fund for their continued support of PyPI and specifically for this significant security milestone for the Python ecosystem.
We would also like to thank Trail of Bits for being a dependable, thorough and thoughtful partner throughout the process.</p>

<!-- Source file information -->


<!-- Was this page helpful? -->




<!-- Comment system -->

                
              </article>
            </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Backblaze Drive Stats for Q3 2023 (235 pts)]]></title>
            <link>https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/</link>
            <guid>38263435</guid>
            <pubDate>Tue, 14 Nov 2023 14:07:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/">https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/</a>, See on <a href="https://news.ycombinator.com/item?id=38263435">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="Backblaze Drive Stats for Q3 2023" itemscope="" itemtype="https://schema.org/CreativeWork"><div itemprop="text">
<figure><img loading="lazy" decoding="async" width="1024" height="583" src="https://wp-admin.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-1024x583.png" alt="A decorative image showing the title Q3 2023 Drive Stats. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-1024x583.png 1024w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-300x171.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-768x437.png 768w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-560x319.png 560w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>

<p>At the end of Q3 2023, Backblaze was monitoring 263,992 hard disk drives (HDDs) and solid state drives (SSDs) in our data centers around the world. Of that number, 4,459 are boot drives, with 3,242 being SSDs and 1,217 being HDDs. The failure rates for the SSDs are analyzed in the <a href="https://www.backblaze.com/blog/ssd-edition-2023-mid-year-drive-stats-review/" target="_blank" rel="noreferrer noopener">SSD Edition: 2023 Drive Stats review</a>.</p>
<p>That leaves us with 259,533 HDDs that we’ll focus on in this report. We’ll review the quarterly and lifetime failure rates of the data drives as of the end of Q3 2023. Along the way, we’ll share our observations and insights on the data presented, and, for the first time ever, we’ll reveal the drive failure rates broken down by data center.</p>
<h2>Q3 2023 Hard Drive Failure Rates</h2>
<p>At the end of Q3 2023, we were managing 259,533 hard drives used to store data. For our review, we removed 449 drives from consideration as they were used for testing purposes, or were drive models which did not have at least 60 drives. This leaves us with 259,084 hard drives grouped into 32 different models.&nbsp;</p>
<p>The table below reviews the annualized failure rate (AFR) for those drive models for the Q3 2023 time period.</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="1 – Quarterly AFR Table – Updated" data-rl_caption="1 – Quarterly AFR Table – Updated" title="1 – Quarterly AFR Table – Updated"><img loading="lazy" decoding="async" width="661" height="1024" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-661x1024.jpg" alt="A table showing the quarterly annualized failure rates of Backblaze hard drives." srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-661x1024.jpg 661w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-194x300.jpg 194w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-560x868.jpg 560w" sizes="(max-width: 661px) 100vw, 661px"></a></figure>
<h3>Notes and Observations on the Q3 2023 Drive Stats</h3>
<ul>
<li><strong>The 22TB drives are here</strong>: At the bottom of the list you’ll see the WDC 22TB drives (model: WUH722222ALE6L4). A <a href="https://www.backblaze.com/blog/vault-cloud-storage-architecture/" target="_blank" rel="noreferrer noopener">Backblaze Vault</a> of 1,200 drives (plus four) is now operational. The 1,200 drives were installed on September 29, so they only have one day of service each in this report, but zero failures so far.</li>
<li><strong>The old get bolder</strong>: At the other end of the time-in-service spectrum are the 6TB Seagate drives (model: ST6000DX000) with an average of 101 months in operation. This cohort had zero failures in Q3 2023 with 883 drives and a lifetime AFR of 0.88%.</li>
<li><strong>Zero failures</strong>: In Q3, six different drive models managed to have zero drive failures during the quarter. But only the 6TB Seagate, noted above, had over 50,000 drive days, our minimum standard for ensuring we have enough data to make the AFR plausible.</li>
<li><strong>One failure</strong>: There were four drive models with one failure during Q3. After applying the 50,000 drive day metric, two drives stood out:
<ol>
<li>WDC 16TB (model: WUH721816ALE6L0) with a 0.15% AFR.</li>
<li>Toshiba 14TB (model: MG07ACA14TEY) with a 0.63% AFR.</li>
</ol>
</li>
</ul>
<h3>The Quarterly AFR Drops</h3>
<p>In Q3 2023, quarterly AFR for all drives was 1.47%. That was down from 2.2% in Q2 and also down from 1.65% a year ago. The quarterly AFR is based on just the data in that quarter, so it can often fluctuate from quarter to quarter.&nbsp;</p>
<p>In our Q2 2023 report, we suspected the 2.2% for the quarter was due to the overall aging of the drive fleet and in particular we pointed a finger at specific 8TB, 10TB, and 12TB drive models as potential culprits driving the increase. That prediction fell flat in Q3 as nearly two-thirds of drive models experienced a decreased AFR quarter over quarter from Q2 and any increases were minimal. This included our suspect 8TB, 10TB, and 12TB drive models.&nbsp;</p>
<p>It seems Q2 was an anomaly, but there was one big difference in Q3: we retired 4,585 aging 4TB drives. The average age of the retired drives was just over eight years, and while that was a good start, there’s another 28,963 4TB drives to go. To facilitate the continuous retirement of aging drives and make the data migration process easy and safe we use CVT, our awesome in-house data migration software which we’ll cover at another time.</p>
<h2>A Hot Summer and the Drive Stats Data</h2>
<p>As anyone should in our business, Backblaze continuously monitors our systems and drives. So, it was of little surprise to us when the folks at <a href="https://www.nasa.gov/news-release/nasa-announces-summer-2023-hottest-on-record/" target="_blank" rel="noreferrer noopener">NASA</a> confirmed the summer of 2023 as Earth’s hottest on record. The effects of this record-breaking summer showed up in our monitoring systems in the form of drive temperature alerts. A given drive in a storage server can heat up for many reasons: it is failing; a fan in the storage server has failed; other components are producing additional heat; the air flow is somehow restricted; and so on. Add in the fact that the ambient temperature within a data center often increases during the summer months, and you can get more temperature alerts.</p>
<p>In reviewing the temperature data for our drives in Q3, we noticed that a small number of drives exceeded the maximum manufacturer’s temperature for at least one day. The maximum temperature for most drives is 60°C, except for the 12TB, 14TB, and 16TB Toshiba drives which have a maximum temperature of 55°C. Of the 259,533 data drives in operation in Q3, there were 354 individual drives (0.0013%) that exceeded their maximum manufacturer temperature. Of those only two drives failed, leaving 352 drives which were still operational as of the end of Q3.</p>
<p>While temperature fluctuation is part of running data centers and temp alerts like these aren’t unheard of, our data center teams are looking into the root causes to ensure we’re prepared for the inevitability of increasingly hot summers to come.</p>
<h3>Will the Temperature Alerts Affect Drive Stats?</h3>
<p>The two drives which exceeded their maximum temperature and failed in Q3 have been removed from the Q3 AFR calculations. Both drives were 4TB Seagate drives (model: ST4000DM000). Given that the remaining 352 drives which exceeded their temperature maximum did not fail in Q3, we have left them in the Drive Stats calculations for Q3 as they did not increase the computed failure rates.</p>
<p>Beginning in Q4, we will remove the 352 drives from the regular Drive Stats AFR calculations and create a separate cohort of drives to track that we’ll name Hot Drives. This will allow us to track the drives which exceeded their maximum temperature and compare their failure rates to those drives which operated within the manufacturer’s specifications. While there are a limited number of drives in the Hot Drives cohort, it could give us some insight into whether drives being exposed to high temperatures could cause a drive to fail more often. This heightened level of monitoring will identify any increase in drive failures so that they can be detected and dealt with expeditiously.</p>
<h2>New Drive Stats Data Fields in Q3</h2>
<p>In Q2 2023, we introduced three new data fields that we started populating in the Drive Stats data we publish: <code>vault_id</code>, <code>pod_id</code>, and <code>is_legacy_format</code>. In Q3, we are adding three more fields into each drive records as follows:</p>
<ul>
<li><code>datacenter</code>: The Backblaze data center where the drive is installed, currently one of these values: <code>ams5</code>, <code>iad1</code>, <code>phx1</code>, <code>sac0</code>, and <code>sac2</code>.</li>
<li><code>cluster_id</code>: The name of a given collection of storage servers logically grouped together to optimize system performance. Note: At this time the <code>cluster_id</code> is not always correct, we are working on fixing that.&nbsp;</li>
<li><code>pod_slot_num</code>: The physical location of a drive within a storage server. The specific slot differs based on the storage server type and capacity: Backblaze (45 drives), Backblaze (60 drives), Dell (26 drives), or Supermicro (60 drives). We’ll dig into these differences in another post.</li>
</ul>
<p>With these additions, the new schema beginning in Q3 2023 is:</p>
<ul>
<li><code>date</code></li>
<li><code>serial_number</code></li>
<li><code>model</code></li>
<li><code>capacity_bytes</code></li>
<li><code>failure</code></li>
<li><strong><code>datacenter </code>(Q3)</strong></li>
<li><strong><code>cluster_id</code> (Q3)</strong></li>
<li><strong><code>vault_id </code>(Q2)</strong></li>
<li><strong><code>pod_id </code>(Q2)</strong></li>
<li><strong><code>pod_slot_num</code> (Q3)</strong></li>
<li><strong><code>is_legacy_format </code>(Q2)</strong></li>
<li><code>smart_1_normalized</code></li>
<li><code>smart_1_raw</code></li>
<li>The remaining SMART value pairs (as reported by each drive model)</li>
</ul>
<p>Beginning in Q3, these data data fields have been added to the <a href="https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/" target="_blank" rel="noreferrer noopener">publicly available Drive Stats files</a> that we publish each quarter.&nbsp;</p>
<h2>Failure Rates by Data Center</h2>
<p>Now that we have the data center for each drive we can compute the AFRs for the drives in each data center. Below you’ll find the AFR for each of five data centers for Q3 2023.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="3 – AFR by data center" data-rl_caption="3 – AFR by data center" title="3 – AFR by data center"><img loading="lazy" decoding="async" width="600" height="430" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg 600w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1-300x215.jpg 300w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1-560x401.jpg 560w" sizes="(max-width: 600px) 100vw, 600px"></a></figure></div>
<h3>Notes and Observations</h3>
<ul>
<li><strong>Null?:</strong> The drives which reported a null or blank value for their data center are grouped in four Backblaze vaults. <a href="https://www.backblaze.com/blog/author/david-winings/" target="_blank" rel="noreferrer noopener">David</a>, the Senior Infrastructure Software Engineer for Drive Stats, <a href="https://www.backblaze.com/blog/overload-to-overhaul-how-we-upgraded-drive-stats-data/" target="_blank" rel="noreferrer noopener">described the process of how we gather all the parts of the Drive Stats data each day</a>. The TL:DR is that vaults can be too busy to respond at the moment we ask, and since the data center field is nice-to-have data, we get a blank field. We can go back a day or two to find the data center value, which we will do in the future when we report this data.</li>
<li><strong>sac0?</strong>: sac0 has the highest AFR of all of the data centers, but it also has the oldest drives—nearly twice as old, on average, versus the next closest in data center, sac2. As discussed previously, <a href="https://www.backblaze.com/blog/drive-failure-over-time-the-bathtub-curve-is-leaking/" target="_blank" rel="noreferrer noopener">drive failures do seem to follow the “bathtub curve”</a>, although recently we’ve seen the curve start out flatter. Regardless, as drive models age, they do generally fail more often. Another factor could be that sac0, and to a lesser extent sac2, has some of the oldest Storage Pods, including a handful of 45-drive units. We are in the process of using CVT to replace these older servers while migrating from 4TB to 16TB and larger drives.</li>
<li><strong>iad1</strong>: The iad data center is the foundation of our eastern region and has been growing rapidly since coming online about a year ago. The growth is a combination of new data and customers using our <a href="https://www.backblaze.com/blog/double-redundancy-support-compliance-and-more-with-cloud-replication-now-live/" target="_blank" rel="noreferrer noopener">cloud replication capability</a> to automatically make a copy of their data in another region.</li>
<li><strong>Q3 Data</strong>: This chart is for Q3 data only and includes all the data drives, including those with less than 60 drives per model. As we track this data over the coming quarters, we hope to get some insight into whether different data centers really have different drive failure rates, and, if so, why.</li>
</ul>
<h2>Lifetime Hard Drive Failure Rates</h2>
<p>As of September 30, 2023, we were tracking 259,084 hard drives used to store customer data. For our lifetime analysis, we collect the number of drive days and the number of drive failures for each drive beginning from the time a drive was placed into production in one of our data centers. We group these drives by model, then sum up the drive days and failures for each model over their lifetime. That chart is below.&nbsp;</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="4 – Lifetime AFR Table" data-rl_caption="4 – Lifetime AFR Table" title="4 – Lifetime AFR Table"><img loading="lazy" decoding="async" width="685" height="1024" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-685x1024.jpg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-685x1024.jpg 685w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-201x300.jpg 201w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-560x838.jpg 560w" sizes="(max-width: 685px) 100vw, 685px"></a></figure>
<p>One of the most important columns on this chart is the confidence interval, which is the difference between the low and high AFR confidence levels calculated at 95%. The lower the value, the more certain we are of the AFR stated. We like a confidence interval to be 0.5% or less. When the confidence interval is higher, that is not necessarily bad, it just means we either need more data or the data is somewhat inconsistent.&nbsp;</p>
<p>The table below contains just those drive models which have a confidence interval of less than 0.5%. We have sorted the list by drive size and then by AFR.</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="5 – Lifetime AFR by CI" data-rl_caption="5 – Lifetime AFR by CI" title="5 – Lifetime AFR by CI"><img loading="lazy" decoding="async" width="710" height="780" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg 710w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1-273x300.jpeg 273w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1-560x615.jpeg 560w" sizes="(max-width: 710px) 100vw, 710px"></a></figure>
<p>The 4TB, 6TB, 8TB, and some of the 12TB drive models are no longer in production. The HGST 12TB models in particular can still be found, but they have been relabeled as Western Digital and given alternate model numbers. Whether they have materially changed internally is not known, at least to us.</p>
<p>One final note about the lifetime AFR data: you might have noticed the AFR for all of the drives hasn’t changed much from quarter to quarter. It has vacillated between 1.39% to 1.45% percent for the last two years. Basically, we have lots of drives with lots of time-in-service so it is hard to move the needle up or down. While the lifetime stats for individual drive models can be very useful, the lifetime AFR for all drives will probably get less and less interesting as we add more and more drives. Of course, a few hundred thousand drives that never fail could arrive, so we will continue to calculate and present the lifetime AFR.</p>
<h2>The Hard Drive Stats Data</h2>
<p>The complete data set used to create the information used in this review is available on our <a href="https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/">Hard Drive Stats Data webpage</a>. You can download and use this data for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data to anyone; it is free.&nbsp;</p>
<p>Good luck and let us know if you find anything interesting.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Lot of Damage in Grindavík (290 pts)]]></title>
            <link>https://icelandmonitor.mbl.is/news/news/2023/11/13/a_lot_of_damage_in_grindavik/</link>
            <guid>38263294</guid>
            <pubDate>Tue, 14 Nov 2023 13:54:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://icelandmonitor.mbl.is/news/news/2023/11/13/a_lot_of_damage_in_grindavik/">https://icelandmonitor.mbl.is/news/news/2023/11/13/a_lot_of_damage_in_grindavik/</a>, See on <a href="https://news.ycombinator.com/item?id=38263294">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><a href="https://cdn.mbl.is/frimg/1/45/18/1451865.jpg" id="a1451865" target="_blank" title="Looking at the fissure." rel="newsimgs"><img src="https://cdn.mbl.is/frimg/1/45/18/1451865.jpg" alt="Looking at the fissure." width="730" height="486"></a>
    </p>
      <p>
        Looking at the fissure.
          <span>mbl.is/Eggert Jóhannesson</span>
      </p>
  </div><div>
        
  
    

        
          <p>
 A lot of damage can be seen around Grindavík after the earthquakes and the formation of a deformation that is moving downwards towards the magma intrusion that is underneath the ground. This depression formation is now estimated to be over 1.2 meters in the northwest end of Grindavík.
</p>

        
          <p>
 A fissure passes through a large part of the town and passes through the road to the sports center. A hot water pipe has been broken in the earthquakes and the sports center seems to be sitting on a meter high pedestal.
</p>

        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    A journalist and a photographer from mbl.is have been travelling around the town today looking at the situation and the damage in town.
   </span>
  </span>
 </span>
</p>

        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    As can be seen in these two pictures from the sports center, the power of the earthquakes has been substantial. The faulting is up to one meter and it is like the pavement at the front has been severed from the building.
   </span>
  </span>
 </span>
</p>

        
          
  
  

  



        
          
  
  

  



        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    A short distance from the sports center, a huge fissure can be seen through the road, and a more than meter-long opening stretches through a part of the town. It has, among other things, at one point broken a hot water pipe and hot air is steaming out of the fissure.
   </span>
  </span>
 </span>
</p>

        
          
  
  

  



        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    The grass is soft and yielding widely close to cracks when walking on it, indicating that there is a widespread lack of soil under the grass. Thus, a rescuer found himself stepping with one foot in one place through grass without being harmed. A journalist at mbl.is has therefore been directed to stay on the tarmac because of this weakness in the soil.
   </span>
  </span>
 </span>
</p>

        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    Today, residents have been allowed into town to retrieve the main necessities and valuable property. They have only a short time to do so. Initially, residents were required to accompany the rescue teams, but as the day went on, it was decided that people could enter in their own cars. However, there are security posts around town where rescue teams are located, pushing people. People had been made aware that they were only expected to be in their home for 5-7 minutes.
   </span>
  </span>
 </span>
</p>

        
          <p>
 Damages are visible on many houses, especially cracks in the concrete, but our journalist from mbl.is could not see any building that could be written off as completely ruined.
</p>
          
      
      
    

    

  

  

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lapce Editor 0.3 (195 pts)]]></title>
            <link>https://github.com/lapce/lapce/releases/tag/v0.3.0</link>
            <guid>38262775</guid>
            <pubDate>Tue, 14 Nov 2023 13:00:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lapce/lapce/releases/tag/v0.3.0">https://github.com/lapce/lapce/releases/tag/v0.3.0</a>, See on <a href="https://news.ycombinator.com/item?id=38262775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:lapce/lapce" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="e3DVR65w5hz69YvA3wjfhPHiqLSx6eARwKkcYP3vij6pvU1pWM2isVjdoFaRVV60Oih20G_kYsgNFP1oMtxwNQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="lapce/lapce" data-current-org="lapce" data-current-owner="" data-logged-in="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Freleases%2Fshow&amp;source=header-repo&amp;source_repo=lapce%2Flapce" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lapce/lapce/releases/tag/v0.3.0&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="e2ad5d9fcbff23b186413b8ae3446dbdc571904917e9c0e8b53856a2d76e4d08" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/releases/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Announcing the MonoGame Foundation (119 pts)]]></title>
            <link>https://community.monogame.net/t/announcing-the-monogame-foundation/19809</link>
            <guid>38262320</guid>
            <pubDate>Tue, 14 Nov 2023 12:11:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.monogame.net/t/announcing-the-monogame-foundation/19809">https://community.monogame.net/t/announcing-the-monogame-foundation/19809</a>, See on <a href="https://news.ycombinator.com/item?id=38262320">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post_1" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        <div>
          
          


          <p><span>
              <time itemprop="datePublished" datetime="2023-11-13T20:11:00Z">
                November 13, 2023,  8:11pm
              </time>
              <meta itemprop="dateModified" content="2023-11-13T20:11:00Z">
          <span itemprop="position">#1</span>
          </span>
        </p></div>
        <div itemprop="articleBody">
          <p>Two months ago, <a href="https://community.monogame.net/t/update-on-monogames-status/19457">we announced</a> our intention to create a non-profit organization to help fund and secure the future of the MonoGame framework as an open-source initiative.</p>
<p>Today, we are pleased to announce that the MonoGame Foundation is officially in business and has applied for non-profit status (which is pending validation).</p>
<p>The MonoGame Foundation’s goal is to bolster the development of the MonoGame framework by voting and funding initiatives to strengthen or expand the open-source project. To this purpose, the Patreon and active donations have been redirected to this new entity.</p>
<p>This new step wouldn’t be possible without a major sponsor: <a href="https://re-logic.com/">Re-Logic</a>, developers of <a href="https://terraria.org/">Terraria</a>, who have committed to support the foundation with a $100,000 donation. On behalf of the foundation board, we would like to celebrate their support and dedication to open-source software, as well as renewing our thanks to past, present, and future donors.</p>
<p>To highlight this shift, we are introducing a new website which is now fully <a href="https://github.com/MonoGame/monogame.github.io">automated with Github</a>. The paint is still fresh, but we hope to expand it with more <a href="https://monogame.net/about.html#foundation">information about the MonoGame Foundation</a>, like how it operates, and how to join it or contribute.</p>
<p>We will soon communicate about the foundation roadmap and the start of projects.</p>
<p>We would like to thank the community for its continuous support, and we hope to build the next iteration of the framework together.</p>
<p>The MonoGame Foundation board:</p>
<ul>
<li>Dean Ellis, President</li>
<li>Marko Jeremic, Chairman</li>
<li>Dominique Louis, Treasurer</li>
<li>Simon Jackson, Secretary</li>
<li>Tom Spilman</li>
<li>Thomas Altenburger</li>
</ul>
        </div>

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">
          <meta itemprop="keywords" content="">

        

         

      </div><div id="post_2" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        <div>
          
          <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            <a itemprop="url" href="https://community.monogame.net/u/mrhelmut"><span itemprop="name">mrhelmut</span></a>
            
              pinned globally 
          </span></p>


          <p><span>
              <time itemprop="datePublished" datetime="2023-11-13T20:11:31Z">
                November 13, 2023,  8:11pm
              </time>
              <meta itemprop="dateModified" content="2023-11-13T20:11:31Z">
          <span itemprop="position">#2</span>
          </span>
        </p></div>
        

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">

        

         

      </div><div id="post_3" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        
        <div itemprop="articleBody">
          <p>Welcome to the new era of MonoGame everyone.</p>
<p>This is a step in the right direction.</p>
<p><img src="https://community.monogame.net/images/emoji/twitter/sake.png?v=12" title=":sake:" alt=":sake:" loading="lazy" width="20" height="20"></p>
        </div>

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">

        

         

      </div><div id="post_4" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        
        <p>Woo! This is such a huge step forwards and having the (massive) backing of a beloved studio like Re-Logic is huge news. I can’t wait to see the growth MonoGame’s going to have now that it has the resources to compete with at least the big FOSS engine that is Godot.</p>

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">

        

         

      </div><div id="post_5" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        
        <p>Excellent news! Congratulations! I hope this leads to many great things for Monogame!</p>

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">

        

         

      </div><div id="post_6" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        
        <p>Congrats to the new MonoGame Foundation.</p>

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">

        

         

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blender 4.0 (629 pts)]]></title>
            <link>https://wiki.blender.org/wiki/Reference/Release_Notes/4.0</link>
            <guid>38262315</guid>
            <pubDate>Tue, 14 Nov 2023 12:11:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.blender.org/wiki/Reference/Release_Notes/4.0">https://wiki.blender.org/wiki/Reference/Release_Notes/4.0</a>, See on <a href="https://news.ycombinator.com/item?id=38262315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      <h4>Download</h4>
                      <p>Get the latest Blender, older versions, or experimental builds.</p>
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Sues Men Who Weaponized DMCA Notices to Crush Competition (344 pts)]]></title>
            <link>https://torrentfreak.com/google-sues-men-who-weaponized-dmca-notices-to-crush-competition-231114/</link>
            <guid>38262124</guid>
            <pubDate>Tue, 14 Nov 2023 11:48:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/google-sues-men-who-weaponized-dmca-notices-to-crush-competition-231114/">https://torrentfreak.com/google-sues-men-who-weaponized-dmca-notices-to-crush-competition-231114/</a>, See on <a href="https://news.ycombinator.com/item?id=38262124">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a href="https://torrentfreak.com/images/dmca-google-s1.png"><img decoding="async" src="https://torrentfreak.com/images/dmca-google-s1.png" alt="dmca-google-s1" width="290" height="205" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20290%20205'%3E%3C/svg%3E" data-lazy-src="https://torrentfreak.com/images/dmca-google-s1.png"></a>While all non-compliant DMCA takedown notices are invalid by default, there’s a huge difference between those sent in error and others crafted for purely malicious purposes.</p>
<p>Bogus DMCA takedown notices are nothing new, but the rise of organized groups using malicious DMCA notices as a business tool has been apparent in recent years. </p>
<p>Since the vast majority of culprits facing zero consequences, that may have acted as motivation to send more. Through a lawsuit filed at a California court on Monday, Google appears to be sending the message that enough is enough.</p>
<h2>Defendants Weaponized DMCA Takedowns</h2>
<p>Google’s complaint targets Nguyen Van Duc and Pham Van Thien, both said to be residents of Vietnam and the leaders of up to 20 Doe defendants. Google says the defendants systematically abused accounts “to submit a barrage” of fraudulent copyright takedown requests aimed at removing their competitors’ website URLs from Google Search results.</p>
<p>“Defendants have weaponized copyright law’s notice-and-takedown process and used it not for its intended purpose of expeditiously removing infringing content, but instead to have the legitimate content of their competitors removed based on false allegations. Defendants’ illegal, fraudulent scheme harms consumers, third-party businesses, and Google; stifles competition; and threatens to tarnish Google’s trusted brand.”</p>
<p>Over the past few years, Nguyen, Pham and those working with them, are said to have created at least 65 Google accounts to send confirmed bogus notices targeting 117,000 URLs, plus another 500,000 URLs via notices that Google suspects are fraudulent too.</p>
<p>“Defendants appear to be connected with websites selling printed t-shirts, and their unlawful conduct aims to remove competing third-party sellers from Google Search results. Defendants have maliciously and illegally exploited Google’s policies and procedures under the DMCA to sabotage and harm their competitors,” the complaint adds.</p>
<h2>Google Aims to Put an End to Abuse, Hold Defendants Accountable</h2>
<p>Google goes on to highlight its position as a major intermediary that processes DMCA notices targeting 600 million URLs every year, and the requirement under the DMCA to remove or disable content notified as allegedly infringing. If the company fails to act expeditiously once in receipt of a DMCA notice that complies with the statutory requirements, the company risks losing its safe harbor protection, Google notes.</p>
<p>Since Google must often rely on the accuracy of statements made in DMCA notices, fraudulent notices can result in content being wrongfully taken down. That damages the company’s search engine advertising business, and the business Google’s customers hoped to attract. In this matter, the defendants’ embarked on a campaign that exploited Google’s systems and the DMCA takedown process to undermine their competitors.</p>
<h2>Fake Names, Fraudulent Representations</h2>
<p>The misrepresentations in notices sent to Google were potentially damaging to other parties too. Under fake names, the defendants falsely claimed to represent large companies such as Amazon, Twitter, and NBC News, plus sports teams including the Philadelphia Eagles, Los Angeles Lakers, San Diego Padres. </p>
<p>In similarly false notices, they claimed to represent famous individuals including Elon Musk, Taylor Swift, LeVar Burton, and Kanye West.</p>
<p>The complaint notes that some notices were submitted under company names that do not exist in the United States, at addresses where innocent families and businesses can be found. Google says that despite these claims, the defendants can be found in Vietnam from where they proudly advertise their ‘SEO’ scheme to others, including via YouTube. </p>
<center><a href="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png"><img decoding="async" src="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png" alt="Fake SEO Fake DMCA" width="610" height="514" srcset="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png 610w, https://torrentfreak.com/images/Fake-SEO-Fake-DMCA-300x253.png 300w" sizes="(max-width: 610px) 100vw, 610px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20610%20514'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png 610w, https://torrentfreak.com/images/Fake-SEO-Fake-DMCA-300x253.png 300w" data-lazy-src="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png"></a></center>
<p>“Bad actors like Defendants use this tactic to attack and fraudulently suppress competitors’ websites and products in Google Search results, making consumers more likely to buy the same or similar products from the bad actors or their affiliates,” the complaint continues.</p>
<p>“Such bad actors know that a fraudulent takedown request often has the same effect as a legitimate one; if a takedown request contains all the elements required under Section 512(c)(3)(A), it likely will trigger removal by Google.</p>
<p>“Unfortunately, to ensure compliance with the DMCA and in reliance on the information submitted in Defendants’ takedown requests, Google’s system removed a significant number of thirdparty website URLs targeted by Defendants for a period of time before Google and/or the websites’ owners figured out what was going on and took appropriate steps to reinstate the URLs.”</p>
<p>A particularly damaging batch of fraudulent notices targeted more than 35,000 URLs operated by a Google customer that spends tens of millions of dollars per year on Google search ads. The effect was a significant drop in traffic during the holiday season, revenue losses for the customer and its sellers of $5 million, and a loss to Google of between $2 and $3 million.</p>
<h2>Holding Defendants Accountable</h2>
<p>Those who knowingly make false statements in a DMCA notice can be held liable for damages, costs, and attorneys’ fees. In this matter the defendants’ conduct is said to have caused Google to suffer economic harm due to lost advertising revenue, damage to business relations, and the allocation of significant resources to investigate their wrongdoing. </p>
<p>Google seeks attorneys’ fees and damages under <a href="https://www.law.cornell.edu/uscode/text/17/512">17 U.S.C. §512(f)</a>, in an amount to be determined at trial.</p>
<p>The complaint adds that when the defendants created dozens of Google accounts, each time they entered into enforceable agreements with Google. While Google says it has “performed all its obligations” under those contracts, the actions of the defendants amount to breaches of their contractual obligations to Google and intentional interference in contractual relationships between Google and its advertising customers.</p>
<p>Google says the defendants should be required to pay all general, special, and actual damages that Google “has sustained or will sustain” due to the fraudulent notices.</p>
<p>Google further requests an order to restrain the defendants (and anyone working in concert with them), from submitting any further fraudulent takedown notices and/or creating any Gmail accounts. Google also wants a ban on the defendants using any of its products or services to advertise their websites or products. </p>
<p><em>The complaint is available <a href="https://torrentfreak.com/images/5-23-cv-05824-Google-v-Nguyen-Van-Duc-Pham-Van-Thien-complaint-231113.pdf">here</a> (pdf)</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Constraints on physical computers in holographic spacetimes (112 pts)]]></title>
            <link>https://arxiv.org/abs/2304.09900</link>
            <guid>38261913</guid>
            <pubDate>Tue, 14 Nov 2023 11:24:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2304.09900">https://arxiv.org/abs/2304.09900</a>, See on <a href="https://news.ycombinator.com/item?id=38261913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2304.09900.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>Within the setting of the AdS/CFT correspondence, we ask about the power of computers in the presence of gravity. We show that there are computations on $n$ qubits which cannot be implemented inside of black holes with entropy less than $O(2^n)$. To establish our claim, we argue computations happening inside the black hole must be implementable in a programmable quantum processor, so long as the inputs and description of the unitary to be run are not too large. We then prove a bound on quantum processors which shows many unitaries cannot be implemented inside the black hole, and further show some of these have short descriptions and act on small systems. These unitaries with short descriptions must be computationally forbidden from happening inside the black hole.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Alex May [<a href="https://arxiv.org/show-email/9e66402e/2304.09900">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 19 Apr 2023 18:00:50 UTC (47 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rust Without Crates.io (177 pts)]]></title>
            <link>https://thomask.sdf.org/blog/2023/11/14/rust-without-crates-io.html</link>
            <guid>38261539</guid>
            <pubDate>Tue, 14 Nov 2023 10:34:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thomask.sdf.org/blog/2023/11/14/rust-without-crates-io.html">https://thomask.sdf.org/blog/2023/11/14/rust-without-crates-io.html</a>, See on <a href="https://news.ycombinator.com/item?id=38261539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Rust is a lovely programming language but I’ve never quite come to terms with crates.io, or any other of these language-specific repositories where everyone uploads and downloads code willy-nilly. I have several objections:</p>

<ul>
  <li>If crates.io goes down or access is otherwise disrupted then the Rust community will stop work. It is profoundly unresilient to have a single point of failure like this. Certainly some people will have <a href="https://doc.rust-lang.org/cargo/commands/cargo-vendor.html">vendored their deps</a> and others will have a <a href="https://crates.io/crates/panamax"><code>panamax</code> mirror</a> handy, but for most, Rust as we know it stops if this one particular web service goes down.</li>
  <li>There is no mediation of any kind between when a new library/version is published and when it is consumed. You need only one author in your maybe-hundreds-of-dependencies tree to be hacked, coerced or in a malicious mood for you to have a <em>really</em> bad day.</li>
  <li>Any tampering with crates.io itself (espionage, disgruntlement, national security) could have an incredibly wide blast radius, or a incredibly wide set of targets from which to choose.</li>
  <li>Since crates.io is <em>the</em> source for crates, it is normal for both developers and CI machines to be hitting this web service all the time. Opportunities for mischief are exacerbated when clients are phoning home so frequently.</li>
</ul>

<p>So what’s the alternative? I think we all need to take a step back from the altar of developer velocity and take a deep breath. I don’t want dependencies hot off the press. Ideally I want someone independent of the authors playing a curatorial role.</p>

<p>Now, actually getting some human review of dependency updates is quite a hard thing to do. <a href="https://github.com/crev-dev/cargo-crev"><code>cargo-crev</code></a> has been trying for years to make this happen. I would love if it was the solution but it isn’t yet, and I think it’s a little ambitious. Yes we would like to have super-experienced software developers reviewing all our libraries with cryptographic stamps of approval, but if they’re not available we could be the target of remote shell in a <code>build.rs</code>. Surely there’s a middle ground here?</p>

<p>What’s interesting is that this problem is largely solved for C and C++: Linux distributions like Debian package such a wide range of libraries that for many things that you want to develop or install, you don’t need any third-party libraries <em>at all</em>. It’s just a matter of finding the right <code>apt-get</code> incantations and off you go. Even if you can get 95% of your libraries from a common trusted source then your risk is decreased considerably.</p>

<p>Rust libraries don’t work quite the same as C/C++ ones. Normal Rust code can’t be dynamically linked—a binary will have all of its dependencies statically linked at build time, so you won’t typically see <code>.so</code> files for Rust libraries that are going to be consumed by other Rust code. Since there is no <code>.so</code> file, Debian has no package that installs the library. However if they want to ship a binary that was written in Rust, their builders can’t just be downloading stuff from crates.io. They need a way to package all of the software that represents that Debian release. To solve that problem they’ve taken all these little dependencies and put their <em>full Rust source code</em> in packages with names like <code>librust-cratename-dev</code>.</p>

<p>Hmm, how many such packages? Running on trixie (testing)…</p>

<div><pre><code>$ aptitude search librust- | grep -vE "^v " | wc -l
2336
</code></pre></div>

<p>This is starting to look like a serious curation of the most important Rust crates, available from any Debian mirror. There are some double-ups to be sure, since in some cases multiple incompatible versions of the same crate had to be packaged. Still. Maybe there is enough Rust in Debian now that it’s viable to write interesting Rust software independently of crates.io? That would solve basically all my concerns and the situation is only going to improve as more Rust software gets packaged.</p>

<p>To be clear, I don’t expect that Debian Developers are auditing these packages in the manner of <code>cargo-crev</code>. The good thing is that they <em>don’t actually need to</em> for it to be a major improvement.</p>

<ul>
  <li>A DD isn’t going to upload a new patch release <em>just ‘cause</em>. It’s going to be because it has an important fix or because some other program has depended on it. On crates.io a maintainer is free to create new releases for any reason and <code>cargo update</code> is not going to evaluate how good that reason is.</li>
  <li>A simple time delay will allow egregious malware like malicious <code>build.rs</code> scripts to be caught, whether that’s the super-long Debian stable cycle or even the several days required to migrate from <em>unstable</em> to <em>testing</em>. I assume that an urgent security issue would be distributed the same as any other Debian update.</li>
  <li>They might decide to give the diff at least a cursory look, which is better than nothing.</li>
</ul>

<p>How do we do this? It’s actually quite easy because the big-brained Debian developers have arranged all the Rust dependencies to follow the format of a cargo <a href="https://doc.rust-lang.org/cargo/reference/source-replacement.html#directory-sources">Directory Source</a>. That is, all of the packages are installed in their own directories under <code>/usr/share/cargo/registry</code>, including implementing <a href="https://github.com/rust-lang/cargo/issues/11063">a cheeky workaround</a> for the required <code>.cargo-checksum.json</code> files.</p>

<p>You can then add some brief incantations to your <code>.cargo/config.toml</code>, whether on a project- or user-wide basis:</p>

<div><pre><code><span>[net]</span>
<span>offline</span> <span>=</span> <span>true</span>

<span>[source]</span>

<span>[source.apt]</span>
<span>directory</span> <span>=</span> <span>"/usr/share/cargo/registry"</span>

<span>[source.crates-io]</span>
<span>replace-with</span> <span>=</span> <span>"apt"</span>
</code></pre></div>

<p>This overrides the default crates.io source and ensures dependencies can only be fulfilled locally by installing the relevant packages. This happily doesn’t require any changes to your projects themselves—you just have to be careful to use versions in your <code>Cargo.toml</code> (and <code>Cargo.lock</code>) that are resolvable on Debian, since it is a subset of those available on the wider crates.io.</p>

<p>I am quite certain that Debian wouldn’t have enough coverage yet for the monorepo at work, but I gave this a go on my <a href="https://github.com/thombles/hashgood">one of my little CLI projects</a> that has half a dozen dependencies. Apart from having to downgrade <code>copypasta</code> from 0.8.2 to 0.8.1 in the <code>Cargo.lock</code>, this builds and runs just fine. What a treat.</p>

<p>This little investigation has given me much more confidence in using Rust generally into the future. I feared that the “grab any dependency version you like” approach facilitated by crates.io would render Rust impervious to any sort of curation effort, such that anyone who was serious about my earlier concerns would have to stick to a language used to the old ways like C++. Fortunately, Debian is here to prove me wrong. A+ work by their Rust packaging team.</p>

<p>All power to those who like to live on the edge; I’ll be over here trying to minimise different types of dependencies.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU Parliament Decides That Your Private Messages Must Not Be Scanned (543 pts)]]></title>
            <link>https://tuta.com/blog/chat-control</link>
            <guid>38261415</guid>
            <pubDate>Tue, 14 Nov 2023 10:17:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tuta.com/blog/chat-control">https://tuta.com/blog/chat-control</a>, See on <a href="https://news.ycombinator.com/item?id=38261415">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Chat control - one of the worst EU plans that is also being described as a surveillance monster - must be stopped. And the
EU Parliament has just decided to do so! In a historic agreement on the EU Commission's Child Sexual Abuse Regulation (CSAR) the European
Parliament wants to remove chat control requirements and safeguard secure encryption. The decision came after extensive backlash
against the original proposal from technology and security experts, to international scientists and to citizens across Europe.
This is a great win for our right to privacy and for upholding our democratic values in Europe, but the fight continues!
</p><div><p>Today the EU Parliament decided on an <a href="https://www.patrick-breyer.de/wp-content/uploads/2023/11/CSAR_LIBE-Verhandlungsmandat.pdf">alternative version of chat control</a> - one that fortunately
does not deserve this name anymore: After huge opposition against the surveillance methods included in the CSA Regulation
(see 'Opposition against chat control' below), the EU Parliament has decided to uphold every citizen's right to privacy
and underlined the importance of upholding our democratic values. We in Europe must not follow autocratic regimes like
China and Russia by monitoring all our citizens.</p>
<p>Patrick Breyer, Member of the EU Parliament and part of the CSAR negotiations says:</p>
<blockquote>
<p>"Under the impression of massive protests against the looming indiscriminate chat control mass scanning of private messages, we managed to
win a broad majority for a different, new approach to protecting young people from abuse and exploitation online. As a pirate and digital
freedom fighter, I am proud of this breakthrough. The winners of this agreement are on the one hand our children, who will be protected much
more effectively and in a court-proof manner, and on the other hand all citizens, whose digital privacy of correspondence and communication security will be guaranteed."</p>
</blockquote>
<blockquote>
<p>"<strong>Even if this compromise, which is supported from the progressive to the conservative camp, is not perfect on all points, it is a historic
success that removing chat control and rescuing secure encryption is the common aim of the entire Parliament.</strong> We are doing the exact opposite of
most EU governments who want to destroy digital privacy of correspondence and secure encryption. Governments must finally accept that this highly
dangerous bill can only be fundamentally changed or not be passed at all. The fight against authoritarian chat control must be pursued with all determination!"</p>
</blockquote>
<h3 id="what-did-the-eu-parliament-decide">What did the EU Parliament decide?</h3>
<p>Breyer <a href="https://www.patrick-breyer.de/en/historic-agreement-on-child-sexual-abuse-proposal-csar-european-parliament-wants-to-remove-chat-control-and-safeguard-secure-encryption/">writes</a>
on his website that internet services and apps must be "secure by design and default". The EU Parliament has agreed to:</p>
<blockquote>
<p>"safeguard the digital secrecy of correspondence and remove the plans for blanket chat control, which violate fundamental
rights and stand no chance in court. The current voluntary chat control of private messages (not social networks) by US internet
companies is being phased out. Targeted telecommunication surveillance and searches will only be permitted with a
judicial warrant and only limited to persons or groups of persons suspected of being linked to child sexual abuse material."</p>
</blockquote>
<p><strong>A huge win for our privacy rights is also that the EU Parliament has decided to
"clearly exclude so-called client-side scanning".</strong></p>
<p>In contrast to the original chat control proposal, the version of the EU Parliament wants that a new EU Child Protection Centre
proactively searches publicly accessible parts of the internet for child sexual abuse material with automatic crawling, which can also
take place in darknet and would be much more efficient than private surveillance measures by providers. Found abuse material
must be reported and taken down by the provider.</p>
<h3 id="fight-is-not-over">Fight is not over</h3>
<p>While the EU Parliament's decision is a huge win, the fight is not over. It is expected that the EU Commission will continue to push for general
surveillance chat control measures. Now is the time for each and everyone of us to join this fight!</p>
<p><strong>You can help fight chat control and uphold our right to privacy. Check at the end of this post, what you can do!</strong></p>
<h2 id="opposition-against-chat-control">Opposition against chat control</h2>
<p>Chat control has been in discussion for along time already, and the criticism of this draft bill is huge. Significant is
not only that technology and security experts agree that <a href="https://tuta.com/blog/posts/eu-client-side-scanning">client-side scanning</a> is not possible without risking
everyone's security. Also scientists, the general public, even the EU's Research Service oppose the EU
Commission's chat control proposal.</p>
<h3 id="scientists-letter-to-eu-parliament">Scientists letter to EU Parliament</h3>
<p>300 scientists from all around the world have sent an open letter to the EU Parliament to call on policymakers
to <strong>stop chat control</strong>, the EU’s proposed Child Sexual Abuse Regulation. They say while it is the responsibility of politicians to protect children from sexual abuse,
"it is our professional recommendation as scientists that such a proposal be not taken forward" because the
scanning techniques the EU is proposing to use are deeply flawed and would endanger the security of everyone using the internet.</p>
<p>The scientists make the EU proposal look like wishful thinking: "Given the horrific nature of child sexual abuse, it is understandable,
and indeed tempting, to hope that there is a technological intervention that can eradicate it. Yet,
looking at the issue holistically, we cannot escape the conclusion that the current proposal is not such an intervention."</p>
<p>There is no magic key that allows the police to scan all chat messages, emails, and more for harmful content while not
risking the security and privacy of everyone. This is technically not possible.</p>
<p>The scientists argue that chat control is too much of a threat to everyone and therefore must be stopped:</p>
<blockquote>
<p>"First and foremost, we acknowledge that child sexual abuse and exploitation is a very serious crime which can cause
lifelong harm to survivors. It is the responsibility of government authorities, with the support of companies and communities,
to undertake effective interventions which prevent this crime and react to it quickly when it does happen."</p>
</blockquote>
<blockquote>
<p>"The European Commission has proposed a law with the stated aim of stopping the spread of child sexual abuse material
online and of grooming of children online. To do so, the law allows authorities to compel providers of any apps or other
online services to scan the messages, pictures, emails, voice mails and other activities of their users. In the case of end-to-end
encrypted apps, the claim is that this scanning can be done on users’ devices – so-called ‘Client-Side Scanning’ (CSS)."</p>
</blockquote>
<blockquote>
<p><strong>"Passing this legislation undermines the thoughtful and incisive work that European researchers have provided in cybersecurity
and privacy, including contributions to the development of global encryption standards. Such undermining will weaken the environment
for security and privacy work in Europe, lowering our ability to build a secure digital society."</strong></p>
</blockquote>
<blockquote>
<p>"The proposed regulation would also set a global precedent for filtering the Internet, controlling who can access it, and taking
away some of the few tools available for people to protect their right to a private life in the digital space. This will have a
chilling effect on society and is likely to negatively affect democracies across the globe."</p>
</blockquote>
<blockquote>
<p>"We therefore strongly warn against pursuing these or similar measures as their success is not possible given current
and foreseeable technology, while their potential for harm is substantial."</p>
</blockquote>
<p>You can read the full open letter <a href="https://docs.google.com/document/d/13Aeex72MtFBjKhExRTooVMWN9TC-pbH-5LEaAbMF91Y/mobilebasic">here</a>.</p>
<h3 id="eus-research-service-opposes-chat-control">EU's Research Service opposes chat control</h3>
<p>In April, the European Parliament's Research Service (EPRS) presented a new study on the legality of the proposed Child Sexual Abuse
Regulation, also called Chat Control.</p>
<p>The EU Commission's plans to fight images of abused children on the Internet are not very effective and violate the fundamental rights of Internet users,
according to this analysis on chat control. While the number of reported cases is likely to go up significantly, the accuracy of the hits is likely to also
decrease significantly, increasing the burden on investigative authorities.</p>
<h3 id="consequences-of-draft-eu-law">Consequences of draft EU law</h3>
<p>The legal experts of the EU Parliament's Scientific Service conclude that:</p>
<blockquote>
<p>"when weighing the fundamental rights affected by the measures of the CSA proposal, it can be established that the <strong>CSA
proposal would violate Articles 7 and 8 of the Charter of Fundamental Rights with regard to users.</strong>"</p>
</blockquote>
<p>The report also says if chat control becomes a law "that this violation of the
prohibition of <strong>general data retention</strong> and the prohibition of <strong>general surveillance obligations</strong> cannot be justified."</p>
<blockquote>
<p>"A detection order on the content of interpersonal data either on the device or the server will <strong>compromise the essence of
the right to privacy</strong> under Article 7 CFR in the form of confidentiality of telecommunications. It constitutes a form of
access on a generalised basis, pursuant to Schrems, where it involves an <strong>analysis of all communications</strong> going through the server.“</p>
</blockquote>
<p>The experts made clear that an "increase in the number of reported contents does not necessarily lead to a corresponding increase in investigations and prosecutions leading to better protection of children.
As long as the capacity of law enforcement agencies is limited to its current size, an increase in reports will make effective prosecution of depictions of abuse more difficult."</p>
<p>In addition, the study on chat control finds: "It is undisputed that children need to be protected from becoming victims of child abuse and depictions of abuse online... but they also need to be able
to enjoy the protection of fundamental rights as a basis for their development and transition into adulthood."</p>
<p>Pirate Party MEP Patrick Breyer, long-time opponent of mass scanning of private communications, comments:</p>
<p>"The EU Parliament's Scientific Service now confirms in crystal clear words what I and numerous human rights activists, law enforcement
officials, legal experts, abuse victims and child protection organisations have been warning about for a long time: the proposed general,
indiscriminate <strong><a href="https://tuta.com/blog/posts/eu-client-side-scanning">scanning of our private conversations and photos</a></strong> destroys the digital privacy of correspondence and violates our fundamental
rights. A flood of mostly false suspicious activity reports would make effective investigations more difficult, criminalise children en masse
and fail to bring the abusers and producers of such material to justice. According to this expertise, searching private communications for
potential child sexual exploitation material, known or unknown, is legally feasible only if the search provisions are targeted and limited
to persons presumably involved in such criminal activity."</p>
<p><strong>"What we really need instead of untargeted chat control and identification obligations for age verification is obliging law enforcement
agencies to have known exploitation material removed from the internet, as well as Europe-wide standards for effective prevention measures,
victim support and counselling, and for effective criminal investigations."</strong></p>
<p>This is also the view of many other experts, such as Mullvad, Edri and others.</p>
<h2 id="stop-chat-control">Stop chat control</h2>
<h3 id="mullvad-really-nails-it-with-their-campaign">Mullvad really nails it with their campaign!</h3>
<p><strong>Chat control is one of the worst EU plans to date and must be stopped. Mullvad VPN has recently launched a great campaign to fight for democracy.</strong></p>
<p>Mullvad's campaign, launched on March 3rd, calls on EU policy makers to stop chat control and rethink their stance in regards to the EU
Commission's proposal for detecting and prosecuting the sharing of child sexual abuse material (CSAM) via the internet.
The EU proposal includes far-reaching surveillance measures such as client-side scanning, which would force online services
to scan every chat message and every email that anybody in the European Union ever sends for child sexual abuse material.</p>
<p><strong>This legislation would de facto deprive EU citizens of any privacy on the Internet, it would even undermine encryption
and thus weaken the security of all Internet users.</strong></p>
<p>For that reason, the <a href="https://tuta.com/blog/posts/eu-csam-scanning">EU plans to scan for CSAM is heavily criticized</a>
by cryptography experts, human rights organizations as well as internet activists across Europe.</p>
<p>Most recently, Germany has made its opposition to <a href="https://tuta.com/blog/posts/eu-client-side-scanning">client-side scanning public</a>.
With resistance in Germany, Ireland, Austria and the Netherlands to the EU proposal,
a blocking minority is within reach.</p>
<h3 id="perfect-timing">Perfect timing</h3>
<p>Mullvad adds to the pressure with their new campaign, which was launched during the Swedish EU Presidency, which started on 1st of January 2023. The timing, thus,
couldn't be better.</p>
<p>Mullvad says on their campaign page:</p>
<blockquote>
<p><strong>Now is the time for debate and actions</strong></p>
</blockquote>
<blockquote>
<p>A democratic society is built upon discussions, before law proposals become reality. We started the conversation on the streets of Sweden, during the country’s EU presidency.</p>
</blockquote>
<p>Along with the digital campaign, they posted large billboards across Sweden to draw attention to the ongoing legal
debate on EU level.</p>
<blockquote><p dir="ltr" lang="en">The EU Commission wants to monitor all the citizens of the European union. The law proposal is called <a href="https://twitter.com/hashtag/chatcontrol?src=hash&amp;ref_src=twsrc%5Etfw">#chatcontrol</a> – and now is the time to stop it. We took the debate to the streets of Sweden, during the country’s EU-presidency. Take a look at <a href="https://t.co/Dx9cPe1ksq">https://t.co/Dx9cPe1ksq</a> <a href="https://t.co/FvqAlQRiig">pic.twitter.com/FvqAlQRiig</a></p>— Mullvad.net (@mullvadnet) <a href="https://twitter.com/mullvadnet/status/1631639744537870336?ref_src=twsrc%5Etfw">March 3, 2023</a></blockquote> 

<h3 id="opposition-to-chat-control">Opposition to chat control</h3>
<p>The digital rights organization EDRi has recently launched the 'Stop Scanning Me' campaign where EU citizens
can sign a petition against the EU's surveillance plan.</p>
<p><strong>Sign the Stop Scanning Me campaign <a href="https://civicrm.edri.org/stop-scanning-me">now</a>!</strong></p>
<h3 id="what-is-chat-control">What is chat control?</h3>
<p>The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2022%3A209%3AFIN">Eu proposal</a> on chat control wants to force
online services to AI scan every message and every email for possible child grooming and child sexual abuse material (known and unknown).
Suspicious messages flagged by the AI will be reported to law enforcement and investigated.</p>
<p>Machine searching for potential child grooming and sexual abuse material is an artificial intelligence (AI) supported procedure.
The AI is not flawless and will flag a high number of harmless, private images, which will then be investigated by the police.
Experts expect that 10-20% of images reported will be false positives.</p>
<p>This is a huge intrusion into the privacy of millions of innocent citizens.</p>
<p><strong>The European Date Protection Supervisor Wiewiórowski <a href="https://www.euractiv.com/section/law-enforcement/news/eu-watchdog-online-child-abuse-draft-law-creates-illusion-of-legality/">calls it</a>
an 'illusion of legality': This type of indiscriminate scanning of private communications "will always be illegal under the Charter of
Fundamental Rights (and probably under several national constitutional laws as well)."</strong></p>
<h3 id="the-risks-of-chat-control">The risks of chat control</h3>
<p>To many the risks of chat control are negligible. After all, as law-abiding citizens what is there to fear?</p>
<p>But the truth is the opposite: The risks of a surveillance tool like chat control are unlimited.</p>
<h4 id="1-you-dont-know-whether-the-laws-will-change">1. You don't know whether the laws will change.</h4>
<p>Jan Penfrat said it perfectly on <a href="https://eupolicy.social/@ilumium/109972484325693478">Mastodon</a>:</p>
<p>"You have nothing to hide until the government suddenly declares your behaviour illegal."</p>
<p>
		<picture>
   			<source type="image/webp" srcset="https://tuta.com/blog/images/abortion-illegal-chatcontrol.webp">
    		<img height="854" width="1440" loading="lazy" alt="Chat control becomes very dangerous as soon as your behaviour is declared illegal. It must be stopped." src="https://tuta.com/blog/images/abortion-illegal-chatcontrol.jpg">
		</picture></p>
<p>The text on the image he posted is taken from news that broke this week via the
<a href="https://www.businessinsider.com/police-getting-help-social-media-to-prosecute-people-seeking-abortions-2023-2">Business Insider</a>:
"Police are prosecuting abortion seekers using their digital data — and Facebook and Google help them do it".</p>
<h4 id="2-compromised-encryption-is-not-encryption">2. Compromised encryption is not encryption</h4>
<p>Once you break encryption to allow access to the 'good guys', the security and privacy promised by encryption is gone.</p>
<p>It is simply not possible to implement an <a href="https://tuta.com/blog/posts/why-a-backdoor-is-a-security-risk">encryption backdoor</a> that can only be used by law enforcement.</p>
<p>This is also nicely illustrated by the <a href="https://tuta.com/blog/posts/encryption-backdoor-fails">best of backdoor fails in history</a>. The truth is: Secret services
have tried to undermine encryption before, but whenever they were successful, others were too. Malicious intruders have become
very powerful.</p>
<p><strong>We in Europe must not weaken the security backbone that our digital life depends on: Encryption.</strong></p>
<h3 id="lets-stop-client-side-scanning">Let's stop client-side scanning</h3>
<p>Now we, as citizens of Europe and members of the civil society, must put pressure on legislators to oppose legislation that
will put every email and every chat message that we send under constant surveillance.</p>
<p><strong>We can stop chat control together!</strong></p>
<ol>
<li><p>Share the <a href="https://mullvad.net/en/chatcontrol/campaign">Mullvad campaign</a> to increase the pressure on politicians.</p>
</li>
<li><p>Call/email your EU representative to make your voice heard: "Stop CSAM scanning. I do not want my personal device to become a surveillance machine!"</p>
</li>
<li><p>Sign the <a href="https://civicrm.edri.org/stop-scanning-me">Stop Scanning Me campaign</a>.</p>
</li>
</ol>
<p>Together we can stop chat control!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Truth Social reports $73M net loss since launch (101 pts)]]></title>
            <link>https://www.reuters.com/technology/trumps-truth-social-reports-73-mln-net-loss-since-launch-2023-11-14/</link>
            <guid>38260815</guid>
            <pubDate>Tue, 14 Nov 2023 09:04:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/trumps-truth-social-reports-73-mln-net-loss-since-launch-2023-11-14/">https://www.reuters.com/technology/trumps-truth-social-reports-73-mln-net-loss-since-launch-2023-11-14/</a>, See on <a href="https://news.ycombinator.com/item?id=38260815">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="primary-image" role="figure" aria-describedby="primary-image-caption"><figure><div data-testid="Image"><p><img src="https://cloudfront-us-east-2.images.arcpublishing.com/reuters/CO4F5ULZPBO6BJHFJNRA4I3FM4.jpg" srcset="https://www.reuters.com/resizer/qEv1IhEtEHGku_Gr8HYX_-O0ddw=/480x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/CO4F5ULZPBO6BJHFJNRA4I3FM4.jpg 480w,https://www.reuters.com/resizer/IZhZGJ1aHfJy2znLmv1U2TUT8uw=/960x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/CO4F5ULZPBO6BJHFJNRA4I3FM4.jpg 960w,https://www.reuters.com/resizer/zQRquT-heMgwKu_08BObVyBVHHU=/1080x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/CO4F5ULZPBO6BJHFJNRA4I3FM4.jpg 1080w,https://www.reuters.com/resizer/LqdJ5x5C2cTWYNI4Fh6bIzZEbqc=/1200x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/CO4F5ULZPBO6BJHFJNRA4I3FM4.jpg 1200w" sizes="(min-width: 1024px) 560px, (min-width: 1440px) 700px, 100vw" width="7485" height="4992" alt="Trump Organization trial in New York State Supreme Court in New York"></p></div><p data-testid="Body"><span>Former U.S. President Donald Trump attends the Trump Organization civil fraud trial, in New York State Supreme Court in the Manhattan borough of New York City, U.S., October 25, 2023. Dave Sanders/Pool via REUTERS//File Photo <a data-testid="Link" href="https://www.reutersagency.com/en/licensereuterscontent/?utm_medium=rcom-article-media&amp;utm_campaign=rcom-rcp-lead" target="_blank" referrerpolicy="no-referrer-when-downgrade"> Acquire Licensing Rights</a></span></p></figure></div><div><p data-testid="paragraph-0">Nov 13 (Reuters) - Former U.S. President Donald Trump's social media platform Truth Social has lost $73 million since its launch in early 2022, a securities filing by Digital World Acquisition Corp <a data-testid="Link" href="https://www.reuters.com/markets/companies/DWAC.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(DWAC.O)</a>, the SPAC that plans to merge with the company, showed on Monday.</p><p data-testid="paragraph-1">Trump had announced the launch of his social media app in Oct 2021, <a data-testid="Link" href="https://www.reuters.com/world/us/former-us-president-donald-trump-launches-new-social-media-platform-2021-10-21/" referrerpolicy="no-referrer-when-downgrade">saying</a> it would "stand up to Big Tech" companies such as Twitter and Facebook that previously barred him.</p><p data-testid="paragraph-2">In 2022, Truth Social posted a loss of $50 million, with net sales of just $1.4 million. It lost $23 million in the first half of this year, with net sales of $2.3 million.</p><p data-testid="paragraph-3">Trump Media &amp; Technology Group's (TMTG) independent registered public accounting firm has indicated that the financial condition raises substantial doubt as to its ability to continue as a going concern, according to the filing.</p><p data-testid="paragraph-4">"TMTG believes that it may be difficult to raise additional funds through traditional financing sources in the absence of material progress toward completing its merger with Digital World."</p><p data-testid="paragraph-5">The company also eliminated several positions in March, the filing said, adding that the action followed a review of all departments, most significantly impacting TMTG's streaming video on demand and infrastructure teams.</p><p data-testid="Body">Reporting by Maria Ponnezhath in Bengaluru; Editing by Nivedita Bhattacharjee</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank" referrerpolicy="no-referrer-when-downgrade">The Thomson Reuters Trust Principles.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Android App Devs now require 20 people to test before publishing to Play Store (274 pts)]]></title>
            <link>https://techcrunch.com/2023/11/09/google-play-tightens-up-rules-for-android-app-developers-to-require-testing-increased-app-review/</link>
            <guid>38258101</guid>
            <pubDate>Tue, 14 Nov 2023 02:13:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/11/09/google-play-tightens-up-rules-for-android-app-developers-to-require-testing-increased-app-review/">https://techcrunch.com/2023/11/09/google-play-tightens-up-rules-for-android-app-developers-to-require-testing-increased-app-review/</a>, See on <a href="https://news.ycombinator.com/item?id=38258101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Google today is <a href="http://android-developers.googleblog.com/2023/11/ensuring-high-quality-apps-on-google-play.html">announcing</a> strengthened protections for Android developers publishing apps to its Google Play store. The changes are a part of Google’s broader efforts at keeping low-quality and unsafe apps out of its app store and off consumers’ devices, which also recently included the launch of a new <a href="https://techcrunch.com/2023/11/04/google-play-android-real-time-app-scanning-sideload-apps/">real-time app scanning feature to combat malicious apps.</a> Today, the company says it will now require new Android developers with personal accounts to test their app with a minimum of 20 people for at least 2 weeks prior to publication. It additionally plans to increase its investment in the app review processes, warning of potential slowdowns in approvals for a small number of apps as these changes roll out.</p>
<p>According to Google, developers that use its testing tools have, on average, 3 times the amount of app installs and user engagement. That, of course, may not be a factor fully attributable to Google’s tools, but to the developers who would utilize such app testing tools before hitting publish. That is, they’re likely developing higher-quality applications. But now, app testing will no longer be optional for developers with newly created Play Console accounts, says Google.</p>
<p>Without providing an exact timeframe, Google says that new developers with individual accounts (as opposed to new Organization accounts) will be required to test apps with 20 people or more for 2 weeks or longer before publishing to production. The company believes this will help developers identify issues and bugs, and gain user feedback before their app’s launch. It says the requirement will arrive in the Play Console in the “coming days.”</p>
<p>Related to this, Google also plans to invest more heavily in its app review process, which, anecdotally, has long been considered to be less stringent than Apple’s with more reliance on automation over human review. Today, Google says its review teams will begin to spend more time assessing new apps to ensure policy compliance and that they don’t defraud users, including within the app or outside the Play Store.</p>
<p>This particular change follows an issue that’s impacted both app stores in India, specifically, where predatory lending apps have targeted financially insecure consumers, and then used unethical tactics to pressure borrowers to pay back debts. Apple <a href="https://techcrunch.com/2023/07/07/apple-purges-predatory-lending-apps-in-india-following-scrutiny/">this summer also had to sweep its App Store of these apps,</a> but Android is more popular in India, which means the issue more <a href="https://techcrunch.com/2022/08/26/loan-apps-abuse-harassment-suicide-indian-users-google-apple-india/">heavily impacts the Play Store.</a></p>
<p>However, Google is also taking aim at apps that ask for elevated permissions with the<a href="https://techcrunch.com/2023/03/08/googles-new-developer-preview-release-of-android-14-focuses-on-privacy-and-security/"> launch of Android 14</a>. With this release, developers can use more granular permission flow options, like asking only to access select photos or videos instead of the entirety of a user’s photo gallery.</p>
<p>As a result of the app review changes, Google warns it may take longer to review “a small portion of apps,” including those that require “certain device permissions” or those aimed at children.</p>
<p>The company announced a few other updates today, as well, including the ability for developers to choose their preferred deadline for meeting <a href="https://android-developers.googleblog.com/2023/07/boosting-trust-and-transparency-in-google-play.html">stricter verification requirements</a> associated with publishing on Google Play. Developers who don’t choose a timeframe before Feb. 29, 2024 will have their deadline set for them, Google says.</p>
<p>It also noted that, in addition to providing users with more information about which apps work well on their devices, and other efforts to highlight local and regional content, Google will add a badge identifying official government apps starting in 2024.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Starship will attempt a launch this weekend (660 pts)]]></title>
            <link>https://www.fly.faa.gov/adv/adv_spt.jsp</link>
            <guid>38257794</guid>
            <pubDate>Tue, 14 Nov 2023 01:32:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fly.faa.gov/adv/adv_spt.jsp">https://www.fly.faa.gov/adv/adv_spt.jsp</a>, See on <a href="https://news.ycombinator.com/item?id=38257794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
		<td><p>MESSAGE:&nbsp;</p></td>
		
		<td><pre>EVENT TIME: 14/0100 - AND LATER
___________________________________________________________________________


THERE WILL BE A FLIGHT EVALUATION EVENT IN AND AROUND N90 AIRSPACE IN THE
MORNING BETWEEN 1130Z - 1230Z WITH MINIMAL IMPACT EXPECTED. PLANNED
TERMINAL INITIATIVES CARRIED IN THE PLAN FOR EWR AND MSP DUE TO WIND, SOUTH
FLORIDA FOR THUNDERSTORMS, AND SFO FOR LOW CEILINGS AND VISIBILITY ALONG
WITH VIP TFR CONSTRAINTS. GULF ROUTE CLOSURES AND SWAP ACTIVITY FOR SOUTH
FLORIDA POSSIBLE DUE TO THUNDERSTORMS.
___________________________________________________________________________

STAFFING TRIGGER(S):
NONE

TERMINAL CONSTRAINT(S):
N90/MSP - WIND
SOUTH FLORIDA - CHC THUNDERSTORMS
SFO - LOW CEILINGS / LOW VISIBILITY
I90 - LOW CEILINGS
SFO/OAK - ASIA-PACIFIC ECONOMIC COOPERATION 2023 UNTIL 11/18/23
L30 - HIGH VOLUME OPERATIONS/LSV AIRSHOW THRU 11/21/23

TERMINAL ACTIVE: 
NONE

TERMINAL PLANNED:
AFTER 1500	-MIA/FLL/PBI GROUND STOP POSSIBLE
AFTER 1545	-SFO GROUND STOP/DELAY PROGRAM PROBABLE
AFTER 1730	-EWR GROUND STOP/DELAY PROGRAM POSSIBLE
AFTER 2200	-MSP GROUND STOP/DELAY PROGRAM POSSIBLE

ENROUTE CONSTRAINT(S): 
THUNDERSTORMS - ZHU/ZJX/ZMA
ZMA - CAPE A STARFIGHTER ATCAA SFC-FL360 1530Z-1630Z  /  1800Z-1900Z
ZMP - QWA - WATFORD CITY, ND ATCRB OTS 1600Z-2000Z

ENROUTE ACTIVE:
UNTIL 0200	-FCA001:N90_PREF-ROUTES

ENROUTE PLANNED: 
AFTER 1000	-N90 PREF ROUTES EXPECTED
AFTER 1300	-GULF ROUTE CLOSURES POSSIBLE
AFTER 1500	-MIA SWAP/ESCAPE ROUTES POSSIBLE

CDR/SWAP:
NONE

RUNWAY/EQUIPMENT/SYSTEM IMPACT REPORTS (SIRs):
TEB - RWY 01/19 CLOSED 11/14/23 1230Z-1700Z
MEM - RWY 18L/36R CLOSED 14/1300Z-14/2030Z
SDL - RWY 03/21 CLOSED NIGHTLY 0400-1300Z 11/13/23-11/17/23 
RDU - RWY 05L/23R CLOSED DAILY 0200-1030Z 11/13/23-11/18/23
TEB - RWY 01/19 CLOSED 11/14/23 1230Z-1700Z
IAH - RWY 15L/33R CLOSED UNTIL 11/18/23 1200Z
MIA - RWY 09/27 CLOSED 0300Z-1200Z NIGHTLY UNTIL 11/20/23 
BOS - RWY 15R/33L CLOSED UNTIL 11/25/23 2359Z
ORD - RWY 09C/27C CONSTRUCTION ACTIVITIES UNTIL 12/15/23 
L X - RWY 06L/24R CLOSED UNTIL 01/09/24 0830Z
PBI - RWY 14/32 CLOSED UNTIL 01/16/24 2359Z	
DFW - RWY 17R/35L CLOSED UNTIL 05/31/24 1200Z

AIRSPACE FLOW PROGRAM(S) ACTIVE:
NONE

AIRSPACE FLOW PROGRAM(S) PLANNED:
NONE

LAUNCH/REENTRY:
SPACE X - STARLINK 6-28 CAPE CANAVERAL SFS, FL
PRIMARY: 	11/17/23	0400Z-0831Z
BACKUP:		11/18/23	0400Z-0831Z
		11/19/23	0400Z-0831Z

SPACE X - STARLINK 7-7 VANDENBERG SFB, CA
PRIMARY:	11/17/23	0738Z-1204Z
BACKUP:		11/18/23	0716Z-1142Z
		11/19/23	0655Z-1121Z

SPACE X STARSHIP SUPER HEAVY FLT 2  BOCA CHICA, TX
PRIMARY:	11/17/23	1300Z-1720Z
BACKUP:		11/18/23	1300Z-1720Z
		11/19/23	1300Z-1720Z

FLIGHT CHECK(S):
AFTER 1130	-N90
AFTER 1400	-IND

VIP MOVEMENT(S):
AFTER 1500	-DEP ADW
AFTER 2100	-ARR SFO

AFTER 1600	-DEP ADW
AFTER 2100	-ARR SFO

NEXT PLANNING WEBINAR: 1215Z
140020-141059
23/11/14 00:20  DCCOPS.lxstn35&nbsp;</pre></td>
	 </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The laptop that won't die (178 pts)]]></title>
            <link>https://clivethompson.medium.com/the-laptop-that-wont-die-0c478c3fe46c</link>
            <guid>38257284</guid>
            <pubDate>Tue, 14 Nov 2023 00:33:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clivethompson.medium.com/the-laptop-that-wont-die-0c478c3fe46c">https://clivethompson.medium.com/the-laptop-that-wont-die-0c478c3fe46c</a>, See on <a href="https://news.ycombinator.com/item?id=38257284">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2 id="0097">My $200, 12-year-old Thinkpad has outlasted two high-end Macbooks</h2><div><a rel="noopener follow" href="https://clivethompson.medium.com/?source=post_page-----0c478c3fe46c--------------------------------"><div aria-hidden="false"><p><img alt="Clive Thompson" src="https://miro.medium.com/v2/resize:fill:88:88/1*C6KlQUX7cSZiV7VlS12Vyw.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div></div><figure><figcaption>My 2011 T420 Thinkpad</figcaption></figure><p id="9cfe">That laptop above?</p><p id="3253">It’s the most indestructible, nonstop, won’t-die computer I’ve ever owned.</p><p id="838c">Full. Stop.</p><p id="d4a9">I’m going to write a whole damn post about it but the tl;dr is …</p><p id="d85d">If you want a modern, sexy, lightweight, high-powered laptop? Go get something pricey from Apple or Microsoft.</p><p id="5dd6">But if you want something that’ll cost almost no money and keep working until the sun explodes?</p><p id="6ff0">Get an old, used Thinkpad.</p><p id="9f6c">Allow me to unpack this …</p><p id="d673">My <em>hardware tale </em>begins a week ago when I was working on my Macbook Pro.</p><p id="2a90">It’s my main laptop, which I bought in 2017. I needed a machine that a) could run Logic Pro (the finest music-editing software available to humanity), b) had a high-resolution screen for my lousy eyesight, and c) had a 1-terabyte hard drive. It was the only machine that fit the bill.</p><p id="5381">It was super expensive, but my goal with laptops is to buy something with sufficiently excellent build-quality that it’ll last for years. I also hate e-waste, so I try to fix my laptops to keep them going as long as I can. I bought my first-ever Macbook Pro in 2010, and I got seven years out of it — including replacing a fried motherboard (thankfully just before the three-year warranty ended, so: It was covered! Woo)</p><p id="de81">This newer, 2017 Macbook Pro? I’d also had it fixed a few times before. It had gotten water damage in an accident, which required some internal work. I’d had <a rel="noopener" href="https://clivethompson.medium.com/ive-typed-22-million-keystrokes-on-apple-s-horrid-butterfly-keyboard-7d441dfadc15">the loathsomely awful “butterfly” keyboard replaced when it died</a>. I’d replaced the battery twice.</p><p id="1a72">But after six years, it was still chugging along!</p><p id="08bd">Until last week, when out of nowhere it went kaput.</p><p id="47a7">I was working on the Macbook, and closed the lid to have lunch. When I opened it again 15 minutes later, the machine had shut down. Nothing I did could coax it back to life.</p><p id="c229">So I jumped on my bike and brought it to a local laptop repair place. A few hours later, the technician texted me to explain what had gone kablooey. Apparently there was an electrical…</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A coder considers the waning days of the craft (650 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2023/11/20/a-coder-considers-the-waning-days-of-the-craft</link>
            <guid>38257094</guid>
            <pubDate>Tue, 14 Nov 2023 00:08:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2023/11/20/a-coder-considers-the-waning-days-of-the-craft">https://www.newyorker.com/magazine/2023/11/20/a-coder-considers-the-waning-days-of-the-craft</a>, See on <a href="https://news.ycombinator.com/item?id=38257094">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>I have always taken it for granted that, just as my parents made sure that I could read and write, I would make sure that my kids could program computers. It is among the newer arts but also among the most essential, and ever more so by the day, encompassing everything from filmmaking to physics. Fluency with code would round out my children’s literacy—and keep them employable. But as I write this my wife is pregnant with our first child, due in about three weeks. I code professionally, but, by the time that child can type, coding as a valuable skill might have faded from the world.</p><p>I first began to believe this on a Friday morning this past summer, while working on a small hobby project. A few months back, my friend Ben and I had resolved to create a <em>Times</em>-style crossword puzzle entirely by computer. In 2018, we’d made a Saturday puzzle with the help of software and were surprised by how little we contributed—just applying our taste here and there. Now we would attempt to build a crossword-making program that didn’t require a human touch.</p><p>When we’ve taken on projects like this in the past, they’ve had both a hardware component and a software component, with Ben’s strengths running toward the former. We once made a neon sign that would glow when the subway was approaching the stop near our apartments. Ben bent the glass and wired up the transformer’s circuit board. I wrote code to process the transit data. Ben has some professional coding experience of his own, but it was brief, shallow, and now about twenty years out of date; the serious coding was left to me. For the new crossword project, though, Ben had introduced a third party. He’d signed up for a ChatGPT Plus subscription and was using GPT-4 as a coding assistant.</p><p>Something strange started happening. Ben and I would talk about a bit of software we wanted for the project. Then, a shockingly short time later, Ben would deliver it himself. At one point, we wanted a command that would print a hundred random lines from a dictionary file. I thought about the problem for a few minutes, and, when thinking failed, tried Googling. I made some false starts using what I could gather, and while I did my thing—programming—Ben told GPT-4 what he wanted and got code that ran perfectly.</p><p>Fine: commands like those are notoriously fussy, and everybody looks them up anyway. It’s not real programming. A few days later, Ben talked about how it would be nice to have an iPhone app to rate words from the dictionary. But he had no idea what a pain it is to make an iPhone app. I’d tried a few times and never got beyond something that half worked. I found Apple’s programming environment forbidding. You had to learn not just a new language but a new program for editing and running code; you had to learn a zoo of “U.I. components” and all the complicated ways of stitching them together; and, finally, you had to figure out how to package the app. The mountain of new things to learn never seemed worth it. The next morning, I woke up to an app in my in-box that did exactly what Ben had said he wanted. It worked perfectly, and even had a cute design. Ben said that he’d made it in a few hours. GPT-4 had done most of the heavy lifting.</p><p>By now, most people have had experiences with A.I. Not everyone has been impressed. Ben recently said, “I didn’t start really respecting it until I started having it write code for me.” I suspect that non-programmers who are skeptical by nature, and who have seen ChatGPT turn out wooden prose or bogus facts, are still underestimating what’s happening.</p><p>Bodies of knowledge and skills that have traditionally taken lifetimes to master are being swallowed at a gulp. Coding has always felt to me like an endlessly deep and rich domain. Now I find myself wanting to write a eulogy for it. I keep thinking of Lee Sedol. Sedol was one of the world’s best Go players, and a national hero in South Korea, but is now best known for losing, in 2016, to a computer program called AlphaGo. Sedol had walked into the competition believing that he would easily defeat the A.I. By the end of the days-long match, he was proud of having eked out a single game. As it became clear that he was going to lose, Sedol said, in a press conference, “I want to apologize for being so powerless.” He retired three years later. Sedol seemed weighed down by a question that has started to feel familiar, and urgent: What will become of this thing I’ve given so much of my life to?</p><p>My first enchantment with computers came when I was about six years old, in Montreal in the early nineties, playing Mortal Kombat with my oldest brother. He told me about some “fatalities”—gruesome, witty ways of killing your opponent. Neither of us knew how to inflict them. He dialled up an FTP server (where files were stored) in an MS-DOS terminal and typed obscure commands. Soon, he had printed out a page of codes—instructions for every fatality in the game. We went back to the basement and exploded each other’s heads.</p><p>I thought that my brother was a hacker. Like many programmers, I dreamed of breaking into and controlling remote systems. The point wasn’t to cause mayhem—it was to find hidden places and learn hidden things. “My crime is that of curiosity,” goes “The Hacker’s Manifesto,” written in 1986 by Loyd Blankenship. My favorite scene from the 1995 movie “Hackers” is when Dade Murphy, a newcomer, proves himself at an underground club. Someone starts pulling a rainbow of computer books out of a backpack, and Dade recognizes each one from the cover: the green book on international Unix environments; the red one on N.S.A.-trusted networks; the one with the pink-shirted guy on I.B.M. PCs. Dade puts his expertise to use when he turns on the sprinkler system at school, and helps right the ballast of an oil tanker—all by tap-tapping away at a keyboard. The lesson was that knowledge is power.</p><p>But how do you actually learn to hack? My family had settled in New Jersey by the time I was in fifth grade, and when I was in high school I went to the Borders bookstore in the Short Hills mall and bought “Beginning Visual C++,” by Ivor Horton. It ran to twelve hundred pages—my first grimoire. Like many tutorials, it was easy at first and then, suddenly, it wasn’t. Medieval students called the moment at which casual learners fail the <em>pons asinorum</em>, or “bridge of asses.” The term was inspired by Proposition 5 of Euclid’s Elements I, the first truly difficult idea in the book. Those who crossed the bridge would go on to master geometry; those who didn’t would remain dabblers. Section 4.3 of “Beginning Visual C++,” on “Dynamic Memory Allocation,” was my bridge of asses. I did not cross.</p><p>But neither did I drop the subject. I remember the moment things began to turn. I was on a long-haul flight, and I’d brought along a boxy black laptop and a CD-<em>ROM</em> with the Borland C++ compiler. A compiler translates code you write into code that the machine can run; I had been struggling for days to get this one to work. By convention, every coder’s first program does nothing but generate the words “Hello, world.” When I tried to run my version, I just got angry error messages. Whenever I fixed one problem, another cropped up. I had read the “Harry Potter” books and felt as if I were in possession of a broom but had not yet learned the incantation to make it fly. Knowing what might be possible if I did, I kept at it with single-minded devotion. What I learned was that programming is not really about knowledge or skill but simply about patience, or maybe obsession. Programmers are people who can endure an endless parade of tedious obstacles. Imagine explaining to a simpleton how to assemble furniture over the phone, with no pictures, in a language you barely speak. Imagine, too, that the only response you ever get is that you’ve suggested an absurdity and the whole thing has gone awry. All the sweeter, then, when you manage to get something assembled. I have a distinct memory of lying on my stomach in the airplane aisle, and then hitting Enter one last time. I sat up. The computer, for once, had done what I’d told it to do. The words “Hello, world” appeared above my cursor, now in the computer’s own voice. It seemed as if an intelligence had woken up and introduced itself to me.</p><p>Most of us never became the kind of hackers depicted in “Hackers.” To “hack,” in the parlance of a programmer, is just to tinker—to express ingenuity through code. I never formally studied programming; I just kept messing around, making computers do helpful or delightful little things. In my freshman year of college, I knew that I’d be on the road during the third round of the 2006 Masters Tournament, when Tiger Woods was moving up the field, and I wanted to know what was happening in real time. So I made a program that scraped the leaderboard on pgatour.com and sent me a text message anytime he birdied or bogeyed. Later, after reading “Ulysses” in an English class, I wrote a program that pulled random sentences from the book, counted their syllables, and assembled haikus—a more primitive regurgitation of language than you’d get from a chatbot these days, but nonetheless capable, I thought, of real poetry:</p><blockquote><p>I’ll flay him alive<br>Uncertainly he waited<br>Heavy of the past</p></blockquote></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>I began taking coding seriously. I offered to do programming for a friend’s startup. The world of computing, I came to learn, is vast but organized almost geologically, as if deposited in layers. From the Web browser down to the transistor, each sub-area or system is built atop some other, older sub-area or system, the layers dense but legible. The more one digs, the more one develops what the race-car driver Jackie Stewart called “mechanical sympathy,” a sense for the machine’s strengths and limits, of what one could make it do.</p><p>At my friend’s company, I felt my mechanical sympathy developing. In my sophomore year, I was watching “Jeopardy!” with a friend when he suggested that I make a playable version of the show. I thought about it for a few hours before deciding, with much disappointment, that it was beyond me. But when the idea came up again, in my junior year, I could see a way through it. I now had a better sense of what one could do with the machine. I spent the next fourteen hours building the game. Within weeks, playing “Jimbo Jeopardy!” had become a regular activity among my friends. The experience was profound. I could understand why people poured their lives into craft: there is nothing quite like watching someone enjoy a thing you’ve made.</p><p>In the midst of all this, I had gone full “Paper Chase” and begun ignoring my grades. I worked voraciously, just not on my coursework. One night, I took over a half-dozen machines in a basement computer lab to run a program in parallel. I laid printouts full of numbers across the floor, thinking through a pathfinding algorithm. The cost was that I experienced for real that recurring nightmare in which you show up for a final exam knowing nothing of the material. (Mine was in Real Analysis, in the math department.) In 2009, during the most severe financial crisis in decades, I graduated with a 2.9 G.P.A.</p><p>And yet I got my first full-time job easily. I had work experience as a programmer; nobody asked about my grades. For the young coder, these were boom times. Companies were getting into bidding wars over top programmers. Solicitations for experienced programmers were so aggressive that they complained about “recruiter spam.” The popularity of university computer-science programs was starting to explode. (My degree was in economics.) Coding “boot camps” sprang up that could credibly claim to turn beginners into high-salaried programmers in less than a year. At one of my first job interviews, in my early twenties, the C.E.O. asked how much I thought I deserved to get paid. I dared to name a number that faintly embarrassed me. He drew up a contract on the spot, offering ten per cent more. The skills of a “software engineer” were vaunted. At one company where I worked, someone got in trouble for using HipChat, a predecessor to Slack, to ask one of my colleagues a question. “Never HipChat an engineer directly,” he was told. We were too important for that.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>This was an era of near-zero interest rates and extraordinary tech-sector growth. Certain norms were established. Companies like Google taught the industry that coders were to have free espresso and catered hot food, world-class health care and parental leave, on-site gyms and bike rooms, a casual dress code, and “twenty-per-cent time,” meaning that they could devote one day a week to working on whatever they pleased. Their skills were considered so crucial and delicate that a kind of superstition developed around the work. For instance, it was considered foolish to estimate how long a coding task might take, since at any moment the programmer might turn over a rock and discover a tangle of bugs. Deadlines were anathema. If the pressure to deliver ever got too intense, a coder needed only to speak the word “burnout” to buy a few months.</p><p>From the beginning, I had the sense that there was something wrongheaded in all this. Was what we did really so precious? How long could the boom last? In my teens, I had done a little Web design, and, at the time, that work had been in demand and highly esteemed. You could earn thousands of dollars for a project that took a weekend. But along came tools like Squarespace, which allowed pizzeria owners and freelance artists to make their own Web sites just by clicking around. For professional coders, a tranche of high-paying, relatively low-effort work disappeared.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a27287&quot;}" href="https://www.newyorker.com/cartoon/a27287" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>“I should have known he has absolutely no morals—I’ve seen how he loads a dishwasher.”</span></p><p><span>Cartoon by Hartley Lin</span></p></div></span></p></figure><p>The response from the programmer community to these developments was just, Yeah, you have to keep levelling up your skills. Learn difficult, obscure things. Software engineers, as a species, love automation. Inevitably, the best of them build tools that make other kinds of work obsolete. This very instinct explained why we were so well taken care of: code had immense leverage. One piece of software could affect the work of millions of people. Naturally, this sometimes displaced programmers themselves. We were to think of these advances as a tide coming in, nipping at our bare feet. So long as we kept learning we would stay dry. Sound advice—until there’s a tsunami.</p><p>When we were first allowed to use A.I. chatbots at work, for programming assistance, I studiously avoided them. I expected that my colleagues would, too. But soon I started seeing the telltale colors of an A.I. chat session—the zebra pattern of call-and-response—on programmers’ screens as I walked to my desk. A common refrain was that these tools made you more productive; in some cases, they helped you solve problems ten times faster.</p><p>I wasn’t sure I wanted that. I enjoy the act of programming and I like to feel useful. The tools I’m familiar with, like the text editor I use to format and to browse code, serve both ends. They enhance my practice of the craft—and, though they allow me to deliver work faster, I still feel that I deserve the credit. But A.I., as it was being described, seemed different. It provided a <em>lot</em> of help. I worried that it would rob me of both the joy of working on puzzles and the satisfaction of being the one who solved them. I could be infinitely productive, and all I’d have to show for it would be the products themselves.</p><p>The actual work product of most programmers is rarely exciting. In fact, it tends to be almost comically humdrum. A few months ago, I came home from the office and told my wife about what a great day I’d had wrestling a particularly fun problem. I was working on a program that generated a table, and someone had wanted to add a header that spanned more than one column—something that the custom layout engine we’d written didn’t support. The work was urgent: these tables were being used in important documents, wanted by important people. So I sequestered myself in a room for the better part of the afternoon. There were lots of lovely sub-problems: How should I allow users of the layout engine to convey that they want a column-spanning header? What should <em>their</em> code look like? And there were fiddly details that, if ignored, would cause bugs. For instance, what if one of the columns that the header was supposed to span got dropped because it didn’t have any data? I knew it was a good day because I had to pull out pen and pad—I was drawing out possible scenarios, checking and double-checking my logic.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>But taking a bird’s-eye view of what happened that day? A table got a new header. It’s hard to imagine anything more mundane. For me, the pleasure was entirely in the process, not the product. And what would become of the process if it required nothing more than a three-minute ChatGPT session? Yes, our jobs as programmers involve many things besides literally writing code, such as coaching junior hires and designing systems at a high level. But coding has always been the root of it. Throughout my career, I have been interviewed and selected precisely for my ability to solve fiddly little programming puzzles. Suddenly, this ability was less important.</p><p>I had gathered as much from Ben, who kept telling me about the spectacular successes he’d been having with GPT-4. It turned out that it was not only good at the fiddly stuff but also had the qualities of a senior engineer: from a deep well of knowledge, it could suggest ways of approaching a problem. For one project, Ben had wired a small speaker and a red L.E.D. light bulb into the frame of a portrait of King Charles, the light standing in for the gem in his crown; the idea was that when you entered a message on an accompanying Web site the speaker would play a tune and the light would flash out the message in Morse code. (This was a gift for an eccentric British expat.) Programming the device to fetch new messages eluded Ben; it seemed to require specialized knowledge not just of the microcontroller he was using but of Firebase, the back-end server technology that stored the messages. Ben asked me for advice, and I mumbled a few possibilities; in truth, I wasn’t sure that what he wanted would be possible. Then he asked GPT-4. It told Ben that Firebase had a capability that would make the project much simpler. Here it was—and here was some code to use that would be compatible with the microcontroller.</p><p>Afraid to use GPT-4 myself—and feeling somewhat unclean about the prospect of paying OpenAI twenty dollars a month for it—I nonetheless started probing its capabilities, via Ben. We’d sit down to work on our crossword project, and I’d say, “Why don’t you try prompting it this way?” He’d offer me the keyboard. “No, you drive,” I’d say. Together, we developed a sense of what the A.I. could do. Ben, who had more experience with it than I did, seemed able to get more out of it in a stroke. As he later put it, his own neural network had begun to align with GPT-4’s. I would have said that he had achieved mechanical sympathy. Once, in a feat I found particularly astonishing, he had the A.I. build him a Snake game, like the one on old Nokia phones. But then, after a brief exchange with GPT-4, he got it to modify the game so that when you lost it would show you how far you strayed from the most efficient route. It took the bot about ten seconds to achieve this. It was a task that, frankly, I was not sure I could do myself.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In chess, which for decades now has been dominated by A.I., a player’s only hope is pairing up with a bot. Such half-human, half-A.I. teams, known as centaurs, might still be able to beat the best humans and the best A.I. engines working alone. Programming has not yet gone the way of chess. But the centaurs have arrived. GPT-4 on its own is, for the moment, a worse programmer than I am. Ben is much worse. But Ben plus GPT-4 is a dangerous thing.</p><p>It wasn’t long before I caved. I was making a little search tool at work and wanted to highlight the parts of the user’s query that matched the results. But I was splitting up the query by words in a way that made things much more complicated. I found myself short on patience. I started thinking about GPT-4. Perhaps instead of spending an afternoon programming I could spend some time “prompting,” or having a conversation with an A.I.</p><p>In a 1978 essay titled “On the Foolishness of ‘Natural Language Programming,’&nbsp;” the computer scientist Edsger&nbsp;W. Dijkstra argued that if you were to instruct computers not in a specialized language like C++ or Python but in your native tongue you’d be rejecting the very precision that made computers useful. Formal programming languages, he wrote, are “an amazingly effective tool for ruling out all sorts of nonsense that, when we use our native tongues, are almost impossible to avoid.” Dijkstra’s argument became a truism in programming circles. When the essay made the rounds on Reddit in 2014, a top commenter wrote, “I’m not sure which of the following is scariest. Just how trivially obvious this idea is” or the fact that “many still do not know it.”</p><p>When I first used GPT-4, I could see what Dijkstra was talking about. You can’t just say to the A.I., “Solve my problem.” That day may come, but for now it is more like an instrument you must learn to play. You have to specify what you want carefully, as though talking to a beginner. In the search-highlighting problem, I found myself asking GPT-4 to do too much at once, watching it fail, and then starting over. Each time, my prompts became less ambitious. By the end of the conversation, I wasn’t talking about search or highlighting; I had broken the problem into specific, abstract, unambiguous sub-problems that, together, would give me what I wanted.</p><p>Having found the A.I.’s level, I felt almost instantly that my working life had been transformed. Everywhere I looked I could see GPT-4-size holes; I understood, finally, why the screens around the office were always filled with chat sessions—and how Ben had become so productive. I opened myself up to trying it more often.</p><p>I returned to the crossword project. Our puzzle generator printed its output in an ugly text format, with lines like <code>"s""c""a""r""*""k""u""n""i""s""*" "a""r""e""a"</code>. I wanted to turn output like that into a pretty Web page that allowed me to explore the words in the grid, showing scoring information at a glance. But I knew the task would be tricky: each letter had to be tagged with the words it belonged to, both the across and the down. This was a detailed problem, one that could easily consume the better part of an evening. With the baby on the way, I was short on free evenings. So I began a conversation with GPT-4. Some back-and-forth was required; at one point, I had to read a few lines of code myself to understand what it was doing. But I did little of the kind of thinking I once believed to be constitutive of coding. I didn’t think about numbers, patterns, or loops; I didn’t use my mind to simulate the activity of the computer. As another coder, Geoffrey Litt, wrote after a similar experience, “I never engaged my detailed programmer brain.” So what <em>did</em> I do?</p><p>Perhaps what pushed Lee Sedol to retire from the game of Go was the sense that the game had been forever cheapened. When I got into programming, it was because computers felt like a form of magic. The machine gave you powers but required you to study its arcane secrets—to learn a spell language. This took a particular cast of mind. I felt selected. I devoted myself to tedium, to careful thinking, and to the accumulation of obscure knowledge. Then, one day, it became possible to achieve many of the same ends without the thinking and without the knowledge. Looked at in a certain light, this can make quite a lot of one’s working life seem like a waste of time.</p><p>But whenever I think about Sedol I think about chess. After machines conquered that game, some thirty years ago, the fear was that there would be no reason to play it anymore. Yet chess has never been more popular—A.I. has enlivened the game. A friend of mine picked it up recently. At all hours, he has access to an A.I. coach that can feed him chess problems just at the edge of his ability and can tell him, after he’s lost a game, exactly where he went wrong. Meanwhile, at the highest levels, grandmasters study moves the computer proposes as if reading tablets from the gods. Learning chess has never been easier; studying its deepest secrets has never been more exciting.</p><p>Computing is not yet overcome. GPT-4 is impressive, but a layperson can’t wield it the way a programmer can. I still feel secure in my profession. In fact, I feel somewhat more secure than before. As software gets easier to make, it’ll proliferate; programmers will be tasked with its design, its configuration, and its maintenance. And though I’ve always found the fiddly parts of programming the most calming, and the most essential, I’m not especially good at them. I’ve failed many classic coding interview tests of the kind you find at Big Tech companies. The thing I’m relatively good at is knowing what’s worth building, what users like, how to communicate both technically and humanely. A friend of mine has called this A.I. moment “the revenge of the so-so programmer.” As coding per se begins to matter less, maybe softer skills will shine.</p><p>That still leaves open the matter of what to teach my unborn child. I suspect that, as my child comes of age, we will think of “the programmer” the way we now look back on “the computer,” when that phrase referred to a person who did calculations by hand. Programming by typing C++ or Python yourself might eventually seem as ridiculous as issuing instructions in binary onto a punch card. Dijkstra would be appalled, but getting computers to do precisely what you want might become a matter of asking politely.</p><p>So maybe the thing to teach isn’t a skill but a spirit. I sometimes think of what I might have been doing had I been born in a different time. The coders of the agrarian days probably futzed with waterwheels and crop varietals; in the Newtonian era, they might have been obsessed with glass, and dyes, and timekeeping. I was reading an oral history of neural networks recently, and it struck me how many of the people interviewed—people born in and around the nineteen-thirties—had played with radios when they were little. Maybe the next cohort will spend their late nights in the guts of the A.I.s their parents once regarded as black boxes. I shouldn’t worry that the era of coding is winding down. Hacking is forever.&nbsp;♦</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My favorite coding question to give candidates (112 pts)]]></title>
            <link>https://carloarg02.medium.com/my-favorite-coding-question-to-give-candidates-17ea4758880c</link>
            <guid>38257024</guid>
            <pubDate>Mon, 13 Nov 2023 23:58:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://carloarg02.medium.com/my-favorite-coding-question-to-give-candidates-17ea4758880c">https://carloarg02.medium.com/my-favorite-coding-question-to-give-candidates-17ea4758880c</a>, See on <a href="https://news.ycombinator.com/item?id=38257024">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2 id="0e9d">A coding interview question, from the viewpoint of an Google/Amazon/Microsoft interviewer</h2><div><a rel="noopener follow" href="https://carloarg02.medium.com/?source=post_page-----17ea4758880c--------------------------------"><div aria-hidden="false"><p><img alt="Carlos Arguelles" src="https://miro.medium.com/v2/resize:fill:88:88/1*CM27oO9pXETXjs2M9_dUFg.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div></div><figure><figcaption>photo <a href="https://unsplash.com/photos/two-people-drawing-on-whiteboard-26MJGnCM0Wc" rel="noopener ugc nofollow" target="_blank">credit</a></figcaption></figure><p id="7da1">There’s so many blogs and videos online showing you answers to LeetCode questions. But the viewpoint is mostly as an <strong>interviewee</strong>, not as an <strong>interviewer.</strong></p><p id="d629">In my 25 years in Big Tech, I’ve conducted over a thousand interviews (eight hundred at Amazon <a href="https://link.medium.com/zkSnwVUqmjb" rel="noopener">as a Bar Raiser</a>, a couple of hundred at Microsoft, and shy of a hundred at Google). Sometimes I get assigned leadership interviews, sometimes systems design, sometimes coding.</p><p id="1ad2">I don’t like <em>hard</em> or <em>tricky</em> questions. I tend to favor easier questions that lead to a high quality conversation where I can learn about the way somebody thinks. For coding interviews, I have often asked a variation of the following problem, loosely based on something I had to do in real life. I tailor it a bit with every candidate: it’s not always quite the same, and I’ve evolved it over time, but since I decided to retire the question, I’ll dissect it today. <strong>I want to focus on explaining what I looked for in a candidate with this question.</strong></p><blockquote><p id="f05f">Let’s say we have a website and we keep track of what pages customers are viewing, for things like business metrics.</p><p id="6782">Every time somebody comes to the website, we write a record to a log file consisting of Timestamp, PageId, CustomerId. At the end of the day we have a big log file with many entries in that format. And for every day we have a new file.</p><p id="e321">Now, given two log files (log file from day 1 and log file from day 2) we want to generate a list of ‘loyal customers’ that meet the criteria of: (a) they came on both days, and (b) they visited at least two unique pages.</p></blockquote><p id="1b35">The question is not particularly difficult, but it does require a little bit of thinking and knowledge of code complexity and data structures. You can easily get the customers that came on both days, you can easily get the customers that visited at least two unique pages, but getting the <em>intersection</em> of those two <em>efficiently</em> requires a little more work.</p><p id="dfb1">I’ve probably asked it 500 times, which has allowed me to calibrate it quite well. I have found that a Hire / No Hire decision from this question aligns with the final loop Hire / No Hire decision on a candidate about 95% of the times. I can also upscale it for higher level candidates, which makes it fairly versatile, and there’s many hints I can give to candidates struggling along the way.</p><h2 id="26f4">Ask Clarifying Questions</h2><p id="d136">Great candidates must ask clarifying questions before jumping into coding. I’m hoping to see some intuition from my candidates as I’ve actually expressed the problem in an <strong>ambiguous</strong> way.</p><p id="5191"><em>Did I mean 2 unique pages </em><strong><em>per day</em></strong><em> or </em><strong><em>overall</em></strong><em>?</em></p><p id="92ed">This significantly impacts the solution that you come up with. I mean “2 unique pages <em>overall</em>” because that’s a much more interesting problem. About half the candidates jump straight into coding without clarifying this, and out of those, half assume incorrectly that I meant “2 unique pages per day.” If it’s a more junior candidate, I’ll hint heavily before they start coding. If it’s a more senior candidate, I’ll wait a bit and see if that comes up as they’re thinking more deeply about the algorithm.</p><p id="dbe4">In real life, engineers deal with ambiguity all the time, and the root of most software problems can be traced back to poorly defined requirements. So I want to get a signal on <em>spotting and dealing with ambiguity</em>.</p><p id="d391">There’s one more clarifying question that 90% of people miss upfront, but that also matters: <strong>What about duplicates? </strong>Visits to the same page in the same day? What about visits to the same page on different days? For this problem, these are duplicates.</p><p id="2e24">Another clarifying question that matters is <strong>what scale are we talking about here? </strong><em>Does the data fit in memory? </em>Can I load the contents of one of these files in memory? Can I load the contents of both?</p><p id="5590">The question was inspired by a real-world system that I worked on at Amazon, called Clickstream, that was responsible for tracking user behavior on amazon.com. In real life, we processed petabytes of events from millions of concurrent customers on a giant Hadoop cluster of ten thousand hosts, and we had an entire team of engineers maintaining and operating the system. For the purposes of a 45-minute interview, I wanted to remove the distributed nature of the problem, and just imagine the data fits in memory, with a much smaller scope.</p><p id="b325">Lastly, another clarifying question that is important is <strong>how much does performance vs memory matter?</strong> There’s a naive solution that is O(n²) in running time, but only uses O(1) of memory. There’s a better solution that has running time of O(n) but uses O(n) of memory. And there’s an in-between solution that does some pre-processing in O(n log n) with O(k) of memory, which allows you to run the main algorithm in O(n) with O(1) of memory. Each one has pros and cons. Bonus points to any candidate that discusses this upfront. Only the top 10% of candidates do this.</p><h2 id="4779">Can’t I just use a Database?</h2><p id="3209">In theory you could write a pretty simple SQL query, and sure, Big Tech companies have giant data warehouses where you can easily do this sort of thing. But for the scope of a coding interview, you wouldn’t want to. Since this is not a distributed systems problem and the data fits in memory, why introduce the additional complexity and dependencies of a database for something that you can solve with 20 lines of simple code?</p><h2 id="6cda">Naive solution first</h2><p id="de00">About 80% of the candidates go for the naive solution first. It’s easiest and most natural. It’s some form of <em>“for each element from file 1, loop for all the contents of file 2, looking for elements with the same customer id, and keeping track of the pages they view.”</em></p><p id="4a54">The problem with the naive solution is that its running time is O(n²).</p><p id="dc2f">I don’t mind getting the naive solution first, but I really want to see my candidate having that <em>aha moment</em> that O(n²) is probably never good in any problem. And I want that aha moment to come pretty quickly and without hints. No great engineer should ever settle for an O(n²) algorithm, unless bound by memory or some other unmovable constraint.</p><p id="5e01">After a candidate puts forth the O(n²), I smile politely and I wait. I am really hoping the next words that come out of their mouth are “…but the complexity of this is O(n²) so can I do better?”</p><p id="fccb">Occasionally, a candidate will think they’re done at this point. 90% of the times that a candidate was done without questioning the quadratic nature of that solution, the final outcome of the loop was No Hire. So that’s another signal for me.</p><p id="3820">I’ll gentle probe, “what’s the running time of this solution”? And most of the times, the candidate will have the aha moment after that hint and move on to a better solution.</p><h2 id="a692"><strong>Tuning O(n²) into O(n)</strong></h2><p id="4919">At this point you need to do some thinking about what data structure you’re going to use, and how you’re going to store your data.</p><p id="9cdb">Poor candidates go for linked lists, or arrays. If arrays, my challenge question is that they don’t know the size of the data upfront. If it’s a linked list, then you should know search is going to cost you O(n) for each element, therefore you’ll end up back to an O(n²) algorithm no matter how hard you try. You can use a Tree, but since search is O(log n), that’ll yield an overall best of O(n log n).</p><p id="71bb">Better candidates have the intuition that a <strong>Map</strong> will provide the O(1) lookup that they need to turn the O(n²) algorithm into an O(n) algorithm. Great candidates will proactively point out the <em>downside</em> is that you’ll use O(n) memory. Faster running time at the expense of more memory is a tradeoff.</p><p id="d8a5"><em>If you’re using a Map, what is your Key, and what is your Value?</em> I’ve seen all kinds of answers here. Some candidates use PageId as the Key, and CustomerId as the Value, but that is not going to help. Candidates then switch it around to have CustomerId as the Key, and PageId as the Value of the Map. But that’s not particularly great either because it overlooks the fact that <em>you can have many pages per customer, not just one.</em> Some candidates have the intuition that they need a <em>Collection</em> of pages as the Value of the Map, but they’ll go for a List, which saddens my soul because it overlooks the fact that you can have <em>duplicates</em>. This is a good opportunity to probe around data structure knowledge on Lists vs. Sets, as candidates think about handling duplicates.</p><p id="83c9">So, <em>Map&lt;CustomerId, Set&lt;PageId&gt;&gt;</em> will do. But will you load the contents of both files into a single Map? Or have two maps, one for each file? Or can you get away with just loading the contents of file 1 into a map, and processing file 2 without storing it?</p><p id="1b32">Great candidates realize they can go for option#3 right away on their own. That’s a lot less memory, and a simpler algorithm. Good candidates get there, but need a little hinting. Poor candidates load the contents of both files into memory.</p><p id="8d11">Now that the algorithm is set, it’s time to write the code. For the most part it’s straight forward but there’s some dragons here.</p><p id="bf9b">The condition <em>“customers that came on both days”</em> is pretty simple: as you’re reading a customer entry from Day2, if the customer is in the Map from Day1, then you know they came on both days:</p><figure></figure><p id="2e34">The condition <em>“customers that visited at least 2 unique pages”</em> tends to be a little harder for candidates to get right, so if they’re stuck I throw a little hint: you have a Set of pages from Day1, and a single page from Day2… how can you determine that this is at least two unique pages?</p><figure></figure><p id="9039">Poor candidates will loop through the elements in the Set to check if the page from Day2 is in there. This turns your O(n) algorithm into O(n²) again. The number of candidates who have done this is surprising.</p><p id="e388">Better candidates will do a <em>.contains() </em>on the Set which is an O(1) operation on a hash set. But there is a catch with the logic.</p><p id="79fa">The intuition to get this right is this: If you are inside that If loop and the customer visited at least two pages in Day1, and they visited <em>any</em> page in Day2, they’re loyal, regardless of which page they visit in Day2. Otherwise, they only visited only one page in Day1, so the question is: is this a different page? If so they’re loyal, else it’s a duplicate so you don’t know and should keep going. So your If statement has an Or:</p><figure></figure><p id="ca91">There’s a need for attention to detail, like using “&gt;” instead of “&gt;=” or missing the “!” in the second statement. I saw these fairly often. I didn’t worry. Great candidates spotted them quickly as they double-checked the algorithm when they were done. Good candidates spotted them after a little bit of hinting. That gave me a good signal on <em>debugging skills</em>.</p><h2 id="bae2">Optimizing Your Solution</h2><p id="1aa8">Great candidates often go the extra mile with little optimizations that show attention to detail, and engineering craftsmanship.</p><p id="5b56">For example, <em>you don’t need to actually keep </em><strong><em>every single page</em></strong><em> from Day 1</em> in the Map, <strong>just two</strong>, since the problem is “at least two pages” so a Set of size 2 or even an array of size 2 will use less memory than an unbounded Set.</p><p id="d209">Or, if you’ve already determined that a customer is loyal, you don’t need to waste CPU cycles going thru the logic again next time you encounter that customer in Day 2.</p><figure></figure><p id="e365">Better yet, you could just remove the entry from Day1 once you know they’re loyal, which will mean that the day1.containsKey() will return false next time and you won’t do additional work for that customer, since you’ve already determined they are loyal:</p><figure></figure><blockquote><p id="0fb7">A word about optimizing: you could argue that <strong>these optimizations make it more difficult to change the algorithm if the requirements change</strong> in the future. That’s a reasonable stand. As long as you can hold a good conversation about pros and cons, I am happy with a high quality discussion on how you would balance these decisions. At the same time, being able to optimize an algorithm when neeeded *is* a trait of a great engineer and you *will* need to do that a time or two in your career.</p></blockquote><h2 id="c618">The Other Solution</h2><p id="229a">There’s a different way of thinking about this problem. The vast majority of candidates go for the Map approach, but sometimes I have a candidate go for the other one. Maybe 5% of the time at most.</p><p id="2af5"><em>What if you pre-processed the files and sorted them by CustomerId, then by PageId?</em></p><p id="3bc1">Pre-processing is a powerful tool in your software engineering arsenal, particularly if you’re going to be performing an operation a bunch of times. You can take the hit of pre-processing with the first one, or do it beforehand, which amortizes the cost over time. Sorting the files can be a logarithmic operation with constant memory.</p><p id="173f">If the files are sorted, then the problem is easier and it’s just a <a href="https://www.geeksforgeeks.org/two-pointers-technique/" rel="noopener ugc nofollow" target="_blank"><strong>two-pointer algorithm</strong></a> that you can execute in O(n) with O(1) of memory. While there are still entries in Day 1 and in Day 2, if CustomerId from Day1 &lt; CustomerId from Day2, move the pointer for Day 1, else if CustomerId from Day1 &gt; CustomerId from Day2, move the pointer for Day 2. Now you’re in a situation where the CustomerId is the same for both days, so you know they came on both days. Since the second sort key is by PageId, you follow another two-pointer algorithm to determine that there are at least two unique pages. So it’s a 2-pointer algorithm within a 2-pointer algorithm. It’s kind of a fun problem! I’ll leave the actual implementation as an exercise for the viewer.</p><h2 id="d0f5">Making the problem harder</h2><p id="1615">If you want to make the problem even more interesting, you can add a third file. I will leave that as an exercise for the reader as well!</p><h2 id="8444">In conclusion</h2><p id="8405">I hope this little insight into a coding problem&nbsp;from the viewpoint of an interviewer, and the ways in which I’ve seen great, good and poor candidates approach it, has been useful to you. Best of luck in your next interview!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nepal bans TikTok and says it disrupts social harmony (652 pts)]]></title>
            <link>https://apnews.com/article/nepal-tiktok-ban-social-media-854846a42ef566fa296ddaacd0099447</link>
            <guid>38256810</guid>
            <pubDate>Mon, 13 Nov 2023 23:32:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/nepal-tiktok-ban-social-media-854846a42ef566fa296ddaacd0099447">https://apnews.com/article/nepal-tiktok-ban-social-media-854846a42ef566fa296ddaacd0099447</a>, See on <a href="https://news.ycombinator.com/item?id=38256810">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-module="" data-padding="none">
                    
                    
                        
                            

    <div><figure>
    

    
        <picture data-crop="medium-3x2">
    
        <source media="(min-width: 1280px)" type="image/webp" width="980" height="653" srcset="https://dims.apnews.com/dims4/default/124cb91/2147483647/strip/true/crop/3000x1999+0+1/resize/980x653!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 1280px)" width="980" height="653" srcset="https://dims.apnews.com/dims4/default/3e22a29/2147483647/strip/true/crop/3000x1999+0+1/resize/980x653!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 1024px)" type="image/webp" width="820" height="546" srcset="https://dims.apnews.com/dims4/default/63e84c7/2147483647/strip/true/crop/3000x1998+0+1/resize/820x546!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 1024px)" width="820" height="546" srcset="https://dims.apnews.com/dims4/default/f37655a/2147483647/strip/true/crop/3000x1998+0+1/resize/820x546!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 768px)" type="image/webp" width="1024" height="683" srcset="https://dims.apnews.com/dims4/default/9f8df58/2147483647/strip/true/crop/2999x2000+1+0/resize/1024x683!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 768px)" width="1024" height="683" srcset="https://dims.apnews.com/dims4/default/b616b38/2147483647/strip/true/crop/2999x2000+1+0/resize/1024x683!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x" loading="lazy">

    

    
        <source media="(min-width: 600px)" type="image/webp" width="767" height="511" srcset="https://dims.apnews.com/dims4/default/9d91526/2147483647/strip/true/crop/3000x1999+0+1/resize/767x511!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/b1d3ef6/2147483647/strip/true/crop/3000x1999+0+1/resize/1534x1022!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    
        <source media="(min-width: 600px)" width="767" height="511" srcset="https://dims.apnews.com/dims4/default/8730597/2147483647/strip/true/crop/3000x1999+0+1/resize/767x511!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/4bcb7fd/2147483647/strip/true/crop/3000x1999+0+1/resize/1534x1022!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    
        <source media="(max-width: 599px)" type="image/webp" width="567" height="378" srcset="https://dims.apnews.com/dims4/default/79f0160/2147483647/strip/true/crop/3000x2000+0+0/resize/567x378!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/e80595f/2147483647/strip/true/crop/3000x2000+0+0/resize/1134x756!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    
        <source media="(max-width: 599px)" width="567" height="378" srcset="https://dims.apnews.com/dims4/default/2cf812d/2147483647/strip/true/crop/3000x2000+0+0/resize/567x378!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/7718ef8/2147483647/strip/true/crop/3000x2000+0+0/resize/1134x756!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    
        <source type="image/webp" width="320" height="213" srcset="https://dims.apnews.com/dims4/default/c0a7c9c/2147483647/strip/true/crop/3000x1997+0+2/resize/320x213!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/7bc5c29/2147483647/strip/true/crop/3000x1997+0+2/resize/640x426!/format/webp/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    
        <source width="320" height="213" srcset="https://dims.apnews.com/dims4/default/f2148d3/2147483647/strip/true/crop/3000x1997+0+2/resize/320x213!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/857a41f/2147483647/strip/true/crop/3000x1997+0+2/resize/640x426!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" loading="lazy">

    

    <img alt="FILE - A view of the TikTok app logo, in Tokyo, Japan, Sept. 28, 2020. The European Union ratcheted up its scrutiny of Big Tech companies on Thursday, Oct. 19, 2023, with demands for Meta and TikTok to detail their efforts on curbing illegal content and disinformation amid the Israel-Hamas war. (AP Photo/Kiichiro Sato, File)" srcset="https://dims.apnews.com/dims4/default/f2148d3/2147483647/strip/true/crop/3000x1997+0+2/resize/320x213!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 1x,https://dims.apnews.com/dims4/default/857a41f/2147483647/strip/true/crop/3000x1997+0+2/resize/640x426!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624 2x" width="320" height="213" src="https://dims.apnews.com/dims4/default/f2148d3/2147483647/strip/true/crop/3000x1997+0+2/resize/320x213!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F4d%2F29%2F4a51b3d52c17bdc7379fac382c5a%2F96139e0480804315b8fa4b530c00f624" loading="lazy">
</picture>

    

    
        <div>
            <div><bsp-read-more data-more-button-text="Read More" data-less-button-text="Read Less" data-expand="ReadMore-expand" data-limit="110" data-main-class="ReadMore">
                    <figcaption><p>FILE - A view of the TikTok app logo, in Tokyo, Japan, Sept. 28, 2020. The European Union ratcheted up its scrutiny of Big Tech companies on Thursday, Oct. 19, 2023, with demands for Meta and TikTok to detail their efforts on curbing illegal content and disinformation amid the Israel-Hamas war. (AP Photo/Kiichiro Sato, File)</p></figcaption>
                </bsp-read-more></div>
            <bsp-lead-superlead-ui>
    
    
</bsp-lead-superlead-ui>
        </div>
    
</figure>
</div>



                        
                    

                    <div>
                                        <p>KATHMANDU, Nepal (AP) — Nepal’s government decided to ban the popular <span><a href="https://apnews.com/article/tiktok-ceo-shou-zi-chew-security-risk-cc36f36801d84fc0652112fa461ef140" target="_blank" rel="noopener">social media app TikTok</a></span> on Monday, saying it was disrupting “social harmony” in the country. </p><p>The announcement was made following a Cabinet meeting. Foreign Minister Narayan Prakash Saud said the app would be banned immediately. </p><p>“The government has decided to ban TikTok as it was necessary to regulate the use of the social media platform that was disrupting social harmony, goodwill and flow of indecent materials,” Saud said.</p>
    

<p>He said that to make social media platforms accountable, the government has asked the companies to register and open a liaison office in Nepal, pay taxes and abide by the country’s laws and regulations. </p><p>It wasn’t clear what triggered the ban or if TikTok had refused to comply with Nepal’s requests. The company did not immediately respond to an email seeking comment. </p><p>TikTok, owned by China’s ByteDance, <span><a href="https://apnews.com/article/tiktok-ban-privacy-cybersecurity-bytedance-china-2dce297f0aed056efe53309bbcd44a04" target="_blank" rel="noopener">has faced scrutiny in a number of countries</a></span> because of concerns that Beijing could use the app to harvest user data or advance its interests. Countries including <span><a href="https://apnews.com/article/tiktok-ban-ceo-congressional-hearing-bytedance-china-44d948c5b0ba18e2a714e0fa62d52779" target="_blank" rel="noopener">the United States</a></span>, Britain and New Zealand have banned the app on government phones despite TikTok repeatedly denying that it has ever shared data with the Chinese government and would not do so if asked.</p><p>Nepal has banned all pornographic sites in 2018.</p>
                                    </div>

                    


                    


                    
    



                    
    


                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google witness accidentally blurts out that Apple gets 36% cut of Safari deal (154 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/11/google-witness-accidentally-blurts-out-that-apple-gets-36-cut-of-safari-deal/</link>
            <guid>38256746</guid>
            <pubDate>Mon, 13 Nov 2023 23:24:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/11/google-witness-accidentally-blurts-out-that-apple-gets-36-cut-of-safari-deal/">https://arstechnica.com/tech-policy/2023/11/google-witness-accidentally-blurts-out-that-apple-gets-36-cut-of-safari-deal/</a>, See on <a href="https://news.ycombinator.com/item?id=38256746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      Witness malfunction    —
</h4>
            
            <h2 itemprop="description">Google and Apple specifically requested that detail be confidential.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/GettyImages-1725649029-800x544.jpg" alt="Google witness accidentally blurts out that Apple gets 36% cut of Safari deal">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 168:single/related:96da591cf0079d41f8ddadcf1d18c52f --><!-- empty -->
<p>Google's default search deal with Apple is worth so much to the search giant that Google pays 36 percent of its search advertising revenue from Safari to keep its search engine set as the default in Apple's browser, <a href="https://www.bloomberg.com/news/articles/2023-11-13/apple-gets-36-of-google-revenue-from-search-deal-witness-says">Bloomberg reported</a>.</p>
<p>Google and Apple objected to making this key detail public from their long-running default search deal. But their closely held secret came out on Monday during testimony from Google's main economics expert, Kevin Murphy, during the Department of Justice's monopoly trial examining Google's search business.</p>
<p>"Probably the biggest slip of the entire trial," Big Tech on Trial, an account dedicated to providing updates from the Google trial, <a href="https://twitter.com/BigTechOnTrial/status/1724136578593718643">posted</a> on X (formerly Twitter).</p>
<p><a href="https://news.bloomberglaw.com/ip-law/apple-gets-36-of-google-revenue-from-search-deal-witness-says">According to Bloomberg Law</a>, Google attorney John Schmidtlein "visibly cringed" when Murphy revealed the confidential information, which Google had initially claimed needed to be kept secret because otherwise it “would unreasonably undermine Google’s competitive standing in relation to both competitors and other counterparties.”</p>
<p>For the DOJ—which has made the <a href="https://arstechnica.com/tech-policy/2023/10/googles-21-year-deal-with-apple-is-the-heart-of-monopoly-case-judge-says/">Google-Apple deal the center of its case</a> alleging that Google maintains an illegal monopoly over search—this detail confirms how valuable default placements on iPhones are to the search leader.</p>
<p>The DOJ has argued that Google pays so much for default search deals to block out competitors, lock search users into its services, and maintain a stronghold over the search industry—a dominant position that could be further entrenched by Google's advances with AI, <a href="https://arstechnica.com/tech-policy/2023/10/googles-claim-that-search-users-have-choice-is-bogus-microsoft-ceo-tells-judge/">Microsoft CEO Satya Nadella testified</a>. In September, an Apple exec testified that the default deal between Google and Apple was seemingly so lucrative that it even <a href="https://arstechnica.com/tech-policy/2023/09/google-deal-may-have-kept-apple-from-building-search-engine-exec-says/">stopped Apple from creating its own rival search engine</a>.</p>
<p>It's still unclear exactly how much money that portion of Google's search advertising revenue that comes from Safari amounts to, but several estimates have been floated.&nbsp;<a href="https://www.statista.com/statistics/266249/advertising-revenue-of-google/">Statista reported</a> that Google's advertising revenue was $224 billion in 2022, and based on that, <a href="https://www.engadget.com/google-reportedly-pays-apple-36-percent-of-ad-search-revenues-from-safari-191730783.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAHZ16G_-rr-pYMpU363ol3cjwErVsOs8lLJgDiRDyapIyCOWFIcEGVZaEmYnPdiIiEKA24pt-r2TWndowKJ-SZiX46kzYXKgmx4w9faq6ioIdOm1USMSKC6MdQjB5sBOGI9MEL7agKiYmW6X7iKhzjvstWsVlYRSl5gSH1wbsqBJ">Engadget estimated</a> that Apple likely gets paid in the tens of billions of dollars for Google's default Safari placements.</p>                                            
                                                        
<p>Previously, sources <a href="https://www.nytimes.com/2023/10/26/technology/google-apple-search-spotlight.html">told The New York Times</a> that Google paid Apple approximately $18 billion in 2021 for the deal, but the exact amount of revenue sharing remained unknown until Monday. The DOJ's trial also recently revealed that <a href="https://arstechnica.com/tech-policy/2023/10/google-paid-26b-for-default-contracts-in-2021-google-exec-testified/">Google paid $26 billion in total for default contracts</a>, which&nbsp;are ostensibly responsible for driving up its search advertising revenue that is right now rapidly climbing. Google's global ad revenue will likely reach nearly $340 billion by 2027, Statista <a href="https://www.statista.com/statistics/539447/google-global-net-advertising-revenues/">reported</a>, driven largely by Google's search engine traffic, which is currently responsible for "roughly 38 percent" of its global ad revenue.</p>
<p>In total, across all those default deals, Digital Content Next CEO Jason Kint estimated in a <a href="https://twitter.com/jason_kint/status/1724152525538959850">post</a> on X that it's possible that Google derives "at least $90 billion of its current annual revenue."</p>
<p>Last month, Google CEO <a href="https://arstechnica.com/tech-policy/2023/10/doj-grilled-sundar-pichai-on-very-valuable-default-deals-deleted-chats/">Sundar Pichai testified</a> that default deals "can make a difference" and can be "very valuable" if "done correctly" but maintained Google's chief defense that partners like Apple enter these deals with Google because Google has a superior search engine.</p>
<p>If the DOJ proves that these default deals ensure that Google maintains an illegal monopoly in general search markets, Google could be ordered to break up its search business, shifting not just Google's bottom line but also its partners, like Apple.</p>
<p>While the trial resumes for another week, Google continues profiting off the deals. From 2022 to 2023, Google's ad revenue increased by $5 billion, <a href="https://searchengineland.com/google-search-ad-revenue-q2-2023-433633">Search Engine Land reported</a>, and seemingly as Nadella predicted, Pichai attributed these gains to AI-driven innovations across Google products, including search.</p>
<p>"We’re continuing to focus on making AI more helpful for everyone; there’s exciting progress and lots more to come,” Pichai said in a statement reported by Search Engine Land.</p>
<p>Judge Amit Mehta, presiding over the antitrust trial, has said that the Google-Apple default deal is the <a href="https://arstechnica.com/tech-policy/2023/10/googles-21-year-deal-with-apple-is-the-heart-of-monopoly-case-judge-says/">"heart" of the DOJ's case against Google</a>. With each new detail revealed about how much Google is willing to pay Apple to maintain their deal, the DOJ hopes to convince Mehta that the deal gives Google an unfair advantage over competitors. This week's slip-up from one of Google's witnesses threatens to disrupt the narrative that Google is trying to build as it winds down its defense of that deal and others.</p>
<p>Mehta is not expected to issue a ruling in the case until 2024.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infinite Context LLMs: Going Beyond RAG with Extended Minds (125 pts)]]></title>
            <link>https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/supersizing-transformers.html</link>
            <guid>38256645</guid>
            <pubDate>Mon, 13 Nov 2023 23:13:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/supersizing-transformers.html">https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/supersizing-transformers.html</a>, See on <a href="https://news.ycombinator.com/item?id=38256645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
  

<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">




<p>Today’s popularized large language models are optimized for the task of producing sequences of tokens that look like they could’ve been present in the training corpus. This is quite distinct from the ways in which LLMs are wielded in such user interfaces as <a href="https://chat.openai.com/?model=gpt-4">ChatGPT</a> or <a href="https://www.perplexity.ai/">Perplexity.ai</a>, where users expect the model to perform complex reasoning tasks and faithfully retrieve factual, topical information. If we hope to use the model as a general reasoning agent and not as a stochastic parrot, we need to provide it with any relevant data at inference time, rather than rely on (1) the salient data having appeared in the training corpus and (2) the model being able to recall said data. Further, surfacing references or citations that highlight which content the model used during its generation is crucial for building applications that truly augment human workflows.</p>
<p>This has prompted much development on methods colloquially referred to as “retrieval”<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Or, methods that help LLMs make use of pertinent documents. <strong>In context learning</strong>, or placing the relevant documents in the context window before the prompt, is the obvious first step. However, in many cases we’re faced with documents longer than the context window of the model. <strong>RAG</strong><a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> attempts to sidestep this by selecting the best subset of documents to include alongside the user’s query. While often effective, RAG is fundamentally limited by the need for a separate search engine. We can’t, for instance, ask the model questions which require synthesizing the entire set of documents. Further, since the retrieval happens before the generation, the best we can do r.e. explainability is report which text was included in the prompt itself. This says nothing about what text the model actually used during generation.</p>
<p>Finetuning<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> seeks to extend the length of the context window itself. Running even a few epochs of training can be a non-trivial undertaking for today’s large models, even with a dedicated ML team. Further, these methods doesn’t contribute to the model’s interpretability. Other methods<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> suggest structural changes to the model. Many of these are exciting, but most require training from scratch or fine-tuning, making them difficult to leverage with pre-trained models.</p>
<p>In this post, we propose and <a href="https://huggingface.co/normalcomputing">open source</a> <strong>extended mind transformers</strong>, which generalize RAG internally. This simple mathematical generalization buys us the performance gains (and more) of RAG, as well as introducing net-new generation controls and granular <em>causal</em> citations. We also get the best of both worlds when it comes to ease of use: seamless integrations (everything is internal to the model), and no fine-tuning required!</p>
<div>
<figure>
<p><img src="https://storage.googleapis.com/normal-blog-artifacts/extended-mind-transformers/otto.png"></p>
<figcaption>Credits: <span data-cites="patrick-blog">Buchen (<a href="#ref-patrick-blog" role="doc-biblioref">2018</a>)</span></figcaption>
</figure>
</div>
<section id="aesthetics-for-extended-mind-transformers">
<h2 data-anchor-id="aesthetics-for-extended-mind-transformers">Aesthetics for Extended Mind Transformers</h2>
<p>As motivation, we provide context from the Philosophy of Mind which served as inspiration for the naming convention and methodology. In <span data-cites="clark-chalmers">Clark and Chalmers (<a href="#ref-clark-chalmers" role="doc-biblioref">1998</a>)</span> “The Extended Mind”, they present the thesis that external information which is constantly and immediately accessible, and automatically endorsed should be considered part of the memory. And further, that this extension should be considered part of the mind. They term this idea <strong>active externalism</strong>. The story of Otto functions as an intuition pump:</p>
<blockquote>
<p>“[L]ike many Alzheimer’s patients, [Otto] relies on information in the environment to help structure his life. Otto carries a notebook around with him everywhere he goes. When he learns new information, he writes it down. When he needs some old information, he looks it up. For Otto, his notebook plays the role usually played by a biological memory. … The information in the notebook functions just like information constituting an ordinary non-occurrent belief; it just happens that this information lies beyond the skin.”<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
</blockquote>
<p>In this piece, we present active externalism for LLMs, a mechanism for bolstering the memory of transformers aesthetically inspired by the Extended Mind Thesis. We call transformers which implement active externalism, extended mind transformers.</p>
</section>
<section id="extended-mind-transformers">
<h2 data-anchor-id="extended-mind-transformers">Extended Mind Transformers</h2>
<section id="definition">
<h3 data-anchor-id="definition">Definition</h3>
<p>Our proposed method, which closely resembles the work of <span data-cites="wu2022memorizing">Wu et al. (<a href="#ref-wu2022memorizing" role="doc-biblioref">2022</a>)</span><a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>, is a simple change to the self-attention mechanism. In addition to the causal self-attention integral to transformers, we also allow each query token to attend to a fixed number of “external memories”. These memories are stored in a non-differentiable cache. The choice of which memories to attend to is made using cosine similarity within each decoder layer and attention head. More precisely, our attention computation is described by:</p>
<p><span>\[
\operatorname{softmax}\left(\frac{Q(K_{R}\oplus K_{L})^{T}}{\sqrt{d}}\right) \times \left(V_{R} \oplus V_{L}\right)
\]</span></p>
<p>Where <span>\((K_{L}, V_{L})\)</span> are key-value pairs from local context, and <span>\((K_{R}, V_{R})\)</span> are key-value pairs from external memories, and <span>\(\oplus\)</span> refers to tensor concatenation. We mask the attention weights such that each query token can only attend to its own retrieved keys, and not those retrieved by previous or following query tokens. In the experiments we present below we use models trained with linear biases rather than positional encodings. When we apply these linear biases to our attention weights, we assign the same index to all retrieved memories.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>Importantly, <strong>active externalism retrieves memories exactly</strong> - it doesn’t summarize or otherwise dampen memories except through the linear biases.</p>
<p>We generate the external memories (key-value pairs) once, and then pass the representations to each decoder layer in an analogous fashion to passing previous “cached” key-values<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a>. In order to speed up the top-k cosine similarity computation we can use a vector database designed exactly for this purpose<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>
<p>We argue that this way of attending to external memories or beliefs is the natural and optimal generalization of methods like RAG, and closely mimics the kind of relationship Otto has with his notebook. The information is constantly and immediately accessible, automatically endorsed, and reliably referenced. We set a similarity threshold such that we always reference our external memories (for every generated token, within all decoder layers), but discard keys that don’t meet some low similarity threshold<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a> to avoid confusing the model with irrelevant information.</p>
<p>Active externalism is not conceptually difficult to implement, but does require getting familiar with a particular model’s implementation since details like the way key-value pairs are stored and read into the self-attention computation need to be hijacked.</p>
</section>
</section>
<section id="benchmark-results">
<h2 data-anchor-id="benchmark-results">Benchmark Results</h2>
<section id="perplexity-experiments">
<h3 data-anchor-id="perplexity-experiments">Perplexity Experiments</h3>
<p>We use perplexity as a metric for model performance. Perplexity is a measure of uncertainty of the model over each generated token, closely related to our cross-entropy loss function. For a full explanation of perplexity as a metric, we suggest checking out this excellent <a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">post</a>.</p>
<p>We show results below for perplexity experiments on the Wikitext-103 benchmark<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a> using Mosaic’s MPT-7b model. We use a stride of 512 tokens in our perplexity experiments, meaning each token is conditioned on at least 512 previous tokens, given that there are indeed 512 tokens to condition on.</p>
<p>Our active externalism method batches each sequence into chunks of increasing length (x-axis), and attends to tokens previous to the last 2048 (max sequence length) as external memories. We show results for varying k, where k is the number of memories we retrieve per query token. We compare active externalism to two baseline methods. The “truncated” baseline simply throws out any tokens previous to the last 2048 during perplexity computations, and the “naive” method which uses all input-length tokens, no matter how long the sequences become.</p>
<p>In the case of the naive method, we observe exactly the phenomenon active externalism seeks to ameliorate: after sequences exceed lengths greater than 2-3k tokens, the performance quickly drops off (in this case, perplexity blows up).</p>
<div>
<figure>
<p><img src="https://storage.googleapis.com/normal-blog-artifacts/extended-mind-transformers/naive.png"></p>
<figcaption>Perplexity results for Naive and Extended Mind MTP-7b, using a stride length of 512 tokens. Documents are batched into lengths of “Input Length” and we report average PPL on Y-Axis.</figcaption>
</figure>
</div>
<p>While we can see that active externalism provides clear benefits over simply doing local attention, in the case of the truncated benchmark. Even more exciting, perplexity continues to decrease as we increase the number of retrieved memories per query token.</p>
<div>
<figure>
<p><img src="https://storage.googleapis.com/normal-blog-artifacts/extended-mind-transformers/truncated.png"></p>
<figcaption>Perplexity results for Truncated and Extended Mind MTP-7b, using a stride length of 512 tokens. Documents are batched into lengths of “Input Length” and we report average PPL on Y-Axis.</figcaption>
</figure>
</div>
</section>
<section id="retrieval-experiments">
<h3 data-anchor-id="retrieval-experiments">Retrieval Experiments</h3>
<p>We also measure performance on retrieval benchmarks, and compare with RAG and simple baselines. Our dataset is a modified version of the recently released <a href="https://huggingface.co/datasets/abacusai/WikiQA-Free_Form_QA">Long context WikiQA benchmark</a> from Abacus.AI.</p>
<p>Our goal is to measure retrieval abilities over varying document lengths, but we also want to control for facts memorized during training, so we edit the dataset by changing the labeled answers to realistic but wrong answers. I.e, we replace every instance of “Lee Hazlewood” with “Terry Allen” in the Wikipedia entry for the song “These Boots Were Made For Walking”, and then ask the model to produce the songwriter’s name, with the <em>correct</em> answer now being “Terry Allen”.</p>
<p>Our intention is to measure the model’s ability to prioritize in context or in memory facts over those it memorized during training. Again, we feel this is an important ability if we’re asking LLMs to be reasoning agents in an evolving world.</p>
<p>In the results below, baseline receives no context at all for the question (we ask it point-blank), RAG selects the best ~2-3k tokens out of the document to include in-context<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a>, and active externalism puts the entire document in memory and uses it as Otto uses his notebook.</p>
<div>
<figure>
<p><img src="https://storage.googleapis.com/normal-blog-artifacts/extended-mind-transformers/retrieval.png"></p>
<figcaption>Retrieval Benchmark Results, by Document Length<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a></figcaption>
</figure>
</div>
<p>We see that while RAG methods drop off with input length, active externalism continues to be effective. While models finetuned to use longer contexts do currently outperform active externalism on some long-range retrieval tasks, active externalism appears to be a more effective way to do retrieval over long contexts for smaller models.</p>
<p>Where active externalism clearly outperforms RAG in large models is precisely where the model has <a href="https://arxiv.org/pdf/2205.10770.pdf">memorized before overfitting</a>. Or, the model’s weights encode factual information even as the model’s performance on test data<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a> continues to improve. Depending on your application, this could be seen as a strength or shortcoming. Certainly when we use LLMs as reasoning agents, this is a shortcoming.</p>
<p>Using active externalism also appears to eliminate some reliance on prompting. Whereas usually we’d need to include some examples of the kind of responses we hope to observe in the prompt (or use a “chat” model which has been RLHF’ed), we observe experimentally that this isn’t necessary when using active externalism.</p>
</section>
</section>
<section id="impact-on-reasoning-engine">
<h2 data-anchor-id="impact-on-reasoning-engine">Impact on reasoning engine</h2>
<p>We discuss two important consequences of active externalism on the LLM’s ability as a reasoning agent: uncertainty awareness and abstraction levers.</p>
<p>If we prompt the model with a question it’s unsure about<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a>, it may not respond in a way that’s transparent about that uncertainty. Active externalism provides a new method for revealing when a model is uncertain about its answer.</p>
<p>Let’s look at an example. We load our model easily from huggingface, and pass a paragraph from Wikipedia’s entry on Grothendieck as external memories.</p>
<div id="cb1" data-execution_count="1"><pre><code><span id="cb1-1"><a href="#cb1-1"></a><span>import</span> transformers</span>
<span id="cb1-2"><a href="#cb1-2"></a><span>from</span> transformers <span>import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a>wikipedia <span>=</span> <span>"""Alexander Grothendieck (/ˈɡroʊtəndiːk/; German pronunciation: [ˌalɛˈksandɐ ˈɡʁoːtn̩ˌdiːk] (listen); French: [ɡʁɔtɛndik]; 28 March 1928 – 13 November 2014) was a stateless (and then, since 1971, French) mathematician who became the leading figure in the creation of modern algebraic geometry.[7][8] His research extended the scope of the field and added elements of commutative algebra, homological algebra, sheaf theory, and category theory to its foundations, while his so-called "relative" perspective led to revolutionary advances in many areas of pure mathematics.[7][9] He is considered by many to be the greatest mathematician of the twentieth century.[10][11]</span></span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a><span>Grothendieck began his productive and public career as a mathematician in 1949. In 1958, he was appointed a research professor at the Institut des hautes études scientifiques (IHÉS) and remained there until 1970, when, driven by personal and political convictions, he left following a dispute over military funding. He received the Fields Medal in 1966 for advances in algebraic geometry, homological algebra, and K-theory.[12] He later became professor at the University of Montpellier[1] and, while still producing relevant mathematical work, he withdrew from the mathematical community and devoted himself to political and religious pursuits (first Buddhism and later, a more Christian vision).[13] In 1991, he moved to the French village of Lasserre in the Pyrenees, where he lived in seclusion, still working tirelessly on mathematics and his philosophical and religious thoughts until his death in 2014.[14]</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span>"""</span></span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a>tokenizer <span>=</span> AutoTokenizer.from_pretrained(<span>'EleutherAI/gpt-neox-20b'</span>)</span>
<span id="cb1-10"><a href="#cb1-10"></a>memory_ids <span>=</span> tokenizer(wikipedia, return_tensors<span>=</span><span>'pt'</span>)[<span>'input_ids'</span>]</span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a>model <span>=</span> AutoModelForCausalLM.from_pretrained(<span>"normalcomputing/extended-mind-mpt-7b"</span>, external_memories<span>=</span>memory_ids, trust_remote_code<span>=</span><span>True</span>)</span></code></pre></div>
<p>Now, let’s ask the model a question we know is answered (albeit a little obscurely) in the above paragraph without using active externalism. We can achieve this by setting the parameter <code>model.use_active_externalism = False</code> or simply passing <code>topk=0</code>. Hint: the correct answer is 1971.</p>
<div data-execution_count="3">
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1"></a>prompt <span>=</span> <span>"When did Alexander Grothendieck get his French citizenship?"</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>input_ids <span>=</span> tokenizer(prompt, return_tensors<span>=</span><span>'pt'</span>)[<span>'input_ids'</span>]</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>50</span>, topk<span>=</span><span>0</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a><span>print</span>(<span>'Baseline Generation: '</span>, tokenizer.decode(out[<span>0</span>]))</span></code></pre></div>
<div>
<pre><code>Baseline Generation:  When did Alexander Grothendieck get his French citizenship?
I am trying to find out when Alexander Grothendieck got his French citizenship. I know that he was born in Germany and that he got his French citizenship in the late 1950s. I am trying to find out when he got his</code></pre>
</div>
</div>
<p>Now let’s enable active externalism, slowly cranking up the number of memories each query token is allowed to attend to using the <code>topk</code> parameter.</p>
<div data-execution_count="4">
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>15</span>, topk<span>=</span><span>5</span>)</span>
<span id="cb4-2"><a href="#cb4-2"></a><span>print</span>(<span>'Generation for k=5: '</span>, tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>15</span>, topk<span>=</span><span>6</span>)</span>
<span id="cb4-5"><a href="#cb4-5"></a><span>print</span>(<span>'Generation for k=6: '</span>,tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>20</span>, topk<span>=</span><span>7</span>)</span>
<span id="cb4-8"><a href="#cb4-8"></a><span>print</span>(<span>'Generation for k=7: '</span>,tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>15</span>, topk<span>=</span><span>8</span>)</span>
<span id="cb4-11"><a href="#cb4-11"></a><span>print</span>(<span>'Generation for k=8: '</span>,tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb4-12"><a href="#cb4-12"></a></span>
<span id="cb4-13"><a href="#cb4-13"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>20</span>, topk<span>=</span><span>30</span>)</span>
<span id="cb4-14"><a href="#cb4-14"></a><span>print</span>(<span>'Generation for k=30: '</span>,tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span></code></pre></div>
<div>
<pre><code>Generation for k=5:  A: I think he got it in the early 1960s.
Generation for k=6:  A: I think he got it in the early 1970s.
Generation for k=7:  A: He was born in France, and he was naturalized in 1971.
&lt;|endoftext|&gt;
Generation for k=8:  A: I think he got it in 1971.
&lt;|endoftext|&gt;Q
Generation for k=30:  A: He was born in Germany, and became a French citizen in 1971.</code></pre>
</div>
</div>
<p>Not only did the model produce the correct answer, but it also expressed increasing certainty about its answer. This evolution of generations signals the model’s original uncertainty.</p>
<p>In cases where the model is certain about the answer, the generations are stable as we increase k over the external context.</p>
<div data-execution_count="5">
<div id="cb6"><pre><code><span id="cb6-1"><a href="#cb6-1"></a>prompt <span>=</span> <span>"What was did Alexander Grothendieck's profession?"</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>input_ids <span>=</span> tokenizer(prompt, return_tensors<span>=</span><span>'pt'</span>)[<span>'input_ids'</span>]</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>25</span>, topk<span>=</span><span>0</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a><span>print</span>(<span>'Baseline Generation: '</span>, tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>15</span>, topk<span>=</span><span>2</span>)</span>
<span id="cb6-8"><a href="#cb6-8"></a><span>print</span>(<span>'Generation for k=2: '</span>, tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>15</span>, topk<span>=</span><span>8</span>)</span>
<span id="cb6-11"><a href="#cb6-11"></a><span>print</span>(<span>'Generation for k=8: '</span>, tokenizer.decode(out[<span>0</span>][input_ids.size(<span>-</span><span>1</span>):]).strip())</span></code></pre></div>
<div>
<pre><code>Baseline Generation:  What was did Alexander Grothendieck's profession?
Alexander Grothendieck was a French mathematician
Generation for k=2:  Alexander Grothendieck was a mathematician.

What
Generation for k=8:  A: He was a mathematician.
&lt;|endoftext|&gt;Q: What</code></pre>
</div>
</div>
<p>A natural extension of this principle might look like the development of a metric based on similarity or attention weight which could communicate this uncertainty in a more compact form, work currently under development at Normal.</p>
<p>The parameter <code>topk</code> also serves as a useful lever for the level of abstraction in the model’s output. E.g., the extent to which we’d like the model to synthesize the memories vs.&nbsp;quote verbatim from the source. We see this clearly in question-answering tasks over code. We show an example using the chat model here, which is best equipped to handle more free form question-answering tasks.</p>
<div id="cb8" data-execution_count="6"><pre><code><span id="cb8-1"><a href="#cb8-1"></a>code_snippet <span>=</span> <span>"""def sieve_of_eratosthenes(limit):</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span>    sieve = [True] * (limit + 1)</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span>    sieve[0] = sieve[1] = False</span></span>
<span id="cb8-4"><a href="#cb8-4"></a><span>    primes = []</span></span>
<span id="cb8-5"><a href="#cb8-5"></a><span>    </span></span>
<span id="cb8-6"><a href="#cb8-6"></a><span>    for current in range(2, int(limit**0.5) + 1):</span></span>
<span id="cb8-7"><a href="#cb8-7"></a><span>        if sieve[current]:</span></span>
<span id="cb8-8"><a href="#cb8-8"></a><span>            primes.append(current)</span></span>
<span id="cb8-9"><a href="#cb8-9"></a><span>            for multiple in range(current*current, limit + 1, current):</span></span>
<span id="cb8-10"><a href="#cb8-10"></a><span>                sieve[multiple] = False</span></span>
<span id="cb8-11"><a href="#cb8-11"></a><span>    </span></span>
<span id="cb8-12"><a href="#cb8-12"></a><span>    for num in range(int(limit**0.5) + 1, limit + 1):</span></span>
<span id="cb8-13"><a href="#cb8-13"></a><span>        if sieve[num]:</span></span>
<span id="cb8-14"><a href="#cb8-14"></a><span>            primes.append(num)</span></span>
<span id="cb8-15"><a href="#cb8-15"></a><span>    </span></span>
<span id="cb8-16"><a href="#cb8-16"></a><span>    return primes</span></span>
<span id="cb8-17"><a href="#cb8-17"></a><span>"""</span></span>
<span id="cb8-18"><a href="#cb8-18"></a>tokenizer <span>=</span> AutoTokenizer.from_pretrained(<span>'EleutherAI/gpt-neox-20b'</span>)</span>
<span id="cb8-19"><a href="#cb8-19"></a>memory_ids <span>=</span> tokenizer(code_snippet, return_tensors<span>=</span><span>'pt'</span>)[<span>'input_ids'</span>]</span>
<span id="cb8-20"><a href="#cb8-20"></a></span>
<span id="cb8-21"><a href="#cb8-21"></a>model <span>=</span> AutoModelForCausalLM.from_pretrained(<span>"normalcomputing/extended-mind-mpt-7b-chat"</span>, external_memories<span>=</span>memory_ids, trust_remote_code<span>=</span><span>True</span>)</span></code></pre></div>
<p>We ask the model to recall what our function does, first with a small <code>topk</code>.</p>
<div data-execution_count="8">
<div id="cb9"><pre><code><span id="cb9-1"><a href="#cb9-1"></a>prompt <span>=</span>  <span>"What does the function sieve_of_eratosthenes do?"</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>input_ids <span>=</span> tokenizer(prompt, return_tensors<span>=</span><span>'pt'</span>)[<span>'input_ids'</span>]</span>
<span id="cb9-3"><a href="#cb9-3"></a></span>
<span id="cb9-4"><a href="#cb9-4"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>100</span>, topk<span>=</span><span>2</span>)</span>
<span id="cb9-5"><a href="#cb9-5"></a><span>print</span>(tokenizer.decode(out[<span>0</span>]))</span></code></pre></div>
<div>
<pre><code>What does the function sieve_of_eratosthenes do?
The function sieve_of_eratosthenes is a Python function that implements the Sieve of Eratosthenes algorithm to generate all prime numbers up to a given limit.

The Sieve of Eratosthenes is a simple algorithm that generates all prime numbers up to a given limit. It works by creating a list of all integers from 2 to the given limit, and then iteratively marking the multiples of each prime number as composite (not prime).</code></pre>
</div>
</div>
<p>We see that with a small <code>topk</code> the model abstracts away the details from the code, providing a natural language description of what the code does. Now let’s try with a larger <code>topk</code>.</p>
<div data-execution_count="9">
<div id="cb11"><pre><code><span id="cb11-1"><a href="#cb11-1"></a>out <span>=</span> model.generate(input_ids, max_length<span>=</span>input_ids.size(<span>-</span><span>1</span>)<span>+</span><span>100</span>, topk<span>=</span><span>14</span>)</span>
<span id="cb11-2"><a href="#cb11-2"></a><span>print</span>(tokenizer.decode(out[<span>0</span>]))</span></code></pre></div>
<div>
<pre><code>What does the function sieve_of_eratosthenes do?(limit):
        primes.append(True)
        for i in range(2, int(limit**0.5) + 1):
            if sieve[i]:
                break
        else:
            for i in range(2, int(limit**0.5) + 1):
                if i % 2 == 0:
                    sieve[i] = False
    
    return primes
```

This implementation of the S</code></pre>
</div>
</div>
<p>Now the model outputs much closer to verbatim code, while abstracting away some variable names. This is the kind of nuanced stylistic choice is very hard to achieve using naive prompting and RAG methods without developing many point solutions specific to the data and prompt. More importantly, this kind of experiment gives us small clues into how the model actually reasons over these key-value pairs. At Normal, we hope to combine work on mechanistic interpretability methods with extended mind transformers, building a unified system for understanding how models store facts and reason over them.</p>
</section>
<section id="explainability">
<h2 data-anchor-id="explainability">Explainability</h2>
<p>Clark and Chalmers write in their paper: “By embracing an active externalism, we allow a more natural explanation of all sorts of actions”, and indeed this is true for our active externalism as well. Using attention weights, we can highlight which memories were used during each generation step. Here we highlight the memories used when generating the correct token “1971”. Since we retrieve memories per layer, per head, we display the mode.</p>
<div>
<figure>
<p><img src="https://storage.googleapis.com/normal-blog-artifacts/extended-mind-transformers/explainability.png"></p>
<figcaption>Tokens retrieved during the generation of token “1971”</figcaption>
</figure>
</div>
<p>Simple methods like this are just the beginning, but granular citations, in fact causal citations at all, are currently impossible using methods like RAG. The best we can get is highlighting those sections that were chosen to include in context. Using self-attention weights can perhaps buy you something, but this is unwieldy data and it’s explanatory power has been <a href="https://arxiv.org/abs/1902.10186">questioned</a>.</p>
</section>
<section id="creating-external-memories">
<h2 data-anchor-id="creating-external-memories">Creating external memories</h2>
<p>There are many interesting hyperparameters to discuss related to active externalism. Alternative masking strategies, restricting active externalism to some subset of decoder layers, and evaluating the role model size plays are all important discussions. We leave most of the discussion for more technical forthcoming papers. But we felt it was important to mention briefly the hyperparameters used in generating the external memories. We create our external memories (at each layer) by passing those external contexts through our model, just like inference. Then we save the internal representations the model generated, and attend to them later. If our external memories are longer than the model’s maximum sequence length, we’ll usually want to generate our representations using a stride. This ensures that all tokens are conditioned on at least stride-length number of previous tokens. Intuitively, all our memories will have “seen” some reasonable amount of context. However, there are situations where increased context may not be aligned with the model’s <em>best</em> representation of the data. For instance, representations of numerical or log-type data may benefit from using a smaller sequence or stride length.</p>
</section>
<section id="summary">
<h2 data-anchor-id="summary">Summary</h2>
<p>At Normal, we believe that there remains a wealth of opportunity to uncover by approaching today’s fractured, albeit proliferative, Enterprise AI landscape from a first principles point of view – even, and arguably especially, where early consensus has begun to form. We strongly believe that interdisciplinary perspectives and research are essential for advancing the field, a fundamentally and historically cross-sectional and constantly evolving discipline.</p>
<p>In “The Extended Mind” Clark and Chalmers conjecture: “In the distant future we may be able to plug various modules into our brain to help us out: a module for extra short-term memory when we need it.”</p>
<p>While this remains a distant goal for humans, we propose a method for achieving exactly this kind of short-term memory boost for LLMs. We’ve shown how a simple and natural extension of the self-attention mechanism for LLMs enables SoTa performance on retrieval tasks over long documents, uncertainty awareness, abstraction levers, granular explainability, and perhaps even given us some insight into the way these models reason internally.</p>
</section>
<section id="whats-next">
<h2 data-anchor-id="whats-next">What’s next</h2>
<p>We’re excited to extend these methods to models that use rotary and relative position encodings.</p>
<p>Making causal citations an out-of-the-box feature is also high on our list.</p>
<p>Distilling the information from the joint evolution of generations and choices of k into an uncertainty metric is another area we’re investing in.</p>
<p>Finally, continuing to develop and run comprehensive benchmarks will be crucial for building a robust understanding of the benefits provided by active externalism.</p>
</section>
<section id="references">
<h2 data-anchor-id="references">References</h2>
<div id="refs" role="list">

<p>
Burtsev, Mikhail S., Yuri Kuratov, Anton Peganov, and Grigory V. Sapunov. 2021. <span>“Memory Transformer.”</span> <a href="https://arxiv.org/abs/2006.11527">https://arxiv.org/abs/2006.11527</a>.
</p>
<div id="ref-clark-chalmers" role="listitem"><p>
Clark, Andy, and David Chalmers. 1998. <span>“The Extended Mind.”</span> <em>Analysis 58</em>, no. 1: 7–19. <a href="http://www.jstor.org/stable/3328150">http://www.jstor.org/stable/3328150</a>.
</p></div>
<p>
Liu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. <span>“Lost in the Middle: How Language Models Use Long Contexts.”</span> <a href="https://arxiv.org/abs/2307.03172">https://arxiv.org/abs/2307.03172</a>.
</p>
<p>
Martins, Pedro Henrique, Zita Marinho, and André F. T. Martins. 2022. <span>“<span>\(\infty\)</span>-Former: Infinite Memory Transformer.”</span> <a href="https://arxiv.org/abs/2109.00301">https://arxiv.org/abs/2109.00301</a>.
</p>
<p>
Press, Ofir, Noah A. Smith, and Mike Lewis. 2022. <span>“Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation.”</span> <a href="https://arxiv.org/abs/2108.12409">https://arxiv.org/abs/2108.12409</a>.
</p>
<p>
Sukhbaatar, Sainbayar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. 2019. <span>“Augmenting Self-Attention with Persistent Memory.”</span> <a href="https://arxiv.org/abs/1907.01470">https://arxiv.org/abs/1907.01470</a>.
</p>
<p>
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention Is All You Need.”</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</p>
<p>
Wu, Yuhuai, Markus N. Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. <span>“Memorizing Transformers.”</span> <a href="https://arxiv.org/abs/2203.08913">https://arxiv.org/abs/2203.08913</a>.
</p>
</div>


</section>


<div id="quarto-appendix"><section id="footnotes" role="doc-endnotes"><h2>Footnotes</h2>

<ol>
<li id="fn1"><p>Indeed, retrieval has thus far become a <a href="https://www.sequoiacap.com/article/generative-ai-act-two/">table stakes</a> part of the modeling stack for building LLM apps.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><strong>RAG</strong>, a popular method for tackling the short context length of LLMs in application settings, attempts to identify the most salient information in a long text for a given query or task, such that the long context can be cut down to “fit in memory”. This is accomplished using a choice of sentence embedding that’s usually external to the model, chunking the long text and comparing with the query vector using a similarity or distance metric. Many <a href="https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/[https://python.langchain.com/docs/integrations/retrievers]">open sourced projects</a> have made implementing such a strategy easier, and the success of <a href="https://www.forbes.com/sites/adrianbridgwater/2023/05/19/the-rise-of-vector-databases/?sh=4472652914a6">“vector databases”</a> demonstrates the rapid adoption of such methods.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Although there’s no technical reason we can’t throw an arbitrarily long sequence into context, performance using today’s models will drop off quickly after we exceed the sequence length the model saw during training. This inability to generalize is largely due to the use of positional embeddings. While originally (in <span data-cites="vaswani2023attention">Vaswani et al. (<a href="#ref-vaswani2023attention" role="doc-biblioref">2023</a>)</span>) only applied once at the beginning of the encoder/decoder stack, in today’s GPT-style transformers positional encodings are usually incorporated at the bottom of each decoder layer. These are unique constants which are either added or multiplied to hidden states in order to encode the index of each token in the sequence. Unless the model is trained further to expect a wider range of positional values, these new tokens quickly become out of distribution. Even given an infinitely long context, faithfully retrieving facts from very long sequences remains a challenge. Recent experiments show that models still struggle to use all the information provided in the larger context window - often forgetting things in the middle in particular, as they show in <span data-cites="liu2023lost">Liu et al. (<a href="#ref-liu2023lost" role="doc-biblioref">2023</a>)</span>.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The architecture described in <span data-cites="martins2022inftyformer">Martins, Marinho, and Martins (<a href="#ref-martins2022inftyformer" role="doc-biblioref">2022</a>)</span> continuously compresses long text inputs such that the text always fits in memory. This has the obvious advantage of supporting input sequences of “infinite” length, but the weakness of summarizing the past such that it necessarily contains less detail. A coarse-grained/RAG analog to this might be using the language model itself to iteratively summarize past inputs and then passing the summary into context. In <span data-cites="sukhbaatar2019augmenting">Sukhbaatar et al. (<a href="#ref-sukhbaatar2019augmenting" role="doc-biblioref">2019</a>)</span>, the authors suggest replacing the feed-forward mechanism in each decoder layer with another attention block, and interpret this “unified mechanism” as an aggregation of global and contextual information. The creative contributors in <span data-cites="burtsev2021memory">Burtsev et al. (<a href="#ref-burtsev2021memory" role="doc-biblioref">2021</a>)</span> propose introducing a <code>[mem]</code> token which they hope the model will learn to leverage as space for storing global information. They implement various decoder architectures which attempt to enforce this with varying strictness. Folks at <a href="https://www.mosaicml.com/blog/mpt-7b">Mosaic</a> have combatted the lack of generalizing position encodings by using attention with linear biases (as presented by <span data-cites="press2022train">Press, Smith, and Lewis (<a href="#ref-press2022train" role="doc-biblioref">2022</a>)</span>).<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span data-cites="clark-chalmers">Clark and Chalmers (<a href="#ref-clark-chalmers" role="doc-biblioref">1998</a>)</span><a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>While the authors of this paper believe the model needs to be trained from scratch or at least fine-tuned to be able to make sense of the extra retrieved tokens, we show that using models trained with ALiBi can make sense of these external key-value pairs innately. While they use a non-differentiable cache on one layer, we cache on every decoder layer.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>I.e., the model interprets those retrieved memories as being some constant distance away from the tokens it considers local context. For simplicity’s sake, we choose this constant index to be that directly following the last in-context index. I.e. if we pass the model a sequence of 1200 tokens, the memories in context will all be assigned position 1201. Certainly there’s room to experiment here - for instance you might choose to bias weights closer to the beginning of the memories more than those toward the end - but we find this is a reasonable and effective choice. We hypothesize that these methods will be effective for models trained with relative positional encodings as well, and will pursue this end in future work.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>a popular mechanism for speeding up inference, as a GPT-style transformer’s output only depends on the previous inputs<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>We support using <a href="https://github.com/facebookresearch/faiss">FAISS</a> in our implementation<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>We find .25 to be a good choice.<a href="#fnref10" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>https://developer.ibm.com/exchanges/data/all/wikitext-103/<a href="#fnref11" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>We use OpenAI’s Ada embeddings, and chunk our document into sequences of 500 tokens with no overlap. We order the documents such that the most similar content is closest to the prompt.<a href="#fnref12" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Each split has on average 200 samples, with more samples in the 2k split and fewer as documents become longer.<a href="#fnref13" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Usually, as measured by cross-entropy<a href="#fnref14" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>Unsure in an epistemic way, i.e.&nbsp;the model didn’t observe this fact during training/can’t infer from the context<a href="#fnref15" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section><h2>Reuse</h2></section></div></main> <!-- /main -->


</div></div>]]></description>
        </item>
    </channel>
</rss>