<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 22 Mar 2025 20:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[California AG Rob Bonta Urgently Issues Consumer Alert for 23andMe Customers (159 pts)]]></title>
            <link>https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers</link>
            <guid>43447421</guid>
            <pubDate>Sat, 22 Mar 2025 17:55:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers">https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers</a>, See on <a href="https://news.ycombinator.com/item?id=43447421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="content:encoded"><p><em>Californians have the right to direct the company to delete their genetic data</em>&nbsp;</p>
<p><b>OAKLAND</b>&nbsp;— California Attorney General Rob Bonta today issued a consumer alert to customers of 23andMe, a genetic testing and information company. The California-based company has publicly reported that it is in financial distress and&nbsp;stated in securities filings that there is substantial doubt about its ability to continue as a going concern.&nbsp;Due to the trove of sensitive consumer data 23andMe has amassed, Attorney General Bonta reminds Californians of their right to&nbsp;direct&nbsp;the deletion of their genetic data under the Genetic Information Privacy Act (GIPA) and California Consumer Protection Act (CCPA).&nbsp;Californians who want to invoke these rights can do so by going to 23andMe's website.&nbsp;</p>
<p>“California has robust privacy laws that allow consumers to take control and request that a company delete their genetic data,”&nbsp;<b>said Attorney General Bonta.</b>&nbsp;“Given 23andMe’s reported financial distress, I remind&nbsp;Californians to consider invoking their rights and directing 23andMe to delete their data and destroy any samples of genetic material held by the company.”&nbsp;</p>
<p><b>To Delete Genetic Data from 23andMe:</b></p>
<ol>
<li>Consumers can delete their account and personal information by taking the following steps:</li>
<li>Log into your 23andMe account on their website.&nbsp;</li>
<li>Go to the “Settings” section of your profile.</li>
<li>Scroll to a section labeled “23andMe Data” at the bottom of the page.&nbsp;</li>
<li>Click “View” next to “23andMe Data”</li>
<li>Download your data: If you want a copy of your genetic data for personal storage, choose the option to download it to your device before proceeding.</li>
<li>Scroll to the “Delete Data” section.&nbsp;</li>
<li>Click “Permanently Delete Data.”&nbsp;</li>
<li>Confirm your request:&nbsp;You’ll receive an email from 23andMe; follow the link in the email to confirm your deletion request.</li>
</ol>
<p><b>To Destroy Your 23andMe Test Sample:</b></p>
<p>If you previously opted to have your saliva sample and DNA stored by 23andMe, but want to change that preference, you can do so from your account settings page, under “Preferences.”</p>
<p><b>To Revoke Permission for Your Genetic Data to be Used for Research:</b></p>
<p>If you previously consented to 23andMe and third-party researchers to use your genetic data and sample for research, you may withdraw consent from the account settings page, under “Research and Product Consents.”</p>
<p>Under GIPA, California consumers can delete their account and genetic data and have their biological sample destroyed.&nbsp;In addition, GIPA permits California consumers to revoke consent that they provided a genetic testing company to collect, use, and disclose genetic data and to store biological samples after the initial testing has been completed.&nbsp;The CCPA also vests California consumers with the right to delete personal information, which includes genetic data, from businesses that collect personal information from the consumer. &nbsp;&nbsp;</p>
<p>To learn more about the CCPA, please visit&nbsp;<a href="https://oag.ca.gov/privacy/ccpa">here</a>.&nbsp;&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook to stop targeting ads at UK woman after legal fight (110 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/c1en1yjv4dpo</link>
            <guid>43446821</guid>
            <pubDate>Sat, 22 Mar 2025 16:22:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/c1en1yjv4dpo">https://www.bbc.co.uk/news/articles/c1en1yjv4dpo</a>, See on <a href="https://news.ycombinator.com/item?id=43446821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header data-component="headline-block"></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 976w" type="image/webp"><img alt="A woman with blonde hair, blue eyes and pink lipstick stares at the camera" src="https://ichef.bbci.co.uk/ace/standard/1200/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 976w" width="1200" height="675"></picture></span><span role="text"><span>Image source, </span>Tanya O'Carroll</span></p><figcaption><span>Image caption, </span><p>Facebook has agreed to stop targeting adverts at Tanya O'Carroll after she filed a lawsuit against its parent company</p></figcaption></figure></div><div data-component="text-block"><p><b>Facebook has agreed to stop targeting adverts at an individual user using personal data after she filed a lawsuit against its parent company, tech giant Meta.</b></p><p>Tanya O'Carroll, 37, who lives in London and works in the tech policy and human rights sector, said it would open a "gateway" for other people wanting to stop the social media company from serving them adverts based on their demographics and interests.</p><p>The Information Commissioner's Office, the UK's data watchdog, said online targeted advertising should be considered direct marketing.</p><p>In a statement, Meta said it provided "robust settings and tools for users to control their data and advertising preferences".</p></div><div data-component="text-block"><p>Ms O'Carroll, who created her Facebook account about 20 years ago, filed a lawsuit against Meta in 2022, asking it to stop using her personal data to fill her social media feeds with targeted adverts based on topics it thought she was interested in.</p><p>"I knew that this kind of predatory, invasive advertising is actually something that we all have a legal right to object to," Ms O'Carroll told Radio 4's Today Programme. </p><p>"I don't think we should have to accept these unfair terms where we consent to all that invasive data tracking and surveillance."</p><p>It was when she found out she was pregnant in 2017 that she realised the extent to which Facebook was targeting adverts at her.</p><p>She said the adverts she got "suddenly started changing within weeks to lots of baby photos and other things - ads about babies and pregnancy and motherhood".</p><p>"I just found it unnerving - this was before I'd even told people in my private life, and yet Facebook had already determined that I was pregnant," she continued.</p></div><div data-component="text-block"><p>General Data Protection Regulation (GDPR) legislation controls how personal information is used by organisations.</p><p>Ms O'Carroll's lawsuit argued that Facebook's targeted advertising system was covered by the UK's definition of direct marketing, giving individuals the right to object.</p><p>Meta said that adverts on its platform could only be targeted to groups of a minimum size of 100 people, rather than individuals, so did not count as direct marketing. But the Information Commissioner's Office (ICO) disagreed.</p><p>"Organisations must respect people's choices about how their data is used," a spokesperson for the ICO said. "This means giving users a clear way to opt out of their data being used in this way."</p><p>Ms O'Carroll said that Meta had agreed to stop using her personal data for direct marketing purposes, "which in non-legalese means I've essentially been able to turn off all the creepy, invasive, targeted ads on Facebook".</p><p>She said that she did not want to stop using Facebook, saying that it is "filled with all of those connections and family and friends, and entire chapters of my life".</p></div><div data-component="text-block"><p>Ms O'Carroll said she hoped her individual settlement would make it easier for others who wanted Facebook to stop giving them targeted adverts.</p><p>"If other people want to exercise their right, I believe they now have a gateway to do so knowing that the UK regulator will back them up," she said.</p><p>Meta said it disagreed with Ms O'Carroll's claims, adding "no business can be mandated to give away its services for free."</p><p>A spokesperson added: "Facebook and Instagram cost a significant amount of money to build and maintain, and these services are free for British consumers because of personalised advertising."</p><p>"Our services support British jobs and economic growth by connecting businesses with the people most likely to buy their products, while enabling universal access to online services regardless of income. We will continue to defend its value while upholding user choice and privacy."</p><p>Facebook and Instagram have a <a href="https://www.bbc.co.uk/news/technology-67226394">subscription service</a> in most of Europe, where users can pay monthly so that they don't get ads on the platform.</p><p>The Meta spokesperson said the company was "exploring the option" of offering a similar service to UK users and would "share further information in due course."</p></div><section data-component="links-block"><p><h2 type="normal">More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon wants a product safety regulator declared unconstitutional (126 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/</link>
            <guid>43446103</guid>
            <pubDate>Sat, 22 Mar 2025 14:56:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/">https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/</a>, See on <a href="https://news.ycombinator.com/item?id=43446103">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[PyTorch Internals: Ezyang's Blog (198 pts)]]></title>
            <link>https://blog.ezyang.com/2019/05/pytorch-internals/</link>
            <guid>43445931</guid>
            <pubDate>Sat, 22 Mar 2025 14:39:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ezyang.com/2019/05/pytorch-internals/">https://blog.ezyang.com/2019/05/pytorch-internals/</a>, See on <a href="https://news.ycombinator.com/item?id=43445931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<!-- -*- mode: rst -*- -->
<p>This post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-01.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-01.png"></p></div>
<p>Hi everyone!  Today I want to talk about the internals of <a href="https://pytorch.org/">PyTorch</a>.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-02.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-02.png"></p></div>
<p>This talk is for those of you who have used PyTorch, and thought to yourself, "It would be great if I could contribute to PyTorch," but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a "tensor library that supports automatic differentiation", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-03.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-03.png"></p></div>
<p>The talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of "extension points", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.</p>
<p>The second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-04.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-04.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-05.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-05.png"></p></div>
<p>The tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-06.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-06.png"></p></div>
<p>There's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-07.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-07.png"></p></div>
<p>A tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.</p>
<p>So, what do strides have to do with this picture?</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-08.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-08.png"></p></div>
<p>Suppose that I want to access the element at position <tt>tensor[1, 0]</tt> in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.</p>
<p>(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)</p>
<p>Strides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-09.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-09.png"></p></div>
<p>Using advanced indexing support, I can just write <tt>tensor[1, :]</tt> to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)</p>
<!--  -->
<blockquote>
<p>Question from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?</p>
<p>Answer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they <a href="https://stackoverflow.com/questions/14161050/java-string-substring-method-potential-memory-leak">fixed this in Java 7u6</a>.</p>
</blockquote>
<p>A more interesting case is if I want to take the first column:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-10.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-10.png"></p></div>
<p>When we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a "stride": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)</p>
<p>The stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the <a href="https://ezyang.github.io/stride-visualizer/index.html">Stride Visualizer</a>.</p>
<p>Let's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-11.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-11.png"></p></div>
<p>There may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.</p>
<p>One thing to realize is that there is always a pair of Tensor-Storage, even for "simple" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with <tt>torch.zeros(2, 2)</tt>).</p>
<!--  -->
<blockquote>
By the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.</blockquote>
<hr>
<p>We've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call <tt>torch.mm</tt>, two dispatches happen:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-12.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-12.png"></p></div>
<p>The first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a <em>dynamic</em> dispatch because these kernels may live in separate libraries (e.g., <tt>libcaffe2.so</tt> versus <tt>libcaffe2_gpu.so</tt>), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.</p>
<p>The second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on <tt>float</tt> is different from the code for <tt>int</tt>.  It stands to reason you need separate kernels for each dtype.</p>
<p>This is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-13.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-13.png"></p></div>
<p>Since we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-14.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-14.png"></p></div>
<p>Our current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:</p>
<ul>
<li>The <strong>device</strong>, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.</li>
<li>The <strong>layout</strong>, which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.</li>
<li>The <strong>dtype</strong>, which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.</li>
</ul>
<p>If you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in <em>principle</em> the combination could make sense, and thus we support expressing it, at the very least.</p>
<p>There's one last way you can make an "extension" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.</p>
<p>When should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-15.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-15.png"></p></div>
<p>Our philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like "What's the size of a tensor?" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the <a href="https://github.com/pytorch/pytorch/blob/master/c10/core/TensorImpl.h">TensorImpl struct</a>),  what we see is a common prefix of all fields that we consider all "tensor"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are <em>so</em> important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-16.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-16.png"></p></div>
<p>I told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)</p>
<p>What does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-17.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-17.png"></p></div>
<p>...and fill in the missing code that actually computes the gradients of your network:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-18.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-18.png"></p></div>
<p>Take a moment to study this diagram.  There's a lot to unpack; here's what to look at:</p>
<ol>
<li>First, rest your eyes on the variables in red and blue.  PyTorch implements <a href="https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation">reverse-mode automatic differentiation</a>, which means that we effectively walk the forward computations "backward" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute <tt>loss</tt>; then, the first thing we do in the blue part of the program is compute <tt>grad_loss</tt>.  <tt>loss</tt> was computed from <tt>next_h2</tt>, so we compute <tt>grad_next_h2</tt>.  Technically, these variables which we call <tt>grad_</tt> are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them <tt>grad</tt> and mostly everyone knows what we mean.</li>
<li>If the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the <tt>tanh</tt> operation is translated into a <tt>tanh_backward</tt> operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced <tt>next_h2</tt>, the backward operation takes <tt>grad_next_h2</tt> as an input.</li>
</ol>
<p>The whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-19.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-19.png"></p></div>
<p>To do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls <tt>loss.backward()</tt> in their PyTorch script.</p>
<!--  -->
<blockquote>
This is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a <a href="https://github.com/pytorch/pytorch/issues/13638">Variable-Tensor merge in C++</a>, following a simple merge which happened to PyTorch's frontend interface.</blockquote>
<p>We also have to update our picture about dispatch:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-20.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-20.png"></p></div>
<p>Before we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.</p>
<p>Some implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)</p>
<hr>
<p>In my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-21.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-21.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-22.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-22.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-23.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-23.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-24.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-24.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-25.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-25.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-26.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-26.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-27.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-27.png"></p></div>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-28.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-28.png"></p></div>
<p>Enough about concepts, let's look at some code.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-29.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-29.png"></p></div>
<p>PyTorch has a lot of folders, and there is a very detailed description of what they are in the <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#codebase-structure">CONTRIBUTING</a> document, but really, there are only four directories you really need to know about:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-30.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-30.png"></p></div>
<ul>
<li>First, <tt>torch/</tt> contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...</li>
<li><tt>torch/csrc/</tt>, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.</li>
<li><tt>aten/</tt>, short for "A Tensor Library" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the "native" operators, which are modern, C++ implementations of operators, and the "legacy" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.</li>
<li><tt>c10/</tt>, which is a pun on Caffe2 and A"Ten" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.</li>
</ul>
<p>That's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in <tt>aten</tt>.</p>
<p>Let's see how this separation of code breaks down in practice:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-31.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-31.png"></p></div>
<p>When you call a function like <tt>torch.add</tt>, what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:</p>
<ol>
<li>We have to translate from Python realm to the C++ realm (Python argument parsing)</li>
<li>We handle <strong>variable</strong> dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)</li>
<li>We handle <strong>device type / layout</strong> dispatch (Type)</li>
<li>We have the actual kernel, which is either a modern native function, or a legacy TH function.</li>
</ol>
<p>Each of these steps corresponds concretely to some code.  Let's cut our way through the jungle.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-32.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-32.png"></p></div>
<p>Our initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like <tt>torch._C.VariableFunctions.add</tt>.  <tt>THPVariable_add</tt> is the implementation of one such implementation.</p>
<p>One important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class <tt>PythonArgParser</tt> to actually pull out C++ objects out of the Python <tt>args</tt> and <tt>kwargs</tt>; we then call a <tt>dispatch_add</tt> function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor <tt>self</tt>.  On its way back, we rewrap the returned <tt>Tensor</tt> back into a <tt>PyObject</tt>.</p>
<p>(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-33.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-33.png"></p></div>
<p>When we call the <tt>add</tt> method on the <tt>Tensor</tt> class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a "Type" object.  This method is the actual virtual method (this is why I say Type is just a "gadget" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named <tt>TypeDefault</tt>.  This happens to be because we have an implementation of <tt>add</tt> that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like <tt><span>CPUFloatType::add</span></tt>.  It is this implementation of the virtual method that finally gets us to the actual kernel code.</p>
<!--  -->
<blockquote>
Hopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing <tt>Type</tt> dispatch with another mechanism which will help us better support PyTorch on mobile.</blockquote>
<p>It's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-34.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-34.png"></p></div>
<p>It's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-35.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-35.png"></p></div>
<p>PyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-36.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-36.png"></p></div>
<p>We generally think of a kernel in PyTorch consisting of the following parts:</p>
<ol>
<li>First, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.</li>
<li>Once you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)</li>
<li>Next, we generally have to allocate the result tensor which we are going to write the output into.</li>
<li>Time for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)</li>
<li>Most performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are "implicitly" parallelized, since their programming model is built on top of massive parallelization).</li>
<li>Finally, you need to access the data and do the computation you wanted to do!</li>
</ol>
<p>In the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-37.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-37.png"></p></div>
<p>To take advantage of all of the code generation which PyTorch brings, you need to write a <em>schema</em> for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the <a href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md">README in native</a> is for more information about this format.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-38.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-38.png"></p></div>
<p>You also may need to define a derivative for your operation in <a href="https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml">derivatives.yaml</a>.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-39.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-39.png"></p></div>
<p>Error checking can be done by way of either a low level or a high level API.  The low level API is just a macro, <tt>TORCH_CHECK</tt>, which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of <tt>operator&lt;&lt;</tt>, and most important data types in PyTorch have <tt>operator&lt;&lt;</tt> implementations.</p>
<p>The high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each <tt>Tensor</tt> into a <tt>TensorArg</tt>, which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g., <tt>checkDim()</tt> tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the <tt>TensorArg</tt> metadata.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-40.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-40.png"></p></div>
<p>One important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write <em>three</em> operators: <tt>abs_out</tt>, which operates on a preallocated output (this implements the <tt>out=</tt> keyword argument), <tt>abs_</tt>, which operates inplace, and <tt>abs</tt>, which is the plain old functional version of an operator.</p>
<p>Most of the time, <tt>abs_out</tt> is the real workhorse, and <tt>abs</tt> and <tt>abs_</tt> are just thin wrappers around <tt>abs_out</tt>; but sometimes writing specialized implementations for each case are warranted.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-41.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-41.png"></p></div>
<p>To do dtype dispatch, you should use the <tt>AT_DISPATCH_ALL_TYPES</tt> macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.</p>
<p>This macro doesn't just "do dispatch", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want <tt>AT_DISPATCH_ALL_TYPES</tt>, but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in <a href="https://github.com/pytorch/pytorch/blob/21ef4cc615a7d9d772ade52a5023900718b09e92/aten/src/ATen/Dispatch.h#L62">Dispatch.h</a> for how to select the correct one for your use-case.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-43.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-43.png"></p></div>
<p>On CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-42.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-42.png"></p></div>
<p>At some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.</p>
<ol>
<li>If you just want to get a value at some specific location, you should use <tt>TensorAccessor</tt>.  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like <tt>x.accessor&lt;float, <span>3&gt;();</span></tt>, we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a <tt>PackedTensorAccessor</tt>, which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha: <tt>TensorAccessor</tt> defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)</li>
<li>If you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the <tt>TensorIterator</tt>.   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.</li>
<li>For true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The <tt>Vec256</tt> class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like <tt>binary_kernel_vec</tt> then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.</li>
</ol>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-44.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-44.png"></p></div>
<p>A lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?</p>
<ol>
<li>It's written in C style, no (or very little) use of C++.</li>
<li>It's manually refcounted (with manual calls to <tt>THTensor_free</tt> to decrease refcounts when you're done using tensors), and</li>
<li>It lives in <tt>generic/</tt> directory, which means that we are actually going to compile the file multiple times, but with different <tt>#define scalar_t</tt>.</li>
</ol>
<p>This code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-45.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-45.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-46.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-46.png"></p></div>
<p>To wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, <strong>you will have a bad time</strong>: it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.</p>
<p>How to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: "It's hard to work on PyTorch."</p>
<ol>
<li>If you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!</li>
<li>Our CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can <a href="https://github.com/pytorch/ossci-job-dsl">download and run the Docker images locally</a></li>
<li>The <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#use-ccache">CONTRIBUTING guide explains how to setup ccache</a>; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.</li>
<li>At the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.</li>
</ol>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-47.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-47.png"></p></div>
<p>So that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.</p>
<p>Where should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled <strong>triaged</strong> mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22high+priority%22+label%3Atriaged">high priority</a> or look up issues specific to some module, e.g., <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3A%22module%3A+autograd%22">autograd</a> or find issues which we think are <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3Asmall">small</a> (word of warning: we're sometimes wrong!)</p>
<p>Even if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I <em>love</em> merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Landrun: Sandbox any Linux process using Landlock, no root or containers (174 pts)]]></title>
            <link>https://github.com/Zouuup/landrun</link>
            <guid>43445662</guid>
            <pubDate>Sat, 22 Mar 2025 13:56:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Zouuup/landrun">https://github.com/Zouuup/landrun</a>, See on <a href="https://news.ycombinator.com/item?id=43445662">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">landrun</h2><a id="user-content-landrun" aria-label="Permalink: landrun" href="#landrun"></a></p>
<p dir="auto">A lightweight, secure sandbox for running Linux processes using Landlock LSM. Think firejail, but with kernel-level security and minimal overhead.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🔒 Kernel-level security using Landlock LSM</li>
<li>🚀 Lightweight and fast execution</li>
<li>🛡️ Fine-grained access control for directories</li>
<li>🔄 Support for read and write paths</li>
<li>⚡ Optional execution permissions for allowed paths</li>
<li>🌐 TCP network access control (binding and connecting)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Zouuup/landrun/blob/main/demo.gif"><img src="https://github.com/Zouuup/landrun/raw/main/demo.gif" alt="landrun demo" width="700" data-animated-image=""></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Linux kernel 5.13 or later with Landlock LSM enabled</li>
<li>Linux kernel 6.8 or later for network restrictions (TCP bind/connect)</li>
<li>Go 1.18 or later (for building from source)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Install</h3><a id="user-content-quick-install" aria-label="Permalink: Quick Install" href="#quick-install"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="go install github.com/zouuup/landrun/cmd/landrun@latest"><pre>go install github.com/zouuup/landrun/cmd/landrun@latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">From Source</h3><a id="user-content-from-source" aria-label="Permalink: From Source" href="#from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/zouuup/landrun.git
cd landrun
go build -o landrun cmd/landrun/main.go
sudo cp landrun /usr/local/bin/"><pre>git clone https://github.com/zouuup/landrun.git
<span>cd</span> landrun
go build -o landrun cmd/landrun/main.go
sudo cp landrun /usr/local/bin/</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Basic syntax:</p>
<div dir="auto" data-snippet-clipboard-copy-content="landrun [options] <command> [args...]"><pre>landrun [options] <span>&lt;</span>command<span>&gt;</span> [args...]</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Options</h3><a id="user-content-options" aria-label="Permalink: Options" href="#options"></a></p>
<ul dir="auto">
<li><code>--ro &lt;path&gt;</code>: Allow read-only access to specified path (can be specified multiple times)</li>
<li><code>--rw &lt;path&gt;</code>: Allow read-write access to specified path (can be specified multiple times)</li>
<li><code>--exec</code>: Allow executing files in allowed paths</li>
<li><code>--bind-tcp &lt;port&gt;</code>: Allow binding to specified TCP port (can be specified multiple times)</li>
<li><code>--connect-tcp &lt;port&gt;</code>: Allow connecting to specified TCP port (can be specified multiple times)</li>
<li><code>--best-effort</code>: Use best effort mode, falling back to less restrictive sandbox if necessary [default: enabled]</li>
<li><code>--log-level &lt;level&gt;</code>: Set logging level (error, info, debug) [default: "error"]</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Important Notes</h3><a id="user-content-important-notes" aria-label="Permalink: Important Notes" href="#important-notes"></a></p>
<ul dir="auto">
<li>You must explicitly add the path to the command you want to run with the <code>--ro</code> flag</li>
<li>For system commands, you typically need to include <code>/usr/bin</code>, <code>/usr/lib</code>, and other system directories</li>
<li>When using <code>--exec</code>, you still need to specify the directories containing executables with <code>--ro</code></li>
<li>Network restrictions require Linux kernel 6.8 or later with Landlock ABI v5</li>
<li>The <code>--best-effort</code> flag allows graceful degradation on older kernels that don't support all requested restrictions</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Environment Variables</h3><a id="user-content-environment-variables" aria-label="Permalink: Environment Variables" href="#environment-variables"></a></p>
<ul dir="auto">
<li><code>LANDRUN_LOG_LEVEL</code>: Set logging level (error, info, debug)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<ol dir="auto">
<li>Run a command with read-only access to a directory:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls /path/to/dir"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls /path/to/dir</pre></div>
<ol start="2" dir="auto">
<li>Run a command with write access to a directory:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --rw /path/to/dir touch /path/to/dir/newfile"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --rw /path/to/dir touch /path/to/dir/newfile</pre></div>
<ol start="3" dir="auto">
<li>Run a command with execution permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --exec /usr/bin/bash"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --exec /usr/bin/bash</pre></div>
<ol start="4" dir="auto">
<li>Run with debug logging:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --log-level debug --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls"><pre>landrun --log-level debug --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls</pre></div>
<ol start="5" dir="auto">
<li>Run with network restrictions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --bind-tcp 8080 --connect-tcp 53 /usr/bin/my-server"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --bind-tcp 8080 --connect-tcp 53 /usr/bin/my-server</pre></div>
<p dir="auto">This will allow the program to only bind to TCP port 8080 and connect to TCP port 53.</p>
<ol start="6" dir="auto">
<li>Run a DNS client with appropriate permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /etc/resolv.conf --connect-tcp 53 dig example.com"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /etc/resolv.conf --connect-tcp 53 dig example.com</pre></div>
<p dir="auto">This allows DNS resolution by granting access to /etc/resolv.conf and permitting connections to port 53 (DNS).</p>
<ol start="7" dir="auto">
<li>Run a web server with selective network permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /var/www --rw /var/log --bind-tcp 80 --bind-tcp 443 /usr/bin/nginx"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /var/www --rw /var/log --bind-tcp 80 --bind-tcp 443 /usr/bin/nginx</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">landrun uses Linux's Landlock LSM to create a secure sandbox environment. It provides:</p>
<ul dir="auto">
<li>File system access control</li>
<li>Directory access restrictions</li>
<li>Execution control</li>
<li>TCP network restrictions</li>
<li>Process isolation</li>
</ul>
<p dir="auto">Landlock is an access-control system that enables processes to securely restrict themselves and their future children. As a stackable Linux Security Module (LSM), it creates additional security layers on top of existing system-wide access controls, helping to mitigate security impacts from bugs or malicious behavior in applications.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Landlock Access Control Rights</h3><a id="user-content-landlock-access-control-rights" aria-label="Permalink: Landlock Access Control Rights" href="#landlock-access-control-rights"></a></p>
<p dir="auto">landrun leverages Landlock's fine-grained access control mechanisms, which include:</p>
<p dir="auto"><strong>File-specific rights:</strong></p>
<ul dir="auto">
<li>Execute files (<code>LANDLOCK_ACCESS_FS_EXECUTE</code>)</li>
<li>Write to files (<code>LANDLOCK_ACCESS_FS_WRITE_FILE</code>)</li>
<li>Read files (<code>LANDLOCK_ACCESS_FS_READ_FILE</code>)</li>
<li>Truncate files (<code>LANDLOCK_ACCESS_FS_TRUNCATE</code>) - Available since Landlock ABI v3</li>
</ul>
<p dir="auto"><strong>Directory-specific rights:</strong></p>
<ul dir="auto">
<li>Read directory contents (<code>LANDLOCK_ACCESS_FS_READ_DIR</code>)</li>
<li>Remove directories (<code>LANDLOCK_ACCESS_FS_REMOVE_DIR</code>)</li>
<li>Remove files (<code>LANDLOCK_ACCESS_FS_REMOVE_FILE</code>)</li>
<li>Create various filesystem objects (char devices, directories, regular files, sockets, etc.)</li>
<li>Refer/reparent files across directories (<code>LANDLOCK_ACCESS_FS_REFER</code>) - Available since Landlock ABI v2</li>
</ul>
<p dir="auto"><strong>Network-specific rights</strong> (requires Linux 6.8+ with Landlock ABI v5):</p>
<ul dir="auto">
<li>Bind to specific TCP ports (<code>LANDLOCK_ACCESS_NET_BIND_TCP</code>)</li>
<li>Connect to specific TCP ports (<code>LANDLOCK_ACCESS_NET_CONNECT_TCP</code>)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Limitations</h3><a id="user-content-limitations" aria-label="Permalink: Limitations" href="#limitations"></a></p>
<ul dir="auto">
<li>Landlock must be supported by your kernel</li>
<li>Network restrictions require Linux kernel 6.8+ with Landlock ABI v5</li>
<li>Some operations may require additional permissions</li>
<li>Files or directories opened before sandboxing are not subject to Landlock restrictions</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Kernel Compatibility Table</h2><a id="user-content-kernel-compatibility-table" aria-label="Permalink: Kernel Compatibility Table" href="#kernel-compatibility-table"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Feature</th>
<th>Minimum Kernel Version</th>
<th>Landlock ABI Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>Basic filesystem sandboxing</td>
<td>5.13</td>
<td>1</td>
</tr>
<tr>
<td>File referring/reparenting control</td>
<td>5.17</td>
<td>2</td>
</tr>
<tr>
<td>File truncation control</td>
<td>6.1</td>
<td>3</td>
</tr>
<tr>
<td>Network TCP restrictions</td>
<td>6.8</td>
<td>5</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">If you receive "permission denied" or similar errors:</p>
<ol dir="auto">
<li>Ensure you've added all necessary paths with <code>--ro</code> or <code>--rw</code></li>
<li>Try running with <code>--log-level debug</code> to see detailed permission information</li>
<li>Check that Landlock is supported and enabled on your system:
<div dir="auto" data-snippet-clipboard-copy-content="grep -E 'landlock|lsm=' /boot/config-$(uname -r)"><pre>grep -E <span><span>'</span>landlock|lsm=<span>'</span></span> /boot/config-<span><span>$(</span>uname -r<span>)</span></span></pre></div>
You should see <code>CONFIG_SECURITY_LANDLOCK=y</code> and <code>lsm=landlock,...</code> in the output</li>
<li>For network restrictions, verify your kernel version is 6.8+ with Landlock ABI v5:

</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Details</h2><a id="user-content-technical-details" aria-label="Permalink: Technical Details" href="#technical-details"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Implementation</h3><a id="user-content-implementation" aria-label="Permalink: Implementation" href="#implementation"></a></p>
<p dir="auto">This project uses the <code>landlock-lsm/go-landlock</code> package for sandboxing, which provides both filesystem and network restrictions. The current implementation supports:</p>
<ul dir="auto">
<li>Read/write/execute restrictions for files and directories</li>
<li>TCP port binding restrictions</li>
<li>TCP port connection restrictions</li>
<li>Best-effort mode for graceful degradation on older kernels</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Best-Effort Mode</h3><a id="user-content-best-effort-mode" aria-label="Permalink: Best-Effort Mode" href="#best-effort-mode"></a></p>
<p dir="auto">When using <code>--best-effort</code> (enabled by default), landrun will gracefully degrade to using the best available Landlock version on the current kernel. This means:</p>
<ul dir="auto">
<li>On Linux 6.8+: Full filesystem and network restrictions</li>
<li>On Linux 6.1-6.7: Filesystem restrictions including truncation, but no network restrictions</li>
<li>On Linux 5.17-6.0: Basic filesystem restrictions including file reparenting, but no truncation control or network restrictions</li>
<li>On Linux 5.13-5.16: Basic filesystem restrictions without file reparenting, truncation control, or network restrictions</li>
<li>On older Linux: No restrictions (sandbox disabled)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future Features</h2><a id="user-content-future-features" aria-label="Permalink: Future Features" href="#future-features"></a></p>
<p dir="auto">Based on the Linux Landlock API capabilities, we plan to add:</p>
<ul dir="auto">
<li>🔒 Enhanced filesystem controls with more fine-grained permissions</li>
<li>🌐 Support for UDP and other network protocol restrictions (when supported by Linux kernel)</li>
<li>🔄 Process scoping and resource controls</li>
<li>🛡️ Additional security features as they become available in the Landlock API</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the GNU General Public License v2</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When you deleted /lib on Linux while still connected via SSH (2022) (108 pts)]]></title>
            <link>https://tinyhack.com/2022/09/16/when-you-deleted-lib-on-linux-while-still-connected-via-ssh/</link>
            <guid>43444160</guid>
            <pubDate>Sat, 22 Mar 2025 07:24:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tinyhack.com/2022/09/16/when-you-deleted-lib-on-linux-while-still-connected-via-ssh/">https://tinyhack.com/2022/09/16/when-you-deleted-lib-on-linux-while-still-connected-via-ssh/</a>, See on <a href="https://news.ycombinator.com/item?id=43444160">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Scallop – A Language for Neurosymbolic Programming (172 pts)]]></title>
            <link>https://www.scallop-lang.org/</link>
            <guid>43443640</guid>
            <pubDate>Sat, 22 Mar 2025 04:45:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scallop-lang.org/">https://www.scallop-lang.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43443640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <div>
          <div>
                <p><img src="https://www.scallop-lang.org/img/icon-language.png"></p><h2>Language</h2>
                <p>
                  Scallop is a declarative language designed to support rich symbolic reasoning in AI applications.
		  It is based on Datalog, a logic rule-based query language for relational databases.
                </p>
              </div>
          <div>
                <p><img src="https://www.scallop-lang.org/img/icon-solver.png"></p><h2>Solver</h2>
                <p>
                  Scallop is a scalable Datalog solver equipped with support for discrete, probabilistic, and
                  differentiable modes of reasoning.
                  These modes are configurable to suit the needs of different AI applications.
                </p>
              </div>
          <div>
                <p><img src="https://www.scallop-lang.org/img/icon-framework.png"></p><h2>Framework</h2>
                <p>
                  Scallop provides bindings to support logic reasoning modules within Python programs.
                  As a result, Scallop can be deeply integrated with existing PyTorch machine
                  learning pipelines.
                </p>
              </div>
        </div>
      <div>
          <h2>Wide Range of Applications</h2>
          <p>
              Scallop can be used to develop a wide variety of applications in vision and NLP that involve symbolic reasoning.
              The reasoning component is specified via logic rules which can then be deeply
              integrated with machine learning models, such as convolutional neural networks and transformers.
            </p>
        </div>
      
      
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[George Foreman, Boxer Turned Foreman Grill Infomercial Star, Dies at 76 (296 pts)]]></title>
            <link>https://variety.com/2025/tv/news/george-foreman-boxer-infomercial-star-dies-1236345523/</link>
            <guid>43442917</guid>
            <pubDate>Sat, 22 Mar 2025 02:56:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://variety.com/2025/tv/news/george-foreman-boxer-infomercial-star-dies-1236345523/">https://variety.com/2025/tv/news/george-foreman-boxer-infomercial-star-dies-1236345523/</a>, See on <a href="https://news.ycombinator.com/item?id=43442917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
	<a href="https://variety.com/t/george-foreman/" id="auto-tag_george-foreman" data-tag="george-foreman">George Foreman</a>, the charismatic boxer turned infomercial star who had a retail hit with his Foreman Grill product line, died Friday. He was 76.</p>



<p>
	The Texas-born Foreman became Heavyweight Champion of the World, and segued into a TV staple and pop culture icon. He was swept up in the swirl of decade-defining events surrounding Muhammad Ali as well as Joe Frazier and other high-wattage pugilists of the 1970s. In the 1990s, Foreman took advantage of the availablity of low-cost TV time to launch his Foreman Grill home grill product through a series of  infomercials that he hosted. </p>



<p>
	Foreman famously had a close call in the ring in 1977 that drove him to quit boxing and declare himself a born-again Christian. He became an ordained minister in 1978 and began preaching in his hometown of Houston. He shocked the sports world when he returned to boxing in 1987 and wound up reclaiming his Heavyweight Champion title in 1994. Foreman retired from the sweet science for good in 1997.

	</p>






<p>
	In addition to his business ventures, Foreman led Houston’s Church of the Lord Jesus Christ, where he preached four times a week.


</p><div data-pmc-adm-ad-id="1234758890">
			<h3>
			Popular on Variety		</h3>
	
		
	</div>




<p>
	In recent years, Foreman had been involved with numerous documentary projects about his life, boxing and the era of his greatest fame. He was also the subject of the 2023 biopic “Big George Foreman,” from director George Tillman Jr. Khris Davis played Foreman in the Mandalay Pictures drama that focused on his improbable return to the ring in the 1980s and ’90s.</p>



<p>
	Foreman’s family confirmed his death in an Instagram post on Friday.</p>



<figure><div>
<blockquote data-instgrm-captioned="" data-instgrm-permalink="https://www.instagram.com/p/DHe5UtIJ4IQ/?utm_source=ig_embed&amp;utm_campaign=loading" data-instgrm-version="14"></blockquote>
</div></figure>



<p>
	Born Jan. 10, 1949, Foreman grew up in extreme poverity in the east Texas city of Marshall, about 40 miles west of Shreveport, La. He first gained national fame after winning an Olympic gold medal in boxing at the 1968 Summer Games in Mexico City.</p>



<p>
	“Foreman often bullied younger children and didn’t like getting up early for school. Foreman became a mugger and brawler on the hard streets of Houston’s Fifth Ward by age 15,” according to <a href="https://www.georgeforeman.com/pages/biography" rel="nofollow" target="_blank"><strong>Foreman’s official website.</strong></a></p>



<p>
	He was eventually steered into boxing through the Lone Star state’s Lyndon B. Johnson Job Corps program. Foreman gained stature in the late 1960s and ultimately secured the Heavyweight Championship in January 1973 by defeating Frazier with six knockouts in a bout held in Kingston, Jamaica. The event also had the distinction of being the first boxing broadcast to air on the then-fledgling pay TV service HBO.

	</p>




<p>
	The following year, Foreman faced a resurgent Ali in the event that received worldwide attention as the “Rumble in the Jungle,” held in what is now the Democratic Republic of Congo. Ali pummelled Foreman in the ring and dominated him on the PR front as well. Foreman went on to went his next five fights by knockout.</p>



<p>
	After his triumph of becoming the world’s oldest Heavyweight Champion, Foreman became a boldface name staple on TV, from daytime talk shows to “The Tonight Show” and “Late Night With David Letterman.” He was known for his folksy charm and for having a sprawling family of children and grandchildren. And his low-cost cooking device that allowed for easy indoor grilling — the George Foreman Lean Mean Grilling Machine — became a retail and direct response sales juggernaut starting in the early 1990s.</p>



<p>
	Foreman also starred in the short-lived 1993 ABC family comedy “George,” playing a retired boxer who runs an after-school program for troubled students. He hosted NBC’s “Saturday Night Live” in 1994.</p>



<p>
	Foreman had cameos and small roles in a host of TV shows and movies over the years, playing himself or a similar character, including “Night at the Museum: Battle of the Smithsonian,” “The Fighter,” “The Masked Singer,” “The Larry Sanders Show,” “Home Improvement” and “King of the Hill.”</p>
















</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Monster Cables picked the wrong guy to threaten (2008) (482 pts)]]></title>
            <link>https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/</link>
            <guid>43442178</guid>
            <pubDate>Sat, 22 Mar 2025 00:30:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/">https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/</a>, See on <a href="https://news.ycombinator.com/item?id=43442178">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody" id="post-113" itemscope="" itemtype="https://schema.org/BlogPosting"><p>Monster Cables, which makes extremely high-priced stereo cables, has apparently sent a <a href="http://www.audioholics.com/news/industry-news/monster-sues-blue-jeans-cable" target="_blank">cease-and-desist letter</a> to <a href="http://www.bluejeanscable.com/">Blue Jeans Cable</a>, alleging various kinds of infringement.  Bad move – the president of Blue Jeans Cable, Kurt Denke, is a former litigator who <a href="http://www.audioholics.com/news/industry-news/blue-jeans-strikes-back">responded pretty forcefully</a>:</p><blockquote><p><em>… Once I have received the above materials and explanations from you, I will undertake to analyze this information and let you know whether we are willing to accede to any of the demands made in your letter. <strong>If my analysis shows that there is any reasonable likelihood that we have infringed in any way any of Monster Cable’s intellectual property rights, we will of course take any and all action necessary to resolve the situation. </strong> If I do not hear from you within the next fourteen days, or if I do hear from you but do not receive </em><em>all of the information requested above, I will assume that you have abandoned these claims and closed your file.</em></p><p><em> As for your requests for information, or for action, directed to me: I would remind you that it is you, not I, who are making claims; and it is you, not I, who must substantiate those claims.  You have not done so.</em></p><p><em> I have seen Monster Cable take untenable IP positions in various different scenarios in the past, and am generally familiar with what seems to be Monster Cable’s </em><em>modus operandi in these matters.  I therefore think that it is important that, before closing, I make you aware of a few points.</em></p><p><em> After graduating from the University of Pennsylvania Law School in 1985, I spent nineteen years in litigation practice, with a focus upon federal litigation involving large damages and complex issues.  My first seven years were spent primarily on the defense side, where <strong>I developed an intense frustration with insurance carriers who would settle meritless claims for nuisance value when the better long-term view would have been to fight against vexatious litigation as a matter of principle.</strong> In plaintiffs’ practice, likewise, I was always a strong advocate of standing upon principle and taking cases all the way to judgment, even when substantial offers of settlement were on the table.  I am “uncompromising” in the most literal sense of the word.  If Monster Cable proceeds with litigation against me I will pursue the same merits-driven approach; I do not compromise with bullies and <strong>I would rather spend fifty thousand dollars on defense than give you a dollar of unmerited settlement funds.</strong> As for signing a licensing agreement for intellectual property which I have not infringed: that will not happen, under any circumstances, whether it makes economic sense or not.</em></p><p><em> I say this because my observation has been that Monster Cable typically operates in a hit-and-run fashion.  Your client threatens litigation, expecting the victim to panic and plead for mercy; and what follows is a quickie negotiation session that ends with payment and a licensing agreement.  Your client then uses this collection of licensing agreements to convince others under similar threat to accede to its demands.  Let me be clear about this: <strong>there are only two ways for you to get anything out of me.  You will either need to (1) convince me that I have infringed, or (2) obtain a final judgment to that effect from a court of competent jurisdiction. </strong>It may be that my inability to see the pragmatic value of settling frivolous claims is a deep character flaw, and I am sure a few of the insurance carriers for whom I have done work have seen it that way; but it is how I have done business for the last quarter-century and you are not going to change my mind.  If you sue me, the case will go to judgment, and I will hold the court’s attention upon the merits of your claims–or, to speak more precisely, the absence of merit from your claims–from start to finish. <strong>Not only am I unintimidated by litigation; I sometimes rather miss it.</strong></em></p></blockquote><p>(Emphasis added; hat tip: Jeff Nolan at <a href="http://jeffnolan.com/wp/2008/04/15/monster-cables-gets-soundly-beaten/#comment-258095">Venture Chronicles</a>.)</p><p>I can relate to Denke’s final comment quoted above ….  I wonder what the attendant publicity is doing for his sales.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Not OK Cupid – A story of poor email address validation (112 pts)]]></title>
            <link>https://www.fastmail.com/blog/not-ok-cupid/</link>
            <guid>43441961</guid>
            <pubDate>Fri, 21 Mar 2025 23:54:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fastmail.com/blog/not-ok-cupid/">https://www.fastmail.com/blog/not-ok-cupid/</a>, See on <a href="https://news.ycombinator.com/item?id=43441961">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pagefind-body="" data-cms-edit="content"> <p>I don’t usually like to call out the bad behaviour of specific companies, but the egregious mis-design and lack of acknowledging it justify this case.</p> <h2 id="welcome-to-ok-cupid" tabindex="-1">Welcome to OkCupid</h2> <p>A couple of weeks ago, I started seeing many “Welcome to OkCupid” emails, both on my personal address and a couple of related addresses, but also to multiple Fastmail official contact addresses — legal, partnerships, press, etc. Specifically, this list included <code>trash@brong.net</code> — an address that has never been used to send or receive email and appears in precisely one place — <a href="https://www.fastmail.com/blog/a-tangled-path-of-workarounds/" target="_blank" rel="noopener">an article on our blog</a>! It seems quite clear that somebody scraped our website and used the addresses to sign up. I’m aware of at least 10 addresses, but there are likely others that either go to someone else or addresses that no longer exist.</p> <p>It didn’t stop there, though. I’ve been getting tons of “someone likes you”, “you have an intro,” and even an “IMPORTANT: We removed your photo on OkCupid.” email saying that inappropriate content was posted to “our” account!</p> <h2 id="the-real-world-consequences-of-poor-email-validation" tabindex="-1">The real-world consequences of poor email validation</h2> <p>This isn’t just an inconvenience — it has real security implications. Websites that fail to properly validate email ownership can be exploited for malicious purposes. Attackers can use unverified sign-ups to flood inboxes, making it easier to hide critical emails among the noise — something we’ve discussed our own experience of in our post on <a href="https://www.fastmail.com/blog/when-two-factor-authentication-is-not-enough/" target="_blank" rel="noopener">2FA vulnerabilities</a>. There are established <a href="https://www.m3aawg.org/sites/default/files/document/M3AAWG_Senders_BCP_Ver3-2015-02.pdf" target="_blank" rel="noopener">best practices</a> (PDF) for handling email sign-ups responsibly, practices that OkCupid is failing to follow.</p> <h2 id="no-way-out" tabindex="-1">No way out</h2> <p>When I tried to unsubscribe using the one-click unsubscribe button in one of the emails, I was met with an error: “Something went wrong, please try again later.”</p> <p>Curious, I tried to recover a password on one of these accounts (the one with my personal email address) and successfully changed the password. Then, I was asked to confirm my login with a message sent to the number associated with the account. A number I didn’t know. A number that wasn’t mentioned on that page, so I still don’t know anything about it — not even which country it was from.</p> <p>This raises further security concerns; the attacker could have also caused random recovery numbers to be texted to another poor victim’s phone. Alternatively, they could confirm that my email address is actively monitored, increasing its value for further attacks. Either way, what I couldn’t do was actually close the account.</p> <h2 id="whack-a-mole" tabindex="-1">Whack-a-mole</h2> <p>So, I contacted OkCupid’s support. Here’s what they said:</p> <p><em>I’ve removed the user from the site and banned the email address to prevent any new accounts from being created. That should resolve the issue, but if you encounter anything like this again in the future, please don’t hesitate to reach out, and we’ll address it right away.</em></p> <p>So, I need to contact support manually for each new email address. This is neither scalable nor acceptable; people don’t have this amount of time.</p> <p>Furthermore, my email address is now on another random blocklist somewhere on the internet, where I have no control and no way to unblock it. I don’t anticipate wanting to use OkCupid’s service, but if I did in the future, I would have to go through another dance to get the address unlocked again — or more likely, treat that particular email address as soiled and create another one.</p> <h2 id="not-ok" tabindex="-1">Not OK</h2> <p>So I say, not OK, OkCupid. Not OK.</p> <p>The usefulness of email depends on responsible behaviour from all service providers. Companies that engage in shady or outright inappropriate practices make the internet worse for everyone.</p> <p>OkCupid’s failure to implement even the <a href="https://en.wikipedia.org/wiki/Opt-in_email#Confirmed_opt-in_(COI)/double_opt-in_(DOI)" target="_blank" rel="noopener">simplest form</a> of email validation is unacceptable. Until they address these issues properly (not through the support response provided here), they remain part of the problem, not the solution.</p> <h2 id="could-we-have-avoided-this" tabindex="-1">Could we have avoided this?</h2> <p>In this case, we published those addresses online. There’s always a risk of receiving spam when you do that, one could even reasonably say “we were asking for it”. We expected spam. If you want to reduce your risk of being spammed, it helps to not publish your email address on the public web!</p> <p>What we we didn’t was expect a relatively reputable service being used to facilitate us being spammed.</p> <p>One great protection is using different address for each different organisation you deal with — that way if your address leaks (or they sell it), you know where the breach happened, and you can more easily block just the problem messages.</p> <p>Fastmail’s masked email feature is a great way to implement this strategy. Masked emails are designed, particularly when integrated with a password manager, to make it very easy to create new addresses, and track where they are expected to be used.</p> <p>Being a good internet citizen is one of <a href="https://www.fastmail.com/company/values/" target="_blank" rel="noopener">Fastmail’s core values</a>. We require verification for sending identities, ensuring that only legitimate users can send from an address they claim they own. This is the level of responsibility every email provider should uphold, and we applaud the others who also do.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EFF Border Search Pocket Guide (119 pts)]]></title>
            <link>https://www.eff.org/document/eff-border-search-pocket-guide</link>
            <guid>43441895</guid>
            <pubDate>Fri, 21 Mar 2025 23:41:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/document/eff-border-search-pocket-guide">https://www.eff.org/document/eff-border-search-pocket-guide</a>, See on <a href="https://news.ycombinator.com/item?id=43441895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>
            <h2>EFF Border Search Pocket Guide</h2>
    </p>
<div><div><p>This is a handy guide designed to be printed, folded, and carried in your pocket while traveling.</p></div>

<div><p><span><img alt="PDF icon" title="application/pdf" src="https://www.eff.org/modules/file/icons/application-pdf.png"> <a href="https://www.eff.org/files/2018/01/11/border-pocket-guide-2.pdf" type="application/pdf; length=799017">border-pocket-guide-2.pdf</a></span></p></div>
</div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MySQL transactions per second vs. fsyncs per second (2020) (103 pts)]]></title>
            <link>https://sirupsen.com/napkin/problem-10-mysql-transactions-per-second</link>
            <guid>43440920</guid>
            <pubDate>Fri, 21 Mar 2025 21:18:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sirupsen.com/napkin/problem-10-mysql-transactions-per-second">https://sirupsen.com/napkin/problem-10-mysql-transactions-per-second</a>, See on <a href="https://news.ycombinator.com/item?id=43440920">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p><time datetime="2020-07-17">Jul 2020</time></p><p><strong>Just wondering how many transactions or writes per second MySQL can handle?</strong> While it depends on many factors, fundamentally, about as many transactions as MySQL can commit to disk per second. A modern disk can do <a href="https://github.com/sirupsen/napkin-math#numbers">~1000 fsyncs per second</a>, but MySQL will group multiple writes with each fsync. An okay rule-of-thumb would be 5000-15,000 writes per second, depending on things like writes per transaction, number of indexes, hardware, size of writes, etc. Read the article to understand this in more depth!</p><nav><ol><li><a href="https://sirupsen.com/napkin/problem-10-mysql-transactions-per-second#problem-10-is-mysqls-maximum-transactions-per-second-equivalent-to-fsyncs-per-second">Problem 10: Is MySQL’s maximum transactions per second equivalent to fsyncs per second?</a></li><li><a href="https://sirupsen.com/napkin/problem-10-mysql-transactions-per-second#problem-9-inverted-index">Problem 9: Inverted Index</a></li></ol></nav><p>Napkin friends, from near and far, it’s time for another napkin problem!</p>
<p>Since the beginning of this newsletter I’ve posed problems for you to try to
answer. Then in the next month’s edition, you hear my answer. Talking with a few
of you, it seems many of you read these as posts regardless of their
problem-answer format.</p>
<p>That’s why I’ve decided to experiment with a simpler format: posts where I both
present a problem and solution in one go. This one will be long, since it’ll
include an answer to last month’s.</p>
<p>Hope you enjoy this format! As always, you are encouraged to reach out with
feedback.</p>
<h2 id="problem-10-is-mysqls-maximum-transactions-per-second-equivalent-to-fsyncs-per-second">Problem 10: Is MySQL’s maximum transactions per second equivalent to fsyncs per second?<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h2>
<p>How many transactions (‘writes’) per second is MySQL capable of?</p>
<p>A naive model of how a write (a SQL insert/update/delete) to an ACID-compliant
database like MySQL works might be the following (this applies equally to
Postgres, or any other relational/ACID-compliant databases, but we’ll
proceed to work with MySQL as it’s the one I know best):</p>
<ol>
<li>Client sends query to MySQL over an existing connection: <code>INSERT INTO products (name, price) VALUES ('Sneaker', 100)</code></li>
<li>MySQL inserts the new record to the write-ahead-log (WAL) and calls
<code>fsync(2)</code> to tell the operating system to tell the filesystem to tell the
disk to make <em>sure</em> that this data is <em>for sure</em>, pinky-swear committed to
the disk. This step, being the most complex, is depicted below.</li>
<li>MySQL inserts the record into an in-memory page in the backing storage engine
(InnoDB) so the record will be visible to subsequent queries. Why commit to
the storage engine <em>and</em> the WAL? The storage engine is optimized for serving
query results the data, and the WAL for writing it in a safe manner — we
can’t serve a <code>SELECT</code> efficiently from the WAL!</li>
<li>MySQL returns <code>OK</code> to the client.</li>
<li>MySQL eventually calls <code>fsync(2)</code> to ensure InnoDB commits the page to disk.</li>
</ol>
<figure><img alt="Napkin_10" fetchpriority="high" width="1759" height="1198" decoding="async" data-nimg="1" sizes="(min-width: 36rem) 36rem 100vw" srcset="https://sirupsen.com/_next/image?url=%2Fimages%2F87759326-21adeb00-c7dc-11ea-89c7-559ca11530e8.png&amp;w=640&amp;q=75 640w, https://sirupsen.com/_next/image?url=%2Fimages%2F87759326-21adeb00-c7dc-11ea-89c7-559ca11530e8.png&amp;w=750&amp;q=75 750w, https://sirupsen.com/_next/image?url=%2Fimages%2F87759326-21adeb00-c7dc-11ea-89c7-559ca11530e8.png&amp;w=828&amp;q=75 828w, https://sirupsen.com/_next/image?url=%2Fimages%2F87759326-21adeb00-c7dc-11ea-89c7-559ca11530e8.png&amp;w=1080&amp;q=75 1080w, https://sirupsen.com/_next/image?url=%2Fimages%2F87759326-21adeb00-c7dc-11ea-89c7-559ca11530e8.png&amp;w=1200&amp;q=75 1200w, https://sirupsen.com/_next/image?url=%2Fimages%2F87759326-21adeb00-c7dc-11ea-89c7-559ca11530e8.png&amp;w=1920&amp;q=75 1920w, https://sirupsen.com/_next/image?url=%2Fimages%2F87759326-21adeb00-c7dc-11ea-89c7-559ca11530e8.png&amp;w=2048&amp;q=75 2048w, https://sirupsen.com/_next/image?url=%2Fimages%2F87759326-21adeb00-c7dc-11ea-89c7-559ca11530e8.png&amp;w=3840&amp;q=75 3840w" src="https://sirupsen.com/_next/image?url=%2Fimages%2F87759326-21adeb00-c7dc-11ea-89c7-559ca11530e8.png&amp;w=3840&amp;q=75"></figure>
<p>In the event of power-loss at any of these points, the behaviour can be defined
without nasty surprises, upholding our dear ACID-compliance.</p>
<p>Splendid! Now that we’ve constructed a naive model of how a relational database
might handle writes safely, we can consider the latency of inserting a new
record into the database. When we consult <a href="https://github.com/sirupsen/napkin-math">the reference napkin numbers</a>, we
see that the <code>fsync(2)</code> in step (2) is by <em>far</em> the slowest operation in the
blocking chain at 1 ms.</p>
<p>For example, the network handling at step (1) takes roughly ~10 μs (TCP Echo
Server is what we can classify as ‘the TCP overhead’). The <code>write(2)</code> itself
prior to the <code>fsync(2)</code> is also negligible at ~10 μs, since this system call
essentially just writes to an in-memory buffer (the ‘page cache’) in the kernel.
This doesn’t guarantee the actual bits are committed on disk, which means an
unexpected loss of power would erase the data, dropping our ACID-compliance on
the floor. Calling <code>fsync(2)</code> guarantees us the bits are persisted on the disk,
which will survive an unexpected system shutdown.  Downside is that it’s 100x
slower.</p>
<p>With that, we should be able to form a simple hypothesis on the maximum
throughput of MySQL:</p>
<blockquote>
<p>The maximum theoretical throughput of MySQL is equivalent to the maximum
number of <code>fsync(2)</code> per second.</p>
</blockquote>
<p>We know that <code>fsync(2)</code> takes 1 ms from earlier, which means we would naively
expect that MySQL would be able to perform in the neighbourhood of: <code>1s / 1ms/fsync = 1000 fsyncs/s = 1000 transactions/s</code> .</p>
<p>Excellent. We followed the first three of the napkin math steps: (1) Model the
system, (2) Identify the relevant latencies, (3) Do the napkin math, (4) Verify
the napkin calculations against reality.</p>
<p>On to (4: Verifying)! We’ll write a simple benchmark in Rust that writes to
MySQL with 16 threads, doing 1,000 insertions each:</p>
<pre><code><span>for</span> i <span>in</span> <span>0</span><span>..</span><span>16</span> <span>{</span>
    handles<span>.</span><span>push</span><span>(</span><span>thread<span>::</span></span><span>spawn</span><span>(</span><span>{</span>
        <span>let</span> pool <span>=</span> pool<span>.</span><span>clone</span><span>(</span><span>)</span><span>;</span>
        <span>move</span> <span><span>|</span><span>|</span></span> <span>{</span>
            <span>let</span> <span>mut</span> conn <span>=</span> pool<span>.</span><span>get_conn</span><span>(</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
            <span>// TODO: we should ideally be popping these off a queue in case of a stall</span>
            <span>// in a thread, but this is likely good enough.</span>
            <span>for</span> _ <span>in</span> <span>0</span><span>..</span><span>1000</span> <span>{</span>
                conn<span>.</span><span>exec_drop</span><span>(</span>
                    <span>r"INSERT INTO products (shop_id, title) VALUES (:shop_id, :title)"</span><span>,</span>
                    <span>params!</span> <span>{</span> <span>"shop_id"</span> <span>=&gt;</span> <span>123</span><span>,</span> <span>"title"</span> <span>=&gt;</span> <span>"aerodynamic chair"</span> <span>}</span><span>,</span>
                <span>)</span>
                <span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
            <span>}</span>
        <span>}</span>
    <span>}</span><span>)</span><span>)</span><span>;</span>

    <span>for</span> handle <span>in</span> handles <span>{</span>
      handle<span>.</span><span>join</span><span>(</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>
    <span>// 3 seconds, 16,000 insertions</span>
<span>}</span>
</code></pre>
<p>This takes ~3 seconds to perform 16,000 insertions, or ~5,300 insertions per
second. This is <strong>5x</strong> more than the 1,000 <code>fsync</code> per second our napkin math
told us would be the theoretical maximum transactional throughput!</p>
<p>Typically with napkin math we aim for being within an order of magnitude, which
we are. But, when I do napkin math it usually establishes a lower-bound for the
system, i.e. from first-principles, how fast <em>could</em> this system perform in
ideal circumstances?</p>
<p>Rarely is the system 5x faster than napkin math. When we identify a
significant-ish gap between the real-life performance and the expected
performance, I call it the “first-principle gap.” This is where curiosity sets
in. It typically means there’s (1) an opportunity to improve the system, or (2)
a flaw in our model of the system. In this case, only (2) makes sense, because
the system is faster than we predicted.</p>
<p>What’s wrong with our model of how the system works? Why aren’t fsyncs per
second equal to transactions per second?</p>
<p>First I examined the benchmark… is something wrong? Nope <code>SELECT COUNT(*) FROM products</code> says 16,000. Is the MySQL I’m using configured to not <code>fsync</code> on every
write? Nope, it’s at the <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit">safe default</a>.</p>
<p>Then I sat down and thought about it. Perhaps MySQL is <em>not</em> doing an <code>fsync</code>
for every <em>single</em> write? If it’s processing 5,300 insertions per second,
perhaps it’s batching multiple writes together as part of writing to the WAL,
step (2) above? Since each transaction is so short, MySQL would benefit from
waiting a few microseconds to see if other transactions want to ride along
before calling the expensive <code>fsync(2)</code>.</p>
<p>We can test this hypothesis by writing a simple <code>bpftrace</code> script to observe the
number of <code>fsync(1)</code> for the ~16,000 insertions:</p>
<pre><code>tracepoint<span>:</span>syscalls<span>:</span>sys_enter_fsync<span>,</span>tracepoint<span>:</span>syscalls<span>:</span>sys_enter_fdatasync
<span>/</span>comm <span>==</span> <span>"mysqld"</span><span>/</span>
<span>{</span>
        <span>@fsyncs</span> <span>=</span> <span>count</span><span>(</span><span>)</span><span>;</span>
<span>}</span>
</code></pre>
<p>Running this during the ~3 seconds it takes to insert the 16,000 records we get
~8,000 <code>fsync</code> calls:</p>
<pre><code>$ <span>sudo</span> bpftrace fsync_count.d
Attaching <span>2</span> probes<span>..</span>.
^C

@fsyncs: <span>8037</span>
</code></pre>
<p>This is a peculiar number. If MySQL was batching fsyncs, we’d expect something
far lower. This number means that we’re on average doing ~2,500 <code>fsync</code> per
second, at a latency of ~0.4ms. This is twice as fast as the <code>fsync</code> latency we
expect, the 1ms mentioned earlier. For sanity, I ran the script to benchmark
<code>fsync</code> outside MySQL again, no, <a href="https://github.com/sirupsen/napkin-math/blob/fe780331c6f0c6f225a70c8a37c21e0740f7c73c/src/main.rs#L491">still 1ms</a>. <a href="https://gist.github.com/sirupsen/9fd5fe9466e82df073ed8a13ed1f661f#file-napkin-bash">Looked at the
distribution</a>, and it was consistently ~1ms.</p>
<p>So there’s two things we can draw from this: (1) We’re able to <code>fsync</code> more than
twice as fast as we expect, (2) Our hypothesis was correct that MySQL is more
clever than doing one <code>fsync</code> per transaction, however, since <code>fsync</code> also was
faster than expected, this didn’t explain everything.</p>
<p>If you remember from above, while committing the transaction could theoretically
be a single <code>fsync</code>, other features of MySQL might also call <code>fsync</code>. Perhaps
they’re adding noise?</p>
<p>We need to group <code>fsync</code> by file descriptor to get a better idea of how MySQL
uses <code>fsync</code>. However, the raw file descriptor number doesn’t tell us much. We
can use <code>readlink</code> and the <code>proc</code> file-system to obtain the file name the file
descriptor points to. Let’s write a <a href="https://github.com/iovisor/bpftrace"><code>bpftrace</code> script</a> to see what’s being
<code>fsync</code>‘ed:</p>
<pre><code>tracepoint<span>:</span>syscalls<span>:</span>sys_enter_fsync<span>,</span>tracepoint<span>:</span>syscalls<span>:</span>sys_enter_fdatasync
<span>/</span>comm <span>==</span> <span>str</span><span>(</span><span>$</span><span>1</span><span>)</span><span>/</span>
<span>{</span>
  <span>@fsyncs</span><span>[</span>args<span>-</span><span>&gt;</span>fd<span>]</span> <span>=</span> <span>count</span><span>(</span><span>)</span><span>;</span>
  <span>if</span> <span>(</span><span>@fd_to_filename</span><span>[</span>args<span>-</span><span>&gt;</span>fd<span>]</span><span>)</span> <span>{</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>@fd_to_filename</span><span>[</span>args<span>-</span><span>&gt;</span>fd<span>]</span> <span>=</span> <span>1</span><span>;</span>
    <span>system</span><span>(</span><span>"echo -n 'fd %d -&gt; ' &amp;1&gt;&amp;2 | readlink /proc/%d/fd/%d"</span><span>,</span>
           args<span>-</span><span>&gt;</span>fd<span>,</span> pid<span>,</span> args<span>-</span><span>&gt;</span>fd<span>)</span><span>;</span>
  <span>}</span>
<span>}</span>

END <span>{</span>
  <span>clear</span><span>(</span><span>@fd_to_filename</span><span>)</span><span>;</span>
<span>}</span>
</code></pre>
<p>Running this while inserting the 16,000 transactions into MySQL gives us:</p>
<pre><code>personal@napkin:~$ <span>sudo</span> bpftrace --unsafe fsync_count_by_fd.d mysqld
Attaching <span>5</span> probes<span>..</span>.
fd <span>5</span> -<span>&gt;</span> /var/lib/mysql/ib_logfile0 <span># redo log, or write-ahead-log</span>
fd <span>9</span> -<span>&gt;</span> /var/lib/mysql/ibdata1 <span># shared mysql tablespace</span>
fd <span>11</span> -<span>&gt;</span> /var/lib/mysql/<span>#ib_16384_0.dblwr # innodb doublewrite-buffer</span>
fd <span>13</span> -<span>&gt;</span> /var/lib/mysql/undo_001 <span># undo log, to rollback transactions</span>
fd <span>15</span> -<span>&gt;</span> /var/lib/mysql/undo_002 <span># undo log, to rollback transactions</span>
fd <span>27</span> -<span>&gt;</span> /var/lib/mysql/mysql.ibd <span># tablespace </span>
fd <span>34</span> -<span>&gt;</span> /var/lib/mysql/napkin/products.ibd <span># innodb storage for our products table</span>
fd <span>99</span> -<span>&gt;</span> /var/lib/mysql/binlog.000019 <span># binlog for replication</span>
^C

@fsyncs<span>[</span><span>9</span><span>]</span>: <span>2</span>
@fsyncs<span>[</span><span>12</span><span>]</span>: <span>2</span>
@fsyncs<span>[</span><span>27</span><span>]</span>: <span>12</span>
@fsyncs<span>[</span><span>34</span><span>]</span>: <span>47</span>
@fsyncs<span>[</span><span>13</span><span>]</span>: <span>86</span>
@fsyncs<span>[</span><span>15</span><span>]</span>: <span>93</span>
@fsyncs<span>[</span><span>11</span><span>]</span>: <span>103</span>
@fsyncs<span>[</span><span>99</span><span>]</span>: <span>2962</span>
@fsyncs<span>[</span><span>5</span><span>]</span>: <span>4887</span>
</code></pre>
<p>What we can observe here is that the majority of the writes are to the “redo
log”, what we call the “write-ahead-log” (WAL). There’s a few <code>fsync</code> calls to
commit the InnoDB table-space, not nearly as often, as we can always recover
this from the WAL in case we crash between them. Reads work just fine prior to
the <code>fsync</code>, as the queries can simply be served out of memory from InnoDB.</p>
<p>The only surprising thing here is the substantial volume of writes to the
binlog, which we haven’t mentioned before. You can think of the binlog as the
“replication stream.” It’s a stream of events such as <code>row a changed from x to y</code>, <code>row b was deleted</code>, and <code>table u added column c</code>. The primary replica
streams this to the read-replicas, which use it to update their own data.</p>
<p>When you think about it, the <code>binlog</code> and the WAL need to be kept exactly in
sync. We can’t have something committed on the primary replica, but not
committed to the replicas. If they’re not in sync, this could cause loss of data
due to drift in the read-replicas. The primary could commit a change to the WAL,
lose power, recover, and never write it to the binlog.</p>
<p>Since <code>fsync(1)</code> can only sync a single file-descriptor at a time, how can you
possibly ensure that the <code>binlog</code> and the WAL contain the transaction?</p>
<p>One solution would be to merge the <code>binlog</code> and the <code>WAL</code> into one log. I’m not
entirely sure why that’s not the case, but likely the reasons are historic. If
you know, let me know!</p>
<p>The solution employed by MySQL is to use a 2-factor commit. This requires three
<code>fsync</code>s to commit the transaction. <a href="https://www.burnison.ca/notes/fun-mysql-fact-of-the-day-everything-is-two-phase">This</a> and <a href="https://kristiannielsen.livejournal.com/12254.html">this reference</a> explain
this process in more detail. Because the WAL is touched twice as part of the
2-factor commit, it explains why we see roughly ~2x the number of <code>fsync</code> to
that over the bin-log from the bpftrace output above. The process of grouping
multiple transactions into one 2-factor commit in MySQL is called ‘group commit.’</p>
<p>What we can gather from these numbers is that it seems the ~16,000 transactions
were, thanks to group commit, reduced into ~2885 commits, or ~5.5 transactions
per commit on average.</p>
<p>But there’s still one other thing remaining… why was the average latency per
<code>fsync</code> twice as fast as in our benchmark? Once again, we write a simple
<code>bpftrace</code> script:</p>
<pre><code>tracepoint:syscalls:sys_enter_fsync,tracepoint:syscalls:sys_enter_fdatasync
/comm == "mysqld"/
{
        @start[tid] = nsecs;
}

tracepoint:syscalls:sys_exit_fsync,tracepoint:syscalls:sys_exit_fdatasync
/comm == "mysqld"/
{
        @bytes = lhist((nsecs - @start[tid]) / 1000, 0, 1500, 100);
        delete(@start[tid]);
}
</code></pre>
<p>Which throws us this histogram, confirming that we’re seeing some <em>very</em> fast
<code>fsync</code>s:</p>
<pre><code><span><span><span>personal@napkin</span><span>:</span><span>~</span></span><span>$</span> <span><span>sudo</span> bpftrace fsync_latency.d</span></span>
<span>Attaching 4 probes...
^C

@bytes:
[0, 100)             439 |@@@@@@@@@@@@@@@                                     |
[100, 200)             8 |                                                    |
[200, 300)             2 |                                                    |
[300, 400)           242 |@@@@@@@@                                            |
[400, 500)          1495 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
[500, 600)           768 |@@@@@@@@@@@@@@@@@@@@@@@@@@                          |
[600, 700)           376 |@@@@@@@@@@@@@                                       |
[700, 800)           375 |@@@@@@@@@@@@@                                       |
[800, 900)           379 |@@@@@@@@@@@@@                                       |
[900, 1000)          322 |@@@@@@@@@@@                                         |
[1000, 1100)         256 |@@@@@@@@                                            |
[1100, 1200)         406 |@@@@@@@@@@@@@@                                      |
[1200, 1300)         690 |@@@@@@@@@@@@@@@@@@@@@@@@                            |
[1300, 1400)         803 |@@@@@@@@@@@@@@@@@@@@@@@@@@@                         |
[1400, 1500)         582 |@@@@@@@@@@@@@@@@@@@@                                |
[1500, ...)         1402 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    |
</span></code></pre>
<p>To understand exactly what’s going on here, we’d have to dig into the
file-system we’re using. This is going to be out of scope (otherwise I’m never
going to be sending anything out). But, to not leave you completely hanging,
presumably, <code>ext4</code> is using techniques similar to MySQL’s group commit to batch
writes together in the journal (equivalent to the write-ahead-log of MySQL). In
ext4’s vocabulary, this seems to be called <a href="https://www.kernel.org/doc/Documentation/filesystems/ext4.txt"><code>max_batch_time</code></a>, but the
documentation on this is scanty at best. The disk could also be doing this in
addition/instead of the file-system. If you know more about this, please
enlighten me!</p>
<p>The bottom-line is that <code>fsync</code> can perform faster during real-life workloads than the
1 ms I obtain on this machine from repeatedly writing and <code>fsync</code>ing a file. Most
likely from the ext4 equivalent of group commit, which we won’t see on a
benchmark that never does multiple <code>fsync</code>s in parallel.</p>
<p>This brings us back around to explaining the discrepancy between real-life and
the napkin-math of MySQL’s theoretical, maximum throughput. We are able to
achieve an at least 5x increase in throughput from raw <code>fsync</code> calls due to:</p>
<ol>
<li>MySQL merging multiple transactions into fewer <code>fsync</code>s through ‘group commits.’</li>
<li>The file-system and/or disk merging multiple <code>fsync</code>s performed in parallel
through its own ‘group commits’, yielding faster performance.</li>
</ol>
<p>In essence, the same technique of batching is used at every layer to improve
performance.</p>
<p>While we didn’t manage to explain <em>everything</em> that’s going on here, I certainly
learned a lot from this investigation. It’d be interesting light of this to play
with changing the <a href="https://mariadb.com/kb/en/group-commit-for-the-binary-log/#changing-group-commit-frequency">group commit settings</a> to optimize MySQL for throughput over
latency. This could also be tuned at the file-system level.</p>
<h2 id="problem-9-inverted-index">Problem 9: Inverted Index<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h2>
<p><a href="https://sirupsen.com/napkin/problem-9/">Last month, we looked at the inverted
index.</a> This data-structure is what’s
behind full-text search, and the way the documents are packed works well for set
intersections.</p>
<figure><img alt="" loading="lazy" width="1007" height="648" decoding="async" data-nimg="1" sizes="(min-width: 36rem) 36rem 100vw" srcset="https://sirupsen.com/_next/image?url=%2Fimages%2F66641ef5-efe4-440a-a616-0d30310e7540.png&amp;w=640&amp;q=75 640w, https://sirupsen.com/_next/image?url=%2Fimages%2F66641ef5-efe4-440a-a616-0d30310e7540.png&amp;w=750&amp;q=75 750w, https://sirupsen.com/_next/image?url=%2Fimages%2F66641ef5-efe4-440a-a616-0d30310e7540.png&amp;w=828&amp;q=75 828w, https://sirupsen.com/_next/image?url=%2Fimages%2F66641ef5-efe4-440a-a616-0d30310e7540.png&amp;w=1080&amp;q=75 1080w, https://sirupsen.com/_next/image?url=%2Fimages%2F66641ef5-efe4-440a-a616-0d30310e7540.png&amp;w=1200&amp;q=75 1200w, https://sirupsen.com/_next/image?url=%2Fimages%2F66641ef5-efe4-440a-a616-0d30310e7540.png&amp;w=1920&amp;q=75 1920w, https://sirupsen.com/_next/image?url=%2Fimages%2F66641ef5-efe4-440a-a616-0d30310e7540.png&amp;w=2048&amp;q=75 2048w, https://sirupsen.com/_next/image?url=%2Fimages%2F66641ef5-efe4-440a-a616-0d30310e7540.png&amp;w=3840&amp;q=75 3840w" src="https://sirupsen.com/_next/image?url=%2Fimages%2F66641ef5-efe4-440a-a616-0d30310e7540.png&amp;w=3840&amp;q=75"></figure>
<p><strong>(A) How long do you estimate it’d take to get the ids for <code>title AND see</code> with 2
million ids for title, and 1 million for see?</strong></p>
<p>Let’s assume that each document id is stored as a 64-bit integer. Then we’re
dealing with <code>1 * 10^6 * 64bit = 8 Mb</code> and <code>2 * 10^6 * 64 bit = 16 Mb</code>. If we
use an exceptionally simple set intersection algorithm of essentially two nested
for-loops, we need to scan ~<code>24Mb</code> of sequential memory. According to the
<a href="https://github.com/sirupsen/napkin-math">reference</a>, we can do this in <code>1Mb/100us * 24Mb = 2.4ms</code>.</p>
<p>Strangely, the Lucene <a href="https://home.apache.org/~mikemccand/lucenebench/AndHighHigh.html">nightly benchmarks</a> are performing these queries at
roughly 22 QPS, or <code>1000ms/22 = 45ms</code> per query. That’s substantially worse than
our prediction. I was ready to explain why Lucene might be <em>faster</em> (e.g. by
compressing postings to less than 64-bit), but not why it might be 20x slower!
We’ve got ourselves another first-principle gap.</p>
<p>Some slowness can be due to reading from disk, but since the access pattern is
sequential, it <a href="https://github.com/sirupsen/napkin-math">should only be 2-3x slower</a>. The hardware could be different
than the reference, but hardly anything that’d explain 20x. Sending the data to
the client might incur a large penalty, but again, 20x seems enormous. This type
of gap points towards missing something fundamental (as we saw with MySQL).
Unfortunately, this month I didn’t have time to dig much deeper than this, as I
prioritized the MySQL post.</p>
<p><strong>(B) What about title OR see?</strong></p>
<p>In this case we’d have to scan roughly as much memory, but handle more documents
and potentially transfer more back to the client. We’d expect to roughly be in
the same ballpark for performance ~<code>2.4ms</code>.</p>
<p>Lucene in this case is doing <a href="https://home.apache.org/~mikemccand/lucenebench/OrHighHigh.html">roughly half the throughput</a>, which aligns with
our relative expectations. But again, in absolute terms, Lucene’s handling these
queries in ~100ms, which is much, much higher than we expect.</p>
<p><strong>(C) How do the Lucene nightly benchmarks compare for (A) and (B)? This file
shows some of the actual terms used. If they don’t line up, how might you
explain the discrepency?</strong></p>
<p>Answered inline with (A) and (B).</p>
<p><strong>(D) Let’s imagine that we want title AND see and order the results by the last
modification date of each document. How long would you expect that to take?</strong></p>
<p>If the postings are not stored in that order, we’d naively expect in the worst
case we’d need to sort roughly ~24Mb of memory, <a href="https://github.com/sirupsen/napkin-math#numbers">at
5ms/Mb</a>. This would land us in the
<code>5mb/mb * 24mb ~= 120ms</code> query time ballpark.</p>
<p>In reality, this seems like an unintentional trick question. If ordered by last
modification date, they’d already be sorted in roughly that order, since new
documents are inserted to the end of the list. Which means they’re already
stored in <em>roughly</em> the right order, meaning our sort has to move far less bits
around. Even if that wasn’t the case, we could store sorted list for just this
column, which e.g. Lucene allows with doc values.</p><div><p>Subscribe through email,</p><!-- --> <p><a href="https://sirupsen.com/atom.xml">RSS</a></p><!-- --><p>or</p><!-- --> <p><a href="https://twitter.com/Sirupsen" title="Twitter" target="_blank" rel="noopener noreferrer">Twitter</a></p><!-- --><p>to new articles!</p><p> <!-- -->3,637<!-- --> <!-- -->subscribers</p></div><p><i>You might also like...</i></p><ul><li><a href="https://sirupsen.com/napkin/problem-14-using-checksums-to-verify">Using checksums to verify syncing 100M database records</a></li><li><a href="https://sirupsen.com/shitlists">Shitlist Driven Development</a></li><li><a href="https://sirupsen.com/napkin/neural-net">Neural Network From Scratch</a></li><li><a href="https://sirupsen.com/napkin/problem-9">Inverted Index Performance and Merkle Tree Syncronization</a></li><li><a href="https://sirupsen.com/napkin/problem-15">Increase HTTP Performance by Fitting In the Initial TCP Slow Start Window</a></li></ul></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[France rejects backdoor mandate (915 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2025/03/win-encryption-france-rejects-backdoor-mandate</link>
            <guid>43440513</guid>
            <pubDate>Fri, 21 Mar 2025 20:35:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2025/03/win-encryption-france-rejects-backdoor-mandate">https://www.eff.org/deeplinks/2025/03/win-encryption-france-rejects-backdoor-mandate</a>, See on <a href="https://news.ycombinator.com/item?id=43440513">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><span>In a moment of clarity after initially moving forward a deeply flawed piece of legislation, the French National Assembly has done the right thing: it rejected a dangerous proposal that would have gutted end-to-end encryption in the name of fighting drug trafficking. Despite heavy pressure from the Interior Ministry, </span><a href="https://www.lemonde.fr/societe/article/2025/03/21/l-assemblee-vote-pour-le-maintien-de-la-confidentialite-des-messageries-cryptees-lors-d-une-nuit-agitee_6584121_3224.html"><span>lawmakers voted Thursday night</span></a><span> (article in French) to strike down a provision that would have forced messaging platforms like Signal and WhatsApp to allow hidden access to private conversations.</span></p>
<p><span>The vote is a victory for digital rights, for privacy and security, and for common sense.</span></p>
<p><span>The proposed law was a surveillance wishlist disguised as anti-drug legislation. Tucked into its text was a resurrection of the </span><a href="https://www.eff.org/deeplinks/2019/01/give-ghost-backdoor-another-name"><span>widely discredited "ghost” participant model</span></a><span>—a backdoor that pretends not to be one. Under this scheme, law enforcement could silently join encrypted chats, undermining the very idea of private communication. Security experts have </span><a href="https://www.justsecurity.org/64968/why-the-ghost-keys-solution-to-encryption-is-no-solution/"><span>condemned</span></a><span> the approach, </span><a href="https://www.internetsociety.org/resources/doc/2020/fact-sheet-ghost-proposals/"><span>warning</span></a><span> it would introduce systemic vulnerabilities, damage trust in secure communication platforms, and create tools ripe for abuse.</span></p>
<p><span>The French lawmakers who voted this provision down deserve credit. They listened—not only to </span><a href="https://www.laquadrature.net/en/warondrugslaw/"><span>French digital rights organizations</span></a><span> and technologists, but also to basic principles of cybersecurity and civil liberties. They understood that encryption protects everyone, not just activists and dissidents, but also journalists, medical professionals, abuse survivors, and ordinary citizens trying to live private lives in an increasingly surveilled world.</span></p>
<h3><b>A Global Signal</b></h3>
<p><span>France’s rejection of the backdoor provision should send a message to legislatures around the world: you don’t have to sacrifice fundamental rights in the name of public safety. Encryption is not the enemy of justice; it’s a tool that </span><a href="https://www.eff.org/deeplinks/2024/03/european-court-human-rights-confirms-undermining-encryption-violates-fundamental"><span>supports our fundamental human rights</span></a><span>, including the right to have a private conversation. It is a pillar of modern democracy and cybersecurity.</span></p>
<p><span>As governments in the U.S., U.K., Australia, and elsewhere </span><a href="https://www.eff.org/deeplinks/2024/12/defending-encryption-us-and-abroad"><span>continue to flirt with anti-encryption laws</span></a><span>, this decision should serve as a model—and a warning. Undermining encryption doesn’t make society safer. It makes everyone more vulnerable.</span></p>
<p><span>This victory was not inevitable. It came after sustained public pressure, expert input, and tireless advocacy from civil society. It shows that pushing back works. But for the foreseeable future, misguided lobbyists for police national security agencies will continue to push similar proposals—perhaps repackaged, or rushed through quieter legislative moments.</span></p>
<p><span>Supporters of privacy should celebrate this win today. Tomorrow, we will continue to keep watch. </span></p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The little book about OS development (352 pts)]]></title>
            <link>https://littleosbook.github.io/</link>
            <guid>43440473</guid>
            <pubDate>Fri, 21 Mar 2025 20:30:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://littleosbook.github.io/">https://littleosbook.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=43440473">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">
<div id="TOC">
<h2>Contents</h2>
<ul>
<li><a href="#introduction"><span>1</span> Introduction</a><ul>
<li><a href="#about-the-book"><span>1.1</span> About the Book</a></li>
<li><a href="#the-reader"><span>1.2</span> The Reader</a></li>
<li><a href="#credits-thanks-and-acknowledgements"><span>1.3</span> Credits, Thanks and Acknowledgements</a></li>
<li><a href="#contributors"><span>1.4</span> Contributors</a></li>
<li><a href="#changes-and-corrections"><span>1.5</span> Changes and Corrections</a></li>
<li><a href="#issues-and-where-to-get-help"><span>1.6</span> Issues and where to get help</a></li>
<li><a href="#license"><span>1.7</span> License</a></li>
</ul></li>
<li><a href="#first-steps"><span>2</span> First Steps</a><ul>
<li><a href="#tools"><span>2.1</span> Tools</a><ul>
<li><a href="#quick-setup"><span>2.1.1</span> Quick Setup</a></li>
<li><a href="#programming-languages"><span>2.1.2</span> Programming Languages</a></li>
<li><a href="#host-operating-system"><span>2.1.3</span> Host Operating System</a></li>
<li><a href="#build-system"><span>2.1.4</span> Build System</a></li>
<li><a href="#virtual-machine"><span>2.1.5</span> Virtual Machine</a></li>
</ul></li>
<li><a href="#booting"><span>2.2</span> Booting</a><ul>
<li><a href="#bios"><span>2.2.1</span> BIOS</a></li>
<li><a href="#the-bootloader"><span>2.2.2</span> The Bootloader</a></li>
<li><a href="#the-operating-system"><span>2.2.3</span> The Operating System</a></li>
</ul></li>
<li><a href="#hello-cafebabe"><span>2.3</span> Hello Cafebabe</a><ul>
<li><a href="#compiling-the-operating-system"><span>2.3.1</span> Compiling the Operating System</a></li>
<li><a href="#linking-the-kernel"><span>2.3.2</span> Linking the Kernel</a></li>
<li><a href="#obtaining-grub"><span>2.3.3</span> Obtaining GRUB</a></li>
<li><a href="#building-an-iso-image"><span>2.3.4</span> Building an ISO Image</a></li>
<li><a href="#running-bochs"><span>2.3.5</span> Running Bochs</a></li>
</ul></li>
<li><a href="#further-reading"><span>2.4</span> Further Reading</a></li>
</ul></li>
<li><a href="#getting-to-c"><span>3</span> Getting to C</a><ul>
<li><a href="#setting-up-a-stack"><span>3.1</span> Setting Up a Stack</a></li>
<li><a href="#calling-c-code-from-assembly"><span>3.2</span> Calling C Code From Assembly</a><ul>
<li><a href="#packing-structs"><span>3.2.1</span> Packing Structs</a></li>
</ul></li>
<li><a href="#compiling-c-code"><span>3.3</span> Compiling C Code</a></li>
<li><a href="#build-tools"><span>3.4</span> Build Tools</a></li>
<li><a href="#further-reading-1"><span>3.5</span> Further Reading</a></li>
</ul></li>
<li><a href="#output"><span>4</span> Output</a><ul>
<li><a href="#interacting-with-the-hardware"><span>4.1</span> Interacting with the Hardware</a></li>
<li><a href="#the-framebuffer"><span>4.2</span> The Framebuffer</a><ul>
<li><a href="#writing-text"><span>4.2.1</span> Writing Text</a></li>
<li><a href="#moving-the-cursor"><span>4.2.2</span> Moving the Cursor</a></li>
<li><a href="#the-driver"><span>4.2.3</span> The Driver</a></li>
</ul></li>
<li><a href="#the-serial-ports"><span>4.3</span> The Serial Ports</a><ul>
<li><a href="#configuring-the-serial-port"><span>4.3.1</span> Configuring the Serial Port</a></li>
<li><a href="#configuring-the-line"><span>4.3.2</span> Configuring the Line</a></li>
<li><a href="#configuring-the-buffers"><span>4.3.3</span> Configuring the Buffers</a></li>
<li><a href="#configuring-the-modem"><span>4.3.4</span> Configuring the Modem</a></li>
<li><a href="#writing-data-to-the-serial-port"><span>4.3.5</span> Writing Data to the Serial Port</a></li>
<li><a href="#configuring-bochs"><span>4.3.6</span> Configuring Bochs</a></li>
<li><a href="#the-driver-1"><span>4.3.7</span> The Driver</a></li>
</ul></li>
<li><a href="#further-reading-2"><span>4.4</span> Further Reading</a></li>
</ul></li>
<li><a href="#segmentation"><span>5</span> Segmentation</a><ul>
<li><a href="#accessing-memory"><span>5.1</span> Accessing Memory</a></li>
<li><a href="#the-global-descriptor-table-gdt"><span>5.2</span> The Global Descriptor Table (GDT)</a></li>
<li><a href="#loading-the-gdt"><span>5.3</span> Loading the GDT</a></li>
<li><a href="#further-reading-3"><span>5.4</span> Further Reading</a></li>
</ul></li>
<li><a href="#interrupts-and-input"><span>6</span> Interrupts and Input</a><ul>
<li><a href="#interrupts-handlers"><span>6.1</span> Interrupts Handlers</a></li>
<li><a href="#creating-an-entry-in-the-idt"><span>6.2</span> Creating an Entry in the IDT</a></li>
<li><a href="#handling-an-interrupt"><span>6.3</span> Handling an Interrupt</a></li>
<li><a href="#creating-a-generic-interrupt-handler"><span>6.4</span> Creating a Generic Interrupt Handler</a></li>
<li><a href="#loading-the-idt"><span>6.5</span> Loading the IDT</a></li>
<li><a href="#programmable-interrupt-controller-pic"><span>6.6</span> Programmable Interrupt Controller (PIC)</a></li>
<li><a href="#reading-input-from-the-keyboard"><span>6.7</span> Reading Input from the Keyboard</a></li>
<li><a href="#further-reading-4"><span>6.8</span> Further Reading</a></li>
</ul></li>
<li><a href="#the-road-to-user-mode"><span>7</span> The Road to User Mode</a><ul>
<li><a href="#loading-an-external-program"><span>7.1</span> Loading an External Program</a><ul>
<li><a href="#grub-modules"><span>7.1.1</span> GRUB Modules</a></li>
</ul></li>
<li><a href="#executing-a-program"><span>7.2</span> Executing a Program</a><ul>
<li><a href="#a-very-simple-program"><span>7.2.1</span> A Very Simple Program</a></li>
<li><a href="#compiling"><span>7.2.2</span> Compiling</a></li>
<li><a href="#finding-the-program-in-memory"><span>7.2.3</span> Finding the Program in Memory</a></li>
<li><a href="#jumping-to-the-code"><span>7.2.4</span> Jumping to the Code</a></li>
</ul></li>
<li><a href="#the-beginning-of-user-mode"><span>7.3</span> The Beginning of User Mode</a></li>
</ul></li>
<li><a href="#a-short-introduction-to-virtual-memory"><span>8</span> A Short Introduction to Virtual Memory</a><ul>
<li><a href="#virtual-memory-through-segmentation"><span>8.1</span> Virtual Memory Through Segmentation?</a></li>
<li><a href="#further-reading-5"><span>8.2</span> Further Reading</a></li>
</ul></li>
<li><a href="#paging"><span>9</span> Paging</a><ul>
<li><a href="#why-paging"><span>9.1</span> Why Paging?</a></li>
<li><a href="#paging-in-x86"><span>9.2</span> Paging in x86</a><ul>
<li><a href="#identity-paging"><span>9.2.1</span> Identity Paging</a></li>
<li><a href="#enabling-paging"><span>9.2.2</span> Enabling Paging</a></li>
<li><a href="#a-few-details"><span>9.2.3</span> A Few Details</a></li>
</ul></li>
<li><a href="#paging-and-the-kernel"><span>9.3</span> Paging and the Kernel</a><ul>
<li><a href="#reasons-to-not-identity-map-the-kernel"><span>9.3.1</span> Reasons to Not Identity Map the Kernel</a></li>
<li><a href="#the-virtual-address-for-the-kernel"><span>9.3.2</span> The Virtual Address for the Kernel</a></li>
<li><a href="#placing-the-kernel-at-0xc0000000"><span>9.3.3</span> Placing the Kernel at <code>0xC0000000</code></a></li>
<li><a href="#higher-half-linker-script"><span>9.3.4</span> Higher-half Linker Script</a></li>
<li><a href="#entering-the-higher-half"><span>9.3.5</span> Entering the Higher Half</a></li>
<li><a href="#running-in-the-higher-half"><span>9.3.6</span> Running in the Higher Half</a></li>
</ul></li>
<li><a href="#virtual-memory-through-paging"><span>9.4</span> Virtual Memory Through Paging</a></li>
<li><a href="#further-reading-6"><span>9.5</span> Further Reading</a></li>
</ul></li>
<li><a href="#page-frame-allocation"><span>10</span> Page Frame Allocation</a><ul>
<li><a href="#managing-available-memory"><span>10.1</span> Managing Available Memory</a><ul>
<li><a href="#how-much-memory-is-there"><span>10.1.1</span> How Much Memory is There?</a></li>
<li><a href="#managing-available-memory-1"><span>10.1.2</span> Managing Available Memory</a></li>
</ul></li>
<li><a href="#how-can-we-access-a-page-frame"><span>10.2</span> How Can We Access a Page Frame?</a></li>
<li><a href="#a-kernel-heap"><span>10.3</span> A Kernel Heap</a></li>
<li><a href="#further-reading-7"><span>10.4</span> Further reading</a></li>
</ul></li>
<li><a href="#user-mode"><span>11</span> User Mode</a><ul>
<li><a href="#segments-for-user-mode"><span>11.1</span> Segments for User Mode</a></li>
<li><a href="#setting-up-for-user-mode"><span>11.2</span> Setting Up For User Mode</a></li>
<li><a href="#entering-user-mode"><span>11.3</span> Entering User Mode</a></li>
<li><a href="#using-c-for-user-mode-programs"><span>11.4</span> Using C for User Mode Programs</a><ul>
<li><a href="#a-c-library"><span>11.4.1</span> A C Library</a></li>
</ul></li>
<li><a href="#further-reading-8"><span>11.5</span> Further Reading</a></li>
</ul></li>
<li><a href="#file-systems"><span>12</span> File Systems</a><ul>
<li><a href="#why-a-file-system"><span>12.1</span> Why a File System?</a></li>
<li><a href="#a-simple-read-only-file-system"><span>12.2</span> A Simple Read-Only File System</a></li>
<li><a href="#inodes-and-writable-file-systems"><span>12.3</span> Inodes and Writable File Systems</a></li>
<li><a href="#a-virtual-file-system"><span>12.4</span> A Virtual File System</a></li>
<li><a href="#further-reading-9"><span>12.5</span> Further Reading</a></li>
</ul></li>
<li><a href="#system-calls"><span>13</span> System Calls</a><ul>
<li><a href="#designing-system-calls"><span>13.1</span> Designing System Calls</a></li>
<li><a href="#implementing-system-calls"><span>13.2</span> Implementing System Calls</a></li>
<li><a href="#further-reading-10"><span>13.3</span> Further Reading</a></li>
</ul></li>
<li><a href="#multitasking"><span>14</span> Multitasking</a><ul>
<li><a href="#creating-new-processes"><span>14.1</span> Creating New Processes</a></li>
<li><a href="#cooperative-scheduling-with-yielding"><span>14.2</span> Cooperative Scheduling with Yielding</a></li>
<li><a href="#preemptive-scheduling-with-interrupts"><span>14.3</span> Preemptive Scheduling with Interrupts</a><ul>
<li><a href="#programmable-interval-timer"><span>14.3.1</span> Programmable Interval Timer</a></li>
<li><a href="#separate-kernel-stacks-for-processes"><span>14.3.2</span> Separate Kernel Stacks for Processes</a></li>
<li><a href="#difficulties-with-preemptive-scheduling"><span>14.3.3</span> Difficulties with Preemptive Scheduling</a></li>
</ul></li>
<li><a href="#further-reading-11"><span>14.4</span> Further Reading</a></li>
</ul></li>
</ul>
</div>
<h2 id="introduction"> Introduction</h2>
<p>This text is a practical guide to writing your own x86 operating system. It is designed to give enough help with the technical details while at the same time not reveal too much with samples and code excerpts. We’ve tried to collect parts of the vast (and often excellent) expanse of material and tutorials available, on the web and otherwise, and add our own insights into the problems we encountered and struggled with.</p>
<p>This book is not about the theory behind operating systems, or how any specific operating system (OS) works. For OS theory we recommend the book <em>Modern Operating Systems</em> by Andrew Tanenbaum <span>[1]</span>. Lists and details on current operating systems are available on the Internet.</p>
<p>The starting chapters are quite detailed and explicit, to quickly get you into coding. Later chapters give more of an outline of what is needed, as more and more of the implementation and design becomes up to the reader, who should now be more familiar with the world of kernel development. At the end of some chapters there are links for further reading, which might be interesting and give a deeper understanding of the topics covered.</p>
<p>In <a href="#first-steps">chapter 2</a> and <a href="#getting-to-c">3</a> we set up our development environment and boot up our OS kernel in a virtual machine, eventually starting to write code in C. We continue in <a href="#output">chapter 4</a> with writing to the screen and the serial port, and then we dive into segmentation in <a href="#segmentation">chapter 5</a> and interrupts and input in <a href="#interrupts-and-input">chapter 6</a>.</p>
<p>After this we have a quite functional but bare-bones OS kernel. In <a href="#the-road-to-user-mode">chapter 7</a> we start the road to user mode applications, with virtual memory through paging (<a href="#a-short-introduction-to-virtual-memory">chapter 8</a> and <a href="#paging">9</a>), memory allocation (<a href="#page-frame-allocation">chapter 10</a>), and finally running a user application in <a href="#user-mode">chapter 11</a>.</p>
<p>In the last three chapters we discuss the more advanced topics of file systems (<a href="#file-systems">chapter 12</a>), system calls (<a href="#system-calls">chapter 13</a>), and multitasking (<a href="#multitasking">chapter 14</a>).</p>

<p>The OS kernel and this book were produced as part of an advanced individual course at the Royal Institute of Technology <span>[2]</span>, Stockholm. The authors had previously taken courses in OS theory, but had only minor practical experience with OS kernel development. In order to get more insight and a deeper understanding of how the theory from the previous OS courses works out in practice, the authors decided to create a new course, which focused on the development of a small OS. Another goal of the course was writing a thorough tutorial on how to develop a small OS basically from scratch, and this short book is the result.</p>
<p>The x86 architecture is, and has been for a long time, one of the most common hardware architectures. It was not a difficult choice to use the x86 architecture as the target of the OS, with its large community, extensive reference material and mature emulators. The documentation and information surrounding the details of the hardware we had to work with was not always easy to find or understand, despite (or perhaps due to) the age of the architecture.</p>
<p>The OS was developed in about six weeks of full-time work. The implementation was done in many small steps, and after each step the OS was tested manually. By developing in this incremental and iterative way, it was often easier to find any bugs that were introduced, since only a small part of the code had changed since the last known good state of the code. We encourage the reader to work in a similar way.</p>
<p>During the six weeks of development, almost every single line of code was written by the authors together (this way of working is also called <em>pair-programming</em>). It is our belief that we managed to avoid a lot of bugs due to this style of development, but this is hard to prove scientifically.</p>
<h2 id="the-reader"> The Reader</h2>
<p>The reader of this book should be comfortable with UNIX/Linux, systems programming, the C language and computer systems in general (such as hexadecimal notation <span>[3]</span>). This book could be a way to get started learning those things, but it will be more difficult, and developing an operating system is already challenging on its own. Search engines and other tutorials are often helpful if you get stuck.</p>
<h2 id="credits-thanks-and-acknowledgements"> Credits, Thanks and Acknowledgements</h2>
<p>We’d like to thank the OSDev community <span>[4]</span> for their great wiki and helpful members, and James Malloy for his eminent kernel development tutorial <span>[5]</span>. We’d also like to thank our supervisor Torbjörn Granlund for his insightful questions and interesting discussions.</p>
<p>Most of the CSS formatting of the book is based on the work by Scott Chacon for the book Pro Git, <a href="http://progit.org/">http://progit.org/</a>.</p>
<h2 id="contributors"> Contributors</h2>
<p>We are very grateful for the patches that people send us. The following users have all contributed to this book:</p>
<ul>
<li><a href="https://github.com/alexschneider">alexschneider</a></li>
<li><a href="https://github.com/Avidanborisov">Avidanborisov</a></li>
<li><a href="https://github.com/nirs">nirs</a></li>
<li><a href="https://github.com/kedarmhaswade">kedarmhaswade</a></li>
<li><a href="https://github.com/vamanea">vamanea</a></li>
<li><a href="https://github.com/ansjob">ansjob</a></li>
</ul>
<h2 id="changes-and-corrections"> Changes and Corrections</h2>
<p>This book is hosted on Github - if you have any suggestions, comments or corrections, just fork the book, write your changes, and send us a pull request. We’ll happily incorporate anything that makes this book better.</p>
<h2 id="issues-and-where-to-get-help"> Issues and where to get help</h2>
<p>If you run into problems while reading the book, please check the issues on Github for help: <a href="https://github.com/littleosbook/littleosbook/issues">https://github.com/littleosbook/littleosbook/issues</a>.</p>
<h2 id="license"> License</h2>
<p>All content is under the Creative Commons Attribution Non Commercial Share Alike 3.0 license, <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/us/">http://creativecommons.org/licenses/by-nc-sa/3.0/us/</a>. The code samples are in the public domain - use them however you want. References to this book are always received with warmth.</p>
<h2 id="first-steps"> First Steps</h2>
<p>Developing an operating system (OS) is no easy task, and the question “How do I even begin to solve this problem?” is likely to come up several times during the course of the project for different problems. This chapter will help you set up your development environment and booting a very small (and primitive) operating system.</p>

<h3 id="quick-setup"> Quick Setup</h3>
<p>We (the authors) have used Ubuntu <span>[6]</span> as the operating system for doing OS development, running it both physically and virtually (using the virtual machine VirtualBox <span>[7]</span>). A quick way to get everything up and running is to use the same setup as we did, since we know that these tools work with the samples provided in this book.</p>
<p>Once Ubuntu is installed, either physical or virtual, the following packages should be installed using <code>apt-get</code>:</p>
<pre><code>    <span>sudo</span> apt-get install build-essential nasm genisoimage bochs bochs-sdl</code></pre>
<h3 id="programming-languages"> Programming Languages</h3>
<p>The operating system will be developed using the C programming language <span>[8]</span><span>[9]</span>, using GCC <span>[10]</span>. We use C because developing an OS requires a very precise control of the generated code and direct access to memory. Other languages that provide the same features can also be used, but this book will only cover C.</p>
<p>The code will make use of one type attribute that is specific for GCC:</p>
<pre><code>    __attribute__((packed))</code></pre>
<p>This attribute allows us to ensure that the compiler uses a memory layout for a <code>struct</code> exactly as we define it in the code. This is explained in more detail in the next chapter.</p>
<p>Due to this attribute, the example code might be hard to compile using a C compiler other than GCC.</p>
<p>For writing assembly code, we have chosen NASM <span>[11]</span> as the assembler, since we prefer NASM’s syntax over GNU Assembler.</p>
<p>Bash <span>[12]</span> will be used as the scripting language throughout the book.</p>
<h3 id="host-operating-system"> Host Operating System</h3>
<p>All the code examples assumes that the code is being compiled on a UNIX like operating system. All code examples have been successfully compiled using Ubuntu <span>[6]</span> versions 11.04 and 11.10.</p>
<h3 id="build-system"> Build System</h3>
<p>Make <span>[13]</span> has been used when constructing the Makefile examples.</p>
<h3 id="virtual-machine"> Virtual Machine</h3>
<p>When developing an OS it is very convenient to be able to run your code in a <em>virtual machine</em> instead of on a physical computer, since starting your OS in a virtual machine is much faster than getting your OS onto a physical medium and then running it on a physical machine. Bochs <span>[14]</span> is an emulator for the x86 (IA-32) platform which is well suited for OS development due to its debugging features. Other popular choices are QEMU <span>[15]</span> and VirtualBox <span>[7]</span>. This book uses Bochs.</p>
<p>By using a virtual machine we cannot ensure that our OS works on real, physical hardware. The environment simulated by the virtual machine is designed to be very similar to their physical counterparts, and the OS can be tested on one by just copying the executable to a CD and finding a suitable machine.</p>
<h2 id="booting"> Booting</h2>
<p>Booting an operating system consists of transferring control along a chain of small programs, each one more “powerful” than the previous one, where the operating system is the last “program”. See the following figure for an example of the boot process:</p>
<div>
<p><img src="https://littleosbook.github.io/images/boot_chain.png" alt="An example of the boot process. Each box is a program."></p><p>An example of the boot process. Each box is a program.</p>
</div>
<h3 id="bios"> BIOS</h3>
<p>When the PC is turned on, the computer will start a small program that adheres to the <em>Basic Input Output System</em> (BIOS) <span>[16]</span> standard. This program is usually stored on a read only memory chip on the motherboard of the PC. The original role of the BIOS program was to export some library functions for printing to the screen, reading keyboard input etc. Modern operating systems do not use the BIOS’ functions, they use drivers that interact directly with the hardware, bypassing the BIOS. Today, BIOS mainly runs some early diagnostics (power-on-self-test) and then transfers control to the bootloader.</p>
<h3 id="the-bootloader"> The Bootloader</h3>
<p>The BIOS program will transfer control of the PC to a program called a <em>bootloader</em>. The bootloader’s task is to transfer control to us, the operating system developers, and our code. However, due to some restrictions<a href="#fn1" id="fnref1"><sup>1</sup></a> of the hardware and because of backward compatibility, the bootloader is often split into two parts: the first part of the bootloader will transfer control to the second part, which finally gives control of the PC to the operating system.</p>
<p>Writing a bootloader involves writing a lot of low-level code that interacts with the BIOS. Therefore, an existing bootloader will be used: the GNU GRand Unified Bootloader (GRUB) <span>[17]</span>.</p>
<p>Using GRUB, the operating system can be built as an ordinary ELF <span>[18]</span> executable, which will be loaded by GRUB into the correct memory location. The compilation of the kernel requires that the code is laid out in memory in a specific way (how to compile the kernel will be discussed later in this chapter).</p>
<h3 id="the-operating-system"> The Operating System</h3>
<p>GRUB will transfer control to the operating system by jumping to a position in memory. Before the jump, GRUB will look for a magic number to ensure that it is actually jumping to an OS and not some random code. This magic number is part of the <em>multiboot specification</em> <span>[19]</span> which GRUB adheres to. Once GRUB has made the jump, the OS has full control of the computer.</p>
<h2 id="hello-cafebabe"> Hello Cafebabe</h2>
<p>This section will describe how to implement of the smallest possible OS that can be used together with GRUB. The only thing the OS will do is write <code>0xCAFEBABE</code> to the <code>eax</code> register (most people would probably not even call this an OS).</p>
<h3 id="compiling-the-operating-system"> Compiling the Operating System</h3>
<p>This part of the OS has to be written in assembly code, since C requires a stack, which isn’t available (the chapter <a href="#getting-to-c">“Getting to C”</a> describes how to set one up). Save the following code in a file called <code>loader.s</code>:</p>
<pre><code>    <span>global</span> loader                   <span>; the entry symbol for ELF</span>

    MAGIC_NUMBER <span>equ</span> <span>0x1BADB002</span>     <span>; define the magic number constant</span>
    FLAGS        <span>equ</span><span> 0x0            </span><span>; multiboot flags</span>
    CHECKSUM     <span>equ</span> -MAGIC_NUMBER  <span>; calculate the checksum</span>
                                    <span>; (magic number + checksum + flags should equal 0)</span>

    <span>section</span> .text:                  <span>; start of the text (code) section</span>
    <span>align</span> <span>4</span>                         <span>; the code must be 4 byte aligned</span>
        <span>dd</span> MAGIC_NUMBER             <span>; write the magic number to the machine code,</span>
        <span>dd</span> FLAGS                    <span>; the flags,</span>
        <span>dd</span> CHECKSUM                 <span>; and the checksum</span>

<span>    loader:</span>                         <span>; the loader label (defined as entry point in linker script)</span>
        <span>mov</span> <span>eax</span>, <span>0xCAFEBABE</span>         <span>; place the number 0xCAFEBABE in the register eax</span>
<span>    .loop:</span>
        <span>jmp</span> .<span>loop</span>                   <span>; loop forever</span></code></pre>
<p>The only thing this OS will do is write the very specific number <code>0xCAFEBABE</code> to the <code>eax</code> register. It is <em>very</em> unlikely that the number <code>0xCAFEBABE</code> would be in the <code>eax</code> register if the OS did <em>not</em> put it there.</p>
<p>The file <code>loader.s</code> can be compiled into a 32 bits ELF <span>[18]</span> object file with the following command:</p>
<pre><code>    <span>nasm</span> -f elf32 loader.s</code></pre>
<h3 id="linking-the-kernel"> Linking the Kernel</h3>
<p>The code must now be linked to produce an executable file, which requires some extra thought compared to when linking most programs. We want GRUB to load the kernel at a memory address larger than or equal to <code>0x00100000</code> (1 megabyte (MB)), because addresses lower than 1 MB are used by GRUB itself, BIOS and memory-mapped I/O. Therefore, the following linker script is needed (written for GNU LD <span>[20]</span>):</p>
<pre><code>ENTRY(loader)                /* the name of the entry label */

SECTIONS {
    . = 0x00100000;          /* the code should be loaded at 1 MB */

    .text ALIGN (0x1000) :   /* align at 4 KB */
    {
        *(.text)             /* all text sections from all files */
    }

    .rodata ALIGN (0x1000) : /* align at 4 KB */
    {
        *(.rodata*)          /* all read-only data sections from all files */
    }

    .data ALIGN (0x1000) :   /* align at 4 KB */
    {
        *(.data)             /* all data sections from all files */
    }

    .bss ALIGN (0x1000) :    /* align at 4 KB */
    {
        *(COMMON)            /* all COMMON sections from all files */
        *(.bss)              /* all bss sections from all files */
    }
}</code></pre>
<p>Save the linker script into a file called <code>link.ld</code>. The executable can now be linked with the following command:</p>
<pre><code>    <span>ld</span> -T link.ld -melf_i386 loader.o -o kernel.elf</code></pre>
<p>The final executable will be called <code>kernel.elf</code>.</p>
<h3 id="obtaining-grub"> Obtaining GRUB</h3>
<p>The GRUB version we will use is GRUB Legacy, since the OS ISO image can then be generated on systems using both GRUB Legacy and GRUB 2. More specifically, the GRUB Legacy <code>stage2_eltorito</code> bootloader will be used. This file can be built from GRUB 0.97 by downloading the source from <a href="ftp://alpha.gnu.org/gnu/grub/grub-0.97.tar.gz">ftp://alpha.gnu.org/gnu/grub/grub-0.97.tar.gz</a>. However, the <code>configure</code> script doesn’t work well with Ubuntu <span>[21]</span>, so the binary file can be downloaded from <a href="http://littleosbook.github.com/files/stage2_eltorito">http://littleosbook.github.com/files/stage2_eltorito</a>. Copy the file <code>stage2_eltorito</code> to the folder that already contains <code>loader.s</code> and <code>link.ld</code>.</p>
<h3 id="building-an-iso-image"> Building an ISO Image</h3>
<p>The executable must be placed on a media that can be loaded by a virtual or physical machine. In this book we will use ISO <span>[22]</span> image files as the media, but one can also use floppy images, depending on what the virtual or physical machine supports.</p>
<p>We will create the kernel ISO image with the program <code>genisoimage</code>. A folder must first be created that contains the files that will be on the ISO image. The following commands create the folder and copy the files to their correct places:</p>
<pre><code>    <span>mkdir</span> -p iso/boot/grub              <span># create the folder structure</span>
    <span>cp</span> stage2_eltorito iso/boot/grub/   <span># copy the bootloader</span>
    <span>cp</span> kernel.elf iso/boot/             <span># copy the kernel</span></code></pre>
<p>A configuration file <code>menu.lst</code> for GRUB must be created. This file tells GRUB where the kernel is located and configures some options:</p>
<pre><code>    default=0
    timeout=0

    title os
    kernel /boot/kernel.elf</code></pre>
<p>Place the file <code>menu.lst</code> in the folder <code>iso/boot/grub/</code>. The contents of the <code>iso</code> folder should now look like the following figure:</p>
<pre><code>    iso
    |-- boot
      |-- grub
      | |-- menu.lst
      | |-- stage2_eltorito
      |-- kernel.elf</code></pre>
<p>The ISO image can then be generated with the following command:</p>
<pre><code>    genisoimage -R                              \
                -b boot/grub/stage2_eltorito    \
                -no-emul-boot                   \
                -boot-load-size 4               \
                -A os                           \
                -input-charset utf8             \
                -quiet                          \
                -boot-info-table                \
                -o os.iso                       \
                iso</code></pre>
<p>For more information about the flags used in the command, see the manual for <code>genisoimage</code>.</p>
<p>The ISO image <code>os.iso</code> now contains the kernel executable, the GRUB bootloader and the configuration file.</p>
<h3 id="running-bochs"> Running Bochs</h3>
<p>Now we can run the OS in the Bochs emulator using the <code>os.iso</code> ISO image. Bochs needs a configuration file to start and an example of a simple configuration file is given below:</p>
<pre><code>    megs:            32
    display_library: sdl
    romimage:        file=/usr/share/bochs/BIOS-bochs-latest
    vgaromimage:     file=/usr/share/bochs/VGABIOS-lgpl-latest
    ata0-master:     type=cdrom, path=os.iso, status=inserted
    boot:            cdrom
    log:             bochslog.txt
    clock:           sync=realtime, time0=local
    cpu:             count=1, ips=1000000</code></pre>
<p>You might need to change the path to <code>romimage</code> and <code>vgaromimage</code> depending on how you installed Bochs. More information about the Bochs config file can be found at Boch’s website <span>[23]</span>.</p>
<p>If you saved the configuration in a file named <code>bochsrc.txt</code> then you can run Bochs with the following command:</p>
<pre><code>    bochs -f bochsrc.txt -q</code></pre>
<p>The flag <code>-f</code> tells Bochs to use the given configuration file and the flag <code>-q</code> tells Bochs to skip the interactive start menu. You should now see Bochs starting and displaying a console with some information from GRUB on it.</p>
<p>After quitting Bochs, display the log produced by Boch:</p>
<pre><code>    cat bochslog.txt</code></pre>
<p>You should now see the contents of the registers of the CPU simulated by Bochs somewhere in the output. If you find <code>RAX=00000000CAFEBABE</code> or <code>EAX=CAFEBABE</code> (depending on if you are running Bochs with or without 64 bit support) in the output then your OS has successfully booted!</p>
<h2 id="further-reading"> Further Reading</h2>
<ul>
<li>Gustavo Duertes has written an in-depth article about what actually happens when a x86 computer boots up, <a href="http://duartes.org/gustavo/blog/post/how-computers-boot-up">http://duartes.org/gustavo/blog/post/how-computers-boot-up</a></li>
<li>Gustavo continues to describe what the kernel does in the very early stages at <a href="http://duartes.org/gustavo/blog/post/kernel-boot-process">http://duartes.org/gustavo/blog/post/kernel-boot-process</a></li>
<li>The OSDev wiki also contains a nice article about booting an x86 computer: <a href="http://wiki.osdev.org/Boot_Sequence">http://wiki.osdev.org/Boot_Sequence</a></li>
</ul>
<h2 id="getting-to-c"> Getting to C</h2>
<p>This chapter will show you how to use C instead of assembly code as the programming language for the OS. Assembly is very good for interacting with the CPU and enables maximum control over every aspect of the code. However, at least for the authors, C is a much more convenient language to use. Therefore, we would like to use C as much as possible and use assembly code only where it make sense.</p>
<h2 id="setting-up-a-stack"> Setting Up a Stack</h2>
<p>One prerequisite for using C is a stack, since all non-trivial C programs use a stack. Setting up a stack is not harder than to make the <code>esp</code> register point to the end of an area of free memory (remember that the stack grows towards lower addresses on the x86) that is correctly aligned (alignment on 4 bytes is recommended from a performance perspective).</p>
<p>We could point <code>esp</code> to a random area in memory since, so far, the only thing in the memory is GRUB, BIOS, the OS kernel and some memory-mapped I/O. This is not a good idea - we don’t know how much memory is available or if the area <code>esp</code> would point to is used by something else. A better idea is to reserve a piece of uninitialized memory in the <code>bss</code> section in the ELF file of the kernel. It is better to use the <code>bss</code> section instead of the <code>data</code> section to reduce the size of the OS executable. Since GRUB understands ELF, GRUB will allocate any memory reserved in the <code>bss</code> section when loading the OS.</p>
<p>The NASM pseudo-instruction <code>resb</code> <span>[24]</span> can be used to declare uninitialized data:</p>
<pre><code>    KERNEL_STACK_SIZE <span>equ</span> <span>4096</span>                  <span>; size of stack in bytes</span>

    <span>section</span> .bss
    <span>align</span> <span>4</span>                                     <span>; align at 4 bytes</span>
<span>    kernel_stack:</span>                               <span>; label points to beginning of memory</span>
        <span>resb</span> KERNEL_STACK_SIZE                  <span>; reserve stack for the kernel</span></code></pre>
<p>There is no need to worry about the use of uninitialized memory for the stack, since it is not possible to read a stack location that has not been written (without manual pointer fiddling). A (correct) program can not pop an element from the stack without having pushed an element onto the stack first. Therefore, the memory locations of the stack will always be written to before they are being read.</p>
<p>The stack pointer is then set up by pointing <code>esp</code> to the end of the <code>kernel_stack</code> memory:</p>
<pre><code>    <span>mov</span> <span>esp</span>, kernel_stack + KERNEL_STACK_SIZE   <span>; point esp to the start of the</span>
                                                <span>; stack (end of memory area)</span></code></pre>
<h2 id="calling-c-code-from-assembly"> Calling C Code From Assembly</h2>
<p>The next step is to call a C function from assembly code. There are many different conventions for how to call C code from assembly code <span>[25]</span>. This book uses the <em>cdecl</em> calling convention, since that is the one used by GCC. The cdecl calling convention states that arguments to a function should be passed via the stack (on x86). The arguments of the function should be pushed on the stack in a right-to-left order, that is, you push the rightmost argument first. The return value of the function is placed in the <code>eax</code> register. The following code shows an example:</p>
<pre><code>    <span>/* The C function */</span>
    <span>int</span> sum_of_three(<span>int</span> arg1, <span>int</span> arg2, <span>int</span> arg3)
    {
        <span>return</span> arg1 + arg2 + arg3;
    }</code></pre>
<pre><code>    <span>; The assembly code</span>
    external sum_of_three   <span>; the function sum_of_three is defined elsewhere</span>

    <span>push</span> <span>dword</span> <span>3</span>            <span>; arg3</span>
    <span>push</span> <span>dword</span> <span>2</span>            <span>; arg2</span>
    <span>push</span> <span>dword</span> <span>1</span>            <span>; arg1</span>
    <span>call</span> sum_of_three       <span>; call the function, the result will be in eax</span></code></pre>
<h3 id="packing-structs"> Packing Structs</h3>
<p>In the rest of this book, you will often come across “configuration bytes” that are a collection of bits in a very specific order. Below follows an example with 32 bits:</p>
<pre><code>Bit:     | 31     24 | 23          8 | 7     0 |
Content: | index     | address       | config  |</code></pre>
<p>Instead of using an unsigned integer, <code>unsigned int</code>, for handling such configurations, it is much more convenient to use “packed structures”:</p>
<pre><code>    <span>struct</span> example {
        <span>unsigned</span> <span>char</span> config;   <span>/* bit 0 - 7   */</span>
        <span>unsigned</span> <span>short</span> address; <span>/* bit 8 - 23  */</span>
        <span>unsigned</span> <span>char</span> index;    <span>/* bit 24 - 31 */</span>
    };</code></pre>
<p>When using the <code>struct</code> in the previous example there is no guarantee that the size of the <code>struct</code> will be exactly 32 bits - the compiler can add some padding between elements for various reasons, for example to speed up element access or due to requirements set by the hardware and/or compiler. When using a <code>struct</code> to represent configuration bytes, it is very important that the compiler does <em>not</em> add any padding, because the <code>struct</code> will eventually be treated as a 32 bit unsigned integer by the hardware. The attribute <code>packed</code> can be used to force GCC to <em>not</em> add any padding:</p>
<pre><code>    <span>struct</span> example {
        <span>unsigned</span> <span>char</span> config;   <span>/* bit 0 - 7   */</span>
        <span>unsigned</span> <span>short</span> address; <span>/* bit 8 - 23  */</span>
        <span>unsigned</span> <span>char</span> index;    <span>/* bit 24 - 31 */</span>
    } __attribute__((packed));</code></pre>
<p>Note that <code>__attribute__((packed))</code> is not part of the C standard - it might not work with all C compilers.</p>
<h2 id="compiling-c-code"> Compiling C Code</h2>
<p>When compiling the C code for the OS, a lot of flags to GCC need to be used. This is because the C code should <em>not</em> assume the presence of a standard library, since there is no standard library available for our OS. For more information about the flags, see the GCC manual.</p>
<p>The flags used for compiling the C code are:</p>
<pre><code>    -m32 -nostdlib -nostdinc -fno-builtin -fno-stack-protector -nostartfiles
    -nodefaultlibs</code></pre>
<p>As always when writing C programs we recommend turning on all warnings and treat warnings as errors:</p>
<pre><code>    -Wall -Wextra -Werror</code></pre>
<p>You can now create a function <code>kmain</code> in a file called <code>kmain.c</code> that you call from <code>loader.s</code>. At this point, <code>kmain</code> probably won’t need any arguments (but in later chapters it will).</p>

<p>Now is also probably a good time to set up some build tools to make it easier to compile and test-run the OS. We recommend using <code>make</code> <span>[13]</span>, but there are plenty of other build systems available. A simple Makefile for the OS could look like the following example:</p>
<pre><code>    <span>OBJECTS </span><span>=</span><span> loader.o kmain.o</span>
    <span>CC </span><span>=</span><span> gcc</span>
    <span>CFLAGS </span><span>=</span><span> -m32 -nostdlib -nostdinc -fno-builtin -fno-stack-protector </span><span>\</span>
             <span>-</span><span>nostartfiles -nodefaultlibs -Wall -Wextra -Werror -c</span>
    <span>LDFLAGS </span><span>=</span><span> -T link.ld -melf_i386</span>
    <span>AS </span><span>=</span><span> nasm</span>
    <span>ASFLAGS </span><span>=</span><span> -f elf</span>

    all: kernel.elf

    kernel.elf: <span>$(</span><span>OBJECTS</span><span>)</span>
        ld <span>$(</span><span>LDFLAGS</span><span>)</span> <span>$(</span><span>OBJECTS</span><span>)</span> -o kernel.elf

    os.iso: kernel.elf
        cp kernel.elf iso/boot/kernel.elf
        genisoimage -R                              \
                    <span>-</span><span>b boot/grub/stage2_eltorito    </span><span>\</span>
                    <span>-</span><span>no-emul-boot                   </span><span>\</span>
                    <span>-</span><span>boot-load-size 4               </span><span>\</span>
                    <span>-</span><span>A os                           </span><span>\</span>
                    <span>-</span><span>input-charset utf8             </span><span>\</span>
                    <span>-</span><span>quiet                          </span><span>\</span>
                    <span>-</span><span>boot-info-table                </span><span>\</span>
                    <span>-</span><span>o os.iso                       </span><span>\</span>
                    iso

    run: os.iso
        bochs -f bochsrc.txt -q

    %.o: %.c
        <span>$(</span><span>CC</span><span>)</span> <span>$(</span><span>CFLAGS</span><span>)</span>  <span>$&lt;</span> -o <span>$@</span>

    %.o: %.s
        <span>$(</span><span>AS</span><span>)</span> <span>$(</span><span>ASFLAGS</span><span>)</span> <span>$&lt;</span> -o <span>$@</span>

    clean:
        rm -rf *.o kernel.elf os.iso</code></pre>
<p>The contents of your working directory should now look like the following figure:</p>
<pre><code>    .
    |-- bochsrc.txt
    |-- iso
    |   |-- boot
    |     |-- grub
    |       |-- menu.lst
    |       |-- stage2_eltorito
    |-- kmain.c
    |-- loader.s
    |-- Makefile</code></pre>
<p>You should now be able to start the OS with the simple command <code>make run</code>, which will compile the kernel and boot it up in Bochs (as defined in the Makefile above).</p>
<h2 id="further-reading-1"> Further Reading</h2>
<ul>
<li>Kernigan &amp; Richie’s book, <em>The C Programming Language, Second Edition</em>, <span>[8]</span> is great for learning about all the aspects of C.</li>
</ul>
<h2 id="output"> Output</h2>
<p>This chapter will present how to display text on the console as well as writing data to the serial port. Furthermore, we will create our first <em>driver</em>, that is, code that acts as a layer between the kernel and the hardware, providing a higher abstraction than communicating directly with the hardware. The first part of this chapter is about creating a driver for the <em>framebuffer</em> <span>[26]</span> to be able to display text on the console. The second part shows how to create a driver for the serial port. Bochs can store output from the serial port in a file, effectively creating a logging mechanism for the operating system.</p>
<h2 id="interacting-with-the-hardware"> Interacting with the Hardware</h2>
<p>There are usually two different ways to interact with the hardware, <em>memory-mapped I/O</em> and <em>I/O ports</em>.</p>
<p>If the hardware uses memory-mapped I/O then you can write to a specific memory address and the hardware will be updated with the new data. One example of this is the framebuffer, which will be discussed in more detail later. For example, if you write the value <code>0x410F</code> to address <code>0x000B8000</code>, you will see the letter A in white color on a black background (see the section on <a href="#the-framebuffer">the framebuffer</a> for more details).</p>
<p>If the hardware uses I/O ports then the assembly code instructions <code>out</code> and <code>in</code> must be used to communicate with the hardware. The instruction <code>out</code> takes two parameters: the address of the I/O port and the data to send. The instruction <code>in</code> takes a single parameter, the address of the I/O port, and returns data from the hardware. One can think of I/O ports as communicating with hardware the same way as you communicate with a server using sockets. The cursor (the blinking rectangle) of the framebuffer is one example of hardware controlled via I/O ports on a PC.</p>
<h2 id="the-framebuffer"> The Framebuffer</h2>
<p>The framebuffer is a hardware device that is capable of displaying a buffer of memory on the screen <span>[26]</span>. The framebuffer has 80 columns and 25 rows, and the row and column indices start at 0 (so rows are labelled 0 - 24).</p>
<h3 id="writing-text"> Writing Text</h3>
<p>Writing text to the console via the framebuffer is done with memory-mapped I/O. The starting address of the memory-mapped I/O for the framebuffer is <code>0x000B8000</code> <span>[27]</span>. The memory is divided into 16 bit cells, where the 16 bits determine both the character, the foreground color and the background color. The highest eight bits is the ASCII <span>[28]</span> value of the character, bit 7 - 4 the background and bit 3 - 0 the foreground, as can be seen in the following figure:</p>
<pre><code>Bit:     | 15 14 13 12 11 10 9 8 | 7 6 5 4 | 3 2 1 0 |
Content: | ASCII                 | FG      | BG      |</code></pre>
<p>The available colors are shown in the following table:</p>
<table>
<thead>
<tr>
<th>Color</th>
<th>Value</th>
<th>Color</th>
<th>Value</th>
<th>Color</th>
<th>Value</th>
<th>Color</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Black</td>
<td>0</td>
<td>Red</td>
<td>4</td>
<td>Dark grey</td>
<td>8</td>
<td>Light red</td>
<td>12</td>
</tr>
<tr>
<td>Blue</td>
<td>1</td>
<td>Magenta</td>
<td>5</td>
<td>Light blue</td>
<td>9</td>
<td>Light magenta</td>
<td>13</td>
</tr>
<tr>
<td>Green</td>
<td>2</td>
<td>Brown</td>
<td>6</td>
<td>Light green</td>
<td>10</td>
<td>Light brown</td>
<td>14</td>
</tr>
<tr>
<td>Cyan</td>
<td>3</td>
<td>Light grey</td>
<td>7</td>
<td>Light cyan</td>
<td>11</td>
<td>White</td>
<td>15</td>
</tr>
</tbody>
</table>
<p>The first cell corresponds to row zero, column zero on the console. Using an ASCII table, one can see that A corresponds to 65 or <code>0x41</code>. Therefore, to write the character A with a green foreground (2) and dark grey background (8) at place (0,0), the following assembly code instruction is used:</p>
<pre><code>    <span>mov</span> [<span>0x000B8000</span>]<span>, 0x4128</span></code></pre>
<p>The second cell then corresponds to row zero, column one and its address is therefore:</p>
<pre><code>    0x000B8000 + 16 = 0x000B8010</code></pre>
<p>Writing to the framebuffer can also be done in C by treating the address <code>0x000B8000</code> as a char pointer, <code>char *fb = (char *) 0x000B8000</code>. Then, writing A at place (0,0) with green foreground and dark grey background becomes:</p>
<pre><code>    fb[<span>0</span>] = <span>'A'</span><span>;</span>
    fb[<span>1</span>] = <span>0x28</span><span>;</span></code></pre>
<p>The following code shows how this can be wrapped into a function:</p>
<pre><code>    <span>/** fb_write_cell:</span>
<span>     *  Writes a character with the given foreground and background to position i</span>
<span>     *  in the framebuffer.</span>
<span>     *</span>
<span>     *  </span><span>@param</span><span> </span><span>i</span><span>  The location in the framebuffer</span>
<span>     *  </span><span>@param</span><span> </span><span>c</span><span>  The character</span>
<span>     *  </span><span>@param</span><span> </span><span>fg</span><span> The foreground color</span>
<span>     *  </span><span>@param</span><span> </span><span>bg</span><span> The background color</span>
<span>     */</span>
    <span>void</span> fb_write_cell(<span>unsigned</span> <span>int</span> i, <span>char</span> c, <span>unsigned</span> <span>char</span> fg, <span>unsigned</span> <span>char</span> bg)
    {
        fb[i] = c;
        fb[i + <span>1</span>] = ((fg &amp; <span>0x0F</span>) &lt;&lt; <span>4</span>) | (bg &amp; <span>0x0F</span>)
    }</code></pre>
<p>The function can then be used as follows:</p>
<pre><code>    <span>#define FB_GREEN     2</span>
    <span>#define FB_DARK_GREY 8</span>

    fb_write_cell(<span>0</span>, 'A', FB_GREEN, FB_DARK_GREY);</code></pre>
<h3 id="moving-the-cursor"> Moving the Cursor</h3>
<p>Moving the cursor of the framebuffer is done via two different I/O ports. The cursor’s position is determined with a 16 bits integer: 0 means row zero, column zero; 1 means row zero, column one; 80 means row one, column zero and so on. Since the position is 16 bits large, and the <code>out</code> assembly code instruction argument is 8 bits, the position must be sent in two turns, first 8 bits then the next 8 bits. The framebuffer has two I/O ports, one for accepting the data, and one for describing the data being received. Port <code>0x3D4</code> <span>[29]</span> is the port that describes the data and port <code>0x3D5</code> <span>[29]</span> is for the data itself.</p>
<p>To set the cursor at row one, column zero (position <code>80 = 0x0050</code>), one would use the following assembly code instructions:</p>
<pre><code>    <span>out</span> <span>0x3D4</span>, <span>14</span>      <span>; 14 tells the framebuffer to expect the highest 8 bits of the position</span>
    <span>out</span> <span>0x3D5, 0x00    </span><span>; sending the highest 8 bits of 0x0050</span>
    <span>out</span> <span>0x3D4</span>, <span>15</span>      <span>; 15 tells the framebuffer to expect the lowest 8 bits of the position</span>
    <span>out</span> <span>0x3D5, 0x50    </span><span>; sending the lowest 8 bits of 0x0050</span></code></pre>
<p>The <code>out</code> assembly code instruction can’t be executed directly in C. Therefore it is a good idea to wrap <code>out</code> in a function in assembly code which can be accessed from C via the cdecl calling standard <span>[25]</span>:</p>
<pre><code>    <span>global</span> outb             <span>; make the label outb visible outside this file</span>

    <span>; outb - send a byte to an I/O port</span>
    <span>; stack: [esp + 8] the data byte</span>
    <span>;        [esp + 4] the I/O port</span>
    <span>;        [esp    ] return address</span>
<span>    outb:</span>
        <span>mov</span> <span>al</span>, [<span>esp</span> + <span>8</span>]    <span>; move the data to be sent into the al register</span>
        <span>mov</span> <span>dx</span>, [<span>esp</span> + <span>4</span>]    <span>; move the address of the I/O port into the dx register</span>
        <span>out</span> <span>dx</span>, <span>al</span>           <span>; send the data to the I/O port</span>
        <span>ret</span>                  <span>; return to the calling function</span></code></pre>
<p>By storing this function in a file called <code>io.s</code> and also creating a header <code>io.h</code>, the <code>out</code> assembly code instruction can be conveniently accessed from C:</p>
<pre><code>    <span>#ifndef INCLUDE_IO_H</span>
    <span>#define INCLUDE_IO_H</span>

    <span>/** outb:</span>
<span>     *  Sends the given data to the given I/O port. Defined in io.s</span>
<span>     *</span>
<span>     *  </span><span>@param</span><span> </span><span>port</span><span> The I/O port to send the data to</span>
<span>     *  </span><span>@param</span><span> </span><span>data</span><span> The data to send to the I/O port</span>
<span>     */</span>
    <span>void</span> outb(<span>unsigned</span> <span>short</span> port, <span>unsigned</span> <span>char</span> data);

    <span>#endif </span><span>/* INCLUDE_IO_H */</span></code></pre>
<p>Moving the cursor can now be wrapped in a C function:</p>
<pre><code>    <span>#include "io.h"</span>

    <span>/* The I/O ports */</span>
    <span>#define FB_COMMAND_PORT         0x3D4</span>
    <span>#define FB_DATA_PORT            0x3D5</span>

    <span>/* The I/O port commands */</span>
    <span>#define FB_HIGH_BYTE_COMMAND    14</span>
    <span>#define FB_LOW_BYTE_COMMAND     15</span>

    <span>/** fb_move_cursor:</span>
<span>     *  Moves the cursor of the framebuffer to the given position</span>
<span>     *</span>
<span>     *  </span><span>@param</span><span> </span><span>pos</span><span> The new position of the cursor</span>
<span>     */</span>
    <span>void</span> fb_move_cursor(<span>unsigned</span> <span>short</span> pos)
    {
        outb(FB_COMMAND_PORT, FB_HIGH_BYTE_COMMAND);
        outb(FB_DATA_PORT,    ((pos &gt;&gt; <span>8</span>) &amp; <span>0x00FF</span>));
        outb(FB_COMMAND_PORT, FB_LOW_BYTE_COMMAND);
        outb(FB_DATA_PORT,    pos &amp; <span>0x00FF</span>);
    }</code></pre>
<h3 id="the-driver"> The Driver</h3>
<p>The driver should provide an interface that the rest of the code in the OS will use for interacting with the framebuffer. There is no right or wrong in what functionality the interface should provide, but a suggestion is to have a <code>write</code> function with the following declaration:</p>
<pre><code>    <span>int</span> write(<span>char</span> *buf, <span>unsigned</span> <span>int</span> len);</code></pre>
<p>The <code>write</code> function writes the contents of the buffer <code>buf</code> of length <code>len</code> to the screen. The <code>write</code> function should automatically advance the cursor after a character has been written and scroll the screen if necessary.</p>
<h2 id="the-serial-ports"> The Serial Ports</h2>
<p>The serial port <span>[30]</span> is an interface for communicating between hardware devices and although it is available on almost all motherboards, it is seldom exposed to the user in the form of a DE-9 connector nowadays. The serial port is easy to use, and, more importantly, it can be used as a logging utility in Bochs. If a computer has support for a serial port, then it usually has support for multiple serial ports, but we will only make use of one of the ports. This is because we will only use the serial ports for logging. Furthermore, we will only use the serial ports for output, not input. The serial ports are completely controlled via I/O ports.</p>
<h3 id="configuring-the-serial-port"> Configuring the Serial Port</h3>
<p>The first data that need to be sent to the serial port is configuration data. In order for two hardware devices to be able to talk to each other they must agree upon a couple of things. These things include:</p>
<ul>
<li>The speed used for sending data (bit or baud rate)</li>
<li>If any error checking should be used for the data (parity bit, stop bits)</li>
<li>The number of bits that represent a unit of data (data bits)</li>
</ul>
<h3 id="configuring-the-line"> Configuring the Line</h3>
<p>Configuring the line means to configure how data is being sent over the line. The serial port has an I/O port, the <em>line command port</em>, that is used for configuration.</p>
<p>First the speed for sending data will be set. The serial port has an internal clock that runs at 115200 Hz. Setting the speed means sending a divisor to the serial port, for example sending 2 results in a speed of <code>115200 / 2 = 57600</code> Hz.</p>
<p>The divisor is a 16 bit number but we can only send 8 bits at a time. We must therefore send an instruction telling the serial port to first expect the highest 8 bits, then the lowest 8 bits. This is done by sending <code>0x80</code> to the line command port. An example is shown below:</p>
<pre><code>    <span>#include "io.h" </span><span>/* io.h is implement in the section "Moving the cursor" */</span>

    <span>/* The I/O ports */</span>

    <span>/* All the I/O ports are calculated relative to the data port. This is because</span>
<span>     * all serial ports (COM1, COM2, COM3, COM4) have their ports in the same</span>
<span>     * order, but they start at different values.</span>
<span>     */</span>

    <span>#define SERIAL_COM1_BASE                0x3F8      </span><span>/* COM1 base port */</span>

    <span>#define SERIAL_DATA_PORT(base)          (base)</span>
    <span>#define SERIAL_FIFO_COMMAND_PORT(base)  (base + 2)</span>
    <span>#define SERIAL_LINE_COMMAND_PORT(base)  (base + 3)</span>
    <span>#define SERIAL_MODEM_COMMAND_PORT(base) (base + 4)</span>
    <span>#define SERIAL_LINE_STATUS_PORT(base)   (base + 5)</span>

    <span>/* The I/O port commands */</span>

    <span>/* SERIAL_LINE_ENABLE_DLAB:</span>
<span>     * Tells the serial port to expect first the highest 8 bits on the data port,</span>
<span>     * then the lowest 8 bits will follow</span>
<span>     */</span>
    <span>#define SERIAL_LINE_ENABLE_DLAB         0x80</span>

    <span>/** serial_configure_baud_rate:</span>
<span>     *  Sets the speed of the data being sent. The default speed of a serial</span>
<span>     *  port is 115200 bits/s. The argument is a divisor of that number, hence</span>
<span>     *  the resulting speed becomes (115200 / divisor) bits/s.</span>
<span>     *</span>
<span>     *  </span><span>@param</span><span> </span><span>com</span><span>      The COM port to configure</span>
<span>     *  </span><span>@param</span><span> </span><span>divisor</span><span>  The divisor</span>
<span>     */</span>
    <span>void</span> serial_configure_baud_rate(<span>unsigned</span> <span>short</span> com, <span>unsigned</span> <span>short</span> divisor)
    {
        outb(SERIAL_LINE_COMMAND_PORT(com),
             SERIAL_LINE_ENABLE_DLAB);
        outb(SERIAL_DATA_PORT(com),
             (divisor &gt;&gt; <span>8</span>) &amp; <span>0x00FF</span>);
        outb(SERIAL_DATA_PORT(com),
             divisor &amp; <span>0x00FF</span>);
    }</code></pre>
<p>The way that data should be sent must be configured. This is also done via the line command port by sending a byte. The layout of the 8 bits looks like the following:</p>
<pre><code>Bit:     | 7 | 6 | 5 4 3 | 2 | 1 0 |
Content: | d | b | prty  | s | dl  |</code></pre>
<p>A description for each name can be found in the table below (and in <span>[31]</span>):</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>d</td>
<td>Enables (<code>d = 1</code>) or disables (<code>d = 0</code>) DLAB</td>
</tr>
<tr>
<td>b</td>
<td>If break control is enabled (<code>b = 1</code>) or disabled (<code>b = 0</code>)</td>
</tr>
<tr>
<td>prty</td>
<td>The number of parity bits to use</td>
</tr>
<tr>
<td>s</td>
<td>The number of stop bits to use (<code>s = 0</code> equals 1, <code>s = 1</code> equals 1.5 or 2)</td>
</tr>
<tr>
<td>dl</td>
<td>Describes the length of the data</td>
</tr>
</tbody>
</table>
<p>We will use the mostly standard value <code>0x03</code> <span>[31]</span>, meaning a length of 8 bits, no parity bit, one stop bit and break control disabled. This is sent to the line command port, as seen in the following example:</p>
<pre><code>    <span>/** serial_configure_line:</span>
<span>     *  Configures the line of the given serial port. The port is set to have a</span>
<span>     *  data length of 8 bits, no parity bits, one stop bit and break control</span>
<span>     *  disabled.</span>
<span>     *</span>
<span>     *  </span><span>@param</span><span> </span><span>com</span><span>  The serial port to configure</span>
<span>     */</span>
    <span>void</span> serial_configure_line(<span>unsigned</span> <span>short</span> com)
    {
        <span>/* Bit:     | 7 | 6 | 5 4 3 | 2 | 1 0 |</span>
<span>         * Content: | d | b | prty  | s | dl  |</span>
<span>         * Value:   | 0 | 0 | 0 0 0 | 0 | 1 1 | = 0x03</span>
<span>         */</span>
        outb(SERIAL_LINE_COMMAND_PORT(com), <span>0x03</span>);
    }</code></pre>
<p>The article on OSDev <span>[31]</span> has a more in-depth explanation of the values.</p>
<h3 id="configuring-the-buffers"> Configuring the Buffers</h3>
<p>When data is transmitted via the serial port it is placed in buffers, both when receiving and sending data. This way, if you send data to the serial port faster than it can send it over the wire, it will be buffered. However, if you send too much data too fast the buffer will be full and data will be lost. In other words, the buffers are FIFO queues. The FIFO queue configuration byte looks like the following figure:</p>
<pre><code>Bit:     | 7 6 | 5  | 4 | 3   | 2   | 1   | 0 |
Content: | lvl | bs | r | dma | clt | clr | e |</code></pre>
<p>A description for each name can be found in the table below:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>lvl</td>
<td>How many bytes should be stored in the FIFO buffers</td>
</tr>
<tr>
<td>bs</td>
<td>If the buffers should be 16 or 64 bytes large</td>
</tr>
<tr>
<td>r</td>
<td>Reserved for future use</td>
</tr>
<tr>
<td>dma</td>
<td>How the serial port data should be accessed</td>
</tr>
<tr>
<td>clt</td>
<td>Clear the transmission FIFO buffer</td>
</tr>
<tr>
<td>clr</td>
<td>Clear the receiver FIFO buffer</td>
</tr>
<tr>
<td>e</td>
<td>If the FIFO buffer should be enabled or not</td>
</tr>
</tbody>
</table>
<p>We use the value <code>0xC7 = 11000111</code> that:</p>
<ul>
<li>Enables FIFO</li>
<li>Clear both receiver and transmission FIFO queues</li>
<li>Use 14 bytes as size of queue</li>
</ul>
<p>The WikiBook on serial programming <span>[32]</span> explains the values in more depth.</p>
<h3 id="configuring-the-modem"> Configuring the Modem</h3>
<p>The modem control register is used for very simple hardware flow control via the Ready To Transmit (RTS) and Data Terminal Ready (DTR) pins. When configuring the serial port we want RTS and DTR to be 1, which means that we are ready to send data.</p>
<p>The modem configuration byte is shown in the following figure:</p>
<pre><code>Bit:     | 7 | 6 | 5  | 4  | 3   | 2   | 1   | 0   |
Content: | r | r | af | lb | ao2 | ao1 | rts | dtr |</code></pre>
<p>A description for each name can be found in the table below:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>r</td>
<td>Reserved</td>
</tr>
<tr>
<td>af</td>
<td>Autoflow control enabled</td>
</tr>
<tr>
<td>lb</td>
<td>Loopback mode (used for debugging serial ports)</td>
</tr>
<tr>
<td>ao2</td>
<td>Auxiliary output 2, used for receiving interrupts</td>
</tr>
<tr>
<td>ao1</td>
<td>Auxiliary output 1</td>
</tr>
<tr>
<td>rts</td>
<td>Ready To Transmit</td>
</tr>
<tr>
<td>dtr</td>
<td>Data Terminal Ready</td>
</tr>
</tbody>
</table>
<p>We don’t need to enable interrupts, because we won’t handle any received data. Therefore we use the configuration value <code>0x03 = 00000011</code> (RTS = 1 and DTS = 1).</p>
<h3 id="writing-data-to-the-serial-port"> Writing Data to the Serial Port</h3>
<p>Writing data to the serial port is done via the data I/O port. However, before writing, the transmit FIFO queue has to be empty (all previous writes must have finished). The transmit FIFO queue is empty if bit 5 of the line status I/O port is equal to one.</p>
<p>Reading the contents of an I/O port is done via the <code>in</code> assembly code instruction. There is no way to use the <code>in</code> assembly code instruction from C, therefore it has to be wrapped (the same way as the <code>out</code> assembly code instruction):</p>
<pre><code>    <span>global</span> inb

    <span>; inb - returns a byte from the given I/O port</span>
    <span>; stack: [esp + 4] The address of the I/O port</span>
    <span>;        [esp    ] The return address</span>
<span>    inb:</span>
        <span>mov</span> <span>dx</span>, [<span>esp</span> + <span>4</span>]       <span>; move the address of the I/O port to the dx register</span>
        <span>in</span>  <span>al</span>, <span>dx</span>              <span>; read a byte from the I/O port and store it in the al register</span>
        <span>ret</span>                     <span>; return the read byte</span></code></pre>
<pre><code>    <span>/* in file io.h */</span>

    <span>/** inb:</span>
<span>     *  Read a byte from an I/O port.</span>
<span>     *</span>
<span>     *  </span><span>@param</span><span>  </span><span>port</span><span> The address of the I/O port</span>
<span>     *  </span><span>@return</span><span>      The read byte</span>
<span>     */</span>
    <span>unsigned</span> <span>char</span> inb(<span>unsigned</span> <span>short</span> port);</code></pre>
<p>Checking if the transmit FIFO is empty can then be done from C:</p>
<pre><code>    <span>#include "io.h"</span>

    <span>/** serial_is_transmit_fifo_empty:</span>
<span>     *  Checks whether the transmit FIFO queue is empty or not for the given COM</span>
<span>     *  port.</span>
<span>     *</span>
<span>     *  </span><span>@param</span><span>  </span><span>com</span><span> The COM port</span>
<span>     *  </span><span>@return</span><span> 0 if the transmit FIFO queue is not empty</span>
<span>     *          1 if the transmit FIFO queue is empty</span>
<span>     */</span>
    <span>int</span> serial_is_transmit_fifo_empty(<span>unsigned</span> <span>int</span> com)
    {
        <span>/* 0x20 = 0010 0000 */</span>
        <span>return</span> inb(SERIAL_LINE_STATUS_PORT(com)) &amp; <span>0x20</span>;
    }</code></pre>
<p>Writing to a serial port means spinning as long as the transmit FIFO queue isn’t empty, and then writing the data to the data I/O port.</p>
<h3 id="configuring-bochs"> Configuring Bochs</h3>
<p>To save the output from the first serial serial port the Bochs configuration file <code>bochsrc.txt</code> must be updated. The <code>com1</code> configuration instructs Bochs how to handle first serial port:</p>
<pre><code>    com1: enabled=1, mode=file, dev=com1.out</code></pre>
<p>The output from serial port one will now be stored in the file <code>com1.out</code>.</p>
<h3 id="the-driver-1"> The Driver</h3>
<p>We recommend that you implement a <code>write</code> function for the serial port similar to the <code>write</code> function in the driver for the framebuffer. To avoid name clashes with the <code>write</code> function for the framebuffer it is a good idea to name the functions <code>fb_write</code> and <code>serial_write</code> to distinguish them.</p>
<p>We further recommend that you try to write a <code>printf</code>-like function, see section 7.3 in <span>[8]</span>. The <code>printf</code> function could take an additional argument to decide to which device to write the output (framebuffer or serial).</p>
<p>A final recommendation is that you create some way of distinguishing the severeness of the log messages, for example by prepending the messages with <code>DEBUG</code>, <code>INFO</code> or <code>ERROR</code>.</p>
<h2 id="further-reading-2"> Further Reading</h2>
<ul>
<li>The book “Serial programming” (available on WikiBooks) has a great section on programming the serial port, <a href="http://en.wikibooks.org/wiki/Serial_Programming/8250_UART_Programming#UART_Registers">http://en.wikibooks.org/wiki/Serial_Programming/8250_UART_Programming#UART_Registers</a></li>
<li>The OSDev wiki has a page with a lot of information about the serial ports, <a href="http://wiki.osdev.org/Serial_ports">http://wiki.osdev.org/Serial_ports</a></li>
</ul>
<h2 id="segmentation"> Segmentation</h2>
<p><em>Segmentation</em> in x86 means accessing the memory through segments. Segments are portions of the address space, possibly overlapping, specified by a base address and a limit. To address a byte in segmented memory you use a 48-bit <em>logical address</em>: 16 bits that specifies the segment and 32-bits that specifies what offset within that segment you want. The offset is added to the base address of the segment, and the resulting linear address is checked against the segment’s limit - see the figure below. If everything works out fine (including access-rights checks ignored for now) the result is a <em>linear address</em>. When paging is disabled, then the linear address space is mapped 1:1 onto the <em>physical address</em> space, and the physical memory can be accessed. (See the chapter <a href="#paging">“Paging”</a> for how to enable paging.)</p>
<div>
<p><img src="https://littleosbook.github.io/images/intel_3_5_logical_to_linear.png" alt="Translation of logical addresses to linear addresses."></p><p>Translation of logical addresses to linear addresses.</p>
</div>
<p>To enable segmentation you need to set up a table that describes each segment - a <em>segment descriptor table</em>. In x86, there are two types of descriptor tables: the <em>Global Descriptor Table</em> (GDT) and <em>Local Descriptor Tables</em> (LDT). An LDT is set up and managed by user-space processes, and all processes have their own LDT. LDTs can be used if a more complex segmentation model is desired - we won’t use it. The GDT is shared by everyone - it’s global.</p>
<p>As we discuss in the sections on virtual memory and paging, segmentation is rarely used more than in a minimal setup, similar to what we do below.</p>
<h2 id="accessing-memory"> Accessing Memory</h2>
<p>Most of the time when accessing memory there is no need to explicitly specify the segment to use. The processor has six 16-bit segment registers: <code>cs</code>, <code>ss</code>, <code>ds</code>, <code>es</code>, <code>gs</code> and <code>fs</code>. The register <code>cs</code> is the code segment register and specifies the segment to use when fetching instructions. The register <code>ss</code> is used whenever accessing the stack (through the stack pointer <code>esp</code>), and <code>ds</code> is used for other data accesses. The OS is free to use the registers <code>es</code>, <code>gs</code> and <code>fs</code> however it want.</p>
<p>Below is an example showing implicit use of the segment registers:</p>
<pre><code><span>    func:</span>
        <span>mov</span> <span>eax</span>, [<span>esp</span><span>+4</span>]
        <span>mov</span> <span>ebx</span>, [<span>eax</span>]
        <span>add</span> <span>ebx</span>, <span>8</span>
        <span>mov</span> [<span>eax</span>], <span>ebx</span>
        <span>ret</span></code></pre>
<p>The above example can be compared with the following one that makes explicit use of the segment registers:</p>
<pre><code><span>    func:</span>
        <span>mov</span> <span>eax</span>, [<span>ss</span>:<span>esp</span><span>+4</span>]
        <span>mov</span> <span>ebx</span>, [<span>ds</span>:<span>eax</span>]
        <span>add</span> <span>ebx</span>, <span>8</span>
        <span>mov</span> [<span>ds</span>:<span>eax</span>], <span>ebx</span>
        <span>ret</span></code></pre>
<p>You don’t need to use <code>ss</code> for storing the stack segment selector, or <code>ds</code> for the data segment selector. You could store the stack segment selector in <code>ds</code> and vice versa. However, in order to use the implicit style shown above, you must store the segment selectors in their indented registers.</p>
<p>Segment descriptors and their fields are described in figure 3-8 in the Intel manual <span>[33]</span>.</p>
<h2 id="the-global-descriptor-table-gdt"> The Global Descriptor Table (GDT)</h2>
<p>A GDT/LDT is an array of 8-byte segment descriptors. The first descriptor in the GDT is always a null descriptor and can never be used to access memory. At least two segment descriptors (plus the null descriptor) are needed for the GDT, because the descriptor contains more information than just the base and limit fields. The two most relevant fields for us are the <em>Type</em> field and the <em>Descriptor Privilege Level</em> (DPL) field.</p>
<p>Table 3-1 in chapter 3 of the Intel manual <span>[33]</span> specifies the values for the Type field. The table shows that the Type field can’t be both writable <em>and</em> executable at the same time. Therefore, two segments are needed: one segment for executing code to put in <code>cs</code> (Type is Execute-only or Execute-Read) and one segment for reading and writing data (Type is Read/Write) to put in the other segment registers.</p>
<p>The DPL specifies the <em>privilege levels</em> required to use the segment. x86 allows for four privilege levels (PL), 0 to 3, where PL0 is the most privileged. In most operating systems (eg. Linux and Windows), only PL0 and PL3 are used. However, some operating system, such as MINIX, make use of all levels. The kernel should be able to do anything, therefore it uses segments with DPL set to 0 (also called kernel mode). The current privilege level (CPL) is determined by the segment selector in <code>cs</code>.</p>
<p>The segments needed are described in the table below.</p>
<table>
<caption>The segment descriptors needed.</caption>
<thead>
<tr>
<th>Index</th>
<th>Offset</th>
<th>Name</th>
<th>Address range</th>
<th>Type</th>
<th>DPL</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td><code>0x00</code></td>
<td>null descriptor</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td><code>0x08</code></td>
<td>kernel code segment</td>
<td><code>0x00000000 - 0xFFFFFFFF</code></td>
<td>RX</td>
<td>PL0</td>
</tr>
<tr>
<td>2</td>
<td><code>0x10</code></td>
<td>kernel data segment</td>
<td><code>0x00000000 - 0xFFFFFFFF</code></td>
<td>RW</td>
<td>PL0</td>
</tr>
</tbody>
</table>
<p>Note that the segments overlap - they both encompass the entire linear address space. In our minimal setup we’ll only use segmentation to get privilege levels. See the Intel manual <span>[33]</span>, chapter 3, for details on the other descriptor fields.</p>
<h2 id="loading-the-gdt"> Loading the GDT</h2>
<p>Loading the GDT into the processor is done with the <code>lgdt</code> assembly code instruction, which takes the address of a struct that specifies the start and size of the GDT. It is easiest to encode this information using a <a href="#packing-structs">“packed struct”</a> as shown in the following example:</p>
<pre><code>    <span>struct</span> gdt {
        <span>unsigned</span> <span>int</span> address;
        <span>unsigned</span> <span>short</span> size;
    } __attribute__((packed));</code></pre>
<p>If the content of the <code>eax</code> register is the address to such a struct, then the GDT can be loaded with the assembly code shown below:</p>
<pre><code>    <span>lgdt</span> [<span>eax</span>]</code></pre>
<p>It might be easier if you make this instruction available from C, the same way as was done with the assembly code instructions <code>in</code> and <code>out</code>.</p>
<p>After the GDT has been loaded the segment registers needs to be loaded with their corresponding segment selectors. The content of a segment selector is described in the figure and table below:</p>
<pre><code>Bit:     | 15                                3 | 2  | 1 0 |
Content: | offset (index)                      | ti | rpl |</code></pre>
<table>
<caption>The layout of segment selectors.</caption>
<colgroup><col width="23%">
<col width="76%">
</colgroup><thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>rpl</td>
<td>Requested Privilege Level - we want to execute in PL0 for now.</td>
</tr>
<tr>
<td>ti</td>
<td>Table Indicator. 0 means that this specifies a GDT segment, 1 means an LDT Segment.</td>
</tr>
<tr>
<td>offset (index)</td>
<td>Offset within descriptor table.</td>
</tr>
</tbody>
</table>
<p>The offset of the segment selector is added to the start of the GDT to get the address of the segment descriptor: <code>0x08</code> for the first descriptor and <code>0x10</code> for the second, since each descriptor is 8 bytes. The Requested Privilege Level (RPL) should be <code>0</code> since the kernel of the OS should execute in privilege level 0.</p>
<p>Loading the segment selector registers is easy for the data registers - just copy the correct offsets to the registers:</p>
<pre><code>    <span>mov</span> <span>ds</span><span>, 0x10</span>
    <span>mov</span> <span>ss</span><span>, 0x10</span>
    <span>mov</span> <span>es</span><span>, 0x10</span>
    .
    .
    .</code></pre>
<p>To load <code>cs</code> we have to do a “far jump”:</p>
<pre><code>    <span>; code here uses the previous cs</span>
    <span>jmp</span> <span>0x08</span>:flush_cs   <span>; specify cs when jumping to flush_cs</span>

<span>    flush_cs:</span>
        <span>; now we've changed cs to 0x08</span></code></pre>
<p>A far jump is a jump where we explicitly specify the full 48-bit logical address: the segment selector to use and the absolute address to jump to. It will first set <code>cs</code> to <code>0x08</code> and then jump to <code>flush_cs</code> using its absolute address.</p>
<h2 id="further-reading-3"> Further Reading</h2>
<ul>
<li>Chapter 3 of the Intel manual <span>[33]</span> is filled with low-level and technical details about segmentation.</li>
<li>The OSDev wiki has a page about segmentation: <a href="http://wiki.osdev.org/Segmentation">http://wiki.osdev.org/Segmentation</a></li>
<li>The Wikipedia page on x86 segmentation might be worth looking into: <a href="http://en.wikipedia.org/wiki/X86_memory_segmentation">http://en.wikipedia.org/wiki/X86_memory_segmentation</a></li>
</ul>
<h2 id="interrupts-and-input"> Interrupts and Input</h2>
<p>Now that the OS can produce <em>output</em> it would be nice if it also could get some <em>input</em>. (The operating system must be able to handle <em>interrupts</em> in order to read information from the keyboard). An interrupt occurs when a hardware device, such as the keyboard, the serial port or the timer, signals the CPU that the state of the device has changed. The CPU itself can also send interrupts due to program errors, for example when a program references memory it doesn’t have access to, or when a program divides a number by zero. Finally, there are also <em>software intterupts</em>, which are interrupts that are caused by the <code>int</code> assembly code instruction, and they are often used for system calls.</p>
<h2 id="interrupts-handlers"> Interrupts Handlers</h2>
<p>Interrupts are handled via the <em>Interrupt Descriptor Table</em> (IDT). The IDT describes a handler for each interrupt. The interrupts are numbered (0 - 255) and the handler for interrupt <em>i</em> is defined at the <em>ith</em> position in the table. There are three different kinds of handlers for interrupts:</p>
<ul>
<li>Task handler</li>
<li>Interrupt handler</li>
<li>Trap handler</li>
</ul>
<p>The task handlers use functionality specific to the Intel version of x86, so they won’t be covered here (see the Intel manual <span>[33]</span>, chapter 6, for more info). The only difference between an interrupt handler and a trap handler is that the interrupt handler disables interrupts, which means you cannot get an interrupt while at the same time handling an interrupt. In this book, we will use trap handlers and disable interrupts manually when we need to.</p>
<h2 id="creating-an-entry-in-the-idt"> Creating an Entry in the IDT</h2>
<p>An entry in the IDT for an interrupt handler consists of 64 bits. The highest 32 bits are shown in the figure below:</p>
<pre><code>Bit:     | 31              16 | 15 | 14 13 | 12 | 11 | 10 9 8 | 7 6 5 | 4 3 2 1 0 |
Content: | offset high        | P  | DPL   | 0  | D  | 1  1 0 | 0 0 0 | reserved  |</code></pre>
<p>The lowest 32 bits are presented in the following figure:</p>
<pre><code>Bit:     | 31              16 | 15              0 |
Content: | segment selector   | offset low        |</code></pre>
<p>A description for each name can be found in the table below:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>offset high</td>
<td>The 16 highest bits of the 32 bit address in the segment.</td>
</tr>
<tr>
<td>offset low</td>
<td>The 16 lowest bits of the 32 bits address in the segment.</td>
</tr>
<tr>
<td>p</td>
<td>If the handler is present in memory or not (1 = present, 0 = not present).</td>
</tr>
<tr>
<td>DPL</td>
<td>Descriptor Privilige Level, the privilege level the handler can be called from (0, 1, 2, 3).</td>
</tr>
<tr>
<td>D</td>
<td>Size of gate, (1 = 32 bits, 0 = 16 bits).</td>
</tr>
<tr>
<td>segment selector</td>
<td>The offset in the GDT.</td>
</tr>
<tr>
<td>r</td>
<td>Reserved.</td>
</tr>
</tbody>
</table>
<p>The offset is a pointer to code (preferably an assembly code label). For example, to create an entry for a handler whose code starts at <code>0xDEADBEEF</code> and that runs in privilege level 0 (therefore using the same code segment selector as the kernel) the following two bytes would be used:</p>
<pre><code>    0xDEAD8E00
    0x0008BEEF</code></pre>
<p>If the IDT is represented as an <code>unsigned integer idt[512]</code> then to register the above example as an handler for interrupt 0 (divide-by-zero), the following code would be used:</p>
<pre><code>    idt[<span>0</span>] = <span>0xDEAD8E00</span>
    idt[<span>1</span>] = <span>0x0008BEEF</span></code></pre>
<p>As written in the chapter <a href="#getting-to-c">“Getting to C”</a>, we recommend that you instead of using bytes (or unsigned integers) use packed structures to make the code more readable.</p>
<h2 id="handling-an-interrupt"> Handling an Interrupt</h2>
<p>When an interrupt occurs the CPU will push some information about the interrupt onto the stack, then look up the appropriate interrupt hander in the IDT and jump to it. The stack at the time of the interrupt will look like the following:</p>
<pre><code>    [esp + 12] eflags
    [esp + 8]  cs
    [esp + 4]  eip
    [esp]      error code?</code></pre>
<p>The reason for the question mark behind error code is that not all interrupts create an <em>error code</em>. The specific CPU interrupts that put an error code on the stack are 8, 10, 11, 12, 13, 14 and 17. The error code can be used by the interrupt handler to get more information on what has happened. Also, note that the interrupt <em>number</em> is <em>not</em> pushed onto the stack. We can only determine what interrupt has occurred by knowing what code is executing - if the handler registered for interrupt 17 is executing, then interrupt 17 has occurred.</p>
<p>Once the interrupt handler is done, it uses the <code>iret</code> instruction to return. The instruction <code>iret</code> expects the stack to be the same as at the time of the interrupt (see the figure above). Therefore, any values pushed onto the stack by the interrupt handler must be popped. Before returning, <code>iret</code> restores <code>eflags</code> by popping the value from the stack and then finally jumps to <code>cs:eip</code> as specified by the values on the stack.</p>
<p>The interrupt handler has to be written in assembly code, since all registers that the interrupt handlers use must be preserved by pushing them onto the stack. This is because the code that was interrupted doesn’t know about the interrupt and will therefore expect that its registers stay the same. Writing all the logic of the interrupt handler in assembly code will be tiresome. Creating a handler in assembly code that saves the registers, calls a C function, restores the registers and finally executes <code>iret</code> is a good idea!</p>
<p>The C handler should get the state of the registers, the state of the stack and the number of the interrupt as arguments. The following definitions can for example be used:</p>
<pre><code>    <span>struct</span> cpu_state {
        <span>unsigned</span> <span>int</span> eax;
        <span>unsigned</span> <span>int</span> ebx;
        <span>unsigned</span> <span>int</span> ecx;
        .
        .
        .
        <span>unsigned</span> <span>int</span> esp;
    } __attribute__((packed));

    <span>struct</span> stack_state {
        <span>unsigned</span> <span>int</span> error_code;
        <span>unsigned</span> <span>int</span> eip;
        <span>unsigned</span> <span>int</span> cs;
        <span>unsigned</span> <span>int</span> eflags;
    } __attribute__((packed));

    <span>void</span> interrupt_handler(<span>struct</span> cpu_state cpu, <span>struct</span> stack_state stack, <span>unsigned</span> <span>int</span> interrupt);</code></pre>
<h2 id="creating-a-generic-interrupt-handler"> Creating a Generic Interrupt Handler</h2>
<p>Since the CPU does not push the interrupt number on the stack it is a little tricky to write a generic interrupt handler. This section will use macros to show how it can be done. Writing one version for each interrupt is tedious - it is better to use the macro functionality of NASM <span>[34]</span>. And since not all interrupts produce an error code the value 0 will be added as the “error code” for interrupts without an error code. The following code shows an example of how this can be done:</p>
<pre><code>    <span>%macro no_error_code_interrupt_handler %1</span>
    <span>global</span> interrupt_handler_<span>%1</span>
    interrupt_handler_<span>%1:</span>
        <span>push</span>    <span>dword</span> <span>0</span>                     <span>; push 0 as error code</span>
        <span>push</span>    <span>dword</span> <span>%1                    ; push the interrupt number</span>
        <span>jmp</span>     common_interrupt_handler    <span>; jump to the common handler</span>
    <span>%endmacro</span>

    <span>%macro error_code_interrupt_handler %1</span>
    <span>global</span> interrupt_handler_<span>%1</span>
    interrupt_handler_<span>%1:</span>
        <span>push</span>    <span>dword</span> <span>%1                    ; push the interrupt number</span>
        <span>jmp</span>     common_interrupt_handler    <span>; jump to the common handler</span>
    <span>%endmacro</span>

<span>    common_interrupt_handler:</span>               <span>; the common parts of the generic interrupt handler</span>
        <span>; save the registers</span>
        <span>push</span>    <span>eax</span>
        <span>push</span>    <span>ebx</span>
        .
        .
        .
        <span>push</span>    <span>ebp</span>

        <span>; call the C function</span>
        <span>call</span>    interrupt_handler

        <span>; restore the registers</span>
        <span>pop</span>     <span>ebp</span>
        .
        .
        .
        <span>pop</span>     <span>ebx</span>
        <span>pop</span>     <span>eax</span>

        <span>; restore the esp</span>
        <span>add</span>     <span>esp</span>, <span>8</span>

        <span>; return to the code that got interrupted</span>
        <span>iret</span>

    no_error_code_interrupt_handler <span>0</span>       <span>; create handler for interrupt 0</span>
    no_error_code_interrupt_handler <span>1</span>       <span>; create handler for interrupt 1</span>
    .
    .
    .
    error_code_handler              <span>7</span>       <span>; create handler for interrupt 7</span>
    .
    .
    .</code></pre>
<p>The <code>common_interrupt_handler</code> does the following:</p>
<ul>
<li>Push the registers on the stack.</li>
<li>Call the C function <code>interrupt_handler</code>.</li>
<li>Pop the registers from the stack.</li>
<li>Add 8 to <code>esp</code> (because of the error code and the interrupt number pushed earlier).</li>
<li>Execute <code>iret</code> to return to the interrupted code.</li>
</ul>
<p>Since the macros declare global labels the addresses of the interrupt handlers can be accessed from C or assembly code when creating the IDT.</p>
<h2 id="loading-the-idt"> Loading the IDT</h2>
<p>The IDT is loaded with the <code>lidt</code> assembly code instruction which takes the address of the first element in the table. It is easiest to wrap this instruction and use it from C:</p>
<pre><code>    <span>global</span>  load_idt

    <span>; load_idt - Loads the interrupt descriptor table (IDT).</span>
    <span>; stack: [esp + 4] the address of the first entry in the IDT</span>
    <span>;        [esp    ] the return address</span>
<span>    load_idt:</span>
        <span>mov</span>     <span>eax</span>, [<span>esp</span><span>+4</span>]    <span>; load the address of the IDT into register eax</span>
        <span>lidt</span>    <span>eax</span>             <span>; load the IDT</span>
        <span>ret</span>                     <span>; return to the calling function</span></code></pre>
<h2 id="programmable-interrupt-controller-pic"> Programmable Interrupt Controller (PIC)</h2>
<p>To start using hardware interrupts you must first configure the Programmable Interrupt Controller (PIC). The PIC makes it possible to map signals from the hardware to interrupts. The reasons for configuring the PIC are:</p>
<ul>
<li>Remap the interrupts. The PIC uses interrupts 0 - 15 for hardware interrupts by default, which conflicts with the CPU interrupts. Therefore the PIC interrupts must be remapped to another interval.</li>
<li>Select which interrupts to receive. You probably don’t want to receive interrupts from all devices since you don’t have code that handles these interrupts anyway.</li>
<li>Set up the correct mode for the PIC.</li>
</ul>
<p>In the beginning there was only one PIC (PIC 1) and eight interrupts. As more hardware were added, 8 interrupts were too few. The solution chosen was to chain on another PIC (PIC 2) on the first PIC (see interrupt 2 on PIC 1).</p>
<p>The hardware interrupts are shown in the table below:</p>
<table>
<thead>
<tr>
<th>PIC 1</th>
<th>Hardware</th>
<th>PIC 2</th>
<th>Hardware</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Timer</td>
<td>8</td>
<td>Real Time Clock</td>
</tr>
<tr>
<td>1</td>
<td>Keyboard</td>
<td>9</td>
<td>General I/O</td>
</tr>
<tr>
<td>2</td>
<td>PIC 2</td>
<td>10</td>
<td>General I/O</td>
</tr>
<tr>
<td>3</td>
<td>COM 2</td>
<td>11</td>
<td>General I/O</td>
</tr>
<tr>
<td>4</td>
<td>COM 1</td>
<td>12</td>
<td>General I/O</td>
</tr>
<tr>
<td>5</td>
<td>LPT 2</td>
<td>13</td>
<td>Coprocessor</td>
</tr>
<tr>
<td>6</td>
<td>Floppy disk</td>
<td>14</td>
<td>IDE Bus</td>
</tr>
<tr>
<td>7</td>
<td>LPT 1</td>
<td>15</td>
<td>IDE Bus</td>
</tr>
</tbody>
</table>
<p>A great tutorial for configuring the PIC can be found at the SigOPS website <span>[35]</span>. We won’t repeat that information here.</p>
<p>Every interrupt from the PIC has to be acknowledged - that is, sending a message to the PIC confirming that the interrupt has been handled. If this isn’t done the PIC won’t generate any more interrupts.</p>
<p>Acknowledging a PIC interrupt is done by sending the byte <code>0x20</code> to the PIC that raised the interrupt. Implementing a <code>pic_acknowledge</code> function can thus be done as follows:</p>
<pre><code>    <span>#include "io.h"</span>

    <span>#define PIC1_PORT_A 0x20</span>
    <span>#define PIC2_PORT_A 0xA0</span>

    <span>/* The PIC interrupts have been remapped */</span>
    <span>#define PIC1_START_INTERRUPT 0x20</span>
    <span>#define PIC2_START_INTERRUPT 0x28</span>
    <span>#define PIC2_END_INTERRUPT   PIC2_START_INTERRUPT + 7</span>

    <span>#define PIC_ACK     0x20</span>

    <span>/** pic_acknowledge:</span>
<span>     *  Acknowledges an interrupt from either PIC 1 or PIC 2.</span>
<span>     *</span>
<span>     *  </span><span>@param</span><span> </span><span>num</span><span> The number of the interrupt</span>
<span>     */</span>
    <span>void</span> pic_acknowledge(<span>unsigned</span> integer interrupt)
    {
        <span>if</span> (interrupt &lt; PIC1_START_INTERRUPT || interrupt &gt; PIC2_END_INTERRUPT) {
          <span>return</span>;
        }

        <span>if</span> (interrupt &lt; PIC2_START_INTERRUPT) {
          outb(PIC1_PORT_A, PIC_ACK);
        } <span>else</span> {
          outb(PIC2_PORT_A, PIC_ACK);
        }
    }</code></pre>
<h2 id="reading-input-from-the-keyboard"> Reading Input from the Keyboard</h2>
<p>The keyboard does not generate ASCII characters, it generates scan codes. A scan code represents a button - both presses and releases. The scan code representing the just pressed button can be read from the keyboard’s data I/O port which has address <code>0x60</code>. How this can be done is shown in the following example:</p>
<pre><code>    <span>#include "io.h"</span>

    <span>#define KBD_DATA_PORT   0x60</span>

    <span>/** read_scan_code:</span>
<span>     *  Reads a scan code from the keyboard</span>
<span>     *</span>
<span>     *  </span><span>@return</span><span> The scan code (NOT an ASCII character!)</span>
<span>     */</span>
    <span>unsigned</span> <span>char</span> read_scan_code(<span>void</span>)
    {
        <span>return</span> inb(KBD_DATA_PORT);
    }</code></pre>
<p>The next step is to write a function that translates a scan code to the corresponding ASCII character. If you want to map the scan codes to ASCII characters as is done on an American keyboard then Andries Brouwer has a great tutorial <span>[36]</span>.</p>
<p>Remember, since the keyboard interrupt is raised by the PIC, you must call <code>pic_acknowledge</code> at the end of the keyboard interrupt handler. Also, the keyboard will not send you any more interrupts until you read the scan code from the keyboard.</p>
<h2 id="further-reading-4"> Further Reading</h2>
<ul>
<li>The OSDev wiki has a great page on interrupts, <a href="http://wiki.osdev.org/Interrupts">http://wiki.osdev.org/Interrupts</a></li>
<li>Chapter 6 of Intel Manual 3a <span>[33]</span> describes everything there is to know about interrupts.</li>
</ul>
<h2 id="the-road-to-user-mode"> The Road to User Mode</h2>
<p>Now that the kernel boots, prints to screen and reads from keyboard - what do we do? Usually, a kernel is not supposed to do the application logic itself, but leave that for applications. The kernel creates the proper abstractions (for memory, files, devices) to make application development easier, performs tasks on behalf of applications (system calls) and <a href="#scheduling">schedules processes</a>.</p>
<p>User mode, in contrast with kernel mode, is the environment in which the user’s programs execute. This environment is less privileged than the kernel, and will prevent (badly written) user programs from messing with other programs or the kernel. Badly written kernels are free to mess up what they want.</p>
<p>There’s quite a way to go until the OS created in this book can execute programs in user mode, but this chapter will show how to easily execute a small program in kernel mode.</p>
<h2 id="loading-an-external-program"> Loading an External Program</h2>
<p>Where do we get the external program from? Somehow we need to load the code we want to execute into memory. More feature-complete operating systems usually have drivers and file systems that enable them to load the software from a CD-ROM drive, a hard disk or other persistent media.</p>
<p>Instead of creating all these drivers and file systems we will use a feature in GRUB called modules to load the program.</p>
<h3 id="grub-modules"> GRUB Modules</h3>
<p>GRUB can load arbitrary files into memory from the ISO image, and these files are usually referred to as <em>modules</em>. To make GRUB load a module, edit the file <code>iso/boot/grub/menu.lst</code> and add the following line at the end of the file:</p>
<pre><code>    module /modules/program</code></pre>
<p>Now create the folder <code>iso/modules</code>:</p>
<pre><code>    mkdir -p iso/modules</code></pre>
<p>The application <code>program</code> will be created later in this chapter.</p>
<p>The code that calls <code>kmain</code> must be updated to pass information to <code>kmain</code> about where it can find the modules. We also want to tell GRUB that it should align all the modules on page boundaries when loading them (see the chapter <a href="#paging">“Paging”</a> for details about page alignment).</p>
<p>To instruct GRUB how to load our modules, the “multiboot header” - the first bytes of the kernel - must be updated as follows:</p>
<pre><code>    <span>; in file `loader.s`</span>


    MAGIC_NUMBER    <span>equ</span> <span>0x1BADB002</span>      <span>; define the magic number constant</span>
    ALIGN_MODULES   <span>equ</span><span> 0x00000001      </span><span>; tell GRUB to align modules</span>

    <span>; calculate the checksum (all options + checksum should equal 0)</span>
    CHECKSUM        <span>equ</span> -(MAGIC_NUMBER + ALIGN_MODULES)

    <span>section</span> .text:                      <span>; start of the text (code) section</span>
    <span>align</span> <span>4</span>                             <span>; the code must be 4 byte aligned</span>
        <span>dd</span> MAGIC_NUMBER                 <span>; write the magic number</span>
        <span>dd</span> ALIGN_MODULES                <span>; write the align modules instruction</span>
        <span>dd</span> CHECKSUM                     <span>; write the checksum</span></code></pre>
<p>GRUB will also store a pointer to a <code>struct</code> in the register <code>ebx</code> that, among other things, describes at which addresses the modules are loaded. Therefore, you probably want to push <code>ebx</code> on the stack before calling <code>kmain</code> to make it an argument for <code>kmain</code>.</p>
<h2 id="executing-a-program"> Executing a Program</h2>
<h3 id="a-very-simple-program"> A Very Simple Program</h3>
<p>A program written at this stage can only perform a few actions. Therefore, a very short program that writes a value to a register suffices as a test program. Halting Bochs after a while and then check that register contains the correct number by looking in the Bochs log will verify that the program has run. This is an example of such a short program:</p>
<pre><code>    <span>; set eax to some distinguishable number, to read from the log afterwards</span>
    <span>mov</span> <span>eax</span>, <span>0xDEADBEEF</span>

    <span>; enter infinite loop, nothing more to do</span>
    <span>; $ means "beginning of line", ie. the same instruction</span>
    <span>jmp</span> <span>$</span></code></pre>
<h3 id="compiling"> Compiling</h3>
<p>Since our kernel cannot parse advanced executable formats we need to compile the code into a flat binary. NASM can do this with the flag <code>-f</code>:</p>
<pre><code>    nasm -f bin program.s -o program</code></pre>
<p>This is all we need. You must now move the file <code>program</code> to the folder <code>iso/modules</code>.</p>
<h3 id="finding-the-program-in-memory"> Finding the Program in Memory</h3>
<p>Before jumping to the program we must find where it resides in memory. Assuming that the contents of <code>ebx</code> is passed as an argument to <code>kmain</code>, we can do this entirely from C.</p>
<p>The pointer in <code>ebx</code> points to a <em>multiboot</em> structure <span>[19]</span>. Download the <code>multiboot.h</code> file from <a href="http://www.gnu.org/software/grub/manual/multiboot/html_node/multiboot.h.html">http://www.gnu.org/software/grub/manual/multiboot/html_node/multiboot.h.html</a>, which describes the structure.</p>
<p>The pointer passed to <code>kmain</code> in the <code>ebx</code> register can be cast to a <code>multiboot_info_t</code> pointer. The address of the first module is in the field <code>mods_addr</code>. The following code shows an example:</p>
<pre><code>    <span>int</span> kmain(<span>/* additional arguments */</span> <span>unsigned</span> <span>int</span> ebx)
    {
        multiboot_info_t *mbinfo = (multiboot_info_t *) ebx;
        <span>unsigned</span> <span>int</span> address_of_module = mbinfo-&gt;mods_addr;
    }</code></pre>
<p>However, before just blindly following the pointer, you should check that the module got loaded correctly by GRUB. This can be done by checking the <code>flags</code> field of the <code>multiboot_info_t</code> structure. You should also check the field <code>mods_count</code> to make sure it is exactly 1. For more details about the multiboot structure, see the multiboot documentation <span>[19]</span>.</p>
<h3 id="jumping-to-the-code"> Jumping to the Code</h3>
<p>The only thing left to do is to jump to the code loaded by GRUB. Since it is easier to parse the multiboot structure in C than assembly code, calling the code from C is more convenient (it can of course be done with <code>jmp</code> or <code>call</code> in assembly code as well). The C code could look like this:</p>
<pre><code>    <span>typedef</span> <span>void</span> (*call_module_t)(<span>void</span>);
    <span>/* ... */</span>
    call_module_t start_program = (call_module_t) address_of_module;
    start_program();
    <span>/* we'll never get here, unless the module code returns */</span></code></pre>
<p>If we start the kernel, wait until it has run and entered the infinite loop in the program, and then halt Bochs, we should see <code>0xDEADBEEF</code> in the register <code>eax</code> via the Bochs log. We have successfully started a program in our OS!</p>
<h2 id="the-beginning-of-user-mode"> The Beginning of User Mode</h2>
<p>The program we’ve written now runs at the same privilege level as the kernel - we’ve just entered it in a somewhat peculiar way. To enable applications to execute at a different privilege level we’ll need to, beside <a href="#segmentation"><em>segmentation</em></a>, do <a href="#paging"><em>paging</em></a> and <a href="#page-frame-allocation"><em>page frame allocation</em></a>.</p>
<p>It’s quite a lot of work and technical details to go through, but in a few chapters you’ll have working user mode programs.</p>
<h2 id="a-short-introduction-to-virtual-memory"> A Short Introduction to Virtual Memory</h2>
<p><em>Virtual memory</em> is an abstraction of physical memory. The purpose of virtual memory is generally to simplify application development and to let processes address more memory than what is actually physically present in the machine. We also don’t want applications messing with the kernel or other applications’ memory due to security.</p>
<p>In the x86 architecture, virtual memory can be accomplished in two ways: <em>segmentation</em> and <em>paging</em>. Paging is by far the most common and versatile technique, and we’ll implement it the next chapter. Some use of segmentation is still necessary to allow for code to execute under different privilege levels.</p>
<p>Managing memory is a big part of what an operating system does. <a href="#paging">Paging</a> and <a href="#page-frame-allocation">page frame allocation</a> deals with that.</p>
<p>Segmentation and paging is described in the <span>[33]</span>, chapter 3 and 4.</p>
<h2 id="virtual-memory-through-segmentation"> Virtual Memory Through Segmentation?</h2>
<p>You could skip paging entirely and just use segmentation for virtual memory. Each user mode process would get its own segment, with base address and limit properly set up. This way no process can see the memory of another process. A problem with this is that the physical memory for a process needs to be contiguous (or at least it is very convenient if it is). Either we need to know in advance how much memory the program will require (unlikely), or we can move the memory segments to places where they can grow when the limit is reached (expensive, causes fragmentation - can result in “out of memory” even though enough memory is available). Paging solves both these problems.</p>
<p>It is interesting to note that in x86_64 (the 64-bit version of the x86 architecture), segmentation is almost completely removed.</p>
<h2 id="further-reading-5"> Further Reading</h2>
<ul>
<li>LWN.net has an article on virtual memory: <a href="http://lwn.net/Articles/253361/">http://lwn.net/Articles/253361/</a></li>
<li>Gustavo Duarte has also written an article about virtual memory: <a href="http://duartes.org/gustavo/blog/post/memory-translation-and-segmentation">http://duartes.org/gustavo/blog/post/memory-translation-and-segmentation</a></li>
</ul>
<h2 id="paging"> Paging</h2>
<p>Segmentation translates a logical address into a linear address. <em>Paging</em> translates these linear addresses onto the physical address space, and determines access rights and how the memory should be cached.</p>
<h2 id="why-paging"> Why Paging?</h2>
<p>Paging is the most common technique used in x86 to enable virtual memory. Virtual memory through paging means that each process will get the impression that the available memory range is <code>0x00000000</code> - <code>0xFFFFFFFF</code> even though the actual size of the memory might be much less. It also means that when a process addresses a byte of memory it will use a virtual (linear) address instead of physical one. The code in the user process won’t notice any difference (except for execution delays). The linear address gets translated to a physical address by the MMU and the page table. If the virtual address isn’t mapped to a physical address, the CPU will raise a page fault interrupt.</p>
<p>Paging is optional, and some operating systems do not make use of it. But if we want to mark certain areas of memory accessible only to code running at a certain privilege level (to be able to have processes running at different privilege levels), paging is the neatest way to do it.</p>
<h2 id="paging-in-x86"> Paging in x86</h2>
<p>Paging in x86 (chapter 4 in the Intel manual <span>[33]</span>) consists of a <em>page directory</em> (PDT) that can contain references to 1024 <em>page tables</em> (PT), each of which can point to 1024 sections of physical memory called <em>page frames</em> (PF). Each page frame is 4096 byte large. In a virtual (linear) address, the highest 10 bits specifies the offset of a page directory entry (PDE) in the current PDT, the next 10 bits the offset of a page table entry (PTE) within the page table pointed to by that PDE. The lowest 12 bits in the address is the offset within the page frame to be addressed.</p>
<p>All page directories, page tables and page frames need to be aligned on 4096 byte addresses. This makes it possible to address a PDT, PT or PF with just the highest 20 bits of a 32 bit address, since the lowest 12 need to be zero.</p>
<p>The PDE and PTE structure is very similar to each other: 32 bits (4 bytes), where the highest 20 bits points to a PTE or PF, and the lowest 12 bits control access rights and other configurations. 4 bytes times 1024 equals 4096 bytes, so a page directory and page table both fit in a page frame themselves.</p>
<p>The translation of linear addresses to physical addresses is described in the figure below.</p>
<p>While pages are normally 4096 bytes, it is also possible to use 4 MB pages. A PDE then points directly to a 4 MB page frame, which needs to be aligned on a 4 MB address boundary. The address translation is almost the same as in the figure, with just the page table step removed. It is possible to mix 4 MB and 4 KB pages.</p>
<div>
<p><img src="https://littleosbook.github.io/images/intel_4_2_linear_address_translation.png" alt="Translating virtual addresses (linear addresses) to physical addresses."></p><p>Translating virtual addresses (linear addresses) to physical addresses.</p>
</div>
<p>The 20 bits pointing to the current PDT is stored in the register <code>cr3</code>. The lower 12 bits of <code>cr3</code> are used for configuration.</p>
<p>For more details on the paging structures, see chapter 4 in the Intel manual <span>[33]</span>. The most interesting bits are <em>U/S</em>, which determine what privilege levels can access this page (PL0 or PL3), and <em>R/W</em>, which makes the memory in the page read-write or read-only.</p>
<h3 id="identity-paging"> Identity Paging</h3>
<p>The simplest kind of paging is when we map each virtual address onto the same physical address, called <em>identity paging</em>. This can be done at compile time by creating a page directory where each entry points to its corresponding 4 MB frame. In NASM this can be done with macros and commands (<code>%rep</code>, <code>times</code> and <code>dd</code>). It can of course also be done at run-time by using ordinary assembly code instructions.</p>
<h3 id="enabling-paging"> Enabling Paging</h3>
<p>Paging is enabled by first writing the address of a page directory to <code>cr3</code> and then setting bit 31 (the PG “paging-enable” bit) of <code>cr0</code> to <code>1</code>. To use 4 MB pages, set the PSE bit (Page Size Extensions, bit 4) of <code>cr4</code>. The following assembly code shows an example:</p>
<pre><code>    <span>; eax has the address of the page directory</span>
    <span>mov</span> <span>cr3</span>, <span>eax</span>

    <span>mov</span> <span>ebx</span>, <span>cr4</span>        <span>; read current cr4</span>
    <span>or</span>  <span>ebx</span><span>, 0x00000010 </span><span>; set PSE</span>
    <span>mov</span> <span>cr4</span>, <span>ebx</span>        <span>; update cr4</span>

    <span>mov</span> <span>ebx</span>, <span>cr0</span>        <span>; read current cr0</span>
    <span>or</span>  <span>ebx</span><span>, 0x80000000 </span><span>; set PG</span>
    <span>mov</span> <span>cr0</span>, <span>ebx</span>        <span>; update cr0</span>

    <span>; now paging is enabled</span></code></pre>
<h3 id="a-few-details"> A Few Details</h3>
<p>It is important to note that all addresses within the page directory, page tables and in <code>cr3</code> need to be physical addresses to the structures, never virtual. This will be more relevant in later sections where we dynamically update the paging structures (see the chapter <a href="#user-mode">“User Mode”</a>).</p>
<p>An instruction that is useful when an updating a PDT or PT is <code>invlpg</code>. It invalidates the <em>Translation Lookaside Buffer</em> (TLB) entry for a virtual address. The TLB is a cache for translated addresses, mapping physical addresses corresponding to virtual addresses. This is only required when changing a PDE or PTE that was previously mapped to something else. If the PDE or PTE had previously been marked as not present (bit 0 was set to 0), executing <code>invlpg</code> is unnecessary. Changing the value of <code>cr3</code> will cause all entries in the TLB to be invalidated.</p>
<p>An example of invalidating a TLB entry is shown below:</p>
<pre><code>    <span>; invalidate any TLB references to virtual address 0</span>
    <span>invlpg</span> [<span>0</span>]</code></pre>
<h2 id="paging-and-the-kernel"> Paging and the Kernel</h2>
<p>This section will describe how paging affects the OS kernel. We encourage you to run your OS using identity paging before trying to implement a more advanced paging setup, since it can be hard to debug a malfunctioning page table that is set up via assembly code.</p>
<h3 id="reasons-to-not-identity-map-the-kernel"> Reasons to Not Identity Map the Kernel</h3>
<p>If the kernel is placed at the beginning of the virtual address space - that is, the virtual address space (<code>0x00000000</code>, <code>"size of kernel"</code>) maps to the location of the kernel in memory - there will be issues when linking the user mode process code. Normally, during linking, the linker assumes that the code will be loaded into the memory position <code>0x00000000</code>. Therefore, when resolving absolute references, <code>0x00000000</code> will be the base address for calculating the exact position. But if the kernel is mapped onto the virtual address space (<code>0x00000000</code>, <code>"size of kernel"</code>), the user mode process cannot be loaded at virtual address <code>0x00000000</code> - it must be placed somewhere else. Therefore, the assumption from the linker that the user mode process is loaded into memory at position <code>0x00000000</code> is wrong. This can be corrected by using a linker script which tells the linker to assume a different starting address, but that is a very cumbersome solution for the users of the operating system.</p>
<p>This also assumes that we want the kernel to be part of the user mode process’ address space. As we will see later, this is a nice feature, since during system calls we don’t have to change any paging structures to get access to the kernel’s code and data. The kernel pages will of course require privilege level 0 for access, to prevent a user process from reading or writing kernel memory.</p>
<h3 id="the-virtual-address-for-the-kernel"> The Virtual Address for the Kernel</h3>
<p>Preferably, the kernel should be placed at a very high virtual memory address, for example <code>0xC0000000</code> (3 GB). The user mode process is not likely to be 3 GB large, which is now the only way that it can conflict with the kernel. When the kernel uses virtual addresses at 3 GB and above it is called a <em>higher-half kernel</em>. <code>0xC0000000</code> is just an example, the kernel can be placed at any address higher than 0 to get the same benefits. Choosing the correct address depends on how much virtual memory should be available for the kernel (it is easiest if all memory above the kernel virtual address should belong to the kernel) and how much virtual memory should be available for the process.</p>
<p>If the user mode process is larger than 3 GB, some pages will need to be swapped out by the kernel. Swapping pages is not part of this book.</p>
<h3 id="placing-the-kernel-at-0xc0000000"> Placing the Kernel at <code>0xC0000000</code></h3>
<p>To start with, it is better to place the kernel at <code>0xC0100000</code> than <code>0xC0000000</code>, since this makes it possible to map (<code>0x00000000</code>, <code>0x00100000</code>) to (<code>0xC0000000</code>, <code>0xC0100000</code>). This way, the entire range (<code>0x00000000</code>, <code>"size of kernel"</code>) of memory is mapped to the range (<code>0xC0000000</code>, <code>0xC0000000  + "size of kernel"</code>).</p>
<p>Placing the kernel at <code>0xC0100000</code> isn’t hard, but it does require some thought. This is once again a linking problem. When the linker resolves all absolute references in the kernel, it will assume that our kernel is loaded at physical memory location <code>0x00100000</code>, not <code>0x00000000</code>, since relocation is used in the linker script (see the section <a href="#linking-the-kernel">“Linking the kernel”</a>). However, we want the jumps to be resolved using <code>0xC0100000</code> as base address, since otherwise a kernel jump will jump straight into the user mode process code (remember that the user mode process is loaded at virtual memory <code>0x00000000</code>).</p>
<p>However, we can’t simply tell the linker to assume that the kernel starts (is loaded) at <code>0xC01000000</code>, since we want it to be loaded at the physical address <code>0x00100000</code>. The reason for having the kernel loaded at 1 MB is because it can’t be loaded at <code>0x00000000</code>, since there is BIOS and GRUB code loaded below 1 MB. Furthermore, we cannot assume that we can load the kernel at <code>0xC0100000</code>, since the machine might not have 3 GB of physical memory.</p>
<p>This can be solved by using both relocation (<code>.=0xC0100000</code>) and the <code>AT</code> instruction in the linker script. Relocation specifies that non-relative memory-references should should use the relocation address as base in address calculations. <code>AT</code> specifies where the kernel should be loaded into memory. Relocation is done at link time by GNU ld <span>[37]</span>, the load address specified by <code>AT</code> is handled by GRUB when loading the kernel, and is part of the ELF format <span>[18]</span>.</p>
<h3 id="higher-half-linker-script"> Higher-half Linker Script</h3>
<p>We can modify the <a href="#linking-the-kernel">first linker script</a> to implement this:</p>
<pre><code>    ENTRY(loader)           /* the name of the entry symbol */

    . = 0xC0100000          /* the code should be relocated to 3GB + 1MB */

    /* align at 4 KB and load at 1 MB */
    .text ALIGN (0x1000) : AT(ADDR(.text)-0xC0000000)
    {
        *(.text)            /* all text sections from all files */
    }

    /* align at 4 KB and load at 1 MB + . */
    .rodata ALIGN (0x1000) : AT(ADDR(.text)-0xC0000000)
    {
        *(.rodata*)         /* all read-only data sections from all files */
    }

    /* align at 4 KB and load at 1 MB + . */
    .data ALIGN (0x1000) : AT(ADDR(.text)-0xC0000000)
    {
        *(.data)            /* all data sections from all files */
    }

    /* align at 4 KB and load at 1 MB + . */
    .bss ALIGN (0x1000) : AT(ADDR(.text)-0xC0000000)
    {
        *(COMMON)           /* all COMMON sections from all files */
        *(.bss)             /* all bss sections from all files */
    }</code></pre>
<h3 id="entering-the-higher-half"> Entering the Higher Half</h3>
<p>When GRUB jumps to the kernel code, there is no paging table. Therefore, all references to <code>0xC0100000 + X</code> won’t be mapped to the correct physical address, and will therefore cause a general protection exception (GPE) at the very best, otherwise (if the computer has more than 3 GB of memory) the computer will just crash.</p>
<p>Therefore, assembly code that doesn’t use relative jumps or relative memory addressing must be used to do the following:</p>
<ul>
<li>Set up a page table.</li>
<li>Add identity mapping for the first 4 MB of the virtual address space.</li>
<li>Add an entry for <code>0xC0100000</code> that maps to <code>0x0010000</code></li>
</ul>
<p>If we skip the identity mapping for the first 4 MB, the CPU would generate a page fault immediately after paging was enabled when trying to fetch the next instruction from memory. After the table has been created, an jump can be done to a label to make <code>eip</code> point to a virtual address in the higher half:</p>
<pre><code>    <span>; assembly code executing at around 0x00100000</span>
    <span>; enable paging for both actual location of kernel</span>
    <span>; and its higher-half virtual location</span>

    <span>lea</span> <span>ebx</span>, [higher_half] <span>; load the address of the label in ebx</span>
    <span>jmp</span> <span>ebx</span>                <span>; jump to the label</span>

<span>    higher_half:</span>
        <span>; code here executes in the higher half kernel</span>
        <span>; eip is larger than 0xC0000000</span>
        <span>; can continue kernel initialisation, calling C code, etc.</span></code></pre>
<p>The register <code>eip</code> will now point to a memory location somewhere right after <code>0xC0100000</code> - all the code can now execute as if it were located at <code>0xC0100000</code>, the higher-half. The entry mapping of the first 4 MB of virtual memory to the first 4 MB of physical memory can now be removed from the page table and its corresponding entry in the TLB invalidated with <code>invlpg [0]</code>.</p>
<h3 id="running-in-the-higher-half"> Running in the Higher Half</h3>
<p>There are a few more details we must deal with when using a higher-half kernel. We must be careful when using memory-mapped I/O that uses specific memory locations. For example, the frame buffer is located at <code>0x000B8000</code>, but since there is no entry in the page table for the address <code>0x000B8000</code> any longer, the address <code>0xC00B8000</code> must be used, since the virtual address <code>0xC0000000</code> maps to the physical address <code>0x00000000</code>.</p>
<p>Any explicit references to addresses within the multiboot structure needs to be changed to reflect the new virtual addresses as well.</p>
<p>Mapping 4 MB pages for the kernel is simple, but wastes memory (unless you have a really big kernel). Creating a higher-half kernel mapped in as 4 KB pages saves memory but is harder to set up. Memory for the page directory and one page table can be reserved in the <code>.data</code> section, but one needs to configure the mappings from virtual to physical addresses at run-time. The size of the kernel can be determined by exporting labels from the linker script <span>[37]</span>, which we’ll need to do later anyway when writing the page frame allocator (see the chapter <a href="#page-frame-allocation">“Page Frame Allocation</a>).</p>
<h2 id="virtual-memory-through-paging"> Virtual Memory Through Paging</h2>
<p>Paging enables two things that are good for virtual memory. First, it allows for fine-grained access control to memory. You can mark pages as read-only, read-write, only for PL0 etc. Second, it creates the illusion of contiguous memory. User mode processes, and the kernel, can access memory as if it were contiguous, and the contiguous memory can be extended without the need to move data around in memory. We can also allow the user mode programs access to all memory below 3 GB, but unless they actually use it, we don’t have to assign page frames to the pages. This allows processes to have code located near <code>0x00000000</code> and the stack at just below <code>0xC0000000</code>, and still not require more than two actual pages.</p>
<h2 id="further-reading-6"> Further Reading</h2>
<ul>
<li>Chapter 4 (and to some extent chapter 3) of the Intel manual <span>[33]</span> are your definitive sources for the details about paging.</li>
<li>Wikipedia has an article on paging: <a href="http://en.wikipedia.org/wiki/Paging">http://en.wikipedia.org/wiki/Paging</a></li>
<li>The OSDev wiki has a page on paging: <a href="http://wiki.osdev.org/Paging">http://wiki.osdev.org/Paging</a> and a tutorial for making a higher-half kernel: <a href="http://wiki.osdev.org/Higher_Half_bare_bones">http://wiki.osdev.org/Higher_Half_bare_bones</a></li>
<li>Gustavo Duarte’s article on how a kernel manages memory is well worth a read: <a href="http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory">http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory</a></li>
<li>Details on the linker command language can be found at Steve Chamberlain’s website <span>[37]</span>.</li>
<li>More details on the ELF format can be found in this presentation: <a href="http://flint.cs.yale.edu/cs422/doc/ELF_Format.pdf">http://flint.cs.yale.edu/cs422/doc/ELF_Format.pdf</a></li>
</ul>
<h2 id="page-frame-allocation"> Page Frame Allocation</h2>
<p>When using virtual memory, how does the OS know which parts of memory are free to use? That is the role of the page frame allocator.</p>
<h2 id="managing-available-memory"> Managing Available Memory</h2>
<h3 id="how-much-memory-is-there"> How Much Memory is There?</h3>
<p>First we need to know how much memory is available on the computer the OS is running on. The easiest way to do this is to read it from the multiboot structure <span>[19]</span> passed to us by GRUB. GRUB collects the information we need about the memory - what is reserved, I/O mapped, read-only etc. We must also make sure that we don’t mark the part of memory used by the kernel as free (since GRUB doesn’t mark this memory as reserved). One way to know how much memory the kernel uses is to export labels at the beginning and the end of the kernel binary from the linker script:</p>
<pre><code>    ENTRY(loader)           /* the name of the entry symbol */

    . = 0xC0100000          /* the code should be relocated to 3 GB + 1 MB */

    /* these labels get exported to the code files */
    kernel_virtual_start = .;
    kernel_physical_start = . - 0xC0000000;

    /* align at 4 KB and load at 1 MB */
    .text ALIGN (0x1000) : AT(ADDR(.text)-0xC0000000)
    {
        *(.text)            /* all text sections from all files */
    }

    /* align at 4 KB and load at 1 MB + . */
    .rodata ALIGN (0x1000) : AT(ADDR(.rodata)-0xC0000000)
    {
        *(.rodata*)         /* all read-only data sections from all files */
    }

    /* align at 4 KB and load at 1 MB + . */
    .data ALIGN (0x1000) : AT(ADDR(.data)-0xC0000000)
    {
        *(.data)            /* all data sections from all files */
    }

    /* align at 4 KB and load at 1 MB + . */
    .bss ALIGN (0x1000) : AT(ADDR(.bss)-0xC0000000)
    {
        *(COMMON)           /* all COMMON sections from all files */
        *(.bss)             /* all bss sections from all files */
    }

    kernel_virtual_end = .;
    kernel_physical_end = . - 0xC0000000;</code></pre>
<p>These labels can directly be read from assembly code and pushed on the stack to make them available to C code:</p>
<pre><code>    <span>extern</span> kernel_virtual_start
    <span>extern</span> kernel_virtual_end
    <span>extern</span> kernel_physical_start
    <span>extern</span> kernel_physical_end

    <span>; ...</span>

    <span>push</span> kernel_physical_end
    <span>push</span> kernel_physical_start
    <span>push</span> kernel_virtual_end
    <span>push</span> kernel_virtual_start

    <span>call</span> kmain</code></pre>
<p>This way we get the labels as arguments to <code>kmain</code>. If you want to use C instead of assembly code, one way to do it is to declare the labels as functions and take the addresses of these functions:</p>
<pre><code>    <span>void</span> kernel_virtual_start(<span>void</span>);

    <span>/* ... */</span>

    <span>unsigned</span> <span>int</span> vaddr = (<span>unsigned</span> <span>int</span>) &amp;kernel_virtual_start;</code></pre>
<p>If you use GRUB modules you need to make sure the memory they use is marked as reserved as well.</p>
<p>Note that the available memory does not need to be contiguous. In the first 1 MB there are several I/O-mapped memory sections, as well as memory used by GRUB and the BIOS. Other parts of the memory might be similarly unavailable.</p>
<p>It’s convenient to divide the memory sections into complete page frames, as we can’t map part of pages into memory.</p>
<h3 id="managing-available-memory-1"> Managing Available Memory</h3>
<p>How do we know which page frames are in use? The page frame allocator needs to keep track of which are free and which aren’t. There are several ways to do this: bitmaps, linked lists, trees, the Buddy System (used by Linux) etc. For more information about the different algorithms see the article on OSDev <span>[38]</span>.</p>
<p>Bitmaps are quite easy to implement. One bit is used for each page frame and one (or more) page frames are dedicated to store the bitmap. (Note that this is just one way to do it, other designs might be better and/or more fun to implement.)</p>
<h2 id="how-can-we-access-a-page-frame"> How Can We Access a Page Frame?</h2>
<p>The page frame allocator returns the physical start address of the page frame. This page frame is not mapped in - no page table points to this page frame. How can we read and write data to the frame?</p>
<p>We need to map the page frame into virtual memory, by updating the PDT and/or PT used by the kernel. What if all available page tables are full? Then we can’t map the page frame into memory, because we’d need a new page table - which takes up an entire page frame - and to write to this page frame we’d need to map its page frame… Somehow this circular dependency must be broken.</p>
<p>One solution is to reserve a part of the first page table used by the kernel (or some other higher-half page table) for temporarily mapping page frames to make them accessible. If the kernel is mapped at <code>0xC0000000</code> (page directory entry with index 768), and 4 KB page frames are used, then the kernel has at least one page table. If we assume - or limit us to - a kernel of size at most 4 MB minus 4 KB we can dedicate the last entry (entry 1023) of this page table for temporary mappings. The virtual address of pages mapped in using the last entry of the kernel’s PT will be:</p>
<pre><code>    (768 &lt;&lt; 22) | (1023 &lt;&lt; 12) | 0x000 = 0xC03FF000</code></pre>
<p>After we’ve temporarily mapped the page frame we want to use as a page table, and set it up to map in our first page frame, we can add it to the paging directory, and remove the temporary mapping.</p>
<h2 id="a-kernel-heap"> A Kernel Heap</h2>
<p>So far we’ve only been able to work with fixed-size data, or directly with raw memory. Now that we have a page frame allocator we can implement <code>malloc</code> and <code>free</code> to use in the kernel.</p>
<p>Kernighan and Ritchie <span>[8]</span> have an example implementation in their book <span>[8]</span> that we can draw inspiration from. The only modification we need to do is to replace calls to <code>sbrk</code>/<code>brk</code> with calls to the page frame allocator when more memory is needed. We must also make sure to map the page frames returned by the page frame allocator to virtual addresses. A correct implementation should also return page frames to the page frame allocator on call to <code>free</code>, whenever sufficiently large blocks of memory are freed.</p>
<h2 id="further-reading-7"> Further reading</h2>
<ul>
<li>The OSDev wiki page on page frame allocation: <a href="http://wiki.osdev.org/Page_Frame_Allocation">http://wiki.osdev.org/Page_Frame_Allocation</a></li>
</ul>
<h2 id="user-mode"> User Mode</h2>
<p>User mode is now almost within our reach, there are just a few more steps required to get there. Although these steps might seem easy they way they are presented in this chapter, they can be tricky to implement, since there are a lot of places where small errors will cause bugs that are hard to find.</p>
<h2 id="segments-for-user-mode"> Segments for User Mode</h2>
<p>To enable user mode we need to add two more segments to the GDT. They are very similar to the kernel segments we added when we <a href="#the-global-descriptor-table-gdt">set up the GDT</a> in the <a href="#segmentation">chapter about segmentation</a>:</p>
<table>
<caption>The segment descriptors needed for user mode.</caption>
<thead>
<tr>
<th>Index</th>
<th>Offset</th>
<th>Name</th>
<th>Address range</th>
<th>Type</th>
<th>DPL</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td><code>0x18</code></td>
<td>user code segment</td>
<td><code>0x00000000 - 0xFFFFFFFF</code></td>
<td>RX</td>
<td>PL3</td>
</tr>
<tr>
<td>4</td>
<td><code>0x20</code></td>
<td>user data segment</td>
<td><code>0x00000000 - 0xFFFFFFFF</code></td>
<td>RW</td>
<td>PL3</td>
</tr>
</tbody>
</table>
<p>The difference is the DPL, which now allows code to execute in PL3. The segments can still be used to address the entire address space, just using these segments for user mode code will not protect the kernel. For that we need paging.</p>
<h2 id="setting-up-for-user-mode"> Setting Up For User Mode</h2>
<p>There are a few things every user mode process needs:</p>
<ul>
<li><p>Page frames for code, data and stack. At the moment it suffices to allocate one page frame for the stack and enough page frames to fit the program’s code. Don’t worry about setting up a stack that can be grow and shrink at this point in time, focus on getting a basic implementation work first.</p></li>
<li><p>The binary from the GRUB module has to be copied to the page frames used for the programs code.</p></li>
<li><p>A page directory and page tables are needed to map the page frames described above into memory. At least two page tables are needed, because the code and data should be mapped in at <code>0x00000000</code> and increasing, and the stack should start just below the kernel, at <code>0xBFFFFFFB</code>, growing towards lower addresses. The U/S flag has to be set to allow PL3 access.</p></li>
</ul>
<p>It might be convenient to store this information in a <code>struct</code> representing a process. This process <code>struct</code> can be dynamically allocated with the kernel’s <code>malloc</code> function.</p>
<h2 id="entering-user-mode"> Entering User Mode</h2>
<p>The only way to execute code with a lower privilege level than the current privilege level (CPL) is to execute an <code>iret</code> or <code>lret</code> instruction - interrupt return or long return, respectively.</p>
<p>To enter user mode we set up the stack as if the processor had raised an inter-privilege level interrupt. The stack should look like the following:</p>
<pre><code>    [esp + 16]  ss      ; the stack segment selector we want for user mode
    [esp + 12]  esp     ; the user mode stack pointer
    [esp +  8]  eflags  ; the control flags we want to use in user mode
    [esp +  4]  cs      ; the code segment selector
    [esp +  0]  eip     ; the instruction pointer of user mode code to execute</code></pre>
<p>See the Intel manual <span>[33]</span>, section 6.2.1, figure 6-4 for more information.</p>
<p>The instruction <code>iret</code> will then read these values from the stack and fill in the corresponding registers. Before we execute <code>iret</code> we need to change to the page directory we setup for the user mode process. It is important to remember that to continue executing kernel code after we’ve switched PDT, the kernel needs to be mapped in. One way to accomplish this is to have a separate PDT for the kernel, which maps all data at <code>0xC0000000</code> and above, and merge it with the user PDT (which only maps below <code>0xC0000000</code>) when performing the switch. Remember that physical address of the PDT has to be used when setting the register <code>cr3</code>.</p>
<p>The register <code>eflags</code> contains a set of different flags, specified in section 2.3 of the Intel manual <span>[33]</span>. Most important for us is the interrupt enable (IF) flag. The assembly code instruction <code>sti</code> can’t be used in privilege level 3 for enabling interrupts. If interrupts are disabled when entering user mode, then interrupts can’t enabled once user mode is entered. Setting the IF flag in the <code>eflags</code> entry on the stack will enable interrupts in user mode, since the assembly code instruction <code>iret</code> will set the register <code>eflags</code> to the corresponding value on the stack.</p>
<p>For now, we should have interrupts disabled, as it requires a little more work to get inter-privilege level interrupts to work properly (see the section <a href="#system-calls">“System calls”</a>).</p>
<p>The value <code>eip</code> on the stack should point to the entry point for the user code - <code>0x00000000</code> in our case. The value <code>esp</code> on the stack should be where the stack starts - <code>0xBFFFFFFB</code> (<code>0xC0000000 - 4</code>).</p>
<p>The values <code>cs</code> and <code>ss</code> on the stack should be the segment selectors for the user code and user data segments, respectively. As we saw in the <a href="#creating-and-loading-the-gdt">segmentation chapter</a>, the lowest two bits of a segment selector is the RPL - the Requested Privilege Level. When using <code>iret</code> to enter PL3, the RPL of <code>cs</code> and <code>ss</code> should be <code>0x3</code>. The following code shows an example:</p>
<pre><code>    USER_MODE_CODE_SEGMENT_SELECTOR <span>equ</span><span> 0x18</span>
    USER_MODE_DATA_SEGMENT_SELECTOR <span>equ</span><span> 0x20</span>
    <span>mov</span> <span>cs</span>, USER_MODE_CODE_SEGMENT_SELECTOR |<span> 0x3</span>
    <span>mov</span> <span>ss</span>, USER_MODE_DATA_SEGMENT_SELECTOR |<span> 0x3</span></code></pre>
<p>The register <code>ds</code>, and the other data segment registers, should be set to the same segment selector as <code>ss</code>. They can be set the ordinary way, with the <code>mov</code> assembly code instruction.</p>
<p>We are now ready to execute <code>iret</code>. If everything has been set up right, we should now have a kernel that can enter user mode.</p>
<h2 id="using-c-for-user-mode-programs"> Using C for User Mode Programs</h2>
<p>When C is used as the programming language for user mode programs, it is important to think about the structure of the file that will be the result of the compilation.</p>
<p>The reason we can use ELF <span>[18]</span> as the file format for for the kernel executable is because GRUB knows how to parse and interpret the ELF file format. If we implemented an ELF parser, we could compile the user mode programs into ELF binaries as well. We leave this as an exercise for the reader.</p>
<p>One thing we can do to make it easier to develop user mode programs is to allow the programs to be written in C, but compile them to flat binaries instead of ELF binaries. In C the layout of the generated code is more unpredictable and the entry point, <code>main</code>, might not be at offset 0 in the binary. One common way to work around this is to add a few assembly code lines placed at offset 0 which calls <code>main</code>:</p>
<pre><code>    <span>extern</span> main

    <span>section</span> .text
        <span>; push argv</span>
        <span>; push argc</span>
        <span>call</span> main
        <span>; main has returned, eax is return value</span>
        <span>jmp</span>  <span>$</span>    <span>; loop forever</span></code></pre>
<p>If this code is saved in a file called <code>start.s</code>, then the following code show an example of a linker script that places these instructions first in executable (remember that <code>start.s</code> gets compiled to <code>start.o</code>):</p>
<pre><code>    OUTPUT_FORMAT("binary")    /* output flat binary */

    SECTIONS
    {
        . = 0;                 /* relocate to address 0 */

        .text ALIGN(4):
        {
            start.o(.text)     /* include the .text section of start.o */
            *(.text)           /* include all other .text sections */
        }

        .data ALIGN(4):
        {
            *(.data)
        }

        .rodata ALIGN(4):
        {
            *(.rodata*)
        }
    }</code></pre>
<p><em>Note</em>: <code>*(.text)</code> will not include the <code>.text</code> section of <code>start.o</code> again.</p>
<p>With this script we can write programs in C or assembler (or any other language that compiles to object files linkable with <code>ld</code>), and it is easy to load and map for the kernel (<code>.rodata</code> will be mapped in as writeable, though).</p>
<p>When we compile user programs we want the following GCC flags:</p>
<pre><code>    -m32 -nostdlib -nostdinc -fno-builtin -fno-stack-protector -nostartfiles
    -nodefaultlibs</code></pre>
<p>For linking, the followings flags should be used:</p>
<pre><code>    -T link.ld -melf_i386  # emulate 32 bits ELF, the binary output is specified
                           # in the linker script</code></pre>
<p>The option <code>-T</code> instructs the linker to use the linker script <code>link.ld</code>.</p>
<h3 id="a-c-library"> A C Library</h3>
<p>It might now be interesting to start thinking about writing a small “standard library” for your programs. Some of the functionality requires <a href="#system-calls">system calls</a> to work, but some, such as the functions in <code>string.h</code>, does not.</p>
<h2 id="further-reading-8"> Further Reading</h2>
<ul>
<li>Gustavo Duarte has an article on privilege levels: <a href="http://duartes.org/gustavo/blog/post/cpu-rings-privilege-and-protection">http://duartes.org/gustavo/blog/post/cpu-rings-privilege-and-protection</a></li>
</ul>
<h2 id="file-systems"> File Systems</h2>
<p>We are not required to have file systems in our operating system, but it is a very usable abstraction, and it often plays a central part of many operating systems, especially UNIX-like operating systems. Before we start the process of supporting multiple processes and system calls we might want to consider implementing a simple file system.</p>
<h2 id="why-a-file-system"> Why a File System?</h2>
<p>How do we specify what programs to run in our OS? Which is the first program to run? How do programs output data or read input?</p>
<p>In UNIX-like systems, with their almost-everything-is-a-file convention, these problems are solved by the file system. (It might also be interesting to read a bit about the Plan 9 project, which takes this idea one step further.)</p>
<h2 id="a-simple-read-only-file-system"> A Simple Read-Only File System</h2>
<p>The simplest file system might be what we already have - one file, existing only in RAM, loaded by GRUB before the kernel starts. When the kernel and operating system grows this is probably too limiting.</p>
<p>A file system that is slightly more advanced than just the bits of one file is a file with metadata. The metadata can describe the type of the file, the size of the file and so on. A utility program can be created that runs at build time, adding this metadata to a file. This way, a “file system in a file” can be constructed by concatenating several files with metadata into one large file. The result of this technique is a read-only file system that resides in memory (once GRUB has loaded the file).</p>
<p>The program constructing the file system can traverse a directory on the host system and add all subdirectories and files as part of the target file system. Each object in the file system (directory or file) can consist of a header and a body, where the body of a file is the actual file and the body of a directory is a list of entries - names and “addresses” of other files and directories.</p>
<p>Each object in this file system will become contiguous, so they will be easy to read from memory for the kernel. All objects will also have a fixed size (except for the last one, which can grow), therefore it is difficult to add new files or modify existing ones.</p>
<h2 id="inodes-and-writable-file-systems"> Inodes and Writable File Systems</h2>
<p>When the need for a writable file system arises, then it is a good idea to look into the concept of an <em>inode</em>. See the section <a href="#further-reading-6">“Further Reading”</a> for recommended reading.</p>
<h2 id="a-virtual-file-system"> A Virtual File System</h2>
<p>What abstraction should be used for reading and writing to devices such as the screen and the keyboard?</p>
<p>A virtual file system (VFS) creates an abstraction on top of the concrete file systems. A VFS mainly supplies the path system and file hierarchy, it delegates operations on files to the underlying file systems. The original paper on VFS is succinct and well worth a read. See the section <a href="#further-reading-6">“Further Reading”</a> for a reference.</p>
<p>With a VFS we could mount a special file system on the path <code>/dev</code>. This file system would handle all devices such as keyboards and the console. However, one could also take the traditional UNIX approach, with major/minor device numbers and <code>mknod</code> to create special files for devices. Which approach you think is the most appropriate is up to you, there is no right or wrong when building abstraction layers (although some abstractions turn out way more useful than others).</p>
<h2 id="further-reading-9"> Further Reading</h2>
<ul>
<li>The ideas behind the Plan 9 operating systems is worth taking a look at: <a href="http://plan9.bell-labs.com/plan9/index.html">http://plan9.bell-labs.com/plan9/index.html</a></li>
<li>Wikipedia’s page on inodes: <a href="http://en.wikipedia.org/wiki/Inode">http://en.wikipedia.org/wiki/Inode</a> and the inode pointer structure: <a href="http://en.wikipedia.org/wiki/Inode_pointer_structure">http://en.wikipedia.org/wiki/Inode_pointer_structure</a>.</li>
<li>The original paper on the concept of vnodes and a virtual file system is quite interesting: <a href="http://www.arl.wustl.edu/~fredk/Courses/cs523/fall01/Papers/kleiman86vnodes.pdf">http://www.arl.wustl.edu/~fredk/Courses/cs523/fall01/Papers/kleiman86vnodes.pdf</a></li>
<li>Poul-Henning Kamp discusses the idea of a special file system for <code>/dev</code> in <a href="http://static.usenix.org/publications/library/proceedings/bsdcon02/full_papers/kamp/kamp_html/index.html">http://static.usenix.org/publications/library/proceedings/bsdcon02/full_papers/kamp/kamp_html/index.html</a></li>
</ul>
<h2 id="system-calls"> System Calls</h2>
<p><em>System calls</em> is the way user-mode applications interact with the kernel - to ask for resources, request operations to be performed, etc. The system call API is the part of the kernel that is most exposed to the users, therefore its design requires some thought.</p>
<h2 id="designing-system-calls"> Designing System Calls</h2>
<p>It is up to us, the kernel developers, to design the system calls that application developers can use. We can draw inspiration from the POSIX standards or, if they seem like too much work, just look at the ones for Linux, and pick and choose. See the section <a href="#further-reading-7">“Further Reading”</a> at the end of the chapter for references.</p>
<h2 id="implementing-system-calls"> Implementing System Calls</h2>
<p>System calls are traditionally invoked with software interrupts. The user applications put the appropriate values in registers or on the stack and then initiates a pre-defined interrupt which transfers execution to the kernel. The interrupt number used is dependent on the kernel, Linux uses the number <code>0x80</code> to identify that an interrupt is intended as a system call.</p>
<p>When system calls are executed, the current privilege level is typically changed from PL3 to PL0 (if the application is running in user mode). To allow this, the DPL of the entry in the IDT for the system call interrupt needs to allow PL3 access.</p>
<p>Whenever inter-privilege level interrupts occur, the processor pushes a few important registers onto the stack - the same ones we used to enter user mode <a href="#user-mode">before</a>, see figure 6-4, section 6.12.1, in the Intel manual <span>[33]</span>. What stack is used? The same section in <span>[33]</span> specifies that if an interrupt leads to code executing at a numerically lower privilege level, a stack switch occurs. The new values for the registers <code>ss</code> and <code>esp</code> is loaded from the current Task State Segment (TSS). The TSS structure is specified in figure 7-2, section 7.2.1 of the Intel manual <span>[33]</span>.</p>
<p>To enable system calls we need to setup a TSS before entering user mode. Setting it up can be done in C by setting the <code>ss0</code> and <code>esp0</code> fields of a “packed struct” that represents a TSS. Before loading the “packed struct” into the processor, a TSS descriptor has to be added to the GDT. The structure of the TSS descriptor is described in section 7.2.2 in <span>[33]</span>.</p>
<p>You specify the current TSS segment selector by loading it into the <code>tr</code> register with the <code>ltr</code> assembly code instruction. If the TSS segment descriptor has index 5, and thus offset <code>5 * 8 = 40 = 0x28</code>, this is the value that should be loaded into the register <code>tr</code>.</p>
<p>When we entered user mode before in the chapter <a href="#entering-user-mode">“Entering User Mode”</a> we disabled interrupts when executing in PL3. Since system calls are implemented using interrupts, interrupts must be enabled in user mode. Setting the IF flag bit in the <code>eflags</code> value on the stack will make <code>iret</code> enable interrupts (since the <code>eflags</code> value on the stack will be loaded into the <code>eflags</code> register by the assembly code instruction <code>iret</code>).</p>
<h2 id="further-reading-10"> Further Reading</h2>
<ul>
<li>The Wikipedia page on POSIX, with links to the specifications: <a href="http://en.wikipedia.org/wiki/POSIX">http://en.wikipedia.org/wiki/POSIX</a></li>
<li>A list of system calls used in Linux: <a href="http://bluemaster.iu.hio.no/edu/dark/lin-asm/syscalls.html">http://bluemaster.iu.hio.no/edu/dark/lin-asm/syscalls.html</a></li>
<li>The Wikipedia page on system calls: <a href="http://en.wikipedia.org/wiki/System_call">http://en.wikipedia.org/wiki/System_call</a></li>
<li>The Intel manual <span>[33]</span> sections on interrupts (chapter 6) and TSS (chapter 7) are where you get all the details you need.</li>
</ul>
<h2 id="multitasking"> Multitasking</h2>
<p>How do you make multiple processes appear to run at the same time? Today, this question has two answers:</p>
<ul>
<li>With the availability of multi-core processors, or on system with multiple processors, two processes can actually run at the same time by running two processes on different cores or processors.</li>
<li>Fake it. That is, switch rapidly (faster than a human can notice) between the processes. At any given moment there is only one process executing, but the rapid switching gives the impression that they are running “at the same time”.</li>
</ul>
<p>Since the operating system created in this book does not support multi-core processors or multiple processors the only option is to fake it. The part of the operating system responsible for rapidly switching between the processes is called the <em>scheduling algorithm</em>.</p>
<h2 id="creating-new-processes"> Creating New Processes</h2>
<p>Creating new processes is usually done with two different system calls: <code>fork</code> and <code>exec</code>. <code>fork</code> creates an exact copy of the currently running process, while <code>exec</code> replaces the current process with one that is specified by a path to the location of a program in the file system. Of these two we recommend that you start implementing <code>exec</code>, since this system call will do almost exactly the same steps as described in the section <a href="#setting-up-for-user-mode">“Setting up for user mode”</a> in the chapter <a href="#user-mode">“User Mode”</a>.</p>
<h2 id="cooperative-scheduling-with-yielding"> Cooperative Scheduling with Yielding</h2>
<p>The easiest way to achieve rapid switching between processes is if the processes themselves are responsible for the switching. The processes run for a while and then tell the OS (via a system call) that it can now switch to another process. Giving up the control of CPU to another process is called <em>yielding</em> and when the processes themselves are responsible for the scheduling it’s called <em>cooperative scheduling</em>, since all the processes must cooperate with each other.</p>
<p>When a process yields the process’ entire state must be saved (all the registers), preferably on the kernel heap in a structure that represents a process. When changing to a new process all the registers must be restored from the saved values.</p>
<p>Scheduling can be implemented by keeping a list of which processes are running. The system call <code>yield</code> should then run the next process in the list and put the current one last (other schemes are possible, but this is a simple one).</p>
<p>The transfer of control to the new process is done via the <code>iret</code> assembly code instruction in exactly the same way as explained in the section <a href="#entering-user-mode">“Entering user mode”</a> in the chapter <a href="#user-mode">“User Mode”</a>.</p>
<p>We <strong>strongly</strong> recommend that you start to implement support for multiple processes by implementing cooperative scheduling. We further recommend that you have a working solution for both <code>exec</code>, <code>fork</code> and <code>yield</code> before implementing preemptive scheduling. Since cooperative scheduling is deterministic, it is much easier to debug than preemptive scheduling.</p>
<h2 id="preemptive-scheduling-with-interrupts"> Preemptive Scheduling with Interrupts</h2>
<p>Instead of letting the processes themselves manage when to change to another process the OS can switch processes automatically after a short period of time. The OS can set up the <em>programmable interval timer</em> (PIT) to raise an interrupt after a short period of time, for example 20 ms. In the interrupt handler for the PIT interrupt the OS will change the running process to a new one. This way the processes themselves don’t need to worry about scheduling. This kind of scheduling is called <em>preemptive scheduling</em>.</p>
<h3 id="programmable-interval-timer"> Programmable Interval Timer</h3>
<p>To be able to do preemptive scheduling the PIT must first be configured to raise interrupts every <em>x</em> milliseconds, where <em>x</em> should be configurable.</p>
<p>The configuration of the PIT is very similar to the configuration of other hardware devices: a byte is sent to an I/O port. The command port of the PIT is <code>0x43</code>. To read about all the configuration options, see the article about the PIT on OSDev <span>[39]</span>. We use the following options:</p>
<ul>
<li>Raise interrupts (use channel 0)</li>
<li>Send the divider as low byte then high byte (see next section for an explanation)</li>
<li>Use a square wave</li>
<li>Use binary mode</li>
</ul>
<p>This results in the configuration byte <code>00110110</code>.</p>
<p>Setting the interval for how often interrupts are to be raised is done via a <em>divider</em>, the same way as for the serial port. Instead of sending the PIT a value (e.g.&nbsp;in milliseconds) that says how often an interrupt should be raised you send the divider. The PIT operates at 1193182 Hz as default. Sending the divider 10 results in the PIT running at <code>1193182 / 10 = 119318</code> Hz. The divider can only be 16 bits, so it is only possible to configure the timer’s frequency between 1193182 Hz and <code>1193182 / 65535 = 18.2</code> Hz. We recommend that you create a function that takes an interval in milliseconds and converts it to the correct divider.</p>
<p>The divider is sent to the channel 0 data I/O port of the PIT, but since only one byte can be sent at at a time, the lowest 8 bits of the divider has to sent first, then the highest 8 bits of the divider can be sent. The channel 0 data I/O port is located at <code>0x40</code>. Again, see the article on OSDev <span>[39]</span> for more details.</p>
<h3 id="separate-kernel-stacks-for-processes"> Separate Kernel Stacks for Processes</h3>
<p>If all processes uses the same kernel stack (the stack exposed by the TSS) there will be trouble if a process is interrupted while still in kernel mode. The process that is being switched to will now use the same kernel stack and will overwrite what the previous process have written on the stack (remember that TSS data structure points to the <em>beginning</em> of the stack).</p>
<p>To solve this problem every process should have it’s own kernel stack, the same way that each process have their own user mode stack. When switching process the TSS must be updated to point to the new process’ kernel stack.</p>
<h3 id="difficulties-with-preemptive-scheduling"> Difficulties with Preemptive Scheduling</h3>
<p>When using preemptive scheduling one problem arises that doesn’t exist with cooperative scheduling. With cooperative scheduling every time a process yields, it must be in user mode (privilege level 3), since yield is a system call. With preemptive scheduling, the processes can be interrupted in either user mode or kernel mode (privilege level 0), since the process itself does not control when it gets interrupted.</p>
<p>Interrupting a process in kernel mode is a little bit different than interrupting a process in user mode, due to the way the CPU sets up the stack at interrupts. If a privilege level change occurred (the process was interrupted in user mode) the CPU will push the value of the process <code>ss</code> and <code>esp</code> register on the stack. If <em>no</em> privilege level change occurs (the process was interrupted in kernel mode) the CPU won’t push the <code>esp</code> register on the stack. Furthermore, if there was no privilege level change, the CPU won’t change stack to the one defined it the TSS.</p>
<p>This problem is solved by calculating what the value of <code>esp</code> was <em>before</em> the interrupt. Since you know that the CPU pushes 3 things on the stack when no privilege change happens and you know how much you have pushed on the stack, you can calculate what the value of <code>esp</code> was at the time of the interrupt. This is possible since the CPU won’t change stacks if there is no privilege level change, so the content of <code>esp</code> will be the same as at the time of the interrupt.</p>
<p>To further complicate things, one must think of how to handle case when switching to a new process that should be running in kernel mode. Since <code>iret</code> is being used without a privilege level change the CPU won’t update the value of <code>esp</code> with the one placed on the stack - you must update <code>esp</code> yourself.</p>
<h2 id="further-reading-11"> Further Reading</h2>
<ul>
<li>For more information about different scheduling algorithms, see <a href="http://wiki.osdev.org/Scheduling_Algorithms">http://wiki.osdev.org/Scheduling_Algorithms</a></li>
</ul>
<div>
<h2> References</h2>
<p>[1] Andrew Tanenbaum, 2007. <em>Modern operating systems, 3rd edition</em>. Prentice Hall, Inc.,</p>
<p>[2] <em>The royal institute of technology</em>, <a href="http://www.kth.se/">http://www.kth.se</a>,</p>
<p>[3] Wikipedia, <em>Hexadecimal</em>, <a href="http://en.wikipedia.org/wiki/Hexadecimal">http://en.wikipedia.org/wiki/Hexadecimal</a>,</p>
<p>[4] OSDev, <em>OSDev</em>, <a href="http://wiki.osdev.org/Main_Page">http://wiki.osdev.org/Main_Page</a>,</p>
<p>[5] James Molloy, <em>James m’s kernel development tutorial</em>, <a href="http://www.jamesmolloy.co.uk/tutorial_html/">http://www.jamesmolloy.co.uk/tutorial_html/</a>,</p>
<p>[6] Canonical Ltd, <em>Ubuntu</em>, <a href="http://www.ubuntu.com/">http://www.ubuntu.com/</a>,</p>
<p>[7] Oracle, <em>Oracle vM virtualBox</em>, <a href="http://www.virtualbox.org/">http://www.virtualbox.org/</a>,</p>
<p>[8] Dennis M. Ritchie Brian W. Kernighan, 1988. <em>The c programming language, second edition</em>. Prentice Hall, Inc.,</p>
<p>[9] Wikipedia, <em>C (programming language)</em>, <a href="http://en.wikipedia.org/wiki/C_(programming_language)">http://en.wikipedia.org/wiki/C_(programming_language)</a>,</p>
<p>[10] Free Software Foundation, <em>GCC, the gNU compiler collection</em>, <a href="http://gcc.gnu.org/">http://gcc.gnu.org/</a>,</p>
<p>[11] NASM, <em>NASM: The netwide assembler</em>, <a href="http://www.nasm.us/">http://www.nasm.us/</a>,</p>
<p>[12] Wikipedia, <em>Bash</em>, <a href="http://en.wikipedia.org/wiki/Bash_%28Unix_shell%29">http://en.wikipedia.org/wiki/Bash_%28Unix_shell%29</a>,</p>
<p>[13] Free Software Foundation, <em>GNU make</em>, <a href="http://www.gnu.org/software/make/">http://www.gnu.org/software/make/</a>,</p>
<p>[14] Volker Ruppert, <em>bochs: The open souce iA-32 emulation project</em>, <a href="http://bochs.sourceforge.net/">http://bochs.sourceforge.net/</a>,</p>
<p>[15] QEMU, <em>QEMU</em>, <a href="http://wiki.qemu.org/Main_Page">http://wiki.qemu.org/Main_Page</a>,</p>
<p>[16] Wikipedia, <em>BIOS</em>, <a href="https://en.wikipedia.org/wiki/BIOS">https://en.wikipedia.org/wiki/BIOS</a>,</p>
<p>[17] Free Software Foundation, <em>GNU gRUB</em>, <a href="http://www.gnu.org/software/grub/">http://www.gnu.org/software/grub/</a>,</p>
<p>[18] Wikipedia, <em>Executable and linkable format</em>, <a href="http://en.wikipedia.org/wiki/Executable_and_Linkable_Format">http://en.wikipedia.org/wiki/Executable_and_Linkable_Format</a>,</p>
<p>[19] Free Software Foundation, <em>Multiboot specification version 0.6.96</em>, <a href="http://www.gnu.org/software/%20%20%20%20%20%20%20%20%20%20%20grub/manual/multiboot/multiboot.html">http://www.gnu.org/software/
           grub/manual/multiboot/multiboot.html</a>,</p>
<p>[20] GNU, <em>GNU binutils</em>, <a href="http://www.gnu.org/software/binutils/">http://www.gnu.org/software/binutils/</a>,</p>
<p>[21] Lars Nodeen, <em>Bug #426419: configure: error: GRUB requires a working absolute objcopy</em>, <a href="https://bugs.launchpad.net/ubuntu/+source/grub/+bug/426419">https://bugs.launchpad.net/ubuntu/+source/grub/+bug/426419</a>,</p>
<p>[22] Wikipedia, <em>ISO image</em>, <a href="http://en.wikipedia.org/wiki/ISO_image">http://en.wikipedia.org/wiki/ISO_image</a>,</p>
<p>[23] Bochs, <em>bochsrc</em>, <a href="http://bochs.sourceforge.net/doc/docbook/user/bochsrc.html">http://bochs.sourceforge.net/doc/docbook/user/bochsrc.html</a>,</p>
<p>[24] NASM, <em>RESB and friends: Declaring uninitialized data</em>, <a href="http://www.nasm.us/doc/nasmdoc3.htm">http://www.nasm.us/doc/nasmdoc3.htm</a>,</p>
<p>[25] Wikipedia, <em>x86 calling conventions</em>, <a href="http://en.wikipedia.org/wiki/X86_calling_conventions">http://en.wikipedia.org/wiki/X86_calling_conventions</a>,</p>
<p>[26] Wikipedia, <em>Framebuffer</em>, <a href="http://en.wikipedia.org/wiki/Framebuffer">http://en.wikipedia.org/wiki/Framebuffer</a>,</p>
<p>[27] Wikipedia, <em>VGA-compatible text mode</em>, <a href="http://en.wikipedia.org/wiki/VGA-compatible_text_mode">http://en.wikipedia.org/wiki/VGA-compatible_text_mode</a>,</p>
<p>[28] Wikipedia, <em>ASCII</em>, <a href="https://en.wikipedia.org/wiki/Ascii">https://en.wikipedia.org/wiki/Ascii</a>,</p>
<p>[29] OSDev, <em>VGA hardware</em>, <a href="http://wiki.osdev.org/VGA_Hardware">http://wiki.osdev.org/VGA_Hardware</a>,</p>
<p>[30] Wikipedia, <em>Serial port</em>, <a href="http://en.wikipedia.org/wiki/Serial_port">http://en.wikipedia.org/wiki/Serial_port</a>,</p>
<p>[31] OSDev, <em>Serial ports</em>, <a href="http://wiki.osdev.org/Serial_ports">http://wiki.osdev.org/Serial_ports</a>,</p>
<p>[32] WikiBooks, <em>Serial programming/8250 uART programming</em>, <a href="http://en.wikibooks.org/wiki/Serial_Programming/%20%20%20%20%20%208250_UART_Programming">http://en.wikibooks.org/wiki/Serial_Programming/
      8250_UART_Programming</a>,</p>
<p>[33] Intel, <em>Intel 64 and iA-32 architectures software developer’s manual vol. 3A</em>, <a href="http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.html/">http://www.intel.com/content/
www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.html/</a>,</p>
<p>[34] NASM, <em>Multi-line macros</em>, <a href="http://www.nasm.us/doc/nasmdoc4.html#section-4.3">http://www.nasm.us/doc/nasmdoc4.html#section-4.3</a>,</p>
<p>[35] SIGOPS, <em>i386 interrupt handling</em>, <a href="http://www.acm.uiuc.edu/sigops/roll_your_own/i386/irq.html">http://www.acm.uiuc.edu/sigops/roll_your_own/i386/irq.html</a>,</p>
<p>[36] Andries Brouwer, <em>Keyboard scancodes</em>, <a href="http://www.win.tue.nl/">http://www.win.tue.nl/</a>,</p>
<p>[37] Steve Chamberlain, <em>Using ld, the gNU linker</em>, <a href="http://www.math.utah.edu/docs/info/ld_toc.html">http://www.math.utah.edu/docs/info/ld_toc.html</a>,</p>
<p>[38] OSDev, <em>Page frame allocation</em>, <a href="http://wiki.osdev.org/Page_Frame_Allocation">http://wiki.osdev.org/Page_Frame_Allocation</a>,</p>
<p>[39] OSDev, <em>Programmable interval timer</em>, <a href="http://wiki.osdev.org/Programmable_Interval_Timer">http://wiki.osdev.org/Programmable_Interval_Timer</a>,</p>
</div>

</div></div>]]></description>
        </item>
    </channel>
</rss>