<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 04 Feb 2024 16:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How to Replace Your CPAP in Only 666 Days (241 pts)]]></title>
            <link>https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days</link>
            <guid>39248631</guid>
            <pubDate>Sun, 04 Feb 2024 08:36:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days">https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days</a>, See on <a href="https://news.ycombinator.com/item?id=39248631">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p><em>This story is not practical advice. For me, it’s closing the book on an almost two-year saga. For you, I hope it’s an enjoyable bit of bureaucratic schadenfreude. For Anthem, I hope it’s the subject of a series of painful but transformative meetings. This is not an isolated event. I’ve had dozens of struggles with Anthem customer support, and they all go like this.</em></p>
<p><em>If you’re looking for practical advice: it’s this. Be polite. Document everything. Keep a log. Follow the claims process. Check the laws regarding insurance claims in your state. If you pass the legally-mandated deadline for your claim, call customer service. Do not allow them to waste a year of your life, or force you to resubmit your claim from scratch. Initiate a complaint with your state regulators, and escalate directly to <a href="mailto:gail.boudreaux@elevancehealth.com">Gail Boudreaux’s team</a>–or whoever Anthem’s current CEO is.</em></p>
<p>To start, experience an equipment failure.</p>
<p>Use your CPAP daily for six years. Wake up on day zero with it making a terrible sound. Discover that the pump assembly is failing. Inquire with Anthem Ohio, your health insurer, about how to have it repaired. Allow them to refer you to a list of local durable medical equipment providers. Start calling down the list. Discover half the list are companies like hair salons. Eventually reach a company in your metro which services CPAPs. Discover they will not repair broken equipment unless a doctor tells them to.</p>
<p>Leave a message with your primary care physician. Call the original sleep center that provided your CPAP. Discover they can’t help, since you’re no longer in the same state. Return to your primary, who can’t help either, because he had nothing to do with your prescription. Put the sleep center and your primary in touch, and ask them to talk.</p>
<p>On day six, call your primary to check in. He’s received a copy of your sleep records, and has forwarded them to a local sleep center you haven’t heard of. They, in turn, will talk to Anthem for you.</p>
<p>On day 34, receive an approval letter labeled “confirmation of medical necessity” from Anthem, directed towards the durable medical equipment company. Call that company and confirm you’re waitlisted for a new CPAP. They are not repairable. Begin using your partner’s old CPAP, which is not the right class of device, but at least it helps.</p>
<p>Over the next 233 days, call that medical equipment company regularly. Every time, inquire whether there’s been any progress, and hear “we’re still out of stock”. Ask them you what the manufacturer backlog might be, how many people are ahead of you in line, how many CPAPs they <em>do</em> receive per month, or whether anyone has ever received an actual device from them. They won’t answer any questions. Realize they are never going to help you.</p>
<p>On day 267, realize there is no manufacturer delay. The exact machine you need is in stock on CPAP.com. Check to make sure there’s a claims process for getting reimbursed by Anthem. Pay over three thousand dollars for it. When it arrives, enjoy being able to breathe again.</p>
<p>On day 282, follow CPAP.com’s documentation to file a claim with Anthem online. Include your prescription, receipt, shipping information, and the confirmation of medical necessity Anthem sent you.</p>
<p>On day 309, open the mail to discover a mysterious letter from Anthem. They’ve received your appeal. You do not recall appealing anything. There is no information about what might have been appealed, but something will happen within 30-60 days. There is nothing about your claim.</p>
<p>On day 418, emerge from a haze of lead, asbestos, leaks, and a host of other home-related nightmares; remember Anthem still hasn’t said anything about your claim. Discover your claim no longer appears on Anthem’s web site. Call Anthem customer service. They have no record of your claim either. Ask about the appeal letter you received. Listen, gobsmacked, as they explain that they decided your claim was in fact an appeal, and transferred it immediately to the appeals department. The appeals department examined the appeal and looked for the claim it was appealing. Finding none, they decided the appeal was moot, and rejected it. At no point did anyone inform you of this. Explain to Anthem’s agent that you filed a claim online, not an appeal. At their instruction, resign yourself to filing the entire claim again, this time using a form via physical mail. Include a detailed letter explaining the above.</p>
<p>On day 499, retreat from the battle against home entropy to call Anthem again. Experience a sense of growing dread as the customer service agent is completely unable to locate either of your claims. After a prolonged conversation, she finds it using a different tool. There is no record of the claim from day 418. There was a claim submitted on day 282. Because the claim does not appear in her system, there is no claim. There is a claim. There is no claim. Experience the cognitive equivalent of the Poltergeist hallway shot as the agent tells you “Our members are not eligible for charges for claim submission”.</p>
<p>Hear the sentence “There is a claim”. Hear the sentence “There is no claim”. Write these down in the detailed log you’ve been keeping of this unfurling Kafkaesque debacle. Ask again if there is anyone else who can help. There is no manager you can speak to. There is no tier II support. “I’m the only one you can talk to,” she says. Write that down.</p>
<p>Call CPAP.com, which has a help line staffed by caring humans. Explain that contrary to their documentation, Anthem now says members cannot file claims for equipment directly. Ask if they are the provider. Discover the provider for the claim is probably your primary care physician, who has no idea this is happening. Leave a message with him anyway. Leave a plaintive message with your original sleep center for good measure.</p>
<p>On day 502, call your sleep center again. They don’t submit claims to insurance, but they confirm that some people <em>do</em> successfully submit claims to Anthem using the process you’ve been trying. They confirm that Anthem is, in fact, hot garbage. Call your primary, send them everything you have, and ask if they can file a claim for you.</p>
<p>On day 541, receive a letter from Anthem, responding to your inquiry. You weren’t aware you filed one.</p>
<blockquote>
<p>Please be informed that we have received your concern. Upon review we have noticed that there is no claim billed for the date of service mentioned in the submitted documents, Please provide us with a valid claim. If not submitted,provide us with a valid claim iamge to process your claim further.</p>
</blockquote>
<p>Stare at the letter, typos and all. Contemplate your insignificance in the face of the vast and uncaring universe that is Anthem.</p>
<p>On day 559, steel your resolve and call Anthem again. Wait as this representative, too, digs for evidence of a claim. Listen with delight as she finds your documents from day 282. Confirm that yes, a claim definitely exists. Have her repeat that so you can write it down. Confirm that the previous agent was lying: members can submit claims. At her instruction, fill out the claim form a third time. Write a detailed letter, this time with a Document Control Number (DCN). Submit the entire package via registered mail. Wait for USPS to confirm delivery eight days later.</p>
<p>On day 588, having received no response, call Anthem again. Explain yourself. You’re getting good at this. Let the agent find a reference number for an appeal, but not the claim. Incant the magic DCN, which unlocks your original claim.  “I was able to confirm that this was a claim submitted form for a member,” he says. He sees your claim form, your receipts, your confirmation of medical necessity. However: “We still don’t have the claim”.</p>
<p>Wait for him to try system after system. Eventually he confirms what you heard on day 418: the claims department transferred your claims to appeals. “Actually this is not an appeal, but it was denied as an appeal.” Agree as he decides to submit your claim manually again, with the help of his supervisor. Write down the call ref number: he promises you’ll receive an email confirmation, and an Explanation of Benefits in 30-40 business days.</p>
<p>“I can assure you this is the last time you are going to call us regarding this.”</p>
<p>While waiting for this process, recall insurance is a regulated industry. Check the Ohio Revised Code. Realize that section 3901.381 establishes deadlines for health insurers to respond to claims. They should have paid or denied each of your claims within 30 days–45 if supporting documentation was required. Leave a message with the Ohio Department of Insurance’s Market Conduct Division. File an insurance complaint with ODI as well.</p>
<p>Grimly wait as no confirmation email arrives.</p>
<p>On day 602, open an email from Anthem. They are “able to put the claim in the system and currenty on processed [sic] to be applied”. They’re asking for more time. Realize that Anthem is well past the 30-day deadline under the Ohio Revised Code for all three iterations of your claim.</p>
<p>On day 607, call Anthem again. She explains that the claim will be received and processed as of your benefits. She asks you to allow 30-45 days from today. Quote section 3901.381 to her. She promises to expedite the request; it should be addressed within 72 business hours. Like previous agents, she promises to call you back. Nod, knowing she won’t.</p>
<p>On day 610, email the Ohio Department of Insurance to explain that Anthem has found entirely new ways to avoid paying their claims on time. It’s been 72 hours without a callback; call Anthem again. She says “You submitted a claim and it was received” on day 282. She says the claim was expedited. Ask about the status of that expedited resolution. “Because on your plan we still haven’t received any claims,” she explains. Wonder if you’re having a stroke.</p>
<p>Explain that it has been 328 days since you submitted your claim, and ask what is going on. She says that since the first page of your mailed claim was a letter, that might have caused it to be processed as an appeal. Remind yourself Anthem told you to enclose that letter. Wait as she attempts to refer you to the subrogation department, until eventually she gives up: the subrogation department doesn’t want to help.</p>
<p>Call the subrogation department yourself. Allow Anthem’s representative to induce in you a period of brief aphasia. She wants to call a billing provider. Try to explain there is none: you purchased the machine yourself. She wants to refer you to collections. Wonder why on earth Anthem would want money from <em>you</em>. Write down “I literally can’t understand what she thinks is going on” in your log. Someone named Adrian will call you by tomorrow.</p>
<p>Contemplate alternative maneuvers. Go on a deep Google dive, searching for increasingly obscure phrases gleaned from Anthem’s bureaucracy. Trawl through internal training PDFs for Anthem’s ethics and compliance procedures. Call their compliance hotline: maybe someone cares about the law. It’s a third-party call center for Elevance Health. Fail to realize this is another name for Anthem. Begin drawing a map of Anthem’s corporate structure.</p>
<p>From a combination of publicly-available internal slide decks, LinkedIn, and obscure HR databases, discover the name, email, and phone number of Anthem’s Chief Compliance Officer. Call her, but get derailed by an internal directory that requires a 10-digit extension. Try the usual tricks with automated phone systems. No dice.</p>
<p>Receive a call from an Anthem agent. Ask her what happened to “72 hours”. She says there’s been no response from the adjustments team. She doesn’t know when a response will come. There’s no one available to talk to. Agree to speak to another representative tomorrow. It doesn’t matter: they’ll never call you.</p>
<p>Do more digging. Guess the CEO’s email from what you can glean of Anthem’s account naming scheme. Write her an email with a short executive summary and a detailed account of the endlessly-unfolding Boschian hellscape in which her company has entrapped you. A few hours later, receive an acknowledgement from an executive concierge at Elevance (Anthem). It’s polite, formal, and syntactically coherent. She and promises to look into things. Smile. Maybe this will work.</p>
<p>On day 617, receive a call from the executive concierge. 355 days after submission, she’s identified a problem with your claim. CPAP.com provided you with an invoice with a single line item (the CPAP) and two associated billing codes (a CPAP and humidifier). Explain that they are integrated components of a single machine. She understands, but insists you need a receipt with multiple line items for them anyway. Anthem has called CPAP.com, but they can’t discuss an invoice unless you call them. Explain you’ll call them right now.</p>
<p>Call CPAP.com. Their customer support continues to be excellent. Confirm that it is literally impossible to separate the CPAP and humidifier, or to produce an invoice with two line items for a single item. Nod as they ask what the hell Anthem is doing. Recall that this is the exact same machine Anthem covered for you eight years ago. Start a joint call with the CPAP.com representative and Anthem’s concierge. Explain the situation to her voicemail.</p>
<p>On day 623, receive a letter from ODI. Anthem has told ODI this was a problem with the billing codes, and ODI does not intervene in billing code issues. They have, however, initiated a secretive second investigation. There is no way to contact the second investigator.</p>
<p>Write a detailed email to the concierge and ODI explaining that it took over three hundred days for Anthem to inform you of this purported billing code issue. Explain again that it is a single device. Emphasize that Anthem has been handling claims for this device for roughly a decade.</p>
<p>Wait. On day 636, receive a letter from Anthem’s appeals department. They’ve received your request for an appeal. You never filed one. They want your doctor or facility to provide additional information to Carelon Medical Benefits Management. You have never heard of Carelon. There is no explanation of how to reach Carelon, or what information they might require. The letter concludes: “There is currently no authorization on file for the services rendered.” You need to seek authorization from a department called “Utilization Management”.</p>
<p>Call the executive concierge again. Leave a voicemail asking what on earth is going on.</p>
<p>On day 637, receive an email: she’s looking into it.</p>
<p>On day 644, Anthem calls you. It’s a new agent who is immensely polite. Someone you’ve never heard of was asked to work on another project, so she’s taking over your case. She has no updates yet, but promises to keep in touch.</p>
<p>She does so. On day 653, she informs you Anthem will pay your claim in full. On day 659, she provides a check number. On day 666, the check arrives.</p>
<p>Deposit the check. Write a thank you email to the ODI and Anthem’s concierge. Write this, too, down in your log.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why is the mouse cursor slightly tilted and not straight? (335 pts)]]></title>
            <link>https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight</link>
            <guid>39248225</guid>
            <pubDate>Sun, 04 Feb 2024 06:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight">https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight</a>, See on <a href="https://news.ycombinator.com/item?id=39248225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainbar" role="main" aria-label="question and answers">
                
<div data-questionid="52336" data-position-on-page="0" data-score="604" id="question">
        

        

<div>
    
    <div itemprop="text">
                
<p>Is this a legacy thing or does a tilted cursor serves a purpose? I can tell that, the angle provides a totally vertical left edge which helps when highlighting text but what else apart from that?</p>

<p>EDIT: When cursor is swapped by the little hand cursor when hovered over buttons, the angle seems to be smaller. Why the difference?</p>
    </div>

        

    <div>
    <div>
        <p>
            asked <span title="2014-02-17 09:41:59Z">Feb 17, 2014 at 9:41</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/7393/thanos"><p><img src="https://www.gravatar.com/avatar/aac452da1312fa333c36015a5e591f01?s=64&amp;d=identicon&amp;r=PG" alt="Thanos's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>




            <p><span itemprop="commentCount">8</span></p>
    </div>



                
                
                <div id="answers">
                    


                                    
<div id="answer-52338" data-answerid="52338" data-parentid="52336" data-score="729" data-position-on-page="1" data-highest-scored="1" data-question-has-accepted-highest-score="1" itemprop="acceptedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                
<p>This is the historical reason:</p>

<p><img src="https://i.stack.imgur.com/e1zH5.png" alt="Concept drawing of the standard mouse cursor at an angle"></p>

<p>(Concept drawing taken from document: <a href="http://bitsavers.trailing-edge.com/pdf/xerox/parc/techReports/VLSI-81-1_The_Optical_Mouse.pdf">VLSI-81-1_The_Optical_Mouse.pdf</a>)</p>

<p>The mouse, and therefore the mouse cursor, was <a href="http://arstechnica.com/features/2005/05/gui/2/">invented by Douglas Engelbart</a>, and was initially <a href="http://origin.arstechnica.com/images/gui/4-NLSgui.jpg">an arrow pointing up</a>. </p>

<p>When the <a href="http://arstechnica.com/features/2005/05/gui/3/">XEROX PARC</a> machine was built, the cursor changed into a tilted arrow. It was found that, given the low resolution of the screens in those days, drawing a straight line (left edge of arrow) and a line at a 45 degree angle (right edge of arrow) was easier to do and more recognizable than the straight cursor.</p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/40110/code-maverick"><p><img src="https://www.gravatar.com/avatar/c056c352518943b11095c83a4ef2b31f?s=64&amp;d=identicon&amp;r=PG" alt="Code Maverick's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 09:47:52Z">Feb 17, 2014 at 9:47</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/5657/bart-gijssens"><p><img src="https://www.gravatar.com/avatar/54b1215ffb534c90dd7ea7f480d28c51?s=64&amp;d=identicon&amp;r=PG" alt="Bart Gijssens's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/5657/bart-gijssens">Bart Gijssens</a><span itemprop="name">Bart Gijssens</span></p><p><span title="reputation score 17,317" dir="ltr">17.3k</span><span>4 gold badges</span><span>49 silver badges</span><span>62 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">9</span></p>
    </div>


                                    
<div id="answer-52370" data-answerid="52370" data-parentid="52336" data-score="393" data-position-on-page="2" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                    

<p>Take your right hand and point to your question.</p>

<p>There, you see. </p>

<p><img src="https://i.stack.imgur.com/xqGFX.jpg" alt="finger pointing at screen"></p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/127876/silas-reel"><p><img src="https://lh5.googleusercontent.com/-yjvPGG9oHpw/AAAAAAAAAAI/AAAAAAAAAAA/AAN31DV4PX0I1kJiPjyoOIMz70ejP2SvbA/mo/photo.jpg?sz=64" alt="Silas Reel's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 18:13:23Z">Feb 17, 2014 at 18:13</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/43246/jturolla"><p><img src="https://www.gravatar.com/avatar/d24c555de0ab090f0b822155f31affe4?s=64&amp;d=identicon&amp;r=PG" alt="jturolla's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/43246/jturolla">jturolla</a><span itemprop="name">jturolla</span></p><p><span title="reputation score " dir="ltr">3,511</span><span>1 gold badge</span><span>10 silver badges</span><span>7 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">27</span></p>
    </div>

                                    
<div id="answer-52349" data-answerid="52349" data-parentid="52336" data-score="189" data-position-on-page="3" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>In addition to <a href="https://ux.stackexchange.com/a/52338/43668">Bart's answer</a>, I'd like to add one more reason. </p>

<p>The reason the arrow was tilted to the left was so that the click position was easier to calculate, because the origin of the cursor's bitmap was in the upper left.  This saved the mouse tracking subroutine a calculation on every click (its not much but it helped on older machines).  </p>

<p><a href="http://www.reddit.com/r/explainlikeimfive/comments/1qhzym/" rel="noreferrer">Source</a></p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/-1/community"><p><img src="https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&amp;d=identicon&amp;r=PG" alt="Community's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 14:40:50Z">Feb 17, 2014 at 14:40</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/21591/jameo"><p><img src="https://www.gravatar.com/avatar/939c911747739eeb05c16b6b8a922ed9?s=64&amp;d=identicon&amp;r=PG" alt="Jameo's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/21591/jameo">Jameo</a><span itemprop="name">Jameo</span></p><p><span title="reputation score " dir="ltr">1,853</span><span>1 gold badge</span><span>11 silver badges</span><span>8 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">14</span></p>
    </div>


                                    
<div id="answer-52558" data-answerid="52558" data-parentid="52336" data-score="124" data-position-on-page="4" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<h2>Low level visual cognition</h2>
<p>In addition to the various answers given, there is also sense in a tilted mouse pointer if one considers the visual processes in our brain.</p>
<p>Visual information arriving from our eyes is first processed in the primary visual cortex by the V1 area, then by the V2 area. These two areas recognise low-level visual features (hue, lightness, size, orientation, etc.).</p>
<h2>The popout effect</h2>
<p>As visual information is processed by these areas, some visual irregularities truly pop out (ie, they are highly distinguishable), which greatly helps visual search (trying to find an item in a visually busy field). The popular name for this phenomenon is <strong>the popout effect</strong>.</p>
<p>A famous research from 1988 - <a href="http://www2.psychology.uiowa.edu/faculty/hollingworth/prosem/Treisman_Gormican_88_PR_FeatureAnalysisIn.pdf" rel="noreferrer">A. Treisman, and S. Gormican: Feature analysis in early vision: Evidence from search asymmetries</a> summarises many of these popout effects, and the irregularities they involve.</p>
<h2>Orientation</h2>
<p>One such irregularity is <strong>orientation</strong>, and it is neatly explained by the following illustration:</p>
<p><img src="https://i.stack.imgur.com/4xWaH.png" alt="3 images showing many vertical lines and how a tilted line pops out"></p>
<p>You should find it next to impossible to find the search target in 1 (a straight line in a group of straight lines). But rather easy in 2 - finding a tilted line in a group of straight lines. In 3 it should be equally next to impossible to find the tilted line in a group of tilted lines (of the same angle).</p>
<p>Since vertical and horizontal orientations are the most common ones on screens (and in life in general) a tilted mouse pointer will be more easily found.</p>
<p>More information can be found in Chapter 2 (What we can easily see) of <a href="http://www.amazon.co.uk/Visual-Thinking-Kaufmann-Interactive-Technologies/dp/0123708966/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1384303964&amp;sr=1-1&amp;keywords=visual+thinking+for+design" rel="noreferrer">Visual Thinking for Design</a>, Ware 2008.</p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/-1/community"><p><img src="https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&amp;d=identicon&amp;r=PG" alt="Community's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-19 23:38:31Z">Feb 19, 2014 at 23:38</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/16924/izhaki"><p><img src="https://www.gravatar.com/avatar/35c050eac0eab06a8c3b6fec8c2bb5c0?s=64&amp;d=identicon&amp;r=PG" alt="Izhaki's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/16924/izhaki">Izhaki</a><span itemprop="name">Izhaki</span></p><p><span title="reputation score 32,465" dir="ltr">32.5k</span><span>5 gold badges</span><span>66 silver badges</span><span>99 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">13</span></p>
    </div>

                                    
<div id="answer-52355" data-answerid="52355" data-parentid="52336" data-score="80" data-position-on-page="5" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                    

<p>I've always thought that the arrow cursor is shaped similarly to your hand if you were point (naturally) at the screen with your (as typically dominant) right hand.</p>

<p>I have no support of this other than my own subjective experience but it strikes me as a natural shape when trying to relate real world interaction into a low resolution computer screen where rendering something resembling a hand would be impossible.</p>

<p>[Edit: Someone stole the only thunder I've ever had on StackAnything. Thanks!]</p>

<p><img src="https://i.stack.imgur.com/qGzNQ.jpg" alt="Hand pointing at screen"></p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-17 15:23:08Z">Feb 17, 2014 at 15:23</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/43174/user43174"><p><img src="https://www.gravatar.com/avatar/a69a703c86958e2d50e517ffd86c5e01?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="user43174's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/43174/user43174">user43174</a><span itemprop="name">user43174</span></p><p><span title="reputation score " dir="ltr">853</span><span>5 silver badges</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">6</span></p>
    </div>

                                    
<div id="answer-52461" data-answerid="52461" data-parentid="52336" data-score="49" data-position-on-page="6" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>In case anyone wonders : some less known interfaces did use a straight arrow as pointed in <a href="http://www.reddit.com/r/explainlikeimfive/comments/1qhzym/">Reddit</a></p>

<p><img src="https://i.stack.imgur.com/rJmmW.gif" alt="enter image description here"></p>

<p><img src="https://i.stack.imgur.com/ukk6x.gif" alt="enter image description here"></p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-19 00:48:06Z">Feb 19, 2014 at 0:48</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/25125/gildas-fr%c3%a9mont"><p><img src="https://i.stack.imgur.com/RHbGy.jpg?s=64&amp;g=1" alt="Gildas Frémont's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-52360" data-answerid="52360" data-parentid="52336" data-score="22" data-position-on-page="7" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>Also, there is another answer to this question. As a rule, the <strong>arrow</strong> mouse cursor must have one sharp tip (vertex) - because it is an arrow :) </p>

<p>On the other hand, it is better for a mouse cursor to look good and slick. </p>

<p>But drawing sharp tip on a rectangular pixel based display is very hard, especially without anti-aliasing. </p>

<p>The 0 degrees (horizontal or vertical) and 45 degrees lines are the only possible lines that look smooth without anti-aliasing. </p>

<p>That is why almost all arrow mouse cursors are based on one straight and one 45 degrees lines. As a result, the bisector line has angle of 45/2 = 22.5 degrees.</p>

<p>The tail of the arrow is much harder to be drawn well, but it is not so important as well. </p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-17 16:30:01Z">Feb 17, 2014 at 16:30</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/25879/johnfound"><p><img src="https://i.stack.imgur.com/R81XM.png?s=64&amp;g=1" alt="johnfound's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/25879/johnfound">johnfound</a><span itemprop="name">johnfound</span></p><p><span title="reputation score " dir="ltr">1,116</span><span>8 silver badges</span><span>16 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">4</span></p>
    </div>

                                    
<div id="answer-68326" data-answerid="68326" data-parentid="52336" data-score="7" data-position-on-page="8" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p><strong>It is a right-handed world.</strong> </p>

<p>It used to be that if you switched our right/left click buttons the arrow would point towards the right (opposite of the images cited). </p>

<p>This supports that the arrow mimics a hand pointing while providing angular contrast. Without a reference, it is an extension of the <em>desktop</em> metaphor.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-12-03 19:19:22Z">Dec 3, 2014 at 19:19</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/10767/ken"><p><img src="https://i.stack.imgur.com/htBYy.png?s=64&amp;g=1" alt="Ken's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/10767/ken">Ken</a><span itemprop="name">Ken</span></p><p><span title="reputation score " dir="ltr">1,232</span><span>7 silver badges</span><span>10 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-97398" data-answerid="97398" data-parentid="52336" data-score="5" data-position-on-page="9" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>The fact that the mouse cursor is slightly tilted to the left makes a lot of sense. 
A very interesting fact:</p>

<p>If it were straight, it would take a nanosecond more to place the cursor on the desired object. Human mind is generally used to perceiving elements from left to the right, that is why the cursor is designed into the opposite direction, anticipating the intent of interaction with the element you are about to click on.</p>

<p>A nanosecond of time optimization is the closest thing to the absolute idea of irrelevance. With that I agree. However, on a perception level, it makes a huge difference. </p>

<p>The tilted cursor becomes similar to an athlete who's always on the start position, ready to take off towards anything you want to click on at any time.</p>

<p>It's a sensation that gives you so much comfort without you realizing why.</p>

<p>Semiotics, Cognitive Science and Psychology are all embedded into the simple and subtle decision of keeping the tilted cursor, just to simplify by a bit your experience.</p>

<p>Why was it tilted in the first place? Well, in its history, it seems like it was only an accident determined by some technical limitations:</p>

<p><a href="http://www.fastcodesign.com/3026625/why-the-mouse-cursor-is-tilted-instead-of-vertical" rel="nofollow">Why Your Mouse Cursor Looks The Way It Does</a></p>
    </div>
    <div>
            
            <div>
    
    <div>
        <a href="https://ux.stackexchange.com/users/54669/devin"><p><img src="https://i.stack.imgur.com/egmb3.jpg?s=64&amp;g=1" alt="Devin's user avatar" width="32" height="32"></p></a>
    </div>
    <div>
        <p><a href="https://ux.stackexchange.com/users/54669/devin">Devin</a></p><p><span title="reputation score 37,762" dir="ltr">37.8k</span><span>15 gold badges</span><span>79 silver badges</span><span>140 bronze badges</span>
        </p>
    </div>
</div>


            <div>
    <div>
        <p>
            answered <span title="2016-07-29 04:24:58Z">Jul 29, 2016 at 4:24</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/86656/mircea"><p><img src="https://graph.facebook.com/10208868140194564/picture?type=large" alt="Mircea's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/86656/mircea">Mircea</a><span itemprop="name">Mircea</span></p><p><span title="reputation score " dir="ltr">522</span><span>3 silver badges</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-68302" data-answerid="68302" data-parentid="52336" data-score="3" data-position-on-page="10" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <div itemprop="text">
<p>The angle, the cursor is inclined at gives a better feeling of pointing something. A cursor straight at 90 degree would not provide a good effect.It provides  improved appearance on low resolution screens.</p>

<p>Also the position calculation would become a lot easier when done from the top left corner of the pixel.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-12-03 13:04:10Z">Dec 3, 2014 at 13:04</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/41491/ashu"><p><img src="https://i.stack.imgur.com/3KnoW.jpg?s=64&amp;g=1" alt="ashu's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/41491/ashu">ashu</a><span itemprop="name">ashu</span></p><p><span title="reputation score " dir="ltr">249</span><span>1 silver badge</span><span>9 bronze badges</span>
        </p>
    </div>
</div>
    
</div>

                                    
<div id="answer-101006" data-answerid="101006" data-parentid="52336" data-score="2" data-position-on-page="11" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <p>A straight cursor would also obscure more of the object underneath raising the same issues when designing for touch interfaces</p>
    <div>
    <div>
        <p>
            answered <span title="2016-11-01 22:07:43Z">Nov 1, 2016 at 22:07</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/47160/mark-c"><p><img src="https://graph.facebook.com/710302166/picture?type=large" alt="Mark C's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/47160/mark-c">Mark C</a><span itemprop="name">Mark C</span></p><p><span title="reputation score " dir="ltr">151</span><span>1 silver badge</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
    
</div>

                                    
<div id="answer-135748" data-answerid="135748" data-parentid="52336" data-score="2" data-position-on-page="12" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <div itemprop="text">
<p>Well, the cursor is a pointer, and mimics pointer angles from real life (~30-45° to the vertical).</p>
<p><a href="https://i.stack.imgur.com/J8xzk.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/J8xzk.png" alt="Pointers in the real-world"></a></p>
<p>Importantly, that angle serves to <strong>guide the eye down the length of the pointer</strong>, in the direction going "into" the screen, <strong>towards a single point</strong>, in the same way as perspective drawings do:</p>
<p><a href="https://i.stack.imgur.com/mKdC5.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/mKdC5.png" alt="Perspective drawing"></a></p>
<p>On the contrary, a straight arrow seems to point in the general up-direction, targeting no one point in particular. Have you ever used, or seen someone use, a pointer stick vertically upwards? That is indeed awkward, and reserved for moments where the object being pointed to is high up and well beyond the height of the person and the length of the stick combined, and can be vague in conveying what is actually being pointed at.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2020-11-26 20:32:45Z">Nov 26, 2020 at 20:32</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/28743/snag"><p><img src="https://www.gravatar.com/avatar/4a6245cc648b214ad6a440bfe18d0152?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="SNag's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/28743/snag">SNag</a><span itemprop="name">SNag</span></p><p><span title="reputation score " dir="ltr">9,597</span><span>3 gold badges</span><span>22 silver badges</span><span>26 bronze badges</span>
        </p>
    </div>
</div>
    
</div>


                                    



                            <h2 data-loc="1">
                                
                            </h2>
                </div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The first amateur radio station on the moon (126 pts)]]></title>
            <link>https://www.arrl.org/news/the-first-amateur-radio-station-on-the-moon-js1ymg-is-now-transmitting</link>
            <guid>39247614</guid>
            <pubDate>Sun, 04 Feb 2024 04:27:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.arrl.org/news/the-first-amateur-radio-station-on-the-moon-js1ymg-is-now-transmitting">https://www.arrl.org/news/the-first-amateur-radio-station-on-the-moon-js1ymg-is-now-transmitting</a>, See on <a href="https://news.ycombinator.com/item?id=39247614">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p><span>02/02/2024</span></p><p>The Japan Aerospace Exploration Agency (JAXA) successfully landed their Smart Lander for Investigating Moon (SLIM) on January 19, 2024. Just before touchdown, SLIM released two small lunar surface probes, LEV-1 and LEV-2.</p>
<p>LEV-2 collects data while moving on the lunar surface, and LEV-1 receives the data.</p>
<p>The JAXA Ham Radio Club (JHRC), JQ1ZVI, secured amateur radio license JS1YMG for LEV-1, which has been transmitting Morse code on 437.41 MHz since January 19. The probe uses a 1 W UHF antenna with circular polarization and is transmitting "matters related to amateur business."</p>
<p>Radio amateurs have been busy analyzing JS1YMG's signal, with&nbsp;<a href="https://destevez.net/2024/01/trying-to-decode-lev-1/" target="_blank">Daniel Estévez's, EA4GPZ, blog</a>&nbsp;introducing the method and extraction results for demodulating Morse code from the signal, as well as extracting the code string.</p>
<p>It's unclear how long signals will be heard. JAXA has said that SLIM was not designed to survive a lunar night, which lasts about 14 days, and is due to return in a few days.</p>
<p>SLIM was launched on September 6, 2023, and landed on January 19, 2024, with the mission of analyzing the composition of rocks to aid research about the origin of the moon. SLIM's landing made Japan the fifth country to achieve a soft touchdown on the moon. The landing was achieved with exceptional precision -- within 180 feet of its targeted touchdown location.</p>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AvaloniaUI: Create Multi-Platform Apps with .NET (149 pts)]]></title>
            <link>https://www.avaloniaui.net/</link>
            <guid>39246988</guid>
            <pubDate>Sun, 04 Feb 2024 02:34:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.avaloniaui.net/">https://www.avaloniaui.net/</a>, See on <a href="https://news.ycombinator.com/item?id=39246988">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

            <!-- Heading -->
            <h2>
              A proven path for WPF app modernization.
            </h2>

            <!-- Text -->
            <p>
                <span>Familiar.</span> Considered a spiritual successor to WPF, Avalonia UI provides a <a href="https://docs.avaloniaui.net/docs/next/get-started/wpf/">familiar developer experience</a> allowing you to leverage years of pre-existing knowledge and investments. With a <a href="https://www.avaloniaui.net/XPF#hybrid">Hybrid XPF</a> license, it's possible to use WPF controls within your Avalonia application from vendors, including Actipro, Telerik, Syncfusion and more. 
            </p>

            <!-- Text -->
            <p>
                  <span>Proven.</span> Trusted by companies including JetBrains, KLM, Canon, Schneider Electric, Unity Games and more for modernising their WPF apps. 
            </p>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Doom didn't kill the Amiga (137 pts)]]></title>
            <link>https://www.datagubbe.se/afb/</link>
            <guid>39246825</guid>
            <pubDate>Sun, 04 Feb 2024 02:05:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.datagubbe.se/afb/">https://www.datagubbe.se/afb/</a>, See on <a href="https://news.ycombinator.com/item?id=39246825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p><b>A detailed account of the rise and fall of an Amiga zealot</b></p>

<p><i>Early 2024</i></p>

<p>
<img src="https://www.datagubbe.se/afb/pics/dotc.png">
<br>
<i>A scene from the Amiga game Defender of the Crown, released in 1986.
For years, no other home computer came close to screens like these.</i>
</p>

<p>
Ever since I saw an Amiga 500 at a friend's house in what was probably late 1988, I wanted one for myself. Back then, computers were uncommon, especially at home. Even though I went to a school in a fairly affluent neighborhood, few kids had home computers or video games.
</p>

<p>
Gradually, that started to change.
</p>

<p>
I bought my own Amiga in February 1992. It was basically the same exact model as the one I had first seen in 1988: An Amiga 500+ with a 7 MHz 68000 CPU, 1 meg of RAM, 8-bit stereo PCM sound and many various graphics modes, of which 320x256 in 16 or 32 colors was the most common for games.
</p>

<h3>Competing Platforms</h3>

<p>
Buying any other computer was completely out of the question. There were practical reasons of course: I knew how it worked and friends had one, which meant we could copy pirated games from each other. And it ran <a href="https://www.datagubbe.se/dpaint/">Deluxe Paint</a>, an era defining graphics program and a killer application for anyone with artistic inclinations, which I'd convinced myself I had.
</p>

<p>
I had also come into contact with other types of machines. The C64 felt like a thing of the past, with blocky graphics and beepy sound. The Mac was a boring monochrome computer made for writing equally boring documents about tax deductions. I intensely remember seeing a PC for the first time, and how disappointed it made me: it was much worse than the Mac, possibly even than the C64. Downright ugly graphics, terrible sound and a mysterious operating system that required you to learn textual incantations by heart.
</p>

<p>
But the real home computer feud during those days was between the Amiga 500 and the Atari ST, in which the latter always seemed to come out losing; worse sound, worse graphics, worse OS. Such a machine was completely out of the question - it just <i>had</i> to be an Amiga.
</p>

<p>
I don't regret this decision one bit. Few gadgets have given me as much joy and positive experiences as my Amigas, and even in 1992 an Amiga 500 wasn't a bad purchase. The PC was still a fairly boring machine, having a hard time keeping up with the Amiga's sound and graphics without costing an exorbitant amount of money. The revolutionary Amiga architecture, released in 1985 and basically unchanged since then, could still hold its own: a testament to its ingenuity.
</p>

<h3>The Price is Right</h3>

<p>
Around this time, PC:s were showing signs of becoming affordable, and Amiga style platform- and arcade games had begun appearing on the platform. But even the most straightforward of these titles required a fast 386 CPU, plenty of memory and a VGA card to come close to the amount of motion and commotion the Amiga had been capable of for ages. This meant the price to play was still high: In Sweden, around Christmas 1992, a 25 MHz 386/SX with 2 megs of RAM and a 40 meg hard drive was selling for $530. To this came the additional cost of a sound card. A Sound Blaster 2.0, guaranteed to be supported by most games, was available for a whopping $129. While you could hook the Amiga to a cheap 14" TV via RGB SCART, the PC required an expensive VGA monitor, on which you couldn't also watch MacGyver. Bummer!
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/sotb.png">
<br>
<i>Shadow of the Beast, released for the Amiga in 1989. It featured
lush graphics, multiple layers of parallax scrolling, massive sprites
and tough as nails gameplay. It looked like crap on pretty much every
other system, and wasn't even ported to the then inferior PC.</i>
</p>

<p>
This put the cost of an entry level 386 system at $660 without software, compared to the Amiga 600's $499, and the A600's 40 MB hard drive came pre-loaded with Deluxe Paint, a word processor and several decent games. It's true that a 386 PC was much faster than the 7 MHz Amiga in some aspects, but most of those aspects didn't matter for people like me and my friends - at least not yet.
</p>

<h3>More Moore</h3>

<p>
It's hard to convey just how intense the effects of Moore's law were during the 90's. When it came to hardware, December 1992 was a <i>very</i> different time compared to January 1992. During this year, Commodore pulled a stunt by first introducing the A500+ and then swiftly replacing it with the A600 - a low cost model with the advantage of having an integrated IDE interface, only to finish off by making the A600 obsolete with the release of the next-generation A1200. They were widely criticized for this, but in their defense, home computing was moving at breakneck speed and nobody could really keep up. At any given moment, something that had been unfeasible just six months ago was suddenly commonplace. One such something was <b>Doom</b> - which was nowhere, and then suddenly everywhere. Some Amiga fanatics still claim that Doom was what killed the Amiga, but I don't believe that to be true. Doom was a symptom, not the disease proper.
</p>

<p>
Commodore's last sigh, the Amiga 1200, has been touted as a bad machine and an architectural mistake. Its 14 MHz 68020 CPU, 2 megabytes of RAM and <a href="https://en.wikipedia.org/wiki/Amiga_Advanced_Graphics_Architecture">AGA</a> chipset is often, in hindsight, dismissed as too little, too late. Considering this, it still sold fairly well in Europe - because as a home computer, it actually wasn't half bad. It could be equipped with a hard drive, it had an expansion slot for CPU and memory upgrades, and the new graphics modes actually rivalled VGA and even SVGA in many ways. The 1280x512 pixel mode in 18-bit color was too slow for games, but when displaying still pictures it was quite a sight to behold.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/500p.jpg">
<br>
<i>One of the few surviving photos of my Amiga 500+. Here I'm
showing my grandmother something that looks like a shell window,
and she's pretending to be interested. T-shirt sizes were very comfortable
in those days.</i>
</p>

<p>
To me, even in 1994, switching to another architecture still wasn't under consideration. I was now an <i>Amiga fanboy</i> and the platform had become my natural home. And why not? An entry level A1200 hooked to a TV set wasn't the worst of choices for a boy in his early teens and it was of course a natural step up from the A500. My peripherals and nearly all of my software still worked and I could keep using my trusty old 14" TV, on which I could also watch The X-Files without parental interference. It was a platform I was comfortable with, and I got a good second hand deal - it even came with a hard drive, which was what I wanted for my computer most of all at that point.
</p>

<p>
Still, times were changing. PCs were no longer just for very rich kids and office professionals: any serious gamer had to consider one, and not just because of Doom. PC games were perhaps not yet as colorful, zippy and funky as Amiga games, but they were often very complex and relied heavily on a fast CPU to manage that complexity. Most people didn't really have a fast enough PC to fully enjoy Doom at its time of release in 1993 - but just a year later, expectations on home computing had changed. Even though 486 machines hadn't dropped <i>that</i> drastically in price yet, hardly anyone even marketed 386 machines anymore.
</p>

<p>
Memory was cheaper, as were hard drives. Better graphics and sound fidelity was expected, and it seemed every new PC graphics card that came out offered higher resolutions and more colors than the previous. CD quality sound was suddenly a thing, as was putting a CD-ROM reader in your computer. Using a flickering 50 Hz PAL TV instead of a rock solid 60 Hz VGA monitor was no longer seen as clever frugality, but rather as a way of hurting your eyes when trying to play Sim City 2000 in its high resolution 640x480 glory - which the A1200 honestly wasn't quite fast enough to do anyway. Adventure games like Monkey Island II, Simon the Sorcerer and Indiana Jones and the Fate of Atlantis came on eleven (11!), nine (9!) and eleven (11!) floppy disks, respectively. The Amiga 500 had traditionally been a floppy-based system, but a hard drive was now more or less required, even for gamers.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/agony.png">
<br>
<i>A cutscene from the Amiga game Agony (1992). It took a while,
but the PC eventually caught up, graphics wise.</i>
</p>

<p>
In spring of 1994, around the time of Commodore's demise, a 33 MHz 486 PC with an SVGA card, 4 megs of RAM, a 200 meg hard drive <i>and</i> a 14" monitor cost $1300. For an Amiga 1200 to even begin to come close to such a system, you'd have to spend at least as much money. And while the PC still came without a sound card, a high resolution monitor for the Amiga had to be able to auto-switch between 15 kHz (PAL) and 31 kHz (VGA), pushing the price of the Amiga system higher still. Add to that the fact that the PC was a big box machine, with plenty of room for, say, a CD-ROM drive - the hot new thing.
</p>

<h3>The DOS conundrum</h3>

<p>
By 1995, Commodore was well and truly dead. It was during this time I started getting acquainted with other platforms in earnest: DOS, Windows, Linux, newer Macs, even Unix workstations. I had plenty of friends who owned PCs, but I stayed true to my Amiga, adding CPU and RAM upgrades that cost as much as the computer itself. I could have saved money and bought a PC - but I honestly didn't see the point. Linux was nice for surfing the net, but it lacked all of the fun software I craved: games, demos (as in the demo scene), graphics programs, tracker music. It didn't even have the things I wanted for school, such as a reasonable word processor with Swedish spell checking.
</p>

<p>
All of this and more was of course available on PCs, and I was frequently exposed to it when visiting my PC owning friends. Deluxe Paint, my beloved killer app for the Amiga, apparently existed on PC as well. And there were lots of other neat programs that appealed to a young demo scener: Fasttracker and Screamtracker for music, QBasic and Turbo Pascal for programming, QPEG for viewing images, TheDraw for making ANSI graphics, and droves of pictures, music, games and scene demos.
</p>

<p>
The strange thing was that all of the fun PC stuff was made for DOS. I just didn't get it. Command lines no longer put me off, having dabbled quite a bit with them on both Amiga and Linux, but DOS lacked that one crucial thing I had grown ever more accustomed to on my Amiga: Native, effortless, pre-emptive multitasking.
</p>

<p>
The PCs were indeed impressive, hardware-wise. SVGA was now commonplace, as were 486/DX processors running in 33 or even 66 MHz. Doom was not only playable, but enjoyable. PC scene demos were running impressive texture-mapped 3D effects much faster and, with the addition of modern sound cards, offering much better audio fidelity than my Amiga was capable of. Ostensibly simple things like loading JPEG images felt instant compared to my A1200, even after it was upgraded with a 28 MHz 68030 CPU.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/msdos.png">
<br>
<i>Things like these were perfectly reasonable to MS-DOS users. Expanded or extended memory? Or maybe just conventional? Choose wisely, or you won't
be able to fully maximize the incredible potential of running <b>a single program at a time</b>.</i>
</p>

<p>
What I couldn't grasp was the point of having (and paying for) all that raw power if you couldn't utilize it fully. On my Amiga, I could run Amos Professional - an advanced BASIC dialect - and Deluxe Paint <i>at the same time</i>. If I wanted to change an image used in an Amos program, switching between the two applications was instant. If I suddenly wanted to take some notes, I could just fire up a text editor. And I could listen to music at the same time! In DOS, even something as mundane as enjoying tracker music was a full time, full screen activity - not something you could keep doing while working in other programs.
</p>

<p>
This multitasking was and is a big part of my affinity for the Amiga, and it opened up the possibility for several other <a href="https://www.datagubbe.se/ltmag/">ingenious features</a>. Workflows were completely customizeable with powerful scripting languages, the modular design of the OS made it extremely adaptable to things like new file formats and file systems, and every aspect of it could be tweaked and configured to suit personal needs. It was (and still is, in many ways) like running a carefully honed environment from a professional workstation on a cheap home computer. DOS, Atari and Mac just couldn't compare.
</p>

<p>
Furthermore, DOS seemed to require inordinate amounts of tweaking and configuration. Hardware interrupts had to be configured manually, the 640 kB base memory had to be carefully guarded, and you had to select and configure your sound and graphics cards in almost every single game you wanted to play. On the Amiga, everything just worked. If you bought a new peripheral, all you had to do was plug it in and boot your computer. Drivers were needed for some things, of course, but the Amiga's <i>Autoconfig</i> took care of peripheral detection and configuration.
</p>

<p>
When Dial-up Internet became (almost) affordable for the masses, the Amiga delivered there, too. PC users had to load up Windows 3.11, with its questionable cooperative multitasking, but the Amiga felt as smooth as ever. While waiting for a slow FTP download, I could chat with friends on IRC, play (simple) games and listen to music. A fast 486 with plenty of RAM could perhaps do the same, but not yet with my Amiga's inherent elegance.
</p>

<p>
Hence, the Amiga still felt like a superior platform. Its swift multitasking, efficient resource usage and <a href="https://www.datagubbe.se/ltmag/">many clever ideas</a> made both DOS and Windows feel clunky and primitive. Even though Windows 95 had entered the scene, it was more or less unusable without at least 8 megs of RAM. And all the fun stuff on PCs was <i>still</i> being made for DOS.
</p>

<h3>Doom me once, shame on you</h3>

<p>
What about Doom, then? John Carmack himself has allegedly said that he didn't consider the Amiga as being capable of running Doom. As an Amiga zealot, I'd of course like to point out that he was wrong - it's since been ported to the Amiga and runs just fine. But in my heart of hearts, I know that in 1993, he was actually right. The only Amiga available at that time powerful enough to make Doom palatable would have been an Amiga 4000 with a 25 MHz 68040 CPU, costing somewhere around $2500 - without a monitor.
</p>

<p>
Commodore was frequently derided for not producing yet another <i>killer computer</i>, a proper new Amiga model as revolutionary as the Amiga 1000 had been in 1985.  This complaint is somewhat valid, but still, I think, misses its target. It's not that the Commodore engineers didn't have plans - and even prototypes - for much more advanced graphics hardware than the AGA chipset in the A1200. There are just plenty of <i>other</i> reasons for why things might not have gone as expected, even if they had reached market.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/rnt.png">
<br>
<i>Ruff'n'Tumble on Amiga (1994): huge bosses, beautiful graphics and frantic action.</i>
</p>

<p>
VGA was deceptively simple. A framebuffer, more or less, which provided the famous <i>chunky</i> Mode 13h - part of what made Doom possible. No hardware sprites. No blitter for fast memory copying. No <i>copper</i> (co-processor) like the Amiga had, which could change the color of a given palette index every scanline. But you <i>could</i> write once to memory for a single given pixel, and get any color from an indexed palette of 256 values. And you could run spreadsheets in a steady, 60 Hz 640x480.
</p>

<p>
The Amiga had been designed when sprite-based games were the hottest thing since sliced toast, when memory was still stupendously expensive and when the ability to display 80 column text was considered a noteworthy feature on many home computers. As opposed to VGA, the Amiga had <i>planar</i> graphics, requiring multiple writes to memory to produce a single color value - a perfectly reasonable choice for the time, and one that enabled a lot of other nifty programming tricks and visual effects. The Amiga architecture was so tuned for arcade action that even in 1994, games like Ruff'n'Tumble showed that the 7 MHz Amiga 500 could still hold its own against powerful PCs when it came to fast paced 2D shooters. The problem was that after having dominated the market since first showing up in arcade cabinets, games in that style were becoming unfashionable.
</p>

<p>
By now it should be clear that what really drove the PC boom was neither Doom nor chunky graphics: it was cheaper and faster CPUs. Chunky-to-planar conversion on the Amiga does steal a few CPU cycles, but even if the A1200 had been equipped with a chunky mode, its 14 MHz 68020 processor would've been far too slow for Doom. Motorola's 486 equivalent, the 68040, hadn't been subjected to the price drops of mass produced PC clones and competition from rival manufacturers, such as AMD's and Cyrix' 486 compatible offerings. Put simply: Commodore could have crammed an actual VGA card into the A1200, but its CPU would still have been far too slow for Doom. And even if Doom had never materialized, an affordable Motorola CPU still couldn't crunch the numbers needed for increasingly complex simulators and strategy games.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/msfsim.png">
<br>
<i>Microsoft Flight Simulator 5.0 - tremendously slow even on the recommended minimum 386 CPU.</i>
</p>

<h3>The later 90's</h3>

<p>
Some time after 1995, I and many fellow Amiga zealots had started feeling an itch that was hard to scratch. It's true that this itch came, in part, from Doom. Not so much from wanting to play it, but from the desire to show the world that yes, the Amiga <i>was</i> in fact capable of running it. With the correct, expensive upgrades, mind you - but still.
</p>

<p>
Alas, the itch was also caused by other, more pressing matters. The remnants of Commodore had been bought by Escom and then Gateway, but no new Amiga models had materialized. Despite this, we had dutifully upgraded our CPUs, expanded our RAM, and kept true to our machines - but nothing of substance had materialized from the new owners of Commodore's IP.
</p>

<p>
There had been poor attempts at Doom clones on the A1200, but they all somehow lacked the parts of Doom that made it Doom. Surfing the net was now, to be honest, quite painful even on an accelerated Amiga with an expensive dual-sync monitor. The higher horizontal refresh rate (31 kHz) congested the AGA chipset and made 256 color screens unbearably slow. And all those JPEG files took forever to decompress! Some pages didn't look right, either: they were designed for Netscape, a program not available on our platform. In short, we were desperate for the launch of new Amiga hardware.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/iout.png">
<br>
<i>The pinnacle of Amiga zealot humor in the mid-90's. Hyuk, hyuk, ackshually...</i>
</p>

<p>
We were used to walkover victories in comparisons between computer brands, but it was now painfully obvious that our beloved Amiga was lagging behind. We didn't know how to respond and many of us considered remarks or even simple questions about our platform to be personal insults. This made us completely insufferable, and we spent inordinate amounts of time jeering both on- and offline about "Micro$oft", <a href="https://en.wikipedia.org/wiki/Pentium_FDIV_bug">buggy Pentiums</a>, cooperative multitasking and "Plug'n'Pray". Littering IRC with statements like "Windows 95 = Amiga 85" achieved nothing except making us look obnoxious. But teenage frustration mixed with sunk cost is a tough mix of emotions to combat: We had invested too much time, money and prestige to give up now. The Amiga would surely rise again! Except, of course, <i>damn you</i> if you even suggested the use of cheap Intel CPU:s at the core of the platform.
</p>

<p>
In 1997, the Doom source code was released and Amiga ports started cropping up. With a very expensive CPU expansion, it was perfectly playable on an A1200 - but nobody cared about Doom anymore. It was all Duke Nukem 3D and Quake now, and MP3 files, and fast Pentium MMX CPUs or even the impressive Pentium II and dedicated 3D graphics cards.
</p>

<p>
I was now running a 40 MHz 68040 CPU in my A1200, but even that couldn't keep up with the cheapest of PCs. DOS was more or less gone by now, and the fun stuff had started, little by little, to move into Windows.
</p>

<p>
In 1998, I finally caved and bought a powerful Pentium II PC. I was running a dual-boot system with both Windows 95 and Linux, but eventually Windows NT 4 won out. I could surf the web as well as anyone, play Quake III (after a graphics card update), program web pages using Microsoft's Personal Web Server, ASP and Access databases, chat with my friends on IRC and even watch DivX movies.
</p>

<p>
But there was still something missing. The fun just wasn't as fun on Linux and Windows. I guess a lot of people like me felt the same: even in 1999, the best demo scene stuff for PC was still clinging to DOS. Personally, I missed all the clever stuff my Amiga and its OS did, and the <a href="https://www.datagubbe.se/dopus/">great</a> and <a href="https://www.datagubbe.se/mkdem/">familiar</a> software it ran. 
</p>

<p>
So I just didn't stop using my Amiga. In the early 2000's, I upgraded to an impressively fast 50 MHz 68060 CPU, the last in Motorola's 68k series. I even bought an expensive, towerized A1200 with a 24-bit ("SVGA") graphics card and an ethernet card. It was ridiculously expensive and underpowered compared to any off the shelf PC available at the time - and yet, such a beefy Amiga was a paradox. It was a boyhood dream five or ten years too late - and also less of an Amiga than I perhaps cared to admit to myself at the time. The really fun stuff like Deluxe Paint, Amos and scene demos didn't run on the graphics card. It was more of a platform for running AmigaOS than an Amiga proper, and I eventually ended up downgrading to a more modest configuration. While it couldn't keep up as a daily driver, it was still a computer I booted up regularly, just to have <i>fun</i>.
</p>

<p>
It still is, and I still do.
</p>

<p>
<a href="https://www.datagubbe.se/afb/pics/anet.gif"><img src="https://www.datagubbe.se/afb/pics/anet.gif"></a>
<br>
<i>Network configuration on a souped up Amiga 1200, circa 2003.</i>
</p>

<h3>Custom Silicon</h3>

<p>
Doom didn't kill the Amiga - it was more like a measure of the many nails in the coffin of custom hardware home computer platforms. The biggest culprit was economies of scale.
</p>

<p>
Popular 8-bit home computers had, over time and quite naturally, been replaced by 16- and 32-bit machines. Many manufacturers of unique 8-bit machines during the Cambrian explosion of home computers simply went defunct or started making PC clones. By 1995, the only remaining, popular 32-bit home computer system was the PC. Apple regularly tried to muscle in on the home market, especially during the mid-90's with their PowerPC machines, but without much success. During this time period, a Mac was more of a niche office machine than the PC had ever been, and Apple survived mostly by selling systems for magazine and print ad production. They were in fact dangerously close to folding when Steve Jobs stepped back in and managed to secure funding from Microsoft (as "anti-trust insurance") and launch the iMac just in time to ride the wave of the Internet boom.
</p>

<p>
Commodore had been able to sell their successful 8-bit machines and early Amiga models cheaply thanks to vertical integration. This basically meant that they owned the chip fab, MOS Technology, that manufactured the Amiga's custom chips (and even the C64's 6502 CPU). At the time of its release, this made the original Amiga almost bizarrely cheap compared to equivalent Mac and PC offerings with similar performance.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/amad.jpg">
<br>
<i>A Swedish Amiga 4000T advertisement from 1997. For the listed SEK 29990 (roughly $3000 in today's exchange rate), you could get a fully specced out Pentium II machine - with a decent monitor - running in circles around the Amiga. If you forked out this much for an A4000 at this point in time, I dare say you were stupid. On the inside of your brain.</i>
</p>

<p>
But in 1994, when Commodore went bust, the PC clone market had been in full swing for over a decade. Cobbling together a 486 system using mass produced parts proved, in the end, to be far more competitive than designing, prototyping and manufacturing your own complex graphics and sound hardware for a single platform. Besides being cheap, this kind of standardization had more advantages. One was that PCs worked the same all over the world. Since traditional home computers were meant to work with television sets, they had to be timed to either the PAL or NTSC signal standard, which also meant that games (and other software) were timed to this as well. Games made for the European market didn't work on US Amigas without being rewritten, and vice versa. PCs didn't have this problem, providing a global market without added development cost.
</p>

<p>
Commodore's never completed <a href="https://en.wikipedia.org/wiki/Advanced_Amiga_Architecture_chipset">AAA</a> and <a href="https://en.wikipedia.org/wiki/Amiga_Hombre_chipset">Hombre</a> architectures sound impressive on paper, but were still not finished when the A1200 and A4000 were released in 1992. Besides, It's easy to list specs for nonexistent hardware in hindsight. Even if AAA had become what was promised, it wouldn't necessarily have been competitively priced. Even with the AGA machines, several cost cutting measures had been taken and <i>they still struggled</i> when it came to pricing. Would an even more advanced architecture somehow, magically, have sold as cheap or even cheaper? I have my doubts.
</p>

<p>
It's quite possible that the Commodore engineers could've worked a chunky mode into the AGA chipset. But, as discussed above, the A1200 would have needed a much faster CPU if Doom - or any other "killer app" game - was ever going to be a possibility. A 68040 would have been far too expensive, but even a 40 or 50 MHz 68030 CPU would've put the machine at a decidedly different price point. Combined with a hypothetical new graphics architecture, we can only speculate about the cost. The Amiga was known for being <i>good and cheap</i>, and it was proving hard not just for Commodore to combine the two.
</p>

<h3>Falcon Heavy</h3>

<p>
Many of Commodore's mistakes are said to have occurred after its founder, Jack Tramiel, left the company - even though the Amiga, a great success by all accounts, was bought by Commodore after his departure. Perhaps Tramiel's hardline approach to business ("Business is war!") and cost cutting ("Computers for the masses, not the classes!") could have led Commodore and (had they still bought it) the Amiga down a different path, but history tells us otherwise.
</p>

<p>
Tramiel left Commodore for Atari Corporation, a company that abandoned the home computer market in 1993 after their last-ditch effort, the Falcon 030. Like the A1200, the Falcon wasn't really a bad computer - but it cost even more than a souped-up Amiga 1200, and was still, in many aspects, underpowered compared to 486 PCs. It didn't come with the impressive specs listed for Commodore's AAA chipset, but its capabilities were far more advanced than those of the A1200. Its graphics chip, ViDEL, could produce a wide array of impressive resolutions, including a chunky 16-bit truecolor graphics mode. It had 16-bit sound, a faster CPU (16 MHz 68030), and a 32 MHz Motorola 56001 digital signal processor. This, together with both IDE, SCSI and networking hardware made for a very capable machine indeed.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/falc2.jpg">
<br>
<i>The Atari Falcon 030 still had the classic "home computer in a keyboard"
form factor.</i>
</p>

<p>
When considering all of this, the Falcon <i>was</i> actually competitively priced - but that didn't matter. People who needed SCSI or networking in 1993 usually had other budgets and priorities than home users, such as running WordPerfect or Lotus 1-2-3 for DOS. People who wanted great sound and graphics for gaming didn't want to pay extra for stuff they'd never use.
</p>

<p>
All those bells and whistles, together with the need for keeping the BLiTTER and YM2149F chips for backwards compatibility, meant the Falcon was too curious, inflexible and expensive for something that was supposedly a home computer. In the end, all its <i>killer architecture</i> managed to kill was, sadly, Atari itself.
</p>

<h3>Early birds and worms</h3>

<p>
What if Commodore had managed to put out a new, revolutionary Amiga model <i>much earlier</i> than 1992? Perhaps this could have saved the platform, ensuring its continued longevity? Maybe - but even if the Amiga 1000 launched in 1985, Amigas didn't become popular until the release of the cheaper, stabler and more mature Amiga 500 in 1987 - the same year IBM launched VGA. Could Commodore have released a VGA killer in 1988? As discussed in the beginning of this text - at this point in time, <i>the Amiga already was a VGA killer</i>. Consumer level PC:s just weren't fast enough to do something interesting with 256 colors, and VGA cards were much too expensive anyway. To keep a truly competitive cutting edge, the Amiga would have needed not just a new graphics architecture, but probably a CPU upgrade as well - raising the total cost of the machine considerably.
</p>

<p>
Commodore could surely have produced something very impressive, but again, at what price point - and would it have mattered? Most PCs sold at this time were still turbo XT clones with crappy CGA graphics and yet, PC dominance was already well established in the business sector. This also meant it had started seeping into homes where parents saw an opportunity of running spreadsheets and letting the kids play games and do their homework on the same machine, instead of buying two expensive family computers.
</p>

<p>
Commodore was often derided for their poor attempts at marketing the Amiga - but I think that's a bit unfair, too. The Amiga was launched at a huge press event where Andy Warhol and Debbie Harry famously appeared to show off the computer's graphics abilities. This was followed by print and TV ads that clearly and vividly showcased the advanced hard- and software. In Europe, Amigas sold like hotcakes and Commodore UK were very successful in bundling hardware with popular software titles. And, due to its graphics and video capabilities, the Amiga was a well known and popular machine in broadcasting circuits - in large parts thanks to the impressive <a href="https://en.wikipedia.org/wiki/Video_Toaster">Video Toaster</a>.
</p>

<p>
Could the Amiga have muscled in on the office market in a meaningful way? I honestly don't think so - Apple couldn't, Atari couldn't, in fact <i>nothing but IBM compatibles could</i>. The Amiga 3000 - a high end model often considered to be some of Commodore's finest work - is said to have piqued the interest of Sun Microsystems, who wanted to license it as a low end UNIX workstation. The deal fell through, much to the chagrin of Amiga fanatics across the globe - another oft-cited example of Commodore's failure as a company. Even so, Sun machines catered to a different niche market than IBM PCs, and while the notion of a mass produced Amiga UNIX workstation <i>sounds cool</i>, it's questionable if it would somehow have made consumer Amigas cheaper, faster and better - or simply led Commodore onto a path of expensive high end machines competing with the likes of SGI, HP and DEC. At the very least, considering it would've been running UNIX, it probably wouldn't have resulted in more resources allocated for AmigaOS development.
</p>

<p>
The real question here is, I think, if we would actually have wanted the Amiga to become yet another boring office machine. Everything that was fun and great about it was, to me at least, also what made business execs so suspicious of it.
</p>

<h3>Last breaths</h3>

<p>
With the demise of Commodore and Atari, the traditional home computer was basically dead. Acorn stayed in the game until 1998, but their expensive Archimedes line of machines was never very popular in actual homes, surviving by being more or less subsidized by British schools. Their subsequent RiscPC models catered mainly to various niche actors in broadcasting, including the BBC. Their legacy is now carried on by the popular Raspberry Pi computer.
</p>

<p>
Even highly specialized machines, such as Unix workstations, were living precarious lives by the end of the 90's. SGI, Sun, Digital and IBM focused their efforts more and more on servers and less on desktop machines. The last new Unix workstation models were both launched in 2006, by IBM and Sun. Even in this lucrative segment, competition from cheap PC hardware ultimately proved insurmountable.
</p>

<p>
With this in mind, a new <i>killer architecture</i> from Commodore may have been impressive, but <i>it might not even have been an Amiga as we know them</i>. It could have been a completely new machine. It could have been (mostly) compatible with the "classic" Amiga by somehow incorporating the old Amiga into the new one - still synced to PAL or NTSC, with all that entails, and of course adding cost to the machine. It could have become some kind of short-lived UNIX workstation or perhaps a games console. Or, it could have become a new Voodoo style graphics card for PCs - a platform that was also manufactured by Commodore, and quite profitably at that.
</p>

<p>
Both the Atari Falcon and Amiga 1200 were already suffering from minor problems with backwards software compatibility, something that probably made a lot of consumers more open to switching to PC. It's of course impossible to say for sure, but certainly not unthinkable, that the AAA architecture would have failed miserably to run the existing, bare metal banging games (and applications) an upgrading user would have expected to work on an "Amiga".
</p>

<p>
In short: Commodore might have lasted longer as a company - in some form - had they made other decisions. Such decisions, however, may not have been ones guaranteeing the continued existence of a platform recognizable as an Amiga. Apart from the Mac - in many ways thanks to Microsoft promising continued support and providing a $150 million cash injection - <i>no other home or desktop computer platform survived the 1990's PC dominance</i> in any meaningful way. It seems highly unlikely that the Amiga would somehow have propelled itself back into a significant market position thanks to a chunky graphics mode or, considering the fate of the Falcon 030, even a reworked architecture.
</p>

<h3>All but gone</h3>

<p>
The Amiga was an amazing platform, so far ahead of its time it stayed alive for much longer than what seems reasonable. It came out during the end of the Cambrian home computer explosion and remained in production for close to ten consecutive years. Saying it wasn't successful because of Commodore's lack of business savvy is doing it a disservice: Many, many millions of Amigas were sold and it was, for several years, the dominant home machine in Europe, where it shaped a generation of curious, capable and creative computer users.
</p>

<p>
It was, in fact, so popular and successful that even long after Commodore's demise, there were drawn-out efforts to modernize the platform. Here we can glean another of the many nails in the Amiga's coffin: the Amiga fanatics themselves. Most were now identifying so deeply with their platform that, say, suggestions of porting the operating system we loved to cheap and plentiful X86 hardware was considered heresy. It had to be PowerPC or nothing, despite the failure of the <a href="https://en.wikipedia.org/wiki/BeBox">BeBox</a> - which in many ways was more of a modern Amiga than any of the officially sanctioned attempts.
</p>

<p>
This eventually resulted in the AmigaOne series of PowerPC machines launched in the early 2000's. Keen supporters of the platform can nowadays purchase a motherboard with a 2 GHz dual core CPU for a mind-boggling $2000 (Yikes!). Add to that the cost for graphics, sound, memory and everything else. Spending that kind of money will get you a computer mostly useful for finding out why you don't want to run outdated ports of Linux software on an operating system without memory protection.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/a1.png">
<br>
<i>Pay stupid prices, win stupid hardware.</i>
</p>

<p>
It's not without irony that the once cheap, integrated, cutting edge Amiga platform has now become a ridiculously expensive PC-style kit computer, running an OS kernel that seems almost as primitive today as MS-DOS once did when we Amiga zealots smugly bragged about multitasking.
</p>

<p>
A humbling journey, to say the least.
</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vision Pro Teardown – Why those fake eyes look so weird (284 pts)]]></title>
            <link>https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird</link>
            <guid>39246664</guid>
            <pubDate>Sun, 04 Feb 2024 01:37:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird">https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird</a>, See on <a href="https://news.ycombinator.com/item?id=39246664">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
<figure><p>
<iframe title="Vision Pro Teardown: Behind the Complex and Creepy Tech" width="456" height="257" src="https://www.youtube-nocookie.com/embed/JVJPAYwY8Us?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>The strangest thing about the Vision Pro is also the thing that makes it most uniquely Apple: it’s got a big shiny bubble glass front, which makes it stand out from the aluminum- and plastic-shrouded competition, even when it’s off. And when it’s on, it’s even stranger—instead of being fully transparent, behind the glass, an odd lenticular screen displays a 3D-ish video of the user’s eyes, emulating their gaze. Apple calls it the EyeSight display, and when the user is looking at you, it kind of, sort of, almost looks like you can see through smokey glass.</p>



<p>Tech journalists have called EyeSight “<a href="https://www.businessinsider.com/apple-vision-pro-eyesight-feature-fans-reaction-2024-1">bizarre</a>,” “<a href="https://www.theverge.com/24054862/apple-vision-pro-review-vr-ar-headset-features-price">uncanny</a>,” and “<a href="https://daringfireball.net/2024/01/the_vision_pro">of highly dubious utility</a>.” But from a repair perspective, it seems like an achilles heel. Why introduce another screen, more connectors, and <em>so many </em>more points of failure—all for the sake of a slightly creepy feature? Of course, we had to dig in and figure out how it works.&nbsp;</p>



<p>We knew it would be tough to get inside (it was). We hoped we wouldn’t break anything (we did). But we knew it would be worth it to see all the new technology Apple squeezed into this thing, from the EyeSight display to the sensor array, the external battery back to the R1 chip. We brought in the heavy hitters for this teardown, including x-ray views of the frame and high-resolution microscope shots of the displays.&nbsp;</p>



<p>We’ve got a lot of observations, some opinions, and a couple educated guesses about <em>why</em> we got the Vision Pro we have today on the teardown table. There is a lot in this device, so we’re splitting our analysis into two, with more detail on the lens system and silicon coming in a few days.</p>



<p>Let’s go spelunking into a never-before-explored cave of glass.</p>



<figure><img decoding="async" fetchpriority="high" width="2000" height="1395" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32.jpeg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32.jpeg 2000w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32-1536x1071.jpeg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32-1290x900.jpeg 1290w" sizes="(max-width: 2000px) 100vw, 2000px"></figure>



<figure><img decoding="async" width="7162" height="4029" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2.jpg 7162w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2-1536x864.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2-2048x1152.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2-1600x900.jpg 1600w" sizes="(max-width: 7162px) 100vw, 7162px"></figure>



<p>The glass panel is glued on, of course, and it took a <em>lot</em> of heat and time, but we removed it without breakage. Granted it didn’t come out unscathed—the glass has a protective plastic film that got a little peeled up and maybe a bit melted. Apple’s retail fixers <em>might</em> have faster hands than us—but they’ll <a href="https://support.apple.com/apple-vision-pro/repair">charge you $799</a> to replace broken front glass.&nbsp;</p>



<figure><img decoding="async" loading="lazy" width="7059" height="3971" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1.jpg 7059w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1-1536x864.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1-2048x1152.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1-1600x900.jpg 1600w" sizes="(max-width: 7059px) 100vw, 7059px"></figure>



<figure><img decoding="async" loading="lazy" width="2000" height="1500" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited.jpg 2000w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited-1536x1152.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited-1200x900.jpg 1200w" sizes="(max-width: 2000px) 100vw, 2000px"></figure>



<h3><a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"><strong></strong></a><strong>Heavy Metal</strong></h3>



<p>At 34 grams, the glass may not be heavy on its own, but fully kitted out with the battery the Vision Pro weighs over a kilogram.&nbsp;</p>



<p>Here’s where Apple has a performed a bit of a sleight of hand. Carefully hidden in most publicity shots is the external battery, which rides along in your pocket rather than on your headset. As in the early days of VR, integrating the battery as it is now would make the device crazy heavy.&nbsp;And hey, we’re big fans of modular batteries, when the battery inevitably stops holding a charge in <a href="https://batteryuniversity.com/article/bu-801b-how-to-define-battery-life">a year or three</a>, you can replace it painlessly. Apple’s hardware team may also be anticipating the <a href="https://repair.eu/news/big-win-for-right-to-repair-with-new-eu-rules-for-batteries-but-legislators-must-get-the-implementation-right/">upcoming EU battery regulation</a>, which will require all electronics to have user-replaceable batteries by 2027. </p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1536x1025.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-2048x1367.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1349x900.jpg 1349w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></figure>



<p>The battery pack alone weighs 353 grams and is made of three iPhone-sized batteries, delivering a grand total of 35.9 Wh, more than double an iPhone 15 Pro’s 17.3 Wh. The cells themselves are 184 g apiece, surprisingly only about half the weight of the full battery pack. To get inside, we had to soften some perimeter adhesive and release a set of single-use metal clips—then twist open Torx screws galore.<a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"></a></p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1536x1025.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1349x900.jpg 1349w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-450x300.jpg 450w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77.jpg 2000w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Three batteries in the aluminum pack, at ~3.8V each in series, 3166 mAh each, supplying 11.34 Volts in total.</figcaption></figure>



<p>Add the weight of the battery pack and the headset together and you get, as mentioned above, over a kilogram—which would be a really heavy pair of glasses. For comparison, the Quest Pro weighs 722 g and the Quest 3 clocks in at 515 g.</p>



<p>But weight isn’t just about how it tips the scales. It’s about balance. The weight of the Vision Pro largely rests on your face, all the tech is at the front and even the Pro Dual Loop Band can’t overcome it all without a counterbalance. <a href="https://www.patentlyapple.com/2023/11/apple-invents-a-battery-mount-on-the-back-of-vision-pros-headband-to-help-counterbalance-the-weight-of-the-hmd-that-causes-n.html">Apple patented a design</a> for a rear-mounted battery pack, which might’ve helped balance out the heavy front—though it’s hard to imagine wanting to wear something 150% as heavy.&nbsp;</p>



<p>So if we’re just counting the weight on your face—the display module, sans battery, in the Meta Quest Pro is 522 grams. The same assembly in the Vision Pro is 532 grams, effectively the same. The key difference in these units is in the weight distribution, and a much heavier pocket battery in the Vision Pro.</p>



<figure><img decoding="async" loading="lazy" width="2158" height="1319" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think.png" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think.png 2158w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think-1536x939.png 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think-2048x1252.png 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think-1472x900.png 1472w" sizes="(max-width: 2158px) 100vw, 2158px"></figure>



<p>First impressions, though, are pretty good. “The weight isn’t as bad as expected, although it’s definitely on my forehead/cheeks as opposed to my head which feels weird, like someone is pushing on my head to tilt it down,” says iFixit teardown veteran <a href="https://www.ifixit.com/User/524640/Sam+Goldheart">Sam Goldheart</a> from the teardown lab.</p>



<h3><strong>Headbands</strong></h3>



<p>The Vision Pro comes with both a 3D-knitted Solo Knit Band and a Dual Loop Band. These attach to the ends of the stems, just behind the speakers. The now-iconic Solo Knit Band is the one that seen in all the publicity shots, and it does look cool. It wraps around the back of your head, and you adjust the fit with a dial on the side, similar to how you might tighten a bike helmet.&nbsp;</p>



<p>So how does it feel? “The fabrics are sooo nice,” says Sam. There’s a very fine, cushy weave on the Solo Knit Band, and it is stretchy enough to accommodate a ponytail and still support the face unit.&nbsp;&nbsp;</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1.jpg"><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>The speakers are fixed onto the two rigid bands that join to the main headset. To release these, you use our old friend, the SIM-card removal tool. The holes are inside the temples of the main headset, and the removable bands have a row of electrical contacts, just like Lighting connectors, again. Easily removable parts? Only demands tools you’ve probably already got? We love to see it. This makes us hope that opening the headset might not be as daunting as we first assumed.<a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"></a></p>



<p>This modular design is similar to the <a href="https://www.ifixit.com/Teardown/AirPods+Max+Teardown/139369">AirPods Max</a>, which we quite liked. Wearables are so easy to damage that it makes good sense to have easily swappable speaker modules. We tried to go further and pry the speaker out of the silicon frame, and instantly broke the molded cables inside. That’s all right, you’re not going to need to pry the speaker modules open. </p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>The speakers—not quite as hard to get into as a pair of AirPods Pro, but almost.</figcaption></figure>



<p>The speakers themselves point back towards your ears. This is a pretty clear indication that you’re not meant to wear this anywhere noisy. You can wear your AirPods Pro if you prefer—and if you want lossless, low-latency audio, they’ll have to be the latest USB-C version.</p>



<p>On the left side is the proprietary battery cable connection, which snaps into place with a magnet and then twists to lock. We understand why Apple used a non-standard connector here, even if we don’t love it—at least it can’t be yanked out by a passing child, or when the cord inevitably catches on your chair. But the plug at the other end of the cable is unforgivable. Instead of terminating with a USB-C plug, it connects to the battery pack with what looks like a proprietary oversized Lightning connector, which you release using a paperclip or SIM-removal tool.</p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>The locking design is great, but why couldn’t it be USB-C Apple? WHY?</figcaption></figure>



<p>This connector means that you can’t just swap in the USB-C battery pack you already own. Lame.</p>


<div>
<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Aww, such a cute family snap!</figcaption></figure></div>


<h3><strong>Light Seals and Face Cushions</strong></h3>



<p>Every face is different, and Apple is selling<a href="https://www.reddit.com/r/VisionPro/comments/19ardw5/all_light_seal_sizes/">&nbsp;28 different light-seal parts</a> to cover all the different face sizes and shapes. Your seal size also changes if you need Zeiss lens inserts. That’s because the seals and cushions are also used to make sure you have the correct eye position relative to the stereo screens and eye sensors. This is why Apple is hand-packing every Vision Pro order—there’s just no “standard” setup. </p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Guess who was wearing makeup?</figcaption></figure>



<p>The seals attach to the main headset using magnets, which is Apple through-and-through—it’s either glued in place, or extremely easy to swap. This modularity is a brute force attempt to get an ideal fit on your face. It will be interesting to see if this is required long-term, or if future devices find a simpler way to accomplish this. For the time being, magnets are better than velcro because they can snap the seals into exact alignment. Think how MagSafe snatches the charger and lines it up perfectly over the iPhone’s inductive charging coil.</p>



<p>As for cleaning the seals, <a href="https://support.apple.com/en-us/HT213964">Apple recommends</a> water and unscented dish soap, which will help stop these sweat-soaking parts from getting too gross, and will be especially good for anyone wearing makeup. In her <a href="https://www.wsj.com/video/series/joanna-stern-personal-technology/vision-pro-review-24-hours-in-apples-mixed-reality-headset/05CD2E77-897D-49A9-A87E-9B8A93E3E45F">Wall Street Journal video</a> where she selflessly wore the headset for 24 hours, Joanna Stern said her makeup caked the inside of the seals. And our own Sam Goldheart had the exact same problem this morning.</p>



<p>Under the magnetic seals is a permanent seal, also wrapped in a knit fabric, but less likely to get smudged. It also happens to be the way into the interior of the headset. Removing it reveals another surprise: a thin stretchy sheet of plastic. Whether it’s to compensate for gaps in the knit, or to keep particulates out of the innner workings, we’re not to sure. But we are certain this bit looks <em>very</em> masked superhero.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim.jpg"><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<h3><strong>EyeSight Display</strong></h3>



<p>The front-facing gogglebox is the defining feature of the Vision Pro, and, now that reviews are pouring in, one of its most controversial.</p>



<p>The <a href="https://patentimages.storage.googleapis.com/51/03/12/4b7f034c90465c/US20240012601A1.pdf">patent for the EyeSight</a> describes three display modes: “internal focus,” “external engagement,” and “do not disturb.” The patent has pages and pages of images that might be displayed on the screen—all kinds of cartoon animal eyes, biometric analysis captured by other sensors, hearts when the user is talking to a loved one. The internal camera might read emotional states and project images based on those emotional states.</p>



<figure><img decoding="async" loading="lazy" width="1906" height="766" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03085115/pasted-image-0-1.jpeg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03085115/pasted-image-0-1.jpeg 1906w, https://valkyrie.cdn.ifixit.com/media/2024/02/03085115/pasted-image-0-1-1536x617.jpeg 1536w" sizes="(max-width: 1906px) 100vw, 1906px"></figure>



<p>Cool thought. In practice, the EyeSight display is so dim and low-resolution that reviewers say it’s hard to see much on it. The WSJ’s Joanna Stern <a href="https://www.youtube.com/watch?v=8xI10SFgzQ8&amp;t=211">called it</a> “hard to see,” and Marques Brownlee (aka MKBHD)<a href="https://www.youtube.com/watch?v=dtp6b76pMak&amp;t=1826"> said</a>, “You can barely see my eyes when I’m wearing the headset.”&nbsp;</p>



<p>It turns out that when the EyeSight displays your eyes, it isn’t just displaying a single video feed of your eyes; it’s showing a <em>bunch</em> of videos of your eyes. Exploring inside the glass shell, we found three layers for the front-facing display: a widening layer, a lenticular layer, and the OLED display itself.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103.jpg"><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p><a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"></a><strong>Why Does EyeSight Look So Wonky?</strong></p>



<figure><img decoding="async" loading="lazy" width="3514" height="960" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1.jpg 3514w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1-1536x420.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1-2048x559.jpg 2048w" sizes="(max-width: 3514px) 100vw, 3514px"></figure>



<p>Apple wanted to achieve something very specific: an animated, 3D-looking face with eyes. They had to make very strategic design choices and compromises to accomplish this.</p>



<p>Human brains are very sensitive to faces and expressions, its why the uncanny valley is a thing, and part of that is depth sensing. Apple needed to create a believable 3D effect. One reason why 3D renderings don’t look truly 3D is because they lack a stereoscopic effect. For something to look 3D, we need to see subtly different images with each eye. The Vision Pro tackles this problem with lenticular lenses.</p>



<figure><video controls="" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03035750/card-movement.mp4"></video></figure>



<p>A lenticular lens displays different images when viewed from different angles. You can use this effect to simulate movement with two frames of an action. Or, you can create a stereoscopic 3D effect with images of the same subject from different angles.</p>



<p>The Vision Pro has a lenticular layer on top of the exterior OLED panel. VisionOS renders multiple face images—call them A and B—slices them up, and displays A from one angle serving your left eye, and  B from another serving your right eye. This creates a 3D face via the stereoscopic effect. And those angles are tiny, and they are legion, it takes a fancy <a href="https://www.evidentscientific.com/en/">Evident Scientific microscope</a> to really see what we mean.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017.jpg"><img decoding="async" loading="lazy" width="2000" height="1106" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017.jpg 2000w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017-1536x849.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017-1627x900.jpg 1627w" sizes="(max-width: 2000px) 100vw, 2000px"></a><figcaption>The curved ridges of the lenticular lens layer.</figcaption></figure>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter.jpg"><img decoding="async" loading="lazy" width="1880" height="1040" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter.jpg 1880w, https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter-1536x850.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter-1627x900.jpg 1627w" sizes="(max-width: 1880px) 100vw, 1880px"></a><figcaption>Pixels bending and shining through the lenticular layer</figcaption></figure>



<p>There are compromises to this approach. The horizontal resolution is dramatically reduced, being divided between each of the multiple images. For example, if two images are displayed on a 2000 pixel wide display, each image only has 1000 horizontal pixels to work with. Even through we don’t know the resolution of the display, nor do we know the number of images being interwoven, the resolution is necessarily reduced. And that is a major reason why EyeSight eyes seem blurry.</p>



<p>In front of the lenticular layer is another plastic lens layer, with similarly lenticular ridges. This layer appears to stretch the projected face wide enough to fit the width of the Vision Pro. Removing this layer and booting the Pro showcases some very oddly pinched eyes.</p>



<figure><video controls="" poster="https://valkyrie.cdn.ifixit.com/media/2024/02/03044324/dsc_7495.mov.00_00_27_13.still002_720.jpg" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03103510/Display-Stack_sm.mp4"></video></figure>



<p>Additionally the lens likely limits the effective viewing angle. Limiting the effect to directly in front of the Vision Pro limits artifacting you might see at extreme angles, sort of like a privacy filter. The downside is that you’re passing an already complex, blurry image through yet another layer of lens. This makes it even blurrier and darker.</p>



<h3><strong>Lens Inserts, Stereo Displays</strong></h3>



<p>You can see the outline of the ovoid lens inserts in this x-rays from our illuminous friends at <a href="https://creativeelectron.com/">Creative Electron</a>, who spent $3,500 so you could see this photo.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray.jpg"><img decoding="async" loading="lazy" width="3546" height="1992" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray.jpg 3546w, https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray-1536x863.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray-2048x1150.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray-1602x900.jpg 1602w" sizes="(max-width: 3546px) 100vw, 3546px"></a></figure>



<p>The Vision Pro itself performs an <a href="https://support.apple.com/guide/apple-vision-pro/important-safety-information-c0c84db82a44/visionos">automatic interpupillary distance adjustment</a> when you first put it on, with motors adjusting the positioning of the lenses. For everything else there’s prescription lenses. </p>



<p>Apple Stores have a machine to determine approximate prescription glasses strength when you come in for a demo.  For users with eye conditions (like strabismus) that might interfere with eye tracking, the Vision Pro offers <a href="https://support.apple.com/en-us/HT213965">alternative interaction controls</a> in the accessibility features. However, we have heard that lenses are not available for people who have astigmatism, which is <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10045990/">40% of the population</a>. If you know anything more about that, leave it in the comments.&nbsp;</p>



<p>The prescription insert lenses themselves require “pairing” with the headset. The decision has already borne poor UI, John Gruber received an <a href="https://daringfireball.net/2024/01/the_vision_pro">incorrect calibration code</a> with his review unit that made eye tracking perform poorly. <a href="https://www.ifixit.com/News/82867/iphone-15-teardown-reveals-software-lockdown">We hate parts pairing</a> on principle, and there’s got to be a way to enable calibration while still allowing third party lenses.</p>



<p>Oh, and Creative Electron was bored after one photo so they shot us a 360 spin. Sweet!</p>



<figure><video autoplay="" controls="" loop="" preload="auto" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03130232/360-color-4.mp4"></video></figure>



<h3><strong>R1 and M2 Chips</strong></h3>



<p>The headset runs on an M2 Mac chip, in tandem with the new R1 chip—which is specifically responsible for processing the input from 12 cameras, the LiDAR sensor, and the TrueDepth camera, all with a minimum of latency. With AR, you need to project the camera view of the real world into the user’s eyes as fast as possible, otherwise their perceived motions won’t match up with what they see, which is a fast ticket to Vomitsville.&nbsp;</p>



<p>To keep up, the R1 <a href="https://www.theverge.com/2023/6/5/23733874/apple-vision-pro-visionos-augmented-reality-os-specs-wwdc-2023">uses</a> a <a href="https://en.wikipedia.org/wiki/Real-time_operating_system">real-time operating system</a>. That means that tasks are always executed in a fixed amount of time. Most of our computers run on a time-sharing operating system, which schedules tasks on the fly, and can result in slowdowns. Think about jittery mouse cursors, or spinning beach balls, and you’ve got the idea. That won’t fly with something as critical as pass-through video and object rendering. Any glitch there would be like a glitch in the Matrix, and would be jarring at best, and utterly nauseating at worst. It might even cause you to stumble and fall.</p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></figure>



<h3>An Incredible Feat, With One Really Weird Design Decision</h3>



<p>The original iPhone did something similar. When its underpowered chips couldn’t keep up with rendering a fast-scrolling page, it would <a href="https://daringfireball.net/2008/10/iphone_3g">switch to a gray-and-white checkerboard</a>, which kept up with all your flicks and swipes. Apple prioritized responsiveness over graphical fidelity. This time around, they have prioritized graphics fidelity <em>and</em> responsiveness, and taken the hit on battery life, weight, and heat. Given how important the experience is to Apple’s AR experience, this is probably the right choice for a first generation device.</p>



<p>The Vision Pro is insanely ambitious. Yes, it’s heavy, and the glass is fragile, and that tethered battery might get annoying. But Apple has managed to pack the power of a Mac, plus the performance of a new dedicated AR chip, into a computer that you can wear on your face.</p>



<p>Repairability-wise, it’s not great, but on the plus side, some of the connections are quite delightful. You should have seen our teardown team jump up when they realized that the side arms could be popped out using the SIM-removal tool, for example, and the magnetic cushions are yet more user-friendly.</p>



<p>So why, when this thing clearly took years and years to create—and is Apple’s latest bet on the future of computing—did Apple fail to live up to their own standards with the EyeSight screen? </p>



<p>It’s dim, it’s low-resolution, and it adds a lot of bulk, weight, complexity, and expense to the most weight-sensitive part of the headset. Did they finally hit the drop dead date and miss their targeted performance? Could it be a late-stage manufacturing error? Regardless, we’re sure bringing it to market was a difficult decision.</p>



<p>We’ve been disassembling VR headsets since the original <a href="https://www.ifixit.com/Device/Oculus_Rift">Oculus</a>, and they continue to surprise and delight. There is so much fascinating mechanical and optical design packed in here. Apple’s seamless integration of sensors for rock-solid location tracking is just phenomenal, and we’re eager to dive into how they did it.</p>



<p>We’re not done with our analysis: there’s lots more to investigate inside this device. Next time, we’ll dive into the internal displays, sensor arrays and we’ll award a repairability score. </p>



<p>What else are you excited to see? IPD calibration motors, cooling, specific chips or circuitry? Follow along on social media, or check back here in a few shakes, we’ve got plenty more coming.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1.jpg"><img decoding="async" loading="lazy" width="7360" height="4136" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1.jpg 7360w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1-1536x863.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1-2048x1151.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1-1602x900.jpg 1602w" sizes="(max-width: 7360px) 100vw, 7360px"></a></figure>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nuclear power saved Armenia (118 pts)]]></title>
            <link>https://thebulletin.org/2024/01/how-nuclear-power-saved-armenia/</link>
            <guid>39246563</guid>
            <pubDate>Sun, 04 Feb 2024 01:20:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thebulletin.org/2024/01/how-nuclear-power-saved-armenia/">https://thebulletin.org/2024/01/how-nuclear-power-saved-armenia/</a>, See on <a href="https://news.ycombinator.com/item?id=39246563">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span><picture title="" decoding="async" loading="lazy">
<source type="image/webp" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1024x768.jpg.webp 1024w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-300x225.jpg.webp 300w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-768x576.jpg.webp 768w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1536x1152.jpg.webp 1536w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-493x370.jpg.webp 493w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image.jpg.webp 1920w" sizes="(max-width: 1024px) 100vw, 1024px">
<img width="1024" height="768" src="https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1024x768.jpg" alt="Armenia's Metsamor nuclear power plant cooling towers. (Credit: Adam Jones via Wikimedia Commons. CC BY-SA 2.0)" decoding="async" loading="lazy" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1024x768.jpg 1024w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-300x225.jpg 300w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-768x576.jpg 768w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1536x1152.jpg 1536w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-493x370.jpg 493w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px">
</picture>
Armenia's Metsamor nuclear power plant cooling towers. (Credit: Adam Jones via Wikimedia Commons. CC BY-SA 2.0)</span></p><div><p>The world is currently in the process of reevaluating its past rejection of nuclear power and is increasingly starting to view it as a reliable source of power that allows for greater energy security. This is at least in part due to the energy crisis that befell Europe after the Russian invasion of Ukraine in 2022, vindicating past worries that over-reliance on fossil fuels from autocratic regimes has made the Western countries vulnerable to political blackmail.</p>
<p>It is now clear that Western use of natural gas and petroleum from aggressive dictatorships—which use cash flows from oil and gas sales to reinforce and expand their hold on power—has backfired badly. In this context, the experience of Armenia—a small country that draws 40 percent of its energy from nuclear power—is instructive, showing how nuclear power can be instrumental in building societal reliance and political stability.</p>
<p><strong>Living in the dark cold.</strong> It is the winter of 1992–1993. As I climb the dark stairs in a freezing-cold Soviet apartment building in Yerevan, the capital of Armenia where my family and I live, the water from the two full buckets I carry is splashing down my legs and freezing on the stairs. My sister Shooshan and I, 14 and 15, are carrying water up to our 11<sup>th</sup>-floor apartment. The water to our apartment shut off weeks ago, and we get at most one hour of electricity each day. I estimate that we need exactly seven gallons of water, if we are careful, for our basic daily needs. So, we repeat the trip every day. During the precious hour when we do get electricity, my mother rushes to the kitchen to cook food for the next 24 hours. I run to the bakery, where I stand in a long queue to buy the half pound of bread that the state has rationed for each one of us.</p>
<p>The daily routine, which goes on for the whole winter, is exhausting. But it is also empowering. As teenagers we feel that we are stronger than the disastrous conditions inflicted on us by the combination of the Soviet collapse, the Nagorno-Karabakh war, and the ensuing severe energy crisis.</p><div id="thebu-1161222088"><p><a href="https://thebulletin.org/doomsday-clock/?utm_source=Website&amp;utm_medium=MobileMediumRectangle&amp;utm_campaign=2024DDCAnnouncementWebAds&amp;utm_content=DoomsdayClock_WebsiteAd_01222023#nav_menu" target="_blank" aria-label="2024 Doomsday Clock – Mobile Medium Rectangle"><img loading="lazy" decoding="async" src="https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1.png" alt="" srcset="https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1.png 600w, https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1-300x250.png 300w, https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1-444x370.png 444w" sizes="(max-width: 600px) 100vw, 600px" width="300" height="250"></a></p></div>
<p>The reasons that my sister and I—and the thousands of other Armenian teenagers like us—had to lug water and plan their lives around the one hour of electricity during that cruel winter go back to the turbulent events that shook Armenia during the preceding decades.</p>
<figure id="attachment_83863" aria-describedby="caption-attachment-83863"><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image-279x300.jpg.webp 279w, https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image.jpg.webp 326w" sizes="(max-width: 281px) 100vw, 281px">
<img loading="lazy" decoding="async" src="https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image-279x300.jpg" alt="Map of Armenia. Credit: The World Factbook 2021. Central Intelligence Agency." width="281" height="302" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image-279x300.jpg 279w, https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image.jpg 326w" sizes="(max-width: 281px) 100vw, 281px">
</picture>
<figcaption id="caption-attachment-83863">Map of Armenia. Credit: The World Factbook 2021. Central Intelligence Agency.</figcaption></figure>
<p>In the 1970s and 1980s, the Soviet Union—which Armenia was part of—rapidly expanded its fleet of nuclear reactors to support its growing industrial energy needs. As a result, two pressurized water reactors (PWR) of the Soviet VVER-440 type were built in the Armenian town of Metsamor, about 30 kilometers west of Yerevan. Started in 1977 and 1980, respectively, the two reactors quickly covered more than half of the energy needs of the Armenian Soviet Socialist Republic. (The remainder of the electricity was generated by Armenia’s hydroelectric stations and gas-fired power plants.) The Armenia of the 1980s was a tiny but prosperous Soviet republic that prided itself in a highly educated labor force, an array of scientific institutes, and a vibrant electronics industry that produced some of the early Soviet computer mainframe designs.</p>
<p>A series of violent events during the collapse of the Soviet Union would dramatically alter the Armenian dream.</p>
<p><strong>Chernobyl. </strong>On April 26, 1986, one of the Soviet-designed, graphite-moderated RBMK reactors at the Chernobyl nuclear power plant underwent a catastrophic power excursion that ripped the reactor open. The explosion and fire that followed propelled an enormous amount of radioactive matter into the open atmosphere leading to what is now known as the Chernobyl nuclear disaster, with widespread radioactive contamination, hundreds of deaths from acute radiation poisoning, and likely thousands of additional deaths due to radiation-induced cancers in the months and years that followed.</p>
<p>The Chernobyl accident resonated worldwide, dramatically undermining public trust in nuclear power as a safe source of energy. The public perception of danger from nuclear power was magnified by the outrageous lies that the Soviet leadership spread about the disaster, the obvious incompetence and irresponsibility of the Soviet nuclear designers who built and operated the Chernobyl reactor, and the poorly executed cleanup efforts which were compounded by miscalculations and gross mistakes.</p>
<p>Overnight, citizens across the Soviet Union and beyond went from a blissful ignorance about radiation to an understandable—yet irrational—fear of anything radiation-related. People in Armenia, despite living more than 2,000 kilometers away from Chernobyl, started perceiving radioactive threats everywhere, often attributing many of their common ailments to radiation. Physicists, like my parents, tried to explain what radiation is and how natural doses of radiation are not dangerous. But their advice was sometimes met with hostility: Weren’t the builders of Chernobyl also scientists?</p>
<p>In one chilling conversation that I witnessed at a dinner party, one of the guests told my father only half in jest, “You physicists… you should all be shot!” To paraphrase Valery Legasov’s eponymous character from HBO’s <a href="https://www.nytimes.com/2019/05/03/arts/television/review-chernobyl-hbo.html">five-part mini-series “Chernobyl”</a>: The danger of the lies is not that we mistake them for the truth, but that when enough lies are told we lose hope in the truth and start believing in stories. (Legasov was a Soviet chemist who actively worked on the causes and consequences of the Chernobyl disaster. <a href="https://vtoraya-literatura.com/pdf/radio_liberty_report_on_the_ussr_vol01_14_1989__ocr.pdf">Concerned</a> by the lack of nuclear safety in the Soviet nuclear industry, he died by suicide on April 27, 1988.)</p>
<p><strong>An earthquake, the Soviet collapse, and war.</strong> In December 1988, the devastating earthquake of Spitak killed 50,000 people—a harrowing 2 percent of Armenia’s population—and destroyed most of the country’s infrastructure. The two VVER-440 reactors at Metsamor were suddenly in the public eye. Would another earthquake rip them open and turn Armenia’s heartland, where half of Armenia’s population lived, into a Chernobyl-like radioactive wasteland?</p>
<p>To be clear, the PWRs at Metsamor are safer than the shoddily designed, graphite-moderated reactors at Chernobyl. Metsamor’s Soviet reactor design is close to the standard PWR designs that are still the most common reactor technology used in Western countries. And the buildings and the reactor structures were reinforced to account for Armenia’s seismic activity. But none of that mattered. After the Soviet government’s grotesque lies about the Chernobyl disaster, the official assurances that the Metsamor reactors were safe did not convince many. Legasov’s intuition was right: The pursuit for truth was replaced with belief in conspiratorial rumors. An environmentalist movement sprang up, calling for the shutdown of the Metsamor reactors. The authorities backed down, and the two reactors were turned off on February 25 and March 18, 1989.</p>
<p>Shortly after the shutdown, the Soviet Union started to crack, finally collapsing in 1991. In neighboring Azerbaijan, an Armenian minority living in the mountainous Nagorno-Karabakh region, feeling marginalized and discriminated against, had long been fighting to protect their civil rights. With the weakening of Soviet power, the protest movement turned into demands for secession from Azerbaijan. The response in Azerbaijan was <a href="https://armenian.usc.edu/baku-pogroms-in-context-of-the-karabakh-conflict/">a series of brutal anti-Armenian pogroms</a> in the cities of Sumgait and Baku that killed hundreds of Armenian civilians and forced about 300,000 others to flee the country. Fearing retaliation, the Azeri civilians living in Armenia fled en masse to Azerbaijan.</p>
<p>A relatively peaceful political disagreement had suddenly turned into a violent conflict, with Azerbaijan’s pogroms against Armenians escalating to a total war against the Armenian people of Nagorno-Karabakh. As the Armenian government supported the Nagorno-Karabakh secessionists, Azerbaijan retaliated by shutting off some of the natural gas pipelines that led to Armenia. In a sense, Azerbaijan’s authorities did to Armenia what Russian President Vladimir Putin is now doing to Western European countries that support Ukraine’s war effort. With its nuclear reactors and natural gas supply shut down, Armenia was left with a reduced capacity to generate electricity.</p>
<p>Then came the winter of 1992–1993. Mountain rivers froze, hydroelectric dams dried up, and suddenly hydropower too was nearly gone. Armenia was getting barely a trickle of electricity. What followed is a period now known in Armenia as “tsurt u mut tariner,” literally the cold and dark years: severe shortages of electricity, freezing concrete apartment complexes, closed schools, and many other disruptions. The economy collapsed, with Armenia’s gross domestic product contracting by an estimated 50 to 80 percent between 1990 and 1993. Then, a massive exodus followed, shrinking Armenia’s population by a quarter in just a few years.</p>
<p><strong>Nuclear power revival.</strong> The Armenian public quickly realized that, by abandoning nuclear power, it had forfeited the country’s energy independence. That vulnerability was—and still is—very effectively leveraged by its arch-enemy Azerbaijan. Was it too late to restore nuclear power?</p>
<p>Understanding their mistake, the Armenian authorities re-evaluated their past decision. The choice was stark: Either indulge in exaggerated fears of radiation and face unpredictable consequences, or sober up and accept nuclear power as a lesser evil. Ultimately the government chose the sober option. But rather than rushing headfirst to hastily restart the Metsamor nuclear power plant, the authorities decided to make significant safety improvements to the reactors.</p>
<p>One of the Metsamor reactors finally restarted on November 5, 1995, just before the winter season. The desperately needed 400 megawatts flowed again into the small country’s languishing power grid. Almost overnight, lights were turned on, water pumps worked again, and industries revved up to capacity. Children like my sister and I stopped their exhausting routine and Armenia became a net exporter of electricity.</p>
<p>Over the 13 years that followed, Armenia’s economy grew by an unprecedented 700 percent. The difficult decision to restore nuclear power had saved Armenia and had put it on a path of development. In 2020, about 35 percent of <a href="https://www.eia.gov/international/data/country/ARM/electricity/electricity-generation?pd=2&amp;p=00000000000000000000000000000fvu&amp;u=0&amp;f=A&amp;v=mapbubble&amp;a=-&amp;i=none&amp;vo=value&amp;t=C&amp;g=none&amp;l=249--7&amp;s=315532800000&amp;e=1609459200000&amp;">electricity generated in Armenia</a> came from nuclear, 25 percent came from renewables (primarily hydropower), and the remaining 40 percent from fossil fuels. (In 2021, the share of nuclear power temporarily dropped to 26 percent because the Metsamor reactor was shut down longer than usual to perform a thermal annealing of the pressure vessel, a maintenance method aimed at managing aging effects.)</p>
<p>Despite its important contribution to the electricity mix, the nuclear power plant at Metsamor is not without problems. Mainly, like most Soviet-era PWRs, the reactor does not have the external containment building that is common with Western designs. It is also an aging machine. Because of Armenia’s growing energy needs, the Metsamor reactor has been issued <a href="https://world-nuclear.org/information-library/country-profiles/countries-a-f/armenia.aspx">multiple lifetime extensions</a>. Based on current plans, Metsamor’s VVER-440 reactor will shut down permanently by 2036. Meanwhile the Armenian government has been busy exploring replacement alternatives, such as possibly <a href="https://en.armradio.am/2023/05/24/armenian-in-talks-with-several-partners-to-build-a-new-power-plant-pm-pashinyan-comments-on-us-plans-to-build-modular-nuclear-reactors/">US-built small modular reactors</a> (SMRs), seen as a viable replacement. Armenian officials have also entered in <a href="https://www.neimagazine.com/news/newsarmenia-considers-nuclear-options-10933703">discussions with Russia</a> about the possibility of replacing the Soviet-era VVER-440 reactor with the much larger and more modern Russian VVER-1200 design. While the US option is not easy—mainly because of the lack of readiness of most SMR designs—the Russian option is particularly fraught. Armenia is reluctant to further increase its energy dependence on Russia, given Putin’s campaign of neo-Soviet expansionism. This is further exacerbated by the technical and economic difficulty of hosting a 1200-megawatt electric VVER-1200 unit on a grid that on average consumes only about 1,000 megawatts.</p>
<p><strong>Survival in the shadow of petro-dictatorship.</strong> In recent years, social scientists have studied the negative impacts of nuclear power on underprivileged communities, such as the effects of uranium mining on indigenous populations. These studies are important for understanding the social cost of this resource. However very rarely have scholars studied the positive impact that nuclear power has had in helping the victims of oppression.</p>
<p>Most of the three million inhabitants in Armenia trace their lineage to the survivors of the Armenian Genocide of 1915. Most live near the border with the perpetrator state of Turkey, which to this day refuses to acknowledge its crime and in the recent past has actively helped Azerbaijan. Since 1993, Azerbaijan has been ruled by the Aliyev dynasty with an iron fist, strengthened by the cash flows from the export of the country’s large hydrocarbon reserves to Western countries. To further strengthen his hold on power, Azerbaijani President Ilham Aliyev (son of Heydar Aliyev who held power in Azerbaijan for several decades) has tapped into Azerbaijanis’ trauma from the 1990s by demonizing Armenians and blaming all of Azerbaijan’s ill on this minority.</p>
<p>Since he took power in 2003, the regime of Aliyev son has been accused of <a href="https://www.theguardian.com/commentisfree/2023/oct/09/azerbaijani-ethnic-cleansing-armenians-nagorno-karabakh-children">curtailing free speech and ethnic cleansing</a> of Armenians, whereas Azerbaijan’s armed forces have been busy mounting a campaign of <a href="https://news.cornell.edu/stories/2022/09/report-shows-near-total-erasure-armenian-heritage-sites">widespread cultural erasure</a>. These decades of threats culminated in last September with a <a href="https://www.economist.com/leaders/2023/09/28/a-humanitarian-disaster-is-under-way-in-nagorno-karabakh">swift military attack</a> on the Nagorno-Karabakh region, which in just one week brought the 3,000-year-old indigenous Armenian presence there effectively to an end. The situation currently is so severe that Luis Moreno Ocampo, a former chief prosecutor of the International Criminal Court, <a href="https://amp.cnn.com/cnn/2023/08/11/asia/nagorno-karabakh-armenians-genocide-intl-hnk/index.html">has warned</a> that a new genocide may be underway.</p>
<p>Armenians, whose newly budding democracy is under constant threat from the various authoritarian governments in the region, cannot achieve cultural and existential security if they do not have a state that ensures their security. And that includes energy security, to which nuclear power generation is key. Of course, Azerbaijan deserves to have a democratic government, too, something that is being hindered by the Western countries’ over-reliance on fossil fuel exports.</p>
<p><strong>The lessons of small nations. </strong>When it comes to understanding the value of nuclear energy, studies tend to focus on the big nuclear powers such as the United States, China, and Russia. They rarely study the experiences of small countries like Armenia. Still, the study of these “insignificant” players is important in terms of understanding the mistakes made, successes achieved, and lessons learned, which can be relevant for the “big” players as well. In a telling example, Germany is learning the hard way about the dangers of complacency when it comes to choosing between nuclear energy and fossil fuels for its energy mix: Over the last 20 years, German politicians preferred to shut down their “scary”—but nonetheless safe—nuclear power plants and increase their potentially destabilizing—but considered harmless—reliance on Russia’s natural gas. Had German policymakers studied Armenia’s experience of the 1990s, they could probably have avoided the energy crisis the country is currently experiencing.</p>
<p>Sadly, it’s hard to tell whether European leaders have learned anything from Armenia’s struggle for energy security. In a now much-criticized statement from 2022, European Commission President Ursula von der Leyen called Azerbaijan’s dictator Ilham Aliyev “<a href="https://ec.europa.eu/commission/presscorner/detail/da/statement_22_4583">a reliable partner</a>.” This gesture is now believed to have, at least partly, emboldened the Aliyev regime’s brutality toward the Armenian population of Nagorno-Karabakh. At least for now, it is as if Europe is merely switching dictators while maintaining the same dependence on fossil fuels.</p>
<p>Only a full reckoning by Western countries of their over-reliance on fossil fuels can put an end to the authoritarian regimes that exist only because of their hydrocarbon exports. Such a reckoning, along with the development of renewable energy and nuclear power, would lead to net gains for the climate and the environment. It would also help strengthen liberal democracies that are being unprecedently threatened.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The DIY Diggers Who Can't Stop Making 'Hobby Tunnels' (229 pts)]]></title>
            <link>https://www.bloomberg.com/news/features/2024-02-03/dig-if-you-will-the-underground-world-of-hobby-tunneling</link>
            <guid>39245893</guid>
            <pubDate>Sat, 03 Feb 2024 23:39:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/features/2024-02-03/dig-if-you-will-the-underground-world-of-hobby-tunneling">https://www.bloomberg.com/news/features/2024-02-03/dig-if-you-will-the-underground-world-of-hobby-tunneling</a>, See on <a href="https://news.ycombinator.com/item?id=39245893">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TCC RISC-V Compiler Runs in the Web Browser (Thanks to Zig Compiler) (161 pts)]]></title>
            <link>https://lupyuen.codeberg.page/articles/tcc.html</link>
            <guid>39245664</guid>
            <pubDate>Sat, 03 Feb 2024 23:03:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lupyuen.codeberg.page/articles/tcc.html">https://lupyuen.codeberg.page/articles/tcc.html</a>, See on <a href="https://news.ycombinator.com/item?id=39245664">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

        <!-- Begin scripts/rustdoc-before.html: Pre-HTML for Custom Markdown files processed by rustdoc, like chip8.md -->

    <!-- Begin Theme Picker -->
    
    
    
    <!-- Theme Picker -->

    <!-- End scripts/rustdoc-before.html -->
    

    
    <nav id="TOC"><ul>
<li><a href="#tcc-in-the-web-browser">1 TCC in the Web Browser</a><ul></ul></li>
<li><a href="#zig-compiles-tcc-to-webassembly">2 Zig compiles TCC to WebAssembly</a><ul></ul></li>
<li><a href="#posix-for-webassembly">3 POSIX for WebAssembly</a><ul></ul></li>
<li><a href="#file-input-and-output">4 File Input and Output</a><ul></ul></li>
<li><a href="#fearsome-fprintf-and-friends">5 Fearsome fprintf and Friends</a><ul></ul></li>
<li><a href="#test-with-apache-nuttx-rtos">6 Test with Apache NuttX RTOS</a><ul></ul></li>
<li><a href="#hello-nuttx">7 Hello NuttX!</a><ul></ul></li>
<li><a href="#whats-next">8 What’s Next</a><ul></ul></li>
<li><a href="#appendix-compile-tcc-with-zig">9 Appendix: Compile TCC with Zig</a><ul></ul></li>
<li><a href="#appendix-javascript-calls-tcc">10 Appendix: JavaScript calls TCC</a><ul></ul></li>
<li><a href="#appendix-pattern-matching">11 Appendix: Pattern Matching</a><ul></ul></li>
<li><a href="#appendix-nuttx-system-call">12 Appendix: NuttX System Call</a><ul></ul></li>
<li><a href="#appendix-build-nuttx-for-qemu">13 Appendix: Build NuttX for QEMU</a><ul></ul></li>
<li><a href="#appendix-missing-functions">14 Appendix: Missing Functions</a><ul></ul></li></ul></nav><p>📝 <em>4 Feb 2024</em></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-title.png" alt="TCC RISC-V Compiler runs in the Web Browser (thanks to Zig Compiler)"></p>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/">(Try the <strong>Online Demo</strong>)</a></p>
<p><a href="https://youtu.be/DJMDYq52Iv8">(Watch the <strong>Demo on YouTube</strong>)</a></p>
<p><em>TCC is a Tiny C Compiler for 64-bit RISC-V (and other platforms)…</em></p>
<p><em>Can we run TCC Compiler in a Web Browser?</em></p>
<p>Let’s do it! We’ll compile <a href="https://github.com/sellicott/tcc-riscv32"><strong>TCC (Tiny C Compiler)</strong></a> from C to WebAssembly with <a href="https://ziglang.org/"><strong>Zig Compiler</strong></a>.</p>
<p>In this article, we talk about the tricky bits of our <strong>TCC ported to WebAssembly</strong>…</p>
<ul>
<li>
<p>We compiled <strong>TCC to WebAssembly</strong> with one tiny fix</p>
</li>
<li>
<p>But we hit some <strong>Missing POSIX Functions</strong></p>
</li>
<li>
<p>So we built minimal <strong>File Input and Output</strong> </p>
</li>
<li>
<p>Hacked up a simple workaround for <strong>fprintf and friends</strong></p>
</li>
<li>
<p>And TCC produces a <strong>RISC-V Binary</strong> that runs OK</p>
<p>(After some fiddling and meddling in RISC-V Assembly)</p>
</li>
</ul>
<p><em>Why are we doing this?</em></p>
<p>Today we’re running <a href="https://lupyuen.codeberg.page/articles/tinyemu2"><strong>Apache NuttX RTOS</strong></a> inside a Web Browser, with WebAssembly + Emscripten + 64-bit RISC-V.</p>
<p>(<strong>Real-Time Operating System</strong> in a Web Browser on a General-Purpose Operating System!)</p>
<p>What if we could <strong>Build and Test NuttX Apps</strong> in the Web Browser…</p>
<ol>
<li>
<p>We type a <strong>C Program</strong> into our Web Browser (pic below)</p>
</li>
<li>
<p>Compile it into an <strong>ELF Executable</strong> with TCC</p>
</li>
<li>
<p>Copy the ELF Executable to the <strong>NuttX Filesystem</strong></p>
</li>
<li>
<p>And <strong>NuttX Emulator</strong> runs our ELF Executable inside the Web Browser</p>
</li>
</ol>
<p>Learning NuttX becomes so cool! This is how we made it happen…</p>
<p><a href="https://youtu.be/DJMDYq52Iv8">(Watch the <strong>Demo on YouTube</strong>)</a></p>
<p><a href="https://research.cs.queensu.ca/home/cordy/pub/downloads/tplus/Turing_Plus_Report.pdf">(Not to be confused with <strong>TTC Compiler</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-web.png" alt="Online Demo of TCC Compiler in WebAssembly"></p>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><em>Online Demo of TCC Compiler in WebAssembly</em></a></p>
<h2 id="tcc-in-the-web-browser"><a href="#tcc-in-the-web-browser">1 TCC in the Web Browser</a></h2>
<p>Click this link to try <strong>TCC Compiler in our Web Browser</strong> (pic above)</p>
<ul>
<li>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><strong>TCC RISC-V Compiler in WebAssembly</strong></a></p>
<p><a href="https://youtu.be/DJMDYq52Iv8">(Watch the <strong>Demo on YouTube</strong>)</a></p>
</li>
</ul>
<p>This <strong>C Program</strong> appears…</p>
<div><pre><code>// Demo Program for TCC Compiler
int main(int argc, char *argv[]) {
  printf("Hello, World!!\n");
  return 0;
}
</code></pre></div>
<p>Click the “<strong>Compile</strong>” button. Our Web Browser calls TCC to compile the above program…</p>
<div><pre><code>## Compile to RISC-V ELF
tcc -c hello.c
</code></pre></div>
<p>And it downloads the compiled <a href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format"><strong>RISC-V ELF <code>a.out</code></strong></a>. We inspect the Compiled Output…</p>
<div><pre><code>## Dump the RISC-V Disassembly
## of TCC Output
$ riscv64-unknown-elf-objdump \
    --syms --source --reloc --demangle \
    --line-numbers --wide  --debugging \
    a.out

main():
   ## Prepare the Stack
   0: fe010113  addi   sp, sp, -32
   4: 00113c23  sd     ra, 24(sp)
   8: 00813823  sd     s0, 16(sp)
   c: 02010413  addi   s0, sp, 32
  10: 00000013  nop

   ## Load to Register A0: "Hello World"
  14: fea43423  sd     a0, -24(s0)
  18: feb43023  sd     a1, -32(s0)
  1c: 00000517  auipc  a0, 0x0
  1c: R_RISCV_PCREL_HI20 L.0
  20: 00050513  mv     a0, a0
  20: R_RISCV_PCREL_LO12_I .text

   ## Call printf()
  24: 00000097  auipc  ra, 0x0
  24: R_RISCV_CALL_PLT printf
  28: 000080e7  jalr   ra  ## 24 &lt;main+0x24&gt;

   ## Clean up the Stack and
   ## return 0 to Caller
  2c: 0000051b  sext.w a0, zero
  30: 01813083  ld     ra, 24(sp)
  34: 01013403  ld     s0, 16(sp)
  38: 02010113  addi   sp, sp, 32
  3c: 00008067  ret
</code></pre></div>
<p>Yep the <strong>64-bit RISC-V Code</strong> looks legit! Very similar to our <a href="https://lupyuen.codeberg.page/articles/app#inside-a-nuttx-app"><strong>NuttX App</strong></a>. (So it will probably run on NuttX)</p>
<p>What just happened? We go behind the scenes…</p>
<p><a href="https://gist.github.com/lupyuen/ab8febefa9c649ad7c242ee3f7aaf974">(See the <strong>Entire Disassembly</strong>)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/app#inside-a-nuttx-app">(About the <strong>RISC-V Instructions</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-zig.jpg" alt="Zig Compiler compiles TCC Compiler to WebAssembly"></p>
<h2 id="zig-compiles-tcc-to-webassembly"><a href="#zig-compiles-tcc-to-webassembly">2 Zig compiles TCC to WebAssembly</a></h2>
<p><em>Will Zig Compiler happily compile TCC to WebAssembly?</em></p>
<p>Amazingly, yes! (Pic above)</p>
<div><pre><code>## Zig Compiler compiles TCC Compiler
## from C to WebAssembly. Produces `tcc.o`
zig cc \
  -c \
  -target wasm32-freestanding \
  -dynamic \
  -rdynamic \
  -lc \
  -DTCC_TARGET_RISCV64 \
  -DCONFIG_TCC_CROSSPREFIX="\"riscv64-\""  \
  -DCONFIG_TCC_CRTPREFIX="\"/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_LIBPATHS="\"{B}:/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_SYSINCLUDEPATHS="\"{B}/include:/usr/riscv64-linux-gnu/include\""   \
  -DTCC_GITHASH="\"main:b3d10a35\"" \
  -Wall \
  -O2 \
  -Wdeclaration-after-statement \
  -fno-strict-aliasing \
  -Wno-pointer-sign \
  -Wno-sign-compare \
  -Wno-unused-result \
  -Wno-format-truncation \
  -Wno-stringop-truncation \
  -I. \
  tcc.c
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/tcc.c">(See the <strong>TCC Source Code</strong>)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-compile-tcc-with-zig">(About the <strong>Zig Compiler Options</strong>)</a></p>
<p>We link the TCC WebAssembly with our <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig"><strong>Zig Wrapper</strong></a> (that exports the TCC Compiler to JavaScript)…</p>
<div><pre><code>## Compile our Zig Wrapper `tcc-wasm.zig` for WebAssembly
## and link it with TCC compiled for WebAssembly `tcc.o`
## Generates `tcc-wasm.wasm`
zig build-exe \
  -target wasm32-freestanding \
  -rdynamic \
  -lc \
  -fno-entry \
  -freference-trace \
  --verbose-cimport \
  --export=compile_program \
  zig/tcc-wasm.zig \
  tcc.o

## Test everything with Web Browser
## or Node.js
node zig/test.js
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig">(See the <strong>Zig Wrapper tcc-wasm.zig</strong>)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test.js">(See the <strong>Test JavaScript test.js</strong>)</a></p>
<p><em>What’s inside our Zig Wrapper?</em></p>
<p>Our Zig Wrapper will…</p>
<ol>
<li>
<p>Receive the <strong>C Program</strong> from JavaScript</p>
</li>
<li>
<p>Receive the <strong>TCC Compiler Options</strong> from JavaScript</p>
</li>
<li>
<p>Call TCC Compiler to <strong>compile our program</strong></p>
</li>
<li>
<p>Return the compiled <strong>RISC-V ELF</strong> to JavaScript</p>
</li>
</ol>
<p>Like so: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L11-L76">tcc-wasm.zig</a></p>
<div><pre><code>/// Call TCC Compiler to compile a
/// C Program to RISC-V ELF
pub export fn compile_program(
  options_ptr: [*:0]const u8, // Options for TCC Compiler (Pointer to JSON Array:  ["-c", "hello.c"])
  code_ptr:    [*:0]const u8, // C Program to be compiled (Pointer to String)
) [*]const u8 { // Returns a pointer to the `a.out` Compiled Code (Size in first 4 bytes)

  // Receive the C Program from
  // JavaScript and set our Read Buffer
  // https://blog.battlefy.com/zig-made-it-easy-to-pass-strings-back-and-forth-with-webassembly
  const code: []const u8 = std.mem.span(code_ptr);
  read_buf = code;

  // Omitted: Receive the TCC Compiler
  // Options from JavaScript
  // (JSON containing String Array: ["-c", "hello.c"])
  ...

  // Call the TCC Compiler
  _ = main(@intCast(argc), &amp;args_ptrs);

  // Return pointer of `a.out` to
  // JavaScript. First 4 bytes: Size of
  // `a.out`. Followed by `a.out` data.
  const slice = std.heap.page_allocator.alloc(u8, write_buflen + 4)   
    catch @panic("Failed to allocate memory");
  const size_ptr: *u32 = @alignCast(@ptrCast(slice.ptr));
  size_ptr.* = write_buflen;
  @memcpy(slice[4 .. write_buflen + 4], write_buf[0..write_buflen]);
  return slice.ptr; // TODO: Deallocate this memory
}
</code></pre></div>
<p>Plus a couple of Magical Bits that we’ll cover in the next section.</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-javascript-calls-tcc">(How JavaScript calls our <strong>Zig Wrapper</strong>)</a></p>
<p><em>Zig Compiler compiles TCC without any code changes?</em></p>
<p>Inside TCC, we stubbed out the <a href="https://github.com/lupyuen/tcc-riscv32-wasm/commit/e30454a0eb9916f820d58a7c3e104eeda67988d8"><strong>setjmp / longjmp</strong></a> to make it compile with Zig Compiler.</p>
<p>Everything else compiles OK!</p>
<p><em>Is it really OK to stub them out?</em></p>
<p><a href="https://en.wikipedia.org/wiki/Setjmp.h"><strong>setjmp / longjmp</strong></a> are called to <strong>Handle Errors</strong> during TCC Compilation. Assuming everything goes hunky dory, we won’t need them.</p>
<p>Later we’ll find a better way to express our outrage. (Instead of jumping around)</p>
<p>We probe the Magical Bits inside our Zig Wrapper…</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-posix.jpg" alt="TCC Compiler in WebAssembly needs POSIX Functions"></p>
<h2 id="posix-for-webassembly"><a href="#posix-for-webassembly">3 POSIX for WebAssembly</a></h2>
<p><em>What’s this POSIX?</em></p>
<p>TCC Compiler was created as a <strong>Command-Line App</strong>. So it calls the typical <a href="https://en.wikipedia.org/wiki/POSIX"><strong>POSIX Functions</strong></a> like <strong>fopen, fprintf, strncpy, malloc,</strong> …</p>
<p>But WebAssembly running in a Web Browser ain’t <strong>No Command Line</strong>! (Pic above)</p>
<p><a href="https://en.wikipedia.org/wiki/C_standard_library">(WebAssembly doesn’t have a <strong>C Standard Library libc</strong>)</a></p>
<p><em>Is POSIX a problem for WebAssembly?</em></p>
<p>We counted <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-missing-functions"><strong>72 POSIX Functions</strong></a> needed by TCC Compiler, but missing from WebAssembly.</p>
<p>Thus we fill in the <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-missing-functions"><strong>Missing Functions</strong></a> ourselves.</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-missing-functions">(About the <strong>Missing POSIX Functions</strong>)</a></p>
<p><em>Surely other Zig Devs will have the same problem?</em></p>
<p>Thankfully we can borrow the POSIX Code from other <strong>Zig Libraries</strong>…</p>
<ul>
<li>
<p><a href="https://github.com/marler8997/ziglibc"><strong>ziglibc</strong></a>: Zig implementation of libc</p>
</li>
<li>
<p><a href="https://github.com/ZigEmbeddedGroup/foundation-libc"><strong>foundation-libc</strong></a>: Freestanding implementation of libc</p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/articles/lvgl3#appendix-lvgl-memory-allocation"><strong>PinePhone Simulator</strong></a>: For malloc</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L447-L774">(See the <strong>Borrowed Code</strong>)</a></p>
</li>
</ul>
<p><em>72 POSIX Functions? Sounds like a lot of work…</em></p>
<p>We might not need all 72 POSIX Functions. We stubbed out <strong>many of the functions</strong> to identify the ones that are called: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L776-L855">tcc-wasm.zig</a></p>
<div><pre><code>// Stub Out the Missing POSIX
// Functions. If TCC calls them, 
// we'll see a Zig Panic. Then we 
// implement them. The Types don't
// matter because we'll halt anyway.

pub export fn atoi(_: c_int) c_int {
  @panic("TODO: atoi");
}
pub export fn exit(_: c_int) c_int {
  @panic("TODO: exit");
}
pub export fn fopen(_: c_int) c_int {
  @panic("TODO: fopen");
}

// And many more functions...
</code></pre></div>
<p>Some of these functions are especially troubling for WebAssembly…</p>
<blockquote>
<p><img src="https://lupyuen.codeberg.page/images/tcc-posix2.jpg" alt="File Input and Output are especially troubling for WebAssembly"></p>
</blockquote>
<h2 id="file-input-and-output"><a href="#file-input-and-output">4 File Input and Output</a></h2>
<p><em>Why no #include in TCC for WebAssembly? And no C Libraries?</em></p>
<p>WebAssembly runs in a Secure Sandbox. <strong>No File Access</strong> allowed, sorry! (Like for Header and Library Files)</p>
<p>That’s why our Zig Wrapper <strong>Emulates File Access</strong> for the bare minimum 2 files…</p>
<ul>
<li>
<p>Read the <strong>C Program</strong>: <strong><code>hello.c</code></strong></p>
</li>
<li>
<p>Write the <strong>RISC-V ELF</strong>: <strong><code>a.out</code></strong></p>
</li>
</ul>
<p><strong>Reading a Source File <code>hello.c</code></strong> is extremely simplistic: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L104-L118">tcc-wasm.zig</a></p>
<div><pre><code>/// Emulate the POSIX Function `read()`
/// We copy from One Single Read Buffer
/// that contains our C Program
export fn read(fd0: c_int, buf: [*:0]u8, nbyte: size_t) isize {

  // TODO: Support more than one file
  const len = read_buf.len;
  assert(len &lt; nbyte);
  @memcpy(buf[0..len], read_buf[0..len]);
  buf[len] = 0;
  read_buf.len = 0;
  return @intCast(len);
}

/// Read Buffer for read
var read_buf: []const u8 = undefined;
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L26-L32">(<strong>read_buf</strong> is populated at startup)</a></p>
<p><strong>Writing the Compiled Output <code>a.out</code></strong> is just as barebones: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L128-L140">tcc-wasm.zig</a></p>
<div><pre><code>/// Emulate the POSIX Function `write()`
/// We write to One Single Memory
/// Buffer that will be returned to 
/// JavaScript as `a.out`
export fn fwrite(ptr: [*:0]const u8, size: usize, nmemb: usize, stream: *FILE) usize {

  // TODO: Support more than one `stream`
  const len = size * nmemb;
  @memcpy(write_buf[write_buflen .. write_buflen + len], ptr[0..]);
  write_buflen += len;
  return nmemb;
}

/// Write Buffer for fputc and fwrite
var write_buf = std.mem.zeroes([8192]u8);
var write_buflen: usize = 0;
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L62-L78">(<strong>write_buf</strong> will be returned to JavaScript)</a></p>
<p><em>Can we handle Multiple Files?</em></p>
<p>Right now we’re trying to embed the simple <a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly"><strong>ROM FS Filesystem</strong></a> into our Zig Wrapper.</p>
<p>The ROM FS Filesystem will be preloaded with the Header and Library Files needed by TCC.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly">(See the updates for <strong>ROM FS Filesystem</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-format.jpg" alt="Our Zig Wrapper uses Pattern Matching to match the C Formats and substitute the Zig Equivalent"></p>
<h2 id="fearsome-fprintf-and-friends"><a href="#fearsome-fprintf-and-friends">5 Fearsome fprintf and Friends</a></h2>
<p><em>Why is fprintf particularly problematic?</em></p>
<p>Here’s the fearsome thing about <strong>fprintf</strong> and friends: <strong>sprintf, snprintf, vsnprintf</strong>…</p>
<ul>
<li>
<p><strong>C Format Strings</strong> are difficult to parse</p>
</li>
<li>
<p><strong>Variable Number of Untyped Arguments</strong> might create Bad Pointers</p>
</li>
</ul>
<p>Hence we hacked up an implementation of <strong>String Formatting</strong> that’s safer, simpler and so-barebones-you-can-make-<em>soup-tulang</em>.</p>
<p><em>Soup tulang? Tell me more…</em></p>
<p>Our Zig Wrapper uses <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-pattern-matching"><strong>Pattern Matching</strong></a> to match the <strong>C Formats</strong> and substitute the <strong>Zig Equivalent</strong> (pic above): <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L189-L207">tcc-wasm.zig</a></p>
<div><pre><code>// Format a Single `%d`
// like `#define __TINYC__ %d`
FormatPattern{

  // If the C Format String contains this...
  .c_spec = "%d",
  
  // Then we apply this Zig Format...
  .zig_spec = "{}",
  
  // And extract these Argument Types
  // from the Varargs...
  .type0 = c_int,
  .type1 = null
}
</code></pre></div>
<p>This works OK (for now) because TCC Compiler only uses <strong>5 Patterns for C Format Strings</strong>: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L189-L207">tcc-wasm.zig</a></p>
<div><pre><code>/// Pattern Matching for C String Formatting:
/// We'll match these patterns when
/// formatting strings
const format_patterns = [_]FormatPattern{

  // Format a Single `%d`, like `#define __TINYC__ %d`
  FormatPattern{
    .c_spec = "%d",  .zig_spec = "{}", 
    .type0  = c_int, .type1 = null
  },

  // Format a Single `%u`, like `L.%u`
  FormatPattern{ 
    .c_spec = "%u",  .zig_spec = "{}", 
    .type0  = c_int, .type1 = null 
  },

  // Format a Single `%s`, like `.rela%s`
  // Or `#define __BASE_FILE__ "%s"`
  FormatPattern{
    .c_spec = "%s", .zig_spec = "{s}",
    .type0  = [*:0]const u8, .type1 = null
  },

  // Format Two `%s`, like `#define %s%s\n`
  FormatPattern{
    .c_spec = "%s%s", .zig_spec = "{s}{s}",
    .type0  = [*:0]const u8, .type1 = [*:0]const u8
  },

  // Format `%s:%d`, like `%s:%d: `
  // (File Name and Line Number)
  FormatPattern{
    .c_spec = "%s:%d", .zig_spec = "{s}:{}",
    .type0  = [*:0]const u8, .type1 = c_int
  },
};
</code></pre></div>
<p>That’s our quick hack for <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L209-L447"><strong>fprintf and friends</strong></a>!</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-pattern-matching">(How we do <strong>Pattern Matching</strong>)</a></p>
<p><em>So simple? Unbelievable!</em></p>
<p>Actually we’ll hit more Format Patterns as TCC Compiler emits various <strong>Error and Warning Messages</strong>. But it’s a good start!</p>
<p>Later our Zig Wrapper shall cautiously and meticulously parse all kinds of C Format Strings. Or we do the <a href="https://github.com/marler8997/ziglibc/blob/main/src/printf.c#L32-L191"><strong>parsing in C</strong></a>, compiled to WebAssembly. (160 lines of C!)</p>
<p>(Funny how <strong>printf</strong> is the first thing we learn about C. Yet it’s incredibly difficult to implement!)</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-nuttx.jpg" alt="Compile and Run NuttX Apps in the Web Browser"></p>
<h2 id="test-with-apache-nuttx-rtos"><a href="#test-with-apache-nuttx-rtos">6 Test with Apache NuttX RTOS</a></h2>
<p><em>TCC in WebAssembly has compiled our C Program to RISC-V ELF…</em></p>
<p><em>Will the ELF run on NuttX?</em></p>
<p><a href="https://nuttx.apache.org/docs/latest/"><strong>Apache NuttX RTOS</strong></a> is a tiny operating system for 64-bit RISC-V that runs on <a href="https://www.qemu.org/docs/master/system/target-riscv.html"><strong>QEMU Emulator</strong></a>. (And many other devices)</p>
<p>We build <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-build-nuttx-for-qemu"><strong>NuttX for QEMU</strong></a> and copy our <a href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format"><strong>RISC-V ELF <code>a.out</code></strong></a> to the <a href="https://lupyuen.codeberg.page/articles/semihost#nuttx-apps-filesystem"><strong>NuttX Apps Filesystem</strong></a> (pic above)…</p>
<div><pre><code>## Copy RISC-V ELF `a.out`
## to NuttX Apps Filesystem
cp a.out apps/bin/
chmod +x apps/bin/a.out
</code></pre></div>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-build-nuttx-for-qemu">(How we build <strong>NuttX for QEMU</strong>)</a></p>
<p>Then we boot NuttX and run <strong><code>a.out</code></strong>…</p>
<div><pre><code>## Boot NuttX on QEMU 64-bit RISC-V
$ qemu-system-riscv64 \
  -semihosting \
  -M virt,aclint=on \
  -cpu rv64 \
  -smp 8 \
  -bios none \
  -kernel nuttx \
  -nographic

## Run `a.out` in NuttX Shell
NuttShell (NSH) NuttX-12.4.0
nsh&gt; a.out
Loading /system/bin/a.out
Exported symbol "printf" not found
Failed to load program 'a.out'
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#test-tcc-output-with-nuttx">(See the <strong>Complete Log</strong>)</a></p>
<p>NuttX politely accepts the RISC-V ELF (produced by TCC). And says that <strong>printf</strong> is missing.</p>
<p>Which makes sense: We haven’t linked our C Program with the <a href="https://github.com/lupyuen/tcc-riscv32-wasm#how-nuttx-build-links-a-nuttx-app"><strong>C Library</strong></a>!</p>
<p><a href="https://gist.github.com/lupyuen/847f7adee50499cac5212f2b95d19cd3#file-nuttx-elf-loader-log-L882-L1212">(Loading a <strong>RISC-V ELF</strong> should look like this)</a></p>
<p><em>How else can we print something in NuttX?</em></p>
<p>To print something, we can make a <a href="https://lupyuen.codeberg.page/articles/app#nuttx-app-calls-nuttx-kernel"><strong>System Call (ECALL)</strong></a> directly to NuttX Kernel (bypassing the POSIX Functions)…</p>
<div><pre><code>// NuttX System Call that prints
// something. System Call Number
// is 61 (SYS_write). Works exactly
// like POSIX `write()`
ssize_t write(
  int fd,           // File Descriptor (1 for Standard Output)
  const char *buf,  // Buffer to be printed
  size_t buflen     // Buffer Length
);

// Which makes an ECALL with these Parameters...
// Register A0 is 61 (SYS_write)
// Register A1 is the File Descriptor (1 for Standard Output)
// Register A2 points to the String Buffer to be printed
// Register A3 is the Buffer Length
</code></pre></div>
<p>That’s the same NuttX System Call that <strong>printf</strong> executes internally.</p>
<p>Final chance to say hello to NuttX…</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-ecall.png" alt="TCC WebAssembly compiles a NuttX System Call"></p>
<h2 id="hello-nuttx"><a href="#hello-nuttx">7 Hello NuttX!</a></h2>
<p><em>We’re making a System Call (ECALL) to NuttX Kernel to print something…</em></p>
<p><em>How will we code this in C?</em></p>
<p>We execute the <a href="https://lupyuen.codeberg.page/articles/app#nuttx-app-calls-nuttx-kernel"><strong>ECALL in RISC-V Assembly</strong></a> like this: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test-nuttx.js#L52-L105">test-nuttx.js</a></p>
<div><pre><code>int main(int argc, char *argv[]) {

  // Make NuttX System Call
  // to write(fd, buf, buflen)
  const unsigned int nbr = 61; // SYS_write
  const void *parm1 = 1;       // File Descriptor (stdout)
  const void *parm2 = "Hello, World!!\n"; // Buffer
  const void *parm3 = 15; // Buffer Length

  // Load the Parameters into
  // Registers A0 to A3
  // Note: This doesn't work with TCC,
  // so we load again below
  register long r0 asm("a0") = (long)(nbr);
  register long r1 asm("a1") = (long)(parm1);
  register long r2 asm("a2") = (long)(parm2);
  register long r3 asm("a3") = (long)(parm3);

  // Execute ECALL for System Call
  // to NuttX Kernel. Again: Load the
  // Parameters into Registers A0 to A3
  asm volatile (

    // Load 61 to Register A0 (SYS_write)
    "addi a0, zero, 61 \n"
    
    // Load 1 to Register A1 (File Descriptor)
    "addi a1, zero, 1 \n"
    
    // Load 0xc0101000 to Register A2 (Buffer)
    "lui   a2, 0xc0 \n"
    "addiw a2, a2, 257 \n"
    "slli  a2, a2, 0xc \n"
    
    // Load 15 to Register A3 (Buffer Length)
    "addi a3, zero, 15 \n"
    
    // ECALL for System Call to NuttX Kernel
    "ecall \n"
    
    // NuttX needs NOP after ECALL
    ".word 0x0001 \n"

    // Input+Output Registers: None
    // Input-Only Registers: A0 to A3
    // Clobbers the Memory
    :
    : "r"(r0), "r"(r1), "r"(r2), "r"(r3)
    : "memory"
  );

  // Loop Forever
  for(;;) {}
  return 0;
}
</code></pre></div>
<p>We copy this into our Web Browser and compile it. (Pic above)</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-nuttx-system-call">(Why so complicated? <strong>Explained here</strong>)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/app#nuttx-kernel-handles-system-call">(Caution: <strong>SYS_write 61</strong> may change)</a></p>
<p><em>Does it work?</em></p>
<p>TCC in WebAssembly compiles the code above to <strong>RISC-V ELF <code>a.out</code></strong>. When we copy it to NuttX and run it…</p>
<div><pre><code>NuttShell (NSH) NuttX-12.4.0-RC0
nsh&gt; a.out
...
## NuttX System Call for SYS_write (61)
riscv_swint:
  cmd: 61
  A0:  3d  ## SYS_write (61)
  A1:  01  ## File Descriptor (Standard Output)
  A2:  c0101000  ## Buffer
  A3:  0f        ## Buffer Length
...
## NuttX Kernel says hello
Hello, World!!
</code></pre></div>
<p>NuttX Kernel prints <strong>“Hello World”</strong> yay!</p>
<p>Indeed we’ve created a C Compiler in a Web Browser, that <strong>produces proper NuttX Apps</strong>!</p>
<p><em>OK so we can build NuttX Apps in a Web Browser… But can we run them in a Web Browser?</em></p>
<p>Yep, a NuttX App built in the Web Browser… Now runs OK with <strong>NuttX Emulator in the Web Browser</strong>! 🎉 (Pic below)</p>
<ul>
<li>
<p><a href="https://youtu.be/DJMDYq52Iv8">Watch the <strong>Demo on YouTube</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser">Find out <strong>How It Works</strong></a></p>
</li>
</ul>
<p><strong>TLDR:</strong> We called <a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser"><strong>JavaScript Local Storage</strong></a>
to copy the RISC-V ELF <code>a.out</code> from TCC WebAssembly to NuttX Emulator… Then we patched <code>a.out</code> into the <a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser"><strong>ROM FS Filesystem</strong></a> for NuttX Emulator. Nifty!</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-emu2.png" alt="NuttX App built in a Web Browser… Runs inside the Web Browser!"></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser"><em>NuttX App built in a Web Browser… Runs inside the Web Browser!</em></a></p>
<h2 id="whats-next"><a href="#whats-next">8 What’s Next</a></h2>
<p>Thanks to the <a href="https://github.com/sellicott/tcc-riscv32"><strong>TCC Team</strong></a>, we have a <strong>64-bit RISC-V Compiler</strong> that runs in the Web Browser…</p>
<ul>
<li>
<p><strong>Zig Compiler</strong> compiles TCC to WebAssembly with one tiny fix</p>
</li>
<li>
<p>But <strong>POSIX Functions</strong> are missing in WebAssembly</p>
</li>
<li>
<p>So we did the bare minimum for <strong>File Input and Output</strong> </p>
</li>
<li>
<p>And cooked up the simplest workaround for <strong>fprintf and friends</strong></p>
</li>
<li>
<p>Finally TCC produces a <strong>RISC-V Binary</strong> that runs OK on Apache NuttX RTOS</p>
</li>
<li>
<p>Now we can <strong>Build and Test NuttX Apps</strong> all within a Web Browser!</p>
</li>
</ul>
<p>How will you use <strong>TCC in a Web Browser</strong>? Please lemme know 🙏</p>
<p><em>(Build and run RISC-V Apps on iPhone?)</em></p>
<p>Many Thanks to my <a href="https://github.com/sponsors/lupyuen"><strong>GitHub Sponsors</strong></a> (and the awesome NuttX and Zig Communities) for supporting my work! This article wouldn’t have been possible without your support.</p>
<ul>
<li>
<p><a href="https://github.com/sponsors/lupyuen"><strong>Sponsor me a coffee</strong></a></p>
</li>
<li>
<p><a href="https://news.ycombinator.com/item?id=39245664"><strong>Discuss this article on Hacker News</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/nuttx-ox64"><strong>My Current Project: “Apache NuttX RTOS for Ox64 BL808”</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/nuttx-star64"><strong>My Other Project: “NuttX for Star64 JH7110”</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/pinephone-nuttx"><strong>Older Project: “NuttX for PinePhone”</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/"><strong>Check out my articles</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/rss.xml"><strong>RSS Feed</strong></a></p>
</li>
</ul>
<p><em>Got a question, comment or suggestion? Create an Issue or submit a Pull Request here…</em></p>
<p><a href="https://github.com/lupyuen/lupyuen.github.io/blob/master/src/tcc.md"><strong>lupyuen.github.io/src/tcc.md</strong></a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-web.png" alt="Online Demo of TCC Compiler in WebAssembly"></p>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><em>Online Demo of TCC Compiler in WebAssembly</em></a></p>
<h2 id="appendix-compile-tcc-with-zig"><a href="#appendix-compile-tcc-with-zig">9 Appendix: Compile TCC with Zig</a></h2>
<p>This is how we run <strong>Zig Compiler to compile TCC Compiler</strong> from C to WebAssembly (pic below)…</p>
<div><pre><code>## Download the (slightly) Modified TCC Source Code.
## Configure the build for 64-bit RISC-V.

git clone https://github.com/lupyuen/tcc-riscv32-wasm
cd tcc-riscv32-wasm
./configure
make cross-riscv64

## Call Zig Compiler to compile TCC Compiler
## from C to WebAssembly. Produces `tcc.o`

## Omitted: Run the `zig cc` command from earlier...
## https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly
zig cc ...

## Compile our Zig Wrapper `tcc-wasm.zig` for WebAssembly
## and link it with TCC compiled for WebAssembly `tcc.o`
## Generates `tcc-wasm.wasm`

## Omitted: Run the `zig build-exe` command from earlier...
## https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly
zig build-exe ...
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/build.sh">(See the <strong>Build Script</strong>)</a></p>
<p><em>How did we figure out the “<code>zig</code> <code>cc</code>” options?</em></p>
<p>Earlier we saw a long list of <a href="https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly"><strong>Zig Compiler Options</strong></a>…</p>
<div><pre><code>## Zig Compiler Options for TCC Compiler
zig cc \
  tcc.c \
  -DTCC_TARGET_RISCV64 \
  -DCONFIG_TCC_CROSSPREFIX="\"riscv64-\""  \
  -DCONFIG_TCC_CRTPREFIX="\"/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_LIBPATHS="\"{B}:/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_SYSINCLUDEPATHS="\"{B}/include:/usr/riscv64-linux-gnu/include\""   \
  ...
</code></pre></div>
<p>We got them from “<strong><code>make</code> <code>--trace</code></strong>”, which reveals the <strong>GCC Compiler Options</strong>…</p>
<div><pre><code>## Show the GCC Options for compiling TCC
$ make --trace cross-riscv64

gcc \
  -o riscv64-tcc.o \
  -c \
  tcc.c \
  -DTCC_TARGET_RISCV64 \
  -DCONFIG_TCC_CROSSPREFIX="\"riscv64-\""  \
  -DCONFIG_TCC_CRTPREFIX="\"/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_LIBPATHS="\"{B}:/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_SYSINCLUDEPATHS="\"{B}/include:/usr/riscv64-linux-gnu/include\""   \
  -DTCC_GITHASH="\"main:b3d10a35\"" \
  -Wall \
  -O2 \
  -Wdeclaration-after-statement \
  -fno-strict-aliasing \
  -Wno-pointer-sign \
  -Wno-sign-compare \
  -Wno-unused-result \
  -Wno-format-truncation \
  -Wno-stringop-truncation \
  -I. 
</code></pre></div>
<p>And we copied above GCC Options to become our <a href="https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly"><strong>Zig Compiler Options</strong></a>.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/build.sh">(See the <strong>Build Script</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-zig.jpg" alt="Zig Compiler compiles TCC Compiler to WebAssembly"></p>
<h2 id="appendix-javascript-calls-tcc"><a href="#appendix-javascript-calls-tcc">10 Appendix: JavaScript calls TCC</a></h2>
<p>Previously we saw some <strong>JavaScript (Web Browser and Node.js)</strong> calling our TCC Compiler in WebAssembly (pic above)…</p>
<ul>
<li>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><strong>TCC WebAssembly in Web Browser</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly"><strong>TCC WebAssembly in Node.js</strong></a></p>
</li>
</ul>
<p>This is how we test the TCC WebAssembly in a Web Browser with a <strong>Local Web Server</strong>…</p>
<div><pre><code>## Download the (slightly) Modified TCC Source Code
git clone https://github.com/lupyuen/tcc-riscv32-wasm
cd tcc-riscv32-wasm

## Start the Web Server
cargo install simple-http-server
simple-http-server ./docs &amp;

## Whenever we rebuild TCC WebAssembly...
## Copy it to the Web Server
cp tcc-wasm.wasm docs/
</code></pre></div>
<p>Browse to this URL and our TCC WebAssembly will appear…</p>
<div><pre><code>## Test TCC WebAssembly with Web Browser
http://localhost:8000/index.html
</code></pre></div>
<p>Check the <strong>JavaScript Console</strong> for Debug Messages.</p>
<p><a href="https://gist.github.com/lupyuen/5f8191d5c63b7dba030582cbe7481572">(See the <strong>JavaScript Log</strong>)</a></p>
<p><em>How does it work?</em></p>
<p>On clicking the <strong>Compile Button</strong>, our JavaScript loads the TCC WebAssembly: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L174-L191">tcc.js</a></p>
<div><pre><code>// Load the WebAssembly Module and start the Main Function.
// Called by the Compile Button.
async function bootstrap() {

  // Load the WebAssembly Module `tcc-wasm.wasm`
  // https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/instantiateStreaming
  const result = await WebAssembly.instantiateStreaming(
    fetch("tcc-wasm.wasm"),
    importObject
  );

  // Store references to WebAssembly Functions
  // and Memory exported by Zig
  wasm.init(result);

  // Start the Main Function
  window.requestAnimationFrame(main);
}        
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L25-L48">(<strong>importObject</strong> exports our <strong>JavaScript Logger</strong> to Zig)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L6-L25">(<strong>wasm</strong> is our <strong>WebAssembly Helper</strong>)</a></p>
<p>Which triggers the <strong>Main Function</strong> and calls our Zig Function <strong>compile_program</strong>: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L48-L90">tcc.js</a></p>
<div><pre><code>// Main Function
function main() {
  // Allocate a String for passing the Compiler Options to Zig
  // `options` is a JSON Array: ["-c", "hello.c"]
  const options = read_options();
  const options_ptr = allocateString(JSON.stringify(options));
  
  // Allocate a String for passing the Program Code to Zig
  const code = document.getElementById("code").value;
  const code_ptr = allocateString(code);

  // Call TCC to compile the program
  const ptr = wasm.instance.exports
    .compile_program(options_ptr, code_ptr);

  // Get the `a.out` size from first 4 bytes returned
  const memory = wasm.instance.exports.memory;
  const data_len = new Uint8Array(memory.buffer, ptr, 4);
  const len = data_len[0] | data_len[1] &lt;&lt; 8 | data_len[2] &lt;&lt; 16 | data_len[3] &lt;&lt; 24;
  if (len &lt;= 0) { return; }

  // Encode the `a.out` data from the rest of the bytes returned
  // `encoded_data` looks like %7f%45%4c%46...
  const data = new Uint8Array(memory.buffer, ptr + 4, len);
  let encoded_data = "";
  for (const i in data) {
    const hex = Number(data[i]).toString(16).padStart(2, "0");
    encoded_data += `%${hex}`;
  }

  // Download the `a.out` data into the Web Browser
  download("a.out", encoded_data);

  // Save the ELF Data to Local Storage for loading by NuttX Emulator
  localStorage.setItem("elf_data", encoded_data);
};
</code></pre></div>
<p>Our Main Function then downloads the <strong><code>a.out</code></strong> file returned by our Zig Function.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L90-L112">(<strong>allocateString</strong> allocates a String from Zig Memory)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L162-L174">(<strong>download</strong> is here)</a></p>
<p><em>What about Node.js calling TCC WebAssembly?</em></p>
<div><pre><code>## Test TCC WebAssembly with Node.js
node zig/test.js
</code></pre></div>
<p><strong>For Easier Testing</strong> (via Command-Line): We copied the JavaScript above into a Node.js Script: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test.js#L46-L78">test.js</a></p>
<div><pre><code>// Allocate a String for passing the Compiler Options to Zig
const options = ["-c", "hello.c"];
const options_ptr = allocateString(JSON.stringify(options));

// Allocate a String for passing Program Code to Zig
const code_ptr = allocateString(`
  int main(int argc, char *argv[]) {
    printf("Hello, World!!\\n");
    return 0;
  }
`);

// Call TCC to compile a program
const ptr = wasm.instance.exports
  .compile_program(options_ptr, code_ptr);
</code></pre></div>
<p><a href="https://gist.github.com/lupyuen/795327506cad9b1ee82206e614c399cd">(See the <strong>Node.js Log</strong>)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test-nuttx.js">(Test Script for NuttX QEMU: <strong>test-nuttx.js</strong>)</a></p>
<p><a href="https://gist.github.com/lupyuen/55a4d4cae26994aa673e6d8451716b27">(Test Log for NuttX QEMU: <strong>test-nuttx.log</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-format.jpg" alt="Our Zig Wrapper doing Pattern Matching for Formatting C Strings"></p>
<h2 id="appendix-pattern-matching"><a href="#appendix-pattern-matching">11 Appendix: Pattern Matching</a></h2>
<p>A while back we saw our Zig Wrapper doing <strong>Pattern Matching</strong> for Formatting C Strings…</p>
<ul>
<li><a href="https://lupyuen.codeberg.page/articles/tcc#fearsome-fprintf-and-friends"><strong>“Fearsome fprintf and Friends”</strong></a></li>
</ul>
<p>How It Works: We search for <strong>Format Patterns</strong> in the C Format Strings and substitute the <strong>Zig Equivalent</strong> (pic above): <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L189-L207">tcc-wasm.zig</a></p>
<div><pre><code>// Format a Single `%d`
// like `#define __TINYC__ %d`
FormatPattern{

  // If the C Format String contains this...
  .c_spec = "%d",
  
  // Then we apply this Zig Format...
  .zig_spec = "{}",
  
  // And extract these Argument Types
  // from the Varargs...
  .type0 = c_int,
  .type1 = null
}
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L438-L446">(<strong>FormatPattern</strong> is defined here)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L191-L209">(See the <strong>Format Patterns</strong>)</a></p>
<p>To implement this, we call <strong>comptime Functions</strong> in Zig: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L276-L327">tcc-wasm.zig</a></p>
<div><pre><code>/// CompTime Function to format a string by Pattern Matching.
/// Format a Single Specifier, like `#define __TINYC__ %d\n`
/// If the Spec matches the Format: Return the number of bytes written to `str`, excluding terminating null.
/// Else return 0.
fn format_string1(
  ap: *std.builtin.VaList,  // Varargs passed from C
  str:    [*]u8,            // Buffer for returning Formatted String
  size:   size_t,           // Buffer Size
  format: []const u8,       // C Format String, like `#define __TINYC__ %d\n`
  comptime c_spec:   []const u8,  // C Format Pattern, like `%d`
  comptime zig_spec: []const u8,  // Zig Equivalent, like `{}`
  comptime T0:       type,        // Type of First Vararg, like `c_int`
) usize {  // Return the number of bytes written to `str`, excluding terminating null

  // Count the Format Specifiers: `%`
  const spec_cnt   = std.mem.count(u8, c_spec, "%");
  const format_cnt = std.mem.count(u8, format, "%");

  // Check the Format Specifiers: `%`
  // Quit if the number of specifiers are different
  // Or if the specifiers are not found
  if (format_cnt != spec_cnt or
      !std.mem.containsAtLeast(u8, format, 1, c_spec)) {
    return 0;
  }

  // Fetch the First Argument from the C Varargs
  const a = @cVaArg(ap, T0);

  // Format the Argument
  var buf: [512]u8 = undefined;
  const buf_slice = std.fmt.bufPrint(&amp;buf, zig_spec, .{a}) catch {
    @panic("format_string1 error: buf too small");
  };

  // Replace the C Format Pattern by the Zig Equivalent
  var buf2 = std.mem.zeroes([512]u8);
  _ = std.mem.replace(u8, format, c_spec, buf_slice, &amp;buf2);

  // Return the Formatted String and Length
  const len = std.mem.indexOfScalar(u8, &amp;buf2, 0).?;
  assert(len &lt; size);
  @memcpy(str[0..len], buf2[0..len]);
  str[len] = 0;
  return len;
}

// Omitted: Function `format_string2` looks similar,
// but for 2 Varargs (instead of 1)
</code></pre></div>
<p>The function above is called by a <strong>comptime Inline Loop</strong> that applies all the <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L191-L209"><strong>Format Patterns</strong></a> that we saw earlier: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L207-L251">tcc-wasm.zig</a></p>
<div><pre><code>/// Runtime Function to format a string by Pattern Matching.
/// Return the number of bytes written to `str`, excluding terminating null.
fn format_string(
  ap: *std.builtin.VaList,  // Varargs passed from C
  str:    [*]u8,            // Buffer for returning Formatted String
  size:   size_t,           // Buffer Size
  format: []const u8,       // C Format String, like `#define __TINYC__ %d\n`
) usize {  // Return the number of bytes written to `str`, excluding terminating null

  // If no Format Specifiers: Return the Format, like `warning: `
  const len = format_string0(str, size, format);
  if (len &gt; 0) { return len; }

  // For every Format Pattern...
  inline for (format_patterns) |pattern| {

    // Try formatting the string with the pattern...
    const len2 =
      if (pattern.type1) |t1|
      // Pattern has 2 parameters
      format_string2(ap, str, size, format, // Output String and Format String
        pattern.c_spec, pattern.zig_spec,   // Format Specifiers for C and Zig
        pattern.type0, t1 // Types of the Parameters
      )
    else
      // Pattern has 1 parameter
      format_string1(ap, str, size, format, // Output String and Format String
        pattern.c_spec, pattern.zig_spec,   // Format Specifiers for C and Zig
        pattern.type0 // Type of the Parameter
      );

    // Loop until we find a match pattern
    if (len2 &gt; 0) { return len2; }
  }

  // Format String doesn't match any Format Pattern.
  // We return the Format String and Length.
  const len3 = format.len;
  assert(len3 &lt; size);
  @memcpy(str[0..len3], format[0..len3]);
  str[len3] = 0;
  return len3;
}
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L327-L382">(<strong>format_string2</strong> is here)</a></p>
<p>And the above function is called by <strong>fprintf and friends</strong>: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L382-L438">tcc-wasm.zig</a></p>
<div><pre><code>/// Implement the POSIX Function `fprintf`
export fn fprintf(stream: *FILE, format: [*:0]const u8, ...) c_int {

  // Prepare the varargs
  var ap = @cVaStart();
  defer @cVaEnd(&amp;ap);

  // Format the string
  var buf = std.mem.zeroes([512]u8);
  const format_slice = std.mem.span(format);
  const len = format_string(&amp;ap, &amp;buf, buf.len, format_slice);

  // TODO: Print to other File Streams.
  // Right now we assume it's stderr (File Descriptor 2)
  return @intCast(len);
}

// Do the same for sprintf, snprintf, vsnprintf
</code></pre></div>
<p><a href="https://gist.github.com/lupyuen/3e650bd6ad72b2e8ee8596858bc94f36">(See the <strong>Formatting Log</strong>)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#fix-the-varargs-functions">(Without <strong>comptime</strong>: Our code gets <strong>super tedious</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/app-syscall.jpg" alt="NuttX Apps make a System Call to print to the console"></p>
<h2 id="appendix-nuttx-system-call"><a href="#appendix-nuttx-system-call">12 Appendix: NuttX System Call</a></h2>
<p>Just now we saw a huge chunk of C Code that makes a <strong>NuttX System Call</strong>…</p>
<ul>
<li><a href="https://lupyuen.codeberg.page/articles/tcc#hello-nuttx"><strong>“Hello NuttX!”</strong></a></li>
</ul>
<p><em>Why so complicated?</em></p>
<p>We refer to the Sample Code for <a href="https://lupyuen.codeberg.page/articles/app#nuttx-app-calls-nuttx-kernel"><strong>NuttX System Calls (ECALL)</strong></a>. Rightfully this <strong>shorter version</strong> should work…</p>
<div><pre><code>// Make NuttX System Call to write(fd, buf, buflen)
const unsigned int nbr = 61; // SYS_write
const void *parm1 = 1;       // File Descriptor (stdout)
const void *parm2 = "Hello, World!!\n"; // Buffer
const void *parm3 = 15; // Buffer Length

// Execute ECALL for System Call to NuttX Kernel
register long r0 asm("a0") = (long)(nbr);
register long r1 asm("a1") = (long)(parm1);
register long r2 asm("a2") = (long)(parm2);
register long r3 asm("a3") = (long)(parm3);

asm volatile (
  // ECALL for System Call to NuttX Kernel
  "ecall \n"

  // NuttX needs NOP after ECALL
  ".word 0x0001 \n"

  // Input+Output Registers: None
  // Input-Only Registers: A0 to A3
  // Clobbers the Memory
  :
  : "r"(r0), "r"(r1), "r"(r2), "r"(r3)
  : "memory"
);
</code></pre></div>
<p>Strangely TCC generates <a href="https://github.com/lupyuen/tcc-riscv32-wasm#ecall-for-nuttx-system-call"><strong>mysterious RISC-V Machine Code</strong></a> that mashes up the RISC-V Registers…</p>
<div><pre><code>main():
// Prepare the Stack
   0:  fc010113  add     sp, sp, -64
   4:  02113c23  sd      ra, 56(sp)
   8:  02813823  sd      s0, 48(sp)
   c:  04010413  add     s0, sp, 64
  10:  00000013  nop
  14:  fea43423  sd      a0, -24(s0)
  18:  feb43023  sd      a1, -32(s0)

// Correct: Load Register A0 with 61 (SYS_write)
  1c:  03d0051b  addw    a0, zero, 61
  20:  fca43c23  sd      a0, -40(s0)

// Nope: Load Register A0 with 1?
// Mixed up with Register A1! (Value 1)
  24:  0010051b  addw    a0, zero, 1
  28:  fca43823  sd      a0, -48(s0)

// Nope: Load Register A0 with "Hello World"?
// Mixed up with Register A2!
  2c:  00000517  auipc   a0,0x0  2c: R_RISCV_PCREL_HI20  L.0
  30:  00050513  mv      a0,a0   30: R_RISCV_PCREL_LO12_I        .text
  34:  fca43423  sd      a0, -56(s0)

// Nope: Load Register A0 with 15?
// Mixed up with Register A3! (Value 15)
  38:  00f0051b  addw    a0, zero, 15
  3c:  fca43023  sd      a0, -64(s0)

// Execute ECALL with Register A0 set to 15.
// Nope: A0 should be 61!
  40:  00000073  ecall
  44:  0001      nop
</code></pre></div>
<p>Thus we <a href="https://github.com/lupyuen/tcc-riscv32-wasm#ecall-for-nuttx-system-call"><strong>hardcode Registers A0 to A3</strong></a> in RISC-V Assembly: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test-nuttx.js#L55-L97">test-nuttx.js</a></p>
<div><pre><code>// Load 61 to Register A0 (SYS_write)
addi  a0, zero, 61

// Load 1 to Register A1 (File Descriptor)
addi  a1, zero, 1

// Load 0xc0101000 to Register A2 (Buffer)
lui   a2, 0xc0
addiw a2, a2, 257
slli  a2, a2, 0xc

// Load 15 to Register A3 (Buffer Length)
addi  a3, zero, 15

// ECALL for System Call to NuttX Kernel
ecall

// NuttX needs NOP after ECALL
.word 0x0001
</code></pre></div>
<p>And it prints “Hello World”!</p>
<p><strong>TODO:</strong> Is there a workaround? Do we paste the ECALL Assembly Code ourselves? <a href="https://github.com/lupyuen/tcc-riscv32-wasm#fix-missing-printf-in-nuttx-app"><strong>NuttX Libraries</strong></a> won’t link with TCC</p>
<p><a href="https://gist.github.com/lupyuen/55a4d4cae26994aa673e6d8451716b27">(See the <strong>TCC WebAssembly Log</strong>)</a></p>
<p><em>What’s with the <code>addi</code> and <code>nop</code>?</em></p>
<p>TCC won’t assemble the “<strong><code>li</code></strong>” and “<strong><code>nop</code></strong>” instructions.</p>
<p>So we used this <a href="https://riscvasm.lucasteske.dev/#"><strong>RISC-V Online Assembler</strong></a> to assemble the code above.</p>
<p>“<strong><code>addi</code></strong>” above is the longer form of “<strong><code>li</code></strong>”, which TCC won’t assemble…</p>
<div><pre><code>// Load 61 to Register A0 (SYS_write)
// But TCC won't assemble `li a0, 61`
// So we do this...

// Add 0 to 61 and save to Register A0
addi a0, zero, 61
</code></pre></div>
<p>“<strong><code>lui / addiw / slli</code></strong>” above is our expansion of “<strong><code>li a2, 0xc0101000</code></strong>”, which TCC won’t assemble…</p>
<div><pre><code>// Load 0xC010_1000 to Register A2 (Buffer)
// But TCC won't assemble `li a2, 0xc0101000`
// So we do this...

// Load 0xC0 &lt;&lt; 12 into Register A2 (0xC0000)
lui   a2, 0xc0

// Add 257 to Register A2 (0xC0101)
addiw a2, a2, 257

// Shift Left by 12 Bits (0xC010_1000)
slli  a2, a2, 0xc
</code></pre></div>
<p><em>How did we figure out that the buffer is at 0xC010_1000?</em></p>
<p>We saw this in our <a href="https://gist.github.com/lupyuen/a715e4e77c011d610d0b418e97f8bf5d#file-nuttx-tcc-app-log-L32-L42"><strong>ELF Loader Log</strong></a>…</p>
<div><pre><code>NuttShell (NSH) NuttX-12.4.0
nsh&gt; a.out
...
Read 576 bytes from offset 512
Read 154 bytes from offset 64
1. 00000000-&gt;c0000000
Read 0 bytes from offset 224
2. 00000000-&gt;c0101000
Read 16 bytes from offset 224
3. 00000000-&gt;c0101000
4. 00000000-&gt;c0101010
</code></pre></div>
<p>Which says that the NuttX ELF Loader copied 16 bytes from our NuttX App Data Section (<strong><code>.data.ro</code></strong>) to <strong><code>0xC010_1000</code></strong>.</p>
<p>That’s all 15 bytes of <em>“Hello, World!!\n”</em>, including the terminating null.</p>
<p>Thus our buffer in NuttX QEMU should be at <strong><code>0xC010_1000</code></strong>.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser">(<strong>NuttX WebAssembly Emulator</strong> uses <strong><code>0x8010_1000</code></strong> instead)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/app#kernel-starts-a-nuttx-app">(More about the <strong>NuttX ELF Loader</strong>)</a></p>
<p><em>Why do we Loop Forever?</em></p>
<div><pre><code>// Omitted: Execute ECALL for System Call to NuttX Kernel
asm volatile ( ... );

// Loop Forever
for(;;) {}
</code></pre></div>
<p>That’s because NuttX Apps are not supposed to <a href="https://github.com/lupyuen/tcc-riscv32-wasm#fix-missing-printf-in-nuttx-app"><strong>Return to NuttX Kernel</strong></a>.</p>
<p>We should call the NuttX System Call <strong><code>__exit</code></strong> to terminate peacefully.</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-demo.png" alt="Online Demo of Apache NuttX RTOS"></p>
<p><a href="https://nuttx.apache.org/demo/"><em>Online Demo of Apache NuttX RTOS</em></a></p>
<h2 id="appendix-build-nuttx-for-qemu"><a href="#appendix-build-nuttx-for-qemu">13 Appendix: Build NuttX for QEMU</a></h2>
<p>Here are the steps to build and run <strong>NuttX for QEMU 64-bit RISC-V</strong> (Kernel Mode)</p>
<ol>
<li>
<p>Install the Build Prerequisites, skip the RISC-V Toolchain…</p>
<p><a href="https://lupyuen.codeberg.page/articles/nuttx#install-prerequisites"><strong>“Install Prerequisites”</strong></a></p>
</li>
<li>
<p>Download the RISC-V Toolchain for <strong>riscv64-unknown-elf</strong>…</p>
<p><a href="https://lupyuen.codeberg.page/articles/riscv#appendix-download-toolchain-for-64-bit-risc-v"><strong>“Download Toolchain for 64-bit RISC-V”</strong></a></p>
</li>
<li>
<p>Download and configure NuttX…</p>
<div><pre><code>## Download NuttX Source Code
mkdir nuttx
cd nuttx
git clone https://github.com/apache/nuttx nuttx
git clone https://github.com/apache/nuttx-apps apps

## Configure NuttX for QEMU RISC-V 64-bit (Kernel Mode)
cd nuttx
tools/configure.sh rv-virt:knsh64
make menuconfig
</code></pre></div>
<p>We use <a href="https://lupyuen.codeberg.page/articles/semihost#nuttx-apps-filesystem"><strong>Kernel Mode</strong></a> because it allows loading of NuttX Apps as ELF Files.</p>
<p>(Instead of Statically Linking the NuttX Apps into NuttX Kernel)</p>
</li>
<li>
<p>(Optional) To enable <strong>ELF Loader Logging</strong>, select…</p>
<p>Build Setup &gt; Debug Options &gt; Binary Loader Debug Features:</p>
<ul>
<li>Enable “Binary Loader Error, Warnings and Info”</li>
</ul>
</li>
<li>
<p>(Optional) To enable <strong>System Call Logging</strong>, select…</p>
<p>Build Setup &gt; Debug Options &gt; SYSCALL  Debug Features:</p>
<ul>
<li>Enable “SYSCALL Error, Warnings and Info”</li>
</ul>
</li>
<li>
<p>Save and exit <strong>menuconfig</strong>.</p>
</li>
<li>
<p>Build the <strong>NuttX Kernel and NuttX Apps</strong>…</p>
<div><pre><code>## Build NuttX Kernel
make -j 8

## Build NuttX Apps
make -j 8 export
pushd ../apps
./tools/mkimport.sh -z -x ../nuttx/nuttx-export-*.tar.gz
make -j 8 import
popd
</code></pre></div></li>
</ol>
<p>This produces the NuttX ELF Image <strong><code>nuttx</code></strong> that we may boot on QEMU RISC-V Emulator…</p>
<div><pre><code>## For macOS: Install QEMU
brew install qemu

## For Debian and Ubuntu: Install QEMU
sudo apt install qemu-system-riscv64

## Boot NuttX on QEMU 64-bit RISC-V
qemu-system-riscv64 \
  -semihosting \
  -M virt,aclint=on \
  -cpu rv64 \
  -smp 8 \
  -bios none \
  -kernel nuttx \
  -nographic
</code></pre></div>
<p>NuttX Apps are located in <strong><code>apps/bin</code></strong>.</p>
<p>We may copy our <strong>RISC-V ELF <code>a.out</code></strong> to that folder and run it…</p>
<div><pre><code>NuttShell (NSH) NuttX-12.4.0-RC0
nsh&gt; a.out
Hello, World!!
</code></pre></div>
<p><img src="https://lupyuen.codeberg.page/images/tcc-posix.jpg" alt="POSIX Functions aren’t supported for TCC in WebAssembly"></p>
<h2 id="appendix-missing-functions"><a href="#appendix-missing-functions">14 Appendix: Missing Functions</a></h2>
<p>Remember we said that POSIX Functions aren’t supported in WebAssembly? (Pic above)</p>
<ul>
<li><a href="https://lupyuen.codeberg.page/articles/tcc#posix-for-webassembly"><strong>“POSIX for WebAssembly”</strong></a></li>
</ul>
<p>We dump the <strong>Compiled WebAssembly</strong> of TCC Compiler, and we discover that it calls <strong>72 POSIX Functions</strong>…</p>
<div><pre><code>## Dump the Compiled WebAssembly
## for TCC Compiler `tcc.o`
$ sudo apt install wabt
$ wasm-objdump -x tcc.o

Import:
 - func[0] sig=1  &lt;env.strcmp&gt; &lt;- env.strcmp
 - func[1] sig=12 &lt;env.memset&gt; &lt;- env.memset
 - func[2] sig=1  &lt;env.getcwd&gt; &lt;- env.getcwd
 ...
 - func[69] sig=2  &lt;env.localtime&gt; &lt;- env.localtime
 - func[70] sig=13 &lt;env.qsort&gt;     &lt;- env.qsort
 - func[71] sig=19 &lt;env.strtoll&gt;   &lt;- env.strtoll
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#missing-functions-in-tcc-webassembly">(See the <strong>Complete List</strong>)</a></p>
<p>Do we need all 72 POSIX Functions? We scrutinise the list…</p>
<hr>
<p><strong>Filesystem Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L87-L166"><em>(Implemented here)</em></a></p>
<p>We’ll simulate these functions for WebAssembly, by embedding the simple <a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly"><strong>ROM FS Filesystem</strong></a> into our Zig Wrapper…</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly">(See the updates for <strong>ROM FS Filesystem</strong>)</a></p>
<hr>
<p><strong>Varargs Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L186-L445"><em>(Implemented here)</em></a></p>
<p>As discussed earlier, Varargs will be <a href="https://lupyuen.codeberg.page/articles/tcc#fearsome-fprintf-and-friends"><strong>tricky to implement</strong></a> in Zig. Probably we should do it in C.</p>
<p><a href="https://github.com/marler8997/ziglibc/blob/main/src/printf.c#L32-L191">(Similar to <strong>ziglibc</strong>)</a></p>
<p>Right now we’re doing simple <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-pattern-matching"><strong>Pattern Matching</strong></a>. But it might not be sufficient when TCC compiles Real Programs…</p>
<hr>
<p><strong>String Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L541-L776"><em>(Implemented here)</em></a></p>
<p>We’ll borrow the String Functions from <a href="https://github.com/marler8997/ziglibc/blob/main/src/cstd.zig"><strong>ziglibc</strong></a>…</p>
<hr>
<p><strong>Semaphore Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L166-L186"><em>(Implemented here)</em></a></p>
<p>Not sure why TCC uses Semaphores? Maybe we’ll understand when we support <strong><code>#include</code></strong> files.</p>
<p>(Where can we borrow the Semaphore Functions?)</p>
<hr>
<p><strong>Standard Library</strong></p>
<p><strong>qsort</strong> isn’t used right now. Maybe for the Linker later?</p>
<p>(Borrow <strong>qsort</strong> from where? We can probably implement <strong>exit</strong>)</p>
<hr>
<p><strong>Time and Math Functions</strong></p>
<p>Not used right now, maybe later.</p>
<p>(Anyone can lend us <strong>ldexp</strong>? How will we do the Time Functions? Call out to JavaScript to <a href="https://lupyuen.codeberg.page/articles/lvgl4#appendix-handle-lvgl-timer"><strong>fetch the time</strong></a>?)</p>
<hr>
<p><strong>Outstanding Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L776-L855"><em>(Implemented here)</em></a></p>
<p>We have implemented (fully or partially) <strong>48 POSIX Functions</strong> from above.</p>
<p>The ones that we haven’t implemented? These <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L776-L855"><strong>24 POSIX Functions will Halt</strong></a> when TCC WebAssembly calls them…</p>

    


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Researchers taking on fraudulent science (117 pts)]]></title>
            <link>https://www.analystnews.org/posts/plagiarism-paper-mills-and-profit-these-scientists-are-fighting-the-epidemic-of-fraudulent-science-research</link>
            <guid>39244601</guid>
            <pubDate>Sat, 03 Feb 2024 20:49:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.analystnews.org/posts/plagiarism-paper-mills-and-profit-these-scientists-are-fighting-the-epidemic-of-fraudulent-science-research">https://www.analystnews.org/posts/plagiarism-paper-mills-and-profit-these-scientists-are-fighting-the-epidemic-of-fraudulent-science-research</a>, See on <a href="https://news.ycombinator.com/item?id=39244601">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="tps_slideContainer_15454">

<p>Nearly two decades ago, British anesthesiologist John Carlisle published an <a href="https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD004125.pub3/information#versionTable" target="_blank" rel="noopener">article</a> on preventing postoperative nausea in the Cochrane Database of Systematic Reviews.&nbsp;</p>



<p>Afterwards, however, Carlisle wasn’t celebrating his publication. Instead, he decided to look more closely at some of the papers included in his literature review. A staggering 68 of the 737 papers reviewed were written by a researcher named Yoshitaka Fujii.</p>



<p>“Peculiar patterns were evident in his papers,” Carlisle tells Analyst News. Rates of headaches or dizziness, for instance, were often precisely the same across groups of patients in Fujii’s clinical trials. “That makes one suspicious. If you see it in not just one paper but lots of papers, then something can’t be right.”</p>



<p>Statistically, Carlisle’s investigation showed, such results were near-impossible — and the data in Fujii’s trials was much more likely fabricated. Indeed, of the 68 papers that Fujii had authored and Carlisle had included, 63 were ultimately retracted from the journals in which they were published.</p>



<p>“We need to be looking out for poor science, whether it’s fabricated or whether it’s unintentionally false,” says Carlisle, a longtime editor for the journal <em>Anaesthesia </em>who has <a href="https://www.nature.com/articles/d41586-019-02241-z" target="_blank" rel="noopener">developed statistical techniques</a> to help identify problematic medical research. His methods have been adopted by at least two top medical journals — and have exposed scientific misconduct and errors in hundreds of papers that have been corrected or retracted.</p>



<p>Experts say a rampant culture of “publish or perish,” plus the rise of <a href="https://www.nature.com/articles/d41586-023-00191-1" target="_blank" rel="noopener">AI-based writing tools</a>, <a href="https://www.newyorker.com/tech/annals-of-technology/paging-dr-fraud-the-fake-publishers-that-are-ruining-science" target="_blank" rel="noopener">predatory science journals</a> and <a href="https://www.science.org/content/article/fake-scientific-papers-are-alarmingly-common" target="_blank" rel="noopener">paper mills</a>, has tainted scientific publication. Per some estimates, <a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002165" target="_blank" rel="noopener">fewer than half</a> of research studies published each year are credible. Such misconduct wastes time and money, damages trust in science, and can even <a href="https://www.science.org/doi/10.1126/science.362.6413.394-a" target="_blank" rel="noopener">endanger patients</a>.</p>



<p>Last year, more than 10,000 research papers were <a href="https://www.nature.com/articles/d41586-023-03974-8" target="_blank" rel="noopener">retracted</a>, marking an all-time high. These retractions could have occurred for a number of reasons: The results in the paper may be considered inaccurate, the paper may have contained plagiarized work, or the authors may have conflicts of interest or used unethical research practices.&nbsp;</p>



<p>The retractions themselves are a good sign, experts say, demonstrating that inaccurate or unethical research is being caught. But scientists worry the retracted articles are only a fraction of all the fraudulent work out there. And a small but growing number of researchers have devoted themselves to investigating and exposing this bad science.</p>



<h2>Meet the science sleuths</h2>



<p>Microbiologist Elisabeth Bik was a researcher at Stanford University when she stumbled across a few cases of unethical science. First her own work had been plagiarized by another scientist, and then she came across a paper reusing the same image to represent two separate experiments.&nbsp;</p>



<p>A decade later, Bik is now a full-time “science integrity consultant,” sleuthing out fraudulent work in research papers. Her speciality: identifying falsified images.&nbsp;</p>



<p>Bik says one of the biggest sources of fraudulent research is paper mills, which she describes as “networks or cartels of people who make a profit selling fake or very low-quality papers to authors who need an authorship.” Per <a href="https://www.nature.com/articles/d41586-023-03464-x" target="_blank" rel="noopener">some</a> <a href="https://publicationethics.org/node/55256" target="_blank" rel="noopener">estimates</a>, about 2% of papers are produced by paper mills.&nbsp;</p>



<blockquote>
<p>“You just can’t work in a field like that. It’s like swimming in garbage. What you have to do is get out of the pool and start trying to clean it up.”&nbsp;</p>
</blockquote>



<p>These paper mills may plagiarize PhD theses found online or offer general templates for research articles, making modifications for customers. Some researchers even pay paper mills to help boost their citation index, which indicates how often their work has been cited by other scientists (generally, more established or well-reputed researchers have high citation indices). Paper mills can generate articles that contain multiple citations to a specific scientist’s work and thus serve as “vessels for citations,” Bik says.</p>



<p>Cancer biology professor Jennifer Byrne, the director of biobanking at New South Wales Health Pathology, began researching publication integrity and research fraud after becoming overwhelmed by the amount of bogus research in her field.&nbsp;</p>



<p>“I feel like I didn’t have any choice, because now in the field that I used to work, I would estimate that there are far more paper mill papers than general papers,” says Byrne, who now works with the Association for Interdisciplinary Meta-Research and Open Science (AIMOS), an international organization promoting trustworthy research practices.</p>



<p>The sheer volume of fraud was appalling, she says, with many repetitive papers and fatal errors.</p>



<p>“You just can’t work in a field like that. It’s like swimming in garbage. What you have to do is get out of the pool and start trying to clean it up.”&nbsp;</p>



<h2>Why fraud flourishes</h2>



<p>In a community that values scientific reasoning and evidence, it may be hard to believe that paper mills and fraudulent research can thrive. But there are financial incentives at play.&nbsp;</p>



<p>For journal editors and publishers, it’s economically advantageous to publish papers, so research is sometimes published without thorough screening. The false information in fraudulent papers can be nuanced or well-disguised, making it difficult for journals to quickly distinguish fraudulent papers from authentic ones.&nbsp;</p>



<p>Experts point to a clear incentivization of quantity, rather than quality, of research output — both with the publishing industry and academic institutions. Publishing articles is critical for success and distinction in scientists’ fields. Universities often require PhD candidates or faculty members to publish a certain number of papers.&nbsp;</p>



<p>Many researchers, however, end up finding negative results —&nbsp;those that do not support their initial hypotheses. While such studies are crucial to the scientific literature, the “whole industry” of journals encourages scientists to publish positive results, Bik explains. Facing pressure to get published but with no compelling results of their own, researchers may turn to paper mills instead.</p>



<p>“The second problem is increasing commercialization of the publication enterprise, where more journals are owned by very, very large companies who are profit-driven,” says Byrne. “Paper mills … are poised to step in when all journals and publishers really care about is that bottom line. Because they will produce for money.”</p>



<p>Part of the problem also lies in the increasing popularity of open-access journals, which do not require readers to pay subscriptions. These open-access journals have helped democratize access to scientific knowledge, experts say, but some publishers are misusing this model and publishing nearly everything they are sent.</p>



<p>“Many open-access publishers publish on a ‘pay-per-paper’ model that drives a much more commercial mindset within publishing,” Bik tells Analyst News. The end result is that journals have compromised the quality of articles for the quantity that they can publish.</p>



<h2>‘A work in progress’</h2>



<p>Experts say systemic changes are needed to reduce the number of fraudulent papers.</p>



<p>Bik says that journals should move towards publishing more negative results — in fact, understanding negative results in one’s field is critical for researchers to design future experiments of their own.&nbsp;</p>



<p>Governments and regulators, too, can work to remove ads for paper mills across social media sites, she says. And open communication about better science research and research practices — such as through conferences like the ones AIMOS organizes — can enable scientists to together uphold ethical standards in their fields.&nbsp;</p>



<p>Up until recently, it was largely individuals like Carlisle, Bik and Byrne detecting fraudulent research on their own. More recently, larger organizations are playing key roles.&nbsp;</p>



<p>The analytics company Clarivate, for example, maintains a list of reputable journals in its Web of Science platform. Periodically, the list is reviewed, and journals that fail to adhere to Clarivate’s standards are removed. Back in March 2023, about 50 journals were <a href="https://www.science.org/content/article/fast-growing-open-access-journals-stripped-coveted-impact-factors" target="_blank" rel="noopener">pulled</a> from the Web of Science, including well-known journals like the <em>International Journal of Environmental Research and Public Health</em>, which had published over 17,000 articles the year prior.</p>



<blockquote>
<p>“The reality is science is constructed by humans who are doing the best that they can. And it’s a work in progress.”</p>
</blockquote>



<p>Public trust in science has <a href="https://www.bloomberg.com/opinion/articles/2023-11-26/covid-public-health-mistakes-fueled-mistrust-in-scientists" target="_blank" rel="noopener">eroded</a> in recent years with several high-profile missteps during the COVID-19 pandemic. Still, experts emphasize the need for continuing to trust the scientific process, acknowledging the limitations of scientific publishing while understanding that the majority of scientists are honest and well-intentioned.&nbsp;</p>



<p>In her own work debunking scientific misconduct, Bik treads carefully to ensure her work doesn’t promote dangerous anti-science narratives.&nbsp;</p>



<p>“It’s a double-edged sword because on one hand, I am worried about fraud in science,” she says. “On the other hand, I also think it’s a relatively small fraction, and I do not want to give the impression that all science is fraudulent. This is, I think, the danger of what I do.”</p>



<p>These scientists’ message to the public is to examine scientific information with a critical eye. Consumers of information in any field should strive to look for credible sources, cross-reference studies and consult experts.</p>



<p>The scientific method is a self-correcting system of inquiry that aims to uncover truths about the universe, experts remind. Overcoming the errors that are inevitably introduced to the scientific body of knowledge — whether from human error, bias or misconduct — demands skepticism and scrutiny from both researchers and the public.</p>



<p>“The world that we live in at the moment doesn’t have a lot of certainty — there are a lot of scary things happening,” Byrne says.&nbsp;</p>



<p>“That can lead people to look at science and think, ‘Oh, here’s some certainty at last.’ But the reality is science is constructed by humans who are doing the best that they can. And it’s a work in progress.”</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A brief history of the U.S. trying to add backdoors into encrypted data (2016) (494 pts)]]></title>
            <link>https://www.atlasobscura.com/articles/a-brief-history-of-the-nsa-attempting-to-insert-backdoors-into-encrypted-data</link>
            <guid>39244254</guid>
            <pubDate>Sat, 03 Feb 2024 20:11:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlasobscura.com/articles/a-brief-history-of-the-nsa-attempting-to-insert-backdoors-into-encrypted-data">https://www.atlasobscura.com/articles/a-brief-history-of-the-nsa-attempting-to-insert-backdoors-into-encrypted-data</a>, See on <a href="https://news.ycombinator.com/item?id=39244254">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article-body">
<p><span><img src="https://img.atlasobscura.com/cTZdkTIv1E4odFfr3bXu_WpdkaWAP1xldupsnW10WHY/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfU3Vw/ZXJjb21wdXRlcl9O/U0EtSUJNMzYwXzg1/LmpwZw.jpg" alt="" width="auto" data-kind="article-image" id="article-image-24797" data-src="https://img.atlasobscura.com/cTZdkTIv1E4odFfr3bXu_WpdkaWAP1xldupsnW10WHY/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfU3Vw/ZXJjb21wdXRlcl9O/U0EtSUJNMzYwXzg1/LmpwZw.jpg"></span></p>
<p><span>A government agent uses an NSA IBM 360/85 console in 1971 (Photo: <a href="https://commons.wikimedia.org/wiki/File:Supercomputer_NSA-IBM360_85.jpg">Wikimedia Commons/NSA</a>).</span></p>
<p><span><span>It’s been a weird week </span>for America’s most valuable company—a firm whose tech products have such consumer goodwill they got away with </span><em><span>forcing us to listen to U2</span></em><span>—who is poised to&nbsp;</span><a href="http://www.apple.com/customer-letter/"><span>go to court against its own government over its users’ right to privacy</span></a><span>. The government is invoking</span><span><a href="https://motherboard.vice.com/read/writs-and-giggles">&nbsp;an obscure law</a>&nbsp;</span><span>dating back almost to the founding of the country to force the company to comply. It’d be a pretty good movie. </span></p>
<p><span>But it’s just the most dramatic flare-up in a lengthy battle between government officials, cybersecurity experts, and the tech industry over how consumer’s technical data is protected, and whether or not the government has a right to access that information. </span></p>
<p><span>In fact, the government has actually won this fight before—secretly.&nbsp;</span></p>
<p><span>Throughout 2015, U.S. politicians and law enforcement officials such as FBI director James Comey have publicly lobbied for the insertion of cryptographic “backdoors” into software and hardware to allow law enforcement agencies to bypass authentication and access a suspect’s data surreptitiously. Cybersecurity experts have unanimously condemned the idea, pointing out that such backdoors would fundamentally undermine encryption and could exploited by criminals, among other issues. While a legal mandate or public agreement would be needed to allow evidence obtained via backdoors to be admissible in court, the NSA has long attempted—and occasionally succeeded—in placing backdoors for covert activities.</span></p>
<p><span><img src="https://img.atlasobscura.com/cFMKwNJo6am6FNH8kW3cgaKHEUINs5YsdNVNFymUifE/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfNjY1/NTc1OTYyNV81YTA2/MzdjZTc4X28uanBn.jpg" alt="" width="auto" data-kind="article-image" id="article-image-24799" data-src="https://img.atlasobscura.com/cFMKwNJo6am6FNH8kW3cgaKHEUINs5YsdNVNFymUifE/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfNjY1/NTc1OTYyNV81YTA2/MzdjZTc4X28uanBn.jpg"></span></p>
<p><span>An Enigma machine at Bletchley Park, long-rumored to be one of the first backdoored devices (Photo: <a href="https://flic.kr/p/b99vsi">Flickr/Adam Foster</a>).</span></p>
<p><span>One of the most important developments in cryptography was the Enigma machine, famously used to encode Nazi communications during World War II. For years, rumors have persisted that the NSA (then SSA) and their British counterparts in the Government Communications Headquarters collaborated with the Enigma’s manufacturer, Crypto AG, to place </span><a href="https://web.archive.org/web/20080202225034/http://www.inteldaily.com/?c=169&amp;a=4686"><span>backdoors into Enigma machines provided to certain countries</span></a><span> after World War II. Crypto AG has repeatedly denied the allegations, and in 2015 the </span><a href="http://www.bbc.com/news/uk-33676028"><span><em>BBC</em> sifted through 52,000 pages of declassified NSA documents</span></a><span> to find the truth. </span></p>
<p><span>The investigation revealed that while no backdoors were placed in the machines, there was a “gentlemen’s agreement” that Crypto AG would keep American and British intelligence appraised of “the technical specifications of different machines and which countries were buying which ones,” allowing analysts to decrypt messages much more quickly. Consider it a security “doggy-door.”</span></p>
<p><span>Next, in 1993, the NSA promoted “Clipper chips,” which were intended to protect private communications while still allowing law enforcement to access them. In 1994, researcher Matt Blaze </span><a href="http://www.crypto.com/papers/eesproto.pdf"><span>uncovered significant vulnerabilities in the “key escrow” system that allowed law enforcement access</span></a><span>, essentially making the chips useless. By 1996, </span><a href="http://arstechnica.com/information-technology/2015/12/what-the-government-shouldve-learned-about-backdoors-from-the-clipper-chip/"><span>Clipper chips were defunct</span></a><span>, as the tech industry adopted more secure, open encryption standards such as </span><a href="https://www.philzimmermann.com/EN/essays/WhyIWrotePGP.html"><span>PGP</span></a><span>.</span></p>
<p><span>In more recent years, the NSA was unequivocally caught inserting a backdoor into the Dual_EC_DRBG algorithm, a cryptographic algorithm that was supposed to generate random bit keys for encrypting data. The algorithm, developed in the early aughts, was championed by the NSA and included in NIST Special Publication 800-90, the official standard for random-number generators released in 2007. Within a matter of months, researchers discovered the backdoor, and awareness that the algorithm was insecure quickly spread, although it continued to be implemented in consumer software </span><a href="https://www.schneier.com/blog/archives/2007/12/dual_ec_drbg_ad.html"><span>Windows Vista</span></a><span>. What was really odd, as crypto expert Bruce Schneier explained i</span><a href="http://www.wired.com/2007/11/securitymatters-1115/"><span>n a 2007 essay published in </span><em><span>Wired</span></em></a><span>, was that Dual_EC_DRBG wasn’t even worth the NSA’s effort:</span></p>
<blockquote>
<p><span>It makes no sense as a trap door: It’s public, and rather obvious. It makes no sense from an engineering perspective: It’s too slow for anyone to willingly use it. And it makes no sense from a backwards-compatibility perspective: Swapping one random-number generator for another is easy.</span></p>
</blockquote>
<p><span><img src="https://img.atlasobscura.com/dsjgOOuhtF-s38VN6eBE_9rEnLJPt2z7u5H8kL9QngM/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfTVlL/LTc4X0NsaXBwZXJf/Y2hpcF9tYXJraW5n/cy5qcGc.jpg" alt="" width="auto" data-kind="article-image" id="article-image-24798" data-src="https://img.atlasobscura.com/dsjgOOuhtF-s38VN6eBE_9rEnLJPt2z7u5H8kL9QngM/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfTVlL/LTc4X0NsaXBwZXJf/Y2hpcF9tYXJraW5n/cy5qcGc.jpg"><span>A Chipper clip—one of the NSA’s unsuccessful backdoor attempts (Photo: <a href="https://commons.wikimedia.org/wiki/File:MYK-78_Clipper_chip_markings.jpg">Wikimedia Commons/Travis Goodspeed</a>).</span></span></p>
<p><span>Although the NSA’s effort puzzled crypto experts, </span><a href="http://www.wired.com/2013/09/nsa-backdoor/all/"><span>documents leaked by Edward Snowden in 2013</span></a><span> proved that the NSA did indeed build a backdoor into Dual_EC_DRBG and paid RSA, a computer security company, to include the compromised algorithm in its software.</span></p>
<p><span>These are the incidents that have been proven. There are, of course, numerous theories and insinuations that the NSA has made many more efforts along these lines—from </span><a href="http://www.cypherspace.org/adam/hacks/lotus-nsa-key.html"><span>backdoors in Lotus Notes</span></a><span> to </span><span><a href="http://www.networkworld.com/article/2458706/microsoft-subnet/about-those-alleged-backdoors-in-microsoft-products.html">persistent</a>&nbsp;</span><a href="https://web.archive.org/web/20000520001558/http://www.microsoft.com/security/bulletins/backdoor.asp"><span>allegations</span></a><span> that Microsoft routinely includes backdoors in its software. Additionally, the Snowden leak </span><a href="http://www.propublica.org/article/the-nsas-secret-campaign-to-crack-undermine-internet-encryption"><span>proved that the NSA is constantly working to decrypt common encryption standards</span></a><span>. </span></p>
<p><span>As our lives become more and more dominated by the digital, security experts have become </span><a href="https://www.washingtonpost.com/news/the-switch/wp/2016/01/11/the-debate-over-government-backdoors-into-encryption-isnt-just-happening-in-the-u-s/"><span>increasingly vocal</span></a><span> in their calls for truly secure encryption, and some governments have begun to listen. </span><a href="http://www.theregister.co.uk/2016/01/04/dutch_government_says_no_to_backdoors/"><span>Holland’s government</span></a><span> has agreed not to use backdoors and support open encryption standards, and despite calls to do so in response to the Paris terrorist attacks, </span><a href="http://fortune.com/2016/01/13/france-encryption/"><span>France refuses to implement a backdoor mandate</span></a><span>. Even former NSA director </span><a href="http://fortune.com/2016/02/19/hayden-apple-fbi/"><span>Michael Hayden has said that backdoors are a bad idea</span></a><span> (and he would know). As Apple vs. FBI wends its way through the courts, we are probably far from the end of this public battle. Whatever the results of this landmark case, the NSA’s classified efforts to subvert cryptography will likely continue.</span></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cork is displacing plastics and creating a billion-dollar industry (139 pts)]]></title>
            <link>https://www.washingtonpost.com/climate-solutions/2024/02/03/cork-sustainable-material/</link>
            <guid>39243816</guid>
            <pubDate>Sat, 03 Feb 2024 19:24:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/climate-solutions/2024/02/03/cork-sustainable-material/">https://www.washingtonpost.com/climate-solutions/2024/02/03/cork-sustainable-material/</a>, See on <a href="https://news.ycombinator.com/item?id=39243816">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/climate-solutions/2024/02/03/cork-sustainable-material/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[How much bigger could Earth be before rockets wouldn't work? (395 pts)]]></title>
            <link>https://space.stackexchange.com/questions/14383/how-much-bigger-could-earth-be-before-rockets-wouldnt-work</link>
            <guid>39243303</guid>
            <pubDate>Sat, 03 Feb 2024 18:36:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://space.stackexchange.com/questions/14383/how-much-bigger-could-earth-be-before-rockets-wouldnt-work">https://space.stackexchange.com/questions/14383/how-much-bigger-could-earth-be-before-rockets-wouldnt-work</a>, See on <a href="https://news.ycombinator.com/item?id=39243303">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainbar" role="main" aria-label="question and answers">
                
<div data-questionid="14383" data-position-on-page="0" data-score="98" id="question">
        

        

<div>
    <p><span>$\begingroup$</span></p><div itemprop="text">
                
<p><strong>hint:</strong> <em>Apparently the Tsiolkovsky rocket equation</em> does not actually say <em>that you can launch a conventional rocket into orbit around an arbitrarily large and massive body.</em></p>

<p>I'm looking for a number based on scaling the earth radius and maintaining the same average density. Must attain LEO, <em>which also gets faster</em> as the planet grows. Don Pettit's <a href="http://www.nasa.gov/mission_pages/station/expeditions/expedition30/tryanny.html" rel="noreferrer">Tyranny</a> mentioned in <a href="https://space.stackexchange.com/a/17426/12102">this nice answer</a> is fun, but does not present enough math.</p>

<p>On <em>this</em> Earth, rockets barely work. Payloads can only be a few percent of the total mass for <a href="https://en.wikipedia.org/wiki/Low_Earth_orbit" rel="noreferrer">LEO</a>, and less than one percent for deep space.</p>

<p>If we define slightly heavier Earths, say Earth<sub>1.1</sub>, Earth<sub>1.2</sub>... where the radii were 1.1, 1.2, etc. times that of Earth and the masses were 1.1<sup>3</sup>, 1.2<sup>3</sup>, etc. times the Earth's mass (in other words same average density, same "iron/rock ratio") what happens? Is there some point where chemical rockets simply will no longer be able to put things in space, or does the payload mass simply become ridiculously tiny? If there is a cut-off, is it different for LEO and deep space?</p>

<p>For our purposes, let's not explore alternative or hybrid launch systems or boost systems (such as balloons, planes, laser beams, space elevators etc.). Just stick to chemical propellant rockets.</p>

<p><strong>edit:</strong> here is a guide. So for a scaling factor <span>$f$</span>:</p>

<p><span>$$
r = f r_{earth}
$$</span>
<span>$$
m = f^3 m_{earth}
$$</span>
<span>$$
g = G \frac{m}{r^2} = \frac{f^3}{f^2}g_{earth} = f g_{earth}
$$</span>
<span>$$
H = \frac{kT}{gm_{molecule}} = f^{-1}H_{earth}
$$</span></p>

<p>We catch a little break here. Assuming same surface atmosphere composition, temperature and pressure (STP), the <a href="http://spacemath.gsfc.nasa.gov/astrob/7Page15.pdf" rel="noreferrer">scale height</a> H actually <em>decreases</em> with increasing <span>$f$</span>. (If we were "world builders" we should probably increase pressure to get more oxygen needed for moving in the higher gravity, but that's <a href="https://worldbuilding.stackexchange.com/">a different Stack Exchange</a>.)</p>

<p>As far as LEO <strong>altitude</strong> is concerned (thanks @Lex for <a href="https://space.stackexchange.com/questions/14383/how-much-bigger-could-earth-be-before-rockets-wouldt-work?noredirect=1#comment73460_14383">catching</a> that) one might define it as the same number of scale heights as would be on Earth. That's not really so useful because the density profiles of the bits of the atmosphere responsible for drag (<a href="https://en.wikipedia.org/wiki/Thermosphere" rel="noreferrer">Thermosphere</a> and <a href="https://en.wikipedia.org/wiki/Exosphere" rel="noreferrer">Exosphere</a> are affected by many phenomenon, including the solar wind, and don't scale at all like the lower layers. Nonetheless for historical reasons I'll leave the following, as it is not essential to the question:</p>

<p><span>$$
h_{LEO} = h_{LEOearth} \frac{H}{H_{earth}} = f^{-1} h_{LEOearth}
$$</span>
<span>$$
v_{LEO}=fv_{LEOearth}
$$</span></p>

<p>The LEO period is independent of the size of a planet, if the average density is fixed. However, the velocity of LEO <em>does</em> scale with radius!</p>
    </div>

        

    <div>
    <div>
        <p>
            asked <span title="2016-03-09 07:45:39Z">Mar 9, 2016 at 7:45</span>
        </p>
        
    </div>
    <div>
        <a href="https://space.stackexchange.com/users/12102/uhoh"><p><img src="https://i.stack.imgur.com/pL4Iw.png?s=64&amp;g=1" alt="uhoh's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://space.stackexchange.com/users/12102/uhoh">uhoh</a><span itemprop="name">uhoh</span></p><p><span title="reputation score 148,425" dir="ltr">148k</span><span>51 gold badges</span><span>471 silver badges</span><span>1463 bronze badges</span>
        </p>
    </div>
</div>
    <p><span>$\endgroup$</span>
</p></div>




            <p><span itemprop="commentCount">31</span></p>
    </div>



                
                
                <div id="answers">
                    


                                    
<div id="answer-17576" data-answerid="17576" data-parentid="14383" data-score="137" data-position-on-page="1" data-highest-scored="1" data-question-has-accepted-highest-score="1" itemprop="acceptedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    <p><span>$\begingroup$</span></p><div itemprop="text">
<p>Because linear increases in delta-v require exponential increases in mass, small changes to the assumptions you make about fuel tank structural mass and engine thrust-to-weight ratio start to make very large changes in the final size of the rocket.</p>

<p>For example, if you're getting off a 3.6g planet with a 7-stage rocket, the difference between 88% fuel fraction and 92% fuel fraction yields about a 10:1 difference in the total mass of the rocket.</p>

<p>So I don't think it's really reasonable to talk about ultimate theoretical limits; too many engineering factors are involved.</p>

<p>Locking down a lot of variables, I can tell you what kind of rocket you'd need for a given surface g, though. Let's make these assumptions:</p>

<ul>
<li>We are placing 1 ton of payload into low planetary orbit.</li>
<li>Required delta-v to reach orbit, including atmospheric and gravity losses, is 10,000m/s per surface g. Seems to hold for Earth, Mars, and the "Earthtoo" which was discussed <a href="https://space.stackexchange.com/questions/17457/can-planet-earthtoo-put-a-tooian-in-orbit-too/17459#17459">in another Q/A</a>. </li>
<li>We can build rocket stages of arbitrary size, with a tankage propellant fraction of 90%; the rocket stage mass is the tank mass plus the engine mass -- ullage rockets, interstage, etc. is all handwaved out.</li>
<li>We have an infinite supply of Apollo-era rocket engines: RL-10, J-2, M-1, H-1, and F-1.</li>
<li>First-stage TWR at ignition must be at least 1.2 (relative to local gravity)</li>
<li>Middle-stage TWR at ignition must be at least 0.8</li>
<li>Final-stage TWR at ignition must be at least 0.5</li>
</ul>

<p>Given those assumptions, here is a table of surface gravity, stage count, first-stage engines, and total rocket mass.</p>

<pre><code>Surface                         First        Total       Saturn V 
Gravity   Stages                Stage      Mass, t     Equivalent
 0.5           2             1x RL-10          4.5
 1.0           3             1x   H-1         49.4          0.02
 1.5           3             1x   F-1        249.2           0.1
 2.0           4             5x   F-1       1329.0           0.5
 2.5           5            40x   F-1       8500.9             3
 3.0           6           274x   F-1      50722.2            17
 3.5           7          2069x   F-1     331430.9           100
 4.0           8         20422x   F-1    2836598.4           950
 4.5           8        392098x   F-1   47 million         15000
 5.0           9    3.5 million   F-1  391 million        130000
 6.0          11    400 million   F-1   38 billion      millions
10.0          18        2.88e19   F-1      1.65e21  quadrillions
</code></pre>

<p>Up above 10g, something really interesting happens that is kind of a theoretical limit. The mass of the rocket reaches <em>a measurable fraction of the mass of the entire planet</em> it's launching from.</p>

<p>At 10.3g, rocket mass is 0.035 of the mass of the planet.
10.4g, rocket mass is one fifth of the mass of the planet. This doesn't actually alter the ∆v requirement -- we're going into orbit around the rocket/planet barycenter!
At 10.47g, the rocket <em>is</em> the planet, and we're... just... chewing it up entirely, pulverizing it in a dust cloud expanding at 4km/s.</p>

<p>These extreme conclusions appear to be corroborated by <a href="https://arxiv.org/pdf/1803.11384.pdf" rel="noreferrer">this independently derived paper</a>, which explores some other related aspects of super-Earth-based chemical rockets. </p>

<p>Another consideration recently brought up by user @uhoh is that as the linear scale of a given rocket stage increases, its mass, and thus the required thrust force to lift it, goes up by the cube of the scale, but the area available at the base of the rocket to mount engines goes up only by the square of the scale; this problem is made even worse here by the increasing surface gravity. The Saturn V was just about at the point where this relation starts to become problematic; the outboard engines on its first stage are mounted at the very edge of the stage in order to make room for their nozzles to gimbal. </p>

<p>Solid rockets don't have the same dimensional constraints, and have very good thrust-to-weight and thrust-to-cost ratios, so they're probably more likely to be used in lower stages for these very large rockets.</p>

<p>Stages much larger than the Saturn V first stage would need to address this with some combination of being shorter and squatter, or compromising engine gimbal range, or mounting engines in pods surrounding the tankage, and there might be fairly hard engineering limits at some point for those reasons. At the 3g mark, for example, the 274 first-stage engines would require a stage about 90 meters in diameter and 9 meters tall, at which point the engineering inefficiencies associated with the fuel tank proportions will be becoming serious.   </p>
    </div>
    <div>
            
            <div>
    
    <div>
        <a href="https://space.stackexchange.com/users/21600/stayontarget"><p><img src="https://i.stack.imgur.com/PK4P1.jpg?s=64&amp;g=1" alt="StayOnTarget's user avatar" width="32" height="32"></p></a>
    </div>
    <div>
        <p><a href="https://space.stackexchange.com/users/21600/stayontarget">StayOnTarget</a></p><p><span title="reputation score " dir="ltr">1,121</span><span>1 gold badge</span><span>7 silver badges</span><span>17 bronze badges</span>
        </p>
    </div>
</div>


            <div>
    <div>
        <p>
            answered <span title="2016-07-30 20:53:41Z">Jul 30, 2016 at 20:53</span>
        </p>
        
    </div>
    <div>
        <a href="https://space.stackexchange.com/users/195/russell-borogove"><p><img src="https://www.gravatar.com/avatar/659060fcfde804c6582d87b432dfa497?s=64&amp;d=identicon&amp;r=PG" alt="Russell Borogove's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://space.stackexchange.com/users/195/russell-borogove">Russell Borogove</a><span itemprop="name">Russell Borogove</span></p><p><span title="reputation score 167,816" dir="ltr">168k</span><span>13 gold badges</span><span>591 silver badges</span><span>697 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    <p><span>$\endgroup$</span>
</p></div>




            <p><span itemprop="commentCount">17</span></p>
    </div>


                                    
<div id="answer-14384" data-answerid="14384" data-parentid="14383" data-score="30" data-position-on-page="2" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    <p><span>$\begingroup$</span></p><div itemprop="text">
<p>First, let us look at the <a href="https://en.wikipedia.org/wiki/Tsiolkovsky_rocket_equation" rel="nofollow noreferrer">rocket equation</a>:</p>
<p><span>$$\Delta v=\ln \left(\frac{m_0}{m_f}\right)v_e$$</span></p>
<p>That tells how much a rocket can change its velocity (the <span>$\Delta v$</span>). The requirements for reaching a higher velocity for a minimal orbit would increase on your heavier Earth. (For constant density it is proportional to the radius.)</p>
<p>How can we increase the <span>$\Delta v$</span> of the rocket to keep up? We can increase the exhaust velocity, <span>$v_e$</span>, of the engine, but that cut-off is around 5000 m/s for chemical engines. The other thing we could do is increasing the mass ratio of the rocket <span>$\left(\frac{m_0}{m_f}\right)$</span>. That is problematic too, as we can not really make the fuel tanks out of soap bubbles. Staging is the option left, you could place a big rocket under a small rocket to get a little more change in velocity. Then you are getting a linear benefit for an exponential expense.</p>
<p>As an example, the Saturn V rocket got into LEO (~9000 m/s), sent a payload towards the Moon (3120 m/s), the service module slowed the stack into LMO (820 m/s), and finally the LM landed and took off again (2*1720 m/s). There are still some unused fuel left in the service module then, so let us just call the total <span>$\Delta v$</span> of the Saturn V/Apollo 17 km/s. That is less than the requirements for a 2x radius Earth. The Apollo program was pretty expensive [citation needed], so it may take a while before a nation of a 2x Earth world attempts to go into orbit. The limit is, as you state, the ridiculously low payload ratio.</p>
<p>Another consideration is the increased surface gravity. (That scales linearly with diameter at constant density). That requires the rocket to have a higher thrust to weight ratio, and that will increase the dry mass, reducing the possible <span>$\Delta v$</span>. (It also increases gravity losses, but that is mostly compensated by the lower scale height of the planet, reducing drag losses).</p>
<p>Eventually, the gravity is so high that even the most powerful engine can not lift itself from the ground. That at least is a definitive limit.</p>
<p><strong>A more theoretical consideration, is <span>$\Delta v$</span> requirements actually a finite limit?</strong></p>
<p>Surprisingly, it is not. Remember what I said about staging earlier: "you are getting a linear benefit for an exponential expense". But there is not limit to what we can expend! Consider the following scenario: We add more and more stages at the bottom of the rocket, each of them has the same mass as all the stages on top of it. Then burning each of them gives the same mass ratio between before and after, therefore each of them are supplying
the same amount of <span>$\Delta v$</span>. To add 10 times that amount, you need 10 stages each doubling the mass. To add 100 times that amount, you need to double a hundred times. The mass grows ridiculously fast, even doubling 10 times are over a thousand times more. But why should we stop :)</p>
<p><strong>But can we <em>really</em> continue to add exponentially larger stages for ever?</strong></p>
<p>After a while, other problems show up. For instance: Rockets are long and thin, to minimize drag. That shape can not be kept for very large rockets. The reason not is the <a href="https://en.wikipedia.org/wiki/Square-cube_law" rel="nofollow noreferrer">square cube law</a>. Conserving the same dimensional proportions, a rocket twice the height has 8 times more mass. But the base area of the rocket has only increased 4 times. That means that each unit of area has to support more mass. Sooner or later, even the strongest materials must give up, and you must give up the traditional rocket shape in favour of a wider base. That adds a lot to the drag! Problems like that are going to continue to show up:</p>
<p>"More mass means more problems, exponentially more mass means exponentially more problems."</p>
<p><strong>Summarized:</strong></p>
<p>A modern design, larger rocket than the Saturn V, with modifications to increase the T/W ratio could probably make it to orbit on a 2x radius, 8x mass Earth. That is a feasibility limit, rockets that are ridiculously much larger may have a few km/s extra <span>$\Delta v$</span>, but that does not alter the numbers a lot. In theory though, rockets can grow until the drag stops them, or the engines can no longer lift even themselves.</p>
<p>Or perhaps you at some point want to use the available resources of the planet to launch a single rocket to orbit.</p>
    </div>
    <div>
            
            <div>
        <a href="https://space.stackexchange.com/users/40073/williaml"><p><img src="https://lh6.googleusercontent.com/-uMbob4yPpuU/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucl-oCiPPcxlfd82VqIJzOIVtxYMyQ/s96-c/photo.jpg?sz=64" alt="WilliamL 's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2016-03-09 08:48:55Z">Mar 9, 2016 at 8:48</span>
        </p>
        
    </div>
    <div>
        <a href="https://space.stackexchange.com/users/8693/se-stop-firing-the-good-guys"><p><img src="https://i.stack.imgur.com/pVEHS.jpg?s=64&amp;g=1" alt="SE - stop firing the good guys's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
        </div>
    <p><span>$\endgroup$</span>
</p></div>




            <p><span itemprop="commentCount">7</span></p>
    </div>

                                    
<div id="answer-26866" data-answerid="26866" data-parentid="14383" data-score="11" data-position-on-page="3" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    <p><span>$\begingroup$</span></p><div itemprop="text">
<p><strong>note:</strong> I've accepted an answer 2.5 years ago. This paper was published recently so I thought I would add this <em>supplemental answer</em> since it may be an interesting reference for future readers.</p>

<hr>

<p>The Space.com article <a href="https://www.space.com/40375-super-earth-exoplanets-hard-aliens-launch.html" rel="noreferrer">No Way Out? Aliens on 'Super-Earth' Planets May Be Trapped by Gravity</a> links to Michael Hippke's ArXiv preprint <a href="https://arxiv.org/abs/1804.04727" rel="noreferrer">Spaceflight from Super-Earths is difficult</a>.</p>

<p>While the calculation is based on escape velocity rather than LSEO (Low Super-Earth Orbit) the conclusion is similar, the problem is exponential and it gets really difficult quickly.</p>

<p>The author uses the example of the planet <a href="https://en.wikipedia.org/wiki/Kepler-20b" rel="noreferrer">Keppler-20b</a> (see also <a href="https://arxiv.org/abs/1608.06836" rel="noreferrer">here</a>), and although there is some uncertainty, the planet's size is roughly 1.9 that of earth, and it's mass is almost 10 times that of Earth.</p>

<blockquote>
  <p>For a mass ratio of 83, the minimum rocket (1 t to <span>$v_{esc}$</span>) would carry 9,000 t of fuel on Kepler-20b, which is 3× larger than a Saturn V (which lifted 45 t). To
  lift a more useful payload of 6.2 t as required for the James Webb Space Telescope on Kepler-20 b, the fuel mass would increase to 55,000 t, about the mass of the largest ocean battleships. <strong>For a classical Apollo moon mission (45 t), the rocket would need to be considerably larger, ∼ 400,000 t. This is of order the mass of the Pyramid of Cheops, and is probably a realistic limit for chemical rockets regarding cost constraints.</strong> (emphasis added)</p>
</blockquote>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2018-04-24 03:56:27Z">Apr 24, 2018 at 3:56</span>
        </p>
        
    </div>
    <div>
        <a href="https://space.stackexchange.com/users/12102/uhoh"><p><img src="https://i.stack.imgur.com/pL4Iw.png?s=64&amp;g=1" alt="uhoh's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://space.stackexchange.com/users/12102/uhoh">uhoh</a><span itemprop="name">uhoh</span></p><p><span title="reputation score 148,425" dir="ltr">148k</span><span>51 gold badges</span><span>471 silver badges</span><span>1463 bronze badges</span>
        </p>
    </div>
</div>
    <p><span>$\endgroup$</span>
</p></div>




            <p><span itemprop="commentCount">4</span></p>
    </div>


                                    
<div id="answer-41462" data-answerid="41462" data-parentid="14383" data-score="7" data-position-on-page="4" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    <p><span>$\begingroup$</span></p><div itemprop="text">
<p>Not a planetological exposition in sight so, I'll add my two cents to this rather theoretical discussion.</p>
<p>Amongst exoplanetologists, the consensus has emerged that 1.6 Earth radii and 5 Earth masses is likely <a href="https://arxiv.org/abs/1407.4457" rel="nofollow noreferrer">to be the upper limit to rocky planets</a>. Simulations have shown that above these figures, the bodies develop <a href="https://en.m.wikipedia.org/wiki/Mini-Neptune" rel="nofollow noreferrer">increasingly Mini-Neptune</a> like characteristics. This means very thick Helium Hydrogen atmospheres and crushing surface pressure.</p>
<p>Also since <a href="https://arxiv.org/abs/1804.04727" rel="nofollow noreferrer">Michael Hippke's slightly whimsical paper</a> was referenced in one of the answers it seems appropriate to mention Ocean worlds at Super Earth masses. Ocean worlds present a host of habitability hurdles including a paucity of certain life critical elements like phosphorus, lack of volcanism, no water rock interface due to high pressure ice on the marine floor and others. These conditions will likely limit or even prevent the establishment of the vibrant prebiotic chemical environments that are necessary for biogenesis.</p>
<p>If the first assumption holds true, the highest gravity on a potentially  habitable world will not exceed approximately 2.5g.(edit: and thus making it not quite so difficult to reach orbit with chemical rockets as would have been the case with a higher g value)</p>
    </div>
    <div>
            
            <div>
    
    <div>
        <a href="https://space.stackexchange.com/users/10831/suma"><p><img src="https://www.gravatar.com/avatar/2bd7c0e93b52c224e284d18ac34752b8?s=64&amp;d=identicon&amp;r=PG" alt="Suma's user avatar" width="32" height="32"></p></a>
    </div>
    <div>
        <p><a href="https://space.stackexchange.com/users/10831/suma">Suma</a></p><p><span title="reputation score " dir="ltr">237</span><span>2 silver badges</span><span>13 bronze badges</span>
        </p>
    </div>
</div>


            <div>
    <div>
        <p>
            answered <span title="2020-02-16 03:20:47Z">Feb 16, 2020 at 3:20</span>
        </p>
        
    </div>
    <div>
        <a href="https://space.stackexchange.com/users/34935/pres1dentkang"><p><img src="https://lh3.googleusercontent.com/a-/AAuE7mBZ397c7Sobr-tBx0I-nEr_fAhCl4d7hCQBVGmZ=k-s64" alt="pres1dentkang's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
        </div>
    <p><span>$\endgroup$</span>
</p></div>




            <p><span itemprop="commentCount">7</span></p>
    </div>

                                    
<div id="answer-33973" data-answerid="33973" data-parentid="14383" data-score="2" data-position-on-page="5" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    <p><span>$\begingroup$</span></p><div itemprop="text">
<p>Great answers have been given, but one of the major themes is they assume a fixed wet to dry mass ratio of 10:1 (ish). The justification is:</p>

<ul>
<li><p>You need to fix this as: there are no meaningful answers with without <em>a</em> value and, <em>which</em> value is subject to engineering nuances, which are difficult to handle.</p></li>
<li><p>10:1 is a good pick. (We can't do much better than this and still have everything work so it seems sensible to stick at this)</p></li>
</ul>

<p>The problem is that's the limit of what we can make work <em>on earth</em>. A lot of the dry mass of a rocket is either:</p>

<ul>
<li><p>directly related to the thrust-to-mass ratio (i.e. number/size of engines)</p></li>
<li><p>indirectly related to TMR (i.e. supports the structural loads)</p></li>
</ul>

<p>Note, to keep gravity loses equivalent in practice the accelerations needed, hence TMR, is linear with the surface gravity. Hence so is a part of the the wet/dry mass ratio.</p>

<p>Once we take that into consideration things look a lot bleaker for the high g super-earths getting something into orbit using chemical rockets.</p>

<p>The actual numbers here are a little difficult to know, but if 5g world leads to a rocket with a w/d mass ratio of 5 to 1 (which I think is about right but...), you're staring down the barrel of a <span>$10^{20}$</span>t type figure for launch mass. To put that into perspective, the 'moon rocket' is no longer a good comparison. That's the mass of the moon it got to.</p>

<p>Theoretical limit? <strong>I'd say so</strong>.</p>

<p>At that mass things start taking a turn for the 'XKCD'. Forget the practical issues they're clearly long gone at "moon-sized-anything". We hit cold hard theoretical limits.
<strong>You start having to deal with your own gravity</strong>.</p>

<p>Firstly those practical issues are big ones even if we laugh a little 'engineering' problems (like money, and where we might find <span>$10^{19}$</span>t of aerospace grade materials). For example that's the sort of size that when you're made out something solid and are already floating in space under 0G, you deforming under you own gravity into a ball. Trying to make that out of mostly liquid fuel and subject it to 5-10g..., you're not staying the shape you started. Doesn't matter what mass-ratio 'hit' you are willing to take. But we've got this far, we aren't going to let a lack of unobtainium stop us.</p>

<p>No the <em>real</em> hard limit is how being so heavy effects your exhaust velocity. At the risk of getting too meta here if you're heavy enough, its difficult to get things to come apart from you. It applies to planet sized rockets as much as it does planets.</p>

<p>If you're a few million kilos, your 'exhaust velocity' is the velocity you can get your propellant to get to. If you have more mass than the moon, your propellant will have lost a lot of momentum by the time it's left your gravitational influence. And this is the fate our rocket meets.
LOX/H2 has an exhaust velocity of about <span>$4,400ms^{-1}$</span>, about as good as we can do.  Let's just say our moon-sized rocket has the density of the moon too, and so has a similar escape velocity of <span>$2,380ms^{-1}$</span>. Then the useful exhaust velocity of our rocket (initial less escape) is less than half. hence half the delta-v. You won't be going to space to day.</p>

<p>"Ok", I hear you say, "that just mean's you can't go to space in <em>that</em> rocket.", "How about a <strong>bigger</strong> one?". Well "No". This is another one of those "Even if everything sort of worked as before, you want go twice as fast, which is going to be a <em>lot</em> more mass." type problems. Except now we really can't just take the "make in 10 orders of magnitude bigger" approach. Apart from the fact that our rocket is now lot bigger than the planet which means we couldn't possibly construct it, now we have no chance of using chemical rockets to propel us anywhere.
To gain any momentum we need to chuck something out of our gravity well, and the exhaust velocity of chemical rockets don't make when we are this big.
We are now <em>truly</em> stuck.</p>

<p>But wait: directly the exhaust doesn't make it out, but I wonder if you could try different way of getting mass out of a very deep gravity well. Shouldn't be too hard. Even if it was only a little bit, we could always just scale it up...</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2019-02-01 17:00:24Z">Feb 1, 2019 at 17:00</span>
        </p>
        
    </div>
    <div>
        <a href="https://space.stackexchange.com/users/29167/drjpizzle"><p><img src="https://www.gravatar.com/avatar/47da31e4ec33974abed0ec0356acd2e3?s=64&amp;d=identicon&amp;r=PG" alt="drjpizzle's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://space.stackexchange.com/users/29167/drjpizzle">drjpizzle</a><span itemprop="name">drjpizzle</span></p><p><span title="reputation score " dir="ltr">349</span><span>1 silver badge</span><span>5 bronze badges</span>
        </p>
    </div>
</div>
    <p><span>$\endgroup$</span>
</p></div>




            <p><span itemprop="commentCount">4</span></p>
    </div>

                                    
<div id="answer-33252" data-answerid="33252" data-parentid="14383" data-score="-2" data-position-on-page="6" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    <p><span>$\begingroup$</span></p><div itemprop="text">
<p>On a practical engineering side of things.  Ultimately you are limited by exhaust velocity.  In theory you can always just make a bigger engine, bigger tanks, etc.  Ridiculously expensive, but possible.  This would seem to set the real limit to material strength.  Material strength is likely to give out before the gravity wells pull exceeds the exhaust velocity of even moderately modern fuels. </p>

<p>For example, LF+LOX typically has an exhaust velocity of around 4,400 m/s.  Which will fight up to 448 G of gravity.  Literally more than the sun.  Practically however much less than that.  So size of the planet itself presents no real deal killers, it just makes  the payloads mass fraction very VERY low.</p>

<p>At some point though other technologies, like nuclear bomb drives (<a href="https://en.wikipedia.org/wiki/Project_Orion_(nuclear_propulsion)" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Project_Orion_(nuclear_propulsion)</a>), become the only feasible affordable way off the planet. </p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2019-01-01 18:00:55Z">Jan 1, 2019 at 18:00</span>
        </p>
        
    </div>
    <div>
        <a href="https://space.stackexchange.com/users/28690/anthony-bachler"><p><img src="https://graph.facebook.com/1324390910913249/picture?type=large" alt="Anthony Bachler's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    <p><span>$\endgroup$</span>
</p></div>




            <p><span itemprop="commentCount">2</span></p>
    </div>


                        


                            <h2 data-loc="1">
                                
                            </h2>
                </div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DIY MBA: My Reading List (2019) (193 pts)]]></title>
            <link>https://chrisstoneman.medium.com/diy-mba-my-reading-list-f7699bd7d0c6</link>
            <guid>39243044</guid>
            <pubDate>Sat, 03 Feb 2024 18:10:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chrisstoneman.medium.com/diy-mba-my-reading-list-f7699bd7d0c6">https://chrisstoneman.medium.com/diy-mba-my-reading-list-f7699bd7d0c6</a>, See on <a href="https://news.ycombinator.com/item?id=39243044">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><div><div><a rel="noopener follow" href="https://chrisstoneman.medium.com/?source=post_page-----f7699bd7d0c6--------------------------------"><div aria-hidden="false"><p><img alt="Chris Stoneman" src="https://miro.medium.com/v2/resize:fill:88:88/2*KFid1acHy0Y3bOhbNhfHrA.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="b299">I’m doing a self-taught DIY MBA (<a href="https://medium.com/@chrisstoneman/why-im-trying-a-diy-mba-why-i-need-your-help-e3b8a08e2ba6" rel="noopener">read more</a>). Here’s my work-in-progress reading list, add anything you think I’m missing in the comments.</p></div><div><h2 id="18de">Accounting &amp; Finance : Completed. <a rel="noopener" href="https://chrisstoneman.medium.com/diymba-1-accounting-finance-bf5f0a556e73">Read the module overview</a></h2><ul><li id="9f78"><em>The 10 Day MBA</em> by Steven Silbiger</li><li id="d40f"><em>The Portable MBA in Finance and Accounting</em> by Grossman &amp; Livingstone</li><li id="119d">An online course is probably better than a book, I did <a href="https://www.acumenlearning.com/why-do-we-teach-the-5-business-driv" rel="noopener ugc nofollow" target="_blank">this one</a> by Acumen Learning.</li></ul></div><div><h2 id="e797">Product, Design &amp; Marketing</h2><ul><li id="7953"><em>Inspired: How to Create Tech Products Customers Love</em> by Marty Cagan (<a rel="noopener" href="https://chrisstoneman.medium.com/notes-on-inspired-how-to-create-tech-products-customers-love-by-marty-cagan-ddb960567a78">notes</a>)</li><li id="622b"><em>Lean Startup</em> by Eric Ries (<a rel="noopener" href="https://chrisstoneman.medium.com/notes-on-lean-startup-by-eric-ries-b6df5b2a3bfd">notes</a>)</li><li id="72f9"><em>Hooked: How to Make Habit Forming Products</em> by Nir Eyal (<a rel="noopener" href="https://chrisstoneman.medium.com/notes-on-hooked-by-nir-eyal-1018b4a3e753">notes</a>)</li><li id="e93a"><em>The Design of Everyday Things</em> by Don Norman (<a rel="noopener" href="https://chrisstoneman.medium.com/notes-on-the-design-of-everyday-things-by-don-norman-8760641593f6">notes</a>)</li><li id="1484"><em>Perennial Seller: The Art of Making and Marketing Work that Lasts</em> by Ryan Holiday</li><li id="3153"><em>Marketing: A Love Story </em>by Bernadette Jiwa (<a rel="noopener" href="https://chrisstoneman.medium.com/notes-on-marketing-by-bernadette-jiwa-e8ceab2472e6">notes</a>)</li><li id="7910"><em>Building a Story Brand</em> by Donald Miller (<a rel="noopener" href="https://chrisstoneman.medium.com/notes-on-building-a-story-brand-by-donald-miller-7ad0cf456a15">notes</a>)</li><li id="886d"><em>Crossing the Chasm</em> by Geoffrey Moore (<a rel="noopener" href="https://chrisstoneman.medium.com/notes-on-crossing-the-chasm-by-geoffrey-moore-6b5f6e0ebb78">notes</a>)</li><li id="664b"><em>Naked Statistics</em> by Charles Wheelan (<a rel="noopener" href="https://chrisstoneman.medium.com/notes-on-naked-statistics-by-charles-wheelan-f05a0d37c2a9">notes</a>)</li></ul></div><div><h2 id="febe">Organisational Behaviour, Management &amp; Communication</h2><ul><li id="781d"><em>High Output Management</em> by Andy Grove (<a href="https://medium.com/@chrisstoneman/high-output-management-99b1a36dff0a" rel="noopener">notes</a>)</li><li id="7cd2"><em>The Coaching Habit</em> by Michael Bungay Stanier</li><li id="25b1"><em>Act Like a Leader, Think Like a Leader</em> by Herminia Ibarra</li><li id="ba66"><em>Talking from 9 to 5</em> by Deborah Tannen</li><li id="1b45"><em>Influence: The Psychology of Persuasion</em> by Robert Cialdini</li><li id="a2b0"><em>Hard Thing About Hard Things</em> and <em>What You Do Is Who You Are</em> by Ben Horowitz.</li><li id="643e"><em>Resilient Management</em> by Lara Hogan</li><li id="63c2"><em>Never Split The Difference</em> by Chris Voss</li></ul></div><div><h2 id="09fb">Global Economics &amp; Investing</h2><ul><li id="7908"><em>Capitalism and Freedom</em> by Milton Friedman</li><li id="3dbd"><em>Debt: The First 5,000 Years</em> by David Graeber</li><li id="1153"><em>Economics: A User’s Guide</em> by Ha-Joon Chang</li><li id="dec6"><em>Winner Take All: China’s Race for Resources and What It Means for the World</em> by Dambisa Moyo</li><li id="9551"><em>Economics in One Lesson</em> by Henry Hazlitt</li><li id="641f"><em>The Most Important Thing: Uncommon Sense for the Thoughtful Investor </em>by Howard Marks</li><li id="3650"><em>The Intelligent Investor</em> by Benjamin Graham</li></ul></div><div><h2 id="469b">Strategy &amp; Systems Thinking</h2><ul><li id="ea6c"><a href="https://www.amazon.co.uk/dp/B005331U7Q/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1" rel="noopener ugc nofollow" target="_blank"><em>Good Strategy/Bad Strategy</em></a> by Richard Rumelt</li><li id="8894"><em>Platform Scale</em> by Sangeet Paul Choudary (<a href="https://medium.com/@chrisstoneman/notes-on-platform-scale-by-sangeet-paul-choudary-9d9f3993b47" rel="noopener">notes</a>)</li><li id="cfa3"><em>7 Powers: Foundations of Business</em> by Hamilton Helmer (<a href="https://medium.com/@chrisstoneman/notes-on-7-powers-foundations-of-business-by-hamilton-helmer-5d20cfecc753" rel="noopener">notes</a>)</li><li id="2d58"><em>Blue Ocean Strategy</em> by W. Chan Kim &amp; Renée Mauborgne</li><li id="8dbf"><a href="http://www.amazon.com/Certain-Win-Strategy-Applied-Business/dp/1413453767" rel="noopener ugc nofollow" target="_blank"><em>Certain to Win</em></a> by Chet Richards</li><li id="aeed"><a href="http://www.lean.org/BookStore/ProductDetails.cfm?SelectedProductId=156&amp;ProductCategoryID=4" rel="noopener ugc nofollow" target="_blank"><em>Getting the Right Things Done</em></a> by Pascal Dennis</li><li id="5c63"><em>Business Model Generation</em> by Alexander Osterwalder &amp; Yves Pigneur</li><li id="c4eb"><em>Theory in Practice</em> by Chris Argyris &amp; Donald Schön</li><li id="0211"><em>Thinking in Systems</em> by Donella Meadows</li><li id="a18f"><em>Antifragile: Things that Gain from Disorder</em> by Nassim Nicholas Taleb</li><li id="c98b"><em>The Fifth Discipline: The Art &amp; Practice</em> of The Learning Organization by Peter Senge</li><li id="8a2f"><em>The Innovator’s Dilemma</em> by Clayton M. Christensen</li></ul></div><div><h2 id="7829">Creativity &amp; Getting Stuff Done</h2><ul><li id="83a4"><em>The Seven Habits of Highly Effective People</em> by Stephen Covey</li><li id="c9e0"><a href="https://www.goodreads.com/book/show/18077903-creativity-inc" rel="noopener ugc nofollow" target="_blank"><em>Creativity, Inc: Overcoming the Unseen Forces That Stand in the Way of True Inspiration</em></a> by Ed Catmull</li><li id="b17a"><em>A Beautiful Constraint</em> by Adam Morgan &amp; Mark Barden</li><li id="d5e7"><em>Steal Like An Artist</em> by Austin Kleon</li><li id="8e24"><em>Finite and Infinite Games</em> by James P Carse</li><li id="410c"><em>The War of Art</em> by Steven Pressfield</li><li id="22b3"><em>The Art of Possibility</em> by Rosamund Stone Zander</li><li id="121b"><em>Deep Work</em> by Cal Newport</li></ul></div><div><p id="01fb">Have I missed anything? Are there better alternatives to these? Let me know with a comment!</p></div></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Low-Power Wi-Fi Extends Signals Up to 3 Kilometers (149 pts)]]></title>
            <link>https://spectrum.ieee.org/wi-fi-halow</link>
            <guid>39242817</guid>
            <pubDate>Sat, 03 Feb 2024 17:50:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/wi-fi-halow">https://spectrum.ieee.org/wi-fi-halow</a>, See on <a href="https://news.ycombinator.com/item?id=39242817">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Low-Power Wi-Fi Extends Signals Up to 3 Kilometers" data-elid="2667126316" data-post-url="https://spectrum.ieee.org/wi-fi-halow" data-authors="Edd Gent" data-page-title="Low-Power Wi-Fi Extends Signals Up to 3 Kilometers - IEEE Spectrum"><p> Most people have probably experienced the frustration of weak Wi-Fi signals. Even getting a network to cover every corner of a fairly modest house can be a challenge. That’s not a problem for Wi-Fi technology developed by the Australian startup <a href="https://www.morsemicro.com/" rel="noopener noreferrer" target="_blank">Morse Micro</a>, which just demonstrated a Wi-Fi signal with a 3-kilometer range.<strong></strong></p><p> Morse Micro has developed a <a href="https://spectrum.ieee.org/tag/system-on-chip" target="_blank">system-on-chip</a> (SoC) design that uses a wireless protocol called <a href="https://www.wi-fi.org/discover-wi-fi/wi-fi-certified-halow" target="_blank">Wi-Fi HaLow</a>, based on the <a href="https://ieeexplore.ieee.org/document/7920364" rel="noopener noreferrer" target="_blank">IEEE 802.11ah standard</a>. The protocol significantly boosts range by using lower-frequency radio signals that propagate further than conventional Wi-Fi frequencies. It is also low power, and is geared toward providing connectivity for <a data-linked-post="2659970239" href="https://spectrum.ieee.org/iot-for-arson-forensics" target="_blank">Internet of Things</a> (IoT) applications.</p><p> To demonstrate the technology’s potential, Morse Micro recently <a href="https://www.youtube.com/watch?v=2xlUijXucoM" rel="noopener noreferrer" target="_blank">conducted a test</a> on the seafront in San Francisco’s Ocean Beach neighborhood. They showed that two tablets connected over a HaLow network could communicate at distances of up to 3 km while maintaining speeds around 1 megabit per second—enough to support a slightly grainy video call.</p><p><span data-rm-shortcode-id="8cf36374a19b908e5344979ec4cf4af5"><iframe type="lazy-iframe" data-runner-src="https://www.youtube.com/embed/2xlUijXucoM?rel=0" width="100%" height="auto" frameborder="0" scrolling="no"></iframe></span><small placeholder="Add Photo Caption...">Wi-Fi HaLow Shatters Limits with a 3-Kilometer Range</small><small placeholder="Add Photo Credit...">
            
    Morse/Micro/<a href="https://www.youtube.com/watch?v=2xlUijXucoM" target="_blank">youtube</a></small></p><p> “It is pretty unprecedented range,” says <a href="https://www.linkedin.com/in/prakashguda/" rel="noopener noreferrer" target="_blank">Prakash Guda</a>, vice president of marketing and product management at Morse Micro. “And it’s not just the ability to send pings but actual megabits of data.”</p><p> The HaLow protocol works in much the same way as conventional Wi-Fi, says Guda, apart from the fact that it operates in the 900-megahertz frequency band rather than the 2.4-gigahertz band. Lower-frequency signals are able to propagate further and are better at penetrating solid objects, which is the secret to HaLow’s much longer ranges.</p><p> The protocol also allows channel bandwidths as narrow as 1 MHz, compared with the 20-MHz channels that are standard in Wi-Fi. Guda says this makes it possible to have many more dedicated channels that don’t interfere with one another, which is useful for IoT applications where you want to connect several devices to the same network.</p><p> The trade-off is lower throughput, says Guda, but as their test shows, HaLow can still support data-intensive applications like video. The technology is also much lower power than conventional Wi-Fi, as it has in-built features that allow devices to remain dormant for long periods and wake only when they need to transmit data. This makes it feasible to power HaLow devices using batteries. Guda says Morse Micro is working with partners building battery-powered IoT devices that will need to operate for years, and in those cases, HaLow won’t run the battery down too soon.<strong></strong></p><p>But IoT connectivity is a crowded space, says <a href="https://www.ictp.it/member/ermanno-pietrosemoli" rel="noopener noreferrer" target="_blank">Ermanno Pietrosemoli</a>, a scientific consultant at the International Center for Theoretical Physics (ICTP) in Trieste, Italy, who specializes in wireless networks. “[HaLow] is a viable competitor, but there are many players in this field,” he says.</p><p> The technology has a considerable range advantage over IoT-specific alternatives like <a data-linked-post="2650250203" href="https://spectrum.ieee.org/busy-as-a-zigbee" target="_blank">ZigBee</a> or <a data-linked-post="2650280437" href="https://spectrum.ieee.org/apptricity-beams-bluetooth-signals-over-20-miles" target="_blank">Bluetooth</a>. And while the <a data-linked-post="2650278012" href="https://spectrum.ieee.org/loras-bid-to-rule-the-internet-of-things" target="_blank">LoRa standard</a> is capable of distances of tens of kilometers, its throughput is too low to enable video or even real-time voice, says Pietrosemoli. HaLow’s high data rates could be useful for more data-heavy applications, like wirelessly connecting lots of security cameras, he adds.</p><p> Despite the HaLow<strong></strong>standard being ratified in 2017, the technology has very low adoption, says Pietrosemoli, which might give developers pause due to concerns around whether the technology will continue to be supported. In contrast, competing technologies like <a data-linked-post="2650277753" href="https://spectrum.ieee.org/stopping-a-wildfire-with-a-lowcost-sensor-network" target="_blank">SigFox</a>, which operates on principles similar to those of HaLow, and <a data-linked-post="2650276476" href="https://spectrum.ieee.org/italy-launches-a-new-wireless-network-for-the-internet-of-things" target="_blank">Narrowband IoT</a> (NB-IoT)—a low-power cellular technology—are already widely deployed.</p><p><a href="https://www.gartner.com/analyst/61747" target="_blank">Bill Ray</a>, a VP analyst at Gartner, is even more circumspect. While Morse Micro has built some impressive technology, he says HaLow has failed to gain any significant traction in the industry. The fact that Qualcomm, which was an early booster of the technology, has lost interest is indicative, he adds.</p><p> “We are not optimistic about the standard, even if it can be stretched to 3 km,” says Ray. “Bluetooth offers greater interoperability, while LoRa offers greater range, putting HaLow in a middle position with limited application.”</p><p>“It is pretty unprecedented range. And it’s not just the ability to send pings but actual megabits of data.” <strong>—Prakash Guda, Morse Micro</strong></p><p> Guda puts a different spin on it. By combining high data rates and long ranges, he says, the technology can serve the whole gamut of IoT applications. “It’s the best of both worlds,” he says.</p><p> He acknowledges the lack of adoption does raise concerns about interoperability and overreliance on a single vendor, but he says things are changing. California-based <a href="https://newracom.com/" rel="noopener noreferrer" target="_blank">Newracom</a> now also produces a HaLow SoC, and in 2021 the Wi-Fi Alliance <a href="https://www.wi-fi.org/news-events/newsroom/wi-fi-certified-halow-delivers-long-range-low-power-wi-fi" rel="noopener noreferrer" target="_blank">launched a certification program</a> for the standard that Guda says will improve interoperability.</p><p> “If you’re trying to make some equipment based on a new technology, you want to make sure that there’s interoperability and there’s more than one supplier,” he says. “That’s the critical mass that we have now.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[.NET on Linux: What a Contrast (108 pts)]]></title>
            <link>https://two-wrongs.com/dotnet-on-linux-update.html</link>
            <guid>39242797</guid>
            <pubDate>Sat, 03 Feb 2024 17:49:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://two-wrongs.com/dotnet-on-linux-update.html">https://two-wrongs.com/dotnet-on-linux-update.html</a>, See on <a href="https://news.ycombinator.com/item?id=39242797">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>
Almost five years ago I wrote <a href="https://two-wrongs.com/dotnet-on-non-windows-platforms-brief-historic-summary.html">a brief history about <abbr>.net</abbr> on non-Windows
platforms</a>. I stumbled over that article now and it resurfaced traumatic
memories of navigating <abbr>.net</abbr> development on Linux and MacOS.
</p>

<p>
Even back in 2019, it was mostly fine to <i>run</i> <abbr>.net</abbr> code on Linux (in
production) but the documentation and tooling for development was not there at
all.<label for="fn.1">1</label><span><sup>1</sup> I got into <abbr>.net</abbr> development when my then-new employer was in the
middle of transitioning from <abbr>.net</abbr> Framework to <abbr>.net</abbr> Core, so my non-Windows
work computer ended up being a lab rat for all sorts of unexpected compatibility
problems. I still don’t fully understand why I thought it was a good idea to try
to port applications I wasn’t even able to <i>compile</i> – much less run – to the
new framework, but it worked out fine in the end. (It was not just that these
applications used the <abbr>fcl</abbr>, they also integrated with Windows functionality.)</span>
</p>

<p>
That experience was, and the article was written, when <abbr>.net</abbr> Core 2.1 was the
latest production-friendly release. In some sense, it was just yesterday, but in
another, it’s a world away. Even just two or three years later, the tooling and
documentation had improved to levels I didn’t anticipate. Today, I still write
<abbr>.net</abbr> code (for a different employer) and I can barely tell whether I’m on
Windows or Linux.
</p>

<p>
It speaks to how much an organisation with a big wallet and willingness to
invest can shift things in a brief amount of time – but also, I think, how other
companies (I’m looking at you, JetBrains) can set a standard for cross-platform
tooling that the original company is forced to live up to in order not to lose
too much market share.
</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Vision Pro and ADHD (117 pts)]]></title>
            <link>https://dansalva.to/apple-vision-pro-and-adhd/</link>
            <guid>39242662</guid>
            <pubDate>Sat, 03 Feb 2024 17:36:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dansalva.to/apple-vision-pro-and-adhd/">https://dansalva.to/apple-vision-pro-and-adhd/</a>, See on <a href="https://news.ycombinator.com/item?id=39242662">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
<p>Apple Vision Pro reminds me that computers have become too efficient.</p>
<p>I'm currently typing this blog post in a floating window in the middle of
snow-covered Yosemite. I am a floating body manifested into the transcendental
Tim Cook matrix. I look into the sky and follow the soft clouds as they pass
overhead. They slowly part, revealing Steve Jobs' smiling face watching over me
from the heavens above. I am merely one more thing.</p>
<p>When I walked into the Apple Store yesterday, the first question I had for the
assistant was the return policy on Apple Vision Pro. I have about 13 days left
to decide if this thing is worth $3,500. My early conclusion is that if you're
an enthusiast for entertainment and you have a lot of money to spend on it, the
Apple Vision Pro might be for you. Even the folks at the Apple Store seemed
convinced that "entertainment" is the current selling point of this device.</p>
<p>I don't fall into that category, so I'm currently trying to figure out what the
hell to use it for.</p>
<p>I tried watching a YouTube video placed over my countertop while making coffee.
The display quality is stunning, but the video pass-through is starkly
disappointing—granted, it's probably the best there is, but the world around me
appears grainy and poorly-lit. On top of that, I'm definitely a bit clumsier
with my hands.</p>
<p>iPad apps also feel clumsy. In visionOS, buttons highlight as your eyes pass
over them, indicating they're selected. This does not always happen in iPad
apps. In Octal (a Hacker News reader), I have to guess whether I'm looking hard
enough at the correct element to bring up the story or the comments. In
Obsidian, pressing the corner buttons to expand the sidebar is nearly
impossible.</p>
<p>If I'm staring at the center of a window, the window edges tend to shimmer as I
move my head around. This tells me visionOS uses higher-quality anti-aliasing
wherever your eyes are focused. Cool performance trick, but human peripheral
vision is highly sensitive to motion, which makes the shimmering somewhat
distracting.</p>
<p>I'm sure I sound pretty underwhelmed by my experience. Perhaps, but the most
exciting thing to me about Apple Vision Pro is that it has flaws like this.
Flaws bring charm to gadgets. They make you use them more selectively and
deliberately. Computers, phones and tablets have gotten boring because there is
no longer such deliberate use—they are always there, able to do anything and
instantly switch tasks as fast as your fingers can move.</p>
<h2>That's where ADHD comes in</h2><p>It's not strictly an ADHD problem that technology has gotten unbelievably
distracting. But as someone with ADHD, I feel acutely aware of my relationship
to technology in this way. It is incredibly hard to focus on a single task and
clear my mind of the "itch to screw around" when all the technology around me is
so goddamn fast and easy to use.</p>
<p>That's what makes Apple Vision Pro interesting for me. Navigating with your eyes
and fingers is (currently) not as fast or easy as using a mouse or a
touchscreen. Positioning windows in 3D space also takes time.</p>
<p>This has an interesting effect. I feel less inclined to mindlessly open and use
apps, because it has become a fundamentally mindful gesture to do so. I actually
have to go out of my way to bring up the Home Screen, launch an app, position it
where I want it, and interact with it by looking directly at it. I can't do
these things on autopilot.</p>
<p>Putting myself in an "environment" also seems to have the effect of eliminating
distractions from the outside world. I'm not glancing around at the stuff in my
office every time I pause to think.</p>
<p>There is an amusing irony here, the way that strapping a $3,500 computer to my
face is making my interaction with the digital world more deliberate and
mindful.</p>
<p>So, am I going to keep this thing after all?</p>
<p>It's still a toss-up. I have a decent idea for a productivity app I want to
build for Apple Vision Pro. If I make solid progress on it this week, I might
keep the headset under justification for business.</p>
<p>However, 14 days is unfortunately not enough time for me to decide whether Apple
Vision Pro is a true quality-of-life improvement for an ADHD lifestyle. Maybe
I'm still swept up in the novelty of it all. If I do end up keeping it, I'll
definitely write a follow-up in the near future. But for this price tag, I can't
justify the purchase on a "maybe".</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Paris Vote Targets SUV Drivers with Parking Fees Set to Triple (128 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-02-03/paris-vote-targets-suv-drivers-with-parking-fees-set-to-triple</link>
            <guid>39242598</guid>
            <pubDate>Sat, 03 Feb 2024 17:31:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-02-03/paris-vote-targets-suv-drivers-with-parking-fees-set-to-triple">https://www.bloomberg.com/news/articles/2024-02-03/paris-vote-targets-suv-drivers-with-parking-fees-set-to-triple</a>, See on <a href="https://news.ycombinator.com/item?id=39242598">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Simple demo of a cold boot attack using a Raspberry Pi (122 pts)]]></title>
            <link>https://github.com/anfractuosity/ramrecovery</link>
            <guid>39242244</guid>
            <pubDate>Sat, 03 Feb 2024 17:03:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/anfractuosity/ramrecovery">https://github.com/anfractuosity/ramrecovery</a>, See on <a href="https://news.ycombinator.com/item?id=39242244">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:anfractuosity/ramrecovery" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Rkd8CJS5tz-Qs51Sv5ePbbN_UY5aJ8kBUKA9yAmpf-z3flwji7Ek79c79RqM0XT0EoXvkyv59Li7PsuBJ-JPVA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="anfractuosity/ramrecovery" data-current-org="" data-current-owner="anfractuosity" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=c9oKsfd%2FBmLSqWtWC58gCxThhrWDvhSML1hloajfcM9H9feG3tpFYqkfRTmf%2BjeSiCpXNWSwGECX4XB9riefeXMsAYF7BxeEXJH5%2FKRbq%2Bd7l1dHDvqTokFMAH4pOq%2BWiGcYfmVvoNOL9%2FACxUBDg7QryRS4f9R%2Bjp9oZ9jQ1sdKRrTuen6xs%2F4yTKmZLfHBq4g6nyqgy8%2FDkh4Royc3EEAxsgDgJLZ7Tm%2F%2FEHJ2vI7oP1sWcCR%2B1oOaWfkxKtU22jf3mJ8oBXvpVmo2BHNfDE0G2ixyIjkGWU8DIopr%2F%2F08Jb%2BbHUOl7KERrCWBbCBiafp0BKpB2E%2BW1q4tuLoA9dRZYrUAnTOwIZBdeS2WHkssTQEn6zcJxJ4q9HVExfKxhgoOPR5uYq767OQGeDbMToVg5zhWC%2B0pNvMQLN3k2%2B9IMZqfktO1Vyyp2OpkKkryqfmyEK4x71ui6uphQ8wHeagA05UqG1bYP1R%2FVstyklGzR1F6m9BPxsoTTBwdmDGPl8vtuSwwWmbp%2BT%2Bz%2FR118o9PwtxBkqryLrI%3D--t9OBlG1io9vq0toY--RAkHMir1swjCb5g1HgrCuQ%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=anfractuosity%2Framrecovery" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/anfractuosity/ramrecovery&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="e421dc9fe3d004bc9299248ba0477b615fe2d61e943c7711e8e2005193678e38" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The engineering behind Figma's vector networks (2019) (198 pts)]]></title>
            <link>https://alexharri.com/blog/vector-networks</link>
            <guid>39241825</guid>
            <pubDate>Sat, 03 Feb 2024 16:26:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexharri.com/blog/vector-networks">https://alexharri.com/blog/vector-networks</a>, See on <a href="https://news.ycombinator.com/item?id=39241825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><p><img src="https://alexharri.com/images/posts/vector-networks/0.svg" width="2160"></p>
<p>Adobe Illustrator introduced the pen tool back in <a target="_blank" href="https://www.youtube.com/watch?v=sT8Y7o-zsVw">1987</a> as a tool for creating and modifying paths. Since then the pen tool has become incredibly widespread, so much so that is has become the de facto icon of the graphic design industry.</p>
<p>The pen tool's functionality hasn't changed significantly in the 30 years since its introduction. Just click and drag to create smooth curves. Designers have learned to work with it, and around its idiosyncrasies.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/1.svg"></p>
<p>The pen tool</p>
<p>But Figma felt like they could improve some aspects of how the pen tool worked, so they had a go at redesigning it. Instead of it being used to work with traditional paths, they improved the pen tool by creating what they call Vector Networks.</p>
<p>In this post we will go through what Vector Networks are and what problems they try to solve. After we've defined what Vector Networks are, we will take a look at some of the engineering challenges you would face if you were to take a stab at implementing them.</p>
<p>This post can be thought of as an introduction to a really interesting problem space, and as a resource for people interesting in making use of some aspects of Vector Networks for future applications. I hope it succeeds in providing value to both developers being introduced to new concepts and ideas, and to designers interesting in learning more about the tool they know and love.</p>
<p>I will start off by laying out the core concepts behind the pen tool, and from there we will move onto Figma's Vector Networks.</p>
<h2>Paths</h2>
<p>The pen tool is used to create and manipulate paths.</p>
<p>If you've every worked with graphics software like <a target="_blank" href="https://www.adobe.com/products/illustrator.html">Illustrator</a> before, you've worked with paths. Paths are a series of lines and curves that may or may not form a loop.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/2.svg"></p>
<p>Some paths</p>
<p>The path to the left loops, while the path to the right doesn't. Both of these are valid paths.</p>
<p>The main characteristic of paths is that they form a single continuous unbroken chain. This means that each node can only be connected to one or two other nodes.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/3.svg"></p>
<p>Not valid paths</p>
<p>However, you could construct these shapes from multiple paths if you position them correctly together.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/4.svg"></p>
<p>Multiple paths are used to create more complex shapes</p>
<p>From a combination of paths, you can create any shape imaginable.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/5.svg"></p>
<p>This beer glass, for example, is just a combination of five different paths positioned and scaled a certain way.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/6.svg"></p>
<h3>The building blocks of paths</h3>
<p>A path is made up of two things, points and lines.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/7.svg"></p>
<p>Points and lines</p>
<p>The points are known as <strong>nodes</strong> (or vertices) and the lines are called <strong>edges</strong>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/8.svg"></p>
<p>Together, they make a path</p>
<p>Any path can be described as a list of nodes and edges.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/9.svg"></p>
<p>This path can be described as the series of nodes <code>(0, 1, 2, 3, 4)</code>. It could also be thought of as the series of the edges that composed it. That list of edges would be <code>(0, 1)</code>, <code>(1, 2)</code>, <code>(2, 3)</code>, <code>(3, 4)</code>, <code>(4, 0)</code>.</p>
<p>You can think of this like the <a target="_blank" href="https://www.google.com/search?q=dot+to+dot+for+kids&amp;tbm=isch">dot to dot puzzles</a> that you used to do as a kid: Draw the edges of the path in the order that the points lay out.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/10.svg"></p>
<p>But instead of a kid drawing lines between numbered points on a paper, a cold calculating machine does it along the cartesian coordinate system.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/11.svg"></p>
<h2>Edges</h2>
<p>An edge is a connection between a pair of nodes. Visually, edges are a line from node <code>a</code> to node <code>b</code>.</p>
<p>But that line can be drawn in a lot of different ways. How do you describe those different types of lines?</p>
<p>Edges fall into two categories, straight and curvy.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/12.svg"></p>
<p>Straight edges are as simple as they seem, just a line from <code>a</code> to <code>b</code>. But how are those curvy edges defined?</p>
<h3>Bezier curves</h3>
<p>Curvy edges are <a target="_blank" href="https://en.wikipedia.org/wiki/B%C3%A9zier_curve">bezier curves</a>. Bezier curves are a special type of curve defined by four points.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/13.svg"></p>
<p>The positions of the two nodes in our edge make up the start and end points of the curve. Each of the two nodes has a <em>control point</em>.</p>
<p>In most applications, these control points are shown as <em>handles</em> that extend from their respective node. These handles are used to control the shape of the curve.</p>
<p>Bezier curves can be chained to make more complex shapes that a single curve can't draw on its own. They can also be combined with straight lines to make some cool designs.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/14.svg"></p>
<p>But what exactly are the handles doing? How do the handles tell the computer to draw the curve like it does?</p>
<p>Computers draw curves by splitting them into straight lines and drawing the individual lines.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/15.svg"></p>
<p>The more lines you split a curve into, the smoother the curve becomes.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/16.svg"></p>
<p>So to draw the curve we need to know how to get the different points that make up the curve. If we compute enough of them, we get a smooth curve.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/17.svg"></p>
<h3>Computing a point on a bezier curve</h3>
<p>Let's compute the point at 25% point of the curve. We can start by connecting the control points with a third blue line.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/18.svg"></p>
<p>Then for each blue line, we draw a blue dot at the 25% point of the line.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/19.svg"></p>
<p>Next, draw two green lines between the three blue dots.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/20.svg"></p>
<p>And we repeat the same step as we did with the blue dots. Draw green points at the 25% points of the green lines.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/21.svg"></p>
<p>And then one more red line between the newly created green points.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/22.svg"></p>
<p>Then we add a red point at the 25% point of the red line.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/23.svg"></p>
<p>And just like that, we've computed the point at the 25% point of the curve.</p>
<p>From now on we'll refer to points on curves through a <code>t</code> value, where <code>t</code> is a number from <code>0</code> to <code>1</code>. In the above example, the point would be at <code>t=0.25</code> (25%).</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/24.svg"></p>
<p>t=0.25, t=0.5 and t=0.75</p>
<p>This way of computing the point at <code>t</code> is called <a target="_blank" href="https://en.wikipedia.org/wiki/De_Casteljau%27s_algorithm">De Casteljau's algorithm</a> and can also be used to subdivide a bezier curve. Using the points we created along the way, we can also subdivide the bezier curve into two smaller bezier curves.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/25.svg"></p>
<p>Bezier curves are pretty amazing things. Shaping the curve by adjusting the handles feels surprisingly natural, and chaining them together allows you to create detailed and complex shapes.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/26.svg"></p>
<p>And for computers, they're stable and inexpensive to compute. For this reason they're used for everything from <a target="_blank" href="https://helpx.adobe.com/illustrator/using/drawing-pen-curvature-or-pencil.html#draw_curves_with_the_pen_tool">vector graphics</a> to <a target="_blank" href="https://www.austinsaylor.com/blog/2015/3/20/4-after-effects-graph-editor-basics-you-need-to-know">animation curves</a> and <a target="_blank" href="https://en.wikipedia.org/wiki/Pierre_B%C3%A9zier#B%C3%A9zier_curve">automobile bodies</a>.</p>
<p>You can see an interactive demo of bezier curves at <a target="_blank" href="https://www.jasondavies.com/animated-bezier/">Jason Davies' site</a>. It's fascinating to watch a series of straight lines trace out a smooth curve.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/27.png"></p>
<p>From <a target="_blank" href="https://jasondavies.com/animated-bezier">https://jasondavies.com/animated-bezier</a></p>
<h2>The creative constraints of paths</h2>
<p>Earlier in this post, paths were defined as a continuous series of lines and curves that may or may not form a loop.</p>
<p>The fact that paths are a single continuous chain is a pretty big limitation.</p>
<p>It means three way intersections are not possible using a single path. To create a three way intersection, two or more paths will have to be used. This means dealing with positioning and grouping different paths together. It also means that changes to a single path can lead to changes to multiple other paths.</p>
<p>But that's simply the routine. Seasoned designers know how paths behave, they can plan around it without really thinking about it. For a static design it doesn't really matter how many paths and layers you have to create if the piece is planned properly upfront.</p>
<p>But for some situations the constraints that paths impose cause a lot of friction.</p>
<h2>Vector Networks</h2>
<p>In 2016, Figma <a target="_blank" href="https://www.figma.com/blog/introducing-vector-networks/">introduced Vector Networks</a>. They lift the “single continuous” limitation by allowing any two nodes to be joined together without restrictions.</p>
<blockquote>
<p>“A vector network improves on the path model by allowing lines and curves between any two points instead of requiring that they all join up to form a single chain.”</p>
</blockquote>
<p><img src="https://alexharri.com/images/posts/vector-networks/28.gif"></p>
<p>Source: <a target="_blank" href="https://www.figma.com/blog/introducing-vector-networks/">https://www.figma.com/blog/introducing-vector-networks/</a></p>
<p>The cube is the quintessential example for demonstrating Vector Networks.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/29.svg"></p>
<p>Via traditional paths, you would have to create at minimum 3 different paths to describe this shape.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/30.svg"></p>
<p>This creates a lot of friction for a seemingly simple and common shape. To modify the cube, you would have to modify two or three different paths. But with Vector Networks you can simply grab an edge and move it around, and the shape behaves like you would expect.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/31.svg"></p>
<p>So if you would want to increase the extrusion of the cube, you could just grab the two appropriate edges and move them together.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/32.svg"></p>
<p>This is the big selling point for Figma's Vector Networks. Ease of use.</p>
<p>Vector Networks don't enable you to create something that you couldn't create with other tools, but it does remove a lot of the friction in the process of creating things.</p>
<p>And you can take this even further. Say you want to add a hole to the side of the cube.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/33.svg"></p>
<p>Just start off by selecting and copying the sides of the cube. You can then duplicate those edges and scale them to the size you want them to be.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/34.svg"></p>
<p>And just like that, you have a cube with a hole.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/35.svg"></p>
<p>And to make this hole believable, you just need the inner edge.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/36.svg"></p>
<p>Again, Vector Networks may not allow you to create something you couldn't otherwise. Instead, they enable workflows that weren't previously possible.</p>
<h2>Creating Vector Networks</h2>
<p>With an understanding of what Vector Networks are, we can now take a look at how we would go about implementing them.</p>
<h3>Graph</h3>
<p>The main data structure behind Vector Networks is a graph. A graph can be thought of as a collection of nodes and edges.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/37.svg"></p>
<p>A graph</p>
<h3>Nodes</h3>
<p>A graph may have any number of nodes. For our purposes nodes have two properties, a unique <code>id</code> and a <code>position</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/38.svg"></p>
<h3>Edges</h3>
<p>Edges are the connection between two nodes. Each edge is a composed of two <em>edge parts</em>. An edge part contains a node's id and an optional control point.</p>
<p>The labels <code>n0</code> and <code>n1</code> will be used to refer to the nodes at the start and end of an edge, respectively. The control points will be labeled <code>cp0</code> and <code>cp1</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/39.svg"></p>
<p>If the control points of the edge are omitted, the edge becomes a straight line.</p>
<h2>Filling the holes</h2>
<p><img src="https://alexharri.com/images/posts/vector-networks/40.gif"></p>
<p>Source: <a target="_blank" href="https://www.figma.com/blog/introducing-vector-networks/">https://www.figma.com/blog/introducing-vector-networks/</a></p>
<p>When working with vector networks, the Fill tool allows you to “toggle” the fill of different “areas” of the graph.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/41.svg"></p>
<p>These areas can be defined as a sequence of node <code>id</code>s that go in a circle, a loop, if you will.</p>
<p>This loopy sequence is referred to as a <em>cycle</em>. In the above example, the cycle would consist of the nodes <code>n0</code>, <code>n1</code>, <code>n3</code>, <code>n4</code>, <code>n5</code>, <code>n6</code> and <code>n7</code>. These cycles will be written like <code>(0, 1, 3, 4, 5, 6, 7)</code>.</p>
<p>If you were to count the different visually distinct “areas” of the cycle your answer would probably be three, but you could easily find more than three cycles.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/42.svg"></p>
<p>What makes this correct or incorrect?</p>
<p>The sequence <code>(0, 1, 2, 3, 4, 5, 6, 7)</code> is a cycle and it loops, but it is not what we're looking for. The problem can be illustrated with the “how many triangles” puzzle you might have seen on Facebook.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/43.svg"></p>
<p>How many triangles are in this image?</p>
<p>You should be able to count 24 different triangles depending on which areas you choose to include.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/44.svg"></p>
<p>But that's not what we want. What we need to find are the 16 small areas.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/45.svg"></p>
<p>We need a way to find the <em>“small cycles”</em> in the graph.</p>
<h2>Minimal cycle basis</h2>
<p>This <a target="_blank" href="https://www.geometrictools.com/Documentation/MinimalCycleBasis.pdf">paper on Minimal Cycle Basis</a> is a bit less dense than most others academic papers (and it has pictures!). Its goal is:</p>
<blockquote>
<p>…to compute a minimal number of cycles that form a cycle basis for the graph.</p>
</blockquote>
<h3>What does “Minimal Cycle Basis” mean?</h3>
<p>It's just a fancy way to refer to all of the “small areas” of a graph. You can think of these as the “visually distinct” areas of a graph. Enclosed areas.</p>
<h3>Left or right?</h3>
<p>The main tool for finding the “minimal cycle basis” will be determining which edge to choose based on left- or rightness.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/47.svg"></p>
<p>Should we go to <code>a</code> or <code>b</code>?</p>
<p>We'll think of this in terms of clockwise and counter clockwise.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/48.svg"></p>
<p><code>curr</code> for current, <code>prev</code> for previous</p>
<p>When traveling left, we choose the counter clockwise most edge (CCW) relative to the previous one.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/49.svg"></p>
<p>CCW</p>
<p>When traveling right, we choose the clockwise most edge (CW) relative to the previous one.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/50.svg"></p>
<p>CW</p>
<h3>The algorithm</h3>
<p>We will be finding the minimal cycle basis for the graph we saw earlier.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/51.svg"></p>
<p>The first step is choosing the leftmost node in the graph.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/52.svg"></p>
<p>When traveling from the first node, we want to go clockwise (CW). But relative to which edge?</p>
<p>For the first node, we imagine that the previous edge is “below” the current one. We then pick the CW edge relative to that.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/53.svg"></p>
<p>In this case <code>a</code> is more CW relative to <code>prev</code> than <code>b</code>, so we'll walk to <code>a</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/54.svg"></p>
<p>After the first walk, we start picking the CCW edge.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/55.svg"></p>
<p>In this case, that's <code>b</code>. We repeat the previous step and keep selecting the CCW edge until we reach the original node.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/56.svg"></p>
<p>When we reach the original node again, a cycle is found.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/57.svg"></p>
<p>We now have the first small cycle in the graph.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/58.svg"></p>
<p>When a cycle has been found, the first edge of the cycle is then removed from the graph.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/59.svg"></p>
<p>The first edge, <code>(n0 n1)</code>, is removed</p>
<p>Then, the <em>filaments</em> of the first two nodes in the cycle are removed.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/60.svg"></p>
<p>In this case, we only have a single filament</p>
<p>Filaments are nodes that only have one adjacent edge. Think of these as dead ends. When a filament is found, we also check whether or not the single adjacent node is a filament. This ensures that the first node of the next cycle has two adjacent nodes. We'll see an example of this later.</p>
<p>Now we pick the first node in the next cycle. In our graph, there are two equally left most nodes.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/61.svg"></p>
<p>When this happens, we pick the bottom node, <code>n1</code> in this case.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/62.svg"></p>
<p>We then repeat the process from before. CW from the bottom for the first node, then CCW from the previous node until we find the first node.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/63.svg"></p>
<p>We find the cycle <code>(1, 2, 3)</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/64.svg"></p>
<p>Now we have the cycles <code>(0, 1, 3, 4, 5, 6, 7)</code> and <code>(1, 2, 3)</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/65.svg"></p>
<p>Then we remove the first edge of the cycle and filaments like before. We start by removing the filaments connected to the first two nodes of the cycle.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/66.svg"></p>
<p>We keep going until there aren't any filaments left.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/67.svg"></p>
<p>Finding the next cycle is pretty obvious.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/68.svg"></p>
<p>CW then CCW</p>
<p>We now have all the cycles of the graph.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/69.svg"></p>
<p>This is the minimal cycle basis of our graph! Now we can toggle the fills of these cycles as we please.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/70.svg"></p>
<h2>The math</h2>
<p>I want to dig into how the clockwise-ness of an edge relative to another edge is determined.</p>
<p>The only prerequisite for understanding this section are <a target="_blank" href="https://www.mathsisfun.com/algebra/vectors.html">vectors</a>; arrows pointing from one point of a 2D coordinate system to another.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/71.svg"></p>
<p>i = (1, 0), j = (0, 1)</p>
<p>With two vectors sitting at the origin, <code>i</code> and <code>j</code>, we can create a square like so.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/72.svg"></p>
<p>For the unit vectors <code>(1, 0)</code> and <code>(0, 1)</code> the square has an area of 1.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/73.svg"></p>
<p>We can do this with any two vectors.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/74.svg"></p>
<p>This shape is called a <em>parallelogram</em>. Parallelograms have one property that we care about, which is that their area is equal to the absolute value of their determinant.</p>
<p>That may sound like jargon, but the determinant happens to be really useful for us. Take a look at what happens when we move the vectors of the one-by-one square closer together.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/75.svg"></p>
<p>When the vectors get closer, their area gets smaller. And when the vectors are parallel, the determinant and area become 0.</p>
<p>At this point the natural question to ask is what happens when we keep going and the blue arrow is to the right of the green arrow?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/76.svg"></p>
<p>The determinant becomes negative!</p>
<p>When the blue vector <code>j</code> is to the left of the green vector <code>i</code> the determinant of the parallelogram becomes negative. When the opposite is true it becomes positive.</p>
<p>The implication for our use case is that we can check whether the determinant of two vectors is positive or negative to determine whether or not a vector is to the left or right of another vector.</p>
<p>And we can do this no matter the direction because the area of a parallelogram does not change depending on the orientation of the vectors that create it.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/77.svg"></p>
<p>The determinant changes when the orientation of the vectors change <strong>relative to each other</strong>.</p>
<p>With this knowledge as our weapon, we can create a function, <code>det(i, j)</code>, that takes in two vectors and returns the determinant.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/78.svg"></p>
<p>The function will return a positive value when <code>j</code> is to the left (CCW) of <code>i</code>.</p>
<h3>Applying the math</h3>
<p>Say we're in the middle of the finding a cycle and we're deciding whether or not to move to <code>n0</code> or <code>n1</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/79.svg"></p>
<p>Let's move this into the coordinate system.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/80.svg"></p>
<p>We'll start off with <code>a</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/81.svg"></p>
<p>We want to get the vector from <code>curr</code> to <code>a</code>, which we do by subtracting <code>curr</code> from <code>a</code>. We'll call this new vector <code>da</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/82.svg"></p>
<p>We can do the same for <code>curr</code> using <code>prev</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/83.svg"></p>
<p>Now we can determine whether <code>a</code> is left of <code>curr</code> by computing the determinant of the parallelogram that <code>da</code> and <code>dcurr</code> form.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/84.svg"></p>
<p>Note that the order is important. If we use <code>da</code> as <code>i</code> the area is negative. If we use it as <code>j</code> it becomes positive.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/85.svg"></p>
<p>We can do the same with <code>b</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/86.svg"></p>
<p>With this information as our weapon, we know whether or not <code>a</code>, <code>b</code> and <code>curr</code> are left or right of each other.</p>
<p>What do we do with this information?</p>
<h2>The green zone</h2>
<p>We will be focusing on determining whether <code>da</code> is more CCW than <code>db</code> relative to <code>curr</code>. Simply put, is <code>da</code> left of <code>db</code>?</p>
<p>If <code>da</code> is more CCW than <code>db</code> relative to <code>dcurr</code>, <code>da</code> can be said to be <em>better</em> than <code>db</code>.</p>
<p>The first step is to determine whether the angle between <code>dcurr</code> and <code>db</code> is convex.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/87.svg"></p>
<p>This “convexity” can more easily visualized by shifting <code>dcurr</code> back and imagining an arc like so:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/88.svg"></p>
<p>The angle is convex</p>
<p>If the angle is convex, we we use the following expression to check whether <code>da</code> is better than <code>db</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/89.svg"></p>
<p>The ∨ symbol represents the logical OR operator in math.</p>
<p>Let's take a look at the individual parts of this expression.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/90.svg"></p>
<p>Is <code>da</code> CCW of <code>dcurr</code>?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/91.svg"></p>
<p>Is <code>da</code> CCW of <code>db</code>?</p>
<p>I find that it's pretty hard to visualize this mentally, so I think of the two different expressions creating a “green zone” where <code>da</code> is better than <code>db</code>.</p>
<p>For the first part of the expression (is <code>da</code> left of <code>dcurr</code>), the green area looks like so.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/92.svg"></p>
<p>The second part of the expression asks if <code>da</code> is left of <code>db</code>. The green area look looks like so.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/93.svg"></p>
<p>And since it's an OR expression, either of these sub-expressions being true would result in <code>a</code> being better than <code>b</code>. Thus, the green area looks like this:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/94.svg"></p>
<p>Is <code>a</code> better than <code>b</code>?</p>
<p>We use this to determine the better-ness of <code>a</code> when the angle is convex.</p>
<p>But what if the angle from <code>dcurr</code> to <code>db</code> is concave?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/95.svg"></p>
<p>Then the expression looks like so:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/96.svg"></p>
<p>The only thing that changed here is that the logical OR operator (∨) changed to the logical AND operator (∧).</p>
<p>Let's take a look at what happens with the green zones using this expression.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/97.svg"></p>
<p>Is <code>da</code> left of <code>dcurr</code>?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/98.svg"></p>
<p>Is <code>da</code> left of <code>db</code>?</p>
<p>And since these sub-expressions are joined by logical AND, the green zone looks like so:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/99.svg"></p>
<p>Using this method, we can always get the CCW or CW most node. And the great thing is that this method is independent of rotation and really cheap to compute.</p>
<h3>Computing the determinant</h3>
<p>Given two vectors, the <a target="_blank" href="https://en.wikipedia.org/wiki/Determinant">determinant</a> can be computed with this formula:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/100.svg"></p>
<h2>Intersections in the graph</h2>
<p>Let's go back to our graph for a bit.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/101.svg"></p>
<p>This graph is the simplest, most optimistic case. This graph only has straight lines, and no two lines cross each other.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/102.svg"></p>
<p>This box shape has an intersection. The edge <code>(0, 2)</code> crosses the edge <code>(1, 3)</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/103.svg"></p>
<p>With the intersection, the above area looks fillable. But defining the “filled area” is pretty difficult.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/104.svg"></p>
<p>What makes defining this area so difficult? Consider this rectangle and line.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/105.svg"></p>
<p>There are two intersections with the edge <code>(4, 5)</code> intersecting <code>(0, 1)</code> and <code>(2, 3)</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/106.svg"></p>
<p>Say the area left of the line is filled. What happens if we move the line left?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/107.svg"></p>
<p>Obviously the area shrinks, but what if we keep going and move the line outside of the rectangle? Which of these outcomes below should be the result, and why?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/108.svg"></p>
<p>In this case kinda feels like the rectangle should be empty. But what if we move the line right?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/109.svg"></p>
<p>Should it then be filled? Sure, but what if we move the line up or down instead? Should the rectangle fill or empty when the line is no longer separating the two sections?</p>
<h2>Expanding the graph</h2>
<p>This is how I believe Figma solves this problem. I call it “expanding the graph”, but the engineers at Figma probably use a different vocabulary to describe it.</p>
<p>Expanding the graph means taking each intersection, creating a node at the point of the intersections, and then splitting the edges that intersected each other at that point.</p>
<p>This is the original graph:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/110.svg"></p>
<p>The edge <code>(0, 2)</code> intersects the edge <code>(1, 3)</code>. When expanded, the graph would look like so:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/111.svg"></p>
<p>A new node, <code>5</code> would be added at the point of the intersection.</p>
<p>The edges <code>(0, 2)</code> and <code>(1, 3)</code> have been removed and replaced by the edges <code>(0, 5)</code>, <code>(5, 2)</code>, <code>(1, 5)</code>, and <code>(5, 3)</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/112.svg"></p>
<p>The structure of the graph has been changed</p>
<p>Here's a graphic that should illustrate this a bit more clearly.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/113.svg"></p>
<p>Expanding an intersection</p>
<h3>Multiple intersections</h3>
<p>These steps are pretty simple for line edges with a single intersections. But each edge can have multiple other edges intersecting it, and two cubic bezier curves can create 9 intersections.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/114.svg"></p>
<p>This complicates things a bit. Let's take a look at a bezier-line intersection.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/115.svg"></p>
<p>The best way to go about this is to treat the intersections for an edge as separate from the edge that intersected it.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/116.svg"></p>
<p>The <code>t</code> values go from 0 at the start of the curve to 1 at the end of it</p>
<p>The line has two intersections at <code>t = 0.3</code> and <code>t = 0.7</code>. The bezier has two intersections, but at <code>t = 0.25</code> and <code>t = 0.75</code>.</p>
<p>Before we move on with this example, I want to introduce a different way of thinking about edges since I believe it will help with the overall understanding of the problem.</p>
<h3>Duplicate edges</h3>
<p>Two nodes may be connected multiple times by different edges.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/117.svg"></p>
<p>In this graph, an edge represented by the node pair <code>(2, 3)</code> could represent either of the two edges that connect <code>n2</code> and <code>n3</code>.</p>
<p>To get around this problem, we will give each edge a unique <code>id</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/118.svg"></p>
<p>For most future examples, I will still refer to edges by the nodes they connect since I feel it's easier to think about. But for the next example it's better to separate nodes and edges.</p>
<h3>Intersection map</h3>
<p>We can structure the data for the intersections of the edges like so:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/119.svg"></p>
<p>Creating nodes at the intersection points of edges</p>
<p>When we encounter an intersection, we create a node whose position is at the intersection. We then add intersections to an <em>intersection map</em> that will contain the intersections for each edge with a corresponding <code>t</code> value and a <code>nodeId</code>. These intersections are sorted by the <code>t</code> value.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/120.svg"></p>
<p>For the intersection with the lowest <code>t</code> value, we create an edge with the first <em>edge part</em> having the <code>nodeId</code> of the first edge part of the original edge. The second edge part should contain the <code>nodeId</code> of the intersection. This creates the edge <code>(2, 4)</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/121.svg"></p>
<p>Subsequent edges will have the first edge part's <code>nodeId</code> be the <code>nodeId</code> of the previous intersection and the second <code>nodeId</code> be the <code>nodeId</code> of the current intersection. In this example, that edge is <code>(4, 5)</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/122.svg"></p>
<p>One additional edge will be created for each edge with any intersections.</p>
<p>The first edge part's <code>nodeId</code> will be the <code>nodeId</code> of the last intersection and the second edge part's <code>nodeId</code> will be the <code>nodeId</code> of the second edge part of the original edge.</p>
<p>This was a bit of a mouthful, so hopefully this graphic helps a bit with understanding that alphabet soup.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/123.svg"></p>
<p>Separating the intersections of an edge from the edges that created those intersections makes it easier to think about. It alleviates some of the complexity that might arise from multiple edges intersecting with each other.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/124.svg"></p>
<h2>Self-intersection</h2>
<p>Cubic beziers can self-intersect.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/125.svg"></p>
<p>This, unfortunately, means that every single cubic bezier edge has to be checked for self-intersection. It's an interesting problem that involves finding the two different <code>t</code> values that the bezier intersects itself at, but I won't be covering how to find those values here.</p>
<p>Once you have the <code>t</code> values, a self-intersecting bezier can be expanded like so:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/126.svg"></p>
<p>The blue node should be invisible to the user</p>
<p>We insert <code>n3</code> since having a node with an edge that has itself on both ends of the edge is problematic, but it should be hidden from the user.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/127.svg"></p>
<p>Intersecting the loop of a self-intersecting bezier</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/128.svg"></p>
<p>Removing n3 at the first opportunity</p>
<h2>Curvy edges</h2>
<p>Earlier we covered the CW - CCW graph traversal algorithm to find the minimal cycle basis (small areas).</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/129.svg"></p>
<p>Finding the better (counter clockwise most) point adjacent to <code>curr</code></p>
<p>But the algorithm described in the paper was designed to work with nodes connected by straight lines that don't intersect. Introducing edges defined by cubic beziers introduces significant complexity.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/130.svg"></p>
<p>Which edge to choose, blue or green?</p>
<p>In the example above, we can find out that the blue edge is better than the green one by using the determinant. We are stilling defining better to mean the CCW most edge.</p>
<p>When working with cubic bezier curves, the naive solution would be to just convert the bezier to a line defined by the points at the start and end of the curve.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/131.svg"></p>
<p>But that idea breaks down as soon as one edge curves over the other.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/132.svg"></p>
<p>Oops</p>
<p>Let's take a fresh look at a bezier curves and try to work from there.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/133.svg"></p>
<p>Looking at this, we notice that the tangent at the start of the curve, <code>n0</code>, is parallel to the line from <code>n0</code> to <code>cp0</code>. So to get the direction at the start of the edge we can use the line <code>(n0, cp0)</code>.</p>
<p>For clarity, the start of our edge, <code>n0</code>, is the same node as <code>curr</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/134.svg"></p>
<p>So by converting edges defined by cubic beziers into a line defined by <code>(n0, cp0)</code>, we get the <em>initial</em> angle of the curve.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/135.svg"></p>
<p>This seems like a good solution when looking at the “curve around” case.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/136.svg"></p>
<p>Looks like we've solved the problem. Right?</p>
<h3>No intersections</h3>
<p>Before we move on to further edge cases, it helps to understand that any solutions assume that no two edges may intersect when deciding which edge to travel.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/137.svg"></p>
<p>This is not allowed</p>
<p>The edges of the graph we're traversing must not have any intersections when we compute the cycles (minimal cycle basis) of the graph.</p>
<p>We can only operate on an <em>expanded graph</em>.</p>
<p>Like we covered earlier, an expanded graph is a graph that has replaced all intersections with new nodes and edges. So if the original, user-defined graph has any intersections, they would have to be expanded before we can find the graph's cycles.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/138.svg"></p>
<p>The same edges as above, but expanded</p>
<h2>Parallel edges</h2>
<p>The next edge case is two edges being parallel (pointing in the same direction).</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/139.svg"></p>
<p>If the lines go in the same direction, determining which is better is impossible without more information.</p>
<p>Here are a few possible solutions for the cases where the control points of the curves are parallel.</p>
<h3>Point at <code>t</code></h3>
<p>What if we just take the point on the curve at, for example, <code>t = 0.1</code>?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/140.svg"></p>
<p>This produces the correct result for curves of a similar length, but we can easily break this with one curve being significantly bigger than the other.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/141.svg"></p>
<p>This is effectively the same problem as the “curve around” case we saw earlier.</p>
<h3>Point at length</h3>
<p>Instead of taking a point at a fixed <code>t</code> value, we could take a point at some length along the curve. The length would be determined by some point on the smaller curve, e.g. at <code>t=0.1</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/142.svg"></p>
<p>I have not tried implementing this since I have another working solution, but this could possibly be a viable and performant solution if it works for all edge cases.</p>
<h3>Lasers!</h3>
<p>The next solution is a bit esoteric but produces the correct result. This is the solution I'm currently using.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/143.svg"></p>
<p>We begin by splitting each bezier at <code>t = 0.05</code> (image above is exaggerated). We then tesselate each part into n points.</p>
<p>Then, for each point of the tesselated bezier, we check whether a line from <code>n0</code> to that point intersects the other edge.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/144.svg"></p>
<p>It's pretty hard to see what's going on at this scale, so let's zoom in a bit.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/145.svg"></p>
<p>When a point intersects the other edge, we use the point before it.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/146.svg"></p>
<p>Found an intersection</p>
<p>Let's zoom in a bit.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/147.svg"></p>
<p>The intersection close up</p>
<p>For the other edge, we have no intersection.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/148.svg"></p>
<p>In that case, we just use the end of the edge as the direction line.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/149.svg"></p>
<p>With this method we've produced lines that seem to represent their respective curves.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/150.svg"></p>
<p>And this also works for the “curve around” case.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/151.svg"></p>
<p>But it fails for a “curve behind” case.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/152.svg"></p>
<p>This would produce the green edge as the more CCW edge, which is wrong.</p>
<p>My solution to this problem is to shoot an infinite laser in the direction of the previous edge.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/153.svg"></p>
<p>We then check whether the points of the tesselated bezier intersect this laser.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/154.svg"></p>
<p>But a line from <code>n0</code> to the points would never intersect the laser.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/155.svg"></p>
<p>Passes right through</p>
<p>Instead, we can create a line from the current point to the previous point and use that for the intersection test.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/156.svg"></p>
<p>When we intersect the laser, we use the previous point. The previous point will always be on the correct side of the laser.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/157.svg"></p>
<p>The point we use</p>
<p>And like that, we have a solution.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/158.svg"></p>
<h2>Parallel, but in reverse!</h2>
<p>It could also be the case that the blue or green edges, <code>a</code> and <code>b</code> respectively, could be parallel to the edge from <code>curr</code> to <code>prev</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/159.svg"></p>
<p><code>a</code> is parallel to <code>prev</code></p>
<p>The process for finding the better edge follows a process similar to the one described above so we will cover this very quickly.</p>
<p>There are two cases:</p>
<h3>A or B are parallel to <code>Prev</code> , but not both</h3>
<p>If either <code>a</code> or <code>b</code>, but not both, are parallel to <code>prev</code>, we can simply compare the parallel edge to <code>prev</code>.</p>
<p>If the parallel edge is CW of <code>prev</code>, the parallel edge is better.</p>
<p>If the parallel edge is CCW of <code>prev</code>, the other edge is better.</p>
<p>Think a bit about why this is true.</p>
<p>If one edge is parallel to <code>prev</code> and curves CW, and the other is not parallel to <code>prev</code>, then the parallel edge is as CCW as can be. This means that the green zone for the other edge is completely empty.</p>
<p>The reverse is true if the parallel edge curves CCW, since it would be as CW as possible. This means that the green zone for the other edge is the whole circle.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/160.svg"></p>
<h3>Both A and B are parallel to Prev</h3>
<p>Using the same laser solution as before, this case is covered.</p>
<h2>Cycles inside of cycles</h2>
<p>Now we're going to look at fills for a bit.</p>
<p>Let's take a look at a basic example of a graph with a cycle inside of another cycle.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/161.svg"></p>
<p>You would expect the graph's areas to be defined like so:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/162.svg"></p>
<p>But as it stands, if you hover over the outer area you get a different, unsatisfactory result.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/163.svg"></p>
<p>But this makes sense. Let's take a look at the nodes of the graph.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/164.svg"></p>
<p>The cycle <code>(0, 1, 2, 3)</code> describes the outer boundary of the area we want, but we aren't describing the “inner boundary” of the area yet.</p>
<p>Let's take a look at how we can do that.</p>
<h3>Even-odd rule</h3>
<p>Telling a computer to draw the outline of a 2D shape is simple enough. But if you want to fill that shape, how do you tell the computer what is “inside” and what is “outside”?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/165.svg"></p>
<p>One way of finding out whether a point is inside a shape or not is by shooting an infinite laser in any direction from that point and counting how many “walls” it passes through.</p>
<p>If the laser intersects an odd number of walls, it's inside of the shape. Otherwise it is outside of the shape.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/166.svg"></p>
<p>Intersects 1 wall, we're inside of the shape</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/167.svg"></p>
<p>Intersects 4 walls, we're outside of the shape</p>
<p>This works for any 2D shape, no matter which point you choose and which direction you shoot the laser in.</p>
<p>This also helps in the case of nested paths.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/168.svg"></p>
<p>This gives us an idea for how we can define the “inner boundary” of a shape.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/169.svg"></p>
<h3>Reducing closed walks</h3>
<p>Let's look at a graph with a cycle nested inside of another cycle, but with an edge connecting two nodes of the cycles.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/170.svg"></p>
<p>This will lead back to how we can think about nested cycles and give us a deeper understanding on how to think about them.</p>
<p>Let's find the cycles. We use the same CW-CCW method as usual.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/171.svg"></p>
<p>With this method, we go on what looks like a small detour around the inner cycle.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/172.svg"></p>
<p>When we reach the node we started at, this is what the cycle looks like.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/173.svg"></p>
<p>This is the first cycle we've seen where we cross a node twice (both <code>n3</code> and <code>n4</code>). Something interesting appears when we take a look at the direction that the cycle takes throughout the graph.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/174.svg"></p>
<p>We start off traveling CCW, but when we cross the edge from the outer cycle to the inner cycle the orientation we travel seems to flip.</p>
<p>I will state for now that we want to separate the outer cycle from the inner cycle and treat the edge between them as if it didn't exist. I will go into the <em>why</em> later and explain the <em>how</em> here.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/175.svg"></p>
<p>We take all repeated nodes, in this case <code>n3</code>, and remove them from the cycle. We also remove any nodes that are between the two repeated nodes.</p>
<p>You might notice that <code>n4</code> is also repeated, but since it's “inside” of the part of the cycle that <code>n3</code> removes, we can ignore it.</p>
<p>We leave one instance of the repeated node, and then we have the cycle that would have been found if the <em>crossing</em> didn't exist.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/176.svg"></p>
<p>We then mark the edge that connected the outer cycle from the inner cycle. I call these marked edges <em>crossings</em>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/177.svg"></p>
<p>It could also be the case that an outer-inner cycle combo has multiple crossings.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/178.svg"></p>
<p>In that case, we mark all edges adjacent to the node connected to the outer cycle as a crossing.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/179.svg"></p>
<p>And after all this is done, our cycles look like so:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/180.svg"></p>
<h2>Subcycles</h2>
<p>Instead of referring to “inner” and “outer” cycles, I will refer to subcycles and parent cycles. This will make it easier to think about multiple cycles relative to each other.</p>
<p>Having said that, let's introduce a third cycle.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/185.svg"></p>
<p>When we hover over the outermost cycle, what do you expect to happen?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/186.svg"></p>
<p>Because of the even-odd rule, the innermost cycle is filled too!</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/187.svg"></p>
<p>To fix this, we can introduce the concept of <em>direct subcycles</em>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/188.svg"></p>
<p>Parent cycles (blue) and their direct subcycles (green)</p>
<p>A parent cycle may have multiple direct subcycles. But due to the non-intersection rule, a subcycle may only have a single parent cycle.</p>
<p>Let's take a look at how this works.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/189.svg"></p>
<p>This graph has a a rectangle, our outermost cycle, which has two direct subcycles: a diamond and an hourglass. The diamond has two direct subcycles of its own, and the hourglass has three direct subcycles.</p>
<p>We will begin with the rectangle and its direct subcycles. We will name them, <code>c0</code>, <code>c1</code> and <code>c2</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/190.svg"></p>
<p>The user has decided to fill some of these cycles, and leave some of them empty.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/191.svg"></p>
<p><code>c0</code> and <code>c1</code> are filled, and <code>c2</code> is empty</p>
<p>Let's draw the graph without a stroke and with a gray fill. When drawing this graph we start with the outermost cycle, <code>c0</code>.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/192.svg"></p>
<p>The graph to the left with the render to the right</p>
<p>Since <code>c0</code> is filled, we draw it. If it were not filled we could skip drawing it. We can shoot a laser out of the rectangle and see that it intersects the walls of the rectangle once, so we can expect it to be filled considering the even-odd rule.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/193.svg"></p>
<p>This may seem really obvious, but it's good to have the rules of the game laid out clearly before we move on.</p>
<p>Next we want to draw <code>c1</code>, the diamond in our graph. It was filled, just like the rectangle so we should draw it as well. But if we try to draw the diamond as well, we get the wrong result.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/194.svg"></p>
<p>Our laser is intersecting two walls as a result of drawing both of the shapes when the have the same fill setting.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/195.svg"></p>
<p>We intersect an even number of walls, so we're “outside” of the shape</p>
<p>So to draw the image the user wanted we can simply skip drawing the diamond since the parent cycle implicitly draws direct subcycles with the same fill setting.</p>
<p>The hourglass, <code>c2</code>, is supposed to be empty. With that being the case, just not drawing it seems like a reasonable conclusion. But since the parent cycle (rectangle) has already drawn the hourglass as if it were filled we need to “flip” the fill by drawing the hourglass.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/196.svg"></p>
<p>And again, if we try to use the laser intersection method we see that the number of intersections is 2, an even number. And with the even-odd rule, an even number of walls means you're “outside” of the shape.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/197.svg"></p>
<p>Now that we've drawn the rectangle and its direct subcycles, we can move onto the direct subcycles of those.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/198.svg"></p>
<p>When working with <code>c3</code> and <code>c4</code>, the direct subcycles of <code>c1</code>, we can treat them as if they're direct subcycles of <code>c0</code> since <code>c1</code> had the same fill setting.</p>
<p>For <code>c3</code>, we want to “flip” the fill setting so we draw it. But <code>c4</code> has the same fill setting as its parent cycle so we don't draw it.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/199.svg"></p>
<p>Even number of intersections so we're outside of the shape</p>
<p>And we can think of <code>c5</code>, <code>c6</code> and <code>c7</code> in the same way. We don't care whether they're filled or empty when rendering them. We care whether or not they have the same fill as their parent cycle.</p>
<p>We only need to draw cycles if their parent cycle has the opposite “fill setting” as themselves. If they have the same fill setting, we don't have to draw them.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/200.svg"></p>
<p>This means that when drawing cycles, start by drawing the outermost “filled” cycle and then look at that cycle's subcycles. If a subcycle has the same fill setting as its parent cycle, it should not be drawn.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/201.svg"></p>
<h2>Contiguous cycles</h2>
<p>A graph may have multiple “clusters” of cycles.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/202.svg"></p>
<p>I use the phrase <em>contiguous cycles</em> to describe the “togetherness” of the cycles, if you will. I often think of these contiguous groups of cycles as being in different colors.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/203.svg"></p>
<p>Finding these contiguous cycles can be done with a depth-first traversal:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/204.svg"></p>
<p>Start at the first node of the cycle</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/205.svg"></p>
<p>Color each node you find</p>
<p>But remember the <em>crossings</em>? In the search, you may not crawl to adjacent nodes by edges marked as crossings.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/206.svg"></p>
<p>So in the end, our colors actually look like this:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/207.svg"></p>
<p>Take this group of contiguous cycles nested inside another group of contiguous cycles:</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/208.svg"></p>
<p>Because of the non-intersection rule we know that if one of the nodes in a group of contiguous cycles is inside of a cycle not in the group, all of them are.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/209.svg"></p>
<p>This “contiguous cycles” idea is maybe not the most interesting part of this post on the surface, but I've found it to be useful when working on Vector Networks.</p>
<h2>Partial expansion</h2>
<p>When hovering an area defined by intersections, we are showing a cycle of the expanded graph.</p>
<p>Take this triangle as an example.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/210.svg"></p>
<p>If we hover over one of its areas, we see an area defined by nodes that don't exist yet.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/211.svg"></p>
<p>What the blue striped area represents is the area whose fill state would be “toggled” if the user clicks the left mouse button. This area does not exist on the graph as the user defined it. It exists as a cycle on the expanded version of the original graph.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/212.svg"></p>
<p>The expanded graph</p>
<p>When the user clicks to toggle the fill state of the area, we would first have to expand the graph for the nodes and edges that make up that area to exist.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/213.svg"></p>
<p>The expanded graph</p>
<p>But by doing that we've expanded two intersections that we didn't need to expand to be able to describe the area. These expansions are destructive in nature and should be avoided when possible.</p>
<p>Instead, we can <em>partially expand</em> the graph by only expanding the intersections that define the selected cycle.</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/214.svg"></p>
<p>Partially expanded graph</p>
<p>This allows us to maintain as much of the original graph as possible while still being able to define the fill.</p>
<h3>Implementing partial expansions</h3>
<p>The basic implementation is reasonably simple. When you create the expanded graph, just add a little metadata to each expanded node that tells you which two edges of the original graph were used to create it and at what <code>t</code> values those intersections occurred.</p>
<p>Then when the cycle is clicked, iterate over each node. If the node exists in the expanded graph but not the original graph, add it to a new partially expanded graph.</p>
<p>There are edge cases, but I will not be covering them here.</p>
<h2>Omitted topics</h2>
<p>Here are some of the topics that I decided to omit for this post. Go have a stab at them yourself!</p>
<h3>Joins</h3>
<p>Figma offers three types of joins. Round, pointy and square. How could these different types of joins be implemented?</p>
<p><img src="https://alexharri.com/images/posts/vector-networks/215.svg"></p>
<h3>Stroke align</h3>
<p>Figma also offers three ways to align the stroke of a graph: center, inside and outside.</p>
<p>How do you determine inside- or outside-ness and what happens when the graph has no cycles?</p>
<h3>Boolean operations</h3>
<p>Figma, like most vector graphics tools, offers <a target="_blank" href="https://help.figma.com/article/65-boolean-operations">boolean operations</a>. How could those be implemented?</p>
<p><a target="_blank" href="http://paperjs.org/">Paper.js</a> is open source and has boolean operations for paths, maybe you can start there?</p>
<h2>Future topics</h2>
<p>These are some of the more open-ended features and ideas I want to explore in the future.</p>
<h3>A different way of working with fills</h3>
<p>There are alternatives to how Figma allows the user to work with fills.</p>
<p>One possible solution I'm interested in exploring is multiple different “fill layers” that use one vector object as a reference. This would solve the “<a target="_blank" href="https://spectrum.chat/figma/feature-requests/paint-bucket-tool-with-multiple-colors~1b55179b-f911-468b-9355-fd361564fda0">one graph, multiple colors</a>” problem without having to duplicate the layer and keep multiple vector objects in sync if you want to make changes later on.</p>
<h3>Animating the graph</h3>
<p>Given an <a target="_blank" href="https://helpx.adobe.com/after-effects/using/expression-basics.html">expression</a> and reference based system similar to After Effects, what could you achieve when you combine it with Vector Networks?</p>
<p>Or maybe we could make use of a node editor similar to <a target="_blank" href="https://docs.blender.org/manual/en/latest/editors/shader_editor/index.html">Blender's shader editor</a> or <a target="_blank" href="https://www.blackmagicdesign.com/products/fusion/visualeffects">Fusion's node based workflow</a>?</p>
<p>There's a lot of exploration to be done here and I'm really excited to dive into this topic.</p>
<h2>In closing</h2>
<p>Thanks for reading this post! I hope it served as a good introduction to what I think is a really interesting problem space. I've been working on this problem alongside school and work for a good while. It's part of an animation editor plus runtime for the web I'm working on. I intend for a modified version of Vector Networks to be the core of a few features.</p>
<p>I've been working on implementing Vector Networks for a bit over half a year now. The vector editor is pretty robust when it comes to creating, modifying and expanding the graph. But the edge cases when modifying the fill state have been stumping me for quite a while now.</p>
<p>I wanted to have a fully working demo before publishing this post, but it's going to be a few months until it's stable enough for it to be usable for people that are not me.</p>
<p>The big idea behind the project is to be a piece of animation software that's tailor-made for creating and running dynamic animations on the web. I'll share more about this project at a later date.</p>
<p>I also just think that Figma's Vector Networks are super cool and it's really hard to find material about it online. I hope this post helps fix the lack of information that I encountered when attempting to find information about Vector Networks.</p></main></div></div>]]></description>
        </item>
    </channel>
</rss>